[{"number": 35200, "title": "[Intel MKL] Adding support to public CI for AVX512 builds for various versions of gcc", "body": "Manually specifying each instruction on the command line was hurting performance in instances where gcc suppressed support for older SIMD instructions when newer instructions were used (e.g. don't use SSE when AVX instructions are available). This commit makes use of gcc's built-in platform-based optimizations rather than rolling our own.\r\n\r\nAlso, the older versions of gcc didn't support newer platform code names, so this commit adds support for detecting the version of gcc in the environment and then adding platform support in the idiom that version of gcc comprehends. ", "comments": ["Hi @penpornk. Do you have a few minutes to review this PR?", "@claynerobison Can you please check build failures? Thanks!", "@claynerobison Could you please help fix Ubuntu Sanity failures? Thanks!"]}, {"number": 35199, "title": "Softmax activations don't get converted to Softmax TFLite operator if ndim > 2", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.1\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): tf-nightly==2.1.0.dev20191203\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nimport pathlib\r\n\r\ninpt = tf.keras.layers.Input(shape=[256, 256, 3])\r\nout = tf.keras.layers.Lambda(lambda x: tf.keras.activations.softmax(x))(inpt)\r\nout = tf.keras.layers.Lambda(lambda x: tf.nn.softmax(x))(out)\r\nmodel = tf.keras.Model(inpt, out)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\npathlib.Path('out.tflite').write_bytes(tflite_model)\r\n```\r\n\r\n**Failure details**\r\n![image](https://user-images.githubusercontent.com/1422280/71025023-3904e880-20d4-11ea-95fb-29cfea49a44d.png)\r\nThis graph shows the difference between the different softmax methods.  When using `tf.keras.activations.softmax`, there is [code](https://github.com/tensorflow/tensorflow/blob/v2.1.0-rc1/tensorflow/python/keras/activations.py#L43-L79) with a workaround for multiple dimensions.  It looks like this was written before the tensorflow op had multi-dimension support. ", "comments": ["@ghop02 Can you please try `experimental_new_converter` and let us know whether the issue persists with most recent converter. I had tried the converter for several complex layers and the converter is working well. I have added `experimental_new_converter` to your code. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/85eebf3e3df8471d9b87b70943b759d0/untitled715.ipynb). \r\n\r\n`converter.experimental_new_converter = True`\r\n\r\nThanks!", "Hi @jvishnuvardhan , thanks for the response!\r\n\r\n![image](https://user-images.githubusercontent.com/1422280/71234090-acba1780-22c5-11ea-8f33-1ea7fc12a051.png)\r\n\r\nI still get a similar result.  If you look at the [link above](https://github.com/tensorflow/tensorflow/blob/v2.1.0-rc1/tensorflow/python/keras/activations.py#L43-L79), the keras softmax is converting to:\r\n```\r\n  elif ndim > 2:\r\n    e = math_ops.exp(x - math_ops.reduce_max(x, axis=axis, keepdims=True))\r\n    s = math_ops.reduce_sum(e, axis=axis, keepdims=True)\r\n    return e / s\r\n```\r\n\r\nI think removing that logic, and just always returning the tf.nn.softmax operation would work, since I believe that code was written before tensorflow had support for `ndim > 2`", "Seems reasonable though I don't have the full history behind the Keras implementation. @fchollet any thoughts (or somebody else who can chime in)?", "Any additional thoughts on this? Just bumping in case it got lost during the holidays!", "Facing same issue with tf.keras(TF 2.1.0), when is use x=Activation(\"softmax\")(ip).\r\nThe tflite conversion is successful; but the layers appear as shown by @ghop02 \r\n\r\n![tfbug](https://user-images.githubusercontent.com/1130185/73448772-9b196480-4387-11ea-82bc-32e6b15e5820.png)\r\n", "@anilsathyan7 if you're blocked, you should be able to make this change to sneak the softmax operator in:\r\n```diff\r\n- out = Activation(\"softmax\")(in)\r\n+ out = tf.keras.layers.Lambda(lambda x: tf.nn.softmax(x))(in)\r\n```", "@ghop2: If you want to submit a PR for updating the [Keras softmax behavior](https://github.com/tensorflow/tensorflow/blob/v2.1.0-rc1/tensorflow/python/keras/activations.py#L73), we'd be happy to approve. Thanks again for flagging the issue.", "> @anilsathyan7 if you're blocked, you should be able to make this change to sneak the softmax operator in:\r\n> \r\n> ```diff\r\n> - out = Activation(\"softmax\")(in)\r\n> + out = tf.keras.layers.Lambda(lambda x: tf.nn.softmax(x))(in)\r\n> ```\r\n\r\nI will also add that TF2.1 works with conversion but TF2.2 does not, when using this method", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35199\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35199\">No</a>\n"]}, {"number": 35198, "title": "Change TrtConversionParams to class from NamedTuple and export it", "body": "- Changes NamedTuple to a class.\r\n- Supports backward compatibility by providing _replace() method to emulate the behavior of _replace() in NamedTuple.\r\n- Exports ConversionParams to API", "comments": ["@pooyadavoodi would you please fix the ubuntu sanity errors? Thanks.", "> @pooyadavoodi would you please fix the ubuntu sanity errors? Thanks.\r\n\r\nDone. Thanks @aaroey \r\n\r\nI always forget to run pylint before pushing PRs. I wish there was a way to automate this.", "@pooyadavoodi this breaks tensorflow/tools/api/tests:api_compatibility_test, could you take a look?", "> @pooyadavoodi this breaks tensorflow/tools/api/tests:api_compatibility_test, could you take a look?\r\n\r\nI believe I fixed the golden api in the last commit. I had to rebase onto the latest master because there was a keras api test failure.", "@sanjoy Please let me know if I should do anything for this PR?", "> @sanjoy Please let me know if I should do anything for this PR?\r\n\r\nThanks for pinging!  Could be that folks who usually do the merge are OOO for holidays.\r\n\r\n@gbaned what needs to happen here?", "> > @sanjoy Please let me know if I should do anything for this PR?\r\n> \r\n> Thanks for pinging! Could be that folks who usually do the merge are OOO for holidays.\r\n> \r\n> @gbaned what needs to happen here?\r\n\r\n@sanjoy  import/copybara check is failing. We can proceed further once it is successful. Thank you!", "@gbaned I don't know anything about import/copybara. It seems that the failure has no log. How can I check what actually failed?", "@pooyadavoodi here is the internal error , can you please chekc once \r\n`Traceback (most recent call last):\r\n  File \"<embedded stdlib>/unittest/case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"<embedded stdlib>/unittest/case.py\", line 605, in run\r\n    testMethod()\r\n  File \"/google3/runfiles/google3/third_party/tensorflow/tools/api/tests/api_compatibility_test.py\", line 428, in testAPIBackwardsCompatibilityV2\r\n    omit_golden_symbols_map=omit_golden_symbols_map)\r\n  File \"/google3/runfiles/google3/third_party/tensorflow/tools/api/tests/api_compatibility_test.py\", line 367, in _checkBackwardsCompatibility\r\n    api_version=api_version)\r\n  File \"/google3/runfiles/google3/third_party/tensorflow/tools/api/tests/api_compatibility_test.py\", line 287, in _AssertProtoDictEquals\r\n    self.fail('%d differences found between API and golden.' % diff_count)\r\n  File \"/google3/runfiles/google3/third_party/py/absl/testing/absltest.py\", line 1761, in fail\r\n    return super(TestCase, self).fail(self._formatMessage(prefix, msg))\r\n  File \"<embedded stdlib>/unittest/case.py\", line 670, in fail\r\n    raise self.failureException(msg)\r\nAssertionError: 1 differences found between API and golden.\r\n`", "@rthadur @sanjoy  I can't repro this error.\r\n\r\nThis is what I am getting when running test `//tensorflow/tools/api/tests:api_compatibility_test`\r\n\r\n```\r\nINFO:tensorflow:No differences found between API and golden.\r\nI0106 19:41:36.972908 140491009787712 api_compatibility_test.py:290] No differences found between API and golden.\r\n[       OK ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV1\r\n[ RUN      ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV2\r\nINFO:tensorflow:No differences found between API and golden.\r\nI0106 19:41:39.017542 140491009787712 api_compatibility_test.py:290] No differences found between API and golden.\r\n[       OK ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV2\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessage\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessage\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessageV1\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessageV1\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessageV2\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessageV2\r\n[ RUN      ] ApiCompatibilityTest.test_session\r\n[  SKIPPED ] ApiCompatibilityTest.test_session\r\n----------------------------------------------------------------------\r\nRan 7 tests in 8.121s\r\n\r\nOK (skipped=1)\r\n================================================================================\r\nTarget //tensorflow/tools/api/tests:api_compatibility_test up-to-date:\r\n  bazel-bin/tensorflow/tools/api/tests/api_compatibility_test\r\nINFO: Elapsed time: 1270.347s, Critical Path: 716.82s\r\nINFO: 6652 processes: 6652 local.\r\nINFO: Build completed successfully, 6655 total actions\r\n//tensorflow/tools/api/tests:api_compatibility_test                      PASSED in 12.1s\r\n  WARNING: //tensorflow/tools/api/tests:api_compatibility_test: Test execution time (12.1s excluding execution overhead) outside of range for MODERATE tests. Consider setting timeout=\"short\" or size=\"small\".\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\nINFO: Build completed successfully, 6655 total actions\r\n```", "@pooyadavoodi thank you , @sanjoy can you please suggest ?", "@annarev we have a case where `ApiCompatibilityTest.testAPIBackwardsCompatibilityV2` is failing in google3 after the import but [passing](https://github.com/tensorflow/tensorflow/pull/35198#issuecomment-571309341) in open source.  Any idea what's going on?", "@alextp had this comment in the internal version of this CL:\r\n\r\n> (for tf-api-owners)\r\n> \r\n> 1. It looks like the default argument is now mutable. This is dangerous as assigning to it can have unexpected consequences to unrelated code. Preserving immutability is a good idea (you can use patterns like .withchange(change) methods which return copies of the tuple).\r\n> \r\n> 2. It doesn't look like the changed type was exposed in the public API. If so, how are users supposed to construct it to pass as arguments to that function?\r\n", "> @alextp had this comment in the internal version of this CL:\r\n> \r\n> > (for tf-api-owners)\r\n> > \r\n> > 1. It looks like the default argument is now mutable. This is dangerous as assigning to it can have unexpected consequences to unrelated code. Preserving immutability is a good idea (you can use patterns like .withchange(change) methods which return copies of the tuple).\r\n> > 2. It doesn't look like the changed type was exposed in the public API. If so, how are users supposed to construct it to pass as arguments to that function?\r\n\r\n@sanjoy I made `_replace()` to return a copy of the object instead of changing it. Please let me know if that is sufficient.\r\n\r\nI also exported `ConversionParams` which we previously missed in TF2.1.\r\n", "Passing the buck to @alextp -- Alex, can you PTAL if your comments in https://github.com/tensorflow/tensorflow/pull/35198#issuecomment-573832904 have been addressed?", "> @annarev we have a case where `ApiCompatibilityTest.testAPIBackwardsCompatibilityV2` is failing in google3 after the import but [passing](https://github.com/tensorflow/tensorflow/pull/35198#issuecomment-571309341) in open source. Any idea what's going on?\r\n\r\nSorry just saw the comment. Is this still an issue?\r\nOne problem could be if the goldens need to be updated both for TF 1.x and TF 2.x versions. ", "@sanjoy @alextp The test is failing on github but passing on my machine locally. Any idea why?\r\n\r\nResults in my machine:\r\n```\r\nINFO:tensorflow:No differences found between API and golden.\r\nI0117 18:57:16.801649 140141113898816 api_compatibility_test.py:290] No differences found between API and golden.\r\n[       OK ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV1\r\n[ RUN      ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV2\r\nINFO:tensorflow:No differences found between API and golden.\r\nI0117 18:57:19.938147 140141113898816 api_compatibility_test.py:290] No differences found between API and golden.\r\n[       OK ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV2\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessage\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessage\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessageV1\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessageV1\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessageV2\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessageV2\r\n[ RUN      ] ApiCompatibilityTest.test_session\r\n[  SKIPPED ] ApiCompatibilityTest.test_session\r\n----------------------------------------------------------------------\r\nRan 7 tests in 11.279s\r\n\r\nOK (skipped=1)\r\n```", "> > @annarev we have a case where `ApiCompatibilityTest.testAPIBackwardsCompatibilityV2` is failing in google3 after the import but [passing](https://github.com/tensorflow/tensorflow/pull/35198#issuecomment-571309341) in open source. Any idea what's going on?\r\n> \r\n> Sorry just saw the comment. Is this still an issue?\r\n> One problem could be if the goldens need to be updated both for TF 1.x and TF 2.x versions.\r\n\r\nHi @annarev, I think this is still an issue, see https://github.com/tensorflow/tensorflow/pull/35198#issuecomment-575753807 ?", "Looks like when I run `bazel test tensorflow/tools/api/tests:api_compatibility_test`, it fails similar to github:\r\n```\r\nERROR:tensorflow:1 differences found between API and golden.\r\nE0122 01:03:45.536555 139929196537664 api_compatibility_test.py:264] 1 differences found between API and golden.\r\nIssue 1 : New object tensorflow.experimental.tensorrt.ConversionParams found (added).\r\n[  FAILED  ] ApiCompatibilityTest.testAPIBackwardsCompatibility\r\n...\r\nERROR:tensorflow:1 differences found between API and golden.\r\nE0122 01:03:54.671969 139929196537664 api_compatibility_test.py:264] 1 differences found between API and golden.\r\nIssue 1 : New object tensorflow.experimental.tensorrt.ConversionParams found (added).\r\n[  FAILED  ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV2\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessage\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessage\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessageV1\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessageV1\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessageV2\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessageV2\r\n[ RUN      ] ApiCompatibilityTest.test_session\r\n[  SKIPPED ] ApiCompatibilityTest.test_session\r\n======================================================================\r\nFAIL: testAPIBackwardsCompatibility (__main__.ApiCompatibilityTest)\r\ntestAPIBackwardsCompatibility (__main__.ApiCompatibilityTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/233a431e54f1cbffefcb6321279dfb61/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/\r\ntests/api_compatibility_test.py\", line 390, in testAPIBackwardsCompatibility\r\n    omit_golden_symbols_map=omit_golden_symbols_map)\r\n  File \"/root/.cache/bazel/_bazel_root/233a431e54f1cbffefcb6321279dfb61/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/\r\ntests/api_compatibility_test.py\", line 367, in _checkBackwardsCompatibility\r\n    api_version=api_version)\r\n  File \"/root/.cache/bazel/_bazel_root/233a431e54f1cbffefcb6321279dfb61/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/\r\ntests/api_compatibility_test.py\", line 287, in _AssertProtoDictEquals\r\n    self.fail('%d differences found between API and golden.' % diff_count)\r\n  File \"/root/.cache/bazel/_bazel_root/233a431e54f1cbffefcb6321279dfb61/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/absl_py/absl/testing/absltest.py\", l\r\nine 1655, in fail\r\n    return super(TestCase, self).fail(self._formatMessage(prefix, msg))\r\nAssertionError: 1 differences found between API and golden.\r\n```\r\n\r\nBut when I run `bazel test tensorflow/tools/api/tests:api_compatibility_test -- --update_goldens True`, although the log reports the same failure, but it reports the final result as OK.\r\n\r\n```\r\nINFO:tensorflow:No differences found between API and golden.\r\nI0117 18:57:16.801649 140141113898816 api_compatibility_test.py:290] No differences found between API and golden.\r\n[       OK ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV1\r\n[ RUN      ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV2\r\nINFO:tensorflow:No differences found between API and golden.\r\nI0117 18:57:19.938147 140141113898816 api_compatibility_test.py:290] No differences found between API and golden.\r\n[       OK ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV2\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessage\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessage\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessageV1\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessageV1\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessageV2\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessageV2\r\n[ RUN      ] ApiCompatibilityTest.test_session\r\n[  SKIPPED ] ApiCompatibilityTest.test_session\r\n----------------------------------------------------------------------\r\nRan 7 tests in 11.279s\r\n\r\nOK (skipped=1)\r\n```\r\n\r\n- Issue 1) When I run with `-- --update_goldens True`, it shows OK at the end although there are failures in the log.\r\n- Issue 2) When I run with `-- --update_goldens True`, it doesn't update the golden.\r\n- Question 1) Should I update the golden manually, given that the script doesn't work?\r\n- Question 2) Should the manual update be adding a new pbtxt file given that we have exported a new object called `ConversionParams`?\r\n- Question 3) What should be the name of the new pbtxt file for `ConversionParams`. Perhaps: \r\n`tensorflow/tools/api/golden/v2/tensorflow.experimental.tensorrt.-conversion-params.pbtxt`?\r\n", "I added a new empty file manually, and then `-- --update_goldens True` updated the file.\r\nIt would be great to document the requirement of adding files manually if it's not already documented.\r\nHopefully this fixes everything.", "> Looks like when I run `bazel test tensorflow/tools/api/tests:api_compatibility_test`, it fails similar to github:\r\n\r\nDid you figure out why it wasn't failing in https://github.com/tensorflow/tensorflow/pull/35198#issuecomment-575753807?\r\n\r\nOther than that, I'm waiting for @alextp here.", "Backward compatibility doesn't require that we cannot change the default\nvalue, just that the new default value has the same meaning as the old\ndefault value. So Sanjoy's suggestion is what needs to happen her.e\n\nOn Thu, Jan 23, 2020 at 5:32 PM Sanjoy Das <notifications@github.com> wrote:\n\n> *@sanjoy* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/compiler/tensorrt/trt_convert.py\n> <https://github.com/tensorflow/tensorflow/pull/35198#discussion_r370439586>\n> :\n>\n> > @@ -951,7 +971,7 @@ def __init__(self,\n>                 input_saved_model_dir=None,\n>                 input_saved_model_tags=None,\n>                 input_saved_model_signature_key=None,\n> -               conversion_params=DEFAULT_TRT_CONVERSION_PARAMS):\n> +               conversion_params=TrtConversionParams()):\n>\n> We can't change the default value to None due to backward compatibility.\n>\n> I may be missing some Python subtleties, but can you default it to None\n> and then within the function do\n>\n> if conversion_params is None:\n>   conversion_params = TrtConversionParams()\n>\n> ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/35198?email_source=notifications&email_token=AAABHRJEHMBJ22ZATNO3TTLQ7JAKRA5CNFSM4J37OG22YY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCS42WVI#discussion_r370439586>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNF3KVVAWGFLFWP7LLQ7JAKRANCNFSM4J37OG2Q>\n> .\n>\n\n\n-- \n - Alex\n", "> > Looks like when I run `bazel test tensorflow/tools/api/tests:api_compatibility_test`, it fails similar to github:\r\n> \r\n> Did you figure out why it wasn't failing in [#35198 (comment)](https://github.com/tensorflow/tensorflow/pull/35198#issuecomment-575753807)?\r\n> \r\n> Other than that, I'm waiting for @alextp here.\r\n\r\nI think those two issues still exist (I worked around them by adding the new pbtxt file manually and looking at the whole log instead of only final results for failure reports):\r\n\r\n- When I run with `-- --update_goldens True`, it shows OK at the end although there are failures in the log.\r\n- When I run with `-- --update_goldens True`, it doesn't update the golden.", "@pooyadavoodi you might be running it on the wrong directory (it assumes it'll be run from within the root directory)", "The windows build is failing: https://source.cloud.google.com/results/invocations/1eb40532-983f-47e5-b172-f91a7c7f842c/targets/%2F%2Ftensorflow%2Ftools%2Fci_build%2Fbuilds:gen_win_out/log\r\n\r\n```\r\nERROR: T:/src/github/tensorflow/tensorflow/python/keras/api/BUILD:116:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_aoh3x156\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_aoh3x156\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 85, in <module>\r\n    from tensorflow.python.ops.standard_ops import *\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_aoh3x156\\runfiles\\org_tensorflow\\tensorflow\\python\\ops\\standard_ops.py\", line 115, in <module>\r\n    from tensorflow.python.compiler.tensorrt import trt_convert_windows as trt\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_aoh3x156\\runfiles\\org_tensorflow\\tensorflow\\python\\compiler\\tensorrt\\trt_convert_windows.py\", line 30, in <module>\r\n    class TrtConversionParams(object):\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_aoh3x156\\runfiles\\org_tensorflow\\tensorflow\\python\\compiler\\tensorrt\\trt_convert_windows.py\", line 35, in TrtConversionParams\r\n    max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,\r\nNameError: name 'DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES' is not defined\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: T:/src/github/tensorflow/tensorflow/python/tools/BUILD:98:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\nINFO: Elapsed time: 1336.958s, Critical Path: 546.13s\r\nINFO: 5660 processes: 3285 remote cache hit, 2375 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```", "> The windows build is failing: https://source.cloud.google.com/results/invocations/1eb40532-983f-47e5-b172-f91a7c7f842c/targets/%2F%2Ftensorflow%2Ftools%2Fci_build%2Fbuilds:gen_win_out/log\r\n> \r\n> ```\r\n> ERROR: T:/src/github/tensorflow/tensorflow/python/keras/api/BUILD:116:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\n> Traceback (most recent call last):\r\n>   File \"\\\\?\\T:\\tmp\\Bazel.runfiles_aoh3x156\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n>     from tensorflow.python.tools.api.generator import doc_srcs\r\n>   File \"\\\\?\\T:\\tmp\\Bazel.runfiles_aoh3x156\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 85, in <module>\r\n>     from tensorflow.python.ops.standard_ops import *\r\n>   File \"\\\\?\\T:\\tmp\\Bazel.runfiles_aoh3x156\\runfiles\\org_tensorflow\\tensorflow\\python\\ops\\standard_ops.py\", line 115, in <module>\r\n>     from tensorflow.python.compiler.tensorrt import trt_convert_windows as trt\r\n>   File \"\\\\?\\T:\\tmp\\Bazel.runfiles_aoh3x156\\runfiles\\org_tensorflow\\tensorflow\\python\\compiler\\tensorrt\\trt_convert_windows.py\", line 30, in <module>\r\n>     class TrtConversionParams(object):\r\n>   File \"\\\\?\\T:\\tmp\\Bazel.runfiles_aoh3x156\\runfiles\\org_tensorflow\\tensorflow\\python\\compiler\\tensorrt\\trt_convert_windows.py\", line 35, in TrtConversionParams\r\n>     max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,\r\n> NameError: name 'DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES' is not defined\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> ERROR: T:/src/github/tensorflow/tensorflow/python/tools/BUILD:98:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\n> INFO: Elapsed time: 1336.958s, Critical Path: 546.13s\r\n> INFO: 5660 processes: 3285 remote cache hit, 2375 local.\r\n> FAILED: Build did NOT complete successfully\r\n> FAILED: Build did NOT complete successfully\r\n> ```\r\n\r\nI added the necessary definitions. \r\nHopefully this fixes the issue. Unfortunately I can't build on Windows locally to test this.", "@rthadur I think it will be easier if I fixed these internally. Can you please import the CL and CC it to me?", "> @rthadur I think it will be easier if I fixed these internally. Can you please import the CL and CC it to me?\r\n\r\nSorry, looks like you already did that. :) Let me see if I can quickly fix these locally."]}, {"number": 35197, "title": "ModuleNotFoundError: No module named 'tensorflow.contrib'", "body": "Hello, I'm trying to use TF but I have an issue...\r\nWhen I run my code I have :\r\n\r\n    import tflearn\r\n  File \"//anaconda3/lib/python3.7/site-packages/tflearn/__init__.py\", line 4, in <module>\r\n    from . import config\r\n  File \"//anaconda3/lib/python3.7/site-packages/tflearn/config.py\", line 5, in <module>\r\n    from .variables import variable\r\n  File \"//anaconda3/lib/python3.7/site-packages/tflearn/variables.py\", line 7, in <module>\r\n    from tensorflow.contrib.framework.python.ops import add_arg_scope as contrib_add_arg_scope\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n\r\nI believe I'm using tensorflow 2.0. \r\n\r\nThe modules in my code : \r\nimport nltk\r\nfrom nltk.stem.lancaster import LancasterStemmer\r\nimport numpy\r\nimport tflearn\r\nimport tensorflow \r\nimport json \r\nimport random\r\n\r\nCan you help me ? \r\nThank you.", "comments": ["It is most probably a version issue, as tensorflow.contrib has been removed in version 2.0, and version <= 1.14 is needed to use tflearn.\r\n\r\nSimilar to issue #30794", "@MrRex42 As mentioned in the above comment try using Tensorflow version <=1.14 and this should solve your issue. I am gonna close this issue as it has been resolved. Thanks!"]}, {"number": 35196, "title": "Update boringssl to 80ca9f9", "body": "This PR is trying to resolve the issue in #35179\r\nwhere boringssl was not the latest version. The old version\r\n7f63442 was released one and half year ago so it is time to update.\r\n\r\nThis PR updates boringssl to the latest 80ca9f9.\r\n\r\nThis PR fixes #35179.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["looks good to me, but checking with @perfinion to see if there is a released version he would rather have us use.", "@perfinion Any update on this PR, please. Thanks!", "@perfinion Any update please ? Thanks!", "BoringSSL doesn't have releases (https://github.com/google/boringssl)\r\n\r\nLet's try to bring this back up to date, picking a commit that is compatible with openssl 1.1.1", "whoops sorry I missed this review. This looks fine to me. The only potential thing to be aware of is that boringssl versions correspond to some openssl versions. When distros build against the system libraries they'll build against openssl. I've had no issues in the last many months so dont expect any issues. I've been building against openssl-1.1."]}, {"number": 35195, "title": "Tensorflow not detecting/recognizing GPU", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: PC\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip/conda (both, neither work)\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 10.0, cuDNN 7\r\n- GPU model and memory: NVIDIA GeForce 940M\r\n\r\n\r\n\r\nI'm using an NVIDIA GeForce 940M, and have followed the instructions to install Tensorflow GPU exactly as given [here](https://www.tensorflow.org/install/gpu):\r\n\r\n    # Add NVIDIA package repositories\r\n    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\r\n    sudo dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\r\n    sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\n    sudo apt-get update\r\n    wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\n    sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\n    sudo apt-get update\r\n    \r\n    # Install NVIDIA driver\r\n    sudo apt-get install --no-install-recommends nvidia-driver-418\r\n    # Reboot. Check that GPUs are visible using the command: nvidia-smi\r\n    \r\n    # Install development and runtime libraries (~4GB)\r\n    sudo apt-get install --no-install-recommends \\\r\n        cuda-10-0 \\\r\n        libcudnn7=7.6.2.24-1+cuda10.0  \\\r\n        libcudnn7-dev=7.6.2.24-1+cuda10.0\r\n    # Install TensorRT. Requires that libcudnn7 is installed above.\r\n    sudo apt-get install -y --no-install-recommends libnvinfer5=5.1.5-1+cuda10.0 \\\r\n        libnvinfer-dev=5.1.5-1+cuda10.0\r\n\r\nThis is the output of `nvidia-smi`:\r\n\r\n    +-----------------------------------------------------------------------------+\r\n    | NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n    |-------------------------------+----------------------+----------------------+\r\n    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n    |===============================+======================+======================|\r\n    |   0  GeForce 940M        On   | 00000000:01:00.0 Off |                  N/A |\r\n    | N/A   52C    P0    N/A /  N/A |    238MiB /  4046MiB |      6%      Default |\r\n    +-------------------------------+----------------------+----------------------+\r\n                                                                                   \r\n    +-----------------------------------------------------------------------------+\r\n    | Processes:                                                       GPU Memory |\r\n    |  GPU       PID   Type   Process name                             Usage      |\r\n    |=============================================================================|\r\n    |    0      1027      G   /usr/lib/xorg/Xorg                            24MiB |\r\n    |    0      1184      G   /usr/bin/gnome-shell                          46MiB |\r\n    |    0      1388      G   /usr/lib/xorg/Xorg                           110MiB |\r\n    |    0      1555      G   /usr/bin/gnome-shell                          52MiB |\r\n    +-----------------------------------------------------------------------------+\r\n\r\nand `nvcc -v`:\r\n\r\n    nvcc: NVIDIA (R) Cuda compiler driver\r\n    Copyright (c) 2005-2018 NVIDIA Corporation\r\n    Built on Sat_Aug_25_21:08:01_CDT_2018\r\n    Cuda compilation tools, release 10.0, V10.0.130\r\n\r\nand I've installed `tensorflow-gpu` using both conda and pip (neither work).\r\n\r\nThe output of\r\n\r\n    from tensorflow.python.client import device_lib\r\n    print(device_lib.list_local_devices())\r\n\r\nis:\r\n\r\n    [name: \"/device:CPU:0\"\r\n    device_type: \"CPU\"\r\n    memory_limit: 268435456\r\n    locality {\r\n    }\r\n    incarnation: 9340164754758349370\r\n    , name: \"/device:XLA_CPU:0\"\r\n    device_type: \"XLA_CPU\"\r\n    memory_limit: 17179869184\r\n    locality {\r\n    }\r\n    incarnation: 3967057350071782501\r\n    physical_device_desc: \"device: XLA_CPU device\"\r\n    ]\r\n\r\nAs seen, tensorflow does not recognize the GPU. What do I do?\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@Meraj99 \r\nTensorFlow automatically places operations on a GPU, if possible. As you only have one GPU in your system, it is used by default.\r\nFind more information here: https://www.tensorflow.org/guide/using_gpu\r\nMake sure you have TensorFlow GPU installed:\r\nhttps://www.tensorflow.org/install/gpu\r\n\r\nPlease, go through the [link1](https://github.com/tensorflow/tensorflow/issues/21411),[link2](https://github.com/tensorflow/tensorflow/issues/31267) and see if it helps you. Thanks!", "I've fixed it by `pip uninstall tensorflow`. The tensorflow-gpu still works."]}, {"number": 35194, "title": "Failed to convert weights to 8 bit precision: \"Quantize weights tool only supports tflite models with one subgraph\" ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab (GPU)\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 2.1.0-dev20191217 \r\n- **Python version**: 3\r\n- **Exact command to reproduce**:\r\n\r\n```bash\r\n!pip install tf-nightly\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Sequential\r\nfrom tensorflow.keras.layers import GRU, Dense, Dropout\r\n\r\nmodel = Sequential()\r\nmodel.add(GRU(100, activation='relu', return_sequences=False, input_shape=(128,2)))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(11, activation='softmax'))\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ntflite_model_quant = converter.convert()\r\n```\r\n\r\n### Error message\r\n```bash\r\nE tensorflow/lite/tools/optimize/quantize_weights.cc:351] Quantize weights tool only supports tflite models with one subgraph.\r\n```\r\n\r\n### Describe the problem\r\nFirst, I used the new converter (with the experimental flag converter.experimental_new_converter = True) to convert an RNN model from TensorFlow to TensorFlow Lite's flat buffer format, as suggested in the issue #32608.\r\n\r\nThis works correctly, but then when I try to perform a post-training weight quantization, I got an error saying that the quantize weights tool only supports tflite models with one subgraph. \r\n\r\nIs there a problem with my procedure? Or is that feature not yet supported? In that case, I would like to request this feature.\r\n\r\nThanks in advance for your help.\r\n\r\n### Source code / logs\r\nThe attached file can be used to reproduce the error with a trained model (.h5). \r\n[GRU_1L.zip](https://github.com/tensorflow/tensorflow/files/3974453/GRU_1L.zip)\r\n\r\n\r\n", "comments": ["I have a very similar setup, with LSTM layers.\r\nI can convert the model with experimental set up and the inference works, but as soon as I try to do post training optimization in form of weight quantization, I get the same error.", "Hey, is there any news on this issue? Thanks!", "I am also interested in quantizing a model with LSTM layers and am having this issue.", "I still get an error, when I try to convert a model with lstm layers into tf lite and use post training optimization in form of: \r\n```\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = custom_generator\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                      tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\nI get this error:\r\n```\r\nRuntimeError: Only models with a single subgraph are supported, model had 5 subgraphs\r\n```\r\nWhen I do not use optimizations, it works. \r\nI am using tf 2.1.0 and tf-nightly version from 10.02.2020", "I get the same error when using `tf.scan` in one of my layers. tf.lite conversion works fine if I don't enable post-training quantization but crashes when I turn it on.\r\n\r\nThere are several issues around this error but its never mentioned whether this is an inherent limitation in the tf.lite converter or if there is a fix coming. Can somebody clarify this?\r\n\r\nThanks!", "Same problem with TensorFlow 2.2.0rc1. Is there a way to bypass this?", "I was able to quantize my model by setting\r\n```python\r\nconverter.experimental_new_quantizer = True\r\n```\r\nMight be worth trying out this setting for your models as well.", "> I was able to quantize my model by setting\r\n> \r\n> ```python\r\n> converter.experimental_new_quantizer = True\r\n> ```\r\n> \r\n> Might be worth trying out this setting for your models as well.\r\n\r\nDoesn't work on my GRU", "> > I was able to quantize my model by setting\r\n> > ```python\r\n> > converter.experimental_new_quantizer = True\r\n> > ```\r\n> > \r\n> > \r\n> > Might be worth trying out this setting for your models as well.\r\n> \r\n> Doesn't work on my GRU\r\n\r\nOk, I have to add that this only works for me when using tf-nightly, maybe give that a try.", "> > > I was able to quantize my model by setting\r\n> > > ```python\r\n> > > converter.experimental_new_quantizer = True\r\n> > > ```\r\n> > > \r\n> > > \r\n> > > Might be worth trying out this setting for your models as well.\r\n> > \r\n> > \r\n> > Doesn't work on my GRU\r\n> \r\n> Ok, I have to add that this only works for me when using tf-nightly, maybe give that a try.\r\n\r\nIt doesn't work with 2.2.0rc1 but it does work with the current nightly. \ud83d\udc4d At least I got no error anymore but I didn't test yet if the output from the model is correct.", "Thanks a lot @Bewe11 and @BigBerny. The originally posted code (+ converter.optimizations = [tf.lite.Optimize.DEFAULT]) seems to be also working for me (tf_nightly-2.2.0.dev20200418-cp37-cp37m-manylinux2010_x86_64.whl).\r\nHowever, I get an exception when I add a representative dataset (converter.representative_dataset = representative_dataset_gen), which is necessary to actually enable integer operations (https://www.tensorflow.org/lite/performance/post_training_quantization). This is the exception I get:\r\n `Traceback (most recent call last):\r\n  File \"installation_path/tensorflow/lite/python/optimize/calibrator.py\", line 51, in __init__\r\n    _calibration_wrapper.CalibrationWrapper(model_content))\r\nTypeError: pybind11::init(): factory function returned nullptr`\r\nIf I remove the GRU, the network is quantized without any problem. \r\n@Bewe11 and @BigBerny, did you provide a representative dataset? did you experience this problem?", "Same situation @AlbertoEsc.\r\nI did some research and found that the mismatch of CUDA version might cause this problem. \r\nhttps://github.com/NVIDIA/TensorRT/issues/308. Though it didn't solve my problem, it may help you. ", "Hello, I'm having the same problem. I manage to convert and quantize the model by using tf-nightly and the new experimental setup, but only when not attempting to do full integer quantization as described in the [post-training quantization docs](https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization). In that case I run into the same issue as @AlbertoEsc described above.\r\n\r\nIs there any news when this will be fixed? Is there a new workaround?\r\n\r\nThank you!", "Hey there, I'm having the same problem when trying to convert the model with quantization. The model is able to convert without quantization just fine. I am using tf-nightly, but still get the following error: ConverterError: <unknown>:0: error: loc(\"lstm/lstm_cell/bias\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable.\r\n\r\nHas anyone encountered this error, or know how to solve it?\r\nThanks!", "@rutrilla \r\nPlease verify on later tf version and let us know of this is still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I have a similar issue with LSTM layers in tf 2.7.0.dev20210706.\r\nI can convert the model and it works, but only without integer post-training optimization, because I get the same error:\r\n`Failed to parse the model: Only models with a single subgraph are supported, model had 4 subgraphs.`\r\n\r\nThanks", "Hi @AnastGerus, we have fixed this issue since \r\nhttps://github.com/tensorflow/tensorflow/commit/cd5a9ad1338b01f5fd8ce64ed86ad09bf26468ee, could you try again with latest version of tf-nightly?", "> Hi @AnastGerus, we have fixed this issue since\r\n> [cd5a9ad](https://github.com/tensorflow/tensorflow/commit/cd5a9ad1338b01f5fd8ce64ed86ad09bf26468ee), could you try again with latest version of tf-nightly?\r\n\r\nIs this also fixed for GRUs?", "Hi @daverim,\r\nYes, it works with the latest version of tf-nightly. Thank you!\r\nBut it still can't convert with integer post-training optimization, because of error:\r\n`Failed to parse the model: Op FlexVarHandleOp missing inputs.`\r\nMy model is a basic LSTM layer that was converted by PyTorch - ONNX - TF path. So when I import the model to TF I can't fix the input shapes by hand. Could you please recommend to me the way to convert the model with integer optimization?\r\n```\r\nonnx_model = onnx.load(path_to_onnx)\r\nmodel = prepare(onnx_model, 'CPU') \r\nmodel.export_graph(path)\r\n```\r\n\r\nThanks a lot!", "@AnastGerus \r\n\r\nWhen you converting from onnx, could you try converting to a saved_model? And then using `converter.from_saved_model`?\r\nYou may need to add `converter.experimental_enable_resource_variables` as well.\r\n\r\nThanks,\r\nDavid", "@daverim \r\n\r\nThat is what I'm doing:\r\n\r\n```\r\nonnx_model = onnx.load(path_to_onnx)\r\nmodel = prepare(onnx_model, 'CPU')\r\nmodel.export_graph(path) \r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(path)\r\nconverter.experimental_enable_resource_variables = True\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS,\r\n  tf.lite.OpsSet.SELECT_TF_OPS\r\n]\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\ndef representative_dataset():\r\n    with open('dataset.p', 'rb') as fp:\r\n        tensors_list = pickle.load(fp)\r\n    for tensor in tensors_list:\r\n        tensor = np.asarray(tensor, dtype = np.float32)\r\n        h_tensor = np.zeros((1,1,512), dtype = np.float32)\r\n        c_tensor = np.zeros((1,1,512), dtype = np.float32)\r\n        yield [h_tensor, c_tensor, tensor]\r\n\r\nconverter.representative_dataset = representative_dataset\r\ntflite_model = converter.convert()\r\n```\r\n\r\nAnd I get:\r\n```\r\n  File \"...\\lib\\site-packages\\tensorflow\\lite\\python\\optimize\\calibrator.py\", line 78, in __init__\r\n    raise ValueError(\"Failed to parse the model: %s.\" % e)\r\nValueError: Failed to parse the model: Op FlexVarHandleOp missing inputs.\r\n```\r\n\r\nI'm trying to do it with the simplest one LSTM layer, but it has the same issue.\r\n\r\nThanks,\r\nAnastasiia\r\n", "Hi @AnastGerus, it seems that the varhandle op is getting converted via\nselect_tf_ops. Do you require this? It seems that if you remove\nselect_tf_ops it should work. Otherwise, the calibrator is missing support\nfor FlexVarHandleOp, and needs a fix.\n\nPlease let me know if removing select tf ops is ok, and if not, will work\non a fix for the calibrator.\n\nOn Mon, Aug 23, 2021 at 9:32 PM AnastGerus ***@***.***> wrote:\n\n> @daverim <https://github.com/daverim>\n>\n> That is what I'm doing:\n>\n> onnx_model = onnx.load(path_to_onnx)\n> model = prepare(onnx_model, 'CPU')\n> model.export_graph(path)\n>\n> converter = tf.lite.TFLiteConverter.from_saved_model(path)\n> converter.experimental_enable_resource_variables = True\n> converter.target_spec.supported_ops = [\n>   tf.lite.OpsSet.TFLITE_BUILTINS,\n>   tf.lite.OpsSet.SELECT_TF_OPS\n> ]\n>\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\n>\n> def representative_dataset():\n>     with open('dataset.p', 'rb') as fp:\n>         tensors_list = pickle.load(fp)\n>     for tensor in tensors_list:\n>         tensor = np.asarray(tensor, dtype = np.float32)\n>         h_tensor = np.zeros((1,1,512), dtype = np.float32)\n>         c_tensor = np.zeros((1,1,512), dtype = np.float32)\n>         yield [h_tensor, c_tensor, tensor]\n>\n> converter.representative_dataset = representative_dataset\n> tflite_model = converter.convert()\n>\n> And I get:\n>\n>   File \"...\\lib\\site-packages\\tensorflow\\lite\\python\\optimize\\calibrator.py\", line 78, in __init__\n>     raise ValueError(\"Failed to parse the model: %s.\" % e)\n> ValueError: Failed to parse the model: Op FlexVarHandleOp missing inputs.\n>\n> I'm trying to do it with the simplest one LSTM layer, but it has the same\n> issue.\n>\n> Thanks,\n> Anastasiia\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35194#issuecomment-903722201>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AASV4JJB4OFWG7KTZL4OS33T6I5XDANCNFSM4J362GWQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>\n> .\n>\n", "Hi @daverim,\r\nUnfortunately \"select_tf_ops\" can't be removed for LSTM, because (even without any optimizations) you will get an error:\r\n```\r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \r\nTF Select ops: AssignVariableOp, ReadVariableOp, VarHandleOp\r\nDetails:\r\n\ttf.AssignVariableOp(tensor<!tf.resource<tensor<2048xf32>>>, tensor<2048xf32>) -> ()\r\n\ttf.AssignVariableOp(tensor<!tf.resource<tensor<2048xf32>>>, tensor<2048xf32>) -> () : {device = \"\"}\r\n\ttf.AssignVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>, tensor<1024x2048xf32>) -> () : {device = \"\"}\r\n\ttf.AssignVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>, tensor<1x1xf32>) -> ()\r\n\ttf.ReadVariableOp(tensor<!tf.resource<tensor<2048xf32>>>) -> (tensor<2048xf32>) : {device = \"\"}\r\n\ttf.ReadVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>) -> (tensor<?x?xf32>) : {device = \"\"}\r\n\ttf.VarHandleOp() -> (tensor<!tf.resource<tensor<2048xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_bias_lstm_7\"}\r\n\ttf.VarHandleOp() -> (tensor<!tf.resource<tensor<?x?xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_kernel_lstm_7\"}\r\n```\r\nSo I suppose it needs a fix in the calibrator.\r\n\r\nBut the possibility to remove \"select_tf_ops\" for LSTM also would be a good solution, because \"select_tf_ops\" makes TFLite library binary size increase drastically.\r\n\r\nThank you,\r\nAnastasiia", "Hi again, @daverim \r\nIn addition to my previous comment, I have reproduced this issue using only the Keras LSTM layer. _AssignVariableOp, ReadVariableOp, VarHandleOp_ are needed when you use `stateful=True` for LSTM layer (that is very important option for \"infinite\" data (e.g. audio stream)).\r\n\r\nCould you please estimate how much time such a fix could take? Is it worth waiting?\r\n\r\nThanks,\r\nAnastasiia\r\n"]}, {"number": 35193, "title": "ImportError: DLL load failed TF 1.15", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\r\n- TensorFlow version: 1.15\r\n- Python version: 3.7.5\r\n- Installed using virtualenv? pip? conda?: pip inside conda\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Nvidia GTX 750 with 1 GB Memory\r\n\r\n\r\n\r\nI get the error that \"Failed to load the native TensorFlow runtime.\" when trying to use Tensorflow with ImageAI.\r\n\r\nThis is my complete stack-trace:\r\n\r\n> Using TensorFlow backend.\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\pyth\r\n> on\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\pyth\r\n> on\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\pyth\r\n> on\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\n> ion)\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\imp.py\", line 242, in load_mo\r\n> dule\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\imp.py\", line 342, in load_dy\r\n> namic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Documents\\ImageAI\\CustomDetectionTraining.py\", line 1, i\r\n> n <module>\r\n>     from imageai.Detection.Custom import DetectionModelTrainer\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\imageai\\Detecti\r\n> on\\__init__.py\", line 2, in <module>\r\n>     from imageai.Detection.keras_retinanet.models.resnet import resnet50_retinan\r\n> et\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\imageai\\Detecti\r\n> on\\keras_retinanet\\models\\resnet.py\", line 19, in <module>\r\n>     import keras\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\keras\\__init__.\r\n> py\", line 3, in <module>\r\n>     from . import utils\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\keras\\utils\\__i\r\n> nit__.py\", line 6, in <module>\r\n>     from . import conv_utils\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\keras\\utils\\con\r\n> v_utils.py\", line 9, in <module>\r\n>     from .. import backend as K\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\keras\\backend\\_\r\n> _init__.py\", line 1, in <module>\r\n>     from .load_backend import epsilon\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\keras\\backend\\l\r\n> oad_backend.py\", line 90, in <module>\r\n>     from .tensorflow_backend import *\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\keras\\backend\\t\r\n> ensorflow_backend.py\", line 5, in <module>\r\n>     import tensorflow as tf\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\__in\r\n> it__.py\", line 24, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im\r\n> port\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\pyth\r\n> on\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\pyth\r\n> on\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\pyth\r\n> on\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\pyth\r\n> on\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\pyth\r\n> on\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\n> ion)\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\imp.py\", line 242, in load_mo\r\n> dule\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Anaconda3\\envs\\imageai\\lib\\imp.py\", line 342, in load_dy\r\n> namic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n> ", "comments": ["https://github.com/tensorflow/tensorflow/issues/13715\r\n\r\nPossible Solutions: \r\n1. Download wheel file with pip\r\n2. ```pip install protobuf==3.6.0```\r\n3. Updating from python 3.6.0 to python 3.6.4\r\n\r\n\r\nA very similar issue as yours. If not resolved with the help of the above, please ping then.", "Hey, what do you mean by wheel file?", "> Hey, what do you mean by wheel file?\r\n\r\nA WHL file is a package saved in the Wheel format, which is the standard built-package format used for Python distributions. It contains all the files for a Python install and metadata, which includes the version of the wheel implementation and specification used to package it. WHL files are compressed using Zip compression.\r\n\r\n", "@Snickbrack ,\r\nHi, Please make sure system supports AVX, also there are similar issues [28848](https://github.com/tensorflow/tensorflow/issues/28848) and [22794](https://github.com/tensorflow/tensorflow/issues/22794) Kindly refer them. Hope you have followed instructions from [TensorFlow website](https://www.tensorflow.org/install/gpu#windows_setup).Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35193\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35193\">No</a>\n"]}, {"number": 35192, "title": "Automatic Mixed Precision and XLA not working with `model.fit_generator`", "body": "### System information\r\n- **OS Platform and Distribution **: `tensorflow/tensorflow:nightly-gpu-py3` Nvidia Docker image (`ec33d38d1b43`)\r\n- **TensorFlow version (use command below)**: 2.1.0-dev20191106\r\n- **Python version**: Python 3.6.8\r\n- **CUDA/cuDNN version**: 10.0\r\n- **GPU model and memory**: Titan RTX 24GB\r\n\r\n### Describe the problem\r\nWhen using `model.fit_generator()`, Automatic Mixed Precision (AMP) and compiling XLA doesn't seem to work. However, it all works fine with `model.fit()`.\r\n\r\nPlease see below for a complete code sample to reproduce this.\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Sequential\r\nfrom tensorflow.keras.layers import Dense, Flatten\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.utils import Sequence\r\nimport numpy as np\r\nimport random\r\n\r\n\r\ndef mixed_precision_test(use_generator):\r\n    # use XLA\r\n    tf.config.optimizer.set_jit(True)\r\n\r\n    input_shape = (100, 100, 100)\r\n    n_samples = 1000\r\n\r\n    # build the model\r\n    model = Sequential()\r\n    model.add(Dense(16, input_shape=input_shape, activation='relu'))\r\n    model.add(Flatten())\r\n    model.add(Dense(8, activation='relu'))\r\n    model.add(Dense(1, activation='sigmoid'))\r\n\r\n    optimiser = Adam(lr=0.001)\r\n     \r\n    # use AMP\r\n    optimiser = tf.train.experimental.enable_mixed_precision_graph_rewrite(\r\n        optimiser)\r\n\r\n    model.compile(optimizer=optimiser,\r\n                  loss='binary_crossentropy',\r\n                  metrics=['accuracy'])\r\n\r\n    if use_generator:\r\n        class DataGen(Sequence):\r\n\r\n            def __len__(self):\r\n                return n_samples\r\n\r\n            def __getitem__(self, index):\r\n                _x = np.random.rand(1, *input_shape)\r\n                _x = np.array(_x, dtype='uint8')\r\n                _y = np.array([random.choice([0, 1])], dtype='uint8')\r\n                return _x, _y\r\n\r\n        model.fit_generator(generator=DataGen())\r\n    else:\r\n        x = np.random.rand(n_samples, *input_shape)\r\n        x = np.array(x, dtype='uint8')\r\n        y = np.array([random.choice([0, 1]) for _ in range(len(x))],\r\n                     dtype='uint8')\r\n\r\n        model.fit(x, y)\r\n\r\n\r\nif __name__ == '__main__':\r\n    mixed_precision_test(use_generator=False)\r\n    mixed_precision_test(use_generator=True)\r\n```\r\n\r\nWhen setting `use_generator=False`, TensorFlow prints out the following logs:\r\n\r\n```\r\n2019-12-17 15:51:43.549128: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1857] Converted 26/409 nodes to float16 precision using 2 cast(s) to float16 (excluding Const and Variable casts)\r\n2019-12-17 15:51:43.755273: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-12-17 15:51:45.048967: I tensorflow/compiler/jit/xla_compilation_cache.cc:242] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\n```\r\n\r\nindicating that AMP and XLA are working as intended.\r\n\r\nWhen setting `use_generator=True`, those logs are not present and GPU memory consumption is higher, suggesting that no casting to FP16 was performed.\r\n", "comments": ["According to [this SO post](https://stackoverflow.com/questions/59380430/how-to-use-model-fit-which-supports-generators-after-fit-generator-deprecation), `fit_generator` is deprecated as of TF 2.1.0. It suggests passing the generator to `model.fit` instead. I am not able to test this right now, but I'll try to update as soon as I can.", "> According to [this SO post](https://stackoverflow.com/questions/59380430/how-to-use-model-fit-which-supports-generators-after-fit-generator-deprecation), `fit_generator` is deprecated as of TF 2.1.0. It suggests passing the generator to `model.fit` instead. I am not able to test this right now, but I'll try to update as soon as I can.\r\n\r\nFor more read here https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/Model#fit_generator", "Can confirm that using  `model.fit(x=DataGen())` allows you to use mixed precision and XLA.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35192\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35192\">No</a>\n"]}, {"number": 35191, "title": "Enable `model.add_metrics` with non-default floatx symbolic tensors.", "body": "#### General context:\r\n\r\nThe keras API allows to instantiate layers and, more generally, objects (such as `Mean` metrics) with an arbitrary `dtype`, which can be very practical to build `float64` (for high-precision computing) or `float16` (for limited memory usage) layers and models *without* tweaking the backend default `floatx` value. However, in a number of places, dtype selection is not working as one would expect; this PR aims at fixing one of those cases.\r\n\r\nThis specific issue can otherwise be avoided by temporarily hijacking the floatx value (_i.e._ using `tf.keras.backend.set_floatx(my_tensor.dtype)` before calling `model.add_metric(my_tensor, ...)` and calling `set_floatx` again afterwards to reset the prior value), but this feels like an improper hack, especially with the keras API enforcing dtype modularity in most places.\r\n\r\n#### Specific issue:\r\n\r\nWhen building a custom model, one might want to add some metrics tensor(s) using the `tf.keras.Model.add_metrics` method, _e.g._ in the case of a multi-output model built using the functional approach with symbolic tensors. In that case, the current implementation will reject said symbolic tensors if their dtype is not that of `tf.keras.backend.floatx()`, due to the use (in the private backend) of either a `Mean` or `AddMetric` wrapper.\r\n\r\nNote that a similar problem could emerge when using the `add_loss` method (due to the backend use of `AddLoss` in certain cases), however I personally never encountered it.\r\n\r\n#### Submitted fix:\r\n\r\nThis PR fixes the issue when adding symbolic tensors as losses or metrics, simply by having the aforementioned wrappers' `dtype` instantiation kwarg set equal to the wrapped tensor's `dtype` attribute. This results in no user-side nor backend function signature change.\r\n\r\n#### Notes on testing:\r\n\r\nI believe that dtype-changes-targetted tests could, in general, be beneficial to TensorFlow, as this is not the first (nor the last) issue of the sort I come across. I am, however, unsure as to how to integrate this in the current tests workflow, hence the lack of additional unit tests in the initial commit submitted with this PR.\r\n\r\nIt would seem to me that adding some dtype modularity to existing tests would be the proper way, but this might result in extensive increases of testing runtimes (it could, however, be a way to identify the dtype support issues that seem to be found here and there). IMHO, this could constitute a separate task altogether.", "comments": []}, {"number": 35190, "title": "Conv2D accepts strings as 'filters' parameter, but fails to handle them correctly", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux, Linux-5.4.2-1-MANJARO-x86_64-with-arch-Manjaro-Linux\r\n- TensorFlow installed from (source or binary): `tensorflow` binary using pip \r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n\r\nI create a Conv2D layer with string `\"12\"` as parameter for `filters`.\r\nCreation of the object does not fail. However trying to create a tensor out of\r\nthis object results in the following exception:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-26-60e14eee1394>\", line 1, in <module>\r\n    a(b)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 817, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2141, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py\", line 165, in build\r\n    dtype=self.dtype)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 522, in add_weight\r\n    aggregation=aggregation)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 744, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 139, in make_variable\r\n    shape=variable_shape if variable_shape else None)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 258, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 219, in _variable_v1_call\r\n    shape=shape)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 197, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 2507, in default_variable_creator\r\n    shape=shape)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 1406, in __init__\r\n    distribute_strategy=distribute_strategy)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 1537, in _init_from_args\r\n    initial_value() if init_from_fn else initial_value,\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 119, in <lambda>\r\n    init_val = lambda: initializer(shape, dtype=dtype)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops_v2.py\", line 421, in __call__\r\n    fan_in, fan_out = _compute_fans(scale_shape)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops_v2.py\", line 749, in _compute_fans\r\n    fan_out = shape[-1] * receptive_field_size\r\nTypeError: can't multiply sequence by non-int of type 'float'\r\n```\r\n\r\nYou can also create a Conv2D object with other strings like `\"test\"` and it will not fail either.\r\nUsing \"`test`\" yields:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-35-60e14eee1394>\", line 1, in <module>\r\n    a(b)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 817, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2141, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py\", line 165, in build\r\n    dtype=self.dtype)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 522, in add_weight\r\n    aggregation=aggregation)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 744, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 126, in make_variable\r\n    variable_shape = tensor_shape.TensorShape(shape)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 776, in __init__\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 776, in <listcomp>\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 718, in as_dimension\r\n    return Dimension(value)\r\n  File \"/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 193, in __init__\r\n    self._value = int(value)\r\nValueError: invalid literal for int() with base 10: 'test'\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect that creation of the Conv2D object fails with a ValueError when used with a string or that it tries to convert the string into int.\r\nIf the string cannot be converted to int, an exception should be raised at the creation time of the layer, not when building the tensor. Right now, this is different behavior to parameters like `strides` which fails in the stage of building the layer object when presented with an not int-convertible string.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow.python.keras as k\r\na = k.layers.Conv2D(filters=\"12\", kernel_size=3)\r\nb = k.layers.Input((100, 100, 3))\r\na(b)\r\n```\r\nand\r\n```\r\nimport tensorflow.python.keras as k\r\na = k.layers.Conv2D(filters=\"test\", kernel_size=3)\r\nb = k.layers.Input((100, 100, 3))\r\na(b)\r\n```", "comments": ["@bakaschwarz \r\nFilters should be int but not strings. If we give integer value for filter we will not see any error message.\r\nPlease go through this [link](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D?version=stable#arguments_2).Thanks!", "> @bakaschwarz\r\n> Filters should be int but not strings. If we give integer value for filter we will not see any error message.\r\n> Please go through this [link](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D?version=stable#arguments_2).Thanks!\r\n\r\nYou are totally right, that filters should be int, but not strings. That is exactly why i am reporting this though.\r\nBecause this gets accepted and can't be compiled into a tensor:\r\n`a = k.layers.Conv2D(filters=\"test\", kernel_size=3)`\r\nThis get accepted and creates a layer, but cannot be compiled into a tensor:\r\n`a = k.layers.Conv2D(filters=\"12\", kernel_size=3)`\r\nThis fails at layer creation time:\r\n`a = k.layers.Conv2D(filters=12, kernel_size=3, strides=\"test\")`\r\nThis gets accepted and creates a layer and can even be compiled into a tensor:\r\n`a = k.layers.Conv2D(filters=12, kernel_size=3, strides=\"3\")`\r\n\r\nThis is just inconsistent behavior. Why can i use strings for `strides` but not for `filters`. It should either work always or should always fail (which i think would be the correct behavior).\r\n\r\nEDIT: I think they should work the same, because both `filters` and `strides` specify integers as their only possible input.", "@bakaschwarz I agree, it is not consistent but it is not a bug as the `inputs` of different functions are clearly defined. I am not sure how much work required to make it kind of consistent.  Thanks!", "This is fixed latest tf-nightly version '2.2.0-dev20200228' . Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35190\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35190\">No</a>\n"]}, {"number": 35189, "title": "Add TfLite micro ExpandDims reference kernel", "body": "Add TfLite micro ExpandDims reference kernel and tests (int8, uint8, float32)\r\n\r\nSigned-off-by: SiCongLi <sicong.li@arm.com>", "comments": ["Hi, would it be possible to have the review for this PR?", "@sicong-li-arm Can you please check rthadur's comments and keep us posted? Thanks!", "Had a discussion with @njeffrie. Closing this as it's not required by most tflite models."]}, {"number": 35188, "title": "Added hlo/lhlo emitters for Abs, Ceil, Convert, Cos, Negate, Remainder, Sign and Tanh ops.", "body": "", "comments": ["Thanks for fixing the tests. This looks good to go now.", "The convert op isn't specified correctly in the dialect, it currently expects to have the same input and output shape. But the whole point of the convert op is to be able to convert the *types* of shapes.\r\nThe tests generated this error:\r\nFAILED: 'xla_lhlo.convert' op requires all operands to have the same type\r\nWe will probably merge this request with a few fixes, but without the convert op."]}, {"number": 35187, "title": "How to solve RuntimeError in Tensorflow 2.0?", "body": "I want to run a TF1.X programm in TF2.0 on a GPU (RTX 2080 TI) in Anaconda, so I changed the code a bit, but I'll get an error in the last line below.\r\n\r\n```\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\r\ntrain_dataset = train_dataset.apply(tf.data.experimental.shuffle_and_repeat(10000, seed=0))\r\ntrain_dataset = train_dataset.batch(self.batchsize)\r\ntrain_dataset = train_dataset.apply(tf.data.experimental.prefetch_to_device(device))\r\n```\r\n\r\nThe error message is \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 77, in <module>\r\n    reward = manager.get_rewards(ModelGenerator, state_space.parse_state_space_list(action))\r\n  File \"C:\\Users\\user\\tf\\tf2.0_gpu_test\\manager.py\", line 83, in get_rewards\r\n    train_dataset = train_dataset.apply(tf.data.experimental.prefetch_to_device(device))\r\n  File \"C:\\Users\\user\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 1369, in apply\r\n    dataset = transformation_func(self)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_core\\python\\data\\experimental\\ops\\prefetching_ops.py\", line 54, in _apply_fn\r\n    copy_to_device(target_device=device)).prefetch(buffer_size)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 1369, in apply\r\n    dataset = transformation_func(self)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_core\\python\\data\\experimental\\ops\\prefetching_ops.py\", line 78, in _apply_fn\r\n    source_device=source_device).with_options(options)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_core\\python\\data\\experimental\\ops\\prefetching_ops.py\", line 102, in __init__\r\n    self._source_device = ops.convert_to_tensor(source_device)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1184, in convert_to_tensor\r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1242, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 286, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 227, in constant\r\n    allow_broadcast=True)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 235, in _constant_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 96, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nRuntimeError: Can't copy Tensor with type string to device /job:localhost/replica:0/task:0/device:GPU:0.\r\n```\r\n\r\nIt would be awesome if someone could help me.", "comments": ["@ChristianISO ,\r\nHi,could you provide code snippet to replicate the above error ?Thanks!", "@oanush sure, but there isn't much more\r\n\r\ntrain.py line 77:\r\n```\r\nreward = manager.get_rewards(ModelGenerator, state_space.parse_state_space_list(action))\r\n```\r\n\r\nmanager.py line 80-83 (part of function `get_rewards()`):\r\n```\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\r\ntrain_dataset = train_dataset.apply(tf.data.experimental.shuffle_and_repeat(10000, seed=0))\r\ntrain_dataset = train_dataset.batch(self.batchsize)\r\ntrain_dataset = train_dataset.apply(tf.data.experimental.prefetch_to_device(device))\r\n```\r\n\r\n`reward` wasn't used before same as `train_dataset`.\r\n\r\n`X_train` and `y_train` are from the dataset (CIFAR-10).", "@ChristianISO Github is only meant for build/install, bug/performance, feature requests or docs related issues. Please post this question in stackoverflow as there is a bigger community to help\r\n\r\nI am going to close this issue for now. Thanks!"]}, {"number": 35186, "title": "Add TfLite micro Squeeze reference kernel", "body": "Add TfLite micro Squeeze reference kernel and tests (uint8, int8, float32).\r\n\r\nSigned-off-by: SiCongLi <sicong.li@arm.com>", "comments": ["@sicong-li-arm Can you please check njeffrie's comments and keep us posted. Thanks!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35186) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35186) for more info**.\n\n<!-- ok -->", "@njeffrie Sorry for the late reply. As far as we know, mobilenet uses squeeze but it also seems that it's only used in tensorflow not tflite?", "@njeffrie @gbaned  Sorry for the late reply. I resolved the comment last week. Could you please review the changes I made? Thanks!", "Hi @njeffrie. Could you please review the new patch I submitted when you have time? Thanks :)  ", "@njeffrie Could you please give me an update on the new patch? Thanks! ", "@sicong-li-arm Did you get a chance to look on njeffrie's  comments? Please let us know on the update. Thanks!", "Had a discussion with @njeffrie . Closing this as it's not required by most tflite models. "]}, {"number": 35185, "title": "Segmentation fault if importing TF2.1 after sklearn", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): via `pip install tensorflow==2.1.0-rc1`\r\n- TensorFlow version (use command below): v2.1.0-rc0-47-g064e153 2.1.0-rc1\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTF crashes.\r\n\r\n**Describe the expected behavior**\r\nTF doesn't crash.\r\n\r\n**Code to reproduce the issue**\r\nTested in env created with `docker run -it --rm python:3.7.5-slim bash`\r\n```\r\n# pip install tensorflow==2.1.0-rc1 scikit-learn==0.21.3\r\n# python -c \"import sklearn; import tensorflow\"\r\nSegmentation fault\r\n```\r\n\r\n**Other info / logs**\r\ntensorflow==2.1.0-rc0, tensorflow-cpu==2.1.0-rc1, tf-nightly==2.1.0.dev20191217 expose the issue.\r\ntensorflow==2.0.0 works as expected\r\n`python -c \"import tensorflow; import sklearn\"` works as expected (changed import order)\r\n\r\nBacktrace:\r\n```\r\n# gdb python\r\nGNU gdb (Debian 8.2.1-2+b3) 8.2.1\r\nCopyright (C) 2018 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\nType \"show copying\" and \"show warranty\" for details.\r\nThis GDB was configured as \"x86_64-linux-gnu\".\r\nType \"show configuration\" for configuration details.\r\nFor bug reporting instructions, please see:\r\n<http://www.gnu.org/software/gdb/bugs/>.\r\nFind the GDB manual and other documentation resources online at:\r\n    <http://www.gnu.org/software/gdb/documentation/>.\r\n\r\nFor help, type \"help\".\r\nType \"apropos word\" to search for commands related to \"word\"...\r\nReading symbols from python...done.\r\n(gdb) r\r\nStarting program: /usr/local/bin/python \r\nwarning: Error disabling address space randomization: Operation not permitted\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\nPython 3.7.5 (default, Oct 19 2019, 00:13:43) \r\n[GCC 8.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import sklearn\r\n[New Thread 0x7f7f5ab88700 (LWP 1983)]\r\n[New Thread 0x7f7f5a387700 (LWP 1984)]\r\n[New Thread 0x7f7f55b86700 (LWP 1985)]\r\n>>> import tensorflow\r\n\r\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\n0x0000557e44ea5fe0 in ?? ()\r\n(gdb) bt\r\n#0  0x0000557e44ea5fe0 in ?? ()\r\n#1  0x00007f7f07b3ac41 in pybind11::detail::make_new_python_type(pybind11::detail::type_record const&) () from /usr/local/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_events_writer.so\r\n#2  0x00007f7f07b411b9 in pybind11::detail::generic_type::initialize(pybind11::detail::type_record const&) () from /usr/local/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_events_writer.so\r\n#3  0x00007f7f07b41a72 in PyInit__pywrap_events_writer () from /usr/local/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_events_writer.so\r\n#4  0x00007f7f5e467e32 in _PyImport_LoadDynamicModuleWithSpec (spec=spec@entry=0x7f7f524079d0, fp=fp@entry=0x0) at ./Python/importdl.c:159\r\n#5  0x00007f7f5e467bb9 in _imp_create_dynamic_impl (module=<optimized out>, file=<optimized out>, spec=0x7f7f524079d0) at Python/import.c:2170\r\n#6  _imp_create_dynamic (module=<optimized out>, args=<optimized out>, nargs=<optimized out>) at Python/clinic/import.c.h:289\r\n#7  0x00007f7f5e377fe5 in _PyMethodDef_RawFastCallDict (method=0x7f7f5e567840 <imp_methods+320>, self=<optimized out>, args=<optimized out>, nargs=1, kwargs=<optimized out>) at Objects/call.c:530\r\n#8  0x00007f7f5e377cd0 in _PyCFunction_FastCallDict (func=0x7f7f5dcb27d0, args=<optimized out>, nargs=<optimized out>, kwargs=<optimized out>) at Objects/call.c:586\r\n#9  0x00007f7f5e3ed48b in do_call_core (kwdict=0x7f7f523e4960, callargs=0x7f7f523f1090, func=0x7f7f5dcb27d0) at Python/ceval.c:4641\r\n#10 _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3191\r\n#11 0x00007f7f5e3e76c1 in _PyEval_EvalCodeWithName (_co=<optimized out>, globals=<optimized out>, locals=locals@entry=0x0, args=args@entry=0x7f7f5d2391e0, argcount=argcount@entry=2, kwnames=0x0, \r\n    kwargs=0x7f7f5d2391f0, kwcount=<optimized out>, kwstep=1, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0, name=0x7f7f5dc9f300, qualname=0x7f7f5dc9f300) at Python/ceval.c:3930\r\n#12 0x00007f7f5e378942 in _PyFunction_FastCallKeywords (func=<optimized out>, stack=0x7f7f5d2391e0, nargs=2, kwnames=<optimized out>) at Objects/call.c:433\r\n#13 0x00007f7f5e3ec222 in call_function (kwnames=0x0, oparg=<optimized out>, pp_stack=<synthetic pointer>) at Python/ceval.c:4616\r\n```", "comments": ["Having the same with Python 3.6.9 and TensorFlow 1.15.0 and scikit-learn 0.20.4 (also 0.22)", "```\r\n$ python -c \"import scipy; import tensorflow\"\r\nSegmentation fault (core dumped)\r\n```\r\n\r\nIt looks like it happened because of scipy release. Here is the problem: https://github.com/scipy/scipy/issues/11237", "@0x0badc0de, I tried on Google colab and Linux local system with `Tf==2.1.0-rc1 scikit-learn==0.21.3/0.21.2`. Its working without error message. Could you try once and let us know if issue still persists or not. Thanks!", "They have made a new release with a fix. I guess, it should work now.", "@gadagashwini I can confirm what @Jihadik says - `scipy==1.4.1` has the issue handled on their side. So commands from initial report don't reproduce segfault. Downgrading scipy with `pip install scipy==1.4.0` brings it back.\r\n\r\nI have also tried:\r\n```\r\n# pip install tf-nightly==2.1.0.dev20191219 scipy==1.4.0\r\n# python -c \"import scipy; import tensorflow\"\r\n```\r\nto check if #35271 has it fixed on TF side but still get the crash. I believe that's because TF fix was merged in after the nightly build was made. So if following commands work tomorrow:\r\n```\r\n# pip install tf-nightly==2.1.0.dev20191220 scipy==1.4.0\r\n# python -c \"import scipy; import tensorflow\"\r\n```\r\nWe can be sure the issue is handled on both sides.", "As said above, the issue comes from `scipy==1.4.0`.\r\n\r\nWe have made changes to that now `scipy` gets pinned to `1.4.1`. We will revisit the pining before new releases\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2d48f82a93a3806add726aa04768cca31eba19c1/tensorflow/tools/pip_package/setup.py#L78-L79\r\n\r\nSince this is fixed, I am going to close this, but please reopen and assign to me if there are other similar issues.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35185\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35185\">No</a>\n", "Note that #35271 is on `r2.1` branch but neither it nor the commit it is cherry-picking are fixing this issue. The only fix is `scipy==1.4.1`, unfortunately"]}, {"number": 35184, "title": "TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.", "body": "from tensorflow.keras.applications.inception_v3 import InceptionV3\r\nmodel = InceptionV3(weights='imagenet')\r\nmodel_new = Model(model.input, model.layers[-2].output)\r\n\r\n**after running the above line i got the below error:**\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-50-8b5bbb7c43a4> in <module>\r\n      1 # Create a new model, by removing the last layer (output layer) from the inception v3\r\n----> 2 model_new = Model(model.input, model.layers[-2].output)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py in wrapper(*args, **kwargs)\r\n     89                 warnings.warn('Update your `' + object_name + '` call to the ' +\r\n     90                               'Keras 2 API: ' + signature, stacklevel=2)\r\n---> 91             return func(*args, **kwargs)\r\n     92         wrapper._original_function = func\r\n     93         return wrapper\r\n\r\n~\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py in __init__(self, *args, **kwargs)\r\n     91                 'inputs' in kwargs and 'outputs' in kwargs):\r\n     92             # Graph network\r\n---> 93             self._init_graph_network(*args, **kwargs)\r\n     94         else:\r\n     95             # Subclassed network\r\n\r\n~\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py in _init_graph_network(self, inputs, outputs, name)\r\n    145         # User-provided argument validation.\r\n    146         # Check for redundancy in inputs.\r\n--> 147         if len(set(self.inputs)) != len(self.inputs):\r\n    148             raise ValueError('The list of inputs passed to the model '\r\n    149                              'is redundant. '\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py in __hash__(self)\r\n    711     if (Tensor._USE_EQUALITY and executing_eagerly_outside_functions() and\r\n    712         (g is None or g._building_function)):  # pylint: disable=protected-access\r\n--> 713       raise TypeError(\"Tensor is unhashable if Tensor equality is enabled. \"\r\n    714                       \"Instead, use tensor.experimental_ref() as the key.\")\r\n    715     else:\r\n\r\nTypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\r\n\r\n", "comments": ["@joshnarani9 \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nRequest you to provide minimal standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster.If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "I was able to run your script successfully in TF 1.15 and TF 2.1.0-rc1. Can you please confirm your TF version? Additionally if using TF Probability, may state its version too?\r\n```python\r\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\r\nfrom tensorflow.keras import Model\r\nmodel = InceptionV3(weights='imagenet')\r\nmodel_new = Model(model.input, model.layers[-2].output)\r\n```", "Keras=2.0.5\nTensorflow=1.14.0\n\nOn Fri, 27 Dec 2019, 04:13 Yasir Modak, <notifications@github.com> wrote:\n\n> I was able to run your script successfully in TF 1.15 and TF 2.1.0-rc1.\n> Can you please confirm your TF version? Additionally if using TF\n> Probability, may state its version too?\n>\n> from tensorflow.keras.applications.inception_v3 import InceptionV3from tensorflow.keras import Model\n> model = InceptionV3(weights='imagenet')\n> model_new = Model(model.input, model.layers[-2].output)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35184?email_source=notifications&email_token=ALU454AKCICHVRZSS46V32DQ2UXQ3A5CNFSM4J322WKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHWHLTA#issuecomment-569144780>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ALU454FY3AGT5BCV45QVNRLQ2UXQ3ANCNFSM4J322WKA>\n> .\n>\n", "Please switch to latest keras version (Keras 2.2.5) and try again. Try creating a virtual environment with updated Keras and TF versions."]}, {"number": 35183, "title": "tensorflow GPU-util on V100", "body": "I trained a BERT model use 4 V100 GPU on one machine, and I noticed that:\r\nA: batch_size = 60, Volatile GPU-util almost 80%~90%(nvidia-smi), global_step/sec=3.57(tf estimator log) \r\nB: batch_size = 80, Volatile GPU-util almost 80%~90%(nvidia-smi), global_step/sec=3.21(tf estimator log)\r\nC: batch_size =10, Volatile GPU-util almost 60%~70%(nvidia-smi), global_step/sec=5.2(tf estimator log)\r\n\r\nSo, i was a little confused about these result. I suppose that throughput of A is 60*3.57=214, B is 80*3.21=256, C is 10*5.2=52. But Volatile GPU-util almost \"same\", which i mean 80%-90%, 80%-90%, 60%-70%. \r\n\r\nTo be more specific, for experiment A, i suppose that 80%~90% GPU core was used, which is same  Volatile GPU-util when increase batch_size to 80(experiment B), so why Volatile GPU-util of A and B is almost same? Even reduce batch_size to 10, Volatile GPU-util even 60%~70%.\r\n\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nRequest you to provide minimal standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster.If you are unclear what to include see the issue template displayed in the [Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "**System information**\r\n- Have I written custom code : with horovod, no others custom code.\r\n- Mobile device : No\r\n- TensorFlow installed from (source or binary): official docker\r\n- TensorFlow version (use command below): 1.13.1 GPU\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): 8.2.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: V100 with 16GB * 4\r\n", "@colourful-tree \r\n\r\nWill it be possible to share simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@colourful-tree \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 35182, "title": "[mlir]", "body": "1.the type of op.getIndex() is unsigned, the value is always greater than or equal to 0\r\n2.the function hasConstantoffsetSizesAndStrides is defined but not used", "comments": ["Please add more to the title of the PR than just a tag.", "Please make a PR on https://github.com/tensorflow/mlir instead for these (third_party/mlir is a read-only copy of that repo here that will be removed)."]}, {"number": 35181, "title": "DropoutWrapper and Exploding gradient behaviour for Recurrent Neural Network", "body": "Dear all,\r\n I have a point about DropoutWrapper and its use with Recurrent Neural Networks.\r\n\r\nDue to the possibility that the dropout can be applied to the state or the output (state_keep_prob and output_keep_prob), I found that, during the recurrent process, the state propagated through the time can take values not bounded in the interval [-1, 1]. This is probably due to the way in which the dropout is implemented (at training time with a scaling instead of testing time with expectation). Since the dropout is applied after the activation (i.e. tanh), the feature values will range between -inf and +inf. This point is a bit strange for me since the current implementation can induce exploding gradient issues in the GRU/LSTM process while such cells were introduced to deal with vanishing as well as exploding gradients.\r\n\r\nPlease, could you supply me some feedback about my issue since, practically, it can impact people that commonly employ such Wrapper that induces behaviours that are divergent w.r.t. the theoretical behaviour of RNN (GRU/LSTM).\r\n\r\nAll the best\r\n", "comments": ["@tanodino, Could you provide the sample code snippet to analyze the reported issue. Thanks!", "Hi, I wrote some code example on a notebook that you can run on colab.\r\nHere the link to the notebook:\r\nhttps://colab.research.google.com/drive/1rNkHlySCXXasRqf3sCbYoSGSK5BdWT2h\r\n\r\nIn this code, I generated some random data and run a GRUCell wrapped by a DropoutWrapper layer. During the training I collected the value of the final state of the RNN and print them. As you can observe, such values are not bounded in the interval [-1, 1] and this can \"theoretically\" impact the gradient propagation since it can induces exploding gradient if the number of timestamps will be high. On the other hand, if you comment the line:\r\n\r\ncell = DropoutWrapper(cell, output_keep_prob=1.0-dropOut, state_keep_prob=1.0-dropOut)\r\n\r\nyou will see that the value of the \"feat\" tensor will be bounded in the interval [-1, 1] respecting the theoretical consideration related to GRU and LSTM cells.\r\n\r\nI hope to haver well highlighted the point I want to raise up.\r\n\r\nBest\r\n\r\n\r\n\r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35181\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35181\">No</a>\n"]}, {"number": 35180, "title": "Windows build failed - Internal compiler error Visual Studio 2017 - FAILED: Build did NOT complete successfully", "body": "**System information**\r\n- OS Platform and Distribution: Windows 7 Professional SP1 64 bit\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: tensorflow-master (downloaded 18/12/2019)\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: YES, conda\r\n- Bazel version (if compiling from source): 1.1.0\r\n- GCC/Compiler version (if compiling from source): MSVC 14.16.27023 (Visual Studio 2017)\r\n- CUDA/cuDNN version: build for CPU\r\n- GPU model and memory: (Quadro K200M)\r\n\r\n**Describe the problem**\r\nFollowed this guide: https://www.tensorflow.org/install/source_windows and just build failed.\r\nI tested different combinations (https://www.tensorflow.org/install/source_windows#cpu) and I was just able to compile just the **r1.14 successfully**, by the way.\r\n**r2.0** failed with the same message as master.\r\nBut back to master release.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n- Donwloaded the `tensorflow-master.zip` from github\r\n- Installed Bazel version 1.1.0\r\n- cmd to folder `\\tensorflow-master`\r\n- `activate python37 environment` (conda)\r\n- Set env var for bazel: `set BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC`\r\n- Check Bazel version\r\n```\r\n(python37) C:\\Users\\username\\bin\\tensorflow-master>bazel --version\r\nbazel 1.1.0\r\n```\r\n- Configure\r\n```\r\n(python37) C:\\Users\\ username\\bin\\tensorflow-master>python ./configure.py\r\nYou have bazel 1.1.0 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\ username\\Anaconda3\\envs\\python37\\python.exe]:\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\ username\\Anaconda3\\envs\\python37\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\ username\\Anaconda3\\envs\\python37\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y\r\nEigen strong inline overridden.\r\n```\r\n- Launch the compilation: `bazel build //tensorflow:tensorflow_cc.lib`\r\n- After time elapsed the process ended with: `FAILED: Build did NOT complete successfully`\r\n\r\n**Any other info / logs**\r\nJust attaching the last rows of the terminal messages\r\n\r\nMessage has Italian chunks of which I'm trying to provide a translation here:\r\n```\r\nnote: vedere il riferimento all'istanza '<Sconosciuto>' della funzione <Sconosciuto> di cui \u00e8 in corso la compilazione\r\nnote: check the reference to instance '<Unknown>' of the function <Unknown> which is compiling\r\n```\r\n\r\n```\r\nErrore interno del compilatore in C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX64\\x64\\cl.exe. Verr\u00e0 richiesto di inviare una segnalazione errori a Microsoft in un momento successivo.\r\nInternal error of the compiler in C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX64\\x64\\cl.exe. It will be required to send an error report to Microsoft next.\r\n```\r\n\r\n```\r\nERRORE INTERNO DEL COMPILATORE in 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX64\\x64\\cl.exe' Per altre informazioni, scegliere Supporto tecnico dal menu ? di Visual C++ o aprire il file della Guida relativo al supporto tecnico\r\nINTERNAL ERROR OF THE COMPILER in 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX64\\x64\\cl.exe' For more information, choose Technical support from ? menu of Visual C++ or open the file from the guide related with technical support\r\n```\r\n\r\n[tensorflow-master-build-fail-log.txt](https://github.com/tensorflow/tensorflow/files/3971998/tensorflow-master-build-fail-log.txt)\r\n\r\n", "comments": ["Looks like a compiler bug to me?\r\n@angerson could you loop in sig-build to see if they can help?", "@giaxxi,\r\n\r\nCan you try building the latest stable version of tensorflow i.e `2.6.0` and lets us know if the issue still persists? You can follow this [guide](https://www.tensorflow.org/install/source_windows) to build from source. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35180\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35180\">No</a>\n"]}, {"number": 35179, "title": "Upgrading Boringssl Version in TensorFlow", "body": "I am trying to build Tensorflow using latest Boringssl. However, facing compilation issues.\r\nIs there any plan to upgrade the Boringssl commit currently being used in TensorFlow to a recent commit?", "comments": ["Created PR #35196 with the upgrade of boringssl", "Issue being tracked in the PR https://github.com/tensorflow/tensorflow/pull/35196.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35179\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35179\">No</a>\n"]}, {"number": 35178, "title": "missing symbols from c++ API", "body": "**System information**\r\n- OS Platform: Windows10\r\n- TensorFlow installed from:source\r\n- TensorFlow version:1.13.0\r\n- Python version: NO\r\n- Bazel version (if compiling from source):0.19\r\n- GCC/Compiler version: msvc14\r\n\r\n\r\n\r\n**Describe the problem**\r\nI want to build tensorflow from the source using the below command \r\n`bazel build -c opt  --config=mkl --config=monolithic --linkopt=\"/FORCE:MULTIPLE\" --define=no_tensorflow_py_deps=true //tensorflow:libtensorflow_cc.so //tensorflow:install_headers   //tensorflow/tools/lib_package:libtensorflow --verbose_failures`\r\n\r\nHowever after the dll file generated, some symbol is not inside of the dll, I will paste some of this error in here(As far as I know this problem is accured because number of symbol is limited in windows, So all functions in tensorflow source can't be exported( I _check the exported function using dumpbin.exe /EXPORT and it's weird because only 3000 symbols is exported )_\r\n`tfwrapper.obj : error LNK2001: unresolved external symbol \"class tensorflow::Output __cdecl tensorflow::ops::Const(class tensorflow::Scope const &,struct tensorflow::Input::Initializer const &)\" (?Const@ops@tensorflow@@YA?AVOutput@2@AEBVScope@2@AEBUInitializer@Input@2@@Z)`\r\n\r\n\r\n`tfwrapper.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::ops::DecodePng::DecodePng(class tensorflow::Scope const &,class tensorflow::Input,struct tensorflow::ops::DecodePng::Attrs const &)\" (??0DecodePng@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@AEBUAttrs@012@@Z)`\r\n\r\n\r\n`tfwrapper.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::ops::Placeholder::Placeholder(class tensorflow::Scope const &,enum tensorflow::DataType)\" (??0Placeholder@ops@tensorflow@@QEAA@AEBVScope@2@W4DataType@2@@Z)`\r\n\r\nand for solving this issue I work on two idea\r\n **first one** is using `def_file_filter.py.tpl`, as may you know in this file I have to add missed symbol to it like below(I just paste some of them in here) and reconfigure & recompile the source(I guess). But this solution can't export any of this missing symbol for me\r\n\r\n     # Header for the def file.\r\n     def_fp.write(\"LIBRARY \" + args.target + \"\\n\")\r\n    def_fp.write(\"EXPORTS\\n\")\r\n    def_fp.write(\"\\t ??1OpDef@tensorflow@@UEAA@XZ\\n\")\r\n\tdef_fp.write(\"\\t ??0DecodePng@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@AEBUAttrs@012@@Z\\n\")\r\n\tdef_fp.write(\"\\t ??6tensorflow@@YAAEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@AEAV12@AEBVStatus@0@@Z\\n\")`\r\n\r\n\r\nSo, I can't solve my problem using this method. So I use another Idea that I got this Idea from `ashley tharp` (you can see her great work in this[ link](https://github.com/robosina/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows)) and this workaround solve 8 of this problems. Main idea of her is to add `TF_EXPORT` macro to the function names to force symbol to include in the dll file. but my problem is raised in the DecodePng file, there is a function in tensorflow source ->`tensorflow/core/kernels/decode_image_op.cc` the function signature is \r\n     \r\n     void DecodePng(OpKernelContext* context, StringPiece input)\r\n I can't find any other. But in examples of the tensorflow we will see this way to read png files\r\n\r\n    image_reader = DecodePng(root.WithOpName(\"png_reader\"), file_reader,\r\n                             DecodePng::Channels(wanted_channels));\r\nthis two function is not same and I couldn't find this function in the tensorflow source code so I can't add `TF_EXPORT` macro to it, but how it is possible to use a function that is not present in source code?? So I look in the bazel genereated files and this function is in there.(it is in `bazel-source/bazel-out/x64_windows-opt/genfiles/tensorflow/cc/ops/image_ops.h`)\r\n\r\n     DecodePng(const ::tensorflow::Scope& scope, ::tensorflow::Input contents);\r\n     DecodePng(const ::tensorflow::Scope& scope, ::tensorflow::Input contents, const\r\n          DecodePng::Attrs& attrs);\r\n\r\n I don't know exactly how bazel created it and how to add `TF_EXPORT` macro to it(because this files is machine generated files and you can't add TF_EXPORT to it, because it will be overwritten in compile time)", "comments": ["@robosina \r\nIs the issue still present?\r\n\r\nCould you refer to the answer of https://github.com/tensorflow/tensorflow/issues/39407 ?\r\n\r\n", "@NeoZhangJianyu , unfortunately no, I tried so many ways but I can't resolve these errors.", "@robosina \r\nIs your case changed now? like TF & bazel version?\r\n\r\nWhat's the current version of the building target? And could you share your build cmd and error log?", "@NeoZhangJianyu Since I can't use tensorflow and it didn't give me proper performance on cpu, I have changed my framework to Pytorch as it has simpler API in c++!", "@robosina \r\nOK! I understand.\r\n\r\nDo you think this issue could be continued or closed?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35178\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35178\">No</a>\n", "@robosina \r\nThank you very much!"]}, {"number": 35177, "title": "tensorflow c++ Not found: Op type not registered 'ImageProjectiveTransformV2'", "body": "I run my c++ code, it load file .pb have op tf.contrib.image.transform() and out error:\r\n```\r\nNot found: Op type not registered 'ImageProjectiveTransformV2' in binary running on mendel. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n```\r\n\r\nI use tensorflow 1.14 build from source on ubuntu16.04.\r\nHow to build to use op ImageProjectiveTransformV2 ?", "comments": ["@anhtu812 ,\r\nCan you please provide standalone code to reproduce the issue being reported here?Thanks!", "Hi @oanush, my code:\r\n```\r\n#include \"tensorflow/core/public/session.h\"\r\n\r\nnamespace tf = tensorflow;\r\n\r\nint main(int argc, char** argv) {\r\n  \r\n    std::string model_path = \"model.pb\";\r\n\ttf::Session* session;\r\n\ttf::Status status;\r\n\ttf::GraphDef graph_def;\r\n\tstatus = tf::ReadBinaryProto(tf::Env::Default(), model_path, &graph_def);\r\n\tif (!status.ok()) {\r\n\t\tstd::cout << status.ToString() << \"\\n\";\r\n\t\texit(-1);\r\n\t}\r\n\r\n\tauto options = tf::SessionOptions();\r\n\tstatus = tf::NewSession(options, &session);\r\n\tif (!status.ok()) {\r\n\t\tstd::cout << status.ToString() << \"\\n\";\r\n\t\texit(-1);\r\n\t}\r\n\tstatus = session->Create(graph_def);\r\n\tif (!status.ok()) {\r\n\t\tstd::cout << status.ToString() << \"\\n\";\r\n\t\texit(-1);\r\n\t}\r\n\r\n\t// to build with link library \"libtensorflow_cc.so.1\" to fix error \"2019-12-18 14:11:44.824639: E tensorflow/core/common_runtime/session.cc:81] Not found: No session factory registered for the given session options: {target: \"\" config: } Registered factories are {}.\"\r\n\ttf::Tensor tensor;\r\n\tauto tensor_1 = tensor.flat<float>();\r\n}\r\n```\r\nfile model: https://1drv.ms/u/s!AhFk3ICqlZI2irgK0pAnbDbE8DWSoQ?e=GhZojc", "Hi @ymodak \r\nhow to fix?", "Hi @oanush, @ymodak not answer.", "Is this still an issue with latest TF version? ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 35176, "title": "How to prevent operator from fusing when converting to TFLite model? ", "body": "GPU delegate does not support operator fusion, so I need to prevent the conversion from fusing the operators.\r\n\r\nTFLiteConvertor will automatically fuse operators, which gives problems.", "comments": ["Hi,\r\n\r\nCurrently the only way is to change the code to skip doing this.\r\nFor example,\r\nif you want to stop fusing Activation function\r\n- In TOCO\r\n  It is defined here\r\n https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/graph_transformations/fuse_activation_functions.cc\r\n to disable specific op from fusing  you can change\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/tooling_util.cc#L469\r\n\r\n- In New Converter\r\nYou need to find the pattern that does this transform.\r\nFor binary ops the pattern is here\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/optimize_patterns.td#L297\r\n\r\nYou can remove the type you don't want or remove the whole pattern completely.\r\n\r\nHope that helps.\r\n\r\nThanks", "@karimnosseir Is there any way to not touch MLIR code? I can't really understand MLIR code.", "Hi @yxchng The current default behavior uses TOCO, so you don't need to touch MLIR code (yet).\r\n\r\nIt will be good to hear from you and others as well, how can we make our new MLIR converter easy for the community to understand and contribute.\r\nDid you check the guide https://github.com/tensorflow/mlir#getting-started-with-mlir ?\r\n\r\nThanks", "@karimnosseir I have found a way to change the behavior of TOCO as I desired. With regards to your advice on MLIR, I have checked the link you referenced but it is still too complicated for me. I have no idea of compilers and LLVM. The documentation is not written in a way that is accessible to people with 0 knowledge of compilers.", "Closing this issue since its resolved. Feel free to reopen if have further questions. Thanks!", "I would like to disable fusion from TensorFlow's code, so is there any easy way to do so?\r\n\r\nI've already checked this file \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/optimize_patterns.td\r\nbut since I'm not familiar with LLVM, i's kind of hard for me.\r\n\r\nCould you maybe give an example of how to prevent fusion for Conv2D and ReLU let's say? Which lines of code should I remove in this case?\r\n\r\nThanks a lot!", "Hi @dimitraka, \r\nThanks for the feedback, we will look in making this easier for users to customize.\r\nFor now this is what triggers the fusing of Conv2D with relu you can comment them and rebuild\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/optimize_patterns.td#L84:L89", "Thank you for the immediate answer!\r\n\r\nCould you tell me how to rebuild as well? Because I have some trouble finding out how to rebuild TensorFlow once I have installed it from source.\r\n\r\nI tried the following:\r\n`bazel build -j 16 //tensorflow/compiler/mlir/lite:tensorflow_lite_optimize` \r\nfor building only a specific target.\r\n\r\nI also tried building TensorFlow, like when installing it for the first time.\r\n`\r\nbazel build -j 16 //tensorflow/tools/pip_package:build_pip_package\r\n`\r\n`\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n`\r\n`\r\npip install /tmp/tensorflow_pkg/tensorflow-2.3.0-cp38-cp38m-linux_x86_64.whl\r\n`\r\n\r\nBut didn't see any difference in my graph in any way. Do you have any suggestions?", "Hi @dimitraka, \r\nYou can build the converter command line and run it.\r\n\r\nbazel build -c opt tensorflow/lite/python:tflite_convert\r\n\r\nthen run it\r\nbazel-bin/tensorflow/lite/python/tflite_convert --saved_model_dir=<path> --output_file=/tmp/my_model.tflite\r\n\r\n", "hi @karimnosseir \r\nI am only aware of building tensorflow lite using :-\r\ntensorflow.lite.TFLiteConverter.from_keras_model(my_model)\r\nand the apply converter optimizations.\r\n\r\nWill this method also do the same job after commenting the mentioned lines in \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/optimize_patterns.td#L84:L89\r\n\r\nHi @dimitraka is your issue resolved?", "Also I am unable to find the batch normalisation layer in the TFLite model that is supposed to be in between convolution and activation layer in a Resnet50 model, what happened to that? It fused between convolution and Relu?"]}, {"number": 35175, "title": "Add folding FusedBatchNormV3", "body": "optimize_for_inference in TensorFlow does not support folding FusedFatchNormV3 node. This work is to add folding FusedFatchNormV3. I have referenced the source code in TFJS project.\r\n\r\n* https://github.com/tensorflow/tfjs/blob/master/tfjs-converter/python/tensorflowjs/converters/fold_batch_norms.py\r\n\r\nI think it is necessary to synchronize the work of model optimization between TFJS and Tensorflow projects.", "comments": ["(I don't work on TF anymore, so removing myself as a reviewer.)"]}, {"number": 35174, "title": "Add folding FusedBatchNormV3", "body": "`optimize_for_inference` in TensorFlow does not support folding FusedFatchNormV3 node. This work is to add folding FusedFatchNormV3. I have referenced the source code in TFJS project. \r\n\r\n* https://github.com/tensorflow/tfjs/blob/master/tfjs-converter/python/tensorflowjs/converters/fold_batch_norms.py\r\n\r\nI think it is necessary to synchronize the work of model optimization between TFJS and Tensorflow projects.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35174) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 35173, "title": "I do not get the from the file or definition of \"gen_nn_ops\" in the place \"tensorflow.python.ops\"", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@chensongkui \r\n\r\nCan you please elaborate the issue you are facing. Also, please provide us the document link, with which you are facing the issue.Thanks!\r\n", "@chensongkui \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 35172, "title": "TypeError when using boolean_mask to evaluate hessian matrix", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (see code snippet below)\r\n- OS Platform and Distribution: both on win10 and Linux Ubuntu 18.04\r\n- TensorFlow installed from: binary (via `pip install tensorflow`)\r\n- TensorFlow version (use command below): `2.0.0`\r\n- Python version: `3.7.3`\r\n- CUDA/cuDNN version: not applicable\r\n- GPU model and memory: not applicable\r\n\r\n**minimum code to reproduce the issue**\r\n\r\n```Python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nhfe = lambda x,y: np.max(np.abs(x-y)/(np.abs(x)+np.abs(y)+1e-3))\r\nN0 = 3\r\nnp0 = np.random.randn(N0)\r\nnp2 = np.sum(np0**3)\r\nhessian_np0 = np.diag(6*np0) #the correct hessian via numpy\r\ntf0 = tf.constant(np0)\r\nwith tf.GradientTape() as tape0:\r\n    tape0.watch(tf0)\r\n    with tf.GradientTape() as tape1:\r\n        tape1.watch(tf0)\r\n        # tf1 = tf0 # @label-0 pass \r\n        # tf1 = tf.gather(tf0*1, tf.range(N0)) # @label-1 pass\r\n        tf1 = tf.boolean_mask(tf0, tf.ones(N0, dtype=tf.bool)) # @label-2 fail TypeError\r\n        tf2 = tf.math.reduce_sum(tf1**3)\r\n    grad_tf0 = tape1.gradient(tf2, tf0)\r\nhessian_tf0 = tape0.jacobian(grad_tf0, tf0) #the hessian via tensorflow\r\nprint('relative error:: np vs tf', hfe(hessian_np0, hessian_tf0.numpy()))\r\n```\r\n\r\n**current behavior**: both `@label-0 @label-1 @label-2` (see comments in the code snippet) could give the correct `tf1` (just identical with `tf0`), but only `@label-0 @label-1` could give the correct hessian matrix as `hessian_np0` (second order derivative), `@label-2` will raise `TypeError` (see attached [traceback.log](https://github.com/tensorflow/tensorflow/files/3971266/traceback.log))\r\n\r\n**expected behavior**: `@label-0 @label-1 @label-2` should give the same hessian matrix", "comments": ["@husisy ,\r\nWhen tried running the code in `TF-nightly `issue seemed to be fixed, kindly refer the [gist](https://colab.sandbox.google.com/gist/oanush/4bf90fdcc8ae193ff478b2fac353474f/35172.ipynb) of colab.Thanks!", "> @husisy ,\r\n> When tried running the code in `TF-nightly `issue seemed to be fixed, kindly refer the [gist](https://colab.sandbox.google.com/gist/oanush/4bf90fdcc8ae193ff478b2fac353474f/35172.ipynb) of colab.Thanks!\r\n\r\nmany thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35172\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35172\">No</a>\n"]}, {"number": 35171, "title": "rnn optimize fail", "body": "Epoch 1/10\r\n2019-12-17 10:30:10.047658: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_gru_5820_6196_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_6314' and '__inference___backward_standard_gru_5820_6196' both implement 'gru_95095eff-9898-4b33-9f85-d96fe50ea8a6' but their signatures do not match.\r\n\r\nwhat's wrong?", "comments": ["@1096125073,\r\nKindly provide the code being used to replicate the issue reported, also provide the TF version and complete error trace.\r\nPlease refer the [similar](https://github.com/tensorflow/tensorflow/issues/30263#issuecomment-508526017) issue and let us know if it helped.\r\nThanks!", "@1096125073 ,\r\nAny update on the issue?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}]