[{"number": 12368, "title": "CrashLoopBackOff", "body": "I am trying to follow the tensorflow deployment on kubernetes following the tutorial here:\r\nhttps://www.tensorflow.org/serving/serving_inception#part_2_deploy_in_kubernetes\r\n\r\nInstead of running on gclound, I am trying to run on local machine. But I ran into the following problem.\r\n\r\n\r\n\r\npangolins:serving$ kubectl get pods\r\nNAME                                    READY     STATUS             RESTARTS   AGE\r\ninception-deployment-2217120516-jmkbm   0/1       CrashLoopBackOff   9          31m\r\ninception-deployment-2217120516-rr04x   0/1       CrashLoopBackOff   9          31m\r\ninception-deployment-2217120516-xvc58   0/1       CrashLoopBackOff   9          31m\r\nmonolith                                1/1       Running            0          1d\r\nnginx-1803751077-5cst4                  1/1       Running            0          2d\r\n\r\npangolins:serving$ bazel-bin/tensorflow_serving/example/inception_client --server=10.0.0.45:32683 --image=./tensorflow/tensorflow/contrib/pi_examples/label_image/data/grace_hopper.jpg\r\nTraceback (most recent call last):\r\n  File \"/home/pangolins/software/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving/example/inception_client.py\", line 56, in <module>\r\n    tf.app.run()\r\n  File \"/home/pangolins/software/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/pangolins/software/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving/example/inception_client.py\", line 51, in main\r\n    result = stub.Predict(request, 10.0)  # 10 secs timeout\r\n  File \"/home/pangolins/anaconda2/lib/python2.7/site-packages/grpc/beta/_client_adaptations.py\", line 324, in __call__\r\n    self._request_serializer, self._response_deserializer)\r\n  File \"/home/pangolins/anaconda2/lib/python2.7/site-packages/grpc/beta/_client_adaptations.py\", line 210, in _blocking_unary_unary\r\n    raise _abortion_error(rpc_error_call)\r\ngrpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.UNAVAILABLE, details=\"Connect Failed\")\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12367, "title": "Estimator classification export feature request: input single string Tensor input limitation.", "body": "Within class `ClassificationOutput` contains the `as_signature_def` method. I was wondering why there is a limitation that this method only allows recievor_tensors of length 1.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/53aef8a3a5920e53f7da3ea2140374546d1bf708/tensorflow/python/estimator/export/export_output.py#L109-L118\r\n\r\nFor example the following code should seemingly just \"work\" for the model created from the [wide and deep model tutorial.\r\n](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/learn/wide_n_deep_tutorial.py)\r\n\r\n```\r\nfeature_inputs = {\r\n        'age': tf.placeholder(dtype=tf.float32, shape=[1,1], name='age'),\r\n        'capital_gain': tf.placeholder(dtype=tf.float32, shape=[1,1], name='age'),\r\n        'capital_loss': tf.placeholder(dtype=tf.float32, shape=[1,1], name='capital_loss'),\r\n        'education': tf.placeholder(dtype=tf.string, shape=[1,1], name='education'),\r\n        'education_num': tf.placeholder(dtype=tf.float32, shape=[1,1], name='education_num'),\r\n        'gender': tf.placeholder(dtype=tf.string, shape=[1,1], name='gender'),\r\n        'hours_per_week': tf.placeholder(dtype=tf.float32, shape=[1,1], name='hours_per_week'),\r\n        'native_country': tf.placeholder(dtype=tf.string, shape=[1,1], name='native_country'),\r\n        'occupation': tf.placeholder(dtype=tf.string, shape=[1,1], name='occupation'),\r\n        'relationship': tf.placeholder(dtype=tf.string, shape=[1,1], name='relationship'),\r\n        'workclass': tf.placeholder(dtype=tf.string, shape=[1,1], name='workclass') \r\n    }\r\n\r\nserving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_inputs)\r\n\r\n# model is the loaded wide and deep model produced\r\nmodel.export_savedmodel(\"./my_dir\",  serving_input_receiver_fn)\r\n```\r\n\r\nHowever there is an error here:\r\n```\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/export/export.pyc in build_all_signature_defs(receiver_tensors, export_outputs)\r\n    158       '{}'.format(output_key or 'None'):\r\n    159       export_output.as_signature_def(receiver_tensors)\r\n--> 160       for output_key, export_output in export_outputs.items()}\r\n    161 \r\n    162   return signature_def_map\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/export/export.pyc in <dictcomp>((output_key, export_output))\r\n    158       '{}'.format(output_key or 'None'):\r\n    159       export_output.as_signature_def(receiver_tensors)\r\n--> 160       for output_key, export_output in export_outputs.items()}\r\n    161 \r\n    162   return signature_def_map\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/export/export_output.pyc in as_signature_def(self, receiver_tensors)\r\n    110     if len(receiver_tensors) != 1:\r\n    111       raise ValueError('Classification input must be a single string Tensor; '\r\n--> 112                        'got {}'.format(receiver_tensors))\r\n    113     (_, examples), = receiver_tensors.items()\r\n    114     if dtypes.as_dtype(examples.dtype) != dtypes.string:\r\n\r\nValueError: Classification input must be a single string Tensor; got {'hours_per_week': <tf.Tensor 'hours_per_week_14_1:0' shape=(?, 1) dtype=float32>, 'native_country': <tf.Tensor 'native_country_14_1:0' shape=(?, 1) dtype=string>, 'relationship': <tf.Tensor 'relationship_14_1:0' shape=(?, 1) dtype=string>, 'gender': <tf.Tensor 'gender_14_1:0' shape=(?, 1) dtype=string>, 'age': <tf.Tensor 'age_14_1:0' shape=(?, 1) dtype=float32>, 'capital_gain': <tf.Tensor 'capital_gain_14_1:0' shape=(?, 1) dtype=float32>, 'workclass': <tf.Tensor 'workclass_14_1:0' shape=(?, 1) dtype=string>, 'capital_loss': <tf.Tensor 'capital_loss_14_1:0' shape=(?, 1) dtype=float32>, 'education': <tf.Tensor 'education_14_1:0' shape=(?, 1) dtype=string>, 'education_num': <tf.Tensor 'education_num_14_1:0' shape=(?, 1) dtype=float32>, 'occupation': <tf.Tensor 'occupation_14_1:0' shape=(?, 1) dtype=string>}\r\n```\r\n\r\n", "comments": ["@martinwicke can you comment or redirect to someone who can? Thanks.", "@nfiedel for restrictions on classification.\r\n\r\nThis is a class meant to fit in with tensorflow/serving, for which the classification signature always goes for a single string tensor (which typically contains a serialized tf.Example record) to a classification result.\r\n\r\nWe also export a generic signature, which takes any input tensors and returns any input tensors.", "Correct me if I'm wrong, the generic signature you are referring to seems to be the `PredictOutput` class\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2133b83376a3439924a0bb9bac7b4a8b716a6fc0/tensorflow/python/estimator/export/export_output.py#L153-L159\r\n\r\nHowever, for the canned estimator model it is using a head which doesn't include the `PredictOutput` class as an export output.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2133b83376a3439924a0bb9bac7b4a8b716a6fc0/tensorflow/python/estimator/canned/head.py#L661-L673\r\n\r\nIs there a clean way for the inputs to a `SavedModel` estimator model to be multiple tensors as opposed to 1 serialized string tensor?", "Adding to Martin's reply, SavedModel's Signatures (SignatureDef) support arbitrarily many input and output tensors.\r\n\r\nMany Canned Estimators are built to match 1:1 with the TensorFlow Serving APIs, most notably for various types of Regression & Classification models. The Predict API supports full generality of arbitrarily many inputs and outputs.\r\n\r\nYou can see an enumeration of these Signature method_names and Tensor names here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/signature_constants.py\r\n\r\nSo I think the remaining question is how to get the input data into your particular model. The options at a high level are, I think:\r\n\r\n1. Convert your input data to tf.Example\r\n2. Figure out how to construct & save a model that takes multiple input tensors.", "We have a similar model [DNNRegressor](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor) that seems to require a single string tensor as well. `saved_model_cli` confirms this i.e.\r\n```\r\n$ saved_model_cli show  --dir /tmp/saved_model_dir --tag_set serve --signature_def serving_default\r\nThe given SavedModel SignatureDef contains the following input(s):\r\ninputs['inputs'] tensor_info:\r\n    dtype: DT_STRING\r\n    shape: (-1)\r\n    name: input_example_tensor:0\r\nThe given SavedModel SignatureDef contains the following output(s):\r\noutputs['outputs'] tensor_info:\r\n    dtype: DT_FLOAT\r\n    shape: (-1, 1)\r\n    name: dnn/head/logits:0\r\nMethod name is: tensorflow/serving/regress\r\n```\r\n\r\n@nfiedel \r\n\r\n> Figure out how to construct & save a model that takes multiple input tensors.\r\n\r\n\r\nDo you have any suggestions on how to save a `Regression` like `tf.Estimator` model so that it would take multiple input tensors?", "Regression estimators also have the constraint. Since tf.estimator.export provides two ways of exporting a model, build_raw_serving_input_receiver_fn and build_parsing_serving_input_receiver_fn, I would say build_parsing_serving_input_receiver_fn is meant to work with TensorFlow Serving, and build_raw_serving_input_receiver_fn is meant to restore models without help of TensorFlow Serving. I think that's the message that \"parsing\" and \"raw\" try to convey. Otherwise how would you restore models without TensorFlow Serving for scoring? Personally I think it's a good feature to add.", "We have removed this restriction. Please check out the code at head, or in the upcoming 1.4 RC. Please reopen if there are issues with it.", "@sudododo -- I am trying to use export_savedmodel() to save a trained DNNLinearCombinedRegressor after training. I do not want to use Tensorflow serving as I intend to use it for a simple local desktop application. When I use build_parsing_serving_input_receiver_fn(), I am able to export the model using:  `servable_model_path = regressor.export_savedmodel(export_dir_base = servable_model_dir, serving_input_fn = export_input_fn, as_text= True).` However, when I use the build_raw_serving_input_receiver_fn() (as I should be, since I don't want to use Tensorflow Serving), I get the following error:\r\n`AttributeError: 'FixedLenFeature' object has no attribute 'get_shape'\r\n`\r\n\r\nThe only thing I did different was creation of export_input_fn. Instead of doing -- \r\n`export_input_fn = tf.contrib.learn.utils.build_parsing_serving_input_fn(feature_spec)`, I did this:\r\n`export_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_spec)`\r\n\r\nHere feature_spec is created as follows:\r\n`feature_spec = tf.feature_column.make_parse_example_spec(feature_cols)`\r\nwhere feature cols are real_valued and embedding columns. I am using tensorflow 1.3.\r\n\r\nThanks for your help!", "@rutadesai Sorry for the late reply. build_parsing_serving_input_fn and build_raw_serving_input_receiver_fn are expecting different inputs. build_parsing_serving_input_fn expect feature_spec which you could consider as some sort of \"data contract\" so that TensorFlow serving can translate the request. So what you did is valid for build_parsing_serving_input_fn. However, it you serve the mode locally (without TensorFlow serving), you need to define the input tensor of your graph. Therefore you have to define a placeholder. Actually if you check the input names of two functions, build_parsing_serving_input_fn is \"feature_spec\" vs \"feature\" for build_raw_serving_input_receiver_fn.", "@martinwicke You said this restriction was removed in 1.4 but I still see it.\r\n\r\nbuild_raw_serving_input_receiver_fn is designed to build multiple placeholders for each input (receiving tensors).\r\nClassificationOutput.as_signature_def still checks for a single receiving tensor.\r\n\r\nSo to me the restriction is still in place and you can't use ClassificationOutput with build_raw_serving_input_receiver_fn with a saved model.\r\n\r\nIt seems wrong that these two functions are incompatible.", "ClassificationOutput is a specific to tensorflow/serving. Do not use it. You want to use PredictOutput. \r\n\r\n@davidsoergel, @nfiedel, maybe we should make this much more clear, and maybe even deprecate ClassificationOutput and RegressionOutput?", "I use the same code and ` tf 1.4.0` version, get the `save_model.pbtxt`, the stucture of `signature_def` is\r\n```\r\n  signature_def {\r\n    key: \"predict\"\r\n    value {\r\n      inputs {\r\n        key: \"age\"\r\n        value {\r\n          name: \"age:0\"\r\n          dtype: DT_FLOAT\r\n          tensor_shape {\r\n            dim {\r\n              size: -1\r\n            }\r\n            dim {\r\n              size: 1\r\n            }\r\n          }\r\n        }\r\n      }\r\n      inputs {\r\n        key: \"capital_gain\"\r\n        value {\r\n          name: \"capital_gain:0\"\r\n          dtype: DT_FLOAT\r\n          tensor_shape {\r\n            dim {\r\n              size: -1\r\n            }\r\n            dim {\r\n              size: 1\r\n            }\r\n          }\r\n        }\r\n      }\r\n      inputs {\r\n        key: \"capital_loss\"\r\n        value {\r\n          name: \"capital_loss:0\"\r\n          dtype: DT_FLOAT\r\n          tensor_shape {\r\n            dim {\r\n              size: -1\r\n            }\r\n            dim {\r\n              size: 1\r\n            }\r\n          }\r\n        }\r\n      }\r\n      inputs {\r\n        key: \"education\"\r\n        value {\r\n          name: \"education:0\"\r\n          dtype: DT_STRING\r\n          tensor_shape {\r\n            dim {\r\n              size: -1\r\n            }\r\n            dim {\r\n              size: 1\r\n            }\r\n          }\r\n        }\r\n      }\r\n      inputs {\r\n        key: \"education_num\"\r\n        value {\r\n          name: \"education_num:0\"\r\n          dtype: DT_FLOAT\r\n          tensor_shape {\r\n            dim {\r\n              size: -1\r\n            }\r\n            dim {\r\n              size: 1\r\n            }\r\n          }\r\n        }\r\n      }\r\n      inputs {\r\n        key: \"hours_per_week\"\r\n        value {\r\n          name: \"hours_per_week:0\"\r\n          dtype: DT_FLOAT\r\n          tensor_shape {\r\n            dim {\r\n              size: -1\r\n            }\r\n            dim {\r\n              size: 1\r\n            }\r\n          }\r\n        }\r\n      }\r\n      inputs {\r\n        key: \"marital_status\"\r\n        value {\r\n          name: \"marital_status:0\"\r\n          dtype: DT_STRING\r\n          tensor_shape {\r\n            dim {\r\n              size: -1\r\n            }\r\n            dim {\r\n              size: 1\r\n            }\r\n          }\r\n        }\r\n      }\r\n      inputs {\r\n        key: \"occupation\"\r\n        value {\r\n          name: \"occupation:0\"\r\n          dtype: DT_STRING\r\n          tensor_shape {\r\n            dim {\r\n              size: -1\r\n            }\r\n            dim {\r\n              size: 1\r\n            }\r\n          }\r\n        }\r\n      }\r\n      inputs {\r\n        key: \"relationship\"\r\n        value {\r\n          name: \"relationship:0\"\r\n          dtype: DT_STRING\r\n          tensor_shape {\r\n            dim {\r\n              size: -1\r\n            }\r\n            dim {\r\n              size: 1\r\n            }\r\n          }\r\n        }\r\n      }\r\n      inputs {\r\n        key: \"workclass\"\r\n        value {\r\n          name: \"workclass:0\"\r\n          dtype: DT_STRING\r\n          tensor_shape {\r\n            dim {\r\n              size: -1\r\n            }\r\n            dim {\r\n              size: 1\r\n            }\r\n          }\r\n        }\r\n      }\r\n      outputs {\r\n        key: \"class_ids\"\r\n        value {\r\n          name: \"head/predictions/classes:0\"\r\n          dtype: DT_INT64\r\n          tensor_shape {\r\n            dim {\r\n              size: -1\r\n            }\r\n            dim {\r\n              size: 1\r\n            }\r\n          }\r\n        }\r\n      }\r\n      outputs {\r\n        key: \"classes\"\r\n        value {\r\n          name: \"head/predictions/str_classes:0\"\r\n          dtype: DT_STRING\r\n          tensor_shape {\r\n            dim {\r\n              size: -1\r\n            }\r\n            dim {\r\n              size: 1\r\n            }\r\n          }\r\n        }\r\n      }\r\n      outputs {\r\n        key: \"logistic\"\r\n        value {\r\n          name: \"head/predictions/logistic:0\"\r\n          dtype: DT_FLOAT\r\n          tensor_shape {\r\n            dim {\r\n              size: -1\r\n            }\r\n            dim {\r\n              size: 1\r\n            }\r\n          }\r\n        }\r\n      }\r\n      outputs {\r\n        key: \"logits\"\r\n        value {\r\n          name: \"head/predictions/logits:0\"\r\n          dtype: DT_FLOAT\r\n          tensor_shape {\r\n            dim {\r\n              size: -1\r\n            }\r\n            dim {\r\n              size: 1\r\n            }\r\n          }\r\n        }\r\n      }\r\n      outputs {\r\n        key: \"probabilities\"\r\n        value {\r\n          name: \"head/predictions/probabilities:0\"\r\n          dtype: DT_FLOAT\r\n          tensor_shape {\r\n            dim {\r\n              size: -1\r\n            }\r\n            dim {\r\n              size: 2\r\n            }\r\n          }\r\n        }\r\n      }\r\n      method_name: \"tensorflow/serving/predict\"\r\n    }\r\n  }\r\n```\r\n\r\nI'm working on using Pure Java binding of tensorflow to do prediction.", "I use this to export, following the code posted [https://github.com/tensorflow/models/blob/72f5834cc19bcfd3aef795dd926575fd9e0db802/official/mnist/mnist.py](here):\r\n```\r\n  features = tf.placeholder(dtype=tf.float64, shape=[15])\r\n  receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(\r\n      features={\"features\": features},\r\n  )\r\n  pixel_classifier.export_savedmodel(\r\n      export_dir_base=output_directory,\r\n      serving_input_receiver_fn=receiver_fn,\r\n      strip_default_attrs=True)\r\n```\r\n\r\nHere is the error I get:\r\n\r\n```\r\ntensorflow/python/framework/tensor_util.py\", line 348, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected float64, got {'features': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float64>} of type 'dict' instead.\r\n\r\n```\r\n\r\n\r\nAnd if I switch to:\r\n```\r\n  features = tf.placeholder(dtype=tf.float64, shape=[15])\r\n  receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(\r\n      features=features,\r\n  )\r\n```\r\n\r\nI would receive the following error:\r\n```\r\ntensorflow/python/estimator/export/export.py\", line 313, in _placeholders_from_receiver_tensors_dict\r\n    for name, t in input_vals.items()\r\nAttributeError: 'Tensor' object has no attribute 'items'\r\n\r\n```\r\n\r\nIt is not clear what kind of input the `build_raw_serving_input_receiver_fn` is expecting.", "@Ouwen ,I have encountered the same problem, how do you solve this problem , look forward to your reply?\r\n"]}, {"number": 12366, "title": "Documentation for windows install is wrong/misleading", "body": "https://www.tensorflow.org/install/install_windows states\r\n\r\nRequirements to run TensorFlow with GPU support\r\ncuDNN v5.\r\n\r\nThis is now incorrect.\r\nTensorFlow 1.3 or later requires cuDNN 6. ('cudnn64_6.dll')\r\n\r\nI also suggest you add a comment and link to this gist\r\n\r\nhttps://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c\r\n\r\nHe just saved me hours.\r\n\r\nRegards\r\n\r\nAidan", "comments": ["/CC @mrry ", "Thank you for posting this! Solved my issue", "@av8ramit Cam we also mention the helpful script @mrry prepared in our windows installation documentation?\r\nhttps://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c", "https://github.com/tensorflow/tensorflow/pull/12981", "Merged!"]}, {"number": 12365, "title": "Merging 1.3 back into master.", "body": "Accidentally made the branch on tf/tf. I'll delete after.", "comments": ["The test failure in \"//bazel_pip/tensorflow/python/eager:core_test\" in \"Linux CPU Test (Python 3)\" is pre-existing and unrelated."]}, {"number": 12364, "title": "Check cuda compute capability >= 3.0 in configure", "body": "Fix #12181", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @yongtang and @vrv to be potential reviewers."]}, {"number": 12363, "title": "Freeze tensor_forest graph for Android problem", "body": "Hi,\r\n\r\nI built a aimple Random Forest model in tensorflow, and want to freeze & optimize it for android.\r\nI used the following function for building the tesnor_forest estimator:\r\n\r\n      def build_estimator(_model_dir, _num_classes, _num_features, _num_trees, _max_nodes):\r\n      params = tensor_forest.ForestHParams(\r\n      num_classes=_num_classes, num_features=_num_features,\r\n      num_trees=_num_trees, max_nodes=_max_nodes, min_split_samples=3)\r\n\r\n    graph_builder_class = tensor_forest.RandomForestGraphs\r\n    return random_forest.TensorForestEstimator(\r\n      params, graph_builder_class=graph_builder_class,\r\n      model_dir=_model_dir)\r\n\r\n\r\n\r\n\r\nThis function stores the textual model to graph.pbtxt file in the specified model directory.\r\n\r\nThen I train it using:\r\n\r\n    est = build_estimator(output_model_dir, 3,np.size(features_eval,1), 5,6)\r\n    train_X = features_eval.astype(dtype=np.float32)\r\n    train_Y = labels_y.astype(dtype=np.float32)\r\n    est.fit(x=train_X, y=train_Y, batch_size=np.size(features_eval,0))\r\n\r\n\r\n(in this simple example: number of trees = 5, max_nodes=6)\r\n\r\nNow I want to freeze the model, so I call this function:\r\n`\r\ndef save_model_android(model_path):\r\n    checkpoint_state_name = \"model.ckpt-1\"\r\n    input_graph_name = \"graph.pbtxt\"\r\n    output_graph_name = \"freezed_model.pb\"\r\n    checkpoint_path = os.path.join(model_path, checkpoint_state_name)\r\n\r\n    input_graph_path = os.path.join(model_path, input_graph_name)\r\n    input_saver_def_path = None\r\n    input_binary = False\r\n    output_node_names = \"output\"\r\n    restore_op_name = \"save/restore_all\"\r\n    filename_tensor_name = \"save/Const:0\"\r\n    output_graph_path = os.path.join(model_path, output_graph_name)\r\n    clear_devices = True\r\n\r\n    freeze_graph(input_graph_path, input_saver_def_path,\r\n                              input_binary, checkpoint_path,\r\n                              output_node_names, restore_op_name,\r\n                              filename_tensor_name, output_graph_path,\r\n                              clear_devices, \"\")\r\n`\r\n\r\nand in the freezed_model.pb file generated I get only 1 op which is the output node.\r\nin the console I get the following message when the freeze_graph function is called:\r\n\r\n> Converted 0 variables to const ops.\r\n1 ops in the final graph.\r\n\r\nDoes anyone knows why only one node get exported when calling to freeze_graph?\r\n\r\nI'm using Tensorflow version  1.2.1 with cuda support , installed from sources on linux", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12362, "title": "tensorflow with cocos2d-x failed to build the project", "body": "**tensorflow with cocos2d-x failed to build the project\r\nwhen building project the terminal throws error**\r\n\r\n`jni/../../../../tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory  #include \"unsupported/Eigen/CXX11/Tensor\"`", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "@reedwm actually I have successfully able to build the .so file for arm-v7 architecture from the sources, and also create a jar file, after that I have tried to build the cocos2d-x project with prebuilt .so file of tensorflow, but while compiling it throws error mentioned above\r\n\r\nfor OS : Android \r\nArchitecture arm-v7\r\nCocos2d-x 3.14 c++\r\nDevelopment environment Mac OS x \r\nlet me know if need anything else", "@gunan any thoughts? I do not know what cocos2d-x is or how it relates to TensorFlow. If the TensorFlow binary is prebuilt, why is it giving an error when building a cocos2d-x project?", "I have no idea what cocos2d-x is. I recommend reaching out to the maintainers of the library. If in those discussions you can come up with something actionable for tensorflow, we will be happy to help.\r\n\r\n@andrewharp @petewarden just in case they know anything", "@reedwm @gunan cocos2d-x is a cross-platform gaming engine based on C++, for Android we have to build cocos2d-x library source and c++ classes with NDK, and to add prebuilt libraries we simply put in a directory and let android.mk know about it.\r\nthe problem arises when we try to build app with NDK it successfully able to detect the prebuilt library, and compile classes (contains tensor.h), but throws the above error.\r\n\r\nThe reason according to my understanding might be\r\n> either the eigen is not supported or has issue with android \r\n> either the prebuilt eigen is not compatible with native android NDK. \r\n\r\ncan you please look into this issue, I have to deliver this app soon", "Sorry, but we can't support Android.mk based builds of TF. Bazel, Makefile, and to some extent cmake are the supported options for Android. You may be able to find the assistance you need over at StackOverflow.\r\n\r\nThat said, might be worth checking out #4680 and also https://github.com/bazelbuild/bazel/issues/2638 to see if any of the workarounds apply at all."]}, {"number": 12361, "title": "Verbs hang fix", "body": "Replaces the sync wrappers for device to device operations in verbs_util, with a call to the async function with a callback function. Resolves Issue #11725 \r\n\r\nAs mentioned in the issue:\r\nI think the problem rises because the Sync deviceToDevice operation blocks the thread, preventing the earlier Async Device to Device operation from finishing- which, for some reason, blocks the later operation.\r\n\r\nI've tested this fix in origin/r1.3 since origin/master is currently broken.\r\n\r\n2nd commit only removes the unused functions from verbs_utils. It is not required for validity. \r\n\r\n@junshi15 , @shamoya , @byronyi , can you please go over this?", "comments": ["Can one of the admins verify this patch?", "@yanivbl6, thanks for your PR! By analyzing the history of the files in this pull request, we identified @caisq, @llhe and @ringw to be potential reviewers.", "Thanks for the fix. Is there any reason you did not get rid of CopyCPUTensorToGPUSync in verbs_util.h?", "CopyCPUTensorToGPUSync is still being used by RdmaRemoteRendezvous [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc#L116)\r\nI am still studying it, so I avoided changing this call. But if my understanding of the bug was correct it is definitely a risk.", "Nice work!\n\nI think I am using the same sync wrappers (yes all of us are lazy) in my GDR patch. I might as well take a closer look on this before someone raises another issue :)", "@yanivbl6 `CopyCPUTensorToGPUSync` can be an issue if not fixed. Can we fix the one in RdmaRemoteRendezvous as well? thanks.", "Sure, I am on it.", "I only have access to two boxes with fairly old OS (RHEL6.5) and CUDA 7.5, which limited me to TF1.1. I modify the patch to fit TF1.1, but got the following error during benchmark test.\r\n`error message: Cannot parse tensor from proto`\r\nNot sure if this is due to my adaptation or it is a problem with this patch. \r\n@shamoya , @bkovalev can you test this patch on TF1.2 or 1.3, if you have access to newer OS/CUDA. Thanks.", "Very good work @yanivbl6 , patch looks good.\r\nPlz just update when u finish checking this patch on TF1.2 and TF1.3.", "I ran successful tf_cnn_benchmarks with r1.3 ( 4 hosts (worker + ps), 8X4 GPUs, Imagenet data, Resnet-50 model). ", "Hi @poxvoculi @junshi15, \r\n\r\nCan we merge this ?\r\nAnd when it's merged to master, can we cherry-pick it to r1.3 (since master verbs code is broken) ? ", "@poxvoculi What's the policy for patching a branch? Given the fix for verbs on master will take time, it will be useful to patch TF 1.3 (and maybe 1.2).", "I don't know about branch patching.  @jhseu might know.", "Jenkins, test this please", "We're not making anymore changes to v1.3. This will appear in TF 1.4, though.", "Thanks @jhseu \r\nThe problem is that the verbs code is broken in master for now due to [this](https://github.com/tensorflow/tensorflow/issues/11825) issue.\r\nWe are working on a fix for, which may require some time.\r\nFor now, people who want to work with the RDMA Verbs need to patch r1.3 manually with this commit.\r\nIt's not so crazy, but less comfortabale.\r\nI thought v1.3.0 is already out, so cherry-picking this to r1.3 doesn't mean a lot.\r\nor are you updating the v1.3 binary version with r1.3 from time to time ?   "]}, {"number": 12360, "title": "BUILD:1227:1", "body": "bazel build //tensorflow:libtensorflow.so\r\nERROR: /Users/dile/tensorflow/tensorflow/core/BUILD:1227:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /Users/dile/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /Users/dile/tensorflow/tensorflow/core/BUILD:1227:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /Users/dile/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /Users/dile/tensorflow/tensorflow/core/BUILD:1227:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /Users/dile/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: Analysis of target '//tensorflow:libtensorflow.so' failed; build aborted.\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Your error messages indicate that you forgot to run configure script. Please make sure to follow the installing from sources documentation when trying to build tensorflow from sources:\r\nhttps://www.tensorflow.org/install/install_sources"]}, {"number": 12359, "title": "error", "body": "bazel build //tensorflow:libtensorflow.so\r\nERROR: /Users/dile/tensorflow/tensorflow/core/BUILD:1227:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /Users/dile/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /Users/dile/tensorflow/tensorflow/core/BUILD:1227:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /Users/dile/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /Users/dile/tensorflow/tensorflow/core/BUILD:1227:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /Users/dile/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: Analysis of target '//tensorflow:libtensorflow.so' failed; build aborted.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12358, "title": "[Feature Request]  Add an extra argument `is_duplicated` to `scatter_sub/*` Ops and make the kernel to multi-thread for speedup. ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nubuntu 16.04\r\n- **TenorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\nv1.3.0-rc2\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.5.2\r\n- **CUDA/cuDNN version**:\r\n8.0/5.1.10\r\n- **GPU model and memory**:\r\nnvidia M40\r\n- **CPU**\r\n32-cores\r\n\r\n\r\n### Describe the problem\r\nI profile the `embedding_lookup_sparse` on single machine with tfprof, in which i put the `params` on cpu. So the `gather` and `scatter_sub` Op are also put on cpu.\r\nProfiling result shows `gather` and `scatter_sub` Op takes so much time, the former takes about 33%, the latter takes about 43%. Making the `gather` op kernel to multi-thread gain about 10x speedup, relate to [PR](https://github.com/tensorflow/tensorflow/pull/12246) and [11709](https://github.com/tensorflow/tensorflow/issues/11709).\r\nFollow the same thought, I want to make the `scatter_sub` to be multi-thread. \r\n- If there is no duplicate index, we can realize lock-free code which could gain about 10x speedup too. \r\n- If there are duplicate indices, use lock which can gain little speedup.\r\n\r\nSo, I think an extra argument `is_duplicated` is a good choice to divide the two situation.\r\n\r\nHere is some profiling result:\r\n### tfprof\r\n```\r\n# orignial\r\nScatterSub                   8589.93MB (65.56%, 32.67%),       45.63ms (62.09%, 43.75%),            0us (45.25%, 0.00%),       45.63ms (62.90%, 45.87%)\r\n# with lock\r\nScatterSub                   8589.93MB (65.56%, 32.67%),       38.10ms (86.95%, 54.44%),            0us (48.97%, 0.00%),       38.10ms (89.98%, 58.78%)\r\n# without lock\r\nScatterSub                   8589.93MB (65.56%, 32.67%),        4.22ms (73.05%, 12.58%),            0us (48.86%, 0.00%),        4.22ms (76.99%, 14.63%)\r\n```\r\n### Code\r\n[embedding_lookup_sparse](https://gist.github.com/nolanliou/c00af5938b2aecfdc5ea1189426b8624)", "comments": ["/CC @alextp", "Since the indices Tensor is much smaller than the values Tensor I think it's possible to check for duplicates cheaply before doing the multi-threaded code, to make it appropriately lock-free. I'd prefer that over an argument we'd need to plumb across the entire interface.", "@alextp Yes, this way just need little change, I got about 2x speedup. If neccessary, I'll send a PR. by the way thanks for your advice.", "After some tests, we found another way whose performance is close to the lock-free code. Declare a mutex array with the size equals to the indices size, then use hash to get the mutex and lock every element's update. lock is cheap and contention is expensive."]}, {"number": 12357, "title": "tf.contrib.slim evaluation: outdated documentation", "body": "https://github.com/tensorflow/tensorflow/blob/d7fa7ae8ac15118393b6a549eb98ec9ca23497c0/tensorflow/contrib/slim/python/slim/evaluation.py\r\nThe documentation to this, in the first section (Evaluating metrics) uses this code to do the evaluation of the metrics directly (within an existing session, without having to reference a specific checkpoint):\r\n```\r\n  with tf.Session() as sess:\r\n    metric_values = slim.evaluation(\r\n        sess,\r\n        num_evals=1,\r\n        inital_op=initial_op,\r\n        eval_op=names_to_updates.values(),\r\n        final_op=name_to_values.values())\r\n```\r\nThis code, however, does not work anymore, as the function `slim.evaluation` doesn't exist now.\r\nwhat would now be the preferred way to do this?  ", "comments": ["@sguada can you comment or redirect to someone who can? Thanks.", "Not sure I understand what is the problem, could you clarify it?", "I have just edited the post - the problem is that the function `slim.evalution`, referenced in the comments at the top, doesn't exist anymore.", "Ok got it, now I understand.\r\n\r\nYeah the documentation need to be updated, do you want to do it?\r\n\r\nIt should be `slim.evaluation.evaluate_once()` and `slim.evaluate.evaluate_loop()`", "I could do that - however, the function evaluate_once() is not equivalent to the one in the documentation, which would have been useful for cases like evaluating the current model (not checkpointed) while in the training loop. Is there a reason why that functionality has been removed? Or has it been replaced by something else?", "@sguada I made some changes. Can you review this? #21020 "]}, {"number": 12356, "title": "CTC decoding with dictionary", "body": "Is there a possibility to use the CTC decoding algorithms provided by TF with a (word) dictionary?\r\nI've seen that there is some testing-code which seems to do just what I want:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/ctc/ctc_beam_search_test.cc\r\n\r\nIs there a Python interface for this task?\r\nOr do I have to build a custom operation out of the code shown above to be usable in Python?\r\n\r\n--\r\nEDIT: solved it by implementing custom op: https://github.com/githubharald/CTCWordBeamSearch", "comments": ["/CC @ebrevdo", "maybe, you have to write a custom op ~", "solved it by implementing a custom op. \r\nA really simple CTC algorithm is presented by Hwang [1]. Its a beam search through the RNN output. Integrating a char-bigram LM into this algorithm is easy.\r\nFor a word-level LM the CTC Token Passing algorithm [2] can be used.\r\n\r\n[1] https://arxiv.org/pdf/1601.06581.pdf\r\n[2] https://www.cs.toronto.edu/~graves/phd.pdf", "@githubharald do you have any resources on how to implement a custom op? I would like to integrate a char-level LM into CTC decoder but I have no idea how to begin. ", "I used this tutorial:\r\nhttps://www.tensorflow.org/extend/adding_an_op\r\n\r\nImplementation see: https://github.com/githubharald/CTCWordBeamSearch", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "There's a [DictionaryBeamScorer\r\n](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/ctc/ctc_beam_search_test.cc#L43) and the [related test case](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/ctc/ctc_beam_search_test.cc#L103) documents how to make use of it in C++.\r\n\r\nInstead of adding a custom op, you could first add some python options to the ctc_beam_search_decoder to allow passing a dictionary file and in the CTCBeamSearchDecoder class make use of DictionaryBeamScorer like the one in the test above.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 12355, "title": "404 for linux GPU python2 download", "body": "HTTP ERROR 404\r\n\r\nClick the url in readme, following occurs:\r\nProblem accessing /view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.3.0rc2-cp27-none-linux_x86_64.whl. Reason:\r\n Not Found", "comments": ["@gunan do you know what the URL should be? It's specifically for the Linux GPU: Python 2 link under the Installation section of https://github.com/tensorflow/tensorflow", "The new URL should not have the rc2.\r\n\r\n@av8ramit  could you take a look?", "Yeah I'll take a look!", "https://github.com/tensorflow/tensorflow/pull/12370", "https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.3.0-cp27-none-linux_x86_64.whl"]}, {"number": 12354, "title": "[XLA] Break the Slice test into different tests for each data type", "body": "This allows our backend (and others) to disable types which they do not support.  Previously all types were tested by the same test function.\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 12353, "title": "how to do 3-D image classification with timedistributed+conv2d", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12352, "title": "Resource Exhausted Error.", "body": "### This is my first experience with git hub, so, please mention if i didn't provide the right details\r\nHere is the Screen shot of the error.\r\nI am running my program on Tesla K20c GPU which has the following properties:\r\n**name: Tesla K20c\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.7055\r\npciBusID 0000:41:00.0\r\nTotal memory: 4.63GiB\r\nFree memory: 4.57GiB**\r\n\r\n![res](https://user-images.githubusercontent.com/31095828/29406999-6f69a618-8360-11e7-97c3-1b36b40503f3.png)\r\n\r\n------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution ( Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (binary)**:\r\n- **TensorFlow version (0.10.0)**:\r\n- **Python version(3.5.2)**: \r\n\r\n\r\n### Describe the problem\r\nI am trying to train the model from scratch and have about 500 train and test images and i am using the batch size as 128. the size of the each image is 512*512.\r\n\r\nI tried changing batch size to even 1, but still it is not working.\r\nWhy is it showing that error?\r\n\r\nThanks. \r\n\r\n", "comments": ["Your GPU must have enough memory for not only your data, but also the model (weights and gradients in particular, but also some extra space for computing the operations). Either make your images smaller or reduce the number of layers. You might find [the profiler](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/README.md) helpful.\r\n\r\nAlso, your question is better suited for [StackOverflow](https://stackoverflow.com/tags/tensorflow).", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12351, "title": "Tensorflow android can not check results for optimized model", "body": "I used this tutorial to run model on android : https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/index.html?index=..%2F..%2Findex#4 . But after pruning `DecodeJpeg` by command : \r\n\r\n    python -m tensorflow.python.tools.optimize_for_inference \\\r\n    --input=tf_files/retrained_graph.pb \\\r\n    --output=tf_files/optimized_graph.pb \\\r\n    --input_names=\"Cast\" \\\r\n    --output_names=\"final_result\"\r\nI can not check the result with this command : \r\n\r\n    python -m scripts.label_image \\\r\n    tf_files/flower_photos/daisy/21652746_cc379e0eea_m.jpg \\\r\n    tf_files/optimized_graph.pb\r\n\r\n,it shows error : `TypeError: Cannot interpret feed_dict key as Tensor: The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.`. So how can i check result for optimized android model on computer ?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12350, "title": "Windows 7 :  python mnist_with_summaries.py, having the error", "body": "## When running \" python tensorflow/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py \", it has occurred the following error: \r\n\r\n- \r\n![image](https://user-images.githubusercontent.com/20549473/29401037-213d52c6-8362-11e7-818d-58cf652e5efa.png)\r\n\r\n\r\n## Why?Any idea on how to resolve this problem?Thank you\uff01\r\n", "comments": ["Try to create a `tmp` folder yourself in `C:\\tmp`.\r\nor\r\nYou can go to `line 204` and `line 210` of `mnist_with_summaries.py` and change `/tmp` to wherever you want, for example `/tmp` -> `.`.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@aaronzs \r\n\r\n- According to your method, I have successfully modified. Thank you! \r\n\r\n- I changed _line 204_ and _line 210_ of _mnist_with_summaries.py_ to the _local directories_, and I created some folders.\r\n- **OR**, don't change the code, and I created some folders in the local disk where is the running environment according to the code. \r\n\r\n  -  line 204: create _/tmp/tensorflow/mnist/input_data_  \r\n\r\n  -  line 210: _create /tmp/tensorflow/mnist/logs/mnist_with_summaries_\r\n- **OR**, I only changed the` \\tmp` to` .`, and it created those folders in the local disk as mnist_with_summaries.py automatically.  (_tensorflow/mnist/input_data_ and _tensorflow/mnist/logs/mnist_with_summaries_)", "@angel1288 \r\nI do as what you told,but I get a new problem.Can you tell how to slove this?\r\n![image](https://user-images.githubusercontent.com/32335442/30921956-581ff0da-a3da-11e7-82c4-e783d03e7409.png)\r\n", "I really need help"]}, {"number": 12349, "title": "update from origin", "body": "merge from origin\r\naccess", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I just what to pull origin to my trunk", "This doesn't make any changes, so I'm closing it out. Please provide a better description in the pull request for future pulls."]}, {"number": 12348, "title": "convert .ckpt to .pb", "body": "I trained my own model using ssd_mobilenets in object_detection, and it works well on my computer. I see the update of android demo, I want to use the ssd_mobilenets model on my android phone. But I only have .ckpt files, and do not know how to trans .ckpt to .pb, because the graph is too comlex to see which is the final tensor name. Does only one know how to genetate .pb of ssd_mobilenets", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12347, "title": "Clean up 'shell=True' in using subprocess module", "body": "Currently, `configure.py` always use the running python interpreter to find the site-packages path. When user has specified a different python in the previous step, the specified one should be used.", "comments": ["@wangqr, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @vrv and @DavidNorman to be potential reviewers.", "Can one of the admins verify this patch?", "Thanks for the PR @wangqr. #12238 just got merged with a similar fix. I saw you also had other clean-ups in your PR. Would you mind merge your change to the current head?", "@tensorflow-jenkins test this please.", "Now this PR mainly do the `shell=True` cleanup, which I am still working. Sorry for pushing the commit before done.", "no worries. Let me know when it is ready for review again. Thanks!", "@yifeif Now I am done. Thanks!", "@tensorflow-jenkins test this please.", "The above commits should have fixed the command not found problem.", "@tensorflow-jenkins test this please"]}, {"number": 12346, "title": "string_input_producer num_epochs not working as expected", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX 10.12.6\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nReading a (csv) file with `string_input_producer()` / `start_queue_runners()` works up to a certain limit. Using `tf.train.shuffle_batch()`, only the first N lines of the file are read.\r\nAlso, num_epochs seems limited to a certain number (30 without batching, lower with batch).\r\n\r\n### Source code / logs\r\n\r\nSimple script for reading a csv file / training a linear model. The code WORKS. But it does NOT respond to changes in num_epochs / the number of steps printed at the end is not what we expect to see.\r\n\r\ncsv file: (model: W=-1, b=1 , y=Wx+b)\r\n\r\n2,-1\r\n3,0\r\n4,1\r\n5,2\r\n(repeated)\r\n\r\n\r\n    import tensorflow as tf\r\n\r\n    W = tf.Variable([.3], dtype=tf.float32)\r\n    b = tf.Variable([-.3], dtype=tf.float32)\r\n    x = tf.placeholder(tf.float32, name=\"x\")\r\n    y = tf.placeholder(tf.float32)\r\n    linear_model = tf.add(W * x, b, name=\"model\")\r\n    squared_deltas = tf.square(linear_model - y)\r\n    loss = tf.reduce_sum(squared_deltas)\r\n\r\n    train = tf.train.AdamOptimizer(1e-3).minimize(loss)\r\n\r\n    # reading input\r\n    filename_queue = tf.train.string_input_producer([\"/tmp/input.csv\"], num_epochs=100)\r\n    reader = tf.TextLineReader(skip_header_lines=0)\r\n    _, csv_row = reader.read(filename_queue)\r\n    record_defaults = [[0.], [0.]]\r\n    col_x, col_label = tf.decode_csv(csv_row, record_defaults=record_defaults)\r\n    features = tf.stack([col_x])\r\n\r\n    with tf.Session() as sess:\r\n      sess.run(tf.local_variables_initializer())\r\n      sess.run(tf.global_variables_initializer())\r\n      coord = tf.train.Coordinator()\r\n      threads = tf.train.start_queue_runners(coord=coord)\r\n      n = 0\r\n      while True:\r\n        try:\r\n          n += 1\r\n          inp, lab = sess.run([features, col_label])\r\n          sess.run(train, feed_dict={x:inp, y:lab})\r\n        except tf.errors.OutOfRangeError:\r\n          break\r\n        finally:\r\n          coord.request_stop()\r\n      coord.join(threads)\r\n      print(n)\r\n\r\nwith batch: adding those lines in reading input\r\n\r\n    batch_size = 20\r\n    min_after_dequeue = 10000\r\n    col_x, col_label = tf.decode_csv(csv_row, record_defaults=record_defaults)\r\n    capacity = 3 * min_after_dequeue + batch_size\r\n    x_batch, label_batch = tf.train.shuffle_batch(\r\n        [col_x, col_label], batch_size=batch_size, capacity=capacity,\r\n        min_after_dequeue=min_after_dequeue)\r\n", "comments": ["See this [question on SO](https://stackoverflow.com/questions/45725369/is-num-epochs-limited-in-tensorflows-csv-file-reader-string-input-producer), seems that I stupidly ordered try/while in the wrong order, especially because of the `finally` part"]}, {"number": 12344, "title": "Feature request: RecordInput to support gzipped tfrecord files.", "body": "Currently [RecordInput](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/record-input) doesn't support gzipped tfrecord files, would be great to have this supported.", "comments": ["Added a PR #12369 for gzip and lib support."]}, {"number": 12343, "title": "[Feature Request] provide a option to not call SessionRunHook in sess.run()", "body": "### Describe the problem\r\nIt is not necessary to execute all the bundled SessionRunHooks in some ```sess.run()``` cases.\r\nFor example, `sess.run(enqueue)` in another thread.\r\nits better to have an option like this:\r\n`session.run(run_hooks=True)`\r\n\r\nI would like to contribute this if someone think its a good idea.", "comments": ["@ispirmustafa can you comment? Thanks.", "@isaprykin is looking in to this.  ", "We added MonitoredSession.run_step_fn.  We hope it satisfies the use case. "]}, {"number": 12342, "title": "Add TensorBoard to 1.3 release notes", "body": "", "comments": ["@jart, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @vrv and @av8ramit to be potential reviewers."]}, {"number": 12341, "title": "Constructor with InputStream only for TensorFlowInferenceInterface", "body": "Added a InputStream Constructor for `TensorFlowInferenceInterface`, which support loading model from network or many other situations that we can only have a `InputStream` of the model.\r\n\r\nOne thing worth mentioning is that the private `modelName` member is not set this new constructor, but the field is private and only use in the original constructor (which I prefer this member should be deleted), I think it should be fine.", "comments": ["Can one of the admins verify this patch?", "@resec, thanks for your PR! By analyzing the history of the files in this pull request, we identified @andrewharp, @asimshankar and @tensorflower-gardener to be potential reviewers.", "Jenkins, test this please", "The tests fail with a syntax error. Mind taking a look?", "Jenkins, test this please", "Jenkins, test this please", "@andrewharp @jhseu thanks for the fix", "Jenkins, test this please", "Was just thinking about adding something like this. Glad to see someone beat me to the punch :)"]}, {"number": 12340, "title": "tf.multinomial with arbitrarily shaped tensors", "body": "It would be nice if once could use tf.multinomial with arbitrary tensors instead of just rank-2 ones. Currently this is only possible by pretending the extends along the other dimensions make for more examples in the batch, i.e. `tf.reshape(tf.multinomial(tf.reshape(x, [None, num_classes]), num_samples), x.get_shape().as_list[:-1])`.", "comments": ["@aselle, any thoughts if we should add this? Most ops only allow for one batch dimension, but some, such as `tf.matmul`, allow for any number of batch dimensions.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 12339, "title": "Feature suggestion: Keep_dim for slicing and list-based slicing with __getitem__ in Tensors and Variables", "body": "Sometimes it would be helpful to maintain the dimension when accessing a particular slice of a tensor or a variable. In NumPy this is possible by slicing with a list index like so:\r\n\r\n    ndarr = np.ones((5, 4, 3))\r\n    ndarr[: 1, :]  # Shape [5, 3]\r\n    ndarr[: [1], :]  # Shape [5, 1, 3]\r\n\r\nIt would be great to have similar list-based indexing which is essentially similar to what `tf.gather_nd`.", "comments": ["I found a simpler way to accomplish this: `ndarr[: 1:2, :]  # Shape [5, 1, 3]`"]}, {"number": 12338, "title": "Tensorflow crashes when i try to quantize my output", "body": "BUG\r\n(https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n```\r\n$ lspci -vvv|grep -i nvi\r\n00:1e.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1) (prog-if 00 [VGA controller])\r\n        Subsystem: NVIDIA Corporation GM204GL [Tesla M60]\r\n        Kernel driver in use: nvidia\r\n        Kernel modules: nvidia_375_drm, nvidia_375\r\n- **Exact command to reproduce**:\r\n```\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nWhen I tried to quantize my output\r\n\r\n-            result = SESS.run(layers[0], {'incept/DecodeJpeg/contents:0': f.read()})\r\n+            result = SESS.run(tf.quantize_v2(layers[0], min_range=0, max_range=10, T=tf.quint8), {'incept/DecodeJpeg/contents:0': f.read()})\r\n \r\nI run out of memory on my GPU\r\n\r\n\r\n```\r\n2017-08-16 20:59:40.313241: W tensorflow/core/common_runtime/bfc_allocator.cc:277] ***********************\r\n*****************************************************************************\r\n2017-08-16 20:59:40.313284: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: OOM when al\r\nlocating tensor with shape[720,1280,3]\r\nTraceback (most recent call last):\r\n  File \"generate_hashes.py\", line 89, in <module>\r\n    for (fn, points) in map_results.result():\r\n  File \"/usr/local/lib/python2.7/dist-packages/pebble/pool/base_pool.py\", line 208, in next\r\n    raise result\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[720,\r\n1280,3]\r\n         [[Node: incept/Cast = Cast[DstT=DT_FLOAT, SrcT=DT_UINT8, _device=\"/job:unknown_job/replica:0/task\r\n:0/gpu:0\"](incept/DecodeJpeg_G511)]]\r\n         [[Node: incept/pool_3_G513 = _Recv[client_terminated=false, recv_device=\"/job:unknown_job/replica\r\n:0/task:0/cpu:0\", send_device=\"/job:unknown_job/replica:0/task:0/gpu:0\", send_device_incarnation=-69665892\r\n98495454362, tensor_name=\"edge_1131_incept/pool_3\", tensor_type=DT_FLOAT, _device=\"/job:unknown_job/replic\r\na:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'incept/Cast', defined at:\r\n  File \"generate_hashes.py\", line 78, in <module>\r\n    load_network(False)\r\n  File \"generate_hashes.py\", line 40, in load_network\r\n    return tf.import_graph_def(graph_def, name='incept')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 311, in impo\r\nrt_graph_def\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_o\r\np\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[720,1280,3]\r\n         [[Node: incept/Cast = Cast[DstT=DT_FLOAT, SrcT=DT_UINT8, _device=\"/job:unknown_job/replica:0/task\r\n:0/gpu:0\"](incept/DecodeJpeg_G511)]]\r\n         [[Node: incept/pool_3_G513 = _Recv[client_terminated=false, recv_device=\"/job:unknown_job/replica\r\n:0/task:0/cpu:0\", send_device=\"/job:unknown_job/replica:0/task:0/gpu:0\", send_device_incarnation=-69665892\r\n98495454362, tensor_name=\"edge_1131_incept/pool_3\", tensor_type=DT_FLOAT, _device=\"/job:unknown_job/replic\r\na:0/task:0/cpu:0\"]()]]\r\n```\r\n", "comments": ["Please fill out the template fully (you left out some sections such as \"Python version\". Also provide the code that runs out of memory when you quantize the output.\r\n\r\nYou may be running out of memory on your GPU because adding the quantize op takes some amount of extra memory. How much memory is used when you do not quantize?", "I broke the quantization into a separate step. Sorry for not filling the form out right. But yes you were right the quant op ran out of memory", "OK, closing the issue since the issue was the GPU ran out of memory."]}]