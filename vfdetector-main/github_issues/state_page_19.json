[{"number": 52105, "title": "Very slow inference on TF Lite>=2.4.0 converted model", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.4 and Ubuntu 18.04 LTS on Colab machine\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.3.0 and 2.6.0\r\n\r\n\r\n### 2. Code\r\n\r\nThis notebook demonstrates the bug with the simplest example I came up with.\r\n\r\nhttps://colab.research.google.com/gist/ebraraktas/30d0a24ce2fb391a22e68a30e2f9cdc1\r\n\r\n### 3. Failure after conversion\r\n\r\nInference takes too much time on models converted by `tensorflow>=2.4.0` (See `Inference Logs`). I have attached the models; but as it is described in the colab, you can create them as well. \r\n\r\nAs you can see from the logs below, quantized `CONV2D` seems to updated to `version '5'` and this **runs approximately 27x slower** on x86_64 machine. However, it runs slightly faster on ARM.\r\n\r\n#### Inference Logs:\r\n\r\n##### x86_64 - Colab\r\n```\r\nTF Runtime Version: 2.6.0\r\nModel path: model_2.3.0.tflite\r\nTest Duration: 0.5242369174957275 s\r\n= = = = = = = = = = = = = = = = = = = = \r\nTF Runtime Version: 2.6.0\r\nModel path: model_2.3.0_quant.tflite\r\nTest Duration: 0.7532312870025635 s\r\n= = = = = = = = = = = = = = = = = = = = \r\nTF Runtime Version: 2.6.0\r\nModel path: model_2.6.0.tflite\r\nTest Duration: 0.532163143157959 s\r\n= = = = = = = = = = = = = = = = = = = = \r\nTF Runtime Version: 2.6.0\r\nModel path: model_2.6.0_quant.tflite\r\nTest Duration: 18.914307594299316 s\r\n= = = = = = = = = = = = = = = = = = = = \r\n= = = = = = = = = = = = = = = = = = = = \r\nTF Runtime Version: 2.3.0\r\nModel path: model_2.3.0.tflite\r\nTest Duration: 0.5360305309295654 s\r\n= = = = = = = = = = = = = = = = = = = = \r\nTF Runtime Version: 2.3.0\r\nModel path: model_2.3.0_quant.tflite\r\nTest Duration: 0.5518338680267334 s\r\n= = = = = = = = = = = = = = = = = = = = \r\nTF Runtime Version: 2.3.0\r\nModel path: model_2.6.0.tflite\r\nTest Duration: 0.5399584770202637 s\r\n= = = = = = = = = = = = = = = = = = = = \r\nTF Runtime Version: 2.3.0\r\nModel path: model_2.6.0_quant.tflite\r\nCannot create Interpreter! Exception:\r\nDidn't find op for builtin opcode 'CONV_2D' version '5'\r\nRegistration failed.\r\n```\r\n\r\n##### ARM - iPhone 12 - TensorFlowLite 2.6.0\r\n\r\n```\r\n| model_2.3.0.tflite | 199.0520 ms | \r\n| model_2.6.0.tflite | 183.2420 ms | \r\n| model_2.3.0_quant.tflite |  71.3949 ms | \r\n| model_2.6.0_quant.tflite |  63.4819 ms | \r\n```\r\n\r\n[models.zip](https://github.com/tensorflow/tensorflow/files/7216038/models.zip)\r\n", "comments": ["Hi Ethan, can you please take a look at this issue?..Thanks!", "Hi, are there any updates on this issue? @ethkim I think it is crucial, because it is easy to be misleaded by benchmark results of the model converted by recent tensorflow version. ", "Ping @ethkim . This is a blocker for me, because tensorflow `2.3` is getting older. Some operations do not have lite kernels in tensorflow `2.3`, but I need to use TF `2.3` to convert a model having `Conv` layer. I have tested the gist above with TF `2.7` and the issue still exists, unfortunately.", "Hi, I have investigated this issue and found some points which would be helpful for the assignee of the issue @ethkim  and anyone having the issue. \r\n\r\nThe new `Conv2D` implementation of the quantized tflite model uses [`HybridConvPerChannel`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/optimized/optimized_ops.h#L1423) which depends on the `ruy`, previously it was using [`HybridConv`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/optimized/optimized_ops.h#L1331).  And current `tensorflow` official python runtime was not built with AVX enabled, hence it uses [(Standard Cpp) implemation of the `ruy:TrMul`](https://github.com/google/ruy/blob/master/ruy/kernel.h#L209). My findinds are:\r\n\r\n1 - It is mentioned by @talumbau in [an issue](https://github.com/tensorflow/tensorflow/issues/42941#issuecomment-722038724) that using `RUY_PATHS=0x20` enables the AVX2 path, and it really runs faster with that environment variable. However, it creates wrong outputs _if tensorflow and `ruy` is not compiled with X86 enhancements_, you can try this on the colab shared in the first comment by setting `os.environ[\"RUY_PATHS\"] = \"0x20\"`. ~Actually, if it is easy to correct this output, it would be the quickest solution of the issue.~\r\n2 - It is possible to [build `tensorflow` from source](https://www.tensorflow.org/install/source) on Ubuntu with those instruction sets enabled using `--copt=-march=native --define=tflite_with_ruy=true` flags. Then using that python version runs even faster than the model converted by tf 2.3.0 (depending on your CPU).\r\n~3 - Unfortunately, the workaround in 2 does not apply to MacOS. See [this issue in `ruy`](https://github.com/google/ruy/issues/299).~ I was wrong at first, you can apply the solution mentioned at 2 on MacOS, as well. My mistake was linking with wrong dylib, please be careful.\r\n\r\nAccording to my findings, it would be better to add another optimized implementation or distribute tensorflow with AVX enabled for the current implementation. ~For the Apple MacOS, I think `ruy` team must update their implementation, I may be mistaken about this.~\r\n"]}, {"number": 52103, "title": "libtensorflowlite.so building problem tf2.3", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.3\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 5.5\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: 1080ti\r\n\r\n\r\n**Describe the problem**\r\nI am going to build **libtensorflowlite.so** for raspberry pi 3b using `bazel build --config=elinux_armhf -c opt //tensorflow/lite:libtensorflowlite.so `.\r\nHowever,  I get the error as shown bellow, I dont know how to fix that proble, it seems like like cross-compile toolchains errors `Internal error thrown during build. Printing stack trace: java.lang.RuntimeException: Unrecoverable error while evaluating node '@local_config_embedded_arm//:aarch64_toolchain_config BuildConfigurationValue.Key[6b681d7708c478d1cd948cd10629a4e4fb7eb97326825b5498e94f895f792b01] false' (requested by nodes '@local_config_embedded_arm//:cc-compiler-aarch64 BuildConfigurationValue.Key[6b681d7708c478d1cd948cd10629a4e4fb7eb97326825b5498e94f895f792b01] false'),` I have tried several ways to download ruy btw,  but still cannot get the bazel build works\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1.  I put the following into **tensorflow/lite/BUILD**\r\n\r\n```\r\ncc_binary(\r\n    name=\"libtensorflowLite.so\",\r\n    linkopts=[\r\n        \"-shared\",\r\n        \"-Wl,-soname=libtensorflowLite.so\"\r\n    ],\r\n    linkshared = 1,\r\n    copts = tflite_copts(),\r\n    deps=[\r\n        \":framework\",\r\n        \"//tensorflow/lite/kernels:builtin_ops\"\r\n    ]\r\n)\r\n```\r\n\r\n2.  `bazel build --config=elinux_armhf -c opt //tensorflow/lite:libtensorflowlite.so\r\n`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=211\r\nINFO: Reading rc options for 'build' from /home/yckj1497/tf_23/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/yckj1497/tf_23/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /home/yckj1497/tf_23/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/yckj2211/anaconda3/bin/python3 --action_env PYTHON_LIB_PATH=/home/yckj2211/anaconda3/lib/python3.6/site-packages --python_path=/home/yckj2211/anaconda3/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /home/yckj1497/tf_23/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/yckj1497/tf_23/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:elinux_armhf in file /home/yckj1497/tf_23/tensorflow/.bazelrc: --config=elinux --cpu=armhf\r\nINFO: Found applicable config definition build:elinux in file /home/yckj1497/tf_23/tensorflow/.bazelrc: --crosstool_top=@local_config_embedded_arm//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nINFO: Found applicable config definition build:linux in file /home/yckj1497/tf_23/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/yckj1497/tf_23/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Repository ruy instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule third_party_http_archive defined at:\r\n  /home/yckj1497/tf_23/tensorflow/third_party/repo.bzl:223:28: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'ruy':\r\n   java.io.IOException: thread interrupted\r\nInternal error thrown during build. Printing stack trace: java.lang.RuntimeException: Unrecoverable error while evaluating node '@local_config_embedded_arm//:aarch64_toolchain_config BuildConfigurationValue.Key[6b681d7708c478d1cd948cd10629a4e4fb7eb97326825b5498e94f895f792b01] false' (requested by nodes '@local_config_embedded_arm//:cc-compiler-aarch64 BuildConfigurationValue.Key[6b681d7708c478d1cd948cd10629a4e4fb7eb97326825b5498e94f895f792b01] false')\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:515)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:399)\r\n\tat java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1386)\r\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\r\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.execLocalTasks(ForkJoinPool.java:1040)\r\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1058)\r\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\r\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\r\nCaused by: java.lang.IllegalStateException: com.google.protobuf.TextFormat$ParseException: 10:13: Invalid escape sequence: '\\y'\r\n\tat com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getLegacyFeatures(CppActionConfigs.java:983)\r\n\tat com.google.devtools.build.lib.rules.cpp.CcModule.ccToolchainConfigInfoFromSkylark(CcModule.java:902)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat com.google.devtools.build.lib.syntax.MethodDescriptor.call(MethodDescriptor.java:130)\r\n\tat com.google.devtools.build.lib.syntax.BuiltinCallable.fastcall(BuiltinCallable.java:73)\r\n\tat com.google.devtools.build.lib.syntax.Starlark.fastcall(Starlark.java:359)\r\n\tat com.google.devtools.build.lib.syntax.Eval.doEval(Eval.java:588)\r\n\tat com.google.devtools.build.lib.syntax.Eval.eval(Eval.java:423)\r\n\tat com.google.devtools.build.lib.syntax.Eval.doEval(Eval.java:674)\r\n\tat com.google.devtools.build.lib.syntax.Eval.eval(Eval.java:423)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execReturn(Eval.java:215)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:259)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:231)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:53)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execFunctionBody(Eval.java:37)\r\n\tat com.google.devtools.build.lib.syntax.StarlarkFunction.fastcall(StarlarkFunction.java:115)\r\n\tat com.google.devtools.build.lib.syntax.Starlark.fastcall(Starlark.java:359)\r\n\tat com.google.devtools.build.lib.syntax.Starlark.call(Starlark.java:334)\r\n\tat com.google.devtools.build.lib.analysis.skylark.SkylarkRuleConfiguredTargetUtil.buildRule(SkylarkRuleConfiguredTargetUtil.java:136)\r\n\tat com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:468)\r\n\tat com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:190)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:888)\r\n\tat com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:899)\r\n\tat com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:338)\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:438)\r\n\t... 7 more\r\nCaused by: com.google.protobuf.TextFormat$ParseException: 10:13: Invalid escape sequence: '\\y'\r\n\tat com.google.protobuf.TextFormat$Tokenizer.parseException(TextFormat.java:1238)\r\n\tat com.google.protobuf.TextFormat$Tokenizer.consumeByteString(TextFormat.java:1228)\r\n\tat com.google.protobuf.TextFormat$Tokenizer.consumeByteString(TextFormat.java:1200)\r\n\tat com.google.protobuf.TextFormat$Tokenizer.consumeString(TextFormat.java:1181)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1987)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)\r\n\tat com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1823)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1944)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)\r\n\tat com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1812)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1944)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)\r\n\tat com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1812)\r\n\tat com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1689)\r\n\tat com.google.protobuf.TextFormat$Parser.merge(TextFormat.java:1675)\r\n\tat com.google.protobuf.TextFormat$Parser.merge(TextFormat.java:1566)\r\n\tat com.google.protobuf.TextFormat.merge(TextFormat.java:1370)\r\n\tat com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getFeature(CppActionConfigs.java:1594)\r\n\tat com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getLegacyFeatures(CppActionConfigs.java:413)\r\n\t... 34 more\r\n\r\nINFO: Elapsed time: 24.924s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (41 packages loaded, 251 targets configured)\r\nInternal error thrown during build. Printing stack trace: java.lang.RuntimeException: Unrecoverable error while evaluating node '@local_config_embedded_arm//:aarch64_toolchain_config BuildConfigurationValue.Key[6b681d7708c478d1cd948cd10629a4e4fb7eb97326825b5498e94f895f792b01] false' (requested by nodes '@local_config_embedded_arm//:cc-compiler-aarch64 BuildConfigurationValue.Key[6b681d7708c478d1cd948cd10629a4e4fb7eb97326825b5498e94f895f792b01] false')\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:515)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:399)\r\n\tat java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1386)\r\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\r\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.execLocalTasks(ForkJoinPool.java:1040)\r\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1058)\r\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\r\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\r\nCaused by: java.lang.IllegalStateException: com.google.protobuf.TextFormat$ParseException: 10:13: Invalid escape sequence: '\\y'\r\n\tat com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getLegacyFeatures(CppActionConfigs.java:983)\r\n\tat com.google.devtools.build.lib.rules.cpp.CcModule.ccToolchainConfigInfoFromSkylark(CcModule.java:902)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat com.google.devtools.build.lib.syntax.MethodDescriptor.call(MethodDescriptor.java:130)\r\n\tat com.google.devtools.build.lib.syntax.BuiltinCallable.fastcall(BuiltinCallable.java:73)\r\n\tat com.google.devtools.build.lib.syntax.Starlark.fastcall(Starlark.java:359)\r\n\tat com.google.devtools.build.lib.syntax.Eval.doEval(Eval.java:588)\r\n\tat com.google.devtools.build.lib.syntax.Eval.eval(Eval.java:423)\r\n\tat com.google.devtools.build.lib.syntax.Eval.doEval(Eval.java:674)\r\n\tat com.google.devtools.build.lib.syntax.Eval.eval(Eval.java:423)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execReturn(Eval.java:215)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:259)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:231)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:53)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execFunctionBody(Eval.java:37)\r\n\tat com.google.devtools.build.lib.syntax.StarlarkFunction.fastcall(StarlarkFunction.java:115)\r\n\tat com.google.devtools.build.lib.syntax.Starlark.fastcall(Starlark.java:359)\r\n\tat com.google.devtools.build.lib.syntax.Starlark.call(Starlark.java:334)\r\n\tat com.google.devtools.build.lib.analysis.skylark.SkylarkRuleConfiguredTargetUtil.buildRule(SkylarkRuleConfiguredTargetUtil.java:136)\r\n\tat com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:468)\r\n\tat com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:190)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:888)\r\n\tat com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:899)\r\n\tat com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:338)\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:438)\r\n\t... 7 more\r\nCaused by: com.google.protobuf.TextFormat$ParseException: 10:13: Invalid escape sequence: '\\y'\r\n\tat com.google.protobuf.TextFormat$Tokenizer.parseException(TextFormat.java:1238)\r\n\tat com.google.protobuf.TextFormat$Tokenizer.consumeByteString(TextFormat.java:1228)\r\n\tat com.google.protobuf.TextFormat$Tokenizer.consumeByteString(TextFormat.java:1200)\r\n\tat com.google.protobuf.TextFormat$Tokenizer.consumeString(TextFormat.java:1181)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1987)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)\r\n\tat com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1823)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1944)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)\r\n\tat com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1812)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1944)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)\r\n\tat com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1812)\r\n\tat com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1689)\r\n\tat com.google.protobuf.TextFormat$Parser.merge(TextFormat.java:1675)\r\n\tat com.google.protobuf.TextFormat$Parser.merge(TextFormat.java:1566)\r\n\tat com.google.protobuf.TextFormat.merge(TextFormat.java:1370)\r\n\tat com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getFeature(CppActionConfigs.java:1594)\r\n\tat com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getLegacyFeatures(CppActionConfigs.java:413)\r\n\t... 34 more\r\njava.lang.RuntimeException: Unrecoverable error while evaluating node '@local_config_embedded_arm//:aarch64_toolchain_config BuildConfigurationValue.Key[6b681d7708c478d1cd948cd10629a4e4fb7eb97326825b5498e94f895f792b01] false' (requested by nodes '@local_config_embedded_arm//:cc-compiler-aarch64 BuildConfigurationValue.Key[6b681d7708c478d1cd948cd10629a4e4fb7eb97326825b5498e94f895f792b01] false')\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:515)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:399)\r\n\tat java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1386)\r\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\r\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.execLocalTasks(ForkJoinPool.java:1040)\r\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1058)\r\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\r\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\r\nCaused by: java.lang.IllegalStateException: com.google.protobuf.TextFormat$ParseException: 10:13: Invalid escape sequence: '\\y'\r\n\tat com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getLegacyFeatures(CppActionConfigs.java:983)\r\n\tat com.google.devtools.build.lib.rules.cpp.CcModule.ccToolchainConfigInfoFromSkylark(CcModule.java:902)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat com.google.devtools.build.lib.syntax.MethodDescriptor.call(MethodDescriptor.java:130)\r\n\tat com.google.devtools.build.lib.syntax.BuiltinCallable.fastcall(BuiltinCallable.java:73)\r\n\tat com.google.devtools.build.lib.syntax.Starlark.fastcall(Starlark.java:359)\r\n\tat com.google.devtools.build.lib.syntax.Eval.doEval(Eval.java:588)\r\n\tat com.google.devtools.build.lib.syntax.Eval.eval(Eval.java:423)\r\n\tat com.google.devtools.build.lib.syntax.Eval.doEval(Eval.java:674)\r\n\tat com.google.devtools.build.lib.syntax.Eval.eval(Eval.java:423)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execReturn(Eval.java:215)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:259)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:231)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:53)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execFunctionBody(Eval.java:37)\r\n\tat com.google.devtools.build.lib.syntax.StarlarkFunction.fastcall(StarlarkFunction.java:115)\r\n\tat com.google.devtools.build.lib.syntax.Starlark.fastcall(Starlark.java:359)\r\n\tat com.google.devtools.build.lib.syntax.Starlark.call(Starlark.java:334)\r\n\tat com.google.devtools.build.lib.analysis.skylark.SkylarkRuleConfiguredTargetUtil.buildRule(SkylarkRuleConfiguredTargetUtil.java:136)\r\n\tat com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:468)\r\n\tat com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:190)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:888)\r\n\tat com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:899)\r\n\tat com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:338)\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:438)\r\n\t... 7 more\r\nCaused by: com.google.protobuf.TextFormat$ParseException: 10:13: Invalid escape sequence: '\\y'\r\n\tat com.google.protobuf.TextFormat$Tokenizer.parseException(TextFormat.java:1238)\r\n\tat com.google.protobuf.TextFormat$Tokenizer.consumeByteString(TextFormat.java:1228)\r\n\tat com.google.protobuf.TextFormat$Tokenizer.consumeByteString(TextFormat.java:1200)\r\n\tat com.google.protobuf.TextFormat$Tokenizer.consumeString(TextFormat.java:1181)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1987)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)\r\n\tat com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1823)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1944)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)\r\n\tat com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1812)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1944)\r\n\tat com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)\r\n\tat com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1812)\r\n\tat com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1689)\r\n\tat com.google.protobuf.TextFormat$Parser.merge(TextFormat.java:1675)\r\n\tat com.google.protobuf.TextFormat$Parser.merge(TextFormat.java:1566)\r\n\tat com.google.protobuf.TextFormat.merge(TextFormat.java:1370)\r\n\tat com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getFeature(CppActionConfigs.java:1594)\r\n\tat com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getLegacyFeatures(CppActionConfigs.java:413)\r\nFAILED: Build did NOT complete successfully (41 packages loaded, 251 targets configured)\r\n\r\n```", "comments": ["Hi @jvishnuvardhan ! ,Could you please look at this issue?", "There is a rule named \"tensorflowlite\" in tensorflow/lite/BUILD.\r\nI wonder why do you need to add your own rule? ", "> There is a rule named \"tensorflowlite\" in tensorflow/lite/BUILD. I wonder why do you need to add your own rule?\r\n\r\nsame issues occur for `bazel build --config=elinux_armhf -c opt //tensorflow/lite:tensorflowlite`"]}, {"number": 52087, "title": "Potential dangling-pointer bug in function `GetPyArrayDescrForTensor` by holding a reference in a list after all its references released (a static analyzer report)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (commit): faad219\r\n- Python version: 3.8.5\r\n\r\n**Static analysis results, no POC.**\r\nThis static analysis report has been manually reviewed to verify its validity.\r\n\r\n**Describe the current behavior**\r\n\r\nThe path provided by the static analyzer is as follows.\r\n\r\n1. A new reference is returned from `PyTuple_New` and pointed to by `field`.\r\nhttps://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/lib/core/ndarray_tensor.cc#L341\r\n\r\n2. A reference is stolen by function `PyList_SetItem`.\r\nhttps://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/lib/core/ndarray_tensor.cc#L350\r\n\r\n3. Refcnt decrement in macro `Py_CLEAR` will make the reference held in the list a dangling pointer.\r\nhttps://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/lib/core/ndarray_tensor.cc#L352\r\n\r\n**Contributing**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\nA potential correct fix can be removing the following line.\r\nhttps://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/lib/core/ndarray_tensor.cc#L352", "comments": ["Thanks for the report!\r\n\r\nThis looks legit to my eyes. I'm filing a fix removing that extra Py_Clear call.\r\nAs a note, the errorring code branch is invokes when calling `.numpy()` on a `DT_RESOURCE` tensor. This is likely rare. \r\n\r\nFor my education, what tool did you use to perform this static analysis?", "@Snape3058 Could you please respond to the [comment](https://github.com/tensorflow/tensorflow/issues/52087#issuecomment-931625027) above ?Thanks! ", "I build the tool on the top of Clang Static Analyzer, and it will be opened-source in recent days.\r\nThere are also some refcnt induced memory leak reports generated by this tool that have not been submitted.\r\nI am busy writing my paper for this tool these days, sorry for the delay.\r\nWhen the tool is ready, I will reply to this issue below for how to use this tool."]}, {"number": 52086, "title": " _EagerConst: Dst tensor is not initialized", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 21.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0-dev20210921 / 2.6\r\n- Python version: 3.9.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.4\r\n- GPU model and memory: NVIDIA TITAN RTX 24220MiB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running TF 2.7 and 2.6 with eager execution gives  the following error: \"in order to run _EagerConst: Dst tensor is not initialized.\" \r\n\r\nIt works with tf.compat.v1.disable_eager_execution() for TF 2.7/2.6 and TF 2.5.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n\r\n\r\nLearningRate of 1.000000e-04\r\nEpoch 1/100\r\n2021-09-22 12:01:03.300192: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\r\n2021-09-22 12:01:04.275126: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\r\n225/225 [==============================] - ETA: 0s - loss: 114.7353 - mae: 0.3077 - mse: 0.1493 - r2: 0.9997 - lr: 1.0000e-04\r\n2021-09-22 12:04:35.644414: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.90GiB (rounded to 5259396608)requested by op _EagerConst\r\nIf the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \r\nCurrent allocation summary follows.\r\nCurrent allocation summary follows.\r\n2021-09-22 12:04:35.644512: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\r\n2021-09-22 12:04:35.644553: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 82, Chunks in use: 73. 20.5KiB allocated for chunks. 18.2KiB in use in bin. 3.8KiB client-requested in use in bin.\r\n2021-09-22 12:04:35.644571: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 2, Chunks in use: 0. 1.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-09-22 12:04:35.644587: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 2, Chunks in use: 1. 2.8KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2021-09-22 12:04:35.644602: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-09-22 12:04:35.644620: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 3, Chunks in use: 2. 16.2KiB allocated for chunks. 8.5KiB in use in bin. 8.0KiB client-requested in use in bin.\r\n2021-09-22 12:04:35.644637: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 4, Chunks in use: 2. 47.0KiB allocated for chunks. 19.8KiB in use in bin. 16.0KiB client-requested in use in bin.\r\n2021-09-22 12:04:35.644653: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 1, Chunks in use: 1. 28.2KiB allocated for chunks. 28.2KiB in use in bin. 28.1KiB client-requested in use in bin.\r\n2021-09-22 12:04:35.644667: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-09-22 12:04:35.644679: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-09-22 12:04:35.644694: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 7, Chunks in use: 6. 1.42MiB allocated for chunks. 1.24MiB in use in bin. 1.15MiB client-requested in use in bin.\r\n2021-09-22 12:04:35.644717: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 3, Chunks in use: 2. 1.07MiB allocated for chunks. 641.0KiB in use in bin. 463.5KiB client-requested in use in bin.\r\n2021-09-22 12:04:35.644732: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 1, Chunks in use: 0. 840.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-09-22 12:04:35.644747: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 2, Chunks in use: 2. 2.85MiB allocated for chunks. 2.85MiB in use in bin. 2.85MiB client-requested in use in bin.\r\n2021-09-22 12:04:35.644762: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 3, Chunks in use: 2. 7.27MiB allocated for chunks. 4.69MiB in use in bin. 4.69MiB client-requested in use in bin.\r\n2021-09-22 12:04:35.644788: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-09-22 12:04:35.644795: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-09-22 12:04:35.644801: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 2, Chunks in use: 2. 53.47MiB allocated for chunks. 53.47MiB in use in bin. 53.47MiB client-requested in use in bin.\r\n2021-09-22 12:04:35.644807: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-09-22 12:04:35.644813: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 0. 80.21MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-09-22 12:04:35.644821: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-09-22 12:04:35.644828: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 3, Chunks in use: 1. 22.13GiB allocated for chunks. 17.62GiB in use in bin. 17.62GiB client-requested in use in bin.\r\n2021-09-22 12:04:35.644837: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 4.90GiB was 256.00MiB, Chunk State: \r\n2021-09-22 12:04:35.644863: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 748.21MiB | Requested Size: 80.21MiB | in_use: 0 | bin_num: 20, prev:   Size: 1.42MiB | Requested Size: 1.42MiB | in_use: 1 | bin_num: -1, next:   Size: 1.42MiB | Requested Size: 1.42MiB | in_use: 1 | bin_num: -1\r\n2021-09-22 12:04:35.644874: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 3.78GiB | Requested Size: 121.76MiB | in_use: 0 | bin_num: 20, prev:   Size: 1.42MiB | Requested Size: 1.42MiB | in_use: 1 | bin_num: -1\r\n2021-09-22 12:04:35.644879: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 23913299968\r\n2021-09-22 12:04:35.644892: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8016000000 of size 18924364800 next 41\r\n2021-09-22 12:04:35.644897: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f847dfae400 of size 28036096 next 79\r\n2021-09-22 12:04:35.644902: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f847fa6b000 of size 28036096 next 80\r\n2021-09-22 12:04:35.644908: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8481527c00 of size 2461184 next 81\r\n2021-09-22 12:04:35.644913: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8481780a00 of size 2461184 next 82\r\n2021-09-22 12:04:35.644919: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84819d9800 of size 256 next 86\r\n2021-09-22 12:04:35.644924: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84819d9900 of size 256 next 87\r\n2021-09-22 12:04:35.644929: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f84819d9a00 of size 84108288 next 60\r\n2021-09-22 12:04:35.644943: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8486a0fe00 of size 1492992 next 149\r\n2021-09-22 12:04:35.644950: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f8486b7c600 of size 784551936 next 127\r\n2021-09-22 12:04:35.644955: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84b57b1600 of size 1492992 next 144\r\n2021-09-22 12:04:35.644960: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f84b591de00 of size 4056293888 next 18446744073709551615\r\n2021-09-22 12:04:35.644964: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 2097152\r\n2021-09-22 12:04:35.644969: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00000 of size 256 next 1\r\n2021-09-22 12:04:35.644974: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00100 of size 1280 next 2\r\n2021-09-22 12:04:35.644978: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00600 of size 256 next 3\r\n2021-09-22 12:04:35.644982: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446a00700 of size 256 next 4\r\n2021-09-22 12:04:35.644987: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00800 of size 256 next 5\r\n2021-09-22 12:04:35.644991: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00900 of size 256 next 6\r\n2021-09-22 12:04:35.644995: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00a00 of size 256 next 7\r\n2021-09-22 12:04:35.645000: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00b00 of size 256 next 8\r\n2021-09-22 12:04:35.645008: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00c00 of size 256 next 9\r\n2021-09-22 12:04:35.645013: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00d00 of size 256 next 140\r\n2021-09-22 12:04:35.645018: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446a00e00 of size 256 next 154\r\n2021-09-22 12:04:35.645024: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00f00 of size 256 next 14\r\n2021-09-22 12:04:35.645029: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a01000 of size 256 next 15\r\n2021-09-22 12:04:35.645033: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446a01100 of size 256 next 24\r\n2021-09-22 12:04:35.645038: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a01200 of size 256 next 161\r\n2021-09-22 12:04:35.645042: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a01300 of size 256 next 20\r\n2021-09-22 12:04:35.645047: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a01400 of size 256 next 21\r\n2021-09-22 12:04:35.645052: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a01500 of size 221184 next 115\r\n2021-09-22 12:04:35.645056: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a37500 of size 256 next 135\r\n2021-09-22 12:04:35.645061: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446a37600 of size 860672 next 162\r\n2021-09-22 12:04:35.645067: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446b09800 of size 4608 next 150\r\n2021-09-22 12:04:35.645073: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446b0aa00 of size 464896 next 110\r\n2021-09-22 12:04:35.645078: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446b7c200 of size 8192 next 157\r\n2021-09-22 12:04:35.645083: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446b7e200 of size 256 next 12\r\n2021-09-22 12:04:35.645087: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446b7e300 of size 12032 next 102\r\n2021-09-22 12:04:35.645092: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446b81200 of size 330240 next 143\r\n2021-09-22 12:04:35.645096: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446bd1c00 of size 256 next 145\r\n2021-09-22 12:04:35.645101: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446bd1d00 of size 256 next 95\r\n2021-09-22 12:04:35.645105: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446bd1e00 of size 256 next 105\r\n2021-09-22 12:04:35.645110: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446bd1f00 of size 188672 next 18446744073709551615\r\n2021-09-22 12:04:35.645123: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 4194304\r\n2021-09-22 12:04:35.645129: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c00000 of size 256 next 23\r\n2021-09-22 12:04:35.645133: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00100 of size 256 next 131\r\n2021-09-22 12:04:35.645138: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00200 of size 256 next 26\r\n2021-09-22 12:04:35.645144: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00300 of size 256 next 27\r\n2021-09-22 12:04:35.645149: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00400 of size 256 next 28\r\n2021-09-22 12:04:35.645153: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00500 of size 256 next 31\r\n2021-09-22 12:04:35.645158: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c00600 of size 256 next 32\r\n2021-09-22 12:04:35.645162: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00700 of size 256 next 34\r\n2021-09-22 12:04:35.645166: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00800 of size 256 next 35\r\n2021-09-22 12:04:35.645172: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00900 of size 256 next 36\r\n2021-09-22 12:04:35.645178: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00a00 of size 256 next 37\r\n2021-09-22 12:04:35.645183: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00b00 of size 256 next 38\r\n2021-09-22 12:04:35.645187: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00c00 of size 256 next 39\r\n2021-09-22 12:04:35.645192: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00d00 of size 256 next 43\r\n2021-09-22 12:04:35.645196: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00e00 of size 256 next 44\r\n2021-09-22 12:04:35.645201: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00f00 of size 256 next 45\r\n2021-09-22 12:04:35.645205: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01000 of size 256 next 46\r\n2021-09-22 12:04:35.645209: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01100 of size 256 next 47\r\n2021-09-22 12:04:35.645214: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01200 of size 256 next 48\r\n2021-09-22 12:04:35.645218: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01300 of size 256 next 49\r\n2021-09-22 12:04:35.645223: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01400 of size 256 next 148\r\n2021-09-22 12:04:35.645230: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01500 of size 256 next 13\r\n2021-09-22 12:04:35.645234: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01600 of size 256 next 57\r\n2021-09-22 12:04:35.645239: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01700 of size 256 next 51\r\n2021-09-22 12:04:35.645243: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01800 of size 256 next 58\r\n2021-09-22 12:04:35.645247: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01900 of size 256 next 98\r\n2021-09-22 12:04:35.645252: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01a00 of size 256 next 106\r\n2021-09-22 12:04:35.645256: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c01b00 of size 256 next 50\r\n2021-09-22 12:04:35.645260: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01c00 of size 256 next 107\r\n2021-09-22 12:04:35.645265: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01d00 of size 256 next 124\r\n2021-09-22 12:04:35.645270: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01e00 of size 256 next 138\r\n2021-09-22 12:04:35.645274: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01f00 of size 256 next 158\r\n2021-09-22 12:04:35.645278: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02000 of size 256 next 112\r\n2021-09-22 12:04:35.645282: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02100 of size 256 next 141\r\n2021-09-22 12:04:35.645288: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02200 of size 256 next 66\r\n2021-09-22 12:04:35.645293: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02300 of size 256 next 67\r\n2021-09-22 12:04:35.645297: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02400 of size 256 next 68\r\n2021-09-22 12:04:35.645301: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02500 of size 256 next 33\r\n2021-09-22 12:04:35.645306: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c02600 of size 768 next 92\r\n2021-09-22 12:04:35.645310: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02900 of size 256 next 101\r\n2021-09-22 12:04:35.645314: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02a00 of size 256 next 62\r\n2021-09-22 12:04:35.645321: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02b00 of size 256 next 103\r\n2021-09-22 12:04:35.645326: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02c00 of size 256 next 54\r\n2021-09-22 12:04:35.645330: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c02d00 of size 256 next 137\r\n2021-09-22 12:04:35.645337: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02e00 of size 256 next 97\r\n2021-09-22 12:04:35.645341: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c02f00 of size 13568 next 30\r\n2021-09-22 12:04:35.645347: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c06400 of size 28928 next 42\r\n2021-09-22 12:04:35.645351: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c0d500 of size 14336 next 120\r\n2021-09-22 12:04:35.645356: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c10d00 of size 256 next 117\r\n2021-09-22 12:04:35.645361: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c10e00 of size 1536 next 146\r\n2021-09-22 12:04:35.645366: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c11400 of size 256 next 10\r\n2021-09-22 12:04:35.645370: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c11500 of size 256 next 152\r\n2021-09-22 12:04:35.645375: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c11600 of size 140288 next 91\r\n2021-09-22 12:04:35.645380: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c33a00 of size 512 next 25\r\n2021-09-22 12:04:35.645384: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c33c00 of size 256 next 53\r\n2021-09-22 12:04:35.645389: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c33d00 of size 256 next 130\r\n2021-09-22 12:04:35.645394: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c33e00 of size 7936 next 22\r\n2021-09-22 12:04:35.645398: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c35d00 of size 4096 next 56\r\n2021-09-22 12:04:35.645403: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c36d00 of size 230144 next 63\r\n2021-09-22 12:04:35.645408: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f000 of size 256 next 69\r\n2021-09-22 12:04:35.645415: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f100 of size 256 next 70\r\n2021-09-22 12:04:35.645420: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f200 of size 256 next 71\r\n2021-09-22 12:04:35.645427: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f300 of size 256 next 72\r\n2021-09-22 12:04:35.645432: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f400 of size 256 next 73\r\n2021-09-22 12:04:35.645437: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f500 of size 256 next 74\r\n2021-09-22 12:04:35.645441: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f600 of size 256 next 75\r\n2021-09-22 12:04:35.645446: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f700 of size 256 next 76\r\n2021-09-22 12:04:35.645451: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f800 of size 256 next 77\r\n2021-09-22 12:04:35.645455: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f900 of size 256 next 78\r\n2021-09-22 12:04:35.645460: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6fa00 of size 228096 next 83\r\n2021-09-22 12:04:35.645465: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446ca7500 of size 228096 next 84\r\n2021-09-22 12:04:35.645469: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446cdf000 of size 253440 next 85\r\n2021-09-22 12:04:35.645474: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446d1ce00 of size 326144 next 17\r\n2021-09-22 12:04:35.645481: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446d6c800 of size 2701312 next 18446744073709551615\r\n2021-09-22 12:04:35.645487: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \r\n2021-09-22 12:04:35.645495: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 73 Chunks of size 256 totalling 18.2KiB\r\n2021-09-22 12:04:35.645500: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\r\n2021-09-22 12:04:35.645505: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4096 totalling 4.0KiB\r\n2021-09-22 12:04:35.645510: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4608 totalling 4.5KiB\r\n2021-09-22 12:04:35.645515: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 8192 totalling 8.0KiB\r\n2021-09-22 12:04:35.645520: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 12032 totalling 11.8KiB\r\n2021-09-22 12:04:35.645524: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 28928 totalling 28.2KiB\r\n2021-09-22 12:04:35.645532: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 140288 totalling 137.0KiB\r\n2021-09-22 12:04:35.645536: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 221184 totalling 216.0KiB\r\n2021-09-22 12:04:35.645541: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 228096 totalling 445.5KiB\r\n2021-09-22 12:04:35.645546: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 230144 totalling 224.8KiB\r\n2021-09-22 12:04:35.645551: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 253440 totalling 247.5KiB\r\n2021-09-22 12:04:35.645555: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 326144 totalling 318.5KiB\r\n2021-09-22 12:04:35.645560: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 330240 totalling 322.5KiB\r\n2021-09-22 12:04:35.645564: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 1492992 totalling 2.85MiB\r\n2021-09-22 12:04:35.645569: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 2461184 totalling 4.69MiB\r\n2021-09-22 12:04:35.645574: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 28036096 totalling 53.47MiB\r\n2021-09-22 12:04:35.645579: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 18924364800 totalling 17.62GiB\r\n2021-09-22 12:04:35.645584: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 17.69GiB\r\n2021-09-22 12:04:35.645588: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 23919591424 memory_limit_: 23919591424 available bytes: 0 curr_region_allocation_bytes_: 34359738368\r\n2021-09-22 12:04:35.645598: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \r\nLimit:                     23919591424\r\nInUse:                     18990380800\r\nMaxInUse:                  21956931328\r\nNumAllocs:                       82937\r\nMaxAllocSize:              18924364800\r\nReserved:                            0\r\nPeakReserved:                        0\r\nLargestFreeBlock:                    0\r\n\r\n2021-09-22 12:04:35.645614: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ********************************************************************************___*_______________*\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n/tmp/ipykernel_24845/1230787370.py in <module>\r\n---> 16 early_history = model.fit(X_train,y_train,validation_data=(X_test,y_test),\r\n     17                     epochs=EPOCHS,initial_epoch=start_step, verbose=1,batch_size=32,\r\n\r\n~/data/python_tensorflow_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)\r\n     65     except Exception as e:  # pylint: disable=broad-except\r\n     66       filtered_tb = _process_traceback_frames(e.__traceback__)\r\n---> 67       raise e.with_traceback(filtered_tb) from None\r\n     68     finally:\r\n     69       del filtered_tb\r\n\r\n~/data/python_tensorflow_env/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n    104       dtype = dtypes.as_dtype(dtype).as_datatype_enum\r\n    105   ctx.ensure_initialized()\r\n--> 106   return ops.EagerTensor(value, ctx.device_name, dtype)\r\n    107 \r\n    108 \r\n\r\nInternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.\r\n", "comments": ["@negf \r\nPlease refer to similar issues and let us know:[link](https://github.com/tensorflow/tensorflow/issues/34391#issuecomment-557789595),[link1](https://stackoverflow.com/questions/37313818/tensorflow-dst-tensor-is-not-initialized)\r\nCan you share a colab gist of the code and error reported", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I don't know if the 2 links are related. As I said, I works with tf 2.5, but fails with tf >= 2.6.\r\n\r\nI cannot run an example on colab gist due to limited resources, but here is an example which fails on my system (with the error message from above)\r\nhttps://colab.research.google.com/drive/1y2YHfcZIv5lTJscgCKopZMHXGdxU1AND?usp=sharing", "@negf \r\nAre you still facing the issue", "Yes, on 2.8.0-dev20211027 the problem persist. ", "Looks like the root cause is that in eager mode, we're running out of memory on the GPU device. As a result, its unable to copy the input to the device. \r\n\r\nCan you try out smaller values of \"n\" in your program to see what succeeds? \r\nAlso can you check nvidia-smi after model creation / compilation to see how much memory is consumed? And compare that between enabling and disabling eager execution?", "With eager memory usage is way higher, see below. With eager execution the memory usage increase also from the first to the second epoch. \r\nAnd I don't know if it is related, while with n=9000 it runs with eager execution on the first run, a subsequent run of model.fit gives the same error message as if it would run out of memory. Similarly with n=4500, there I can make 2 runs before I run out of memory on the third execution of model.fit. In the 2nd run the memory usage reported from nvidia-smi is double to that of the first run. Looks like the memory is not properly deallocated. This  does not happen when eager execution is turned off.\r\n\r\n                     n       MEM(e=1)   MEM(e>1)\r\nwith eager  9000  19413MiB   23765MiB\r\n w/o eager  9000    1365MiB     1365MiB\r\nwith eager  4500  10389MiB    12629MiB\r\nw/o eager   4500    1365MiB      1365MiB\r\n\r\n\r\n", "Hi, I am facing the same issue. My code works with tf 2.4 but fails with tf>=2.6. Has this been resolved? \r\n\r\nFor me, the error occurs when trying to create a tf.data.Dataset from large numpy arrays.\r\n\r\nIs it because tf>=2.6 handles GPU memory differently? Is it possible that it tries to load the dataset onto the GPU, while in earlier versions the dataset was stored on the CPU?"]}, {"number": 52085, "title": "Potential use-after-free bug in function `EagerTensor_shape_tuple` by using an object after all its references released (a static analyzer report)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): faad219f\r\n- Python version: 3.8.5\r\n\r\n**Static analysis results, no POC available.**\r\nThis static analysis report has been manually reviewed to verify its validity.\r\n\r\n**Describe the current behavior**\r\n\r\nThe path provided by the static analyzer is as follows.\r\n\r\n1. A new reference is returned from `PyLong_FromLongLong` and pointed to by `dim`.\r\nhttps://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/eager/pywrap_tensor.cc#L605\r\n\r\n2. A reference is stolen by function `PyTuple_SetItem`. Assume error occurs, take the true branch (line 608~609).\r\nhttps://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/eager/pywrap_tensor.cc#L609\r\n\r\n3. Use after free in macro `Py_DECREF`.\r\nhttps://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/eager/pywrap_tensor.cc#L618\r\n\r\nPython API function `PyTuple_SetItem` will always steal a reference for the item set, and it will decrease the refcnt on error. Therefore, the refcnt will be first decreased in this API function, then decreased again on line 618.\r\n\r\n**Contributing**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\nA potential correct fix can be removing the following line.\r\nhttps://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/eager/pywrap_tensor.cc#L618\r\n", "comments": ["Hi @jvishnuvardhan  ,Could you please look at this issue ?", "Thanks for finding this!  To me, it looks like removing that line alone might be little confusing to read as the execution of `PyTuple_SetItem(shape, i, dim)` is more conditional than that.  Anyways, contributions welcome!"]}, {"number": 52079, "title": "make error in Tensorflow Lite Micro", "body": "**System information**\r\n- Linux Ubuntu 20.04\r\n- Python version: 3.8.10\r\n\r\n\r\n**The problem**\r\nI'm trying to generate the example projects from the tensorflow C++ library with Make and I run in this error:\r\n\r\n/bin/sh: 1: python: not found\r\nmake: *** [tensorflow/lite/micro/examples/network_tester/Makefile.inc:52 tensorflow/lite/micro/tools/make/gen/linux_x86_64default/prj/network_tester_test/keil/keil_project.uvprojx] Error 127\r\n\r\n**Sequence of commands**\r\ngit clone https://github.com/tensorflow/tflite-micro\r\ncd tflite-micro\r\nmake -f tensorflow/lite/micro/tools/make/Makefile generate_projects\r\n", "comments": ["Do you have python installed.\r\nYou can check by typing\r\npython\r\nin the terminal ", "I've checked and I confirm that I have python version 3.8.10", "@GiacomoMarchioni,\r\n\r\nCan you provide us the detailed error trace to expedite the trouble-shooting process? Thanks!", "the following is the whole message after the make command is executed for the first time:\r\n\r\ngiacomo@LAPTOP-KEM6AB3A:~/tflite-micro$ make -f tensorflow/lite/micro/tools/make/Makefile generate_projects\r\n--2021-09-23 01:41:57--  https://github.com/google/flatbuffers/archive/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip\r\nResolving github.com (github.com)... 140.82.121.4\r\nConnecting to github.com (github.com)|140.82.121.4|:443... connected.\r\nHTTP request sent, awaiting response... 302 Found\r\nLocation: https://codeload.github.com/google/flatbuffers/zip/dca12522a9f9e37f126ab925fd385c807ab4f84e [following]\r\n--2021-09-23 01:41:58--  https://codeload.github.com/google/flatbuffers/zip/dca12522a9f9e37f126ab925fd385c807ab4f84e\r\nResolving codeload.github.com (codeload.github.com)... 140.82.121.9\r\nConnecting to codeload.github.com (codeload.github.com)|140.82.121.9|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 1760478 (1.7M) [application/zip]\r\nSaving to: \u2018/tmp/tmp.GHo8RbXi4W/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip\u2019\r\n\r\n/tmp/tmp.GHo8RbXi4W/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip    100%[==================================================================================================================================================================>]   1.68M   551KB/s    in 3.1s\r\n\r\n2021-09-23 01:42:02 (551 KB/s) - \u2018/tmp/tmp.GHo8RbXi4W/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip\u2019 saved [1760478/1760478]\r\n\r\nApplying /home/giacomo/tflite-micro/tensorflow/lite/micro/tools/make/downloads/flatbuffers/.//../../flatbuffers.patch to /home/giacomo/tflite-micro/tensorflow/lite/micro/tools/make/downloads/flatbuffers/./\r\nSwitched to a new branch 'tflm-patch'\r\nCloning into 'tensorflow/lite/micro/tools/make/downloads/pigweed'...\r\nremote: Sending approximately 19.19 MiB ...\r\nremote: Counting objects: 6, done\r\nremote: Finding sources: 100% (6/6)\r\nremote: Total 29760 (delta 13305), reused 29757 (delta 13305)\r\nReceiving objects: 100% (29760/29760), 19.12 MiB | 2.70 MiB/s, done.\r\nResolving deltas: 100% (13305/13305), done.\r\nNote: switching to '47268dff45019863e20438ca3746c6c62df6ef09'.\r\n\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by switching back to a branch.\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -c with the switch command. Example:\r\n\r\n  git switch -c <new-branch-name>\r\n\r\nOr undo this operation with:\r\n\r\n  git switch -\r\n\r\nTurn off this advice by setting config variable advice.detachedHead to false\r\n\r\nHEAD is now at 47268dff pw_hdlc_lite: Client I/O improvements\r\nApplying /home/giacomo/tflite-micro/tensorflow/lite/micro/tools/make/downloads/pigweed/.//../../pigweed.patch to /home/giacomo/tflite-micro/tensorflow/lite/micro/tools/make/downloads/pigweed/./\r\nSwitched to a new branch 'tflm-patch'\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\" \"7e8191b24853d75de2af87622ad293ba\" tensorflow/lite/micro/tools/make/downloads/gemmlowp\r\ndownloading https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\r\n+ [[ 0 -eq 0 ]]\r\n+ break\r\n++ openssl dgst -md5 /tmp/tmp.9jJDGT6r7s/temp_file\r\n++ sed 's/.* //g'\r\n+ DOWNLOADED_MD5=7e8191b24853d75de2af87622ad293ba\r\n+ '[' 7e8191b24853d75de2af87622ad293ba '!=' 7e8191b24853d75de2af87622ad293ba ']'\r\n++ echo https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\r\n++ sed 's/\\?.*//'\r\n+ url=https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\r\n+ [[ https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip == *gz ]]\r\n+ [[ https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip == *tar.xz ]]\r\n+ [[ https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip == *bz2 ]]\r\n+ [[ https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip == *zip ]]\r\n+ unzip /tmp/tmp.9jJDGT6r7s/temp_file -d /tmp/tmp.ss16fJXxBU\r\n++ find /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425 -maxdepth 0\r\n++ wc -l\r\n+ '[' 1 = 1 ']'\r\n+ '[' -d /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425 ']'\r\n+ cp -R /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/AUTHORS /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/BUILD /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/CONTRIBUTING /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/CONTRIBUTORS /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/LICENSE /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/Makefile.travis /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/README.md /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/WORKSPACE /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/contrib /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/doc /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/eight_bit_int_gemm /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/fixedpoint /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/flags.bzl /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/internal /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/jni /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/meta /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/profiling /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/public /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/scripts /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/standalone /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/test /tmp/tmp.ss16fJXxBU/gemmlowp-719139ce755a0f31cbf1c37f7f98adcc7fc9f425/todo tensorflow/lite/micro/tools/make/downloads/gemmlowp/\r\n+ rm -rf /tmp/tmp.ss16fJXxBU /tmp/tmp.9jJDGT6r7s\r\n+ find tensorflow/lite/micro/tools/make/downloads/gemmlowp -type f -name '*BUILD' -delete\r\n+ [[ '' == \\p\\a\\t\\c\\h\\_\\a\\m\\_\\s\\d\\k ]]\r\n+ [[ '' == \\p\\a\\t\\c\\h\\_\\k\\i\\s\\s\\f\\f\\t ]]\r\n+ [[ '' == \\p\\a\\t\\c\\h\\_\\c\\i\\f\\a\\r\\1\\0\\_\\d\\a\\t\\a\\s\\e\\t ]]\r\n+ [[ '' == \\b\\u\\i\\l\\d\\_\\e\\m\\b\\a\\r\\c\\_\\m\\l\\i ]]\r\n+ [[ '' == \\s\\e\\t\\u\\p\\_\\z\\e\\p\\h\\y\\r ]]\r\n+ [[ -n '' ]]\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/google/ruy/archive/d37128311b445e758136b8602d1bbd2a755e115d.zip\" \"abf7a91eb90d195f016ebe0be885bb6e\" tensorflow/lite/micro/tools/make/downloads/ruy\r\ndownloading https://github.com/google/ruy/archive/d37128311b445e758136b8602d1bbd2a755e115d.zip\r\n+ [[ 0 -eq 0 ]]\r\n+ break\r\n++ openssl dgst -md5 /tmp/tmp.ucWBxUdbkV/temp_file\r\n++ sed 's/.* //g'\r\n+ DOWNLOADED_MD5=abf7a91eb90d195f016ebe0be885bb6e\r\n+ '[' abf7a91eb90d195f016ebe0be885bb6e '!=' abf7a91eb90d195f016ebe0be885bb6e ']'\r\n++ echo https://github.com/google/ruy/archive/d37128311b445e758136b8602d1bbd2a755e115d.zip\r\n++ sed 's/\\?.*//'\r\n+ url=https://github.com/google/ruy/archive/d37128311b445e758136b8602d1bbd2a755e115d.zip\r\n+ [[ https://github.com/google/ruy/archive/d37128311b445e758136b8602d1bbd2a755e115d.zip == *gz ]]\r\n+ [[ https://github.com/google/ruy/archive/d37128311b445e758136b8602d1bbd2a755e115d.zip == *tar.xz ]]\r\n+ [[ https://github.com/google/ruy/archive/d37128311b445e758136b8602d1bbd2a755e115d.zip == *bz2 ]]\r\n+ [[ https://github.com/google/ruy/archive/d37128311b445e758136b8602d1bbd2a755e115d.zip == *zip ]]\r\n+ unzip /tmp/tmp.ucWBxUdbkV/temp_file -d /tmp/tmp.F2WR9Q1PzY\r\n++ find /tmp/tmp.F2WR9Q1PzY/ruy-d37128311b445e758136b8602d1bbd2a755e115d -maxdepth 0\r\n++ wc -l\r\n+ '[' 1 = 1 ']'\r\n+ '[' -d /tmp/tmp.F2WR9Q1PzY/ruy-d37128311b445e758136b8602d1bbd2a755e115d ']'\r\n+ cp -R /tmp/tmp.F2WR9Q1PzY/ruy-d37128311b445e758136b8602d1bbd2a755e115d/BUILD /tmp/tmp.F2WR9Q1PzY/ruy-d37128311b445e758136b8602d1bbd2a755e115d/CMakeLists.txt /tmp/tmp.F2WR9Q1PzY/ruy-d37128311b445e758136b8602d1bbd2a755e115d/CONTRIBUTING.md /tmp/tmp.F2WR9Q1PzY/ruy-d37128311b445e758136b8602d1bbd2a755e115d/LICENSE /tmp/tmp.F2WR9Q1PzY/ruy-d37128311b445e758136b8602d1bbd2a755e115d/README.md /tmp/tmp.F2WR9Q1PzY/ruy-d37128311b445e758136b8602d1bbd2a755e115d/WORKSPACE /tmp/tmp.F2WR9Q1PzY/ruy-d37128311b445e758136b8602d1bbd2a755e115d/cmake /tmp/tmp.F2WR9Q1PzY/ruy-d37128311b445e758136b8602d1bbd2a755e115d/doc /tmp/tmp.F2WR9Q1PzY/ruy-d37128311b445e758136b8602d1bbd2a755e115d/example /tmp/tmp.F2WR9Q1PzY/ruy-d37128311b445e758136b8602d1bbd2a755e115d/ruy /tmp/tmp.F2WR9Q1PzY/ruy-d37128311b445e758136b8602d1bbd2a755e115d/third_party tensorflow/lite/micro/tools/make/downloads/ruy/\r\n+ rm -rf /tmp/tmp.F2WR9Q1PzY /tmp/tmp.ucWBxUdbkV\r\n+ find tensorflow/lite/micro/tools/make/downloads/ruy -type f -name '*BUILD' -delete\r\n+ [[ '' == \\p\\a\\t\\c\\h\\_\\a\\m\\_\\s\\d\\k ]]\r\n+ [[ '' == \\p\\a\\t\\c\\h\\_\\k\\i\\s\\s\\f\\f\\t ]]\r\n+ [[ '' == \\p\\a\\t\\c\\h\\_\\c\\i\\f\\a\\r\\1\\0\\_\\d\\a\\t\\a\\s\\e\\t ]]\r\n+ [[ '' == \\b\\u\\i\\l\\d\\_\\e\\m\\b\\a\\r\\c\\_\\m\\l\\i ]]\r\n+ [[ '' == \\s\\e\\t\\u\\p\\_\\z\\e\\p\\h\\y\\r ]]\r\n+ [[ -n '' ]]\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"http://mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/v130.zip\" \"438ba1fef5783cc5f5f201395cc477ca\" tensorflow/lite/micro/tools/make/downloads/kissfft patch_kissfft\r\ndownloading http://mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/v130.zip\r\n+ [[ 0 -eq 0 ]]\r\n+ break\r\n++ openssl dgst -md5 /tmp/tmp.DhpnNpXt7U/temp_file\r\n++ sed 's/.* //g'\r\n+ DOWNLOADED_MD5=438ba1fef5783cc5f5f201395cc477ca\r\n+ '[' 438ba1fef5783cc5f5f201395cc477ca '!=' 438ba1fef5783cc5f5f201395cc477ca ']'\r\n++ echo http://mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/v130.zip\r\n++ sed 's/\\?.*//'\r\n+ url=http://mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/v130.zip\r\n+ [[ http://mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/v130.zip == *gz ]]\r\n+ [[ http://mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/v130.zip == *tar.xz ]]\r\n+ [[ http://mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/v130.zip == *bz2 ]]\r\n+ [[ http://mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/v130.zip == *zip ]]\r\n+ unzip /tmp/tmp.DhpnNpXt7U/temp_file -d /tmp/tmp.TjPZqCpIdY\r\n++ find /tmp/tmp.TjPZqCpIdY/kissfft-130 -maxdepth 0\r\n++ wc -l\r\n+ '[' 1 = 1 ']'\r\n+ '[' -d /tmp/tmp.TjPZqCpIdY/kissfft-130 ']'\r\n+ cp -R /tmp/tmp.TjPZqCpIdY/kissfft-130/CHANGELOG /tmp/tmp.TjPZqCpIdY/kissfft-130/COPYING /tmp/tmp.TjPZqCpIdY/kissfft-130/Makefile /tmp/tmp.TjPZqCpIdY/kissfft-130/README /tmp/tmp.TjPZqCpIdY/kissfft-130/README.simd /tmp/tmp.TjPZqCpIdY/kissfft-130/TIPS /tmp/tmp.TjPZqCpIdY/kissfft-130/_kiss_fft_guts.h /tmp/tmp.TjPZqCpIdY/kissfft-130/kiss_fft.c /tmp/tmp.TjPZqCpIdY/kissfft-130/kiss_fft.h /tmp/tmp.TjPZqCpIdY/kissfft-130/kissfft.hh /tmp/tmp.TjPZqCpIdY/kissfft-130/test /tmp/tmp.TjPZqCpIdY/kissfft-130/tools tensorflow/lite/micro/tools/make/downloads/kissfft/\r\n+ rm -rf /tmp/tmp.TjPZqCpIdY /tmp/tmp.DhpnNpXt7U\r\n+ find tensorflow/lite/micro/tools/make/downloads/kissfft -type f -name '*BUILD' -delete\r\n+ [[ patch_kissfft == \\p\\a\\t\\c\\h\\_\\a\\m\\_\\s\\d\\k ]]\r\n+ [[ patch_kissfft == \\p\\a\\t\\c\\h\\_\\k\\i\\s\\s\\f\\f\\t ]]\r\n+ patch_kissfft tensorflow/lite/micro/tools/make/downloads/kissfft\r\n+ sed -i -E 's@#ifdef FIXED_POINT@// Patched automatically by download_dependencies.sh so default is 16 bit.\\\r\n#ifndef FIXED_POINT\\\r\n#define FIXED_POINT (16)\\\r\n#endif\\\r\n// End patch.\\\r\n\\\r\n#ifdef FIXED_POINT@g' tensorflow/lite/micro/tools/make/downloads/kissfft/kiss_fft.h\r\n+ sed -i -E '/^#include <sys\\/types.h>/d' tensorflow/lite/micro/tools/make/downloads/kissfft/kiss_fft.h\r\n+ sed -i -E 's@#ifdef FIXED_POINT@#ifdef FIXED_POINT\\\r\n#include <stdint.h> /* Patched. */@g' tensorflow/lite/micro/tools/make/downloads/kissfft/kiss_fft.h\r\n+ sed -i -E 's@#define KISS_FFT_MALLOC malloc@#define KISS_FFT_MALLOC(X) (void*)(0) /* Patched. */@g' tensorflow/lite/micro/tools/make/downloads/kissfft/kiss_fft.h\r\n+ sed -i -E 's@#define KISS_FFT_FREE free@#define KISS_FFT_FREE(X) /* Patched. */@g' tensorflow/lite/micro/tools/make/downloads/kissfft/kiss_fft.h\r\n+ sed -ir -E 's@(fprintf.*\\);)@/* \\1 */@g' tensorflow/lite/micro/tools/make/downloads/kissfft/tools/kiss_fftr.c\r\n+ sed -ir -E 's@(exit.*\\);)@return; /* \\1 */@g' tensorflow/lite/micro/tools/make/downloads/kissfft/tools/kiss_fftr.c\r\n+ echo 'Finished patching kissfft'\r\nFinished patching kissfft\r\n/bin/sh: 1: python: not found\r\nmake: *** [tensorflow/lite/micro/examples/network_tester/Makefile.inc:52: tensorflow/lite/micro/tools/make/gen/linux_x86_64_default/prj/network_tester_test/keil/keil_project.uvprojx] Error 127", "@GiacomoMarchioni This is more related to tflite micro. I was able to run those commands in colab. [Here](https://colab.research.google.com/gist/jvishnuvardhan/4af29c05d36d04488e2c75c6c067fa78/untitled.ipynb) is a gist for our reference.\r\n\r\nAs this is more related to TFlite micro, posting it on TFlite micro repository would result in faster resolution  https://github.com/tensorflow/tflite-micro\r\n\r\nThanks"]}, {"number": 52076, "title": "Python: XNNPack delegate performance with num_threads is slower than normal tflite without any delagte", "body": "Hi,\r\n\r\nFor the python-based yolov4 (tflite fp32 model) detection pipeline observed the performance is slower with xnn pack delegate (with num threads options )compared to tflite without any delegate\r\n\r\n**System Information**\r\n\r\n- Ubuntu 20.04\r\n- Intel i5 8th gen\r\n- Installed TF from source\r\n- TF version 2.6\r\n- Python version 3.9\r\n- Bazel version 3.7.2\r\n- GCC 9.3.0 \r\n\r\nWith reference to the previous issue #42277 I tried to use setNumThreads, but for python, I couldn't find the function. I have also set num threads in `tflite.interpreter (model, num_threads)`  API. With that num threads, I didn't observe performance improvement. I could also see only one thread being utilized in my system\r\n\r\nSo to use all the threads in my machine I hardcoded the thread value in the \"tflite_with_xnnpack_optional.cc\" file\r\n\r\n`  opts.num_threads = 8;//num_threads > 1 ? num_threads : 0;  (hard coded it to 8)`\r\n\r\nWith this change for my python pipeline, I could see a 30% improvement in performance \r\n\r\nKindly redirect me to the API to enable the num threads option for python ( similar to c++)\r\n\r\nThe command I used to compile TF from source\r\n`bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both -k --define tflite_with_xnnpack=true //tensorflow/tools/pip_package:build_pip_package \r\n`\r\n\r\nI have also ensured that my tflite model has delegate kernels support by running in benchmark tool (13 delegate kernels)\r\n", "comments": ["When I was trying to debug and trace the flow,\r\ninterpreter() ====> Interpreter Operator() ( there is another operator() with num thread as arguments) but in the trace observed that Interpreter Operator() with arguments is not being called and also observed while registering the delegates num threads is -1 ", "> When I was trying to debug and trace the flow,\r\n> interpreter() ====> Interpreter Operator() ( there is another operator() with num thread as arguments) but in the trace observed that Interpreter Operator() with arguments is not being called and also observed while registering the delegates num threads is -1\r\n\r\nSorry for the late reply, and many thanks for debugging this issue! Yes, this was indeed an issue. \r\n\r\nCoincidentally, because of the fix of a different issue, based on the current workflow, I think this issue should be fixed by commit 3d3c6db1ca2d50f6f07722cd800144f8f736167c where we did lazy initialization on the XNNPACK delegate instance in AllocateTensors(...). It means the \"#threads\" set either by InterpreterBuilder(...) or SetNumThreads(...) before calling AllocateTensors(...) should be captured, and I think this is the case with the Python implementation where the [SetNumThreads](https://github.com/tensorflow/tensorflow/blob/2766734171c1587c520c99f9881d38875c87ead3/tensorflow/lite/python/interpreter.py#L490) is called during [__init__](https://github.com/tensorflow/tensorflow/blob/2766734171c1587c520c99f9881d38875c87ead3/tensorflow/lite/python/interpreter.py#L401).\r\n\r\nIn light of this, @umadevimcw, could you sync to this commit or ahead and check whether this issue is fixed or not? Many thanks!"]}, {"number": 52074, "title": "Swap indices in detect_objects, reflecting reordered output tensors", "body": "Fixes issue https://github.com/tensorflow/tensorflow/issues/51591", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/52074\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52074) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "AFAIK it may not be a simple change. Leave it to @ziyeqinghan for judgement.", "@ziyeqinghan Any update on this PR? Please. Thanks!", "@ziyeqinghan Any update on this PR? Please. Thanks!", "@jawj Can you please resolve conflicts? Thanks!", "@jawj Can you please resolve conflicts? Thanks!", "All I did was change some index numbers in a way that fixed things for me at the time. I don't fully understand the changes that conflict here, so I'm not the right person to fix the conflict.", "Hi @jawj  Please click on the resolve conflicts button and check the lines of the code.  Thank you."]}, {"number": 52064, "title": "TF Lite `Conv1D` conversion bug", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.4 and Ubuntu 18.04 LTS on Colab machine\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.3.0 and 2.6.0\r\n\r\n### 2. Code\r\n\r\nThis notebook demonstrates the bug with the simplest example I came up with.\r\n\r\nhttps://colab.research.google.com/gist/ebraraktas/69995d036a35a8d7f744c845a163863e\r\n\r\nYou can set `use_tf_2_3 = True` to see that it runs on tensorflow `2.3.0` with no error.\r\n\r\n### 3. Failure after conversion\r\n\r\nAs you can see in the Colab gist, model can be converted to TF Lite model successfully, but different tensorflow versions creates different graphs (see [Netron](https://github.com/lutzroeder/netron) outputs below). And tensorflow lite versions >= 2.4.0 fail to reshape and allocate tensors. See runtime error below:\r\n\r\n```\r\nRuntimeError: tensorflow/lite/kernels/reshape.cc:69 num_input_elements != num_output_elements (25000 != 0)Node number 2 (RESHAPE) failed to prepare.\r\n```\r\n\r\n| TensorFlow 2.3.0 | TensorFlow 2.6.0 |\r\n| --- | --- |\r\n| ![model_2 3 0](https://user-images.githubusercontent.com/62459770/133984264-8e2149f3-df82-4b3b-9cfe-1a5b4b2dabf5.png) | ![model_2 6 0](https://user-images.githubusercontent.com/62459770/133984266-4aac1db5-99df-4d7c-b5cf-7bde923971a8.png) |\r\n ", "comments": ["@ebraraktas,\r\n\r\nCan you take a look at this [link](https://stackoverflow.com/questions/62386682/node-number-x-reshape-failed-to-prepare-tensor-resize-with-tflite-v2-2) which addresses similar issue? Thanks!", "@sanatmpa1 ,\r\nAs I wrote in the description, I can already convert model using TensorFlow 2.3 and run inference. This bug still exists in newer versions. \r\n\r\nThe workaround that you have sent fixes the input shape of the model, but I want it to have **dynamic shape** at runtime (like the model converted by TF 2.3.0). ", "@sanatmpa1 ,\r\nI have found a workaround which allows me to convert model and run inference on `tensorflow>=2.4.0`. If you change this line:\r\n```python\r\n_ = model.predict(tf.random.normal((1, 100, 80)))\r\n```\r\nwith this:\r\n```python\r\nmodel.build(input_shape=(None, None, 80))\r\n```\r\nyou can convert and run without an error. \r\n\r\nI think this may give you an insight about fixing the bug.\r\n", "@ebraraktas,\r\n\r\nThanks for the update on how you fixed it, as it may help in future. Please feel free to close this issue if your question is solved.", "@sanatmpa1 ,\r\n\r\nI don't consider this issue as fixed. My suggestion was just a workaround. And the bug still exists on latest stable version `2.6.0`. It would be better to close the issue when it is really fixed."]}, {"number": 52053, "title": "loss=nan issue in Tensorflow 2", "body": "There is a bug in Tensorflow 2 that happens when all of the following conditions are met:\r\n1.\tMulti-GPU is enabled. \r\n2.\tCustom loss function is used. \r\n3.\tThe number of training samples is not an integer multiple of the batch size (e.g. 3 training samples and a batch size of 2). \r\n\r\nUnder these conditions, you'll end up with a nan loss. \r\n(The bug also happens if you use multiple GPU cards with batch_size=1, nobody would actually do that.)\r\nI have tried Tensorflow 2.1, 2.4, and 2.5, so it seems like the bug is in all versions of Tensorflow 2. \r\n\r\nHere is a sample code you can run to reproduce the issue:\r\n\r\n```\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\" # use a single GPU card, and bug doesn't happen\r\nimport tensorflow as tf\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\nfor device in physical_devices:\r\n  tf.config.experimental.set_memory_growth(device, True)\r\nprint(\"tensorflow.__version__: \" + str(tf.__version__))\r\nimport numpy as np\r\nModel = tf.keras.models.Model\r\nInput = tf.keras.layers.Input\r\nConv2D = tf.keras.layers.Conv2D\r\nK = tf.keras.backend\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nnum_training_samples = 3 # if = 4, bug doesn't happen because 4 is a multiple of 2\r\nbatch_size = 2 # bug also happens with batch_size = 1 if you set multiple GPU cards above, regardless of if the number of training samples is an even multiple of the number of GPU cards\r\n\r\ninputdata = np.zeros((num_training_samples,5,5,1))\r\noutputdata = np.ones((num_training_samples,5,5,1))\r\n\r\nwith strategy.scope(): # remove this, and bug doesn't happen\r\n  input_layer = Input(shape=(5,5,1))\r\n  output_layer = Conv2D(filters=1, kernel_size=(3,3), padding='same', activation='tanh', data_format='channels_last', kernel_initializer='glorot_uniform')(input_layer)\r\n  model = Model(inputs=input_layer, outputs=output_layer)\r\n\r\n  def custom_loss(y_true, y_pred):\r\n    diff = K.square(y_true - y_pred)\r\n    loss = K.mean(diff)\r\n    return loss\r\n\r\n  model.compile(loss=custom_loss, # if loss='mse', bug doesn't happen\r\n                optimizer='sgd')\r\n\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((inputdata, outputdata))\r\ntrain_dataset = train_dataset.with_options(options)\r\ntrain_dataset = train_dataset.batch(batch_size)\r\nmodel.fit(train_dataset, epochs=10, verbose=1)\r\n#model.fit(inputdata, outputdata, batch_size=batch_size, epochs=10, verbose=1) # bug still happens even if you don't use Tensorflow Dataset\r\n```\r\n", "comments": ["@jvishnuvardhan,\r\n\r\nAs colab doesn't support multi GPU environment, The code runs fine([gist](https://colab.research.google.com/gist/sanatmpa1/face09972613ddec08460a1a397358db/52053.ipynb)) and I am not able to replicate the issue. Can you take a look into this one?", "This could be due to the lack of precision about the reduction strategy used. Could you please look at [this](https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function) and tell us if it helps.", "I am having a similar issue getting loss nan when using gpu but training works fine when using cpu", "@jshu12 Can you please check with recent TF versions and let us know whether this is still an issue for you. Meanwhile, I would also check on my system. Thanks!", "I verified on Tensorflow 2.8 and the bug is still there. "]}, {"number": 52048, "title": "Profile TensorFlow Performance: Multiple occurences of single Conv2D op", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.6\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: 11.4/8.2\r\n- GPU model and memory: RTX3090/24GB\r\n\r\n\r\n**Describe the current behavior**\r\nMy goal is to profile NN layers runtime on GPU device (I am concerned only with inference). I am using\r\ntf.profiler module.\r\nEven though single convolution operation is used in the network, profile shows multiple Conv2D occurrences.\r\n\r\n**Describe the expected behavior**\r\nWith one Conv2D used in the nwtrok, shouldn't profiler be showing only one Conv2D?\r\n\r\n**Standalone code to reproduce the issue**\r\nI have created a simple network as shown below.\r\ndummy_net.py\r\n```\r\nimport tensorflow as tf\r\nimport os\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\nclass Net(tf.keras.Model):\r\n    def __init__(self) -> None:\r\n        super().__init__()\r\n        self.conv = tf.keras.layers.Conv2D(6,3)\r\n    \r\n    def call(self, x):\r\n        x = self.conv(x)\r\n        return x\r\n```\r\nand profiling it on random input.\r\ntest_dummy_net.py\r\n\r\n```\r\nimport tensorflow as tf\r\nimport dummy_net\r\n\r\ninp = tf.random.uniform([1,300,300,3])\r\nmodel = dummy_net.Net()\r\ntf.profiler.experimental.start('./dummy_infer_logs')\r\ny = model(inp)\r\ntf.profiler.experimental.stop()\r\ntf.debugging.set_log_device_placement(True)\r\n```\r\n\r\n**Other info / logs**\r\nWhen I check profile logs in the TensorBoard, it shows that Conv2D op has occurred multiple times.\r\n![image](https://user-images.githubusercontent.com/25697952/133830669-c238655e-13bd-4fc8-8bc7-b0932f7e056c.png)\r\n\r\nWhen I add more Conv2D layers, occurrences go on increasing. For example, for 4 Conv2D layers more than 1900 occurrences are shown. This makes it difficult to get exact run times.\r\n\r\nAlso, why same operations are also run on the host?\r\n![image](https://user-images.githubusercontent.com/25697952/133830755-44f70f07-8d18-41e7-9a90-6cf00e8dae1a.png)\r\n", "comments": ["@shelkesagar29 Could you please confirm if this issue is related tensorboard ? If so please post this issue in [tensorboard repo](https://github.com/tensorflow/tensorboard/issues).Thanks!\r\n ", "How can I confirm that?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 52045, "title": "constant_folding ignores epsilon", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. [https://gist.github.com/szutenberg/76b35f503195e8c7b2f1d6d764ee1a6f]\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0 (CPU version)\r\n- Python version: 3.6.9\r\n\r\nWhen running the script https://gist.github.com/szutenberg/76b35f503195e8c7b2f1d6d764ee1a6f I'm getting:\r\n```\r\n=== EPSILON =  1e-09  dtype =  <dtype: 'float32'>\r\nin  =  [0.000000e+00 1.000000e-09 9.999999e-01 1.000000e+00]\r\nout =  [-0.       -0.       15.942385       inf]\r\n=== EPSILON =  1e-07  dtype =  <dtype: 'float32'>\r\nin  =  [0.000000e+00 1.000000e-09 9.999999e-01 1.000000e+00]\r\nout =  [-1.1920928e-07 -1.1920928e-07  1.5249238e+01  1.5942385e+01]\r\n=== EPSILON =  1e-09  dtype =  <dtype: 'float64'>\r\nin  =  [0.000000e+00 1.000000e-09 9.999999e-01 1.000000e+00]\r\nout =  [-1.00000008e-09  0.00000000e+00  1.61081453e+01  2.07232658e+01]\r\n=== EPSILON =  1e-07  dtype =  <dtype: 'float64'>\r\nin  =  [0.000000e+00 1.000000e-09 9.999999e-01 1.000000e+00]\r\nout =  [-9.99999951e-08 -9.89999951e-08  1.54249485e+01  1.61180957e+01]\r\n```\r\n\r\nAdd( Sub(1, logits), epsilon) is simplified by grappler (constant folding) to Sub(1+epsilon, logits)\r\n\r\nUser doesn't expect inf because epsilon was used for this purpose. It's expected to see:\r\n```\r\n=== EPSILON =  1e-09  dtype =  <dtype: 'float32'>\r\nin  =  [0.000000e+00 1.000000e-09 9.999999e-01 1.000000e+00]\r\nout =  [-0.        -0.        15.9340315 20.723267 ]\r\n=== EPSILON =  1e-07  dtype =  <dtype: 'float32'>\r\nin  =  [0.000000e+00 1.000000e-09 9.999999e-01 1.000000e+00]\r\nout =  [-1.1920928e-07 -1.1920928e-07  1.5333239e+01  1.6118095e+01]\r\n=== EPSILON =  1e-09  dtype =  <dtype: 'float64'>\r\nin  =  [0.000000e+00 1.000000e-09 9.999999e-01 1.000000e+00]\r\nout =  [-1.00000008e-09  0.00000000e+00  1.61081453e+01  2.07232658e+01]\r\n=== EPSILON =  1e-07  dtype =  <dtype: 'float64'>\r\nin  =  [0.000000e+00 1.000000e-09 9.999999e-01 1.000000e+00]\r\nout =  [-9.99999951e-08 -9.89999951e-08  1.54249485e+01  1.61180957e+01]\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/37601244/133803576-2b598fef-247a-4794-9df4-3e6208ec8b5c.png)\r\nAs we can see, it's impossible to represent 1+1e-9 in float32. It's rounded to 1.0 which causes inf (which very likely will break the training with NaN).\r\n\r\nThis case is extracted from model using hard negative mining. The input comes from sigmoid.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): Yes\r\n- Briefly describe your candidate solution(if contributing):\r\nconstant folding should not simplify A+B = A when B != 0\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://gist.github.com/szutenberg/76b35f503195e8c7b2f1d6d764ee1a6f\r\n\r\nDoes it make sense to prepare patch resolving this issue?", "comments": ["@Saduf2019\r\nCould you please look at this issue! I was able to reproduce the issue!  in tf [2.5](https://colab.research.google.com/gist/mohantym/bb53196c75e3ab96330dbd93c480c0a0/github_52045.ipynb#scrollTo=oS1QZpXWkL_Z),[2.6 ](https://colab.research.google.com/gist/mohantym/5daa1fbb63634d089a455d5a7da86bc9/github_52045.ipynb#scrollTo=oS1QZpXWkL_Z)and [2.7](https://colab.research.google.com/gist/mohantym/49df0055e3b3061a61e03fb2fa0451c4/github_52045.ipynb#scrollTo=mnaSBNXHWSf5)."]}, {"number": 52030, "title": "Error in lowering tf ops to HLO using tf-opt", "body": "I am trying to lower following tf ops to HLO using :tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt  -xla-legalize-tf  sample.mlir\r\n    tf.CropAndResize \r\n    tf.StridedSlice \r\n    tf.Unique\r\n    tf.Where \r\n    tf.SparseToDense \r\n    tf.NonMaxSuppressionV4 \r\n    tf.TensorListFromTensor \r\n    tf.TensorListGetItem \r\n    tf.TensorListReserve       \r\n    tf.TensorListSetItem \r\n    tf.TensorListStack \r\n    tf.TopKV2\r\n    tf.ResizeBilinear\r\n    tf.ResizeNearestNeighbor\r\nI am getting error as \r\nThe following operations cannot be legalized. These legalization failure(s) may be due to missing TF to HLO lowerings and/or unsupported attributes, etc.\r\nI am trying to lower these ops with dynamic input shape.\r\nI have attached tf dialect mlir files for each failed ops.\r\n[mlir_files.zip](https://github.com/tensorflow/tensorflow/files/7177750/mlir_files.zip)\r\n\r\nThanks\r\n\r\n", "comments": ["@tilakrayal ,\r\nEnvironment details:\r\n1.tensorflow version 2.7.0-dev20210831\r\n\r\n2.tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt --version\r\nLLVM (http://llvm.org/):\r\nLLVM version 14.0.0git\r\nOptimized build.\r\nDefault target: x86_64-unknown-linux-gnu\r\nHost CPU: cascadelake\r\n3.tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate --version\r\nLLVM (http://llvm.org/):\r\nLLVM version 14.0.0git\r\nOptimized build.\r\nDefault target: x86_64-unknown-linux-gnu\r\nHost CPU: cascadelak", "@ArunaKote,\r\n\r\nAs you're using `2.7.0-dev20210831` which is the nightly-version and not a stable release, Can you clarify if the same issue exists even in `2.6.0`?", "@sanatmpa1 ,yes in tf 2.6.0 also same issue exists.", "Legalize is only for the 2nd phase, you need to run first phase of the bridge too to convert TensorList ones. Assigning to bridge team to triage.", "Thank you.For the static inputs,TensorList ops are decomposed using tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt  -tf-tensor-list-ops-decomposition tf.mlir then I able to convert it to HLO. But for dynamic inputs its not decomposing  and that's why I am not able to lower these ops to HLO. Actually I wanted lower above tf-ops to HLO with dynamic shape. These ops are failing in MRCNN network.", "@smit-hinsu,lowering tf.ResizeBilinear is not available with  -xla-legalize-tf .  Sir, for ResizeBilinear op lowering the below flow can be used? or is there any other optimized flow which can be used for lowering.\r\nFor input_image=hxw and output_image=h1xw1\r\ntx=w/w1\r\nty=h/h1\r\nfor (i=0;i<h1;i++){\r\nfor(j=0;j<w1;j++){\r\nx=int(tx*j)\r\ny=int(ty*i)\r\nx_diff=((tx*j)-x)\r\ny_diff=((ty*i)-y)\r\noutput_image[i][j]=input_image[y][x]*(1-x_diff))*(1-y_diff)+\r\n                                input_image[y][x+1]*(x_diff))*(1-y_diff)+\r\n\t\t        input_image[y+1][x]*(1-x_diff))*(y_diff)+\r\n\t\t        input_image[y+1][x+1]*(x_diff))*(y_diff)\r\n\r\n}}\r\n\r\nThanks", "@ArunaKote HI, Did you solve this problem\uff1fI also met the tf.ResizeBilinear operation cannot be legalized.", "@ArunaKote Dynamic inputs are not yet supported for many ops like ResizeBilinear. We could discuss if you would like to contribute a change to support dynamic inputs.", "Is there any update regarding support for dynamic input ?"]}, {"number": 52027, "title": "eigen_mkldnn_contraction_kernel_test has ambigious select", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.6.0\r\n\r\n**Describe the problem**\r\n\r\nThe build fails when using `--config=mkl_aarch64` with \r\n\r\n```\r\nAnalyzing: 913 targets (107 packages loaded, 28576 targets configured)\r\nINFO: Repository remote_java_tools_linux instantiated at:\r\n  /DEFAULT.WORKSPACE.SUFFIX:237:6: in <toplevel>\r\n  /tmp/tmpon64uebn-bazel-tf/a0cb78cfb3034d90b816abe220cf6cc9/external/bazel_tools/tools/build_defs/repo/utils.bzl:201:18: in maybe\r\nRepository rule http_archive defined at:\r\n  /tmp/tmpon64uebn-bazel-tf/a0cb78cfb3034d90b816abe220cf6cc9/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\r\nERROR: Analysis of target '//tensorflow/core/kernels:eigen_mkldnn_contraction_kernel_test' failed; build aborted: /tmp/boegel/TensorFlow/2.6.0/foss-2021a/TensorFlow/tensorflow-2.6.0/tensorflow/core/kernels/BUILD:2957:11: Illegal ambiguous match on configurable attribute \"srcs\" in //tensorflow/core/kernels:eigen_mkldnn_contraction_kernel_test:\r\n//tensorflow:arm_any\r\n//tensorflow/core/kernels:no_mkldnn_contraction_kernel\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nIssue is simple: https://github.com/tensorflow/tensorflow/blob/72fd2bfa42a8ad909baf8d2b7b674563d256514d/tensorflow/core/kernels/BUILD#L3028-L3034 is faulty as `no_mkldnn_contraction_kernel` and any of the arch-selects can match, in this case the `arm_any` which makes this invalid\r\n", "comments": ["Hi @Flamefire \r\n\r\nI would recommend to have a look at the official Intel Optimization for TensorFlow page: \r\nhttps://www.intel.com/content/www/us/en/developer/articles/guide/optimization-for-tensorflow-installation-guide.html\r\n\r\nThe official flag is just `--config=mkl`.\r\n\r\nAlso, JFYI, you can always download the pre-compiled binaries from Anaconda or get it from the Intel AI Analytics Toolkit. The steps are also documented in the link above. For your convenience, here's the quick way to get it:\r\n\r\n`conda install tensorflow -c anaconda`\r\n", "@shailensobhee Thanks for your answer. However it does not help for installing TensorFlow from source or at all on ARM architectures. Besides that the condition is clearly buggy: A flag to disable something is mixed with arch-specific defaults leading to ambiguity and hence being unable to use that flag on many archs"]}, {"number": 52014, "title": "[TFLite] Use integer-only operations for the quantized reference REDUCE_MEAN and REDUCE_SUM kernels", "body": "Hello,\r\n\r\nThis PR refactors the different quantized reference reduce mean kernels into a single `QuantizedMeanOrSum` solving the following [TODO](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/reduce.cc#L359) and also removes the now unused code.\r\n\r\nThe `QuantizedMeanOrSum` kernel is modified to only use integer operations (solving this [TODO](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/reference/reduce.h#L444)) and to avoid costly divisions by integrating the mean division into the MultiplyByQuantizedMultiplier parameters.\r\n\r\nNote that we currently keep the same int32 `temp_sum` accumulator but we could consider to raise it to int64_t (mainly for int16 but it would also be useful for large int8 tensors).\r\n\r\nThibaut", "comments": ["Thanks for this PR. I have created https://github.com/tensorflow/tflite-micro/pull/593 to handle downstream TFLM changes and we can merge this PR as is.", "@renjie-liu Can you please review this PR ? Thanks!", "@renjie-liu Can you please review this PR ? Thanks!", "@renjie-liu Can you please review this PR ? Thanks!", "I saw the tflite-micro PR is closed. Can this PR be submitted at current state?", "The change in tflite-micro is still necessary unfortunately. Would need to check with @advaitjain on why the PR was closed.", "Hi @advaitjain, @Tessil  Any update on this PR? Please. Thank you!", "Hi @advaitjain, @Tessil Any update on this PR? Please. Thank you!"]}, {"number": 52002, "title": "[BUG FIX]fix mkl relu6 backward bug.", "body": "fix mkl relu6 backward bug and use eltwise_clip_v2 to substitute eltwise_bounded_relu\r\n", "comments": ["@penpornk  Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!"]}, {"number": 52000, "title": "Handle `attrs` objects in `tensorflow.python.util.nest.assert_shallow_structure`", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): *Yes*\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 20.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): **Binary**\r\n- TensorFlow version (use command below): **v1.12.1-63725-gfeb49693266 2.7.0-dev20210914**\r\n- Python version: **3.7.11**\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nError when trying to use the methods in `tensorflow.python.util.nest` that call `assert_shallow_structure`, e.g. `map_structure_up_to`, on an instance of a class created by `attrs`.\r\n\r\nThis happens when trying to subclass `tensorflow_probability.python.distributions.Distribution` where `_event_shape` is an `attrs` object and the public `event_shape` method calls `map_structure_up_to`.\r\n\r\n**Describe the expected behavior**\r\n\r\nThis call should run without an error on `attrs` objects like `map_structure` does.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): **yes**\r\n- Briefly describe your candidate solution(if contributing): **Add another case before the default one that checks `_is_attrs` and makes sure that the names are the same using `_get_attrs_items`.**\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport attr\r\nimport tensorflow as tf\r\nfrom tensorflow.python.util import nest\r\n\r\n@attr.attrs(auto_attribs=True)\r\nclass Container:\r\n    a: object\r\n    b: object\r\n\r\nshape_object = Container(a=[1, 2], b=[3])\r\nshallow_object = Container(a=None, b=None)\r\nshape_res = nest.map_structure_up_to(shallow_object, tf.TensorShape, shape_object)\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"scratch.py\", line 14, in <module>\r\n    shape_res = nest.map_structure_up_to(shallow_object, tf.TensorShape, shallow_object)\r\n  File \"/home/cswa648/miniconda3/envs/tftest/lib/python3.7/site-packages/tensorflow/python/util/nest.py\", line 1380, in map_structure_up_to\r\n    **kwargs)\r\n  File \"/home/cswa648/miniconda3/envs/tftest/lib/python3.7/site-packages/tensorflow/python/util/nest.py\", line 1462, in map_structure_with_tuple_paths_up_to\r\n    expand_composites=expand_composites)\r\n  File \"/home/cswa648/miniconda3/envs/tftest/lib/python3.7/site-packages/tensorflow/python/util/nest.py\", line 1090, in assert_shallow_structure\r\n    if len(input_tree) != len(shallow_tree):\r\nTypeError: object of type 'Container' has no len()\r\n```\r\n", "comments": ["@christiaanjs I tried to run the code on Colab using `TF v2.6` and `tf-nightly` ,and faced `TypeError: object of type 'Container' has no len()` ,please find the gist[ here ](https://colab.research.google.com/gist/sushreebarsa/bc5dcd38caed585940d3b88b887671f8/untitled199.ipynb)for reference. Could you please confirm the same ? Thanks!", "@sushreebarsa yes, this is the same error I\u2019m reporting (see the output I provided)"]}, {"number": 51996, "title": "A few documentation errors and omissions for \"Install TensorFlow for C\"", "body": "The page **https://www.tensorflow.org/install/lang_c** \"Install TensorFlow for C\" has several errors and omissions:-\r\n1. The first link \"bindings for other languages\" is a dead link (error 404)\r\n2. It says that it is built nightly, but the second link \"libtensorflow-nightly GCS bucket\" says that it was last built about a year ago\r\n3. It looks as though it was last built about a month ago.  It would be good to have the date that it was last built somewhere on the page\r\n4. For \"Supported Platforms\", it says that it works for \"macOS, Version 10.12.6 (Sierra) or higher\", but the download file says that it is for a x86_64 (ie. Intel) CPU.  The latest Macs (for almost a year now) use the M1 processor.  Does it support the M1 Mac.", "comments": ["@EricTheRed20 \r\nWe have created a CL for this once the CL is closed, we will update this issue to resolved.", "I've tested the libtensorflow.dylib file (and the other ones) for compatibility with the M1 Mac by running the XCode command: xcrun lipo -info \"libtensorflow.dylib\" and it returns \"libtensorflow.dylib is architecture x86_64\".  It should return \"libtensorflow.dylib are: arm64 x86_64\" in order to support both the Intel Mac and the M1 Mac.\r\n\r\nThe line \"macOS, Version 10.12.6 (Sierra) or higher\" can therefore be updated to say \"macOS, Version 10.12.6 (Sierra) or higher for Intel Macs only (M1 Mac not available yet)", "@EricTheRed20 \r\nIs this still an issue.", "Item 1 has been fixed, but items 2, 3 and 4 haven't been fixed, so yes, it is still an issue.  The most important one to fix is item 4.  The library provided only works for old Mac's.  It needs to work for the new Mac's."]}, {"number": 51988, "title": "XLA HLO profiling not supported on GPU", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nwhen enable hlo profile via export XLA_FLAGS=\"--xla_hlo_profile\", log says \r\n```\r\n2021-09-14 17:15:45.256221: E tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:893] --xla_hlo_profile for GPU is unsupported.\r\n```\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nMachine learning system researchers/engineers would benefit from this feature when profiling machine learning models.\r\n**Any Other info.**\r\n", "comments": ["Hi @jvishnuvardhan ,Could you please look at this issue ."]}, {"number": 51970, "title": "Setting OMP_DYNAMIC=true ruins training on TF-MKL", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): git\r\n- Python version: 3.9\r\n- Bazel version (if compiling from source): 4.2.1\r\n- GCC/Compiler version (if compiling from source): 11\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nSome background: I notices a big (several times) time difference between TF-Eigen and TF-MKL. With Eigen being faster. Trying to tune MKL to better performance I experimented with OMP environment variables.  While I was able to get much faster times with TF-MKL, I discovered that the convergence is ruined (the same code does not converge anymore). It seems that the offensive flag is `OMP_DYNAMIC=true ` \r\n\r\nJust to give some numbers, running with and without the variable set:\r\n```\r\nOMP_DYNAMIC=true ./demo.py\r\n\r\nEpoch 1/100\r\n   1020/Unknown - 37s 36ms/step - loss: nan - accuracy: 0.0928   \r\n```\r\n And without it:\r\n```\r\n./demo.py\r\nEpoch 1/100\r\n   1003/Unknown - 112s 111ms/step - loss: 12.1739 - accuracy: 0.9124  \r\n```\r\n\r\nNote how much faster the first run is (37ms vs 111ms). However, its accuracy and loss are much worse. At some point, the loss becomes NaN...\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nConvergence should not be affected by OMP flags.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nUnfortunately it is not easy to provide a short code that can reproduce the problem. I hope the developers would be able to run some internal tests. \r\n\r\n", "comments": ["@vpirogov can you please have a look at this since the issue is probably with MKL.\r\n\r\nThanks. ", "Some additional info (MKL_VERBOSE=1) that looks suspicious and crushes (why?!)\r\n\r\n```\r\nOMP_DYNAMIC=true MKL_VERBOSE=1 ./demo.py\r\n\r\nMKL_VERBOSE oneMKL 2021.0 Update 3 Product build 20210617 for Intel(R) 64 architecture Intel(R) Advanced Vector Extensions 2 (Intel(R) AVX2) enabled processors, Lnx 3.10GHz lp64 intel_thread\r\nMKL_VERBOSE SDOT(2,0x55c50c00b6e0,1,0x55c50c00b6e0,1) 1.24ms CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nOMP: Error #15: Initializing libiomp5.so, but found libomp.so already initialized.\r\nOMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.\r\nFatal Python error: Aborted\r\n```", "Preloading Intel's `libiomp5.so` (via `LD_PRELOAD`)  removes the error of two `omp` libraries loaded, but does not solve the main issue with poor convergence. \r\n\r\n", "@eli-osherovich, based on the messages it looks like your TF build has two OpenMP libraries linked, as the message indicates this may (and usually does) lead to nasty consequences. How do you build Tensorflow?\r\n\r\n+@agramesh1", "Thanks @vpirogov, @agramesh1 \r\n\r\nI use the standard building procedure:\r\n```\r\nbazel build --verbose_failures  --config=nogcp --config=nonccl  --config=noaws --config=nohdfs --config=mkl  -c opt --copt=-march=native --copt=\"-O3\" -s //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nIs it really two omp libraries? Since preloading Intel's `libiomp.so` does not solve the problem. I also tried to set KMP_DUPLICATE_LIB_OK, but it does not help. Namely, the command below still produces very poor convergence.\r\n\r\n```\r\nKMP_DUPLICATE_LIB_OK=TRUE OMP_DYNAMIC=true MKL_VERBOSE=1 ./demo.py\r\n```", "@vpirogov , @agramesh1 \r\n\r\nThe issue is reproduced with Intel's TF (installed in a clean conda environment):\r\n\r\n```\r\nconda create -n test -c intel  python=3.9 tensorflow\r\n```\r\nNo conflicting libraries.  This is definitely a bug. \r\n", "hi @eli-osherovich, is it possible to share your `demo.py` with us to properly investigate the issue?", "@preethivenkatesh  I can share with specific people from Intel/Google. \r\n", "@eli-osherovich I had share my email address a while ago. Please reach out to me via email for additional support", "@eli-osherovich To also get a full picture, could you please kindly share with @preethivenkatesh your `demo.py` script? We would like to create a similar environment and replicate the results. Thank you! ", "Thanks, @preethivenkatesh , unfortunately, after a long discussion, I cannot release our company's code.\r\n"]}, {"number": 51967, "title": "tf.nn.sigmoid_cross_entropy_with_logits should support broadcasting", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6\r\n- Python version: 3.9\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\n`tf.nn.sigmoid_cross_entropy_with_logits` does not support broadcasting. \r\nThis leads to some wrong behavior in certain cases (e.g., keras-team/keras#15247) \r\n**Describe the expected behavior**\r\nI would expect it to support boradcasting\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing):\r\nAdd broadcasting for `labels`\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\ny = tf.random.uniform((10, 1))\r\n\r\n# This one works\r\ntf.keras.losses.binary_crossentropy(0.5, y) \r\n\r\n# This one fails\r\ntf.keras.losses.binary_crossentropy(0.5, y, from_logits=True)\r\n```\r\n\r\n\r\n", "comments": ["Hi @eli-osherovich ! y_true and y_predict should have the same number of elements inside. Right! Issue does not replicate same no of elements .Attaching [gist](https://colab.research.google.com/gist/mohantym/83fe136b930375f4a4f7ff8279642017/github_51967.ipynb) for reference.  Thanks!", "@mohantym \r\nThis is exactly the point -- the requirement to have the same number of elements is unjustified in my view and should be removed.  Please have a look at the example where keras fails due to this requirement. ", "Ok! @sanatmpa1 ! could you please  look at this issue ?"]}, {"number": 51951, "title": "host to device transfer is not utilizing close to PCIE bandwidth", "body": "**System information**\r\n- TensorFlow version (use command below): 2.3.0, 2.5.0, 2.6.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: see colab on K80s, but this seems to happen on T4s, V100s, and A100s too\r\n\r\n**Describe the current behavior**\r\ndevice memory bandwidth is marketed as quite high, e.g.\r\n> pciBusID: 0000:00:07.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n\r\n**Describe the expected behavior**\r\ncannot get over a few GB/s transferring tensors from CPU -> GPU\r\n\r\n**Standalone code to reproduce the issue**\r\nOn a K80 in [colab](https://colab.research.google.com/drive/1RXQ79Phc5niKvtiq3FW403KtO36ok5zV#scrollTo=oP2iH3qXYOCn), we perform the following:\r\n\r\n1. create a dataset that generates `100` batches of `[1024, 10000]` float32s\r\n2. move tensor to GPU\r\n3. multiple by own transpose\r\n\r\nwe measure the time it takes to do this `100` batches, effectively moving \r\n `10000 float32 * 4 bytes/float32 * 1024 elements / step * 100 step * 1 GB / 1e9 bytes = 4.1 GB` of data from CPU to GPU.\r\n \r\nwe do this in both tensorflow and torch.\r\n\r\nI get around `8.7s (0.47 GB/s)` in tensorflow and `2.5s (1.65 GB/s)` in torch; torch is 3.5x faster no matter how I move my tensors to the device in tensorflow.  (I tried a variety of approaches as discussed in https://github.com/tensorflow/tensorflow/issues/43905)\r\n\r\nAccording to [this](https://www.techpowerup.com/gpu-specs/tesla-k80.c2616), K80s have PCIE 3.0x16 which look to be `15.75 GB/s` based on [wikipedia](https://en.wikipedia.org/wiki/PCI_Express)\r\n\r\n**Neither pytorch nor tensorflow seem to approach this speed**\r\n ", "comments": ["Hi @rllin , was able to  replicate issue in  [2.6](https://colab.research.google.com/gist/mohantym/ac446bbb9d6ad57b933109b4e433fb4f/htd_test.ipynb#scrollTo=QlgVm-dvT0M0) or  but seems to be resolving in Tf-nightly [2.7.0 dev ](https://colab.research.google.com/gist/mohantym/74f5e2d826986cd477319006fc73813a/htd_test.ipynb#scrollTo=oP2iH3qXYOCn)in colab ,  Thanks!", "@mohantym did you link the right colab?  (looks like it is bc i see 2.7.0 dev as the tf version)\r\n\r\nbut i see still ~8s which comes in at `0.47 GB/s`  same exact time as the 2.6\r\n\r\n![image](https://user-images.githubusercontent.com/1923997/133124804-f6b182b1-be34-4690-9784-5677f6388fe4.png)\r\n", "Yes ,The  above values match with my 2.7 colab gist , But I saw a difference in Torch timing in 2.7 (11.493 Torch 8.02  TF) unlike in TF 2.6 (3.13 Torch , 8.02 TF ). ", "Hi @sanatmpa1 ,Could you look into this issue! providing gist for reference in [2.5](https://colab.research.google.com/gist/mohantym/adb7d99b2ccd373fb0d9d0b81bf1295e/htd_test.ipynb#scrollTo=oP2iH3qXYOCn) ,[2.6](https://colab.research.google.com/gist/mohantym/ac446bbb9d6ad57b933109b4e433fb4f/htd_test.ipynb#scrollTo=QlgVm-dvT0M0) and [2.7](https://colab.research.google.com/gist/mohantym/74f5e2d826986cd477319006fc73813a/htd_test.ipynb#scrollTo=_Wp8cu72rz2t)", "@mohantym ah i see, good to know.  any thoughts so far?  perhaps torch 11 saw a similar regression with how htd is done that exists in tf?  because the most salient issue at hand is that neither of these throughputs approach hardware limits", "thinking more about this @mohantym @sanatmpa1 , it doesn't matter really that torch is slower, and i should have put them in separate notebooks.  bc i would guess there's just contention with gpu memory when used in the same kernel as tensorflow.\r\n\r\nthe important comparison is tensorflow against the PCIEx16 bandwidth, and the torch replication is just a verification that higher _is possible_\r\n\r\n- tf (all versions) takes `8s` to transfer `4.1GB`\r\n- torch (in its own [kernel/notebook](https://colab.research.google.com/drive/1rdoqs-tAQDDBvCp7CukvBKTInEk7Rx3O#scrollTo=Nx2MEFo7xioO)) takes `2.6s` to transfer `4.1GB`\r\n- PCIEx16 should give up to `16 GB/s`", "compare [against](https://colab.research.google.com/drive/1M5yFqlAgSY9KYpwuZf5AHyq0IH5EkTq9?usp=sharing) **NOT** using `tf.data` and just moving `tf.tensor`s manually from CPU -> GPU\r\n\r\n- create `100` random `tf.tensor`s of `[1024, 10000]` of `tf.float32`\r\n- move each of these one by one from CPU to GPU\r\n\r\nthis is also moving `4.1GB` total but only takes `0.62s`\r\n", "if I replace the `tf.random.uniform` call with something faster:\r\n\r\n```\r\nfeatures = np.random.uniform(size=[1024, 10000])\r\nds = tf.data.Dataset.from_tensors(features).repeat()\r\n```\r\n\r\ni get more reasonable speeds, `~1 s, 4 GB/s`\r\n\r\nthis means `tf.data` and manually moving `tf.tensor` have similar speeds, but this is still quite below the `16 GB/s` the hardware permits however", "I ran into similar issue and created [stackoverflow question](https://stackoverflow.com/questions/70081599/is-cpu-to-gpu-data-transfer-slow-in-tensorflow). The question includes simple benchmark code, which I hope provides fair comparison. In my test, CPU to GPU transfer is 2-8x faster in PyTorch (depending on the tensor size) with similar behavior with TF 2.4 and 2.6 (on RTX 2080ti and GTX 1080ti). Interestingly, Tensorflow slows down with large(r) tensors, PyTorch behaves well. This is frustratingly slow. \r\n\r\nTiming in TF:\r\n<pre>\r\ncode: tf.cast(x, dtype=tf.float32)[0, 0]\r\nBatch size 1; Batch time 0.0005; BPS 1851.8; FPS 1851.8; MB/S 364.1\r\nBatch size 2; Batch time 0.0004; BPS 2223.5; FPS 4447.1; MB/S 874.3\r\nBatch size 4; Batch time 0.0006; BPS 1555.2; FPS 6220.6; MB/S 1223.0\r\nBatch size 8; Batch time 0.0006; BPS 1784.8; FPS 14278.7; MB/S 2807.3\r\nBatch size 16; Batch time 0.0013; BPS 755.3; FPS 12084.7; MB/S 2376.0\r\nBatch size 32; Batch time 0.0023; BPS 443.8; FPS 14201.3; MB/S 2792.1\r\nBatch size 64; Batch time 0.0035; BPS 282.5; FPS 18079.5; MB/S 3554.6\r\nBatch size 128; Batch time 0.0061; BPS 163.4; FPS 20916.4; MB/S 4112.3\r\nBatch size 256; Batch time 0.0241; BPS 41.5; FPS 10623.0; MB/S 2088.6\r\nBatch size 512; Batch time 0.0460; BPS 21.7; FPS 11135.8; MB/S 2189.4\r\n</pre>\r\n\r\nTiming in pytorch:\r\n<pre>\r\nCode: torch.from_numpy(x).to(self.device).type(torch.float32)[0, 0].cpu()\r\nBatch size 1; Batch time 0.0001; BPS 10756.6; FPS 10756.6; MB/S 2114.8\r\nBatch size 1; Batch time 0.0001; BPS 12914.7; FPS 12914.7; MB/S 2539.1\r\nBatch size 2; Batch time 0.0001; BPS 10204.4; FPS 20408.7; MB/S 4012.5\r\nBatch size 4; Batch time 0.0002; BPS 5841.1; FPS 23364.3; MB/S 4593.6\r\nBatch size 8; Batch time 0.0003; BPS 3994.4; FPS 31955.4; MB/S 6282.7\r\nBatch size 16; Batch time 0.0004; BPS 2713.8; FPS 43421.3; MB/S 8537.0\r\nBatch size 32; Batch time 0.0007; BPS 1486.3; FPS 47562.7; MB/S 9351.2\r\nBatch size 64; Batch time 0.0015; BPS 679.3; FPS 43475.9; MB/S 8547.7\r\nBatch size 128; Batch time 0.0028; BPS 359.5; FPS 46017.7; MB/S 9047.5\r\nBatch size 256; Batch time 0.0054; BPS 185.2; FPS 47404.1; MB/S 9320.0\r\nBatch size 512; Batch time 0.0108; BPS 92.9; FPS 47564.5; MB/S 9351.6\r\n</pre>"]}, {"number": 51932, "title": "Build on FreeBSD fails with protbuf related error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : FreeBSD\r\n- TensorFlow installed from (source or binary): Source -\r\n- TensorFlow version: 2.6\r\n- Python version:3.8.11\r\n- Installed using virtualenv? pip? conda?: compile \r\n- Bazel version (if compiling from source): 0.29\r\n- GCC/Compiler version (if compiling from source): gcc10\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: n/a\r\n\r\n**Protobuf version -3.17.3**\r\n\r\n\r\n**Describe the problem**\r\nDuring compile, I got the following error - I googled but did not find any related comments that could help me move forward. I did find one comment on gentoo where it was suggested the json file in questions needs to updated. Could not gather what exactly needs to be updated to get the build going.\r\n\r\n[Gentoo issue link ](https://bugs.gentoo.org/800824)\r\n\r\nLet me know if any additional info you need to support triage.\r\n\r\n**ERROR: /usr/home/xxxx/Downloads/TensorFlow/FreeBSD-Tensorflow-master/science/py-tensorflow/work-py38/tensorflow-2.1.4/tensorflow/core/platform/BUILD:53:1: C++ compilation of rule '//tensorflow/core/platform:human_readable_json_impl' failed (Exit 1)\r\ntensorflow/core/platform/default/human_readable_json.cc:36:29: error: no member named 'error_message' in 'google::protobuf::util::status_internal::Status'\r\n   auto error_msg = status.error_message();\r\n                    ~~~~~~ ^\r\ntensorflow/core/platform/default/human_readable_json.cc:54:29: error: no member named 'error_message' in 'google::protobuf::util::status_internal::Status'\r\n   auto error_msg = status.error_message();\r\n                    ~~~~~~ ^\r\n2 errors generated.****\r\n\r\n\r\n\r\n**Provide the exact sequence of commands/steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 51930, "title": "multiple tflite_runtime interpreters perform bad when loaded and invoked at the same time", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: YOCTO Linux\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**: not installed (tflite runtime only)\r\n-   **TensorFlow version (use command below)**: not installed (tflite runtime only)\r\n-   **Python version**: 3.6\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI have a ARM64 board running YOCTO Linux equipped with a NPU and I have some quantized tflite models (one detection model and two classification models). I am testing a script that acquires images and makes inferences using the tflite_runtime module (with NNAPI delegate). Since the first inferences are slow, I introduced a warmup phase where I load the models and make an inference for each of them. However, this causes the models to produce wrong/unstable outputs during the normal functioning of the script. The same does not seem to happen when the interpreters are invoked at different times.\r\n\r\nThe detection model is a SSDMobileNetV2. The classification models are very simple custom architectures.\r\n\r\n### Source code / logs\r\n\r\n```\r\n# relevant import\r\nimport tflite_runtime.interpreter as tflite\r\n\r\nclass DetectionModel(object):\r\n\tdef __init__(self, path):\r\n\t\tself.interpreter = tflite.Interpreter(model_path=path)\r\n\t\tself.interpreter.allocate_tensors()\r\n\t\tself.input_details = self.interpreter.get_input_details()\r\n\t\tself.output_details = self.interpreter.get_output_details()\r\n\t\tself.input_shape = self.input_details[0]['shape']\r\n\r\n\tdef predict(self, input_img):\r\n\t\tR, C, _ = input_img.shape\r\n\t\timg = cv2.resize(input_img, (self.input_shape[2], self.input_shape[1]))\t\r\n\t\tif not self.input_details[0]['dtype'] == np.uint8:\r\n\t\t\timg = img.astype(np.float32)\r\n\t\t\timg = (img-128.0)/128.0\r\n\t\timg = np.expand_dims(img, 0)\r\n\t\tself.interpreter.set_tensor(self.input_details[0]['index'], img)\t\t\t\r\n\t\tself.interpreter.invoke()\t\t\t\r\n\t\tboxes = self.interpreter.get_tensor(self.output_details[0]['index'])\r\n\t\tclasses = self.interpreter.get_tensor(self.output_details[1]['index'])\r\n\t\tscores = self.interpreter.get_tensor(self.output_details[2]['index'])\r\n\r\n\t\treturn boxes, classes, scores\r\n\t\t\r\ndef initialize_classification_interpreter(path):\r\n\tinterpreter = tflite.Interpreter(model_path=path)\r\n\tinterpreter.allocate_tensors()\r\n\tinput_details = interpreter.get_input_details()\r\n\toutput_details = interpreter.get_output_details()\r\n\r\n\tinput_shape = input_details[0]['shape']\r\n\r\n\r\n\treturn interpreter, input_details, output_details\r\n\r\ndef classify(interpreter, input_details, output_details, img):\r\n\t\r\n\tinput_shape = input_details[0]['shape']\t\t\r\n\r\n\timg = (img/255.0).astype(np.float32)\r\n\r\n\timg = np.expand_dims(img, 0)\r\n\tinterpreter.set_tensor(input_details[0]['index'], img)\t\t\t\t\r\n\tinterpreter.invoke()\t\t\t\r\n\toutput = interpreter.get_tensor(output_details[0]['index'])\r\n\toutput = np.squeeze(output)\r\n\r\n\treturn output\r\n\r\ndetection_model = DetectionModel(detection_path)\r\ninterpreter1, input_details1, output_details1 = initialize_classification_interpreter(classification_path_1)\r\ninterpreter2, input_details2, output_details2 = initialize_classification_interpreter(classification_path_2)\r\n\r\n# warmup: this causes the issue\r\nimg = cv2.imread('test_img.jpg')\r\ndetection_model.predict(img)\r\nclassify(interpreter1, input_details1, output_details1, img)\r\n\r\ncap = cv2.VideoCapture(0)\r\n\r\nwhile cap.isOpened():\r\n    ret, img = cap.read()\r\n    # different conditions require different inferences\r\n    if condition_1:\r\n        result = detection_model.predict(img)\r\n    if condition_2:\r\n        result = classify(interpreter1, input_details1, output_details1, img)\r\n    if condition_3:\r\n        result = classify(interpreter2, input_details2, output_details2, img)\r\n\r\n    # other operations...\r\n\r\n\r\n```", "comments": []}, {"number": 51925, "title": "build pip package failed when setting --config=c++17_gcc", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 20.04\r\n- TensorFlow installed from source\r\n- TensorFlow version: 2.6.0\r\n- Python version: 3.8.10\r\n- Bazel version: 3.7.2\r\n- GCC/Compiler version: 9.3.0-17ubuntu1~20.04\r\n\r\n**Describe the problem**\r\nbuild failed by \r\n```\r\nERROR: /home/ubuntu/tensorflow/tensorflow/python/keras/api/BUILD:147:19: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow6StatusC1ENS_5error4CodeEN4absl12lts_2021032411string_viewEOSt6vectorINS_10StackFrameESaIS7_EE\r\n```\r\n**build commands**\r\n```\r\nbazel -s --config=c++17_gcc --copt=-march=native --config=opt -c opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nThe target `//tensorflow:libtensorflow_cc.so` build finished and worked, so I guess there are some bugs in building the python library not properly setting the c++17 flag. \r\n\r\n", "comments": ["Hi @jvishnuvardhan ,Could you please look at this issue!", "We are not yet ready for C++17 support", "Hi, @dolmens  Have you resolved this issue? I met same issue when trying to compile r1.15 version using C++17.", "> We are not yet ready for C++17 support\r\n\r\nHi, @mihaimaruseac are there any workarounds for this issue? Thanks in advance.", "Don't compile with C++17 for now"]}, {"number": 51913, "title": "Attention and AdditiveAttention Layers not well documented.", "body": "The attention layers (Attention and AdditiveAttention Layers) are not well documented. For example, I am able to pass a 4D input without issue when the current documentation say we should pass a 3D input.\r\n\r\nWhat does it mean when I pass 4D input? Please provide more examples.\r\n\r\nThank you!\r\n\r\nNektarios\r\n\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@nectario Could you please have a look at the [link1](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention),[ link2](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention), Please let us know if it helps?Thanks!", "@sushreebarsa These examples I know about. They do not mention of what happens when 4D tensors are passed. Attention mechanism is a difficult topic, and more examples would be appreciated.", "I really want to see more examples and information. I have a hard time understanding what a query is and value in regards to data that is not NLP related. ", "Hey, is this issue still available to be worked upon?? If yes, then pls assign it to me."]}, {"number": 51896, "title": "Tensorflow does not work with python -OO because of __doc__ usage", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): any\r\n- TensorFlow installed from (source or binary): pip wheel\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.9\r\n\r\n**Describe the current behavior**\r\n\r\nTensorflow crashes when being executed in a context where python was started with the [`-OO`](https://docs.python.org/3/using/cmdline.html#cmdoption-oo) flag. This happens because tensorflow is dynamically changing [`__doc__`](https://github.com/tensorflow/tensorflow/search?q=__doc__) variables during runtime and all of these variables become `None` when `python -OO` is used. An exception will be raised whenever a source file is loaded which tries to modify a `__doc__` variable, usually something along the lines of\r\n\r\n    TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\r\n\r\nwhich is raised by e.g. [/tensorflow/python/ops/array_ops.py#L452](https://github.com/tensorflow/tensorflow/blob/1f0c1d8ec778b029028a27a47a89acf6a93637ad/tensorflow/python/ops/array_ops.py#L452)\r\n\r\n    listdiff.__doc__ = gen_array_ops.list_diff.__doc__ + \"\\n\" + listdiff.__doc__\r\n\r\nbut there are lots of other places in the code where this can happen.\r\n\r\n**Describe the expected behavior**\r\n\r\nTensorflow should play nicely with `python -OO`. All code that modifies [`__doc__`](https://github.com/tensorflow/tensorflow/search?q=__doc__) variables during runtime would require `None` checks.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n`python -OO -c \"from tensorflow.python.ops import array_ops\"` fails with\r\n\r\n    TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\r\n\r\nBut `python -O -c \"from tensorflow.python.ops import array_ops\"` works fine, because [`-O`](https://docs.python.org/3/using/cmdline.html#cmdoption-o) does not discard docstrings. I would expect tensorflow to work with both `-O` and `-OO`.\r\n\r\n*edit*: updated code to reproduce", "comments": ["@klamann ,\r\nCan you please check this [SO](https://stackoverflow.com/questions/15036594/how-to-fix-typeerror-unsupported-operand-types-for-nonetype-and-str) link for the issue with similar error.It helps.Thanks!", "Also In order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.", "There is no dataset required and I provided a complete working example. You can try it in your terminal, it's just one call:\r\n\r\n    python -OO -m tensorflow.python.ops.array_ops\r\n\r\nIt will fail with the mentioned `TypeError`.\r\n\r\nThe linked SO post has nothing to do with this issue.", "@klamann ,\r\nCan you please share a colab gist with the issue reported, it helps to debug the issue from our side.Thanks", "Hi @tilakrayal, I don't want to register a google account and I'm not sure that you can change the launch parameters in a google colab document anyways. The steps to reproduce are really easy:\r\n\r\n1. open a terminal\r\n2. run `python -OO -m tensorflow.python.ops.array_ops`\r\n3. compare the output with `python -O -m tensorflow.python.ops.array_ops`\r\n\r\nStep 2 will raise an exception, step 3 will not.\r\nI'm not sure what else to say.", "@klamann,\r\n\r\nStep 2 produces the similar issue as you mentioned, but I am getting the below error for step 3. Please take a look at the [gist here](https://colab.research.google.com/gist/sanatmpa1/ed7db8bf974a570477ffe0281a8e2e0f/51896.ipynb).\r\n\r\n`KeyError: \"Registering two gradient with name 'FakeQuantWithMinMaxArgs'! (Previous registration was in register /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/registry.py:69)\"`", "you're right, I didn't notice this one. It seems to be unrelated, since `tensorflow.python.ops.array_ops` is not meant to be called as a module. We can work around this by just importing the module (without doing anything else).\r\n\r\nThis should work:\r\n\r\n    python -c \"from tensorflow.python.ops import array_ops\"\r\n    python -O -c \"from tensorflow.python.ops import array_ops\"\r\n\r\nand this fails with the mentioned `TypeError`:\r\n\r\n    python -OO -c \"from tensorflow.python.ops import array_ops\"\r\n", "@klamann, thanks for identifying this issue. We shall fix this for the sake of consistency, though it will be quite an effort to actually do it (as you have discovered there are many such occurrences).\r\n\r\nAre you still blocked by TF unable to work with -O0, or did you find a workaround? Did -O0 actually shrink the sizes of pyc/pyo files (the presumed benefit) for you?", "I changed my build pipeline to work around this issue, `-OO` was just the default setting I've been using, since I haven't run into a similar problem before. Still, the error was unexpected and kinda difficult to figure out.\r\n\r\nI didn't run any measurements regarding the size of the binaries or the memory footprint of the application, but due to UTF-16, strings aren't exactly lightweight in Python and since I don't need docstrings in memory during runtime, I prefer to get rid of them.", "Indeed it was surprising -- Tensorflow'ers probably shouldn't have assumed __doc__ is not None when the codebase was written years ago, but who would have known.. Be aware that  -OO can have other surprises too, like removing assertion statements. \r\n\r\nAnother possible avenue is that -OO could have set __doc__ to \"\" instead of None to maximize the compatibility with runtime docstring generation; and an option to keep assertion's \"side\" effects in place would be nice.\r\n\r\nI did some measurements on a typical self-contained training binary with -OO. The size went down by 30MB out of 1.4GB. We do tend to produce very large binaries in Google, so your ratios may be very different."]}, {"number": 51895, "title": "tf.nn.max_pool_with_argmax to support multiple dimensions (ND) ", "body": "**System information**\r\n- TensorFlow version (you are using): 2.x\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrent Behavior:\r\n\r\nCurrently, there is a function called [tf.nn.max_pool_with_argmax](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool_with_argmax). \r\n\r\nFeature Request:\r\n\r\nHowever, it would be great if the function supported several dimensions, not only 2D images.  [`tf.nn.max_pool`](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) for example supports any dimensionality. \r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nI think the easiest will be to implement a new method like `tf.nn.max_pool_with_argmax_nd` not to have compatibility issues. \r\nBecause it is a new method, this will also enable the fixing of the `tf.nn.max_pool_with_argmax` bug:\r\n\r\n> This is a bug, but fixing it is difficult to do in a safe backwards compatible way, especially due to flattening.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nThis function is useful to, for example, implement ND [MaxUnpooling](https://www.oreilly.com/library/view/hands-on-convolutional-neural/9781789130331/6476c4d5-19f2-455f-8590-c6f99504b7a5.xhtml).\r\n\r\nIn particular, it will help me implement the [feature request](https://github.com/NEGU93/cvnn/issues/10) asked on my repository.\r\n\r\n**Any Other info.**\r\n\r\nCorrect formatting of [this](https://github.com/tensorflow/tensorflow/issues/51787) feature request.\r\n", "comments": ["Hi! @Saduf2019 ,Could you please look at this feature request?"]}, {"number": 51877, "title": "tensorflow/python/kernel_tests/metrics_test.py fail in assertAlmostEqual comparison; E TypeError: type numpy.ndarray doesn't define _round_ method", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.6.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nE TypeError: type numpy.ndarray doesn't define _round_ method\r\n\r\nAbove failure can potentially come from assert all close comparision when expected scalar value is compared against the actual numpy output from the device via self.eval.\r\n\r\nIssue may not be seen on some devices if actual output is identical to expected value and so round call in assert all close implementation will not be called.\r\n\r\n**Describe the expected behavior**\r\n One quick place where we can quickly reproduce the issue is in tensorflow/python/kernel_tests/metrics_test.py::PrecisionRecallThresholdsTest::testWithMultipleUpdates\r\n\r\nOverwrite expected prec as an example  - 0.9393305437365417\r\nOverwirte prec Actual output as a  numpy array with value [0.93933064]\r\n\r\nself.assertAlmostEqual(expected_prec, self.evaluate(prec), 2)\r\n\r\nBased upon the values, assert almost equal call should pass but \"E TypeError: type numpy.ndarray doesn't define _round_ method\" will be thrown.\r\n\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing): \r\nTo fix it, we can first compare the shapes of actual output as [1] and then in assertAlmostEqual call, change as below:\r\nself.assertAlmostEqual(expected_prec, self.evaluate(prec)[0], 2).\r\n\r\nThere are other test files too which have this issue whenever assertAllClose will be called and will fail if actual output is passed as numpy array and is not identical to expected ouput.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nReproducible by just running the test itself.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nNA", "comments": ["@puneeshkhanna ,\r\nCan you please refer this SO [link](https://stackoverflow.com/questions/41319292/error-typeerror-type-numpy-ndarray-doesnt-define-round-method) with similar error.It helps.Thanks!", "Usage of assertAllClose is done correctly in tensorflow/python/feature_column/sequence_feature_column_integration_test.py. \r\nSomeone realized there this issue.\r\nself.assertEqual(list(ctx_result['float_ctx'].shape), [1])\r\nself.assertAlmostEqual(ctx_result['float_ctx'][0], 123.6, places=1)\r\n\r\nThis is how it needs to be changed wherever applicable and not pass numpy array to the call.\r\n", "@puneeshkhanna ,\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet you are trying to execute.Thanks", "```\r\nimport numpy as np\r\nimport unittest\r\nclass test(unittest.TestCase):\r\n    def test_assert_1(self):\r\n        actual = np.array([0.93933064])\r\n        expected = 0.9393305437365417\r\n        self.assertAlmostEqual(expected, actual, 2)\r\n\r\n    def test_assert_2(self):\r\n        actual = np.array([0.93933064])\r\n        expected = 0.9393305437365417\r\n        self.assertEqual(list(actual.shape), [1])\r\n        self.assertAlmostEqual(expected, actual[0], 2)\r\n\r\nf = test()\r\nf.test_assert_2()\r\nf.test_assert_1()\r\n```\r\n\r\nPlease execute above code in jupyter or colab or any python environment. Test cases which use assertAlmostEqual need to redesigned as shown in test_assert_2(). Right now they are written as test_assert_1().", "@puneeshkhanna ,\r\nI ran the code shared and face a different error, please find the gist [here](https://colab.research.google.com/gist/tilakrayal/5688c716f71604e9b9815c0028ff46fe/untitled74.ipynb) and share all dependencies to replicate the issue or share a colab gist with the reported error.", "```\r\nimport numpy as np\r\nimport unittest\r\nclass test(unittest.TestCase):\r\n    def test_assert_1(self):\r\n        actual = np.array([0.93933064])\r\n        expected = 0.9393305437365417\r\n        self.assertAlmostEqual(expected, actual, 2)\r\n\r\n    def test_assert_2(self):\r\n        actual = np.array([0.93933064])\r\n        expected = 0.9393305437365417\r\n        self.assertEqual(list(actual.shape), [1])\r\n        self.assertAlmostEqual(expected, actual[0], 2)\r\n\r\nf = test()\r\nf.test_assert_2()\r\nf.test_assert_1()\r\n```", "Please try above in Colab. I ran it at my end. Sorry there was a typo in my earlier code.", "Hope the issue is clear now.", "@sanatmpa1 ,\r\nI was able to reproduce the issue in tf v2.5, v2.6 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a58301ba706b73c67166dd4686761e33/unti51877.ipynb).", "Thanks for the thorough investigation.\r\n\r\n `ndarray.__round__` is necessary to support unittest.TestCase.assertAlmostEqual() with single item ndarray input. See the example below.\r\n\r\nThe lack of `__round__` in ndarray is discussed in numpy/numpy#6248.\r\nAlthough I note that even with `__round__` added, assertAlmostEqual for arrays with more than 1 item is still not clearly defined (due to the `== 0` comparison on ndarray in the same line).\r\n\r\n```\r\nimport numpy as np\r\nclass A(np.ndarray):\r\n\r\n  def __round__(self, places):\r\n    return np.round_(self, places)\r\n\r\nimport unittest\r\nclass test(unittest.TestCase):\r\n    def test_assert_1(self):\r\n        actual = np.array([0.93933064]).view(type=A)\r\n        expected = 0.9393305437365417\r\n        self.assertAlmostEqual(expected, actual, 2)\r\n    def test_assert_2(self):\r\n        actual = np.array(0.93933064).view(type=A)\r\n        expected = 0.9393305437365417\r\n        self.assertAlmostEqual(expected, actual, 2)\r\n    def test_assert_3(self):\r\n        actual = np.array(0.93933064).view(type=np.ndarray)\r\n        expected = 0.9393305437365417\r\n        self.assertAlmostEqual(expected, actual, 2)\r\n\r\nf = test()\r\nf.test_assert_1() \r\nf.test_assert_2()\r\nf.test_assert_3()\r\n```\r\nAs there are no current test failures, a reasonable approach fixing calls to assertAlmostEqual on a case-by-case basis as failures are encountered.\r\n\r\nAlso, I think `TensorflowTestCase.assertAllclose` checks both the shape and the value, which appears to be the desired behavior you described? If so, maybe we should replace / override assertAlmostEqual with assertAllclose in TensorflowTestCase?"]}, {"number": 51871, "title": "Not able to lower tf.sets.intersection to HLO", "body": "Tensorflow version 2.6\r\n\r\nMy python code snippet is like:\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.eager import context\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.platform import test\r\nfrom tensorflow.python.framework import config\r\nimport tensorflow.compat.v1 as tf\r\nconfig.enable_mlir_bridge()\r\ntf.config.experimental.enable_mlir_bridge()\r\n\r\nclass CustomModule(tf.Module):\r\n\r\ndef init(self):\r\nsuper(CustomModule, self).init()\r\nself.condition = tf.Variable(np.array([[True, False, False],[False, True, False],[True, True, True]]), dtype = tf.bool)\r\nself.x = tf.Variable(np.array([[1, 2, 3],[4, 5, 6],[7, 8, 9]]), dtype = tf.int32)\r\nself.y =tf.Variable(np.array([[11, 12, 13],[14, 15, 16],[17, 18, 19]]), dtype = tf.int32)\r\n\r\n@tf.function\r\ndef call(self, x):\r\nr = tf.where(self.condition, self.x, self.y)\r\nm= tf.where(self.condition, self.x, self.y)\r\n\r\nc=tf.sets.intersection(tf.expand_dims(r, 0),tf.expand_dims(m, 0))\r\n\r\nreturn c\r\n\r\nmodule = CustomModule()\r\n\r\nmodule_with_signature_path = os.path.join(\"/data/aruna/tf_ops\", \u2018sets_intersection\u2019)\r\ncall = module.call.get_concrete_function(tf.TensorSpec(shape=(), dtype=tf.int32))\r\nsignatures = {\u2018predict\u2019: call}\r\ntf.saved_model.save(module, module_with_signature_path, signatures=call)\r\nprint(\u2018Saving model\u2026\u2019)\r\n\r\nif name == \u2018main\u2019:\r\ntest.main()\r\nI ran this python code and got saved_model.pb.\r\nThen I used following commands:\r\ntensorflow/compiler/mlir/tf-mlir-translate --savedmodel-objectgraph-to-mlir --tf-savedmodel-exported-names=predict -tf-enable-shape-inference-on-import=true $PWD -o sample.mlir\r\ntensorflow/compiler/mlir/tf-opt --tf-executor-to-functional-conversion --tf-shape-inference -xla-legalize-tf --print-ir-before-all sample.mlir\r\n\r\nTF dialect looks like:\r\n// -----// IR Dump Before LegalizeTF //----- //\r\nbuiltin.func private @__inference___call___750(%arg0: tensor {tf._user_specified_name = \u201cx\u201d}, %arg1: tensor<!tf_type.resource>, %arg2: tensor<!tf_type.resource>, %arg3: tensor<!tf_type.resource>) \u2192 (tensor<?x3xi64>, tensor<?xi32>, tensor<3xi64>) attributes {tf._construction_context = \u201ckEagerRuntime\u201d, tf._input_shapes = [#tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {\r\n%cst = \u201ctf.Const\u201d() {device = \u201c\u201d, value = dense<0> : tensor} : () \u2192 tensor\r\n%cst_0 = \u201ctf.Const\u201d() {device = \u201c\u201d, value = dense<0> : tensor}2021-09-08 09:56:50.579733: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n: () \u2192 tensor\r\n%0 = \u201ctf.ReadVariableOp\u201d(%arg2) {device = \u201c\u201d} : (tensor<!tf_type.resource>) \u2192 tensor<3x3xi32>\r\n%1 = \u201ctf.ReadVariableOp\u201d(%arg2) {device = \u201c\u201d} : (tensor<!tf_type.resource>) \u2192 tensor<3x3xi32>\r\n%2 = \u201ctf.ReadVariableOp\u201d(%arg3) {device = \u201c\u201d} : (tensor<!tf_type.resource>) \u2192 tensor<3x3xi32>\r\n%3 = \u201ctf.ReadVariableOp\u201d(%arg3) {device = \u201c\u201d} : (tensor<!tf_type.resource>) \u2192 tensor<3x3xi32>\r\n%4 = \u201ctf.ReadVariableOp\u201d(%arg1) {device = \u201c\u201d} : (tensor<!tf_type.resource>) \u2192 tensor<3x3xi1>\r\n%5 = \u201ctf.Select\u201d(%4, %0, %2) {device = \u201c\u201d} : (tensor<3x3xi1>, tensor<3x3xi32>, tensor<3x3xi32>) \u2192 tensor<3x3xi32>\r\n%6 = \u201ctf.ExpandDims\u201d(%5, %cst) {device = \u201c\u201d} : (tensor<3x3xi32>, tensor) \u2192 tensor<1x3x3xi32>\r\n%7 = \u201ctf.ReadVariableOp\u201d(%arg1) {device = \u201c\u201d} : (tensor<!tf_type.resource>) \u2192 tensor<3x3xi1>\r\n\u201ctf.NoOp\u201d() {_acd_function_control_output = true, device = \u201c\u201d} : () \u2192 ()\r\n%8 = \u201ctf.Select\u201d(%7, %1, %3) {device = \u201c\u201d} : (tensor<3x3xi1>, tensor<3x3xi32>, tensor<3x3xi32>) \u2192 tensor<3x3xi32>\r\n%9 = \u201ctf.ExpandDims\u201d(%8, %cst_0) {device = \u201c\u201d} : (tensor<3x3xi32>, tensor) \u2192 tensor<1x3x3xi32>\r\n%10:3 = \u201ctf.DenseToDenseSetOperation\u201d(%6, %9) {T = i32, device = \u201c\u201d, set_operation = \u201cintersection\u201d, validate_indices = true} : (tensor<1x3x3xi32>, tensor<1x3x3xi32>) \u2192 (tensor<?x3xi64>, tensor<?xi32>, tensor<3xi64>)\r\n%11 = \u201ctf.Identity\u201d(%10#0) {device = \u201c\u201d} : (tensor<?x3xi64>) \u2192 tensor<?x3xi64>\r\n%12 = \u201ctf.Identity\u201d(%10#1) {device = \u201c\u201d} : (tensor<?xi32>) \u2192 tensor<?xi32>\r\n%13 = \u201ctf.Identity\u201d(%10#2) {device = \u201c\u201d} : (tensor<3xi64>) \u2192 tensor<3xi64>\r\nreturn %11, %12, %13 : tensor<?x3xi64>, tensor<?xi32>, tensor<3xi64>\r\n}\r\n\r\nError is:\r\nsample.mlir:5:3: error: The following operations cannot be legalized: tf.DenseToDenseSetOperation (count: 1); tf.NoOp (count: 1); tf.ReadVariableOp (count: 6). These legalization failure(s) may be due to missing TF to HLO lowerings and/or unsupported attributes, etc.\r\nbuiltin.func private @__inference___call___340(%arg0: tensor {tf._user_specified_name = \u201cx\u201d}, %arg1: tensor<!tf_type.resource>, %arg2: tensor<!tf_type.resource>, %arg3: tensor<!tf_type.resource>) \u2192 (tensor<?x3xi64>, tensor<?xi32>, tensor<3xi64>) attributes {tf._construction_context = \u201ckEagerRuntime\u201d, tf._input_shapes = [#tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {\r\n^\r\nsample.mlir:5:3: error: Emitting more detail about one op that failed to legalize\u2026\r\nbuiltin.func private @__inference___call___340(%arg0: tensor {tf._user_specified_name = \u201cx\u201d}, %arg1: tensor<!tf_type.resource>, %arg2: tensor<!tf_type.resource>, %arg3: tensor<!tf_type.resource>) \u2192 (tensor<?x3xi64>, tensor<?xi32>, tensor<3xi64>) attributes {tf._construction_context = \u201ckEagerRuntime\u201d, tf._input_shapes = [#tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {\r\n^\r\nsample.mlir:20:61: error: \u2018tf.DenseToDenseSetOperation\u2019 op is not legalizable\r\n%outputs_23:3, %control_24 = tf_executor.island wraps \u201ctf.DenseToDenseSetOperation\u201d(%outputs_14, %outputs_21) {T = i32, device = \u201c\u201d, set_operation = \u201cintersection\u201d, validate_indices = true} : (tensor<1x3x3xi32>, tensor<1x3x3xi32>) \u2192 (tensor<?x3xi64>, tensor<?xi32>, tensor<3xi64>)\r\n", "comments": ["@ArunaKote,\r\n\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@sanatmpa1 ,\r\nI have attached python file\r\n[tf_sets_intersection.zip](https://github.com/tensorflow/tensorflow/files/7149267/tf_sets_intersection.zip)\r\n", "@jvishnuvardhan \r\nEnvironment details:\r\n1.tensorflow version 2.7.0-dev20210831\r\n\r\n2.tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt --version\r\nLLVM (http://llvm.org/):\r\nLLVM version 14.0.0git\r\nOptimized build.\r\nDefault target: x86_64-unknown-linux-gnu\r\nHost CPU: cascadelake\r\n3.tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate --version\r\nLLVM (http://llvm.org/):\r\nLLVM version 14.0.0git\r\nOptimized build.\r\nDefault target: x86_64-unknown-linux-gnu\r\nHost CPU: cascadelake", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}]