[{"number": 12215, "title": "CTC loss with dynamic length", "body": "This is a very specific questions, I'm afraid nobody on stackoverflow will ever answer this. I will copy the text from there, the original question can be found there: \r\nhttps://stackoverflow.com/questions/45568266/tensorflow-ctc-loss-ctc-merge-repeated-parameter\r\n\r\nAnd I'm not 100% sure if this behaviour is wanted or a bug (I think its the former one but I'm really not sure).\r\n\r\n\r\nI'm using Tensorflow 1.0 and its CTC loss [1]. When training, I sometimes get the \"No valid path found.\" warning (which harms learning). It is not due to a high learning rate as sometimes reported by other Tensorflow users.\r\n\r\nAfter analyzing it a bit, I found the pattern that causes this warning:\r\n - feeding an input sequence into the ctc_loss with length seqLen\r\n - feeding a label with labelLen characters\r\n - label has numRepeatedChars repeated chars in it, where I count \"ab\" as 0, \"aa\" as 1, \"aaa\" as 2 and so on \r\n - warning occurs, when: seqLen - labelLen < numRepeatedChars\r\n\r\n\r\nThree examples:\r\n\r\n - Ex.1: label=\"abb\", len(label)=3, len(inputSequence)=3 => (3-3=0)<1 is true --> warning\r\n - Ex.2: label=\"abb\", len(label)=3, len(inputSequence)=4 => (4-3=1)<1 is false --> no warning\r\n - Ex.3: label=\"bbb\", len(label)=3, len(inputSequence)=4 => (4-3=1)<2 is true --> warning\r\n\r\nWhen I now set the ctc_loss parameter ctc_merge_repeated=False, then the warning disappears.\r\n\r\nThree questions:\r\n\r\n - Q1: why is there a warning when repeated chars occur? I thought, as long as the input sequence is not shorter than the target labelling, there is no problem. And when repeated chars are merged in the label, then it gets even shorter, therefore the condition that the input sequence is not shorter still holds.\r\n - Q2: why does the ctc_loss in its default settings produce this warning? Repeated chars are common in the domains CTCs are used such as handwritten text recognition (HTR)\r\n - Q3: what settings should I use when doing HTR? Of course labels can have repeated chars. Therefore ctc_merge_repeated=False would make sense. Any suggestions?\r\n\r\nPython program to reproduce warning:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef createGraph():\r\n    tinputs=tf.placeholder(tf.float32, [100, 1, 65]) # max 100 time steps, 1 batch element, 64+1 classes\r\n    tlabels=tf.SparseTensor(tf.placeholder(tf.int64, shape=[None,2]) , tf.placeholder(tf.int32,[None]), tf.placeholder(tf.int64,[2])) # labels\r\n    tseqLen=tf.placeholder(tf.int32, [None]) # list of sequence length in batch\r\n    tloss=tf.reduce_mean(tf.nn.ctc_loss(labels=tlabels, inputs=tinputs, sequence_length=tseqLen, ctc_merge_repeated=True)) # ctc loss\r\n    return (tinputs, tlabels, tseqLen, tloss)\r\n\r\ndef getNextBatch(nc): # next batch with given number of chars in label\r\n    indices=[[0,i] for i in range(nc)]\r\n    values=[i%65 for i in range(nc)]\r\n    values[0]=0\r\n    values[1]=0 # TODO: (un)comment this to trigger warning\r\n    shape=[1, nc]\r\n    labels=tf.SparseTensorValue(indices, values, shape)\r\n    seqLen=[nc]\r\n    inputs=np.random.rand(100, 1, 65)\r\n    return (labels, inputs, seqLen) \r\n\r\n\r\n(tinputs, tlabels, tseqLen, tloss)=createGraph()\r\n\r\nsess=tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nnc=3 # number of chars in label\r\nprint('next batch with 1 element has label len='+str(nc))\r\n(labels, inputs, seqLen)=getNextBatch(nc)\r\nres=sess.run([tloss], { tlabels: labels, tinputs:inputs, tseqLen:seqLen } )\r\n```\r\n\r\nThis is the C++ Tensorflow code [2] where the warning comes from:\r\n```\r\n// It is possible that no valid path is found if the activations for the\r\n// targets are zero.\r\nif (log_p_z_x == kLogZero) {\r\n    LOG(WARNING) << \"No valid path found.\";\r\n    dy_b = y;\r\n    return;\r\n}\r\n```\r\n\r\n[1] https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/ctc_loss\r\n[2] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/ctc/ctc_loss_calculator.cc\r\n\r\n\r\n-----\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: costum\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.0.0-rc2-15-g47bba63-dirty, 1.0.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: [problem also on CPU]\r\n- **GPU model and memory**: [problem also on CPU]\r\n- **Exact command to reproduce**: see code above\r\n", "comments": ["Hi,\r\n\r\nThe warning message you're seeing, as mentioned in the comment in the code, can appear also in cases when ctc_merge_repeated=False.\r\n\r\nIt appears whenever it's impossible to determine a valid path for the target labeling based on the network activation through the forward/backward variables (which is where the ctc_merge_repeated is applied - note: the code has some references to the Alex Graves' thesis, which I find useful going through).\r\n\r\nQ1: why is there a warning when repeated chars occur? I thought, as long as the input sequence is not shorter than the target labelling, there is no problem. And when repeated chars are merged in the label, then it gets even shorter, therefore the condition that the input sequence is not shorter still holds.\r\n\r\nAgain, this warning changes as training progress (assuming not all the samples in your training set raise this problem), since the network activations change. The warning states that there couldn't be a valid path found based on the forward/backward computation, but not that that the sequence is invalid.\r\n\r\nNote that ctc_merge_repeated is different from collapsing repeated labels, which is done through preprocess_collapse_repeated. The former controls how the CTC loss penalizes internally repeated labels ('a' 'a' 'b' becomes 'blank' 'a' 'blank' 'a' 'blank' 'b' 'blank' for the forward/backward computation), while the latter actually modifies the ground truth. ('a' 'a' 'b' first becomes 'a' 'b' and then 'blank' 'a' 'blank' 'b' 'blank' for the forward/backward computation).\r\n\r\nQ2: why does the ctc_loss in its default settings produce this warning? Repeated chars are common in the domains CTCs are used such as handwritten text recognition (HTR)\r\n\r\nThis should be the default CTC behavior, where collapsing labels for the loss/gradient computation allows focusing on learning the label transitions instead of where the label appears.\r\n\r\nQ3: what settings should I use when doing HTR? Of course labels can have repeated chars. Therefore ctc_merge_repeated=False would make sense. Any suggestions?\r\n\r\nHaving ctc_merge_repeated=True does allow learning repeated characters. Please see the detailed comments in the documentation, specifically about the behavior of the different combination of flags \"Regarding the arguments...\" \r\n\r\n(https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/connectionist_temporal_classification__ctc_)", "Hi Victor, thanks for your answer.\r\n\r\nThe warning always appears on those samples on which the condition (see first post) holds. Even after training.\r\nIn the test program, the input is a random input, but even when repeating the code for multiple random inputs, the warning always occurs.\r\nThe only way to toggle the warning is by changing the target labelling ... so it appears to me that it has something to to with this target labelling!?\r\n\r\nI further decreased the tensor sizes of the test program to have a closer look at a simple example - code see below.\r\nThe input has shape T=5 (timesteps) and C=6 (classes).\r\nThe target labelling is [0, 0, 2]. The probability for a path p which yields the labelling [0,0,2] is not zero.\r\nE.g. the path 0->blank->0->2->blank (with blank=C-1=5) yields the target labelling. And so do some more paths. \r\nTherefore, the probability of seeing the target labelling given the input should not be zero.\r\n\r\n**Output of test program:**\r\ninput:  \r\n[[[ 0.59413256  0.23575521  0.95215654  0.46271613  0.08859916  0.43706737]]\r\n\r\n [[ 0.11860785  0.11229196  0.01522826  0.96020464  0.34278502  0.27091574]]\r\n\r\n [[ 0.29061551  0.27465308  0.94130146  0.12166673  0.86577444  0.37988687]]\r\n\r\n [[ 0.77218394  0.07898683  0.85200237  0.4295197   0.76858367  0.72602145]]\r\n\r\n [[ 0.09071068  0.43796645  0.6265344   0.48837476  0.24057374  0.03459447]]]\r\n \r\nlabel:  SparseTensorValue(indices=[[0, 0], [0, 1], [0, 2]], values=[0, 0, 2], dense_shape=[1, 3])\r\n\r\nloss:  [inf]\r\n\r\n\r\n**Test program:**\r\n\timport tensorflow as tf\r\n\timport numpy as np\r\n\r\n\tbatchSize=1\r\n\tnumClasses=6\r\n\tnumTimesteps=5\r\n\r\n\tdef createGraph():\r\n\t\ttinputs=tf.placeholder(tf.float32, [numTimesteps, batchSize, numClasses])\r\n\t\ttlabels=tf.SparseTensor(tf.placeholder(tf.int64, shape=[None,2]) , tf.placeholder(tf.int32,[None]), tf.placeholder(tf.int64,[2])) # labels\r\n\t\ttseqLen=tf.placeholder(tf.int32, [None]) # list of sequence length in batch\r\n\t\ttloss=tf.reduce_mean(tf.nn.ctc_loss(labels=tlabels, inputs=tinputs, sequence_length=tseqLen, ctc_merge_repeated=True)) # ctc loss\r\n\t\treturn (tinputs, tlabels, tseqLen, tloss)\r\n\r\n\tdef getNextBatch(nc): # next batch with given number of chars in label\r\n\t\tindices=[[0,i] for i in range(nc)]\r\n\t\tvalues=[i%3 for i in range(nc)]\r\n\t\tvalues[0]=0\r\n\t\tvalues[1]=0 # TODO: (un)comment this to trigger warning\r\n\t\tshape=[1, nc]\r\n\t\tlabels=tf.SparseTensorValue(indices, values, shape)\r\n\t\tseqLen=[nc]\r\n\t\tinputs=np.random.rand(numTimesteps, batchSize, numClasses)\r\n\t\treturn (labels, inputs, seqLen) \r\n\r\n\r\n\t(tinputs, tlabels, tseqLen, tloss)=createGraph()\r\n\r\n\tsess=tf.Session()\r\n\tsess.run(tf.global_variables_initializer())\r\n\r\n\tnc=3 # number of chars in label\r\n\tprint('next batch with 1 element has label len='+str(nc))\r\n\t(labels, inputs, seqLen)=getNextBatch(nc)\r\n\tresLoss=sess.run([tloss], { tlabels: labels, tinputs:inputs, tseqLen:seqLen } )\r\n\tprint('input: ', inputs)\r\n\tprint('label: ', labels)\r\n\tprint('loss: ', resLoss)", "ok, got it, that's not a bug, that's just how CTC works:\r\nlet's take an example for which the warning occurs: length of input sequence is 2, labelling is \"aa\", also with length 2.\r\nNow the shortest path which yields \"aa\" is a->blank->a (length 3). But for a labelling \"ab\", the shortest path is a->b (length 2).\r\nThat shows why for repeated labels like in \"aa\" the input sequence must be longer. Its simply the way that repeated labels get encoded in the CTC (by inserting blanks).", "@vcarbune \r\nNote that ctc_merge_repeated is different from collapsing repeated labels, which is done through preprocess_collapse_repeated. The former controls how the CTC loss penalizes internally repeated labels ('a' 'a' 'b' becomes 'blank' 'a' 'blank' 'a' 'blank' 'b' 'blank' for the forward/backward computation), while the latter actually modifies the ground truth. ('a' 'a' 'b' first becomes 'a' 'b' and then 'blank' 'a' 'blank' 'b' 'blank' for the forward/backward computation).\r\n\r\nI think ctc insert blank between labels as default behaviour, what does ctc_merge_repeated do? What if I set it to false ?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @tatianashp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatianashp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatianashp: It has been 180 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 12214, "title": "tensorflow in python ", "body": "Hi,\r\nI am trying to use tensorflow in ipython, Linux and I got the following error\r\n==================================================================\r\nh@h:~$ source activate tensorflow  \r\n(tensorflow) hx@hx:~$ python \r\nPython 2.7.13 |Continuum Analytics, Inc.| (default, Dec 20 2016, 23:09:15) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\nAnaconda is brought to you by Continuum Analytics.\r\nPlease check out: http://continuum.io/thanks and https://anaconda.org\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *\r\n  File \"/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/framework_lib.py\", line 100, in <module>\r\n    from tensorflow.python.framework.subscribe import subscribe\r\n  File \"/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/subscribe.py\", line 26, in <module>\r\n    from tensorflow.python.ops import variables\r\n  File \"/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 26, in <module>\r\n    from tensorflow.python.ops import control_flow_ops\r\n  File \"/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 70, in <module>\r\n    from tensorflow.python.ops import tensor_array_ops\r\n  File \"/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 33, in <module>\r\n    from tensorflow.python.util import tf_should_use\r\n  File \"/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 28, in <module>\r\n    from backports import weakref  # pylint: disable=g-bad-import-order\r\nImportError: cannot import name weakref\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\nImportError: cannot import name pywrap_tensorflow\r\n>>> \r\n====================================================================\r\nand not in tensorflow like this:\r\nh@h:~$ python\r\nPython 2.7.13 |Anaconda custom (64-bit)| (default, Dec 20 2016, 23:09:15) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\nAnaconda is brought to you by Continuum Analytics.\r\nPlease check out: http://continuum.io/thanks and https://anaconda.org\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: No module named tensorflow\r\n>>> \r\n=====================================================================\r\nThanks!\r\n\r\n\r\n", "comments": []}, {"number": 12213, "title": "Including scatter_nd in android ops crashes.", "body": "System information\r\n \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\nYes (as described below)\r\n \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\nUbuntu 16 64 bit\r\n \r\n- TensorFlow installed from (source or binary): \r\nSource\r\n \r\n- TensorFlow version : \r\n1.2.1 (Perhaps using a newer version may fix the issue)\r\n \r\n- Bazel version (if compiling from source): \r\nBuild label: 0.5.3\r\n \r\n- Exact command to reproduce:\r\n\r\n`bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --verbose_failures`\r\n\r\n\r\nI'm trying to port a encoder-decoder net model to android and I was able to add all the required operations to the core_ops/ extended_ops and compile it.\r\nThe last method that needs to be included in the scatter_nd Operation for unpooling (I'm using a custom implementation which uses scatter_nd from here https://github.com/tensorflow/tensorflow/issues/2169).\r\n\r\nI included the \r\n\r\n`\"scatter_nd_op.h\" in filegroup: \"mobile_srcs\" `\r\n\r\n```\r\n\"scatter_nd_op.h\",\r\n\"scatter_nd_op.cc\", in filegroup: \"android_core_ops\"\r\n```\r\n\r\nI also commented out the line \r\n\r\n`\"scatter_nd_op*\", in exclude[] filegroup : \"android_all_ops\"`\r\n\r\nand tried to compile.\r\n\r\nError Message:\r\n\r\n```\r\ntensorflow/core/kernels/scatter_nd_op.cc:373: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 5>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 5u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'\r\ntensorflow/core/kernels/scatter_nd_op.cc:372: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 4>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 4u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'\r\ntensorflow/core/kernels/scatter_nd_op.cc:371: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 3>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 3u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'\r\ntensorflow/core/kernels/scatter_nd_op.cc:370: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 2>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 2u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'\r\ntensorflow/core/kernels/scatter_nd_op.cc:369: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 1>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 1u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'\r\ntensorflow/core/kernels/scatter_nd_op.cc:255: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 5>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 5u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'\r\ntensorflow/core/kernels/scatter_nd_op.cc:254: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 4>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 4u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'\r\ntensorflow/core/kernels/scatter_nd_op.cc:253: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 3>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 3u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'\r\ntensorflow/core/kernels/scatter_nd_op.cc:252: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 2>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 2u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'\r\ntensorflow/core/kernels/scatter_nd_op.cc:251: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 1>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 1u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'\r\n```\r\n\r\nDo I need to add any more dependencies to the BUILD file. \r\n\r\nOr are there any alternatives to scatter_nd which can be used in Android ?\r\n\r\nThanks", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@anandcu3 Have you fixed this? I'm having the same issue and it will be greatly appreciated if you share how you've tackled this."]}, {"number": 12212, "title": " Remove useless RecvTensorResponse allocation in GDR", "body": "See [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc#L347) for your reference.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 12211, "title": "Please develop a 32-bit version,thank you very much\uff01\uff01\uff01\uff01\uff01\uff01", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Closing as a duplicate of #12203."]}, {"number": 12210, "title": "tensorflow/contrib/session_bundle/example/export_half_plus_two.py fails with ValueError: invalid option", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.2.1-0-gb4957ff', '1.2.1')\r\n- **Python version**: Python 2.7.9\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: Not used\r\n- **Exact command to reproduce**: python tensorflow/contrib/session_bundle/example/export_half_plus_two.py\r\n\r\n\r\n### Describe the problem\r\nThe given script is used to generate testdata for savedmodel. \r\n`python tensorflow/contrib/session_bundle/example/export_half_plus_two.py` Fails with below error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tensorflow/contrib/session_bundle/example/export_half_plus_two.py\", line 159, in <module>\r\n    help=\"If true, write v2 checkpoint files.\"\r\n  File \"/usr/lib/python2.7/argparse.py\", line 1280, in add_argument\r\n    kwargs = self._get_optional_kwargs(*args, **kwargs)\r\n  File \"/usr/lib/python2.7/argparse.py\", line 1410, in _get_optional_kwargs\r\n    raise ValueError(msg % tup)\r\nValueError: invalid option string 'bool': must start with a character '-'\r\n```\r\n\r\n### Source code / logs\r\ntensorflow/contrib/session_bundle/example/export_half_plus_two.py\r\n\r\n\r\n\r\n\r\n", "comments": ["Below code patch resolves this issue:\r\n```\r\ndiff --git a/tensorflow/contrib/session_bundle/example/export_half_plus_two.py b/tensorflow/contrib/session_bundle/example/export_half_plus_two.py\r\nindex 4a56509..a822e9f 100644\r\n--- a/tensorflow/contrib/session_bundle/example/export_half_plus_two.py\r\n+++ b/tensorflow/contrib/session_bundle/example/export_half_plus_two.py\r\n@@ -152,7 +152,7 @@ if __name__ == \"__main__\":\r\n   )\r\n   parser.add_argument(\r\n       \"--use_checkpoint_v2\",\r\n-      \"bool\",\r\n+      \"--bool\",\r\n       nargs=\"?\",\r\n       const=True,\r\n       default=False,\r\n```\r\n\r\nShould I submit a PR for this change?\r\n", "/CC @vrv", "https://github.com/tensorflow/tensorflow/blob/17bc79f4eb0608f5fd82fe08bb50688773a41bc7/tensorflow/python/tools/optimize_for_inference.py#L126\r\n\r\nis probably a better form of using 'booleans', along with\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/17bc79f4eb0608f5fd82fe08bb50688773a41bc7/tensorflow/python/tools/optimize_for_inference.py#L105\r\n\r\nWould you like to send a PR?", "Oh it does look the same pattern is being employed.  Let me try to reproduce first.", "It's just missing a named argument, I can send a PR.", "usage: extract_embeddings.py [-h] -i DATASET/XUAN/ -e OUTPUT/EMBEDDINGS.PICKLE\r\n                             -d FACE_DETECTION_MODEL -m\r\n                             OPENFACE_NN4.SMALL2.V1.T7 [-c CONFIDENCE]\r\nextract_embeddings.py: error: the following arguments are required: -i/--dataset/Xuan/, -e/--output/embeddings.pickle, -d/--face_detection_model, -m/--openface_nn4.small2.v1.t7\r\nAn exception has occurred, use %tb to see the full traceback.\r\n\r\nSystemExit: 2\r\n"]}, {"number": 12209, "title": "Make LayerNormBasicLSTMCell compatible with datatypes other than float32", "body": "`LayerNormBasicLSTMCell` only supported float32 so far. With this patch other datatypes such as float64 are available too. The datatype doesn't have to be specified explicitly but it is deducted from the input data as it already happens for example for BasicLSTMCell. ", "comments": ["Can one of the admins verify this patch?", "@ebrevdo any cycles for this?", "No one else should call _norm from outside this module... Does anyone?\n\nOn Sep 17, 2017 9:40 AM, \"Maximilian Bachl\" <notifications@github.com>\nwrote:\n\n> *@muxamilian* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/contrib/rnn/python/ops/rnn_cell.py\n> <https://github.com/tensorflow/tensorflow/pull/12209#discussion_r139315193>\n> :\n>\n> > @@ -1303,24 +1303,25 @@ def state_size(self):\n>    def output_size(self):\n>      return self._num_units\n>\n> -  def _norm(self, inp, scope):\n> +  def _norm(self, inp, scope, dtype=dtypes.float32):\n>\n> No, you don't need it but in case some legacy code calls _norm directly \u2013\n> for whatever reason \u2013 the default argument prevents this code from breaking.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12209#discussion_r139315193>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim38RuWEV-z0Dx8SGawkg-QP8qf89ks5sjUtxgaJpZM4O0ckU>\n> .\n>\n", "Not that I know of anyone doing that... I thought the default value wouldn't hurt anyone either. ", "sounds fine.\n\nOn Sun, Sep 17, 2017 at 11:37 AM, Maximilian Bachl <notifications@github.com\n> wrote:\n\n> Not that I know of anyone doing that... I thought the default value\n> wouldn't hurt anyone either.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12209#issuecomment-330069160>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9rK7nWl59QSKJ9uEEc1NkNgl3kdks5sjWblgaJpZM4O0ckU>\n> .\n>\n", "Any updates on this?", "Jenkins, test this please.", "Kokoro isn't reporting status atm."]}, {"number": 12208, "title": "Document changes in notMNIST data sets", "body": "### Describe the problem\r\nThe notMNIST data set used in Assignment 1 and maybe elsewhere in this repo repeatedly changed size over the last 12 months. Last year, in issue #4693, a size of about 50 MB is reported for notMNIST_large.tar.gz. Now this file is 236 MB. People doing the assignment are comparing their classifier accuracy to results published by previous assignment takers and some are talking about publishing research using this data set. If the data set is modified unknowingly to the users, observed accuracy figures are not comparable and can mislead people to wrong conclusions.\r\n\r\n@yaroslavvb The files is also bigger now on your personal website. However, the time stamps of the .atr.gz and all files inside are still from 2011.\r\n\r\n### Source code / logs\r\nThe changes in size can also be tracked in changes to the calls of maybe_download() in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb\r\n\r\n### Suggestions\r\n\r\n- The .tar.gz files should contain a readme and changelog with version information.\r\n- The filename should include the version number.\r\n- References to the data set in the assignments should be changed from \"the notMNIST dataset\" to \"version 1.23 of the notMNIST dataset\".\r\n- Result tables produced by code should quote the version of the data set.\r\n", "comments": ["As far as I know, there has been no updates to the dataset whatsoever. I would not recommend using this dataset to publish, it adds nothing compared to MNIST.", "Thanks. Diving deeper into the git log for `1_notmnist.ipynb` and reading more threads mentioning issues with the size of the data set downloads, it seems indeed more plausible that the files did not change on the server and the deviant sizes were related to unexpected interactions between http server and client with the presence or absence of certain http headers and/or proxy servers."]}, {"number": 12207, "title": "Fix typos", "body": "This PR fixes some typos.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 12206, "title": "BUG: Estimator.eval() runs feature_engineering_fn more than once.", "body": "The PR is aimed to fix #12205.\r\n\r\n### What changes were proposed in this pull request?\r\n\r\neliminate the second call of `feature_engineering_fn` in `evaluate` method.\r\n\r\n\r\n### How was this patch tested?\r\n\r\n+ [x] add an unit test.", "comments": ["Can one of the admins verify this patch?", "By the way, @ispirmustafa . I'm a newbie. If I want to run only one unit test, say the test case I added, could you tell me how to do it? Thanks.", "I have found it: `bazel test -c opt //tensorflow/contrib/learn:estimators_test`, and it seems OK.\r\n\r\nCould you let @tensorflow-jenkins  test the PR? Thanks.", "Hi, @ispirmustafa . The PR has been modified as suggested, and could you review it? Thanks.", "Jenkins, test this please", "Thanks, @ispirmustafa and @jhseu ."]}, {"number": 12205, "title": "BUG: TypeError in DNNClassifier.eval() when using same name for feature in feature_engineering_fn", "body": "### Describe the problem\r\n\r\nIf we use the  same key to replace a feature, tensorflow might throw TypeError when evaluating:\r\n\r\neg:\r\n```python\r\ndef feature_engineering_fn(features, label):\r\n  features[\"x\"] = some_func(features[\"x\"])\r\n```\r\n\r\nWhen `features` is `dict`, it is a mutable object. Hence the bug is caused by `evaluate` method which runs `feature_engineering_fn` again, see [code](https://github.com/facaiy/tensorflow/blob/c7b80d51da4fb6d51ea54a0bdf2601afa379d60c/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L1165).\r\n\r\nI'll open a PR later.\r\n", "comments": []}, {"number": 12204, "title": "No registered 'MirrorPad' OpKernel for XLA_CPU_JIT devices", "body": "It seems that mirror padding is not supported with XLA. The message is triggered when running tfcompile. Any ideas on when this (or other ops like #11905) might be available?", "comments": ["nvm #11890"]}, {"number": 12203, "title": "Why not develop  32-bit\uff1fThank you very much!", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Googling yielded some people building the project themselves on 32bit CPU Only implementations https://stackoverflow.com/questions/33634525/tensorflow-on-32-bit-linux", "because it's completely nonsense to use 32-bit machine for development of deep learning", "Unfortunately we don't have the bandwidth to support every platform and architecture. I'll mark this community support in case someone who can support 32-bit builds wants to contribute.", "@ybxgood \r\nMoving this to closed status with respect to [this comment](https://github.com/tensorflow/tensorflow/issues/32315#issuecomment-532778229)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/12203\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/12203\">No</a>\n"]}, {"number": 12202, "title": "Using keras built-in models", "body": "#### Version\r\nv1.2.0-rc2-21-g12f033d\r\n\r\n#### Problem\r\nThe Keras built-in models in `tf.contrib.keras.applications.*` cannot be used as a subgraph in TF.\r\n\r\n#### Example\r\nhttps://stackoverflow.com/questions/45585546/error-with-tf-contrib-keras-tf-placeholder\r\n\r\n#### Cause\r\nCalling `keras.applications.InceptionV3(weights='imagenet')(input_tensor)` is supposed to load pre-trained weights only for related variables, but it initializes the entire TF graph.\r\n", "comments": ["@fchollet can you take a look?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "That's correct, all graph variables are initialized in the Keras session when you run Keras models (or load weights in Keras models), that's intended behavior. Keras simply couldn't work without this behavior (it's impossible to rely on users to explicitly initialize variables that they create themselves before using them together with Keras models)."]}, {"number": 12201, "title": "Adding support for s390x for changing GetCycleCounterFrequency", "body": "Closing #12180 and re-creating the PR based on review comments.", "comments": ["@namrata-ibm, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @sandipmgiri and @caisq to be potential reviewers.", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 12200, "title": "fix build issue for --config=sycl", "body": "", "comments": ["Can one of the admins verify this patch?", "@guoyejun, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @dave-andersen to be potential reviewers.", "@guoyejun Please rebase.", "i rebased to master this morning, fixed the build issue and created this PR, looks that the issue is fixed in the new master. will close this PR."]}, {"number": 12199, "title": "Branch 164929133", "body": "", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 12198, "title": "Branch 164929133", "body": "", "comments": []}, {"number": 12197, "title": "Branch 164929133", "body": "", "comments": []}, {"number": 12196, "title": "Add regression tests  and fix some minor bugs", "body": "This pull request adds some additional unit tests for the `add_n`  and `accumulate_n` operators. I'm adding these tests as preparation for working on https://github.com/tensorflow/tensorflow/issues/10607 . Specifically, I added tests of the argument validation code in `accumulate_n` and a test of gradient computation for `add_n`.\r\n\r\nThis pull request also fixes two minor bugs that I encountered in the process of adding the unit tests:\r\n* `accumulate_n` ignores its `tensor_dtype` argument when the operator has only one input. I've added additional error-handling logic to `accumulate_n` that rejects values of `tensor_dtype` that are different from the input type.\r\n* The  test`UnaryOpTest.testComplex64Basic` was failing on my Mac because the `acosh` and `asinh` functions for single-precision complex floating point are only accurate to 6 decimal places. Under the assumption that 6 significant figures is good enough, I increased the threshold that the test cases use when comparing gradients. \r\n\r\nNote that if 6 significant figures is *not* good enough for `acosh` and `asinh`, a more involved fix will be necessary for the second problem.", "comments": ["Can one of the admins verify this patch?", "@frreiss, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @benoitsteiner and @keveman to be potential reviewers.", "@tensorflow-jenkins test this please", "@frreiss please fix the Python linter errors.\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/5801/consoleFull"]}, {"number": 12195, "title": "Fix typos", "body": "This PR fixes some typos: `explictly`, `initialised`, `paritally`, `unecessary`, `substitue`, and `Paramterized`.", "comments": ["Can one of the admins verify this patch?", "@taehoonlee, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @protoget and @vrv to be potential reviewers.", "@taehoonlee Thanks for the fixes!"]}, {"number": 12194, "title": "Incorrect Command Line in Image Training Tutorial", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.6\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.2.0-rc2-21-g12f033d 1.2.0\r\n- **Python version**: 3.5.3\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**: \r\n```\r\nbazel build tensorflow/examples/label_image:label_image && \\\r\nbazel-bin/tensorflow/examples/label_image/label_image \\\r\n--graph=/tmp/output_graph.pb --labels=/tmp/output_labels.txt \\\r\n--output_layer=final_result \\\r\n--image=$HOME/flower_photos/daisy/21652746_cc379e0eea_m.jpg\r\n```\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nThe [tutorial for image retraining](https://www.tensorflow.org/tutorials/image_retraining) has an incorrect command line, which prevents the label_image classifier from running.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nIf you run the command as is, you get the following output:\r\n```E tensorflow/examples/label_image/main.cc:349] Running model failed: Not found: FeedInputs: unable to find feed output input```\r\n\r\n### Solution\r\nPlease see [this StackOverflow post](https://stackoverflow.com/questions/43022516/tensorflow-inception-feedinputs-unable-to-find-feed-output-input).\r\n\r\nThe solution is to add the option `--input_layer=Mul` to the command line. The new command line should read:\r\n```\r\nbazel build tensorflow/examples/label_image:label_image && \\\r\nbazel-bin/tensorflow/examples/label_image/label_image \\\r\n--graph=/tmp/output_graph.pb --labels=/tmp/output_labels.txt \\\r\n--input_layer=Mul \\\r\n--output_layer=final_result \\\r\n--image=$HOME/flower_photos/daisy/21652746_cc379e0eea_m.jpg\r\n```", "comments": ["I am able to reproduce the issue. This issue seems similar to #2883.\r\n\r\n@petewarden, any thoughts?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "This has been fixed in master."]}, {"number": 12193, "title": "Error Building project tf_core_framework", "body": "26>------ Build started: Project: tf_core_framework, Configuration: Debug x64 ------\r\n26>  Generating __force_rebuild\r\n26>\r\n26>  Generating E:/AIMLDL/TensorFlow/tensorflow/tensorflow/core/util/version_info.cc\r\n26>  The system cannot find the path specified.\r\n26>C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 3.\r\n\r\nWhen I try to build the tf_label_image_example project, tf_core_framework errors out with code 3.\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 12192, "title": "Remove casting of int64 for reverse_sequence", "body": "This fix remove unneeded cast of int64 for reverse_sequence:\r\n```\r\nlengths = math_ops.to_int64(lengths)\r\n```\r\nas int32 has already been enabled for reverse_sequence.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?"]}, {"number": 12191, "title": "Fix bugs of rounding in converting image from float to int", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@ychfan I think it's a potentially dangerous change that breaks existing behavior. It is probably better to add a new option for this functionality. Feel free to amend the PR if you think that's worth the trouble.", "@ychfan ping", "@drpngx but the current implementation is not working as it supposed, in the \"else\" branch, image is converted from float(between 0 to 1) to unsigned int(for example 0 to 255), then dtype.max is 255. The origin code uses dtype.max + 0.5, makes the scale non-uniform.", "@ychfan Right, I see. Can you add a test for that?", "@drpngx I added a very simple test.", "Jenkins, test this please.", "OK, thanks. We'll need to be careful when merging into our internal repo since this is a bugfix that might break existing behavior.\r\n\r\n/CC: @martinwicke FYI", "@gunan @yifeif kokoro is wedged? On this PR and #12504?", "Looks like we need to ping to a newer version of https://github.com/tensorflow/tensorflow/blob/master/WORKSPACE#L3?\r\n@jart ", "I'm not sure I'd describe the old behavior a problem or a bug. This just appears to be round instead of floor. I'm assuming there's a performance advantage to using `+ 0.5` instead of `tf.round`?\r\n\r\n@yifeif What's wrong with the Closure Rules dependency? How can I help?", "Thanks for the comment @jart. Sorry should have provided more info. The kokoro build is failing with `/tmpfs/tmp/bazel/external/io_bazel_rules_closure/closure/private/defs.bzl:27:16: The `set` constructor for depsets is deprecated and will be removed. Please use the `depset` constructor instead. You can temporarily enable the deprecated `set` constructor by passing the flag --incompatible_disallow_set_constructor=false`. \r\n\r\nBut I just realized this is due to the \"io_bazel_rules_closure\" dependency in the PR branch is too old.\r\n\r\n@ychfan do you mind do a rebase?", "Thank you for figuring this out!\n\nOn Tue, Oct 31, 2017, 9:36 PM Yifei Feng <notifications@github.com> wrote:\n\n> Thanks for the comment @jart <https://github.com/jart>. Sorry should have\n> provided more info. The kokoro build is failing with /tmpfs/tmp/bazel/external/io_bazel_rules_closure/closure/private/defs.bzl:27:16:\n> Thesetconstructor for depsets is deprecated and will be removed. Please\n> use thedepsetconstructor instead. You can temporarily enable the\n> deprecatedsetconstructor by passing the flag\n> --incompatible_disallow_set_constructor=false.\n>\n> But I just realized this is due to the \"io_bazel_rules_closure\" dependency\n> in the PR branch is too old.\n>\n> @ychfan <https://github.com/ychfan> do you mind do a rebase?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12191#issuecomment-340975946>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_Sba4FE0vTT4tfRaYL_7lOWi-TFrAHks5sx_VSgaJpZM4O0DiT>\n> .\n>\n", "@jart `+ 0.5` can do the round things, but it has to be applied to the image data not scaler", "rebased", "Jenkins, test this please.", "@andrewharp is the `split` problem at `master`?", "Looks like these failures are directly related to this cl:\r\nhttps://source.cloud.google.com/results/invocation/543774f5-bfbb-4362-806e-63e4e3b31004/targets/%2F%2Ftensorflow%2Fpython:image_ops_test?page=log\r\n\r\nCould you fix them?", "The test case is for `rgb_to_grayscale` and it calls `convert_image_dtype` to convert image from `float32` to `uint8`. I guess the test case may be wrong ", "I am still not convinced this actually is a needed bugfix:\r\n\r\nThe old behavior is\r\n```\r\nnew = old * (max+0.5)\r\n```\r\nwith the assumption that values are between 0 and 1 before. The desired behavior is a clean floor, i.e., if `old == n/max`, then `new = n`. The old way does this, I think, while the new could produce either `new = n` or `new = n+1` depending on how the numerics dice fall.\r\n\r\nBut it's been a bit, I may have messed up too.\r\n", "I will close this PR for now; I don't believe this is a bug. @aselle, if you want to change my mind, please review. \r\n\r\nI would be happy to consider a PR which clarifies the definition of the intensity representation using integers, since it appears to be confusing.", "Can one of the admins verify this patch?"]}, {"number": 12190, "title": "Create CI build script for Raspberry Pi", "body": "This integrates the work by @ebrevdo to create a build script for the Raspberry Pi with the CI system. This script creates a Python wheel for TensorFlow on the Pi, and is a step towards creating automatic redistributable binaries for the platform.\r\n\r\nI had to update oauth_client.cc for a missing header on this platform, and work around the eigen updating issue tracked in #9697.", "comments": ["@petewarden, thanks for your PR! By analyzing the history of the files in this pull request, we identified @rinugun, @tensorflower-gardener and @hawkinsp to be potential reviewers."]}, {"number": 12189, "title": "CI script for building Raspberry Pi wheels", "body": "This CL expands on @ebrevdo's original PR adding Pi support by integrating it into the CI build scripts. This is a step towards nightly builds of Pi wheels and distribution of prebuilt binaries.\r\n\r\nIssue #9697 is still blocking us from switching to the latest Eigen version, which is required for ARM compilation, so the script makes a local change to work around this for now. There's also one header added to oauth_client.cc, because it's required by boringssl on this platform.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "Fixed commit emails.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 12188, "title": "learner.run Couldn't find trained model", "body": "I have built my custom experiment like this:\r\n\r\n```\r\ndef experiment_fn(run_config, hparams):\r\n    hooks = [\r\n        tf.train.CheckpointSaverHook ( \r\n            checkpoint_dir = run_config.model_dir,\r\n            save_steps = 5,\r\n        ), \r\n        \r\n        tf.train.SummarySaverHook ( \r\n            save_steps = 5, \r\n            output_dir = run_config.model_dir, \r\n            scaffold= tf.train.Scaffold(),\r\n            summary_op=tf.summary.merge_all()\r\n        )\r\n    ]\r\n    \r\n    return learn.Experiment (\r\n        estimator = learn.Estimator (\r\n            model_fn = model_fn, \r\n            config = run_config,\r\n            params = hparams\r\n        ),\r\n        train_input_fn = lambda: input_fun(train_set),\r\n        eval_input_fn = lambda: input_fun(eval_set),\r\n        eval_metrics = model_eval_metrics(),\r\n        train_steps = 20,\r\n        train_monitors = hooks,\r\n        eval_hooks = hooks,\r\n        min_eval_frequency = 1,\r\n        export_strategies = saved_model_export_utils.make_export_strategy (\r\n            serving_input_fn = serving_input_fn,\r\n        )\r\n    )\r\n```\r\n\r\n```\r\nlearn_runner.run (\r\n    experiment_fn = experiment_fn,\r\n    run_config = tf.contrib.learn.RunConfig (\r\n        model_dir = output_dir,\r\n    ),\r\n    schedule = \"train_and_evaluate\",\r\n    hparams =  tf.contrib.training.HParams (\r\n        ...some parameters...\r\n    )\r\n)\r\n```\r\n\r\nWhen I run it, I have files created in \\model with names like 'model.ckpt-6.meta' plus the monitor shows the following results:\r\n\r\n> Monitors are deprecated. Please use tf.train.SessionRunHook.\r\n> INFO:tensorflow:step = 1, loss = 0.0437659\r\n> INFO:tensorflow:Saving checkpoints for 1 into model\\model.ckpt.\r\n> INFO:tensorflow:Saving checkpoints for 6 into model\\model.ckpt.\r\n> INFO:tensorflow:Saving checkpoints for 11 into model\\model.ckpt.\r\n> INFO:tensorflow:Saving checkpoints for 16 into model\\model.ckpt.\r\n> INFO:tensorflow:Saving checkpoints for 20 into model\\model.ckpt.\r\n> INFO:tensorflow:Loss for final step: 0.0438498.\r\n\r\n> ---------------------------------------------------------------------------\r\n> NotFittedError                            Traceback (most recent call last)\r\n> <ipython-input-3-4a9282b8a5f5> in <module>()\r\n>     177         learning_rate = 0.01,\r\n>     178         decay_rate = 0.96,\r\n> --> 179         decay_steps = 10\r\n>     180     )\r\n>     181 )\r\n> \r\n> C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_runner.py in run(experiment_fn, output_dir, schedule, run_config, hparams)\r\n>     208   schedule = schedule or _get_default_schedule(run_config)\r\n>     209 \r\n> --> 210   return _execute_schedule(experiment, schedule)\r\n>     211 \r\n>     212 \r\n> \r\n> C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_runner.py in _execute_schedule(experiment, schedule)\r\n>      45     logging.error('Allowed values for this experiment are: %s', valid_tasks)\r\n>      46     raise TypeError('Schedule references non-callable member %s' % schedule)\r\n> ---> 47   return task()\r\n>      48 \r\n>      49 \r\n> \r\n> C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\experiment.py in train_and_evaluate(self)\r\n>     499                                       metrics=self._eval_metrics,\r\n>     500                                       name=eval_dir_suffix,\r\n> --> 501                                       hooks=self._eval_hooks)\r\n>     502     export_results = self._maybe_export(eval_result)\r\n>     503     return eval_result, export_results\r\n> \r\n> C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\experiment.py in _call_evaluate(self, _sentinel, input_fn, steps, metrics, name, checkpoint_path, hooks)\r\n>     686                                       name=name,\r\n>     687                                       checkpoint_path=checkpoint_path,\r\n> --> 688                                       hooks=hooks)\r\n>     689 \r\n>     690 \r\n> \r\n> C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py in new_func(*args, **kwargs)\r\n>     287             'in a future version' if date is None else ('after %s' % date),\r\n>     288             instructions)\r\n> --> 289       return func(*args, **kwargs)\r\n>     290     return tf_decorator.make_decorator(func, new_func, 'deprecated',\r\n>     291                                        _add_deprecated_arg_notice_to_docstring(\r\n> \r\n> C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py in evaluate(self, x, y, input_fn, feed_fn, batch_size, steps, metrics, name, checkpoint_path, hooks, log_progress)\r\n>     541         checkpoint_path=checkpoint_path,\r\n>     542         hooks=hooks,\r\n> --> 543         log_progress=log_progress)\r\n>     544 \r\n>     545     if eval_results is not None:\r\n> \r\n> C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py in _evaluate_model(self, input_fn, steps, feed_fn, metrics, name, checkpoint_path, hooks, log_progress)\r\n>     814       if not latest_path:\r\n>     815         raise NotFittedError(\"Couldn't find trained model at %s.\"\r\n> --> 816                              % self._model_dir)\r\n>     817       checkpoint_path = latest_path\r\n>     818 \r\n> \r\n> NotFittedError: Couldn't find trained model at \\model.\r\n\r\nWhy it cannot find the model ?\r\n\r\nThanks for assistance!", "comments": ["Did you copy and/or move the directory containing the checkpoints and the checkpoint file? If so, you need to edit model/checkpoint and correct the absolute paths listed.", "I'm running the learning.run with default parameters. So it runs train_eval. No prob with the train part but when it tried to eval the error arises . ", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12187, "title": "No builds for Linux Py3.6-gpu TF1.3 RC2?", "body": "Using Python 3.6 in Anaconda on Ubuntu 16.04\r\n\r\n```\r\nsource activate py36env \r\npip install --upgrade \\\r\n> https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.3.0rc2-cp35-cp35m-manylinux1_x86_64.whl\r\ntensorflow_gpu-1.3.0rc2-cp35-cp35m-manylinux1_x86_64.whl is not a supported wheel on this platform. \r\n```\r\nI checked the build stream and don't see a Linux PY3.6 GPU being built?\r\n\r\nSeems to me that since I have TF 1.2 installed (it had the PY3.6GPU support that new RC's should also include builds for Py3.6/GPU or a lot of people just won't build a separt downgraded test environment for the RC?\r\n\r\nOr is the intention to drop support?? (I hope not!)\r\n", "comments": ["https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.3.0rc2-cp36-cp36m-linux_x86_64.whl\r\n", "Awesome Thanks for the quick response!", "grr now I just have to upgrade my cdnn, for libcudnn.so.6 which will probably break the older TF env but I think that is fixable with a symlink..\r\n", "Hi @dartdog, has your issue been resolved?"]}, {"number": 12186, "title": "How to run the inceptionv3 on the Knights Landing Intel Xeon Phi?", "body": "I have built tensorflow from scratch for the Knights Landing and it seems to give only 4-5images/sec. Can someone give a procedure to run these standard benchmarks?", "comments": ["@cryptox31 : Did you try https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture", "@VishnuMadhu Yes, I did. The instructions are only for building tensorflow and not how to run the inceptionv3 benchmarks. Do you know what are the command line parameters for tf_cnn_benchmark on the xeon phi?", "@cryptox31 Can you specify what benchmarks you are running? Thank you.", "@reedwm I am running the inception_v3 model (tf_cnn_benchmarks.py)", "@tfboyd can you comment?", "This may be the same person as on tensorflow-discuss, which is great and I am pasting the same response.\r\n\r\nA few things to help get you started.\r\n\r\n- You will want to compile with the MKL to get the most our ot Xeon Phi\r\n- You will need to set the MKL env flags\r\n- You will need to set the num_inter_threads and num_intra_threads flags on the tf_cnn_benchmark.\r\n- For MKL you want to use data_format=NCHW (which will not work if MKL is not compiled in).\r\n\r\nI will try to followup with you again but if I do not here is an example:\r\n\r\n- [tf_cnn_benchmarks.py ](https://github.com/tfboyd/benchmarks/blob/mkl/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks_MKL.py) with some code changes to be slightly more optimized for the MKL  (I have PR to make it easier to set MKL env vars in our script but have not had time to get it approved and this link is better than nothing)\r\n- This file sets the ENV vars for common models: https://github.com/tfboyd/benchmarks/blob/mkl/scripts/tf_cnn_benchmarks/setenvs.py\r\n\r\nTo compile MKL from head I believe you now you only need to do `blaze --config=mkl`.  We have been working to simplify it but I believe that now works.  Previously (1.2.1) you had to answer a few questions in ./configure.  If you want to just try a pre-compiled version here is a link to a binary I created.  I make ZERO promises in regard to the binary.  \r\n\r\nhttps://storage.googleapis.com/tf-performance/tf_binary/tensorflow-1.2.0.tag.12f033d.MKL_NOGPU-cp27-cp27mu-linux_x86_64.whl\r\n\r\nThat should get you started.  Intel published some [benchmark numbers](https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture) that you can use to see if you are on the right track. \r\n\r\nSorry for the sloppy response, I felt giving you the information in a non-professional structure was better than not responding.  Good luck and ping this thread if you have questions.", "Hey @tfboyd,\r\nThank you for commenting on both my posts. I was unaware of the tf_cnn_benchmark_MKL.py script maybe because the master branch is 1 commit away from the MKL branch. \r\nI would like to know \r\n\r\n> **what value can I set for the num_inter_threads and num_intra_threads flags on the tf_cnn_benchmark_MKL.py script depending on the Knight Landing with 288 logical cores?**\r\n\r\n \r\nI am aware that intel published the article with the updated wheel a few days ago, I think the article should point to the MKL branch on github, so that people know that the new version of the script is out.\r\n\r\nThank you @reedwm for pointing him to this post.\r\n\r\nRegards,\r\nKrishna", "The MKL script is in my personal repo.  I am trying to make time to add some of the Flags to the main benchmark code.  To answer your question in a few ways:\r\n\r\nOn the [Intel blog](https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture) they list out the settings that they used.  I have only tested on Xeon Broadwell as I do not have access to Phi, but between their article and my testing I am confident I can get you in the right direction:\r\n-  For Xeon Phi you will want to set inter_op = the number of Physical CPUs (likely 1 or 2) and set intra_op = either the number physical or logical cores in your case 144 or 288.  Their testing showed the best option depends on the model.  You can also use this [document](https://github.com/tfboyd/benchmarks/blob/mkl/scripts/tf_cnn_benchmarks/setenvs.py) for reference.  In their scrit 'knl' referse to knights landing and they tested with 68 physical cores.  \r\n\r\nYou want to set the intra_op = to whatever you use for the OMP_NUM_THREADS. \r\n\r\nSo for example if you want to test with tf_cnn_benchmarks.py and test resnet50 you would run this command assuming you are using 2 Xeon Phis with 72 physical cores each for a total of 288 logical cores and 144 physical.  I am typing this from memory so you may need to fix some typos:\r\n\r\n`KMP_BLOCKTIME=0 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 OMP_NUM_THREADS=288 python tf_cnn_benchmarks.py --model=resnet50 --batch_size=128 --num_inter_threads=2 --num_intra_threads=288 --device=cpu --variable_update=parameter_server --local_parameter_device=cpu`\r\n\r\nAll of those fields may not be needed but I like to be specific and avoid the defaults so when I share the command line there is less ambiguity.  \r\n\r\nIf you are using real data Intel suggested OMP_NUM_THREADS= 50 for 68 physical cores.  In your setup you could try 144 (total physical cores) or something slightly smaller to match what Intel was testing and use 120 or so and see what happens.  What I like to do is setup a script and run the test for 100 steps 5 times for a variety of options to get a feel for the pros and cons of each settings.  \r\n\r\nLet me know your results and if you get something similar to Intel's.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "\r\nMy Xeon Phi's config:\r\n```\r\nCPU(s):                288\r\nOn-line CPU(s) list:   0-287\r\nThread(s) per core:    4\r\nCore(s) per socket:    72\r\nSocket(s):             1\r\nNUMA node(s):          2\r\n```\r\n\r\nI ran this command:\r\n\r\n`KMP_BLOCKTIME=2 KMP_AFFINITY=verbose,granularity=fine,compact OMP_NUM_THREADS=288 python tf_cnn_benchmarks_MKL.py --model=inception3 --batch_size=128 --num_inter_threads=2 --num_intra_threads=288 --device=cpu --variable_update=parameter_server --local_parameter_device=cpu\r\n`\r\n```\r\nRunning on CPU : bdw\r\nTensorFlow:  1.2\r\nModel:       inception3\r\nMode:        training\r\nBatch size:  128 global\r\n             128 per device\r\nDevices:     ['/cpu:0']\r\nData format: NCHW\r\nOptimizer:   sgd\r\nVariables:   parameter_server\r\n==========\r\nGenerating model\r\n2017-08-15 15:29:31.811735: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-15 15:29:31.811882: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-15 15:29:31.811940: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-15 15:29:31.811985: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-15 15:29:31.812028: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX512F instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-15 15:29:31.812122: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-15 15:29:38.808628: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes\r\n2017-08-15 15:29:38.808756: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes\r\n2017-08-15 15:29:38.808818: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr\r\nRunning warm up\r\n_User settings:\r\n\r\n   KMP_AFFINITY=granularity=fine,verbose,compact,1,0\r\n   KMP_BLOCKTIME=30\r\n   KMP_SETTINGS=1\r\n   OMP_NUM_THREADS=288\r\n\r\nEffective settings:\r\n\r\n   KMP_ABORT_DELAY=0\r\n   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\r\n   KMP_ALIGN_ALLOC=64\r\n   KMP_ALL_THREADPRIVATE=1152\r\n   KMP_ALL_THREADS=2147483647\r\n   KMP_ATOMIC_MODE=2\r\n   KMP_BLOCKTIME=30\r\n   KMP_CPUINFO_FILE: value is not defined\r\n   KMP_DETERMINISTIC_REDUCTION=false\r\n   KMP_DISP_NUM_BUFFERS=7\r\n   KMP_DUPLICATE_LIB_OK=false\r\n   KMP_FORCE_REDUCTION: value is not defined\r\n   KMP_FOREIGN_THREADS_THREADPRIVATE=true\r\n   KMP_FORKJOIN_BARRIER='2,2'\r\n   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\r\n   KMP_FORKJOIN_FRAMES=true\r\n   KMP_FORKJOIN_FRAMES_MODE=3\r\n   KMP_GTID_MODE=3\r\n   KMP_HANDLE_SIGNALS=false\r\n   KMP_HOT_TEAMS_MAX_LEVEL=1\r\n   KMP_HOT_TEAMS_MODE=0\r\n   KMP_INIT_AT_FORK=true\r\n   KMP_INIT_WAIT=2048\r\n   KMP_ITT_PREPARE_DELAY=0\r\n   KMP_LIBRARY=throughput\r\n   KMP_LOCK_KIND=queuing\r\n   KMP_MALLOC_POOL_INCR=1M\r\n   KMP_NEXT_WAIT=1024\r\n   KMP_NUM_LOCKS_IN_BLOCK=1\r\n   KMP_PLAIN_BARRIER='2,2'\r\n   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\r\n   KMP_REDUCTION_BARRIER='1,1'\r\n   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\r\n   KMP_SCHEDULE='static,balanced;guided,iterative'\r\n   KMP_SETTINGS=true\r\n   KMP_SPIN_BACKOFF_PARAMS='4096,100'\r\n   KMP_STACKOFFSET=64\r\n   KMP_STACKPAD=0\r\n   KMP_STACKSIZE=4M\r\n   KMP_STORAGE_MAP=false\r\n   KMP_TASKING=2\r\n   KMP_TASK_STEALING_CONSTRAINT=1\r\n   KMP_USER_LEVEL_MWAIT=false\r\n   KMP_VERSION=false\r\n   KMP_WARNINGS=true\r\n   OMP_CANCELLATION=false\r\n   OMP_DEFAULT_DEVICE=0\r\n   OMP_DISPLAY_ENV=false\r\n   OMP_DYNAMIC=false\r\n   OMP_MAX_ACTIVE_LEVELS=2147483647\r\n   OMP_MAX_TASK_PRIORITY=0\r\n   OMP_NESTED=false\r\n   OMP_NUM_THREADS='288'\r\n   OMP_PLACES: value is not defined\r\n   OMP_PROC_BIND='intel'\r\n   OMP_SCHEDULE='static'\r\n   OMP_STACKSIZE=4M\r\n   OMP_THREAD_LIMIT=2147483647\r\n   OMP_WAIT_POLICY=PASSIVE\r\n   KMP_AFFINITY='verbose,warnings,respect,granularity=fine,duplicates,compact,1,0'\r\n\r\nOMP: Info #204: KMP_AFFINITY: decoding x2APIC ids.\r\nOMP: Info #202: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\r\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,...}\r\nOMP: Info #156: KMP_AFFINITY: 288 available OS procs\r\nOMP: Info #157: KMP_AFFINITY: Uniform topology\r\nOMP: Info #179: KMP_AFFINITY: 1 packages x 72 cores/pkg x 4 threads/core (72 total cores)\r\nOMP: Info #206: KMP_AFFINITY: OS proc to physical thread map:\r\nOMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 72 maps to package 0 core 0 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 144 maps to package 0 core 0 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 216 maps to package 0 core 0 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 73 maps to package 0 core 1 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 145 maps to package 0 core 1 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 217 maps to package 0 core 1 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 74 maps to package 0 core 2 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 146 maps to package 0 core 2 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 218 maps to package 0 core 2 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 75 maps to package 0 core 3 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 147 maps to package 0 core 3 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 219 maps to package 0 core 3 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 4 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 76 maps to package 0 core 4 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 148 maps to package 0 core 4 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 220 maps to package 0 core 4 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 5 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 77 maps to package 0 core 5 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 149 maps to package 0 core 5 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 221 maps to package 0 core 5 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 6 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 78 maps to package 0 core 6 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 150 maps to package 0 core 6 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 222 maps to package 0 core 6 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 7 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 79 maps to package 0 core 7 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 151 maps to package 0 core 7 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 223 maps to package 0 core 7 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 8 maps to package 0 core 8 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 80 maps to package 0 core 8 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 152 maps to package 0 core 8 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 224 maps to package 0 core 8 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 9 maps to package 0 core 9 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 81 maps to package 0 core 9 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 153 maps to package 0 core 9 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 225 maps to package 0 core 9 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 10 maps to package 0 core 10 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 82 maps to package 0 core 10 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 154 maps to package 0 core 10 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 226 maps to package 0 core 10 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 11 maps to package 0 core 11 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 83 maps to package 0 core 11 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 155 maps to package 0 core 11 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 227 maps to package 0 core 11 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 12 maps to package 0 core 12 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 84 maps to package 0 core 12 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 156 maps to package 0 core 12 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 228 maps to package 0 core 12 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 13 maps to package 0 core 13 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 85 maps to package 0 core 13 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 157 maps to package 0 core 13 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 229 maps to package 0 core 13 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 14 maps to package 0 core 14 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 86 maps to package 0 core 14 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 158 maps to package 0 core 14 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 230 maps to package 0 core 14 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 15 maps to package 0 core 15 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 87 maps to package 0 core 15 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 159 maps to package 0 core 15 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 231 maps to package 0 core 15 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 16 maps to package 0 core 16 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 88 maps to package 0 core 16 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 160 maps to package 0 core 16 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 232 maps to package 0 core 16 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 17 maps to package 0 core 17 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 89 maps to package 0 core 17 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 161 maps to package 0 core 17 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 233 maps to package 0 core 17 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 18 maps to package 0 core 18 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 90 maps to package 0 core 18 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 162 maps to package 0 core 18 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 234 maps to package 0 core 18 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 19 maps to package 0 core 19 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 91 maps to package 0 core 19 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 163 maps to package 0 core 19 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 235 maps to package 0 core 19 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 20 maps to package 0 core 20 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 92 maps to package 0 core 20 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 164 maps to package 0 core 20 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 236 maps to package 0 core 20 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 21 maps to package 0 core 21 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 93 maps to package 0 core 21 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 165 maps to package 0 core 21 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 237 maps to package 0 core 21 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 22 maps to package 0 core 22 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 94 maps to package 0 core 22 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 166 maps to package 0 core 22 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 238 maps to package 0 core 22 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 23 maps to package 0 core 23 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 95 maps to package 0 core 23 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 167 maps to package 0 core 23 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 239 maps to package 0 core 23 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 24 maps to package 0 core 24 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 96 maps to package 0 core 24 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 168 maps to package 0 core 24 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 240 maps to package 0 core 24 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 25 maps to package 0 core 25 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 97 maps to package 0 core 25 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 169 maps to package 0 core 25 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 241 maps to package 0 core 25 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 26 maps to package 0 core 26 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 98 maps to package 0 core 26 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 170 maps to package 0 core 26 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 242 maps to package 0 core 26 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 27 maps to package 0 core 27 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 99 maps to package 0 core 27 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 171 maps to package 0 core 27 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 243 maps to package 0 core 27 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 28 maps to package 0 core 28 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 100 maps to package 0 core 28 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 172 maps to package 0 core 28 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 244 maps to package 0 core 28 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 29 maps to package 0 core 29 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 101 maps to package 0 core 29 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 173 maps to package 0 core 29 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 245 maps to package 0 core 29 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 30 maps to package 0 core 30 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 102 maps to package 0 core 30 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 174 maps to package 0 core 30 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 246 maps to package 0 core 30 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 31 maps to package 0 core 31 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 103 maps to package 0 core 31 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 175 maps to package 0 core 31 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 247 maps to package 0 core 31 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 32 maps to package 0 core 32 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 104 maps to package 0 core 32 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 176 maps to package 0 core 32 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 248 maps to package 0 core 32 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 33 maps to package 0 core 33 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 105 maps to package 0 core 33 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 177 maps to package 0 core 33 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 249 maps to package 0 core 33 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 34 maps to package 0 core 34 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 106 maps to package 0 core 34 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 178 maps to package 0 core 34 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 250 maps to package 0 core 34 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 35 maps to package 0 core 35 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 107 maps to package 0 core 35 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 179 maps to package 0 core 35 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 251 maps to package 0 core 35 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 36 maps to package 0 core 36 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 108 maps to package 0 core 36 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 180 maps to package 0 core 36 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 252 maps to package 0 core 36 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 37 maps to package 0 core 37 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 109 maps to package 0 core 37 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 181 maps to package 0 core 37 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 253 maps to package 0 core 37 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 38 maps to package 0 core 38 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 110 maps to package 0 core 38 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 182 maps to package 0 core 38 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 254 maps to package 0 core 38 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 39 maps to package 0 core 39 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 111 maps to package 0 core 39 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 183 maps to package 0 core 39 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 255 maps to package 0 core 39 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 40 maps to package 0 core 40 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 112 maps to package 0 core 40 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 184 maps to package 0 core 40 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 256 maps to package 0 core 40 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 41 maps to package 0 core 41 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 113 maps to package 0 core 41 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 185 maps to package 0 core 41 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 257 maps to package 0 core 41 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 42 maps to package 0 core 42 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 114 maps to package 0 core 42 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 186 maps to package 0 core 42 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 258 maps to package 0 core 42 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 43 maps to package 0 core 43 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 115 maps to package 0 core 43 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 187 maps to package 0 core 43 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 259 maps to package 0 core 43 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 44 maps to package 0 core 46 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 116 maps to package 0 core 46 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 188 maps to package 0 core 46 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 260 maps to package 0 core 46 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 45 maps to package 0 core 47 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 117 maps to package 0 core 47 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 189 maps to package 0 core 47 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 261 maps to package 0 core 47 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 46 maps to package 0 core 48 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 118 maps to package 0 core 48 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 190 maps to package 0 core 48 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 262 maps to package 0 core 48 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 47 maps to package 0 core 49 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 119 maps to package 0 core 49 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 191 maps to package 0 core 49 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 263 maps to package 0 core 49 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 48 maps to package 0 core 50 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 120 maps to package 0 core 50 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 192 maps to package 0 core 50 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 264 maps to package 0 core 50 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 49 maps to package 0 core 51 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 121 maps to package 0 core 51 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 193 maps to package 0 core 51 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 265 maps to package 0 core 51 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 50 maps to package 0 core 52 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 122 maps to package 0 core 52 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 194 maps to package 0 core 52 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 266 maps to package 0 core 52 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 51 maps to package 0 core 53 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 123 maps to package 0 core 53 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 195 maps to package 0 core 53 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 267 maps to package 0 core 53 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 52 maps to package 0 core 54 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 124 maps to package 0 core 54 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 196 maps to package 0 core 54 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 268 maps to package 0 core 54 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 53 maps to package 0 core 55 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 125 maps to package 0 core 55 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 197 maps to package 0 core 55 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 269 maps to package 0 core 55 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 54 maps to package 0 core 56 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 126 maps to package 0 core 56 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 198 maps to package 0 core 56 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 270 maps to package 0 core 56 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 55 maps to package 0 core 57 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 127 maps to package 0 core 57 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 199 maps to package 0 core 57 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 271 maps to package 0 core 57 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 56 maps to package 0 core 58 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 128 maps to package 0 core 58 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 200 maps to package 0 core 58 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 272 maps to package 0 core 58 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 57 maps to package 0 core 59 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 129 maps to package 0 core 59 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 201 maps to package 0 core 59 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 273 maps to package 0 core 59 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 58 maps to package 0 core 60 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 130 maps to package 0 core 60 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 202 maps to package 0 core 60 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 274 maps to package 0 core 60 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 59 maps to package 0 core 61 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 131 maps to package 0 core 61 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 203 maps to package 0 core 61 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 275 maps to package 0 core 61 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 60 maps to package 0 core 62 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 132 maps to package 0 core 62 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 204 maps to package 0 core 62 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 276 maps to package 0 core 62 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 61 maps to package 0 core 63 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 133 maps to package 0 core 63 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 205 maps to package 0 core 63 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 277 maps to package 0 core 63 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 62 maps to package 0 core 64 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 134 maps to package 0 core 64 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 206 maps to package 0 core 64 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 278 maps to package 0 core 64 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 63 maps to package 0 core 65 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 135 maps to package 0 core 65 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 207 maps to package 0 core 65 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 279 maps to package 0 core 65 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 64 maps to package 0 core 66 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 136 maps to package 0 core 66 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 208 maps to package 0 core 66 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 280 maps to package 0 core 66 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 65 maps to package 0 core 67 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 137 maps to package 0 core 67 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 209 maps to package 0 core 67 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 281 maps to package 0 core 67 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 66 maps to package 0 core 68 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 138 maps to package 0 core 68 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 210 maps to package 0 core 68 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 282 maps to package 0 core 68 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 67 maps to package 0 core 69 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 139 maps to package 0 core 69 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 211 maps to package 0 core 69 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 283 maps to package 0 core 69 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 68 maps to package 0 core 70 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 140 maps to package 0 core 70 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 212 maps to package 0 core 70 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 284 maps to package 0 core 70 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 69 maps to package 0 core 71 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 141 maps to package 0 core 71 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 213 maps to package 0 core 71 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 285 maps to package 0 core 71 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 70 maps to package 0 core 72 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 142 maps to package 0 core 72 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 214 maps to package 0 core 72 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 286 maps to package 0 core 72 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 71 maps to package 0 core 73 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 143 maps to package 0 core 73 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 215 maps to package 0 core 73 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 287 maps to package 0 core 73 thread 3\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 0 bound to OS proc set {0}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 1 bound to OS proc set {1}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 2 bound to OS proc set {2}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 3 bound to OS proc set {3}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 6 bound to OS proc set {6}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 7 bound to OS proc set {7}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 8 bound to OS proc set {8}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 9 bound to OS proc set {9}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 10 bound to OS proc set {10}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 11 bound to OS proc set {11}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 12 bound to OS proc set {12}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 13 bound to OS proc set {13}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 14 bound to OS proc set {14}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 15 bound to OS proc set {15}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 16 bound to OS proc set {16}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 17 bound to OS proc set {17}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 18 bound to OS proc set {18}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 19 bound to OS proc set {19}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 5 bound to OS proc set {5}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 4 bound to OS proc set {4}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 28 bound to OS proc set {28}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 30 bound to OS proc set {30}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 29 bound to OS proc set {29}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 21 bound to OS proc set {21}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 32 bound to OS proc set {32}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 24 bound to OS proc set {24}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 27 bound to OS proc set {27}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 26 bound to OS proc set {26}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 31 bound to OS proc set {31}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 22 bound to OS proc set {22}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 23 bound to OS proc set {23}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 25 bound to OS proc set {25}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 20 bound to OS proc set {20}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 46 bound to OS proc set {46}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 47 bound to OS proc set {47}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 48 bound to OS proc set {48}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 50 bound to OS proc set {50}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 49 bound to OS proc set {49}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 51 bound to OS proc set {51}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 39 bound to OS proc set {39}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 36 bound to OS proc set {36}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 33 bound to OS proc set {33}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 52 bound to OS proc set {52}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 38 bound to OS proc set {38}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 40 bound to OS proc set {40}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 53 bound to OS proc set {53}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 41 bound to OS proc set {41}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 35 bound to OS proc set {35}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 34 bound to OS proc set {34}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 37 bound to OS proc set {37}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 44 bound to OS proc set {44}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 42 bound to OS proc set {42}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 45 bound to OS proc set {45}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 43 bound to OS proc set {43}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 55 bound to OS proc set {55}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 54 bound to OS proc set {54}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 58 bound to OS proc set {58}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 62 bound to OS proc set {62}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 57 bound to OS proc set {57}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 63 bound to OS proc set {63}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 65 bound to OS proc set {65}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 66 bound to OS proc set {66}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 67 bound to OS proc set {67}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 56 bound to OS proc set {56}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 59 bound to OS proc set {59}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 60 bound to OS proc set {60}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 61 bound to OS proc set {61}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 64 bound to OS proc set {64}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 68 bound to OS proc set {68}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 69 bound to OS proc set {69}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 70 bound to OS proc set {70}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 71 bound to OS proc set {71}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 72 bound to OS proc set {72}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 73 bound to OS proc set {73}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 75 bound to OS proc set {75}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 74 bound to OS proc set {74}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 76 bound to OS proc set {76}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 77 bound to OS proc set {77}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 78 bound to OS proc set {78}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 79 bound to OS proc set {79}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 80 bound to OS proc set {80}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 81 bound to OS proc set {81}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 83 bound to OS proc set {83}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 82 bound to OS proc set {82}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 84 bound to OS proc set {84}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 85 bound to OS proc set {85}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 87 bound to OS proc set {87}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 88 bound to OS proc set {88}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 89 bound to OS proc set {89}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 90 bound to OS proc set {90}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 95 bound to OS proc set {95}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 92 bound to OS proc set {92}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 94 bound to OS proc set {94}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 100 bound to OS proc set {100}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 96 bound to OS proc set {96}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 97 bound to OS proc set {97}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 98 bound to OS proc set {98}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 93 bound to OS proc set {93}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 91 bound to OS proc set {91}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 99 bound to OS proc set {99}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 101 bound to OS proc set {101}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 86 bound to OS proc set {86}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 102 bound to OS proc set {102}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 104 bound to OS proc set {104}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 105 bound to OS proc set {105}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 103 bound to OS proc set {103}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 107 bound to OS proc set {107}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 106 bound to OS proc set {106}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 109 bound to OS proc set {109}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 108 bound to OS proc set {108}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 110 bound to OS proc set {110}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 111 bound to OS proc set {111}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 112 bound to OS proc set {112}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 113 bound to OS proc set {113}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 114 bound to OS proc set {114}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 115 bound to OS proc set {115}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 116 bound to OS proc set {116}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 119 bound to OS proc set {119}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 118 bound to OS proc set {118}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 117 bound to OS proc set {117}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 120 bound to OS proc set {120}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 121 bound to OS proc set {121}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 122 bound to OS proc set {122}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 123 bound to OS proc set {123}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 124 bound to OS proc set {124}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 127 bound to OS proc set {127}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 135 bound to OS proc set {135}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 137 bound to OS proc set {137}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 131 bound to OS proc set {131}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 136 bound to OS proc set {136}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 132 bound to OS proc set {132}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 125 bound to OS proc set {125}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 143 bound to OS proc set {143}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 140 bound to OS proc set {140}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 133 bound to OS proc set {133}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 130 bound to OS proc set {130}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 139 bound to OS proc set {139}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 126 bound to OS proc set {126}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 141 bound to OS proc set {141}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 129 bound to OS proc set {129}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 142 bound to OS proc set {142}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 134 bound to OS proc set {134}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 138 bound to OS proc set {138}\r\nOMP: Info #242: KMP_AFFINITY: pid 189059 thread 128 bound to OS proc set {128}_\r\nDone warm up\r\nDone warm up\r\nStep\tImg/sec\tloss\r\nStarting real work at step 10 at time Tue Aug 15 15:30:45 2017\r\n1\timages/sec: 26.0 +/- 0.0 (jitter = 0.0)\t7.263\r\n10\timages/sec: 25.9 +/- 0.1 (jitter = 0.1)\t7.159\r\n20\timages/sec: 25.9 +/- 0.0 (jitter = 0.1)\t7.046\r\n30\timages/sec: 26.0 +/- 0.0 (jitter = 0.1)\t6.934\r\n40\timages/sec: 26.0 +/- 0.0 (jitter = 0.1)\t6.794\r\n50\timages/sec: 26.0 +/- 0.0 (jitter = 0.1)\t6.702\r\n60\timages/sec: 25.9 +/- 0.0 (jitter = 0.1)\t6.593\r\n70\timages/sec: 25.9 +/- 0.0 (jitter = 0.1)\t6.479\r\n80\timages/sec: 25.9 +/- 0.0 (jitter = 0.1)\t6.418\r\n90\timages/sec: 25.9 +/- 0.0 (jitter = 0.2)\t6.324\r\nFinishing real work at step 109 at time Tue Aug 15 15:38:55 2017\r\n100\timages/sec: 25.9 +/- 0.0 (jitter = 0.2)\t6.227\r\n----------------------------------------------------------------\r\ntotal images/sec: 25.87\r\n----------------------------------------------------------------\r\n\r\n\r\n```\r\n\r\n@tfboyd  It seems to show an increase from 6images/sec earlier, but still off the mark by alot.\r\nIt does give a warning and error, so I don't really think the MKL library is working properly.\r\n\r\nHow do I check if the MKL library is working/installed properly?\r\n\r\n\r\n", "You can tell becuase you see the configs information.  _User settings:\r\n\r\n   KMP_AFFINITY=granularity=fine,verbose,compact,1,0\r\n   KMP_BLOCKTIME=30\r\n   KMP_SETTINGS=1\r\n   OMP_NUM_THREADS=288\r\n\r\nYou should try multiple settings.  The Errors and Warning are fine.  So if I am not mistaken you have \r\n1 Xeon Phi with 72 cores.  (You said 288 logical which is why I guessed you had 2 cards for 144 cores and 288 logical)\r\n\r\nGiven your number of cores you need to back down the number of threads.  For inception3 you should be doing, I am not sure why you set the BLOCK_TIME=2 but it ended up being set to 30 and should be 0 for inception3:\r\n\r\n`KMP_SETTINGS=1 KMP_BLOCKTIME=0 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 OMP_NUM_THREADS=72 python tf_cnn_benchmarks.py --model=inception3 --batch_size=128 --num_inter_threads=1 --num_intra_threads=72 --device=cpu --variable_update=parameter_server --local_parameter_device=cpu`\r\n\r\nAnd if you are training real data you might back the threads down to ~60.  I have not tested on PHi but that is the advice in the article Intel wrong and you can see the same in setenv.\r\n\r\nFor reference I get **21 images/sec** batch_size=32 running a similar script training inception3 on \r\n2x Intel Xeon E5-2686 v4 on AWS r4.16xlarge, which would be 36 physical cores total if I remember correctly and not really comparable to the PHi.\r\n\r\nIf you are not seeing the KMP_BLOCKTIME you set showing up in the debug info then you may want to check how you typed it or consider just commenting out this line in the script as you are setting these on your own.  https://github.com/tfboyd/benchmarks/blob/mkl/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks_MKL.py#L1318\r\n\r\n\r\n\r\n\r\n", "@tfboyd  With the command you provided I get 22images/sec. I ran it thrice. I checked the total CPU utilization and only 15% is utilized. \r\nWhat numbers do you expect to see with these configurations?", "I think your config is still not correct but with out the debug info I do not know.  In your first post your passed BLOCK_TIME=2 but the config printed BLOCK_TIME=20  Please share the entire output and make sure to print out the MKL configs.   I should have included the print config in the command.  \r\n\r\nIf I had to make an educated guess I would guess with synthetic data: 60-100 images/sec.  22 images/sec suggests something is wrong as I get that with 2xBroadwell.   Intel was getting ~60 images/sec with VGG (not sure how many layers so that is a bit of an unknown) but that is is informing my guess.  \r\n\r\nI am happy to keep helping but I need a few things.  \r\n- Make sure set_envars.py (my last comment linked the line to remove) is not being called, that is overwriting your env variables from the commandline\r\n- include the full console logs and make sure to set `KMP_SETTINGS=1`\r\n\r\nOn the plus side if you figure this out you will be familiar with the important  MKL environment variables.\r\n\r\n\r\n\r\n\r\n", "This is the whole output. I have removed L1318 from the script. KMP_BLOCK_TIME is set to 0.\r\n```\r\n$KMP_SETTINGS=1 KMP_BLOCKTIME=0 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 \r\nOMP_NUM_THREADS=72 python tf_cnn_benchmarks_MKL.py --model=inception3 --batch_size=128\r\n --num_inter_threads=1 --num_intra_threads=72 --device=cpu\r\n\r\nRunning on CPU : bdw\r\nTensorFlow:  1.2\r\nModel:       inception3\r\nMode:        training\r\nBatch size:  128 global\r\n             128 per device\r\nDevices:     ['/cpu:0']\r\nData format: NCHW\r\nOptimizer:   sgd\r\nVariables:   parameter_server\r\n==========\r\nGenerating model\r\n2017-08-16 14:47:13.894021: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-16 14:47:13.894170: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-16 14:47:13.894230: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-16 14:47:13.894279: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-16 14:47:13.894324: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX512F instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-16 14:47:13.894364: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-16 14:47:21.069981: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes\r\n2017-08-16 14:47:21.070140: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes\r\n2017-08-16 14:47:21.070207: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr\r\nRunning warm up\r\n\r\nUser settings:\r\n\r\n   KMP_AFFINITY=granularity=fine,verbose,compact,1,0\r\n   KMP_BLOCKTIME=0\r\n   KMP_SETTINGS=1\r\n   OMP_NUM_THREADS=72\r\n\r\nEffective settings:\r\n\r\n   KMP_ABORT_DELAY=0\r\n   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\r\n   KMP_ALIGN_ALLOC=64\r\n   KMP_ALL_THREADPRIVATE=1152\r\n   KMP_ALL_THREADS=2147483647\r\n   KMP_ATOMIC_MODE=2\r\n   KMP_BLOCKTIME=0\r\n   KMP_CPUINFO_FILE: value is not defined\r\n   KMP_DETERMINISTIC_REDUCTION=false\r\n   KMP_DISP_NUM_BUFFERS=7\r\n   KMP_DUPLICATE_LIB_OK=false\r\n   KMP_FORCE_REDUCTION: value is not defined\r\n   KMP_FOREIGN_THREADS_THREADPRIVATE=true\r\n   KMP_FORKJOIN_BARRIER='2,2'\r\n   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\r\n   KMP_FORKJOIN_FRAMES=true\r\n   KMP_FORKJOIN_FRAMES_MODE=3\r\n   KMP_GTID_MODE=3\r\n   KMP_HANDLE_SIGNALS=false\r\n   KMP_HOT_TEAMS_MAX_LEVEL=1\r\n   KMP_HOT_TEAMS_MODE=0\r\n   KMP_INIT_AT_FORK=true\r\n   KMP_INIT_WAIT=2048\r\n   KMP_ITT_PREPARE_DELAY=0\r\n   KMP_LIBRARY=throughput\r\n   KMP_LOCK_KIND=queuing\r\n   KMP_MALLOC_POOL_INCR=1M\r\n   KMP_NEXT_WAIT=1024\r\n   KMP_NUM_LOCKS_IN_BLOCK=1\r\n   KMP_PLAIN_BARRIER='2,2'\r\n   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\r\n   KMP_REDUCTION_BARRIER='1,1'\r\n   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\r\n   KMP_SCHEDULE='static,balanced;guided,iterative'\r\n   KMP_SETTINGS=true\r\n   KMP_SPIN_BACKOFF_PARAMS='4096,100'\r\n   KMP_STACKOFFSET=64\r\n   KMP_STACKPAD=0\r\n   KMP_STACKSIZE=4M\r\n   KMP_STORAGE_MAP=false\r\n   KMP_TASKING=2\r\n   KMP_TASK_STEALING_CONSTRAINT=1\r\n   KMP_USER_LEVEL_MWAIT=false\r\n   KMP_VERSION=false\r\n   KMP_WARNINGS=true\r\n   OMP_CANCELLATION=false\r\n   OMP_DEFAULT_DEVICE=0\r\n   OMP_DISPLAY_ENV=false\r\n   OMP_DYNAMIC=false\r\n   OMP_MAX_ACTIVE_LEVELS=2147483647\r\n   OMP_MAX_TASK_PRIORITY=0\r\n   OMP_NESTED=false\r\n   OMP_NUM_THREADS='72'\r\n   OMP_PLACES: value is not defined\r\n   OMP_PROC_BIND='intel'\r\n   OMP_SCHEDULE='static'\r\n   OMP_STACKSIZE=4M\r\n   OMP_THREAD_LIMIT=2147483647\r\n   OMP_WAIT_POLICY=PASSIVE\r\n   KMP_AFFINITY='verbose,warnings,respect,granularity=fine,duplicates,compact,1,0'\r\n\r\nOMP: Info #204: KMP_AFFINITY: decoding x2APIC ids.\r\nOMP: Info #202: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\r\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,...}\r\nOMP: Info #156: KMP_AFFINITY: 288 available OS procs\r\nOMP: Info #157: KMP_AFFINITY: Uniform topology\r\nOMP: Info #179: KMP_AFFINITY: 1 packages x 72 cores/pkg x 4 threads/core (72 total cores)\r\nOMP: Info #206: KMP_AFFINITY: OS proc to physical thread map:\r\nOMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 72 maps to package 0 core 0 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 144 maps to package 0 core 0 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 216 maps to package 0 core 0 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 73 maps to package 0 core 1 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 145 maps to package 0 core 1 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 217 maps to package 0 core 1 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 74 maps to package 0 core 2 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 146 maps to package 0 core 2 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 218 maps to package 0 core 2 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 75 maps to package 0 core 3 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 147 maps to package 0 core 3 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 219 maps to package 0 core 3 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 4 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 76 maps to package 0 core 4 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 148 maps to package 0 core 4 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 220 maps to package 0 core 4 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 5 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 77 maps to package 0 core 5 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 149 maps to package 0 core 5 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 221 maps to package 0 core 5 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 6 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 78 maps to package 0 core 6 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 150 maps to package 0 core 6 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 222 maps to package 0 core 6 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 7 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 79 maps to package 0 core 7 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 151 maps to package 0 core 7 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 223 maps to package 0 core 7 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 8 maps to package 0 core 8 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 80 maps to package 0 core 8 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 152 maps to package 0 core 8 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 224 maps to package 0 core 8 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 9 maps to package 0 core 9 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 81 maps to package 0 core 9 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 153 maps to package 0 core 9 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 225 maps to package 0 core 9 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 10 maps to package 0 core 10 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 82 maps to package 0 core 10 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 154 maps to package 0 core 10 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 226 maps to package 0 core 10 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 11 maps to package 0 core 11 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 83 maps to package 0 core 11 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 155 maps to package 0 core 11 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 227 maps to package 0 core 11 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 12 maps to package 0 core 12 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 84 maps to package 0 core 12 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 156 maps to package 0 core 12 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 228 maps to package 0 core 12 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 13 maps to package 0 core 13 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 85 maps to package 0 core 13 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 157 maps to package 0 core 13 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 229 maps to package 0 core 13 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 14 maps to package 0 core 14 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 86 maps to package 0 core 14 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 158 maps to package 0 core 14 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 230 maps to package 0 core 14 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 15 maps to package 0 core 15 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 87 maps to package 0 core 15 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 159 maps to package 0 core 15 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 231 maps to package 0 core 15 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 16 maps to package 0 core 16 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 88 maps to package 0 core 16 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 160 maps to package 0 core 16 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 232 maps to package 0 core 16 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 17 maps to package 0 core 17 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 89 maps to package 0 core 17 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 161 maps to package 0 core 17 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 233 maps to package 0 core 17 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 18 maps to package 0 core 18 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 90 maps to package 0 core 18 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 162 maps to package 0 core 18 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 234 maps to package 0 core 18 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 19 maps to package 0 core 19 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 91 maps to package 0 core 19 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 163 maps to package 0 core 19 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 235 maps to package 0 core 19 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 20 maps to package 0 core 20 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 92 maps to package 0 core 20 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 164 maps to package 0 core 20 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 236 maps to package 0 core 20 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 21 maps to package 0 core 21 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 93 maps to package 0 core 21 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 165 maps to package 0 core 21 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 237 maps to package 0 core 21 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 22 maps to package 0 core 22 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 94 maps to package 0 core 22 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 166 maps to package 0 core 22 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 238 maps to package 0 core 22 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 23 maps to package 0 core 23 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 95 maps to package 0 core 23 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 167 maps to package 0 core 23 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 239 maps to package 0 core 23 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 24 maps to package 0 core 24 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 96 maps to package 0 core 24 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 168 maps to package 0 core 24 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 240 maps to package 0 core 24 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 25 maps to package 0 core 25 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 97 maps to package 0 core 25 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 169 maps to package 0 core 25 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 241 maps to package 0 core 25 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 26 maps to package 0 core 26 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 98 maps to package 0 core 26 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 170 maps to package 0 core 26 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 242 maps to package 0 core 26 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 27 maps to package 0 core 27 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 99 maps to package 0 core 27 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 171 maps to package 0 core 27 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 243 maps to package 0 core 27 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 28 maps to package 0 core 28 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 100 maps to package 0 core 28 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 172 maps to package 0 core 28 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 244 maps to package 0 core 28 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 29 maps to package 0 core 29 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 101 maps to package 0 core 29 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 173 maps to package 0 core 29 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 245 maps to package 0 core 29 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 30 maps to package 0 core 30 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 102 maps to package 0 core 30 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 174 maps to package 0 core 30 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 246 maps to package 0 core 30 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 31 maps to package 0 core 31 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 103 maps to package 0 core 31 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 175 maps to package 0 core 31 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 247 maps to package 0 core 31 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 32 maps to package 0 core 32 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 104 maps to package 0 core 32 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 176 maps to package 0 core 32 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 248 maps to package 0 core 32 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 33 maps to package 0 core 33 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 105 maps to package 0 core 33 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 177 maps to package 0 core 33 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 249 maps to package 0 core 33 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 34 maps to package 0 core 34 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 106 maps to package 0 core 34 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 178 maps to package 0 core 34 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 250 maps to package 0 core 34 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 35 maps to package 0 core 35 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 107 maps to package 0 core 35 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 179 maps to package 0 core 35 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 251 maps to package 0 core 35 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 36 maps to package 0 core 36 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 108 maps to package 0 core 36 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 180 maps to package 0 core 36 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 252 maps to package 0 core 36 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 37 maps to package 0 core 37 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 109 maps to package 0 core 37 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 181 maps to package 0 core 37 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 253 maps to package 0 core 37 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 38 maps to package 0 core 38 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 110 maps to package 0 core 38 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 182 maps to package 0 core 38 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 254 maps to package 0 core 38 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 39 maps to package 0 core 39 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 111 maps to package 0 core 39 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 183 maps to package 0 core 39 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 255 maps to package 0 core 39 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 40 maps to package 0 core 40 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 112 maps to package 0 core 40 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 184 maps to package 0 core 40 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 256 maps to package 0 core 40 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 41 maps to package 0 core 41 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 113 maps to package 0 core 41 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 185 maps to package 0 core 41 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 257 maps to package 0 core 41 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 42 maps to package 0 core 42 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 114 maps to package 0 core 42 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 186 maps to package 0 core 42 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 258 maps to package 0 core 42 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 43 maps to package 0 core 43 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 115 maps to package 0 core 43 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 187 maps to package 0 core 43 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 259 maps to package 0 core 43 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 44 maps to package 0 core 46 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 116 maps to package 0 core 46 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 188 maps to package 0 core 46 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 260 maps to package 0 core 46 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 45 maps to package 0 core 47 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 117 maps to package 0 core 47 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 189 maps to package 0 core 47 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 261 maps to package 0 core 47 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 46 maps to package 0 core 48 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 118 maps to package 0 core 48 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 190 maps to package 0 core 48 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 262 maps to package 0 core 48 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 47 maps to package 0 core 49 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 119 maps to package 0 core 49 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 191 maps to package 0 core 49 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 263 maps to package 0 core 49 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 48 maps to package 0 core 50 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 120 maps to package 0 core 50 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 192 maps to package 0 core 50 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 264 maps to package 0 core 50 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 49 maps to package 0 core 51 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 121 maps to package 0 core 51 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 193 maps to package 0 core 51 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 265 maps to package 0 core 51 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 50 maps to package 0 core 52 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 122 maps to package 0 core 52 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 194 maps to package 0 core 52 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 266 maps to package 0 core 52 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 51 maps to package 0 core 53 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 123 maps to package 0 core 53 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 195 maps to package 0 core 53 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 267 maps to package 0 core 53 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 52 maps to package 0 core 54 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 124 maps to package 0 core 54 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 196 maps to package 0 core 54 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 268 maps to package 0 core 54 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 53 maps to package 0 core 55 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 125 maps to package 0 core 55 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 197 maps to package 0 core 55 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 269 maps to package 0 core 55 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 54 maps to package 0 core 56 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 126 maps to package 0 core 56 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 198 maps to package 0 core 56 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 270 maps to package 0 core 56 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 55 maps to package 0 core 57 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 127 maps to package 0 core 57 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 199 maps to package 0 core 57 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 271 maps to package 0 core 57 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 56 maps to package 0 core 58 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 128 maps to package 0 core 58 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 200 maps to package 0 core 58 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 272 maps to package 0 core 58 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 57 maps to package 0 core 59 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 129 maps to package 0 core 59 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 201 maps to package 0 core 59 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 273 maps to package 0 core 59 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 58 maps to package 0 core 60 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 130 maps to package 0 core 60 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 202 maps to package 0 core 60 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 274 maps to package 0 core 60 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 59 maps to package 0 core 61 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 131 maps to package 0 core 61 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 203 maps to package 0 core 61 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 275 maps to package 0 core 61 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 60 maps to package 0 core 62 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 132 maps to package 0 core 62 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 204 maps to package 0 core 62 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 276 maps to package 0 core 62 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 61 maps to package 0 core 63 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 133 maps to package 0 core 63 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 205 maps to package 0 core 63 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 277 maps to package 0 core 63 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 62 maps to package 0 core 64 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 134 maps to package 0 core 64 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 206 maps to package 0 core 64 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 278 maps to package 0 core 64 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 63 maps to package 0 core 65 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 135 maps to package 0 core 65 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 207 maps to package 0 core 65 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 279 maps to package 0 core 65 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 64 maps to package 0 core 66 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 136 maps to package 0 core 66 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 208 maps to package 0 core 66 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 280 maps to package 0 core 66 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 65 maps to package 0 core 67 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 137 maps to package 0 core 67 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 209 maps to package 0 core 67 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 281 maps to package 0 core 67 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 66 maps to package 0 core 68 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 138 maps to package 0 core 68 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 210 maps to package 0 core 68 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 282 maps to package 0 core 68 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 67 maps to package 0 core 69 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 139 maps to package 0 core 69 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 211 maps to package 0 core 69 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 283 maps to package 0 core 69 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 68 maps to package 0 core 70 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 140 maps to package 0 core 70 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 212 maps to package 0 core 70 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 284 maps to package 0 core 70 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 69 maps to package 0 core 71 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 141 maps to package 0 core 71 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 213 maps to package 0 core 71 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 285 maps to package 0 core 71 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 70 maps to package 0 core 72 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 142 maps to package 0 core 72 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 214 maps to package 0 core 72 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 286 maps to package 0 core 72 thread 3\r\nOMP: Info #171: KMP_AFFINITY: OS proc 71 maps to package 0 core 73 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 143 maps to package 0 core 73 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 215 maps to package 0 core 73 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 287 maps to package 0 core 73 thread 3\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 0 bound to OS proc set {0}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 2 bound to OS proc set {2}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 1 bound to OS proc set {1}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 3 bound to OS proc set {3}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 4 bound to OS proc set {4}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 5 bound to OS proc set {5}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 6 bound to OS proc set {6}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 7 bound to OS proc set {7}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 8 bound to OS proc set {8}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 9 bound to OS proc set {9}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 10 bound to OS proc set {10}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 12 bound to OS proc set {12}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 11 bound to OS proc set {11}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 13 bound to OS proc set {13}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 14 bound to OS proc set {14}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 15 bound to OS proc set {15}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 16 bound to OS proc set {16}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 17 bound to OS proc set {17}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 18 bound to OS proc set {18}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 19 bound to OS proc set {19}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 20 bound to OS proc set {20}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 21 bound to OS proc set {21}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 22 bound to OS proc set {22}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 23 bound to OS proc set {23}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 24 bound to OS proc set {24}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 25 bound to OS proc set {25}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 26 bound to OS proc set {26}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 27 bound to OS proc set {27}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 28 bound to OS proc set {28}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 30 bound to OS proc set {30}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 29 bound to OS proc set {29}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 31 bound to OS proc set {31}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 32 bound to OS proc set {32}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 33 bound to OS proc set {33}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 34 bound to OS proc set {34}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 35 bound to OS proc set {35}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 36 bound to OS proc set {36}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 37 bound to OS proc set {37}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 38 bound to OS proc set {38}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 39 bound to OS proc set {39}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 40 bound to OS proc set {40}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 41 bound to OS proc set {41}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 42 bound to OS proc set {42}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 43 bound to OS proc set {43}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 45 bound to OS proc set {45}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 44 bound to OS proc set {44}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 46 bound to OS proc set {46}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 47 bound to OS proc set {47}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 48 bound to OS proc set {48}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 50 bound to OS proc set {50}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 49 bound to OS proc set {49}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 51 bound to OS proc set {51}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 52 bound to OS proc set {52}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 53 bound to OS proc set {53}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 54 bound to OS proc set {54}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 55 bound to OS proc set {55}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 56 bound to OS proc set {56}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 57 bound to OS proc set {57}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 58 bound to OS proc set {58}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 59 bound to OS proc set {59}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 60 bound to OS proc set {60}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 61 bound to OS proc set {61}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 62 bound to OS proc set {62}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 63 bound to OS proc set {63}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 64 bound to OS proc set {64}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 65 bound to OS proc set {65}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 66 bound to OS proc set {66}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 67 bound to OS proc set {67}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 68 bound to OS proc set {68}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 69 bound to OS proc set {69}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 70 bound to OS proc set {70}\r\nOMP: Info #242: KMP_AFFINITY: pid 202958 thread 71 bound to OS proc set {71}\r\nDone warm up\r\nStarting real work at step 10 at time Wed Aug 16 14:48:34 2017\r\nStep\tImg/sec\tloss\r\n1\timages/sec: 22.3 +/- 0.0 (jitter = 0.0)\t7.253\r\n10\timages/sec: 22.6 +/- 0.1 (jitter = 0.3)\t7.131\r\n20\timages/sec: 22.6 +/- 0.0 (jitter = 0.3)\t6.982\r\n30\timages/sec: 22.6 +/- 0.0 (jitter = 0.2)\t6.870\r\n40\timages/sec: 22.6 +/- 0.0 (jitter = 0.2)\t6.770\r\n50\timages/sec: 22.6 +/- 0.0 (jitter = 0.2)\t6.629\r\n60\timages/sec: 22.6 +/- 0.0 (jitter = 0.2)\t6.519\r\n70\timages/sec: 22.6 +/- 0.0 (jitter = 0.2)\t6.413\r\n80\timages/sec: 22.6 +/- 0.0 (jitter = 0.2)\t6.302\r\n90\timages/sec: 22.6 +/- 0.0 (jitter = 0.2)\t6.223\r\nFinishing real work at step 109 at time Wed Aug 16 14:57:54 2017\r\n100\timages/sec: 22.6 +/- 0.0 (jitter = 0.2)\t6.108\r\n----------------------------------------------------------------\r\ntotal images/sec: 22.62\r\n----------------------------------------------------------------\r\n```", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Last activity was August.  I honestly did not skim to see if this was 100% resolved but given it was almost 6 months ago it might not matter either way and I honestly apologize.  Please add a comment and @ me if there is new info as I will see it even with the issue closed.  "]}]