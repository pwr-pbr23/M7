[{"number": 24414, "title": "ppc64le: no_mkl_dnn_contraction_kernel define causes build failure", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 ppc64le\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?:  source\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source):  4.8\r\n- CUDA/cuDNN version:  10.0/7.3\r\n- GPU model and memory: V100\r\n\r\n\r\n\r\n**Describe the problem**\r\nA recent commit (https://github.com/tensorflow/tensorflow/commit/10ef7edc881ee715eaae48656fcb431fe128441f) changed the default contraction kernel to be MKL based. Code was added for non-intel platforms to avoid MKL, but it has an error.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nBasic build: `bazel build //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nERROR: /home/jenkins/workspace/TensorFlow_PPC64LE_GPU_Build/tensorflow/core/kernels/BUILD:610:12: Illegal ambiguous match on configurable attribute \"deps\" in //tensorflow/core/kernels:eigen_contraction_kernel:\r\n//tensorflow:linux_ppc64le\r\n//tensorflow/core/kernels:no_mkldnn_contraction_kernel\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: \r\n\r\n/home/jenkins/workspace/TensorFlow_PPC64LE_GPU_Build/tensorflow/core/kernels/BUILD:610:12: Illegal ambiguous match on configurable attribute \"deps\" in //tensorflow/core/kernels:eigen_contraction_kernel:\r\n//tensorflow:linux_ppc64le\r\n//tensorflow/core/kernels:no_mkldnn_contraction_kernel\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\n```\r\n\r\nThere ends up being multiple matches in the select call:\r\n```\r\ndefines = select({\r\n        \"//tensorflow:android\": [],\r\n        \"//tensorflow:arm\": [],\r\n        \"//tensorflow:ios\": [],\r\n        \"//tensorflow:linux_ppc64le\": [],\r\n        \":no_mkldnn_contraction_kernel\": [],\r\n        \"//conditions:default\": [\r\n            \"TENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL\",\r\n            \"TENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL\",\r\n        ],\r\n    }),\r\n```\r\nEither the arch-specific entries in the `select` call should be removed, or the `no_mkldnn_contraction_kernel` entry should be removed for this to work. \r\n\r\nI'll follow up with a proposed patch set.\r\n\r\n", "comments": ["@ezhulenev is already working on it. See https://github.com/tensorflow/tensorflow/pull/24416. ", "addressed with https://github.com/tensorflow/tensorflow/commit/7c9323bedc48c98be3c07b72ec1d6f4dccdefb35", "The fix for this issue was reverted with:\r\n\r\n[Temporarily disable MKL-DNN contraction kernels by default] https://github.com/tensorflow/tensorflow/commit/ca8791fc6db991c6c7406f3274ca742d867e0c6b\r\n\r\nTemporarily, but its been almost a month.\r\n\r\nI had suggested a fix that made sense to me: https://github.com/tensorflow/tensorflow/pull/24416\r\nBut it wasn't accepted.\r\n\r\n@penpornk What is the plan for this setting in the .bazelrc ?  If the idea is to switch it on and off regularly, please know that it breaks all non Intel builds.\r\n\r\nPlease advise.", "The problem is `tensorflow_mkldnn_contraction_kernel` is only ever defined if on x86_64. So setting it at all (on or off) is a problem for other arches. My attempted fix in https://github.com/tensorflow/tensorflow/pull/24416 was to let that be defined for other arches too, so that disabling it will have an effect.\r\n\r\nIf I could get clarity on what was unacceptable in https://github.com/tensorflow/tensorflow/pull/24416 perhaps I could change it so this setting would work for everyone. Any feedback?", "I'm so sorry for the inconvenience! I forgot that disabling the feature would reintroduce this issue.\r\n\r\n> @penpornk What is the plan for this setting in the .bazelrc ? If the idea is to switch it on and off regularly, please know that it breaks all non Intel builds.\r\n\r\nWe planned to leave it on indefinitely. I switched it off to debug an issue.\r\n\r\n> If I could get clarity on what was unacceptable in https://github.com/tensorflow/tensorflow/pull/24416 perhaps I could change it so this setting would work for everyone. Any feedback?\r\n\r\nI apologize that we didn't explain properly. Back then, we were about to turn the feature on (i.e., not matching with `no_mkldnn_contraction_kernel`). The following targets don't support the feature so we had to keep them to prevent matching with `//conditions:default`\r\n```\r\n        \"//tensorflow:android\": [],\r\n        \"//tensorflow:arm\": [],\r\n        \"//tensorflow:ios\": [],\r\n        \"//tensorflow:linux_ppc64le\": [],\r\n```\r\nI'll delete these targets for now and add them back when I re-enable the feature.\r\n\r\nIf you'd like to contribute to a permanent fix, the conditions are\r\n- Never match with `//conditions:default` for `//tensorflow:android`, `//tensorflow:arm`, `//tensorflow:ios`, and `//tensorflow:linux_ppc64le`, regardless of the `no_mkldnn_contraction_kernel` target.\r\n- If none of the above four target matches, match with `no_mkldnn_contraction_kernel` iff `--define=tensorflow_mkldnn_contraction_kernel=0` is set.\r\n\r\nThank you again for raising this issue!\r\n", "@jayfurmanek Please let me know whether https://github.com/tensorflow/tensorflow/commit/1a26797203e62390623756fd67e69e7665b69389 has solved the problem. Thank you!", "Thanks @penpornk ! What you did in https://github.com/tensorflow/tensorflow/commit/1a26797203e62390623756fd67e69e7665b69389 makes sense to me. I'll give it a shot on ppc64le to confirm.", "Confirmed. No dual match on ppc64le when `--define=tensorflow_mkldnn_contraction_kernel=0` is set.\r\nThanks @penpornk.\r\n\r\nClosing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=24414\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=24414\">No</a>\n", "@jayfurmanek Great. Thank you again for bringing this up and sorry it took us this long!"]}, {"number": 24413, "title": "The gradient op of bias_add supports 3/4/5D NCHW format", "body": "Ref: #22127, #23004", "comments": ["Probably not related to your changes, but\r\n\r\n//tensorflow/python/kernel_tests/distributions:student_t_test\r\n//tensorflow/contrib/constrained_optimization:candidates_test\r\n\r\nare failing. Can you rebase to master and try again?", "Thank you, Frank.\r\nYes, unrelated failures. Because [MacOs Build](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html) of master branch also fails, I doubt whether it works (rebase). ", "@frankchn Still fails. But I think they are unrelated.", ":( -- can you rebase to head and try again?", "Sure :-)"]}, {"number": 24412, "title": "TPU support is incomplete", "body": "TensorFlow version (use command below): **2.0 preview**\r\n\r\nTPU support is work in progress, and the 2.0 preview does not yet contain a DistributionStrategy for TPU.\r\n\r\nThis is a tracking issue which will be updated when progress is made on this issue.", "comments": ["Hello @martinwicke, Thanks for set up this thread for the main tracking issue for TPU support with TF 2.0.\r\n\r\nDo we have any plan of ETA now?", "@jhseu Any timeline you can share?", "It works right now at master, but we don't have a matching Cloud TPU release. We'll release an official Cloud TPU version alongside TF 2.0 final.", "@jhseu Thanks for letting me know that the master had already worked!\r\n\r\nDo we have any code example to show how it works in TF 2.0 with TPU?\r\n\r\nA demo with serval lines of core API calls will be enough, thanks!", "@huan Yeah, there's an example here:\r\nhttps://www.tensorflow.org/guide/distribute_strategy\r\n\r\nYou would use TPUStrategy instead of MirroredStrategy.", "@jhseu Hi, Is it work on Colab TPU ? I got this error \"InvalidArgumentError: /job:tpu_worker/replica:0/task:1/device:CPU:0 unknown device.\"", "@jhseu @TTaEE Same problem here. \r\n\r\nIt seems that there is an issue with the job_name parameter in TPUClusterResolver. \r\nEither 'worker' or 'tpu_worker' don't work when using the TPUStrategy scope() method, in combination with a call to tf.config.experimental_connect_to_host.\r\n\r\nI have submitted a bug report at #27992 , but it would be super helful to get a working notebook using TF 2.0 and TPUStrategy on Colab.\r\n", "The following code generates an exception on Colab with Tensorflow version 2.0.0-dev20190421 when instatiating a basic Keras model with the scope of a TPUStrategy.\r\n\r\nValueError: variable object with name 'cd2c89b7-88b7-44c8-ad83-06c2a9158347' already created. Use get_variable() if reuse is desired.\r\n\r\n\r\n\r\n```\r\n!pip install --upgrade tensorflow==2.0.0-alpha0\r\n!pip install --upgrade tf-nightly-2.0-preview\r\n\r\nimport tensorflow as tf\r\nimport os\r\nimport sys\r\n\r\nprint(\"Tensorflow version \" + tf.__version__)\r\n\r\nTPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\ntf.config.experimental_connect_to_host(TPU_WORKER)\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver() \r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\ndevices=tf.config.experimental_list_devices()\r\nprint(*devices,sep=\"\\n\")\r\n\r\nwith strategy.scope():\r\n  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\r\n  model.compile(loss='mse', optimizer='sgd')\r\n\r\n```", "It would be good to have one working example with TF2.0.\r\n1. This is a great article, but unfortunately, the code - [does not work with TF2.0 ](https://medium.com/tensorflow/tf-keras-on-tpus-on-colab-674367932aa0)\r\n[see](https://colab.research.google.com/drive/1kQLwk1Y1IIBVMp3SWyNA-KZzHI4REHWV#scrollTo=G-piuNdoWJGS)\r\ntpu_model = tf.contrib.tpu.keras_to_tpu_model(\r\n    model,\r\n    strategy=tf.contrib.tpu.TPUDistributionStrategy(\r\n        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))\r\n\r\nKeras support is now deprecated in support of TPU Strategy. Please follow the distribution strategy guide on tensorflow.org to migrate to the 2.0 supported version.\r\n\r\n2. On the other hand the example here [on distribution strategy ](https://www.tensorflow.org/guide/distribute_strategy)\r\ndoes not seem to work [either](https://colab.research.google.com/drive/1NPCfh6ztGNJHTEMyHjlk1n_QaukA5Ptr#scrollTo=S954A0R22Ct2)\r\nas already mentioned above\r\n", "Given that TF 2.0 beta is now out, is there an update with regard to this issue (about either the status or the timeline of TPU support)? ", "@lukemelas +1\r\n\r\nA roadmap or ETA would be very helpful for the cloud TPU fans!", "With reference to #29550, TPUStrategy in Tensorflow 2.0 Beta has not been working for me.", "Is there any progress for the TPU usability for TF 2.0 RC?\r\n\r\nWould love to hear from the TF team about the TPU support plan because I still can not find any news after did a hard search on the internet.", "Yeah, the gist is that we intend to announce support for TPUStrategy alongside TensorFlow 2.1. TensorFlow 2.0 will work under limited use-cases but has many improvements (bug fixes, performance improvements) that we're including in TensorFlow 2.1, so we don't consider it ready yet.\r\n\r\nWe have some examples of usage here:\r\nCustom training loop: https://github.com/tensorflow/tpu/blob/master/models/experimental/resnet50_keras/resnet50_ctl_tf2.py\r\nKeras compile/fit:\r\nhttps://github.com/tensorflow/tpu/blob/master/models/experimental/resnet50_keras/resnet50_tf2.py", "@jhseu Thank you very much for the update information and the examples, it is great to know that we will officially announce support for TPUStrategy with TF 2.1!\r\n\r\nI had just explored the examples you provided, it's great, and it will be greater if we could have a Colab to demonstrate those codes in notebook online because the Colab will be easy to get started.\r\n\r\nDo we have any Colab Notebook to demo the TF 2.0 with TPU right now?", "Hey everyone, I can confirm that I was able to train a custom keras model on TPUs, using `tf-nightly-gpu-preview-2.0`. and the `tf.distribute.experimental.TPUStrategy`.\r\nI faced a lot of issues in the process:\r\n1) I had to wrap my model with the `Sequential` API, otherwise I was getting an error stating that `functional model was expected, but got subclass Model instead`. \r\n2) The slicing operation does not seem to work on TPUs. My output layer needs to return the first token out of a sequence of tokens, so I was basically doing something like that: `return x[:, 0, :]`. However, this returns the strange error: `FetchOutputs node strided_slice/stack:0: not found`. Moving this operation to the loss function resolved the issue, but I can't understand why is that the case.\r\n3) I needed to build the model before compiling it, however that was not the case when training on GPUs.\r\n\r\nI am wondering whether the aforementioned barriers are because of me not understanding fully the documentation of training on TPUs with Tensorflow 2, or because of actual bugs that would be resolved when TF 2.1 will become available. At any case, kudos for your excellent work with TF2 and I truly hope that TPUs training will be supported fully soon.", "Is there any super simple example of using a TPU in Google colab? It seems so hard to get going in comparison to a GPU, I've been using a tonne of code online trying to get a pipeline that works, with no luck. ", "@EoinKenny This is not specifically for 2.0 but it's a TPU in colab tutorial.  https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/custom_training.ipynb", "Almost a year has passed. Is there any update about running `TF 2.x` on `TPU`?", "Same question. Was thinking to run TF2.0 on Colab TPU.", "We're targeting this for 2.1.\n", "> We're targeting this for 2.1.\r\n\r\nAre you going to update your tutorials `https://cloud.google.com/tpu/docs/colabs` with `2.1` release?", "Hello. I would like to run my model on TPU. As I see here, TPUs from Colab doesn't work with TF2. Can I use for this TPUs from Google Cloud services?", "According to the release notes of v2.1.0:\r\n\r\n> Experimental support for Keras .compile, .fit, .evaluate, and .predict is available for Cloud TPUs, Cloud TPU, for all types of Keras models (sequential, functional and subclassing models).\r\n\r\nSo i guess this issue is kind of solved?", "Wath is work with tensorfloat in Jupiter Lab?", "I\u00b4m work in conda\r\n", "Thanks and regards for alll", "Yes, TPUs work with TF2.1 now. The guide can be found here https://www.tensorflow.org/guide/tpu"]}, {"number": 24411, "title": "r1.13-rc0 cherrypick request: Raise warning for dataset pipeline in different graphs", "body": "This cherry pick adds a warning to dataset pipelines that are split across different graphs and warns the user that this behavior would lead to an error in later releases of tensorflow. There is a change in the semantics of dataset construction coming up and this would help prepare users for this change in advance.\r\n\r\nPiperOrigin-RevId: 225875718", "comments": []}, {"number": 24410, "title": "Euclidean Distance Transform ", "body": "Given the importance and wide use of the distance transform of binary images in different computer vision applications, I wonder if it would be possible to add a Euclidean distance transform with gradient support. \r\n\r\nThere exists very nice O(n) solutions already implemented in C/C++. For example. Opencv has implemented this [paper](https://www.realestatetrading.com/storage/documents/Borgefors86__Work.pdf) [here](https://docs.opencv.org/4.0.0/d7/d1b/group__imgproc__misc.html#ga8a0b7fdfcb7a13dde018988ba3a43042). \r\n\r\nHere's another fast [implementation](https://diplib.github.io/diplib-docs/group__distance.html#ga5feb444b79b00710cdd5b371d55fd221) in PyDIP. \r\n\r\nThis would be a tremendously useful feature. Thanks! ", "comments": ["@martinwicke @ebrevdo @rmlarsen do you know if there's a TensorFlower who'd be interested in this?", "I am  a beginner contributer?Is this suitable  for  first timer?Which  lebel  for  first time or beginner in  tensorflow?", "Unfortunately I do not think this is a good project for a beginner. This is probably best for someone with experience writing TensorFlow ops.", "@skye I've written a few TensorFlow ops, could I take up this issue?", "Hi @Squadrick, I'm trying to find someone on the TF team who would be a good reviewer for this (and also give a better estimate of how much work this is, etc.). In the meantime though, you could start development in https://github.com/tensorflow/addons if you like.", "@sofienbouaziz would be a good reviewer for an op. I would suggest starting in tensorflow/addons.\r\n\r\n@seanpmorgan FYI.", "I'll get started right away. ", "Are there any updates on this? This will be an amazing feature when available!", "Following :) ", "Hey, guys. Should be done within this week. I'll update this thread once I open a PR. \r\n", "Thank you so much for your great work.", "Is there an update on this feature by any chance? Thanks!", "@skye Sorry about the delay, I've created a PR with a barebones implementation. I'll add faster algorithms and test cases over the next few hours. ", "Great job !  How would we be able to access this custom_op ? ", "@ahatamiz \r\n\r\nThe changes haven't been merged into the main repository yet, so clone my [repository](https://github.com/Squadrick/addons), and switch to this branch: `euclidean-distance-transform`.\r\n\r\n[Here's](https://github.com/Squadrick/addons/blob/euclidean-distance-transform/README.md#installation) the build guide for tensorflow_addons.\r\n\r\nThen you can use the transform as such:\r\n```\r\nfrom tensorflow_addons.custom_ops.image import euclidean_dist_transform\r\n\r\nx = ... # binary image\r\ny = euclidean_dist_transform(x)\r\n```\r\n\r\nNote:\r\n\r\n1. `x` must be a binary image (0 and 1 only)\r\n2. `x` must be of type `tf.uint8`. Use `tf.cast` if it isn't.\r\n3. `x` must be of shape `[N, H, W, 1]` or `[H, W, 1]`\r\n4. The dtype of `y` can be set like this: `y = euclidean_dist_transform(x, dtype=tf.float32)`. `tf.float32` is the default.\r\n5. The current algorithm is of quadratic complexity `O(N^2)` where `N` is the number of pixels. I'm working on the faster `O(N)` algorithm right now, and I'll push it soon.\r\n6. The Op is currently CPU only. \r\n7. The Op is not differentiable, and running `tf.gradients(y)` will return `[None]`.\r\n\r\nLet me know if you find any problems or issues with the Op.", "@Squadrick  \r\n\r\nThat's amazing ! this op will be tremendously helpful for many researchers and I am sure people will appreciate its importance soon ! \r\n\r\nThe only concern though is the fact that  this Op is non-differentiable. It would be great if we could have gradients defined for it.\r\n\r\nIn many applications, this op will be needed for connecting two layers in a sense that it initializes the second layer with the output of the first layer. In essence, both of the layers have learn-able parameters thus not being able to back-propagate the error through this op would limit its use. ", "@Squadrick awesome! Once it's ready with full test cases etc., maybe ping @sofienbouaziz to take a look.", "@skye Sure thing.\r\n\r\n@ahatamiz Couldn\u2019t really figure out what the gradient of the operation would be, considering the fact that the function is defined on binary images. If you could help out with that part, I can implement it fairly easily. ", "@Squadrick Thanks for the contribution! Just quickly parsing through the code I have the same comment than @ahatamiz that this op is non-differentiable and it would be great if we could back-propagate through it. The algorithm currently used is O(n^2)  is the goal to then implement a O(n) algorithm?", "@sofienbouaziz I'm working on the `O(n)` algorithm right now, I'll push it within a day. \r\n\r\nLike I mentioned before, I'm not sure how this op can be differentiable considering the fact that the op is defined on binary images.", "@Squadrick Awesome for the O(n) version! Let me think about how to differentiate this op during the week-end. Just out of curiosity do you think it would be possible to develop this op purely in python?", "@sofienbouaziz I think @Squadrick is right that gradients make no sense for a binary input. However, there is an [generalization for arbitrary functions](http://www.theoryofcomputing.org/articles/v008a019/v008a019.pdf) (that you and @ataiya [implemented?](https://github.com/ataiya/dtform)) that if I understand correctly should be differentiable. If there was an op for this you would also get the binary version for free by composing it with an indicator function.\r\n\r\nI'm not sure if the generalized version would do exactly what @ahatamiz wants, but it would be very nice to have an op for.", "As an experiment I tried [implementing a simple version of the generalized transform in python](https://colab.research.google.com/drive/1-Fykc6FVi3pUtFx7spP3OAdn267TKwbD), and it is indeed differentiable. With a careful choice of indicator function it seems you can even \"learn\" a binary image by supervising its distance transform, so it might be suitable for use as a layer in a network as @ahatamiz suggested.\r\n\r\nBelow is an example showing the gradients. Here, the mask is learned from an L2 loss between the DT of the mask and the DT of the ground truth mask.\r\n\r\n![smiley](https://user-images.githubusercontent.com/5511462/54079136-8aaa5800-42a4-11e9-9903-30bca37fce31.gif)\r\n", "Thanks @Squadrick for you great work on this and thanks @sofienbouaziz for taking a look into this. Also thanks for your thoughtful contribution to the discussion @drebain  \r\n\r\n@Squadrick, you are right in the sense that the op may not be differentiable if we were to utilize on binary images only. However,  what I had in mind is very close the implementation by @drebain. One can pass the output of sigmoid, before thresholding, to a (generalized) distance transform layer and still allow for gradient flow. This is useful since in most vision applications, the output of sigmoid (or softmax) can be fed into this layer for more interesting operation down-stream. \r\n\r\nThe implementation by @drebain is definitely correct, but unfortunately time and space-wise, we will run into some issues due its O(n^2) complexity.  I did one experiment on an image of size 256 by 256 on TitanXP and it ran out of memory.\r\n\r\nIn any events, thanks all for your efforts to make this a Tensorflow op. It will become a highly utilized and sought-after op due to its many applications. ", "According to its documentation, the default O(n) L2 implementation in OpenCV uses the multi-pass algorithm based on the generalized transform, so if @Squadrick has implemented that, then I think all that would need to be done is to expose a version of the op that doesn't apply the thresholding indicator function.", "> @ahatamiz\r\n> \r\n> The changes haven't been merged into the main repository yet, so clone my [repository](https://github.com/Squadrick/addons), and switch to this branch: `euclidean-distance-transform`.\r\n> \r\n> [Here's](https://github.com/Squadrick/addons/blob/euclidean-distance-transform/README.md#installation) the build guide for tensorflow_addons.\r\n> \r\n> Then you can use the transform as such:\r\n> \r\n> ```\r\n> from tensorflow_addons.custom_ops.image import euclidean_dist_transform\r\n> \r\n> x = ... # binary image\r\n> y = euclidean_dist_transform(x)\r\n> ```\r\n> Note:\r\n> \r\n> 1. `x` must be a binary image (0 and 1 only)\r\n> 2. `x` must be of type `tf.uint8`. Use `tf.cast` if it isn't.\r\n> 3. `x` must be of shape `[N, H, W, 1]` or `[H, W, 1]`\r\n> 4. The dtype of `y` can be set like this: `y = euclidean_dist_transform(x, dtype=tf.float32)`. `tf.float32` is the default.\r\n> 5. The current algorithm is of quadratic complexity `O(N^2)` where `N` is the number of pixels. I'm working on the faster `O(N)` algorithm right now, and I'll push it soon.\r\n> 6. The Op is currently CPU only.\r\n> 7. The Op is not differentiable, and running `tf.gradients(y)` will return `[None]`.\r\n> \r\n> Let me know if you find any problems or issues with the Op.\r\n\r\n@Squadrick . I cloned the branch you mentioned and built from the source, but it seems like we are missing a file: \r\n\r\n`/addons-euclidean-distance-transform/tensorflow_addons/custom_ops/image/python/_image_ops.so  cannot open shared object file: No such file or directory`\r\n\r\nAlso, as @drebain  mentioned, I think it would be helpful to also allow for feeding logits before sigmoid or softmax and not just binary images ( for a back-propagating error ) . Currently, this layer seems to only accept binary images.  ", "@ahatamiz I haven't gotten to fully review the PR, but I recognize that error and you likely just need to move out of the src directory after pip install. The .so files aren't built until bazel compile and your current working directory does not contain them", "> @ahatamiz I haven't gotten to fully review the PR, but I recognize that error and you likely just need to move out of the src directory after pip install. The .so files aren't built until bazel compile and your current working directory does not contain them\r\n\r\nThanks @seanpmorgan . However I also have an issue with version incompatblity as only Tensorflow 2.0.0 is allowed. I have not yet transferred everything to Tensorflow 2.0.0 and was hoping to get it working on older versions.  ", "@ahatamiz Ah, thats a bit of a problem. Addons is only scoped to be TF 2.x+ and it'll take a lot of convincing to get us to reconsider that. You can however make it work on your own fork of the repo by altering this line:\r\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/__init__.py#L54", "@drebain Very cool colab! Indeed the [code](https://github.com/ataiya/dtform) you are referring to is an old implementation I did of this particular [paper](http://www.theoryofcomputing.org/articles/v008a019/v008a019.pdf) and as you pointed out it is differentiable. Yes, you are right that the gradient doesn't make sense in the case of binary image, however I was thinking we could do something similar than what has been done for quantization where the forward pass would be computed on the binary image and the backward pass on some sort of relaxation. ", "I am closing this issue as the Euclidean Distance Transform op was merged. If updates are needed let's reopen one with the necessary details. \r\n@Squadrick Thanks for this very useful op.", "Thanks you @Squadrick  and @sofienbouaziz . \r\n\r\nIs it also possible to provide GPU support for this op ? ", "@ahatamiz GPU support for TensorFlow Addons is currently WIP. Ref [#294](https://github.com/tensorflow/addons/pull/294). Once this is merged, I'll add GPU support.", "Hi,\r\nI just want to confirm.  Can the current implementation of the TF addons Euclidean Distance Transform be used in a loss function?  I noticed the discussion above on the issue of gradients when the distance transform is applied on binary images.  Was this resolved? ", "@emckenzi123 No. The current implementation of euclidean distance transform does not have a gradient defined.", "@emckenzi123 If you do not need to run on large images, the code in my notebook from above ( https://colab.research.google.com/drive/1-Fykc6FVi3pUtFx7spP3OAdn267TKwbD ) might work as a loss function. You do need to allow arbitrary real values for pixels (not just binary) in order for the gradients of a distance transform to be meaningful though.", "@drebain I tried your posted method (thank you for making that available) however I ran into an OOM error since my images are large.  I looked into the [paper](http://www.theoryofcomputing.org/articles/v008a019/v008a019.pdf) you linked to, and found a python version [here](https://github.com/hbristow/distance-transform).  It uses Cython, and I naively tried to just use this code in a training loop's loss, without success (no training of weights).  I went through and tried to replace ops with tensorflow ops to see if I could get something, but that just ended up being really slow.  I should note that if I use the Cython code outside of tensorflow, it's super fast.  I could try to keep working on this, or do you guys think it's actually non-trivial to turn the linked python code into something TF can use in a loss function?", "@emckenzi123 I haven't used Cython and TF together before, but I think you could write a wrapper around the code you linked that manually defines the gradient using [tf.py_function](https://www.tensorflow.org/api_docs/python/tf/py_function) and [tf.custom_gradient](https://www.tensorflow.org/api_docs/python/tf/custom_gradient). If I'm reading correctly that the code gives you the index of the pixel corresponding to the minimum of d(p,q)+f(q) you should be able to compute the gradient. That would let you keep the Cython implementation I think, while still being differentiable.", "Thank you so much for your help.  I defined the gradient to be 2 * sqrt(distance transform), since the derivative of (p-q)**2 is 2 * (p-q).  \r\nIn case someone in the future runs into something similar, I have used @drebain 's previous colab notebook as a template to demonstrate the code for this.  The link is [here](https://colab.research.google.com/drive/13IxLV__fl20fWa3nCBruuYTcDH0-kwM5?usp=sharing)\r\n\r\nedit: FYI this was done using TF 2.4.0 in case that matters for future people", "Needed a tensorflow op that did the generalized edt (i.e. `out[p] = min_q (||q-p||^2 + in[q])`) .  Implemented here, CPU/GPU, float32/float64.  \r\n\r\nhttps://github.com/jacksonloper/tensorflow-felzenszwalb-edt\r\n\r\nWheel here\r\n\r\nhttps://github.com/jacksonloper/tensorflow-felzenszwalb-edt/releases/download/v0.0.1/tensorflow_felzenszwalb_edt-0.0.1-py37-none-linux_x86_64.whl\r\n\r\nCaveat emptor, haven't tested it on any platforms besides my own.  No idea how it compares in practice in speed to the excellent (non-differentiable) morphological edt op (i.e. `out[p] = min_q (||q-p||^2 + infinity*in[q])`) implemented by @Squadrick.\r\n\r\nI guess if it seems like there is sufficient interest, I could package it up and submit PR.  Dunno what common practice is here... e.g. if 2000 people have downloaded the release, then maybe it is worth doing?"]}, {"number": 24409, "title": "Partially-defined shapes and group_norm", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `v1.12.0-0-ga6d8ffae09 1.12.0`\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: 1080 Ti\r\n\r\n**Describe the current behavior**\r\nGiven the following minimal code raises an error:\r\n```python\r\nscan_var = tf.placeholder(tf.float32, [None, 128, 128, 64], name='scan')\r\nnorm = tf.contrib.layers.group_norm(scan_var, groups=16, channels_axis=3, reduction_axes=(1, 2))\r\n```\r\nWhen using `tf.contrib.layers.group_norm` with a Tensor that has partially-defined shape (e.g. `[None, H, W, C]`), the following error is raised:\r\n```\r\nTypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [None, 128, 128, 16, 4]. Consider casting elements to a supported type.\r\n```\r\nThe error is due to [normalization.py#L303](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/layers/python/layers/normalization.py#L303), since the value of variable `inputs_shape` is `[None, 128, 128, 16, 4]` which cannot be converted to a Tensor. The correct value should be `[-1, 128, 128, 16, 4]`.\r\nThis error can be fixed (probably addressing this [TODO](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/layers/python/layers/normalization.py#L250)) with the addition of the two following lines after Line 252 in `normalization.py`:\r\n```python\r\noriginal_shape = [-1 if s.value is None else s for s in original_shape]\r\nif len([s for s in original_shape if s == -1]) > 1:\r\n    raise ValueError('Only one axis dimension can be undefined in the input tensor')\r\n```\r\nand the following line after Line 302:\r\n```python\r\ninputs_shape = [-1 if s is None else s for s in inputs_shape]\r\n```\r\n\r\nIf it seems reasonable I can submit a pull request.", "comments": ["`contrib.layers.group_norm` might get removed from `tf.contrib` soon, according to https://github.com/tensorflow/community/pull/37. Therefore it may not be worth it to fix it.\r\nOr you can ask the team to keep it in TF in the above PR. It's currently in the state of \" Code that will not be moved from tf.contrib (to addons) pending objections\"\r\n\r\nI authored the paper and my version of group norm (which handles partially-defined batch dimension) in TF is [here](https://github.com/tensorpack/tensorpack/blob/79148350eabd6800133a49b101eea4c56e78e4e8/examples/ImageNetModels/vgg16.py#L19-L46).", "Thank you for the reply @ppwwyyxx. \r\nI also found this PR for fixing this exact problem: https://github.com/tensorflow/tensorflow/pull/21390\r\nHowever, I think a PR needs to be submitted to addons when they start moving `tf.contrib` to `addons` and if they decide to keep `group_norm`.\r\n", "@ppwwyyxx @erfannoury We'll be adding GroupNorm as a generalized layer_norm and instance_norm in Addons. tensorflow/addons#6\r\n\r\nIt'll be refactored though in a similar manner to the keras_contrib [GroupNormalization](https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/layers/normalization.py#L415), so I'm not sure this issue will still be valid in the new implementation. We will at least include a test for it.", "@seanpmorgan this is a good idea. I will then follow the progress of tensorflow/addons#6 for the new implementation of group normalization. This issue probably can be closed then.", "Hey Guys!! I am making my own object detector and for that when I go for the training part, I am getting this error:\r\n**Traceback (most recent call last):\r\n  File \"train.py\", line 51, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"C:\\Users\\Bilal\\Documents\\models\\research\\object_detection\\builders\\model_builder.py\", line 52, in <module>\r\n    from object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobileNetV2FeatureExtractor\r\n  File \"C:\\Users\\Bilal\\Documents\\models\\research\\object_detection\\models\\ssd_mobilenet_v2_feature_extractor.py\", line 26, in <module>\r\n    from nets.mobilenet import mobilenet_v2\r\n  File \"C:\\Users\\Bilal\\Documents\\models\\research\\slim\\nets\\mobilenet\\mobilenet_v2.py\", line 89, in <module>\r\n    'normalizer_fn': tf.contrib.layers.group_norm,  # pylint: disable=C0330\r\nAttributeError: module 'tensorflow.contrib.layers' has no attribute 'group_norm'**\r\n\r\nAny help ? ", "As mentioned above, the function is removed.", "what's the alternative?", "As mentioned above, an alternative is to use my version of group norm (which handles partially-defined batch dimension).", "from nets.mobilenet import mobilenet_v2\r\n  File \"C:\\tensorflow\\models\\research\\slim\\nets\\mobilenet\\mobilenet_v2.py\", line 91, in <module>\r\n    'normalizer_fn': contrib_layers.group_norm,  # pylint: disable=C0330\r\nAttributeError: module 'tensorflow.contrib.layers' has no attribute 'group_norm'\r\n\r\n\r\nany help\r\n\r\n", "> from nets.mobilenet import mobilenet_v2\r\n> File \"C:\\tensorflow\\models\\research\\slim\\nets\\mobilenet\\mobilenet_v2.py\", line 91, in\r\n> 'normalizer_fn': contrib_layers.group_norm, # pylint: disable=C0330\r\n> AttributeError: module 'tensorflow.contrib.layers' has no attribute 'group_norm'\r\n> \r\n> any help\r\n\r\ndo you have the solution now???\r\n", "I've got a problem when using slim.group_norm as 'normalizer_fn=slim.group_norm' for many conv layers, it cannot get the value of batch_size, h and w, so that it cannot use -1 to solve this problem. Did anyone meet this problem?", "@erfannoury ,\r\nIs this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.6 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24409\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24409\">No</a>\n"]}, {"number": 24408, "title": "CheckpointInputPipelineHook not restoring the correct iterator state", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs / Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.11 / 1.12\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\n1. CheckpointInputPipelineHook is not restoring the input checkpoint if there is multiple \"estimator.train\" method calls. Probably because the restore of the input checkpoint is done during the init of the hook. It restarts from the beginning of the dataset.\r\n2. If both MirroredStrategy and CheckpointInputPipelineHook are used, one step is by passing when the input checkpoint is restored. It restarts from (state step + 1).\r\n\r\n**Describe the expected behavior**\r\nAfter restoring the input checkpoint, the train restart at the right dataset iterator step.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nIn this sample code, I have one dataset (range between 0 and 7, batch of 1, 1 epoch), I train the model for 4 steps, stop it, restart for 4 steps the training by restoring the checkpoints (model and input).\r\nExpected:\r\nTrain First Run: 4 steps -> features 0 to 3\r\nSave checkpoint model-ckpt and input-ckpt 4\r\nRestore model-ckpt and input-ckpt 4\r\nTrain Second Run: 4 steps -> features 4 to 7\r\nSave checkpoint 8\r\n```\r\nimport tensorflow as tf\r\n\r\nsample_count = 8\r\n\r\n\r\ndef run(model_dir: str, reinit_hook: bool = False, distribution_strategy: bool = False, gpu_count: int = 1):\r\n    # Config\r\n    if distribution_strategy:\r\n        session_config = tf.ConfigProto(allow_soft_placement=True)\r\n        train_distribute = tf.contrib.distribute.MirroredStrategy(num_gpus=gpu_count)\r\n    else:\r\n        session_config = None\r\n        train_distribute = None\r\n\r\n    run_config = tf.estimator.RunConfig(\r\n        model_dir=model_dir,\r\n        session_config=session_config,\r\n        train_distribute=train_distribute)\r\n\r\n    # Estimator\r\n    estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)\r\n\r\n    # Hook\r\n    hooks = [tf.contrib.data.CheckpointInputPipelineHook(estimator)]\r\n\r\n    # First Run 1/2\r\n    estimator.train(input_fn=train_input_fn,\r\n                    steps=sample_count // 2 // gpu_count,\r\n                    hooks=hooks)\r\n\r\n    # Reinit Hook\r\n    if reinit_hook:\r\n        hooks = [tf.contrib.data.CheckpointInputPipelineHook(estimator)]\r\n\r\n    # Second Run 1/2\r\n    estimator.train(input_fn=train_input_fn,\r\n                    steps=sample_count // 2 // gpu_count,\r\n                    hooks=hooks)\r\n\r\n\r\ndef train_input_fn():\r\n    return tf.data.Dataset \\\r\n        .range(sample_count) \\\r\n        .map(lambda x: (x, x * 2)) \\\r\n        .batch(1) \\\r\n        .repeat(1)\r\n\r\n\r\ndef model_fn(features, labels, mode):\r\n    input_layer = tf.cast(tf.reshape(features, [-1, 1]), tf.float32)\r\n    expected_output = tf.cast(tf.reshape(labels, [-1, 1]), tf.float32)\r\n\r\n    logit = tf.layers.dense(input_layer, 1, None, False)\r\n    loss = tf.losses.mean_squared_error(expected_output, logit)\r\n\r\n    logging_hook = tf.train.LoggingTensorHook(tensors={\"feature_value\": features.name}, every_n_iter=1)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = tf.train.AdamOptimizer(0.001)\r\n        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          train_op=train_op,\r\n                                          training_hooks=[logging_hook])\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.logging.set_verbosity(tf.logging.DEBUG)\r\n\r\n    # Expected:\r\n    # Train First Run: 4 steps -> features 0 to 3\r\n    # Save checkpoint model-ckpt and input-ckpt 4\r\n    # Restore model-ckpt and input-ckpt 4\r\n    # Train Second Run: 4 steps -> features 4 to 7\r\n    # Save checkpoint 8\r\n\r\n    # run_1: 1 gpu / no distribution strategy / Only one init of the hook\r\n    # Actual KO: dataset restarts from 0\r\n    # \r\n    # Train First Run: 4 steps -> features 0 to 3\r\n    # Save checkpoint model-ckpt and input-ckpt 4\r\n    # Restore model-ckpt 4\r\n    # Train Second Run: 4 steps -> features 0 to 3\r\n    # Save checkpoint 8\r\n    run(model_dir='output_run_1', reinit_hook=False, distribution_strategy=False)\r\n\r\n    # run_2: 1 gpu / no distribution strategy / reinit of the hook between two trainings\r\n    # Actual OK\r\n    # Train First Run: 4 steps -> features 0 to 3\r\n    # Save checkpoint model-ckpt and input-ckpt 4\r\n    # Restore model-ckpt and input-ckpt 4\r\n    # Train Second Run: 4 steps -> features 4 to 7\r\n    # Save checkpoint 8\r\n    run(model_dir='output_run_2', reinit_hook=True, distribution_strategy=False)\r\n\r\n    # run_3:\r\n    # Actual KO: dataset restarts from iterator state step + 1\r\n    # Train First Run: 4 steps -> features 0 to 3\r\n    # Save checkpoint model-ckpt and input-ckpt 4\r\n    # Restore model-ckpt and input-ckpt 4\r\n    # Train Second Run: 3 steps -> features 5 to 7\r\n    # Save checkpoint 7\r\n    run(model_dir='output_run_3', reinit_hook=True, distribution_strategy=True)\r\n\r\n    # run_4:\r\n    # Actual KO: dataset restarts from iterator state step + 1\r\n    # Train First Run: 2 steps -> features 0 to 3\r\n    # Save checkpoint model-ckpt and input-ckpt 2\r\n    # Restore model-ckpt and input-ckpt 2\r\n    # Train Second Run: 1 steps -> features 6 to 7\r\n    # Save checkpoint 3\r\n    run(model_dir='output_run_4', reinit_hook=True, distribution_strategy=True, gpu_count=2)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi all,\r\nAny updates on this? Or any workaround?\r\nI couldn't find a way to do preemption on distributed training.\r\n\r\nThanks.\r\nDenis.", "@rohan100jain is this something you can look into? (or someone else from tf.data?)", "Firstly, run_2 onwards (i.e. reinit_hook=True) is the way to go. \r\n\r\nSo run_2 seems to work and in the non-distribution strategies case, things seem to be working fine.\r\n\r\nFor the distribution strategies case, the problem is that it uses MultiDeviceIterator (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/multi_device_iterator_ops.py#L190) (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/multi_device_iterator_ops.cc#L43) under the hood who's state we currently don't checkpoint. This is something we need to fix on our end and will require a non-trivial amount of work. I currently don't have cycles to spend on this but contributions are welcome! ", "@rohan100jain Thanks for the update. I will try to take a look. Is there any tests I can rely one?", "Hi @Silb78dg! \r\nHi ! we are  checking to see if you are still looking for assistance in this issue.\r\nCould you please try on latest stable version of  TF 2.6  and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 24407, "title": "[TFLite] Feature request: Add support for CropAndResize op", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (or github SHA if from source): ebe98d94a5340386429da4b6e930b8cda9c7e854\r\n- Are you willing to contribute it (Yes/No): Yes, if feasible / not under current development\r\n\r\n```\r\n2018-12-17 15:14:34.782227: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 4345 operators, 7950 arrays (0 quantized)\r\n2018-12-17 15:14:35.080813: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 4054 operators, 7480 arrays (0 quantized)\r\n2018-12-17 15:14:35.472064: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 4054 operators, 7480 arrays (0 quantized)\r\n2018-12-17 15:14:35.713793: F tensorflow/lite/toco/graph_transformations/resolve_constant_slice.cc:59] Check failed: dim_size >= 1 (0 vs. 1)\r\nAborted (core dumped)\r\n```\r\n\r\nLink to a GraphDef or the model: http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2018_01_28.tar.gz\r\n\r\n#### Summary\r\nCurrently CropAndResize is not supported in the TFLite runtime, and because of this Faster R-CNN object detection models can not be deployed on mobile. Faster R-CNN has been [shown to outperform SSD models w.r.t accuracy](https://arxiv.org/pdf/1611.10012.pdf), and as compute speed increases it seems intuitive to support this architecture.\r\n\r\nMy team and I would be interested in adding this functionality, but wanted to make sure that it's feasible and not under current development. Very much related is tensorflow/models/issues/4848, where it was said that there is no fundamental issue with implementing it. ", "comments": ["CropAndResize is one of the select TF ops supported via tflite_convert.\r\n\r\nAs we work toward fleshing out the builtin op library for TensorFlow Lite, we've been working on an experimental feature that allows using select TensorFlow ops from within the TensorFlow Lite runtime. The goal is to help reduce some of the friction for using models that rely on ops not yet natively supported by TensorFlow Lite (at the cost of increased binary size). This feature requires opting in during model conversion, as well as adding an additional dependency. More details can be found [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/using_select_tf_ops.md).\r\n\r\nFeedback is very much appreciated (either via GitHub or directly via tflite@tensorflow.org), and we'll be adding and refining functionality over the coming weeks. Cheers.\r\n\r\n", "@achowdhery Thanks this is very informative. Very excited to see the whitelist:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/tflite/whitelisted_flex_ops.cc\r\n\r\n", "> @achowdhery Thanks this is very informative. Very excited to see the whitelist:\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/tflite/whitelisted_flex_ops.cc\r\n\r\nDid you finally manage to convert FRCNN to TFLITE? If so, how did you do it? I tried converting but ran into errors", "@seanpmorgan In case you were successful in converting the Faster-RCNN model to TFLite could you please provide some details for that? As per my research the the implementation of the ops required to convert the models is still not there and as of now only SSD models can be converted to TFLite.\r\n", "Hi all, \r\nAny news about custom implementation of CropAndResize operator @seanpmorgan ? or about feature allows using select TensorFlow ops from within the TensorFlow Lite runtime @achowdhery ?", "> Hi all,\r\n> Any news about custom implementation of CropAndResize operator @seanpmorgan ? or about feature allows using select TensorFlow ops from within the TensorFlow Lite runtime @achowdhery ?\r\n\r\nUse the MLIR `experimental_new_converter` flag and you can use control flow in TFlite:\r\nhttps://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter", "> @seanpmorgan In case you were successful in converting the Faster-RCNN model to TFLite could you please provide some details for that? As per my research the the implementation of the ops required to convert the models is still not there and as of now only SSD models can be converted to TFLite.\r\n\r\nAre you able to convert frcnn model to tflite? I tried and ran into many issues. Please let me know if you have succeeded"]}, {"number": 24406, "title": "Tensorflow GPU installation in Windows 10 with anaconda", "body": "**System information**\r\n- **OS Platform and Distribution**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: I don't know\r\n- **TensorFlow version**: 1.12.0\r\n- **Python version**:  3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)]\r\n- **Installed using virtualenv? pip? conda?**: I don't remember\r\n- **Bazel version (if compiling from source)**: I don't know\r\n- **GCC/Compiler version (if compiling from source)**: I don't know\r\n- **CUDA/cuDNN version**: Cuda compilation tools, release 10.0, V10.0.130 / cuDNN v. 7.4.1\r\n- **GPU model and memory**:  GeForce GT 635M   \r\n\r\n**Describe the problem**\r\n\r\nI can't install the GPU version of Tensorflow\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI try to install the GPU version of Tensorflow\r\nSo I run the following lines in a Windows command prompt\r\n\r\n```\r\nconda create --name tf_gpu\r\nactivate tf_gpu\r\nconda install tensorflow-gpu\r\n```\r\nAfter running `conda create --name tf_gpu`, I got the following message \r\n\r\n> Preparing transaction: done\r\n> Verifying transaction: \\\r\n> SafetyError: The package for setuptools located at C:\\Users\\User\\Miniconda4\\pkgs\\setuptools-40.6.2-py36_0\r\n> appears to be corrupted. The path \u2018Scripts/easy_install.exe\u2019\r\n> has a sha256 mismatch.\r\n>  reported sha256: 993203a406e04936a07829b1f482fd27d739b640482e213f4c49ea1ee78a5fcf\r\n>  actual sha256: dd0f7f55b91f4c77c6064f6ed3219e3573ae2b4ada1190094887da40d6b7fde1\r\n> SafetyError: The package for wheel located at C:\\Users\\User\\Miniconda4\\pkgs\\wheel-0.32.3-py36_0\r\n> appears to be corrupted. The path \u2018Scripts/wheel.exe\u2019\r\n> has a sha256 mismatch.\r\n>  reported sha256: 993203a406e04936a07829b1f482fd27d739b640482e213f4c49ea1ee78a5fcf\r\n>  actual sha256: e94002e198d87676ccfef870c0648d33d57bd30288ba00dbab112597e223759d\r\n> done\r\n> Executing transaction: done\r\n\r\nBut I think, that is a minor problem. However, when I run `conda install tensorflow-gpu` I get this \r\n\r\n> 2018\u201312\u201316 16:15:23.825085: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX\r\n> 2018\u201312\u201316 16:15:24.615875: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] **failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected**\r\n> 2018\u201312\u201316 16:15:24.623130: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: DESKTOP-85SA7VT\r\n> 2018\u201312\u201316 16:15:24.627495: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: DESKTOP-85SA7VT\r\n> Device mapping: no known devices.\r\n> 2018\u201312\u201316 16:15:24.645904: I tensorflow/core/common_runtime/direct_session.cc:307] Device mapping:\r\n> .\r\n> \r\n\r\nIs there something wrong with the CUDA/cuDNN installation ?\r\n\r\nTensorflow with CPU does work but not GPU Tensorflow. Thanks", "comments": ["Maybe your graphicscard is too old for current Cuda versions.", "My GPU card is  a GeForce GT 635M. It is listed in the CUDA compatible cards [here](https://developer.nvidia.com/cuda-gpus)", "> **System information**\r\n> \r\n> * **OS Platform and Distribution**: Windows 10\r\n> * **TensorFlow installed from (source or binary)**: I don't know\r\n> * **TensorFlow version**: 1.12.0\r\n> * **Python version**:  3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)]\r\n> * **Installed using virtualenv? pip? conda?**: I don't remember\r\n> * **Bazel version (if compiling from source)**: I don't know\r\n> * **GCC/Compiler version (if compiling from source)**: I don't know\r\n> * **CUDA/cuDNN version**: Cuda compilation tools, release 10.0, V10.0.130 / cuDNN v. 7.4.1\r\n> * **GPU model and memory**:  GeForce GT 635M\r\n> \r\n> **Describe the problem**\r\n> \r\n> I can't install the GPU version of Tensorflow\r\n> \r\n> **Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n> \r\n> I try to install the GPU version of Tensorflow\r\n> So I run the following lines in a Windows command prompt\r\n> \r\n> ```\r\n> conda create --name tf_gpu\r\n> activate tf_gpu\r\n> conda install tensorflow-gpu\r\n> ```\r\n> After running `conda create --name tf_gpu`, I got the following message\r\n> \r\n> > Preparing transaction: done\r\n> > Verifying transaction: \r\n> > SafetyError: The package for setuptools located at C:\\Users\\User\\Miniconda4\\pkgs\\setuptools-40.6.2-py36_0\r\n> > appears to be corrupted. The path \u2018Scripts/easy_install.exe\u2019\r\n> > has a sha256 mismatch.\r\n> > reported sha256: 993203a406e04936a07829b1f482fd27d739b640482e213f4c49ea1ee78a5fcf\r\n> > actual sha256: dd0f7f55b91f4c77c6064f6ed3219e3573ae2b4ada1190094887da40d6b7fde1\r\n> > SafetyError: The package for wheel located at C:\\Users\\User\\Miniconda4\\pkgs\\wheel-0.32.3-py36_0\r\n> > appears to be corrupted. The path \u2018Scripts/wheel.exe\u2019\r\n> > has a sha256 mismatch.\r\n> > reported sha256: 993203a406e04936a07829b1f482fd27d739b640482e213f4c49ea1ee78a5fcf\r\n> > actual sha256: e94002e198d87676ccfef870c0648d33d57bd30288ba00dbab112597e223759d\r\n> > done\r\n> > Executing transaction: done\r\n> \r\n> But I think, that is a minor problem. However, when I run `conda install tensorflow-gpu` I get this\r\n> \r\n> > 2018\u201312\u201316 16:15:23.825085: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX\r\n> > 2018\u201312\u201316 16:15:24.615875: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] **failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected**\r\n> > 2018\u201312\u201316 16:15:24.623130: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: DESKTOP-85SA7VT\r\n> > 2018\u201312\u201316 16:15:24.627495: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: DESKTOP-85SA7VT\r\n> > Device mapping: no known devices.\r\n> > 2018\u201312\u201316 16:15:24.645904: I tensorflow/core/common_runtime/direct_session.cc:307] Device mapping:\r\n> > .\r\n> \r\n> Is there something wrong with the CUDA/cuDNN installation ?\r\n> \r\n> Tensorflow with CPU does work but not GPU Tensorflow. Thanks\r\n\r\nTensorflow-GPU requires Nvidia GPUs with CUDA compute capability 3.5 or higher. The compute capability of the GT635M is only 2.1(see https://developer.nvidia.com/cuda-gpus).In fact, the GT635M uses the GF106/GF116 chip based on the Fermi architecture.Not the usual Kepler architecture of the Geforce 600 series.The Fermi architecture is too old, and now the Tensorflow GPU is no longer supported.You need to buy new hardware to use the Tensorflow GPU. In addition, Tensorflow currently installed via PIP requires CUDA 9.0, not 10.0.I hope these can solve your problem.\r\n", "@rootkitchao Thank you for your thorough explanation. By the way when installing Tensorflow with`conda` command it installs CUDA 9.0, not 10.0. \r\nDo you recommend to use `pip` or `conda` for the installation of Tensorflow ? The regular installation of Tensorflow GPU seems a bit of a struggle.", "@JanVanIm Both installation methods are OK, I did not use anaconda.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24405, "title": "Improper behaviour of recurrent batch normalization with bidirectional rnn", "body": "I have already asked this question on [stackoverflow](https://stackoverflow.com/questions/53812656/recurrent-batch-normalization-problem-to-get-the-update-op-properly) and I got no answer so that is why I'm posting it here.\r\n\r\n\r\nMy recurrent batch normalization class is as following:\r\n```python\r\nclass BNLSTMCell(RNNCell):\r\n\r\n\r\n\r\ndef __init__(\r\n    self,\r\n    num_units,\r\n    is_training=True,\r\n    use_peepholes=False,\r\n    cell_clip=None,\r\n    initializer=None,\r\n    num_proj=None,\r\n    proj_clip=None,\r\n    forget_bias=1.0,\r\n    state_is_tuple=True,\r\n    activation=tf.tanh,\r\n    reuse=None,\r\n    ):\r\n\r\n\r\n    super(BNLSTMCell, self).__init__(_reuse=reuse)\r\n    if not state_is_tuple:\r\n        tf.logging.log_first_n(tf.logging.WARN,\r\n                               '%s: Using a concatenated state is slower and  will soon be deprecated.  Use state_is_tuple=True.'\r\n                               , 1, self)\r\n\r\n    self.num_units = num_units\r\n    self.is_training = is_training\r\n    self.use_peepholes = use_peepholes\r\n    self.cell_clip = cell_clip\r\n    self.num_proj = num_proj\r\n    self.proj_clip = proj_clip\r\n    self.initializer = initializer\r\n    self.forget_bias = forget_bias\r\n    self.state_is_tuple = state_is_tuple\r\n    self.activation = activation\r\n\r\n    if num_proj:\r\n        self._state_size = (LSTMStateTuple(num_units,\r\n                            num_proj) if state_is_tuple else num_units\r\n                            + num_proj)\r\n        self._output_size = num_proj\r\n    else:\r\n        self._state_size = (LSTMStateTuple(num_units,\r\n                            num_units) if state_is_tuple else 2\r\n                            * num_units)\r\n        self._output_size = num_units\r\n\r\n@property\r\ndef state_size(self):\r\n    return self._state_size\r\n\r\n@property\r\ndef output_size(self):\r\n    return self._output_size\r\n\r\ndef call(self, inputs, state):\r\n\r\n\r\n    num_proj = (self.num_units if self.num_proj\r\n                is None else self.num_proj)\r\n\r\n    if self.state_is_tuple:\r\n        (c_prev, h_prev) = state\r\n    else:\r\n        c_prev = tf.slice(state, [0, 0], [-1, self.num_units])\r\n        h_prev = tf.slice(state, [0, self.num_units], [-1,\r\n                          num_proj])\r\n\r\n    dtype = inputs.dtype\r\n    input_size = inputs.get_shape().with_rank(2)[1]\r\n\r\n    if input_size.value is None:\r\n        raise ValueError('Could not infer input size from inputs.get_shape()[-1]'\r\n                         )\r\n    scope = tf.get_variable_scope()\r\n    with tf.variable_scope(scope or type(self).__name__):\r\n\r\n        W_xh = tf.get_variable('input_kernel', [input_size, 4\r\n                               * self.num_units],\r\n                               initializer=orthogonal_initializer())\r\n        W_hh = tf.get_variable('state_kernel', [num_proj, 4\r\n                               * self.num_units],\r\n                               initializer=bn_lstm_identity_initializer(0.95))\r\n\r\n        xh = tf.matmul(inputs, W_xh)\r\n        hh = tf.matmul(h_prev, W_hh)\r\n\r\n        bn_xh = batch_norm(xh, 'input', self.is_training)\r\n        bn_hh = batch_norm(hh, 'state', self.is_training)\r\n\r\n        bias = tf.get_variable('bias', [4 * self.num_units])\r\n\r\n  # i:input gate, j:new input, f:forget gate, o:output gate\r\n\r\n        lstm_matrix = tf.nn.bias_add(tf.add(bn_xh, bn_hh), bias)\r\n        (i, j, f, o) = tf.split(value=lstm_matrix,\r\n                                num_or_size_splits=4, axis=1)\r\n\r\n  # Diagonal connections\r\n\r\n        if self.use_peepholes:\r\n            w_f_diag = tf.get_variable('W_F_diag',\r\n                    shape=[self.num_units], dtype=dtype)\r\n            w_i_diag = tf.get_variable('W_I_diag',\r\n                    shape=[self.num_units], dtype=dtype)\r\n            w_o_diag = tf.get_variable('W_O_diag',\r\n                    shape=[self.num_units], dtype=dtype)\r\n\r\n        if self.use_peepholes:\r\n            c = c_prev * tf.sigmoid(f + self.forget_bias + w_f_diag\r\n                    * c_prev) + tf.sigmoid(i + w_i_diag * c_prev) \\\r\n                * self.activation(j)\r\n        else:\r\n            c = c_prev * tf.sigmoid(f + self.forget_bias) \\\r\n                + tf.sigmoid(i) * self.activation(j)\r\n\r\n        if self.cell_clip is not None:\r\n            c = tf.clip_by_value(c, -self.cell_clip, self.cell_clip)\r\n\r\n        bn_c = batch_norm(c, 'cell', self.is_training)\r\n\r\n        if self.use_peepholes:\r\n            h = tf.sigmoid(o + w_o_diag * c) * self.activation(bn_c)\r\n        else:\r\n            h = tf.sigmoid(o) * self.activation(bn_c)\r\n\r\n        if self.num_proj is not None:\r\n            w_proj = tf.get_variable('projection/kernel',\r\n                    [self.num_units, num_proj], dtype=dtype)\r\n\r\n            h = tf.matmul(h, w_proj)\r\n            if self.proj_clip is not None:\r\n                h = tf.clip_by_value(h, -self.proj_clip,\r\n                        self.proj_clip)\r\n\r\n    new_state = (LSTMStateTuple(c,\r\n                 h) if self.state_is_tuple else tf.concat(values=[c,\r\n                 h], axis=1))\r\n    return (h, new_state)\r\n\r\n\r\n```\r\n\r\nand my batch normalization function is as following \r\n```python\r\ndef batch_norm(x, name_scope, is_training):\r\nwith tf.variable_scope(name_scope):\r\n    return tf.layers.batch_normalization(inputs=x,training=is_training,fused=True)\r\n```\r\n\r\nIn my trainer.py file I have a function that handles the update_op in which we take care of the moving_mean and moving_variance of the batch_normalization.\r\n\r\n```python\r\ndef _update(self, loss, learning_rate, cluster):\r\n    '''\r\n    create the op to update the model\r\n\r\n    args:\r\n        loss: the loss to minimize\r\n        learning_rate: the learning rate\r\n        cluster: the tf cluster\r\n\r\n    returns: the update op\r\n    '''\r\n\r\n    #create the optimizer\r\n    optimizer = tf.train.AdamOptimizer(learning_rate)\r\n\r\n    #create an optimizer that aggregates gradients\r\n    if int(self.conf['numbatches_to_aggregate']) > 0:\r\n        if 'local' in cluster.as_dict():\r\n            num_workers = 1\r\n        else:\r\n            num_workers = len(cluster.as_dict()['worker'])\r\n\r\n        optimizer = tf.train.SyncReplicasOptimizer(\r\n            opt=optimizer,\r\n            replicas_to_aggregate=int(\r\n                self.conf['numbatches_to_aggregate']),\r\n            total_num_replicas=num_workers)\r\n\r\n\r\n    tf.summary.scalar('training_loss', loss,\r\n                      collections=['training_summaries'])\r\n\r\n    #get the list of trainable variables\r\n    trainable = tf.trainable_variables()\r\n\r\n    #get the list of variables to be removed from the trainable\r\n    #variables\r\n    untrainable = tf.get_collection('untrainable')\r\n\r\n    #remove the variables\r\n    trainable = [var for var in trainable\r\n                 if var not in untrainable]\r\n\r\n    #compute the gradients\r\n    grads_and_vars = optimizer.compute_gradients(\r\n        loss=loss,\r\n        var_list=trainable)\r\n\r\n    with tf.variable_scope('clip'):\r\n        #clip the gradients\r\n        grads_and_vars = [(tf.clip_by_value(grad, -1., 1.), var)\r\n                          for grad, var in grads_and_vars]\r\n\r\n\r\n    #opperation to apply the gradients\r\n    apply_gradients_op = optimizer.apply_gradients(\r\n        grads_and_vars=grads_and_vars,\r\n        name='apply_gradients')\r\n\r\n    #all remaining operations with the UPDATE_OPS GraphKeys\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    print(\"update_ops {}\".format(update_ops))\r\n    print(\"################\")\r\n    #create an operation to update the gradients, the batch_loss\r\n    #and do all other update ops\r\n    update_op = tf.group(\r\n        *([apply_gradients_op] + update_ops),\r\n        name='update')\r\n\r\n    return update_op\r\n\r\n```\r\n\r\nThe error message is as following\r\n\r\n> node train/update (defined at /home/ubuntu/workspace/reproduce/jobs/nabu/nabu/neuralnetworks/trainers/trainer.py:578) has inputs from different frames. The input node train/Listener/features/layer1/BLSTM/bidirectional_rnn/fw/fw/while/fw/bnlstm_cell/bnlstm_cell/state/batch_normalization/AssignMovingAvg (defined at /home/ubuntu/workspace/reproduce/jobs/nabu/nabu/neuralnetworks/components/recurrent_batch.py:61) is in frame 'train/Listener/features/layer1/BLSTM/bidirectional_rnn/fw/fw/while/while_context'. The input node train/apply_gradients/Assign (defined at /home/ubuntu/workspace/reproduce/jobs/nabu/nabu/neuralnetworks/trainers/trainer.py:569) is in frame ''.\r\n\r\nAny idea how to fix this?", "comments": ["any updates on this issue please?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 24404, "title": "Support SparseReduceSum on GPU ", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nFrom a generated trace I see that tensorflow performs SparseReduceSum on CPU while the rest of computation on GPU. I'm using `tf.sparse.reduce_sum`\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nEveryone, reduction on CPU kills performance completely. \r\n\r\n**Any Other info.**\r\nCould you point to data structures and/or algorithms that you are going to use, if any?\r\n", "comments": ["this is particularly a problem if loading a graph with this op onto GPU. ", "@sh1ng,\r\nSorry for the delayed response. Can you please let us know if [tf.raw_ops.SparseReduceSum](https://www.tensorflow.org/api_docs/python/tf/raw_ops/SparseReduceSum?hl=ko.Souten%20Ki%20Beti%20HD%20Jeetendra%20Rekha%20Jaya%20Prada%20Hindi%20Full%20MovieShyam) in the latest **`Tensorflow Version (2.5)`** runs on **`GPU`**? Thanks!\r\n ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Please reopen it, tensorflow should solve this problem"]}, {"number": 24403, "title": "frechet_classifier_distance triggers cuSolverDN issues", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): r1.13 from source\r\n- TensorFlow version (use command below): 'v1.12.0-4609-g040ee45aa8' 1.12.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): 0.19\r\n- GCC/Compiler version (if compiling from source): 7.3.1\r\n- CUDA/cuDNN version: CUDA 10.0 with cuDNN 7.4.1\r\n- GPU model and memory: RTX 2070 8 GB\r\n\r\n**Describe the current behavior**\r\nTrying to calculate FID triggers `Check failed: cusolverDnCreate(&cusolver_dn_handle) == CUSOLVER_STATUS_SUCCESS Failed to create cuSolverDN instance.`, and the program fails to execute. \r\n\r\nI have checked that `tf.contrib.gan.eval.run_inception` runs without problem, so the problem must be in `tf.contrib.gan.eval.frechet_classifier_distance`. \r\n\r\n**Describe the expected behavior**\r\nIt should calculate a frechet inception distance. \r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nreal = tf.zeros([1, 299, 299, 3], tf.float32)\r\nfake = tf.ones([1, 299, 299, 3], tf.float32)\r\ncalc_fid = tf.contrib.gan.eval.frechet_classifier_distance(real, fake, tf.contrib.gan.eval.run_inception)\r\n\r\nsess = tf.Session()\r\nfid = sess.run(calc_fid)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\nWARNING:tensorflow:From /home/michael/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2018-12-17 11:39:37.256810: W tensorflow/core/framework/op_def_util.cc:355] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\r\nWARNING:tensorflow:From /home/michael/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.flatten instead.\r\nWARNING:tensorflow:From /home/michael/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:701: to_double (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\n2018-12-17 11:39:39.198829: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-12-17 11:39:39.218429: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4001425000 Hz\r\n2018-12-17 11:39:39.218916: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55b50cbbed30 executing computations on platform Host. Devices:\r\n2018-12-17 11:39:39.218937: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2018-12-17 11:39:39.359019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-12-17 11:39:39.359747: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55b50c9b9ca0 executing computations on platform CUDA. Devices:\r\n2018-12-17 11:39:39.359771: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5\r\n2018-12-17 11:39:39.360173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.71\r\npciBusID: 0000:1c:00.0\r\ntotalMemory: 7.76GiB freeMemory: 7.16GiB\r\n2018-12-17 11:39:39.360188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2018-12-17 11:39:39.584743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-12-17 11:39:39.584776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2018-12-17 11:39:39.584784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2018-12-17 11:39:39.585158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6882 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:1c:00.0, compute capability: 7.5)\r\n2018-12-17 11:39:42.809448: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\n2018-12-17 11:39:44.695693: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x55b50fba7850\r\n2018-12-17 11:39:44.812389: F tensorflow/core/kernels/cuda_solvers.cc:94] Check failed: cusolverDnCreate(&cusolver_dn_handle) == CUSOLVER_STATUS_SUCCESS Failed to create cuSolverDN instance.\r\n[1]    25740 abort (core dumped)  python test.py\r\n```", "comments": ["Is this still an issue?\r\nI was able to execute your code snippet successfully using Google Colab GPU accelerator using TF 1.13.0-rc1.  Can you please confirm? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): r1.13 from source\r\n- TensorFlow version (use command below): 'v1.12.0-4609-g040ee45aa8' 1.13.1\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 10.0 with cuDNN 7\r\n- GPU model and memory: RTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nHere's the full logs from running `test.py`\r\n\r\n```\r\n>> Downloading http://download.tensorflow.org/models/frozen_inception_v1_2015_12_05.tar.gz 100.0%\r\n2019-04-20 01:10:46.046521: W tensorflow/core/framework/op_def_util.cc:355] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.flatten instead.\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:701: to_double (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\n2019-04-20 01:10:48.460746: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-04-20 01:10:48.576805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-04-20 01:10:48.577507: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5b3dd40 executing computations on platform CUDA. Devices:\r\n2019-04-20 01:10:48.577523: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2019-04-20 01:10:48.595965: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz\r\n2019-04-20 01:10:48.596645: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4aeea50 executing computations on platform Host. Devices:\r\n2019-04-20 01:10:48.596661: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-04-20 01:10:48.597154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.73GiB freeMemory: 9.89GiB\r\n2019-04-20 01:10:48.597168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-04-20 01:10:48.597759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-04-20 01:10:48.597768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-04-20 01:10:48.597773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-04-20 01:10:48.598012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9621 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-04-20 01:10:49.943217: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\n2019-04-20 01:10:51.767228: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x4a3f850\r\n2019-04-20 01:10:51.922493: F tensorflow/core/kernels/cuda_solvers.cc:94] Check failed: cusolverDnCreate(&cusolver_dn_handle) == CUSOLVER_STATUS_SUCCESS Failed to create cuSolverDN instance.\r\nAborted (core dumped)\r\n```\r\nTrying to calculate FID triggers Check failed: cusolverDnCreate(&cusolver_dn_handle) == CUSOLVER_STATUS_SUCCESS Failed to create cuSolverDN instance., and the program fails to execute.\r\n\r\nI have checked that tf.contrib.gan.eval.run_inception runs without problem, so the problem must be in tf.contrib.gan.eval.frechet_classifier_distance.\r\n\r\n**Describe the expected behavior**\r\nIt should calculate a frechet inception distance.\r\n\r\nCode to reproduce the issue\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nreal = tf.zeros([1, 299, 299, 3], tf.float32)\r\nfake = tf.ones([1, 299, 299, 3], tf.float32)\r\ncalc_fid = tf.contrib.gan.eval.frechet_classifier_distance(real, fake, tf.contrib.gan.eval.run_inception)\r\n\r\nsess = tf.Session()\r\nfid = sess.run(calc_fid)\r\n```\r\n\r\nI ran the code inside of the `tensorflow/tensorflow:latest-gpu` docker image, and also inside `nvidia/cuda:10.0-cudnn7-runtime-ubuntu18.04` with `tensorflow-gpu` installed via `pip`.\r\n\r\nI've also seen the same code run inside of a Google Collab notebook, so there must be some difference between that environment and this one.. But I haven't been able to figure it out.\r\n\r\nI'd really like it if someone took a second look at this. If I could get some direction I'd have a look myself.", "Try using conda to install the environment. I solved it this way. `conda create -n tf python==3.6.8` and then `conda install tensorflow-gpu`. It will get you tensorflow 1.13.1 with cuda 10 and cudnn 7.3.1. "]}, {"number": 24402, "title": "[OpenMP] Fix undeclared identifier in eigen_support.cc", "body": "When OpenMP is enabled, the following error occurs during a\r\ncompilation of the `tensorflow/lite/kernels/eigen_support.cc` unit:\r\n\r\ntensorflow/lite/kernels/eigen_support.cc:42:23: error: use of undeclared\r\nidentifier 'context'\r\n  Eigen::setNbThreads(context->recommended_num_threads);\r\n\r\nThe `SetEigenNbThreads` method already gets the number of threads as the\r\n`threads` parameter and doesn't need to calculate it using a method of\r\nthe `context` variable, so the invocation of the `Eigen::setNbThreads`\r\nmember must be changed a little bit.\r\n\r\nSigned-off-by: Pavel Samolysov <samolisov@gmail.com>", "comments": ["Nagging Reviewer @jdduke: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 24401, "title": "How can I use the gradients of variable 'a' to update variable 'b'? ", "body": "Hi, I have two sets of weights 'a' and 'b' in my model, I want to use the gradients of 'a' to update 'b'. Can I call \"compute_gradients\" on 'a' to get the gradients, and use the result to call \"apply_gradients\" for 'b' to update weight 'b'? If feasible , what should I do?\r\n\r\n", "comments": ["Does a and b has the same shape?", "> Does a and b has the same shape?\r\n\r\nYes, it does. In fact, I want to transfer resnet50 to a binary net, so there are two sets of weights in my model. One is a set of weight in float, named \"w\", the other is a set of binary weights, named \"bin_w\", just like this:\r\n\r\n`   \r\n    def binarize(self,x):\r\n        gb = tf.get_default_graph()\r\n        with ops.name_scope(\"binarized\") as name:\r\n            with gb.gradient_override_map({\"Sign\":\"Identity\"}):\r\n                return tf.sign(x)\r\n\r\n    def HardTanh(self,x):\r\n        with tf.variable_scope(\"HardTanh\"):\r\n            return tf.clip_by_value(x,-1,1)\r\n\r\n    def bin_conv(self,x,num_out_layers,kernel_size,stride=1,name=None,activation_fn=\"HardTanh\"):\r\n        #num_in_layers=x.get_shape().as_list()[3]\r\n        with tf.variable_scope(name,reuse=False):\r\n            p = np.floor((kernel_size - 1) / 2).astype(np.int32)\r\n            p_x = tf.pad(x, [[0, 0], [p, p], [p, p], [0, 0]])\r\n            num_in_layers=p_x.get_shape().as_list()[3]\r\n            my_w=tf.get_variable('weight',[kernel_size,kernel_size,num_in_layers,num_out_layers],initializer=tf.contrib.layers.xavier_initializer_conv2d())\r\n            my_w=tf.clip_by_value(my_w,-1,1)\r\n            \r\n            bin_w=self.binarize(my_w)\r\n            bin_x=self.binarize(p_x)\r\n\r\n            out=tf.nn.conv2d(bin_x,bin_w,strides=[1,stride,stride,1],padding='VALID')\r\n\r\n            my_b=tf.get_variable(\"bias\",[num_out_layers],initializer=tf.zeros_initializer)\r\n\r\n            out=tf.nn.bias_add(out,my_b)\r\n\r\n            ##########################################################\r\n            out=tf.contrib.layers.batch_norm(out)\r\n            ##########################################################\r\n\r\n            if (activation_fn == None):\r\n                return out\r\n            else:\r\n                return self.HardTanh(out)\r\n`\r\nNow I want to use the gradients of 'bin_w' to update 'w', and binarize new 'w' to update 'bin_w', and take bin_w as the final result in my checkpoint file. \r\n\r\nI refer to the tensorflow implement of BinaryNet:[https://github.com/itayhubara/BinaryNet.tf](url)"]}, {"number": 24400, "title": "ImportError: DLL load failed: \u041d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u0434\u0443\u043b\u044c.", "body": "**System information**\r\n- OS: `Windows 10 Pro x64`\r\n- TensorFlow installed from `pip`\r\n- TensorFlow version: `1.12.1`\r\n- Python version: `3.6.5`\r\n- CUDA version: `10.0`, cuDNN version: `7`\r\n- GPU: `940MX with GDDR5 memory`\r\n\r\nLet's start do describe this.\r\nI used to create some variables in tensorflow/python/platforms/build_info.py\r\n```python\r\nsome_path = \"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin/\"\r\n\r\nmsvcp_dll_name = 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/jre/bin/msvcp140.dll'\r\ncudart_dll_name = some_path + 'cudart64_100.dll'\r\ncuda_version_number = '10.0'\r\n# nvcuda_dll_name = 'nvcuda.dll'\r\ncudnn_dll_name = some_path + 'cudnn64_7.dll'\r\ncudnn_version_number = '7'\r\n```\r\n\r\nBut I'd got an error. Logs:\r\n```\r\nC:\\Users\\Ar4ikov\\PycharmProjects\\QTApp\\similar_text>python similar_text.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Ar4ikov\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Ar4ikov\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Ar4ikov\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Ar4ikov\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Ar4ikov\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u041d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u0434\u0443\u043b\u044c.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"similar_text.py\", line 8, in <module>\r\n    import tflearn as tf\r\n  File \"C:\\Users\\Ar4ikov\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tflearn\\__init__.py\", line 4, in <module>\r\n    from . import config\r\n  File \"C:\\Users\\Ar4ikov\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tflearn\\config.py\", line 3, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Ar4ikov\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Ar4ikov\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Ar4ikov\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Ar4ikov\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Ar4ikov\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Ar4ikov\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Ar4ikov\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Ar4ikov\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u041d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u0434\u0443\u043b\u044c.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\nWhat's wrong?", "comments": ["I see that you are using CUDA 10. Unfortunately,Our have prebuilt binaries support CUDA 9 as of now. Therefore you have to build TF yourself from sources in order to use cuda 10. ", "I will turn back to CUDA 9, Thanks", "Still not working.", "Did you update the environment variables as well? You have to [set env variables](https://www.tensorflow.org/install/gpu#windows_setup) to cuda 9.0.\r\nAlso you need to install the Microsoft Visual C++ 2015 Redistributable Update 3.", "@Ar4ikov  What version of cuDNN are you using with CUDA 9 ? Generally  cuDNN >= 7.2 for CUDA 9.0  should be used.", "@Ar4ikov Is this still an issue for you?", "yes it is", "Apologies for the delay in response. \r\n\r\n> @Ar4ikov What version of cuDNN are you using with CUDA 9 ? Generally cuDNN >= 7.2 for CUDA 9.0 should be used.\r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24399, "title": "Tensorflow Dataset API, OutOfRangeError: End of sequence", "body": "Hi - \r\n\r\nI am trying to do a RNN in Tensorflow using Dataset API for handling the \"spoon feeding\" of data into the estimation process. But, I get a OutOf Range Error for no apparent reason. My code is simple and I cannot figure out why the problem occurs. \r\n\r\nBr, \r\n\r\n`\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\n\r\nt_min, t_max = 0, 30\r\nresolution = 0.1\r\n\r\ndef time_series(t):\r\n    return t * np.sin(t) / 3 + 2 * np.sin(t*5)\r\n\r\nt = np.linspace(t_min, t_max, int((t_max - t_min) / resolution))\r\n\r\nn_steps = 20\r\nt_instance = np.linspace(12.2, 12.2 + resolution * (n_steps + 1), n_steps + 1)\r\n\r\nx_state = time_series(t)\r\nx_state = np.c_[x_state]\r\n\r\nx_test = time_series(t_instance)\r\nx_test = np.c_[x_test]\r\n\r\n# spoonfed for Tensorflow \r\ndata_x = np.array(x_state,dtype=np.float32)\r\ndata_test = np.array(x_test,dtype=np.float32)\r\n# model parameters \r\n\r\n\r\nn_inputs = 1\r\nn_neurons = 100\r\nn_outputs = 1\r\n\r\n\r\nm = len(data_x)     # number of observations\r\nn_dims = n_inputs   # number of input dimension\r\nbatch_size = 40    # the number of batches\r\n#n_steps = 20        # size of the batch = n_steps in the RNN model\r\nn_shift = 1         # target is one time steps into the future for the label\r\nn_epochs = 70       # number of the epochs\r\n\r\n\r\nDataTensor = tf.placeholder(tf.float32, [m, n_inputs])\r\n\r\n\r\nseries_dataset = tf.data.Dataset.from_tensor_slices(DataTensor)\r\n\r\ndef shifted_slices(offset):\r\n    return series_dataset.skip(offset).batch(n_steps + n_shift, drop_remainder=True)\r\n\r\ndef split_in_out(sequence):\r\n    return sequence[:, :n_steps], sequence[:, n_shift:]\r\n\r\ndataset = tf.data.Dataset.range(n_steps + n_shift)\r\ndataset = dataset.interleave(shifted_slices, n_steps + n_shift)\r\ndataset = dataset.shuffle(buffer_size=m)\r\ndataset = dataset.repeat(n_epochs)\r\ndataset = dataset.batch(batch_size)\r\ndataset = dataset.map(split_in_out)\r\ndataset = dataset.prefetch(1)\r\n\r\nsess = tf.Session()\r\n\r\niterator = dataset.make_initializable_iterator()\r\n#iterator = dataset.make_one_shot_iterator()\r\nfeature, label = iterator.get_next()\r\n\r\ncell = tf.contrib.rnn.OutputProjectionWrapper(\r\n    tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu),output_size=n_outputs)\r\noutputs, states = tf.nn.dynamic_rnn(cell, feature, dtype=tf.float32)\r\n\r\nlearning_rate = 0.001\r\n\r\nloss = tf.reduce_mean(tf.square(outputs - label)) # MSE\r\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\ntraining_op = optimizer.minimize(loss)\r\n\r\nsaver = tf.train.Saver()\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    init.run()\r\n    sess.run(iterator.initializer, feed_dict={ DataTensor: data_x } )\r\n    print('Start training process ...')\r\n    \r\n    for i in range (n_epochs):\r\n            tot_loss = 0\r\n            for _ in range (batch_size):\r\n                _, loss_value = sess.run([training_op, loss])\r\n                tot_loss += loss_value\r\n            print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / batch_size)) \r\n\r\n    saver.save(sess, \"./SavedModels/Trial\")\r\n```\r\n`\r\nThe output: \r\n\r\n`Start training process ...\r\nIter: 0, Loss: 3.4887\r\nIter: 1, Loss: 0.7238\r\nIter: 2, Loss: 0.4668\r\nIter: 3, Loss: 0.2963\r\nIter: 4, Loss: 0.1895\r\nIter: 5, Loss: 0.1205\r\nIter: 6, Loss: 0.0831\r\nIter: 7, Loss: 0.0679\r\nIter: 8, Loss: 0.0613\r\nIter: 9, Loss: 0.0585\r\nIter: 10, Loss: 0.0566\r\nIter: 11, Loss: 0.0548\r\n---------------------------------------------------------------------------\r\nOutOfRangeError                           Traceback (most recent call last)`", "comments": ["OutOfRangeError means no more dataset can be read. It is normal.\r\nFor example:\r\n```\r\n   dataset = ...  # A tf.data.Dataset object.\r\n    iterator = dataset.make_initializable_iterator()\r\n    next_element = iterator.get_next()\r\n    # Build a TensorFlow graph that does something with each element.\r\n    loss = model_function(next_element)\r\n    optimizer = ...  # A tf.train.Optimizer object.\r\n    train_op = optimizer.minimize(loss)\r\n    with tf.Session() as sess:\r\n      try:\r\n        while True:\r\n          sess.run(train_op)\r\n      except tf.errors.OutOfRangeError:\r\n        pass\r\n```", " For follow up questions, They are better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "> OutOfRangeError means no more dataset can be read. It is normal.\r\n> For example:\r\n> \r\n> ```\r\n>    dataset = ...  # A tf.data.Dataset object.\r\n>     iterator = dataset.make_initializable_iterator()\r\n>     next_element = iterator.get_next()\r\n>     # Build a TensorFlow graph that does something with each element.\r\n>     loss = model_function(next_element)\r\n>     optimizer = ...  # A tf.train.Optimizer object.\r\n>     train_op = optimizer.minimize(loss)\r\n>     with tf.Session() as sess:\r\n>       try:\r\n>         while True:\r\n>           sess.run(train_op)\r\n>       except tf.errors.OutOfRangeError:\r\n>         pass\r\n> ```\r\n\r\nI think this question is more about how to reset the data iterator after each epoch. It doesn't really help when you `pass` because it won't let you start the next epoch", "iterator should be initialized in each epoch.\r\n```\r\nwith tf.Session() as sess:\r\n    init.run()\r\n    print('Start training process ...')\r\n    \r\n    for i in range (n_epochs):\r\n            tot_loss = 0\r\n            sess.run(iterator.initializer, feed_dict={ DataTensor: data_x } )\r\n            for _ in range (batch_size):\r\n                _, loss_value = sess.run([training_op, loss])\r\n                tot_loss += loss_value\r\n            print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / batch_size)) \r\n\r\n    saver.save(sess, \"./SavedModels/Trial\")\r\n```\r\nthe output:\r\n```\r\nStart training process ...\r\nIter: 0, Loss: 4.6970\r\nIter: 1, Loss: 1.0409\r\nIter: 2, Loss: 0.5996\r\nIter: 3, Loss: 0.3767\r\nIter: 4, Loss: 0.2356\r\nIter: 5, Loss: 0.1494\r\nIter: 6, Loss: 0.1006\r\nIter: 7, Loss: 0.0794\r\nIter: 8, Loss: 0.0718\r\nIter: 9, Loss: 0.0620\r\nIter: 10, Loss: 0.0597\r\nIter: 11, Loss: 0.0581\r\nIter: 12, Loss: 0.0565\r\nIter: 13, Loss: 0.0551\r\nIter: 14, Loss: 0.0541\r\nIter: 15, Loss: 0.0529\r\nIter: 16, Loss: 0.0531\r\nIter: 17, Loss: 0.0523\r\nIter: 18, Loss: 0.0524\r\nIter: 19, Loss: 0.0510\r\nIter: 20, Loss: 0.0506\r\nIter: 21, Loss: 0.0504\r\nIter: 22, Loss: 0.0494\r\nIter: 23, Loss: 0.0498\r\nIter: 24, Loss: 0.0489\r\nIter: 25, Loss: 0.0486\r\nIter: 26, Loss: 0.0487\r\nIter: 27, Loss: 0.0475\r\nIter: 28, Loss: 0.0476\r\nIter: 29, Loss: 0.0473\r\nIter: 30, Loss: 0.0466\r\nIter: 31, Loss: 0.0461\r\nIter: 32, Loss: 0.0461\r\nIter: 33, Loss: 0.0462\r\nIter: 34, Loss: 0.0458\r\nIter: 35, Loss: 0.0464\r\nIter: 36, Loss: 0.0453\r\nIter: 37, Loss: 0.0456\r\nIter: 38, Loss: 0.0456\r\nIter: 39, Loss: 0.0450\r\nIter: 40, Loss: 0.0446\r\nIter: 41, Loss: 0.0444\r\nIter: 42, Loss: 0.0443\r\nIter: 43, Loss: 0.0453\r\nIter: 44, Loss: 0.0442\r\nIter: 45, Loss: 0.0433\r\nIter: 46, Loss: 0.0436\r\nIter: 47, Loss: 0.0432\r\nIter: 48, Loss: 0.0432\r\nIter: 49, Loss: 0.0429\r\nIter: 50, Loss: 0.0427\r\nIter: 51, Loss: 0.0429\r\nIter: 52, Loss: 0.0429\r\nIter: 53, Loss: 0.0431\r\nIter: 54, Loss: 0.0426\r\nIter: 55, Loss: 0.0423\r\nIter: 56, Loss: 0.0420\r\nIter: 57, Loss: 0.0416\r\nIter: 58, Loss: 0.0423\r\nIter: 59, Loss: 0.0419\r\nIter: 60, Loss: 0.0426\r\nIter: 61, Loss: 0.0416\r\nIter: 62, Loss: 0.0414\r\nIter: 63, Loss: 0.0413\r\nIter: 64, Loss: 0.0417\r\nIter: 65, Loss: 0.0416\r\nIter: 66, Loss: 0.0413\r\nIter: 67, Loss: 0.0425\r\nIter: 68, Loss: 0.0416\r\nIter: 69, Loss: 0.0409\r\n```"]}, {"number": 24398, "title": "Errors in runtime binaries with selective registration ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.9\r\n- Python version: 3.6.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nI am trying to build using the selective registration and able to generate the ops_to_register.h and tensorflow binary but it crashes at the run time.\r\n\r\nI have tried this https://www.bountysource.com/issues/43940878-errors-when-building-ios-binaries-with-selective-registration but that seems to have lot of compilation issues for me.\r\n\r\nBelow is the crash log\r\n#0  1.8.0/tensorflow/contrib/makefile/gen/protobuf-host/include/google/protobuf/arenastring.h:83\r\n#1  name (this=<optimized out>) at /usr/src/debug/tensorflow-1.8.0/tensorflow/contrib/makefile/gen/proto/tensorflow/core/framework/node_def.pb.h:336\r\n#2  _ZNK10tensorflow4Node4nameB5cxx11Ev (this=this@entry=0x0) at tensorflow/core/graph/graph.cc:140\r\n#3  0xf699877a in name (this=0xab499320) at tensorflow/cc/framework/ops.h:76\r\n#4  tensorflow::ClientSession::Run (this=this@entry=0xf11f8bc0, run_options=..., inputs=..., fetch_outputs=..., run_outputs=..., outputs=outputs@entry=0xf11f8be8, run_metadata=run_metadata@entry=0x0)\r\n    at tensorflow/cc/client/client_session.cc:118\r\n#5  0xf6998af4 in tensorflow::ClientSession::Run (this=0xf11f8bc0, inputs=..., fetch_outputs=..., run_outputs=..., outputs=outputs@entry=0xf11f8be8) at tensorflow/cc/client/client_session.cc:90\r\n#6  0xf6998bf6 in tensorflow::ClientSession::Run (this=<optimized out>, inputs=..., fetch_outputs=..., outputs=0xf11f8be8) at tensorflow/cc/client/client_session.cc:82\r\n\r\n\r\nIs the below correct or we need to use the strstrc\r\ndefine SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))\r\n\r\nAny suggestion will be helpful.\r\n\r\n", "comments": ["is there any updates on this?\r\nCan you please let me know the right way for using the selective_registration .\r\n\r\n#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))\r\n\r\nis it the right definition? do we need to use find_in or strstrc as mentioned in  https://www.bountysource.com/issues/43940878-errors-when-building-ios-binaries-with-selective-registration", "Found the root cause..some of the custom ops used out side the graph were missing.\r\nclosing the issues."]}, {"number": 24397, "title": "tf.losses.softmax_cross_entropy does not handle sequence loss when using label smoothing", "body": "When using `tf.losses.softmax_cross_entropy` with a 3D Tensor for `labels` and `logits` (which you may do if your output is a sequence), as well as the `label_smoothing` parameter, the result is incorrect. From a cursory inspection of the python source I would guess that in [line 796](https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/ops/losses/losses_impl.py#L796), the number of classes is assumed to be `shape(onehot_labels)[1]`, rather than `shape(onehot_labels)[-1]`. The former assumes a 2D labels tensor which is neither described by the documentation nor enforced by the method.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint Tumbleweed\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: 9.0/7.1.2\r\n- GPU model and memory: GeForce GTX 1080 Ti 11 GB\r\n", "comments": ["Thanks for your investigation. In order to expedite the trouble-shooting process, can please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Your diagnostic looks correct. Do you want to send us a pull request?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 24396, "title": "FeedInputs: unable to find feed output input", "body": "- Have I written custom code : No\r\n- OS Platform and Distribution : Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.12\r\n- Python version:3.5\r\n\r\nI have got below error:\r\n**FeedInputs: unable to find feed output input**\r\nExpected : No issue such as unable to find feed output input\r\n\r\n\r\nI have used tf.keras to build the model and used model.fit to train the model instead of using sess.run  & feed_dict. \r\n \r\nCommand used to run transfrom graph is as follows :\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=~/saved_model2.pb --out_graph=optimized_graph1.pb --inputs=input --outputs=output/kernel --transforms='strip_unused_nodes(type=float, shape=\"1,28,28,1\") remove_nodes(op=Identity, op=CheckNumerics) fold_constants fold_batch_norms fold_old_batch_norms'  \r\n\r\n\r\n", "comments": ["Unfortunately I don't have enough information to debug this, so closing for now. A Stack Overflow question with more details might get a response? Thanks!"]}, {"number": 24395, "title": "make_one_shot_iterator API have changed", "body": " tf.compat.v1.data.make_one_shot_iterator(dataset)  to  tf.data.Dataset.make_one_shot_iterator(dataset)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!\n\ngooglebot <notifications@github.com> \u4e8e2018\u5e7412\u670817\u65e5\u5468\u4e00 \u4e0b\u53483:13\u5199\u9053\uff1a\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here (e.g. I\n> signed it!) and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google.com/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/24395#issuecomment-447743725>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APbxEq1QhMKcn04upMzpqM09aaoyBcKdks5u50QigaJpZM4ZVyg0>\n> .\n>\n", "CLAs look good, thanks!\n\n<!-- ok -->", "closing this PR based on latest comments."]}, {"number": 24394, "title": "TF-Lite | Add custom objects when loading keras model", "body": "If the exported tf.keras model has any tf operations or Lambda layers that use tf internally loading the model will fail with the error `tf is not defined`.\r\nTo fix that it is important to load tf as a custom object when loading a keras model like that - `cusotm_objects={'tf': tf)`.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@gargn Can you please take a look? Thanks!", "Any news from this? I see pull request is not merged yet.", "This functionality has been added by https://github.com/tensorflow/tensorflow/commit/09deaeb03ca4ceb40cf600a337083e2054e65390."]}, {"number": 24393, "title": "Equivalent c_api for TFE_Py_RecordGradient", "body": "Hi, I'm trying to implement a C# wrapper for Tensorflow, but stuck in the c_api which is work like `TFE_Py_RecordGradient`.\r\n\r\nIs there any equivalent c_api? or any work around?\r\n\r\nI want to implement a C# code to do `var x = tf.placeholder(tf.float32, shape: new TensorShape(1024, 1024));`\r\n\r\nAny related information will be helpful, thanks.", "comments": ["The only clue I've got in the c_api.cc:\r\n```\r\n// Creates a placeholder representing an input to the cond or body graph.\r\n// TODO(skyewm): remove these from final graph\r\nbool CreateInput(const TF_Output& parent_input, TF_Graph* g, const char* name,\r\n                 TF_Output* input, TF_Status* status) {\r\n  TF_OperationDescription* desc = TF_NewOperation(g, \"Placeholder\", name);\r\n  TF_SetAttrType(desc, \"dtype\", TF_OperationOutputType(parent_input));\r\n  // TODO(skyewm): set placeholder shape\r\n  TF_Operation* oper = TF_FinishOperation(desc, status);\r\n  if (!status->status.ok()) return false;\r\n  *input = {oper, 0};\r\n  return true;\r\n}\r\n```", "I believe most language bindings that wish to support an TF Eager-like API (e.g. GradientTape) wrap the TFE_* functions in addition to the TF_* functions. Does this help?", "@Oceania2018,\r\nCan you please respond to the above comment? Thanks! ", "It\u2019s been resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24393\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24393\">No</a>\n"]}, {"number": 24392, "title": "TFLite:Models with multiple inputs and outputs", "body": "How can I handle the situations when my tensorflow models have mutiple inputs and outputs? It's not possible to give a fixed input_shape, so can any one help?", "comments": ["I've already noticed this \r\n\r\n>Each output should be an array or multi-dimensional array of the supported primitive types, or a ByteBuffer of the appropriate size. _**`Note that some models have dynamic outputs, where the shape of output tensors can vary depending on the input. There's no straightforward way of handling this with the existing Java inference API, but planned extensions will make this possible.`**_\r\n\r\nat tensorflow lite's offical API guide, but I wonder is there any possible way to deal with it in C++ source code.\r\nAny help will be appreciated, thanks so much!", "how about it now ?", "ok now!", "Hi @laohur !\r\nI have met the same problem. Can you show the example~~\r\n"]}, {"number": 24391, "title": "remove KERB_TICKET_CACHE_PATH env", "body": "if the \"KERB_TICKET_CACHE_PATH\" is set, memory leak will occur as new FileSystem object will be created each time \"HadoopFileSystem::Connect\" is called.\r\n\r\nrelated issue of hadoop: https://issues.apache.org/jira/browse/HDFS-3545\r\n", "comments": ["I don't have an informed opinion on this one. @jhseu could you have a look?", "The failed tasks seem irrelevant. Can @jhseu and @superbobry help to check and merge this?", "Thanks @shengofsun for the contribution \ud83d\udc4d "]}, {"number": 24390, "title": "Variable sized tf records", "body": "While storing data in tf records, say an image of size (N,3,224,224), its possible to keep N as variable, but is it possible to have another dimension variable. Like I have voxels of objects that I need to store and number of objects in an 3D world is random, so it will be something like (batch_size, num_objects, 128,128,128). Tensorflow does not seem to allow having one more dimension. Is there a way to solve this?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 24389, "title": "Build issue with triSYCL - EIGEN_MAX_ALIGN_BYTES redefined", "body": "**Describe the problem**\r\n\r\nI am trying to build tensorflow with triSYCL (the open source implementation). But the compilation fails because of a macro redefinition:\r\n```\r\n<command-line>: warning: \"EIGEN_MAX_ALIGN_BYTES\" redefined\r\n<command-line>: note: this is the location of the previous definition\r\nERROR: /home/emile/Workspace/MyAurPackages/tensorflow-triSYCL-git/src/tensorflow/tensorflow/python/BUILD:537:1: C++ compilation of rule '//tensorflow/python:python_op_gen' failed (Exit 1)\r\n```\r\n\r\n**System information**\r\n- OS Platform and Distribution: Archlinux\r\n- TensorFlow version: last from github\r\n- Bazel version (if compiling from source): 0.19.2- (@non-git)\r\n- GCC/Compiler version (if compiling from source): gcc version 8.2.1 20180831 (GCC) \r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nTo compile, I use\r\n```\r\n./configure\r\nbazel build --config=opt --config=sycl_trisycl //tensorflow:libtensorflow.so //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nThe most complete version of the error is:\r\n```\r\n<command-line>: warning: \"EIGEN_MAX_ALIGN_BYTES\" redefined\r\n<command-line>: note: this is the location of the previous definition\r\nERROR: /home/emile/Workspace/MyAurPackages/tensorflow-triSYCL-git/src/tensorflow/tensorflow/python/BUILD:537:1: C++ compilation of rule '//tensorflow/python:python_op_gen' failed (Exit 1)\r\n<command-line>: warning: \"EIGEN_MAX_ALIGN_BYTES\" redefined\r\n<command-line>: note: this is the location of the previous definition\r\nIn file included from /usr/include/CL/sycl/device/detail/host_device.hpp:18,\r\n                 from /usr/include/CL/sycl/device.hpp:23,\r\n                 from /usr/include/CL/sycl/context/detail/context.hpp:12,\r\n                 from /usr/include/CL/sycl/context/detail/host_context.hpp:18,\r\n                 from /usr/include/CL/sycl/context.hpp:14,\r\n                 from /usr/include/CL/sycl/buffer/detail/buffer_base.hpp:27,\r\n                 from /usr/include/CL/sycl/command_group/detail/task.hpp:23,\r\n                 from /usr/include/CL/sycl/accessor/detail/local_accessor.hpp:21,\r\n                 from /usr/include/CL/sycl/accessor.hpp:15,\r\n                 from /usr/include/CL/sycl.hpp:37,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:22,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/framework/types.h:23,\r\n                 from tensorflow/python/framework/python_op_gen_internal.cc:33:\r\n/usr/include/CL/sycl/device/detail/device.hpp:59:16: error: 'any' in namespace 'std' does not name a type\r\n   virtual std::any get_info(info::device param) const = 0;\r\n                ^~~\r\n/usr/include/CL/sycl/device/detail/device.hpp:59:11: note: 'std::any' is only available from C++17 onwards\r\n   virtual std::any get_info(info::device param) const = 0;\r\n           ^~~\r\nIn file included from /usr/include/CL/sycl/device.hpp:23,\r\n                 from /usr/include/CL/sycl/context/detail/context.hpp:12,\r\n                 from /usr/include/CL/sycl/context/detail/host_context.hpp:18,\r\n                 from /usr/include/CL/sycl/context.hpp:14,\r\n                 from /usr/include/CL/sycl/buffer/detail/buffer_base.hpp:27,\r\n                 from /usr/include/CL/sycl/command_group/detail/task.hpp:23,\r\n                 from /usr/include/CL/sycl/accessor/detail/local_accessor.hpp:21,\r\n                 from /usr/include/CL/sycl/accessor.hpp:15,\r\n                 from /usr/include/CL/sycl.hpp:37,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:22,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/framework/types.h:23,\r\n                 from tensorflow/python/framework/python_op_gen_internal.cc:33:\r\n/usr/include/CL/sycl/device/detail/host_device.hpp:101:8: error: 'any' in namespace 'std' does not name a type\r\n   std::any\r\n        ^~~\r\n/usr/include/CL/sycl/device/detail/host_device.hpp:101:3: note: 'std::any' is only available from C++17 onwards\r\n   std::any\r\n   ^~~\r\nIn file included from /usr/include/CL/sycl/context/detail/context.hpp:12,\r\n                 from /usr/include/CL/sycl/context/detail/host_context.hpp:18,\r\n                 from /usr/include/CL/sycl/context.hpp:14,\r\n                 from /usr/include/CL/sycl/buffer/detail/buffer_base.hpp:27,\r\n                 from /usr/include/CL/sycl/command_group/detail/task.hpp:23,\r\n                 from /usr/include/CL/sycl/accessor/detail/local_accessor.hpp:21,\r\n                 from /usr/include/CL/sycl/accessor.hpp:15,\r\n                 from /usr/include/CL/sycl.hpp:37,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:22,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/framework/types.h:23,\r\n                 from tensorflow/python/framework/python_op_gen_internal.cc:33:\r\n/usr/include/CL/sycl/device.hpp: In member function 'typename cl::sycl::info::param_traits<cl::sycl::info::device, Param>::return_type cl::sycl::device::get_info() const':\r\n/usr/include/CL/sycl/device.hpp:222:17: error: 'any_cast' is not a member of 'std'\r\n     return std::any_cast<\r\n                 ^~~~~~~~\r\n/usr/include/CL/sycl/device.hpp:222:17: note: 'std::any_cast' is only available from C++17 onwards\r\n/usr/include/CL/sycl/device.hpp:224:7: error: expected '(' before '>' token\r\n       >(implementation->get_info(param));\r\n       ^\r\n       (\r\n/usr/include/CL/sycl/device.hpp:224:25: error: 'using element_type = class cl::sycl::detail::device' {aka 'class cl::sycl::detail::device'} has no member named 'get_info'; did you mean 'get_platform'?\r\n       >(implementation->get_info(param));\r\n                         ^~~~~~~~\r\n                         get_platform\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:108,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/framework/types.h:23,\r\n                 from tensorflow/python/framework/python_op_gen_internal.cc:33:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h: In member function 'void* Eigen::QueueInterface::allocate(std::size_t) const':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:163:108: error: no matching function for call to 'cl::sycl::buffer<unsigned char, 1, SyclAllocator<unsigned char, 16> >::get_access<discard_write, host_buffer>()'\r\n     auto ptr =buf.get_access<cl::sycl::access::mode::discard_write, cl::sycl::access::target::host_buffer>().get_pointer();\r\n                                                                                                            ^\r\nIn file included from /usr/include/CL/sycl.hpp:40,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:22,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/framework/types.h:23,\r\n                 from tensorflow/python/framework/python_op_gen_internal.cc:33:\r\n/usr/include/CL/sycl/buffer.hpp:347:3: note: candidate: 'template<cl::sycl::access::mode Mode, cl::sycl::access::target Target> cl::sycl::accessor<T, Dimensions, Mode, Target> cl::sycl::buffer<T, Dimensions, Allocator>::get_access(cl::sycl::handler&) [with cl::sycl::access::mode Mode = Mode; cl::sycl::access::target Target = Target; T = unsigned char; int Dimensions = 1; Allocator = SyclAllocator<unsigned char, 16>]'\r\n   get_access(handler &command_group_handler) {\r\n   ^~~~~~~~~~\r\n/usr/include/CL/sycl/buffer.hpp:347:3: note:   template argument deduction/substitution failed:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:108,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/framework/types.h:23,\r\n                 from tensorflow/python/framework/python_op_gen_internal.cc:33:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:163:108: note:   candidate expects 1 argument, 0 provided\r\n     auto ptr =buf.get_access<cl::sycl::access::mode::discard_write, cl::sycl::access::target::host_buffer>().get_pointer();\r\n                                                                                                            ^\r\nIn file included from /usr/include/CL/sycl.hpp:40,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:22,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/framework/types.h:23,\r\n                 from tensorflow/python/framework/python_op_gen_internal.cc:33:\r\n/usr/include/CL/sycl/buffer.hpp:376:3: note: candidate: 'template<cl::sycl::access::mode Mode> cl::sycl::accessor<T, Dimensions, Mode, (cl::sycl::access::target)2018> cl::sycl::buffer<T, Dimensions, Allocator>::get_access() [with cl::sycl::access::mode Mode = Mode; T = unsigned char; int Dimensions = 1; Allocator = SyclAllocator<unsigned char, 16>]'\r\n   get_access() {\r\n   ^~~~~~~~~~\r\n/usr/include/CL/sycl/buffer.hpp:376:3: note:   template argument deduction/substitution failed:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:108,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/framework/types.h:23,\r\n                 from tensorflow/python/framework/python_op_gen_internal.cc:33:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:163:108: error: wrong number of template arguments (2, should be 1)\r\n     auto ptr =buf.get_access<cl::sycl::access::mode::discard_write, cl::sycl::access::target::host_buffer>().get_pointer();\r\n                                                                                                            ^\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSycl.h:110,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:156,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/framework/types.h:23,\r\n                 from tensorflow/python/framework/python_op_gen_internal.cc:33:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h: In member function 'void Eigen::EigenConvolutionKernel1D<CoeffReturnType, KernelType, HostExpr, FunctorExpr, Index, InputDims, Kernel_accessor, Buffer_accessor, Local_accessor, TupleType>::operator()(cl::sycl::nd_item<2>)':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:54:47: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_local'; did you mean 'set_local'?\r\n     const size_t plane_kernel_offset = itemID.get_local(1) * num_x_input;\r\n                                               ^~~~~~~~~\r\n                                               set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:56:95: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_global'; did you mean 'set_global'?\r\n     const size_t plane_tensor_offset =indexMapper.mapCudaInputPlaneToTensorInputOffset(itemID.get_global(1));\r\n                                                                                               ^~~~~~~~~~\r\n                                                                                               set_global\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:58:28: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_local'; did you mean 'set_local'?\r\n     for (size_t i = itemID.get_local(0); i < num_x_input ; i += itemID.get_local_range()[0]) {\r\n                            ^~~~~~~~~\r\n                            set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:61:72: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_global'; did you mean 'set_global'?\r\n       if(((i + first_input_start) < (range_x +kernelSize-1)) && itemID.get_global(1)< range_y){\r\n                                                                        ^~~~~~~~~~\r\n                                                                        set_global\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:71:15: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_global'; did you mean 'set_global'?\r\n     if(itemID.get_global(0)< range_x && itemID.get_global(1)< range_y){\r\n               ^~~~~~~~~~\r\n               set_global\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:71:48: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_global'; did you mean 'set_global'?\r\n     if(itemID.get_global(0)< range_x && itemID.get_global(1)< range_y){\r\n                                                ^~~~~~~~~~\r\n                                                set_global\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:73:56: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_local'; did you mean 'set_local'?\r\n       const size_t index = plane_kernel_offset+ itemID.get_local(0);\r\n                                                        ^~~~~~~~~\r\n                                                        set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:77:93: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_global'; did you mean 'set_global'?\r\n       const size_t tensor_index = indexMapper.mapCudaOutputPlaneToTensorOutputOffset(itemID.get_global(1))\r\n                                                                                             ^~~~~~~~~~\r\n                                                                                             set_global\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:78:67: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_local'; did you mean 'set_local'?\r\n       +indexMapper.mapCudaOutputKernelToTensorOutputOffset(itemID.get_local(0) + first_output_start);\r\n                                                                   ^~~~~~~~~\r\n                                                                   set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h: In member function 'void Eigen::EigenConvolutionKernel2D<CoeffReturnType, KernelType, HostExpr, FunctorExpr, Index, InputDims, Kernel_accessor, Buffer_accessor, Local_accessor, TupleType>::operator()(cl::sycl::nd_item<3>)':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:112:95: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?\r\n     const size_t plane_input_offset = indexMapper.mapCudaInputPlaneToTensorInputOffset(itemID.get_global(2));\r\n                                                                                               ^~~~~~~~~~\r\n                                                                                               set_global\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:113:47: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?\r\n     const size_t plane_kernel_offset = itemID.get_local(2) * num_y_input;\r\n                                               ^~~~~~~~~\r\n                                               set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:118:28: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?\r\n     for (size_t j = itemID.get_local(1); j < num_y_input; j += itemID.get_local_range()[1]) {\r\n                            ^~~~~~~~~\r\n                            set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:120:30: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?\r\n       for (size_t i = itemID.get_local(0); i < num_x_input ; i += itemID.get_local_range()[0]) {\r\n                              ^~~~~~~~~\r\n                              set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:123:137: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?\r\n         if(((i + first_x_input_start) < (range_x +kernelSize_x-1))  &&((j + first_y_input_start) < (range_y +kernelSize_y-1)) && itemID.get_global(2)< range_z){\r\n                                                                                                                                         ^~~~~~~~~~\r\n                                                                                                                                         set_global\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:135:15: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?\r\n     if(itemID.get_global(0)< range_x && itemID.get_global(1)< range_y && itemID.get_global(2)< range_z){\r\n               ^~~~~~~~~~\r\n               set_global\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:135:48: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?\r\n     if(itemID.get_global(0)< range_x && itemID.get_global(1)< range_y && itemID.get_global(2)< range_z){\r\n                                                ^~~~~~~~~~\r\n                                                set_global\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:135:81: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?\r\n     if(itemID.get_global(0)< range_x && itemID.get_global(1)< range_y && itemID.get_global(2)< range_z){\r\n                                                                                 ^~~~~~~~~~\r\n                                                                                 set_global\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:139:76: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?\r\n         const size_t index = (num_x_input*(plane_kernel_offset + j+ itemID.get_local(1))) + itemID.get_local(0);\r\n                                                                            ^~~~~~~~~\r\n                                                                            set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:139:100: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?\r\n         const size_t index = (num_x_input*(plane_kernel_offset + j+ itemID.get_local(1))) + itemID.get_local(0);\r\n                                                                                                    ^~~~~~~~~\r\n                                                                                                    set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:144:93: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?\r\n       const size_t tensor_index = indexMapper.mapCudaOutputPlaneToTensorOutputOffset(itemID.get_global(2))\r\n                                                                                             ^~~~~~~~~~\r\n                                                                                             set_global\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:145:67: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?\r\n       +indexMapper.mapCudaOutputKernelToTensorOutputOffset(itemID.get_local(0) + fitst_x_output_start, itemID.get_local(1) + fitst_y_output_start);\r\n                                                                   ^~~~~~~~~\r\n                                                                   set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:145:111: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?\r\n       +indexMapper.mapCudaOutputKernelToTensorOutputOffset(itemID.get_local(0) + fitst_x_output_start, itemID.get_local(1) + fitst_y_output_start);\r\n                                                                                                               ^~~~~~~~~\r\n                                                                                                               set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h: In member function 'void Eigen::EigenConvolutionKernel3D<CoeffReturnType, KernelType, HostExpr, FunctorExpr, Index, InputDims, Kernel_accessor, Buffer_accessor, Local_accessor, TupleType>::operator()(cl::sycl::nd_item<3>)':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:189:30: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?\r\n       for (size_t k = itemID.get_local(2); k < num_z_input; k += itemID.get_local_range()[2]) {\r\n                              ^~~~~~~~~\r\n                              set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:190:32: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?\r\n         for (size_t j = itemID.get_local(1); j < num_y_input; j += itemID.get_local_range()[1]) {\r\n                                ^~~~~~~~~\r\n                                set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:191:34: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?\r\n           for (size_t i = itemID.get_local(0); i < num_x_input ; i += itemID.get_local_range()[0]) {\r\n                                  ^~~~~~~~~\r\n                                  set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:208:17: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?\r\n       if(itemID.get_global(0)< range_x && itemID.get_global(1)< range_y && itemID.get_global(2)< range_z){\r\n                 ^~~~~~~~~~\r\n                 set_global\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:208:50: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?\r\n       if(itemID.get_global(0)< range_x && itemID.get_global(1)< range_y && itemID.get_global(2)< range_z){\r\n                                                  ^~~~~~~~~~\r\n                                                  set_global\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:208:83: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?\r\n       if(itemID.get_global(0)< range_x && itemID.get_global(1)< range_y && itemID.get_global(2)< range_z){\r\n                                                                                   ^~~~~~~~~~\r\n                                                                                   set_global\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:214:54: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?\r\n               const size_t local_index = ((i+ itemID.get_local(0))+  num_x_input*((j+ itemID.get_local(1)) + num_y_input * (k+ itemID.get_local(2))));\r\n                                                      ^~~~~~~~~\r\n                                                      set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:214:94: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?\r\n               const size_t local_index = ((i+ itemID.get_local(0))+  num_x_input*((j+ itemID.get_local(1)) + num_y_input * (k+ itemID.get_local(2))));\r\n                                                                                              ^~~~~~~~~\r\n                                                                                              set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:214:135: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?\r\n               const size_t local_index = ((i+ itemID.get_local(0))+  num_x_input*((j+ itemID.get_local(1)) + num_y_input * (k+ itemID.get_local(2))));\r\n                                                                                                                                       ^~~~~~~~~\r\n                                                                                                                                       set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:220:69: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?\r\n         +indexMapper.mapCudaOutputKernelToTensorOutputOffset(itemID.get_local(0) + fitst_x_output_start, itemID.get_local(1) + fitst_y_output_start, itemID.get_local(2) + fitst_z_output_start );\r\n                                                                     ^~~~~~~~~\r\n                                                                     set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:220:113: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?\r\n         +indexMapper.mapCudaOutputKernelToTensorOutputOffset(itemID.get_local(0) + fitst_x_output_start, itemID.get_local(1) + fitst_y_output_start, itemID.get_local(2) + fitst_z_output_start );\r\n                                                                                                                 ^~~~~~~~~\r\n                                                                                                                 set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:220:157: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?\r\n         +indexMapper.mapCudaOutputKernelToTensorOutputOffset(itemID.get_local(0) + fitst_x_output_start, itemID.get_local(1) + fitst_y_output_start, itemID.get_local(2) + fitst_z_output_start );\r\n                                                                                                                                                             ^~~~~~~~~\r\n                                                                                                                                                             set_local\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSycl.h:115,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:156,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/framework/types.h:23,\r\n                 from tensorflow/python/framework/python_op_gen_internal.cc:33:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclFunctors.h: In member function 'void Eigen::TensorSycl::internal::GenericKernelReducer<CoeffReturnType, OP, OutputAccessor, InputAccessor, LocalAccessor>::operator()(cl::sycl::nd_item<1>)':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclFunctors.h:31:32: error: 'struct cl::sycl::nd_item<1>' has no member named 'get_global'; did you mean 'set_global'?\r\n       size_t globalid = itemID.get_global(0);\r\n                                ^~~~~~~~~~\r\n                                set_global\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclFunctors.h:32:31: error: 'struct cl::sycl::nd_item<1>' has no member named 'get_local'; did you mean 'set_local'?\r\n       size_t localid = itemID.get_local(0);\r\n                               ^~~~~~~~~\r\n                               set_local\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSycl.h:117,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:156,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/framework/types.h:23,\r\n                 from tensorflow/python/framework/python_op_gen_internal.cc:33:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContractionSycl.h: In member function 'void Eigen::KernelConstructor<HostExpr, OutScalar, LhsScalar, RhsScalar, LHSFunctorExpr, RHSFunctorExpr, LhsLocalAcc, RhsLocalAcc, OutAccessor, Index, ContractT, LeftNocontractT, RightNocontractT, lhs_inner_dim_contiguous, rhs_inner_dim_contiguous, rhs_inner_dim_reordered, TileSizeDimM, TileSizeDimN, TileSizeDimK, WorkLoadPerThreadM, WorkLoadPerThreadN, LocalThreadSizeM, LocalThreadSizeN, LoadPerThreadLhs, LoadPerThreadRhs, LHSTupleType, RHSTupleType, Device>::operator()(cl::sycl::nd_item<2>)':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContractionSycl.h:232:43: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_local'; did you mean 'set_local'?\r\n       const Index mLocalThreadId = itemID.get_local(0); // Local ID row\r\n                                           ^~~~~~~~~\r\n                                           set_local\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContractionSycl.h:233:43: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_local'; did you mean 'set_local'?\r\n       const Index nLocalThreadId = itemID.get_local(1); // Local ID col\r\n                                           ^~~~~~~~~\r\n                                           set_local\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:157,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/framework/types.h:23,\r\n                 from tensorflow/python/framework/python_op_gen_internal.cc:33:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h: At global scope:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:400:7: error: template argument 'Eigen::internal::IsTileable<Eigen::SyclDevice, Expression>::value' involves template parameter(s)\r\n class TensorExecutor<Expression, SyclDevice, Vectorizable> {\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/python/framework/python_op_gen_internal.cc: In member function 'virtual std::__cxx11::string tensorflow::python_op_gen_internal::GenPythonOp::Code()':\r\ntensorflow/python/framework/python_op_gen_internal.cc:545:44: warning: comparison of integer expressions of different signedness: 'int' and 'std::vector<tensorflow::python_op_gen_internal::ParamNames>::size_type' {aka 'long unsigned int'} [-Wsign-compare]\r\n   for (int i = op_def_.input_arg_size(); i < params_no_default.size(); ++i) {\r\n                                          ~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/python/framework/python_op_gen_internal.cc:548:21: warning: comparison of integer expressions of different signedness: 'int' and 'std::vector<tensorflow::python_op_gen_internal::ParamNames>::size_type' {aka 'long unsigned int'} [-Wsign-compare]\r\n   for (int i = 0; i < params_with_default.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\n```\r\n", "comments": ["Can you show us where you define `--config=sycl_trisycl`. It looks like your linking is conflicting / setup wrong. Also did you run `./configure`?", "Yes, I did run `./configure` prior to calling bazel.\r\nThe `--config=sycl_trisycl` is part of the command when I call bazel `bazel build --config=opt --config=sycl_trisycl //tensorflow:libtensorflow.so //tensorflow/tools/pip_package:build_pip_package`\r\nAnd yes, indeed, based on the conflict, I think I have two macros, with different definitions. One in Tensorflow, one in triSYCL. Thus, the compiler doesn't know what to do at preprocess time (I guess).", "While it should eventually be fixed I wouldn't worry too much about the redefinition of EIGEN_MAX_ALIGN_BYTES, it is only a warning. It looks like it is defined twice in the command line: one is coming from [eigen.BUILD](https://github.com/tensorflow/tensorflow/blob/master/third_party/eigen.BUILD#L67) and the other from [trisycl.tpl](https://github.com/tensorflow/tensorflow/blob/master/third_party/sycl/crosstool/trisycl.tpl#L56). I don't know what the proper fix would be but I would remove the occurrence in eigen.BUILD if I were you.\r\n\r\nYour main issue here is that c++17 is not enabled which is required for triSYCL. I'm not sure how to build TF with triSYCL but it looks like you have to enable c++17 on the command line and see where it leads you.", "I think triSYCL GitHub platform can be good place to raise this question.(https://github.com/triSYCL/triSYCL/issues)"]}, {"number": 24388, "title": "Way to use exported graph from speech recognition/commands example in TF serving", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12.0\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI am using speech commands TF example and have trained and built the model with filename extension .pb and I would like to use somehow that model in tensorflow serving and thats where we have problem.\r\nTensorflow serving accepts other format of model that have trainedmodel.pb file and \"variables\" subdirectory.\r\n\r\nTF serving accepted model view:\r\n![image](https://user-images.githubusercontent.com/37185376/50054342-2e221b80-0141-11e9-9176-66a964c8a844.png)\r\n\r\nexported model from speech recognition example:\r\n![image](https://user-images.githubusercontent.com/37185376/50054362-5ad63300-0141-11e9-8953-870a8c9e9ea5.png)\r\n\r\nIf we are missing something here, excuse our incompetence and would likely help in any way here.\r\n\r\nAlso, if request is valid, there could be be used two approaches to tackle this request, one is allowing exporting of model from tf examples to format accepted by tf serving or to add feature to tf serving to accept more model variants.\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\nAny developer that is using tensorflow where model version variances would not be an obstacle to use tensorflow serving\r\n\r\n**Any Other info.**\r\n", "comments": ["This issue is more suitable for TF Serving repo. Please post it on TF Serving from [here](https://github.com/tensorflow/serving/issues). Thanks!"]}, {"number": 24387, "title": "[Feature Request]Deformable Convolutional Op Support", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):1.12.0\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nDeformable Convolutional Network(https://arxiv.org/pdf/1703.06211.pdf,https://arxiv.org/pdf/1703.06211.pdf) has achieved good results in the du detection task.But the current version of Tensorflow (1.12)does not seem to implement deformable convolution operations.The efficiency of some third-party implementations of deformable convolution operations is lower than that of the mxnet version(https://github.com/msracver/Deformable-ConvNets).I hope that Tensorflow can add native deformable convolution operations in future versions.\r\nThanks.\r\n**Will this change the current api? How?**\r\nYes,Add a new API\r\n**Who will benefit with this feature?**\r\nAnyone who wants to use Tensorflow for object detection\r\n\r\n**Any Other info.**\r\nN/A", "comments": ["https://arxiv.org/abs/1811.11168", "This is going to be an important contribution. I hope someone takes a look", "@tensorflowbutler Hi, could you please answer this question? Deformable convolution is effective and fast and maybe it could be a possible improvement in the future version of tensorflow.", "This Is half-duplicated with https://github.com/tensorflow/addons/issues/179", "Hi @rootkitchao!\r\nWe are checking to see whether you still need help in this issue . Have you checked this [thread](https://discuss.tensorflow.org/t/deformable-convolution-and-other-custom-ops/1951) yet?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 24386, "title": "TensorForest: Performance issues in time and space", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary, pip install tensorflow==1.12.0\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: No usage\r\n- GPU model and memory: No usage\r\n- Machine RAM and #Cores: **64GB RAM and 12 CPU Cores** \r\n\r\n**Describe the current behavior**\r\n\r\nCurrently, I trained TensorForest on a relatively large train data (1.5M examples, 1024 features) with 500 trees, max_nodes. Moreover, I compared its performance with Scikit-learn implementation.\r\nIt seems that the TensorForest is inferior to Scikit-Learn ExtraTrees implementation both in time and space.\r\n\r\n**In terms of space costs**, training a TensorForest forest  consists of 500 trees with 10k nodes will throw the following exception in my machine (64GB RAM and 12 CPU Cores).\r\n```\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1500000,500,10] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[node stack (defined at /home/experiment/huqiu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/tensor_forest/python/tensor_forest.py:519)  = Pack[N=500, T=DT_FLOAT, axis=1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](TreePredictionsV4, TreePredictionsV4_1, TreePredictionsV4_2, TreePredictionsV4_3, TreePredictionsV4_4, ...... TreePredictionsV4_499)\r\n```\r\nBut Scikit-learn based extra trees classifier only costs up to **18GB** of memory.\r\n\r\nThis issue makes it unfeasible to train TensorForest on large datasets or large number of trees.\r\nAs the paper said, It seems that TensorForest should not have such a big performance gap compared with scikit-learn, right? \r\nThus I wonder whether it is my mis-use?\r\n\r\n**In terms of time costs**,  training a TensorForest forest  consists of **only 2** trees with 10k nodes will cost **1235 second** with 116 training steps on the machine. But Scikit-learn based implementation costs only **20** seconds.\r\nThis 60-fold gap is clearly unreasonable, right?\r\n\r\n**Describe the expected behavior**\r\n\r\n1. Isn't it an online algorithm? It seems that the memory consumption of the TensorForest is relatively large?\r\n2. The original paper said that, in large dataset (like HIGGS, 11M examples, 28 features), TensorForest with 100 trees and 10k nodes per tree trains in about **one percent** of the time taken by scikit-learn, even without taking advantage of distributed training. But I cannot reproduce that fast training speed.\r\n\r\nThus I am sincerely looking forward to your help or explanations.\r\nGreat, Great, Great thanks : )\r\n@yupbank @yongtang @tensorflower-gardener\r\n\r\n**Code to reproduce the issue**\r\n\r\nComplete Code Snippet, you can run it directely without any modification.\r\n============================== Space Cost Comparison =========================\r\nTensorForest:\r\n```\r\nimport time\r\nimport tensorflow as tf\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\ntf.keras.backend.set_image_data_format('channels_first')\r\n\r\n(x_train, y_train), _ = tf.keras.datasets.cifar10.load_data()\r\nx_train = x_train.reshape((-1, 32*32)).astype('float32')   # produce more examples\r\ny_train = y_train.repeat(3, axis=0).astype('int')\r\nx_train = x_train.repeat(10, axis=0)     # repeat 10 times to produce more examples\r\ny_train = y_train.repeat(10, axis=0).reshape(-1)\r\n\r\nprint(x_train.shape, y_train.shape)      # totally 1500000 examples\r\n# (1500000, 1024) (1500000,)\r\n\r\nstart_time = time.time()\r\nest_args = {'num_classes': 10, 'num_features': 1024, 'regression': False,\r\n                   'num_trees': 500, 'max_nodes': 10000,\r\n                   'base_random_seed': 0}\r\n\r\nparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(**est_args)\r\nestimator = tf.contrib.tensor_forest.client.random_forest.TensorForestEstimator(params)\r\nestimator.fit(x_train, y_train)\r\nprint(\"Time Cost: {}\".format(time.time() - start_time))\r\n\r\n# Exception: ResourceExhaustedError\r\n```\r\n\r\nScikit-learn:\r\n```\r\nimport time\r\nimport tensorflow as tf\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\ntf.keras.backend.set_image_data_format('channels_first')\r\n\r\n(x_train, y_train), _ = tf.keras.datasets.cifar10.load_data()\r\nx_train = x_train.reshape((-1, 32*32)).astype('float32')   # produce more examples\r\ny_train = y_train.repeat(3, axis=0).astype('int')\r\nx_train = x_train.repeat(10, axis=0)     # repeat 10 times to produce more examples\r\ny_train = y_train.repeat(10, axis=0).reshape(-1)\r\n\r\nprint(x_train.shape, y_train.shape)      # totally 1500000 examples\r\n# (1500000, 1024) (1500000,)\r\n\r\nstart_time = time.time()\r\nest_args = {'num_classes': 10, 'num_features': 1024, 'regression': False,\r\n                   'num_trees': 500, 'max_nodes': 10000,\r\n                   'base_random_seed': 0}\r\n\r\nfrom sklearn.ensemble import ExtraTreesClassifier\r\nestimator = ExtraTreesClassifier(n_estimators=500, n_jobs=-1)\r\nestimator.fit(x_train, y_train)\r\nprint(\"Time Cost: {}\".format(time.time() - start_time))\r\n\r\n# Time Cost: 885.44 (seconds)\r\n```\r\n\r\n============================== Time Cost Comparison =========================\r\nTensorForest:\r\n```\r\nimport time\r\nimport tensorflow as tf\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\ntf.keras.backend.set_image_data_format('channels_first')\r\n\r\n(x_train, y_train), _ = tf.keras.datasets.cifar10.load_data()\r\nx_train = x_train.reshape((-1, 32*32)).astype('float32')   # produce more examples\r\ny_train = y_train.repeat(3, axis=0).astype('int')\r\nx_train = x_train.repeat(10, axis=0)     # repeat 10 times to produce more examples\r\ny_train = y_train.repeat(10, axis=0).reshape(-1)\r\n\r\nprint(x_train.shape, y_train.shape)      # totally 1500000 examples\r\n# (1500000, 1024) (1500000,)\r\n\r\nstart_time = time.time()\r\nest_args = {'num_classes': 10, 'num_features': 1024, 'regression': False,\r\n                   'num_trees': 2, 'max_nodes': 10000,\r\n                   'base_random_seed': 0}\r\n\r\nparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(**est_args)\r\nestimator = tf.contrib.tensor_forest.client.random_forest.TensorForestEstimator(params)\r\nestimator.fit(x_train, y_train)\r\nprint(\"Time Cost: {}\".format(time.time() - start_time))\r\n\r\n# Time Cost: 1235 (seconds)\r\n```\r\n\r\nScikit-learn:\r\n```\r\nimport time\r\nimport tensorflow as tf\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\ntf.keras.backend.set_image_data_format('channels_first')\r\n\r\n(x_train, y_train), _ = tf.keras.datasets.cifar10.load_data()\r\nx_train = x_train.reshape((-1, 32*32)).astype('float32')   # produce more examples\r\ny_train = y_train.repeat(3, axis=0).astype('int')\r\nx_train = x_train.repeat(10, axis=0)     # repeat 10 times to produce more examples\r\ny_train = y_train.repeat(10, axis=0).reshape(-1)\r\n\r\nprint(x_train.shape, y_train.shape)      # totally 1500000 examples\r\n# (1500000, 1024) (1500000,)\r\n\r\nstart_time = time.time()\r\nest_args = {'num_classes': 10, 'num_features': 1024, 'regression': False,\r\n                   'num_trees': 500, 'max_nodes': 10000,\r\n                   'base_random_seed': 0}\r\n\r\nfrom sklearn.ensemble import ExtraTreesClassifier\r\nestimator = ExtraTreesClassifier(n_estimators=500, n_jobs=-1)\r\nestimator.fit(x_train, y_train)\r\nprint(\"Time Cost: {}\".format(time.time() - start_time))\r\n\r\n# Time Cost: 20.39 (seconds)\r\n```\r\n\r\n@yupbank \r\n", "comments": ["Thanks for the issue I\u2019ll take a look at it ", "@yupbank thanks : ) \r\nI think I know where the problem occurs.\r\nThe official guide said that using `estimator = tf.contrib.tensor_forest.client.random_forest.TensorForestEstimator(params)`\r\nto train and predict.\r\nBut the arguments passing brought the problem.\r\nIf we do not provide the batch size when fit, the default batch size should be 128.\r\nBut if we directly use `estimator.fit(x_train, y_train)`, the batch size will be None and finally becomes the size of full dataset.\r\nThus, we actually do full batch training!\r\n\r\nTo fix it, I currently wrap the TensorForestEstimator in the SKCompat.\r\nThat is to say, I use:\r\n```\r\nfrom tensorflow.contrib.learn.python.learn.estimators.estimator import SKCompat\r\nparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(**est_args)\r\nestimator = SKCompat(tf.contrib.tensor_forest.client.random_forest.TensorForestEstimator(params))\r\nestimator.fit(x_train, y_train)\r\n```\r\nThis code will use batchsize = 128 to train an online TensorForest.\r\n\r\nThank you : )\r\n", "Thanks for the explanation! Yes the batch size matters here. \r\n\r\nA lot of the API you are using is being deprecated, but we havnt merge the core estimator code. So yeah ... \r\n\r\n", "@yupbank yes, I know some APIs are being deprecated, will they appear in the next version? in other forms? e.g. convert TensorForestEstimator to an estimator of `tf.estimators` ?", "Yes yes I\u2019m working on that\n\nSent from my iPhone\n\n> On Dec 15, 2018, at 22:22, Qiu Hu <notifications@github.com> wrote:\n> \n> @yupbank yes, I know some APIs are being deprecated, will they appear in the next version? in other forms? e.g. convert TensorForestEstimator to an estimator of tf.estimators ?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "Thanks.\r\nWishing for your significant work!"]}, {"number": 24385, "title": "Tensorflow : Encountered error while reading extension file 'closure/defs.bzl' issue", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):\r\n- TensorFlow installed from (source):\r\n- TensorFlow version:latest\r\n- Python version:3\r\n- Installed using pip\r\n- Bazel version (0.20):\r\n- GCC/Compiler version (if compiling from source):\r\n\r\n-Compiling on Odroid\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI am compiling tensorflow on arm architecture and getting the following error\r\nCommand Run:\r\nodroid@odroid:~/Desktop/Pratik/tensorflow$ bazel build -c opt --copt=\"-mfpu=neon-vfpv4\" --copt=\"-funsafe-math-optimizations\" --copt=\"-ftree-vectorize\" --copt=\"-fomit-frame-pointer\" --local_resources 1024,1.0,1.0 --verbose_failures tensorflow/tools/pip_package:build_pip_package\r\n\r\nError:\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/home/odroid/Desktop/Pratik/tensorflow/tools/bazel.rc\r\nINFO: Invocation ID: 09bae6ef-fd25-4864-86b4-a9ccaaba3340\r\nERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': The native http_archive rule is deprecated. load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\") for a drop-in replacement.\r\nUse --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.\r\nERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': The native http_archive rule is deprecated. load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\") for a drop-in replacement.\r\nUse --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.\r\nINFO: Elapsed time: 0.307s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    Fetching @io_bazel_rules_closure; fetching\r\n\r\n", "comments": ["Downgrade to bazel 0.19.1", "crosspost: https://stackoverflow.com/a/53708415\r\n> As of version 1.12.0, TensorFlow uses some deprecated Bazel features that are being completely dropped in recent versions of Bazel. Instead of using the most recent version, try using an older one for now. I was able to build TensorFlow 1.12.0 on Windows using Bazel 1.18.1, most likely that should work with TensorFlow 1.11 too.\r\n> answered Dec 10 at 15:08\r\n> jdehesa", "Please dont release a version of Bazel if it breaks tensorflow. \r\n\r\nTensorflow is the reason a lot of us are using bazel and there is always an incompatibility with the latest bazel that requires users to fish around and find the last stable version of Bazel. \r\n\r\n", "@gandhipratik203 Is this still an issue for you?", "This is not the first time this has happened, and I'm tired of the latest release of TensorFlow not building with the latest release of Bazel.  Like powderluv said, many of us are only using Bazel because of TensorFlow, so some better integration testing before releases would really be nice.", "> Please dont release a version of Bazel if it breaks tensorflow.\r\n\r\nTensorflow doesn't release Bazel. Bazel is a build tool, like Make, Cmake etc.\r\n\r\nhttps://docs.bazel.build/versions/master/bazel-overview.html\r\n\r\n> Tensorflow is the reason a lot of us are using bazel and there is always an incompatibility with the latest bazel that requires users to fish around and find the last stable version of Bazel.\r\n\r\nAs adamcrume points out, a better approach is for Tensorflow to test against the latest version of Bazel.", "> > Please dont release a version of Bazel if it breaks tensorflow.\r\n> \r\n> Tensorflow doesn't release Bazel. Bazel is a build tool, like Make, Cmake etc.\r\n\r\nUnderstood. But Tensorflow stuffs Bazel down our throat.  If CMake was a first class citizen a lot of us wouldn't touch bazel with a 10foot pole :)\r\n\r\n> \r\n> https://docs.bazel.build/versions/master/bazel-overview.html\r\n> \r\n> > Tensorflow is the reason a lot of us are using bazel and there is always an incompatibility with the latest bazel that requires users to fish around and find the last stable version of Bazel.\r\n> \r\n> As adamcrume points out, a better approach is for Tensorflow to test against the latest version of Bazel.\r\n\r\nAgreed. But the problem arises when top of master TF is ok but the latest stable branch doesn't work with the latest bazel that is pushed onto all Ubuntu systems which installed via the ppa. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Note starting from TF 1.13 we have updated the ```configure.py``` which checks for the bazel version. The min, max versions are '0.19.0', '0.21.0' Thanks!\r\n", "My reduction of tensorflow to 1.12 bazel to 0.18.1 is still a problem.", "> Please dont release a version of Bazel if it breaks tensorflow.\r\n> \r\n> Tensorflow is the reason a lot of us are using bazel and there is always an incompatibility with the latest bazel that requires users to fish around and find the last stable version of Bazel.\r\n\r\nYes! I am nearly crazy by the bazel!!!! I only want to use tensorflow!", "I'm also facing the same problem. I've complied Bazel from source and the bazel has been compiled and installed successfully.\r\n\r\nThe installed version is shown in below image.\r\n\r\n![Screenshot from 2019-10-01 16-37-34](https://user-images.githubusercontent.com/47119373/65957191-dc494880-e469-11e9-822d-17d48c971988.png)\r\n\r\nI've clone the tensorflow-1.13.1 repository and configured it and ran the code `bazel build -c opt --copt=\"-funsafe-math-optimizations\" --copt=\"-ftree-vectorize\" --copt=\"-fomit-frame-pointer\" --verbose_failures tensorflow/tools/pip_package:build_pip_package` when I run it, It produced the error as shown below.\r\n\r\n![Screenshot from 2019-10-01 16-41-37](https://user-images.githubusercontent.com/47119373/65957413-672a4300-e46a-11e9-8cfe-76177728c569.png)\r\n\r\nWhat went wrong here?\r\n\r\n"]}]