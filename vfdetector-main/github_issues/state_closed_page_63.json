[{"number": 53379, "title": "Python Configuration Error: Problem getting numpy include path while building with Bazel", "body": "<em>I am facing error trying to build tensorflow from source. My goal is to use tensorflow API in C++. I am following [this doc](https://www.tensorflow.org/install/source) for that. But whenever I run `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` I get `Python Configuration Error: Problem getting numpy include path.`</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 2.1\r\n- Python version: 3.6.9\r\n- Bazel version: 0.29.1\r\n\r\n\r\n\r\n**Describe the problem**\r\nI have tried to build tensorflow in Ubuntu 18.04 created in Oracle VM VirtualBox.  When I tried to run the `bazel build [--config=option] //tensorflow/tools/pip_package:build_pip_package` command to build _tensorflow cpu-only_ version, The following error message persists:\r\n```\r\nERROR: no such target '//:[--config=option]': target '[--config=option]' not declared in package '' defined by /home/niaz/Desktop/TF_C++_API/tensorflow/BUILD\r\nINFO: Elapsed time: 0.268s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded)\r\n```\r\nThen according to [this reply](https://github.com/tensorflow/tensorflow/issues/44876#issuecomment-727388512) in another issue, I changed the command into `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`. But then the following error persists:\r\n```\r\nPython Configuration Error: Problem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named numpy\r\nIs numpy installed?\r\nINFO: Elapsed time: 0.966s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (2 packages loaded, 5 targets con\\\r\nfigured)\r\n\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nAccording to [the doc](https://www.tensorflow.org/install/source) I have followed the following steps to install Python and the TensorFlow package dependencies:\r\n`sudo apt install python3-dev python3-pip`\r\n`pip install -U --user pip numpy wheel`\r\n`pip install -U --user keras_preprocessing --no-deps`\r\nThen I have installed Bazel 0.29.1 from [bazel github](https://github.com/bazelbuild/bazel/releases?page=6) using _bazel-0.29.1-linux-x86_64_.\r\nAfter that, I have cloned the [Tensorflow repo](https://github.com/tensorflow/tensorflow.git) and checked into _r2.1_ branch.\r\nThen I have run `./configure` and the following is the output:\r\n```\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: Waiting for server process to terminate (waited 5 seconds, waiting at most 60)\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.29.1 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n```\r\n\r\nAfter that, I have run `bazel build [--config=option] //tensorflow/tools/pip_package:build_pip_package` command and the following is the output:\r\n```\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=79\r\nINFO: Reading rc options for 'build' from /home/niaz/Desktop/TF_C++_API/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --config=v2\r\nINFO: Reading rc options for 'build' from /home/niaz/Desktop/TF_C++_API/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --host_force_python=PY2 --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages --python_path=/usr/bin/python --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /home/niaz/Desktop/TF_C++_API/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nERROR: Skipping '[--config=option]': no such target '//:[--config=option]': target '[--config=option]' not declared in package '' defined by /home/niaz/Desktop/TF_C++_API/tensorflow/BUILD\r\nERROR: no such target '//:[--config=option]': target '[--config=option]' not declared in package '' defined by /home/niaz/Desktop/TF_C++_API/tensorflow/BUILD\r\nINFO: Elapsed time: 2.548s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n```\r\nThen according to [this reply](https://github.com/tensorflow/tensorflow/issues/44876#issuecomment-727388512) in another issue, I changed the command to `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`. But then the following is the output:\r\n```\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=79\r\nINFO: Reading rc options for 'build' from /home/niaz/Desktop/TF_C++_API/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --config=v2\r\nINFO: Reading rc options for 'build' from /home/niaz/Desktop/TF_C++_API/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --host_force_python=PY2 --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages --python_path=/usr/bin/python --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /home/niaz/Desktop/TF_C++_API/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:opt in file /home/niaz/Desktop/TF_C++_API/tensorflow/.tf_configure.bazelrc: --copt=-march=native --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true\r\nINFO: Call stack for the definition of repository 'local_config_python' which is a python_configure (rule definition at /home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl:347:20):\r\n - /home/niaz/Desktop/TF_C++_API/tensorflow/tensorflow/workspace.bzl:77:5\r\n - /home/niaz/Desktop/TF_C++_API/tensorflow/WORKSPACE:19:1\r\nERROR: An error occurred during the fetch of repository 'local_config_python':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl\", line 345\r\n\t\t_create_local_python_repository(repository_ctx)\r\n\tFile \"/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl\", line 296, in _create_local_python_repository\r\n\t\t_get_numpy_include(repository_ctx, python_bin)\r\n\tFile \"/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl\", line 276, in _get_numpy_include\r\n\t\t_execute(repository_ctx, [python_bin, \"-c\",...\"], <2 more arguments>)\r\n\tFile \"/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl\", line 56, in _execute\r\n\t\t_fail(\"\\n\".join([error_msg.strip() if ... \"\"]))\r\n\tFile \"/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl\", line 27, in _fail\r\n\t\tfail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: Problem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named numpy\r\nIs numpy installed?\r\nINFO: Call stack for the definition of repository 'gast_archive' which is a tf_http_archive (rule definition at /home/niaz/Desktop/TF_C++_API/tensorflow/third_party/repo.bzl:121:19):\r\n - /home/niaz/Desktop/TF_C++_API/tensorflow/tensorflow/workspace.bzl:336:5\r\n - /home/niaz/Desktop/TF_C++_API/tensorflow/WORKSPACE:19:1\r\nINFO: Call stack for the definition of repository 'astor_archive' which is a tf_http_archive (rule definition at /home/niaz/Desktop/TF_C++_API/tensorflow/third_party/repo.bzl:121:19):\r\n - /home/niaz/Desktop/TF_C++_API/tensorflow/tensorflow/workspace.bzl:312:5\r\n - /home/niaz/Desktop/TF_C++_API/tensorflow/WORKSPACE:19:1\r\nERROR: While resolving toolchains for target //tensorflow:libtensorflow_framework.so.2.1.4: invalid registered toolchain '@local_config_python//:py_toolchain': no such package '@local_config_python//': Traceback (most recent call last):\r\n\tFile \"/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl\", line 345\r\n\t\t_create_local_python_repository(repository_ctx)\r\n\tFile \"/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl\", line 296, in _create_local_python_repository\r\n\t\t_get_numpy_include(repository_ctx, python_bin)\r\n\tFile \"/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl\", line 276, in _get_numpy_include\r\n\t\t_execute(repository_ctx, [python_bin, \"-c\",...\"], <2 more arguments>)\r\n\tFile \"/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl\", line 56, in _execute\r\n\t\t_fail(\"\\n\".join([error_msg.strip() if ... \"\"]))\r\n\tFile \"/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl\", line 27, in _fail\r\n\t\tfail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: Problem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named numpy\r\nIs numpy installed?\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: invalid registered toolchain '@local_config_python//:py_toolchain': no such package '@local_config_python//': Traceback (most recent call last):\r\n\tFile \"/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl\", line 345\r\n\t\t_create_local_python_repository(repository_ctx)\r\n\tFile \"/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl\", line 296, in _create_local_python_repository\r\n\t\t_get_numpy_include(repository_ctx, python_bin)\r\n\tFile \"/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl\", line 276, in _get_numpy_include\r\n\t\t_execute(repository_ctx, [python_bin, \"-c\",...\"], <2 more arguments>)\r\n\tFile \"/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl\", line 56, in _execute\r\n\t\t_fail(\"\\n\".join([error_msg.strip() if ... \"\"]))\r\n\tFile \"/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl\", line 27, in _fail\r\n\t\tfail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: Problem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named numpy\r\nIs numpy installed?\r\nINFO: Elapsed time: 3.397s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (70 packages loaded, 127 targets \\\r\nconfigured)\r\n\r\n```\r\n\r\n\r\n\r\n**Any other info / logs**\r\nI have tried this with Ubuntu 20.04 first and Ubuntu 18.04 after that as according to [this bazel build doc](https://docs.bazel.build/versions/main/install-ubuntu.html) Bazel is testedly supported for Ubuntu 18.04 and Ubuntu 16.04, but the problem still persists.\r\nI have also checked with `pip3 freeze` and `pip3 list` that `numpy==1.19.5` version exists in my environment. I am not using any virtual environment currently. I have checked some related issues but none of them solved my problem.\r\n", "comments": ["@nitolpalak We see that you are using older version of TF 2.1 , could you please try to upgrade to latest TF v2.7.0 and let us know the outcome? Thank you! ", "@sushreebarsa I was able to solve the issue after installing _numpy_ and _future_ using both pip and pip3. Looks like the problem was previously I installed numpy and future only in pip3. After installing those packages in pip too, the previous error was fixed. But currently I am having a new issue. I would really like to use TF 2.1 if possible as some other libraries I am currently working with depends on this. The current error is as below:\r\n```\r\n[7,040 / 8,707] 6 actions running\r\n    Compiling tensorflow/core/kernels/broadcast_to_op.cc; 723s local\r\n    Compiling tensorflow/core/kernels/bias_op.cc; 717s local\r\n[7,040 / 8,707] 6 actions running\r\n    Compiling tensorflow/core/kernels/broadcast_to_op.cc; 725s local\r\n    Compiling tensorflow/core/kernels/bias_op.cc; 719s local\r\nERROR: /home/niaz/Desktop/TF_C++_API/tensorflow/tensorflow/core/kernels/BUILD:1048:1: C++ compilation of rule '//tensorflow/core/kernels:broadcast_to_op' failed (Exit 4)\r\n[7,041 / 8,707] 5 actions running\r\n    Compiling tensorflow/core/kernels/bias_op.cc; 719s local\r\n    Compiling tensorflow/core/kernels/conv_grad_input_ops.cc; gcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\n[7,041 / 8,707] 5 actions running\r\n    Compiling tensorflow/core/kernels/bias_op.cc; 719s local\r\n    Compiling tensorflow/core/kernels/conv_grad_input_ops.cc; [7,041 / 8,707] 5 actions running\r\n    Compiling tensorflow/core/kernels/bias_op.cc; 719s local\r\n    Compiling tensorflow/core/kernels/conv_grad_input_ops.cc; [7,041 / 8,707] 5 actions running\r\n    Compiling tensorflow/core/kernels/bias_op.cc; 720s local\r\n    Compiling tensorflow/core/kernels/conv_grad_input_ops.cc; [7,041 / 8,707] 5 actions running\r\n    Compiling tensorflow/core/kernels/bias_op.cc; 721s local\r\n    Compiling tensorflow/core/kernels/conv_grad_input_ops.cc; Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2744.898s, Critical Path: 725.69s\r\nINFO: 4377 processes: 4377 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "@sushreebarsa I have tried to build tensorflow 2.7 in my windows 10 environment and posted an issue regarding that [here](https://github.com/tensorflow/tensorflow/issues/53382)", "@nitolpalak We are supporting TF v2.4 and later .Older versions are no longer supported.As per your above [comment](https://github.com/tensorflow/tensorflow/issues/53379#issuecomment-990528488) ,if you want to use TF v2.1 then could you please post this issue in [TF forum](https://discuss.tensorflow.org/) where there is a larger community to help? We are tracking this #53382 ticket as well. Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53379\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53379\">No</a>\n"]}, {"number": 53378, "title": "[oneDNN] Making a slight tweak to the cost model based on benchmarking.", "body": "This PR tweaks the cost model based on recent benchmarking.", "comments": []}, {"number": 53377, "title": "ValueError: Unsupported data type 14 in tensor", "body": "_<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>_\r\n\r\n**System information**\r\n- Have I written custom code : `interpreter = tf.lite.Interpreter(model_path=\"models/saved_model.tflite\")`\r\n- OS Platform and Distribution : **`RaspberryPi 3B+`**\r\n- TensorFlow Git version : `v2.4.0-0-g582c8d2` \r\n- TensorFlow version : `2.4.0`\r\n- Installation Source : [`https://github.com/bitsy-ai/tensorflow-arm-bin/releases/download/v2.4.0/tensorflow-2.4.0-cp37-none-linux_armv7l.whl`](https://github.com/bitsy-ai/tensorflow-arm-bin/releases/download/v2.4.0/tensorflow-2.4.0-cp37-none-linux_armv7l.whl)\r\n- Python version: `Python 3.7.3`\r\n- More Info about the **tflite** Model : Custom trained **EfficientDet-B0** on `40` classes\r\n\r\n#### ERROR : \r\n```\r\nTraceback (most recent call last):\r\n  File \"detect.py\", line 8, in <module>\r\n    interpreter = tf.lite.Interpreter(model_path=\"models/saved_model.tflite\")\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py\", line 209, in __init__\r\n    model_path, self._custom_op_registerers))\r\nValueError: Unsupported data type 14 in tensor\r\nUnsupported data type 14 in tensor\r\nUnsupported data type 14 in tensor\r\nUnsupported data type 14 in tensor\r\nUnsupported data type 14 in tensor\r\nUnsupported data type 14 in tensor\r\n```\r\n\r\n> Note : If you are suggesting me to upgrade or downgrade the packages, please provide necessary code with it.\r\n\r\nThanks @tilakrayal !!\r\n\r\n\r\n\r\n", "comments": ["@gaurxvreddy ,\r\nCould you please update TensorFlow to the latest stable version v2.7 and let us know if you are facing the same error. Thanks!", "Is there `TF 2.7.0` available on Raspberry Pi, If yes, Can you provide me with the instructions to install it ? ", "Hey @tilakrayal, \r\n\r\nIt would be great if we could get some traction on this issue, is it possible for you to look at this?", "Update : Still facing the issue\r\n@tilakrayal \r\n", "@gaurxvreddy ,\r\nCan you please take a look at this [doc](https://www.tensorflow.org/lite/guide/build_arm) for the installation.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53377\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53377\">No</a>\n"]}, {"number": 53376, "title": "Explicitly reset interpreter to prevent memory leaks when using delegate", "body": "Internally at Broadcom, we ran into a memory leak when running the label_image app using an external HW delegate.  The delegate was freed/released before the some of the graph data, causing our libraries to report an memory leak.\r\n\r\nThe fix, calling \"Interpreter.reset()\" is already done in other test apps.  E.g. see: https://github.com/tensorflow/tensorflow/commit/23873d4d6518a6f169600a076001dc2d7a661dd0 by @terryheo \r\n\r\nChange-Id: I884cde7e31d84b1bcd7b90fe77eb4b2321f3df0c", "comments": ["@mihaimaruseac, Yes, the destructor called at the end of scope, but the order of freeing memory matters when using a delegate because the memory may be owned/managed by the delegate itself.  The interpreter contains references to data structures that may have been allocated by the delegate.  Without this fix, the delegate is destroyed **before** the interpreter.  Our delegate detects and reports a warning about the memory leak.  The proposed fix forces the interpreter to free the memory before the delegate is destroyed.\r\n\r\nTechnically, in either case, the memory is _eventually_ freed, but the current implementation is simply not correct since the memory might be end up being used elsewhere.  \r\n\r\nYou'll see that many other example apps also explicitly call interpreter.reset().  I'm only adding the same fix to the label_image example app.\r\n\r\nPerhaps a better solution is, at the end of scope, to somehow destroy interpreter before the delegate.  "]}, {"number": 53375, "title": "[*.h,*.hpp,*.cpp,*.proto] clang-format", "body": "`clang-format` 13.0.0 at 9ebfeab\r\n\r\n```\r\n$ fd -eh -ec -ehpp -ecpp -eproto -x clang-format -i --style=Google {} \\;\r\n```\r\n\r\n---\r\n\r\n**Related**:\r\n\r\n   - https://github.com/tensorflow/addons does a bunch of auto format and auto linting\r\n   - My recently merged PR: #53329\r\n   - My #53360\r\n   - My #53374\r\n\r\n**Disadvantages**:\r\n\r\n  - Impact just about every file in the codebase\r\n  - Require open PRs to be modified and forks to be updated\r\n\r\n**Mitigation of disadvantages**:\r\n\r\n  - To avoid stepping on toes, one can write a pull request scanner to avoid touching any file that has a current pull request lodged against it. I'm happy to write this script, but will also need someone to run this internally so it knows what files are being affected by non public PRs\r\n\r\n**Advantages**:\r\n\r\n  - Easier for automated tooling to make automated fixes (whitespace / prettyprinting issues don't get in the way)\r\n  - More consistent codebase (which is somewhat of a quality metric)", "comments": ["Closed as per @MarkDaoust's comments"]}, {"number": 53374, "title": "[*.py,*.ipynb] blacken", "body": "Using version 21.12b0 from Python 3.10:\r\n\r\n```sh\r\n$ python -m black .\r\n[\u2026]\r\nAll done! \u2728 \ud83c\udf70 \u2728\r\n2767 files reformatted, 77 files left unchanged.\r\n```\r\n\r\n---\r\n\r\n**Related**:\r\n\r\n   - https://github.com/tensorflow/addons does a bunch of auto format and auto linting\r\n   - My recently merged PR: #53329\r\n   - My #53360\r\n\r\n**Disadvantages**:\r\n\r\n  - Impact just about every file in the codebase\r\n  - Require open PRs to be modified and forks to be updated\r\n\r\n**Mitigation of disadvantages**:\r\n\r\n  - To avoid stepping on toes, one can write a pull request scanner to avoid touching any file that has a current pull request lodged against it. I'm happy to write this script, but will also need someone to run this internally so it knows what files are being affected by non public PRs\r\n\r\n**Advantages**:\r\n\r\n  - Easier for automated tooling to make automated fixes (whitespace / prettyprinting issues don't get in the way)\r\n  - More consistent codebase (which is somewhat of a quality metric)", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/53374\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>", "Good enthusiasm, but there's no way we can accept this. \r\n\r\n* We have our own [style guide](https://google.github.io/styleguide/pyguide.html) and autoformatter. That works for us. \r\n* The google python style says 2-space indent, but you're changing it to 4. \r\n* There's no way we can properly review a 2700 file change.", "@MarkDaoust Yeah I expected as much which is why I created #53360 to discuss\u2026 but then he told me to open PRs.\r\n\r\nDo you apply your autoformatter across the whole codebase, and have linters to validate that new commits are formatted correctly?", "> but then he told me to open PRs.\r\n\r\nYeah, the first line issue management team doesn't quite have the same insight as the people on the project so there are sometimes little misunderstandings like this. I hope we didn't waste much of your time.\r\n\r\n> Do you apply your autoformatter across the whole codebase, and have linters to validate that new commits are formatted correctly?\r\n\r\nYes. Notmally changes don't go anywhere unless the linters pass. There are ways to `--force` past it, but usually people just run the format-all command and it's fixed."]}, {"number": 53373, "title": "tfjs-v3.12.0 and Wasm 3.12.0 for tfjs-models/pose-detection/  MoveNet  multipose wasm", "body": "When I try to run the demo of the model : tfjs-models/pose-detection/  : https://storage.googleapis.com/tfjs-models/demos/pose-detection/index.html?model=movenet \r\n\r\nWith tfjs-v3.12.0 and Wasm 3.12.0 and configuration model MoveNet type Multipose and backend Wasm  i got the following error :\r\n\r\nError: Kernel 'Reciprocal' not registered for backend 'wasm'\r\n\r\n![image](https://user-images.githubusercontent.com/32233417/145433791-63df6bdb-87a1-4e08-a7cb-b72e41773c18.png)\r\n\r\nThe demo Woks well for the singlepose type (lightning).\r\n\r\nRegards", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53373\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53373\">No</a>\n", "Open issue in good repo : https://github.com/tensorflow/tfjs/issues/5932\r\n"]}, {"number": 53372, "title": "How to train specific variables in SavedModel in Tensorflow format ?", "body": "I have a pre-trained model in Tensorflow (not keras) format which I want to retrain .\r\n[mobilenetv2.zip](https://github.com/tensorflow/tensorflow/files/7685965/mobilenetv2.zip)\r\n\r\nHowever, in this model the list of variables is empty. The re-training needs to be done on a certain set of variables but those variables are not declared as trainable. How to access the variables from this model and make them trainable?\r\n\r\n\r\n", "comments": ["@adjhawar Could you please refer to the [SavedModel](https://www.tensorflow.org/guide/saved_model) format and let us know if it helps?Thank you!", "@sushreebarsa  I have tried to access the .trainable_variables and .variables attributes from the SavedModel but the lists returned from both are empty. However, I am able to run inference on the model, which implies that the variables are there in the model. \r\nI wanted to know if there was a way to get the list of variables.\r\nThe task I am trying to accomplish is to retrain the some part of the model and for that I require trainable_variables, but the list returned from the pre-trained SavedModel is empty", "@adjhawar \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@sushreebarsa \r\ntf_model = tf.saved_model.load(\"attached model\")\r\nprint(tf_model.signatures[\"serving_default\"].variables)\r\nprint(tf_model.variables)\r\n\r\nOutput:\r\n\r\n()\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-00ec584780bb> in <module>\r\n      1 print(tf_model.signatures[\"serving_default\"].variables)\r\n----> 2 print(tf_model.variables)\r\n\r\nAttributeError: '_UserObject' object has no attribute 'variables'\r\n", "@adjhawar \r\nThis is not a bug or feature request ,could you please refer to the [tutorial](https://www.tensorflow.org/hub/tutorials/tf2_image_retraining) , article[ link](https://medium.com/@sneha.bhat/retraining-tensorflow-model-with-new-dataset-for-object-detection-in-an-android-application-eb7461516a94) and for any further queries you may open this issue in [tf discussion forum](https://discuss.tensorflow.org/) as there is a larger community there.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53371, "title": "[MHLO] eliminate redundant transpose", "body": "Simplify Transpose(Tsanspose(X)) to Transpose(X)", "comments": []}, {"number": 53370, "title": "Quantized Convolution Layers Operation in TF-lite", "body": "Hello to everyone, for academic and research purposes I am trying to understand the operation behind a quantized convolution layer in Tensorflow Lite. For this purpose, I chose EffiecientNet-lite0 model. So I downloaded pretrained EfficientNet-lite0 float32 and int8 tflite files from the official [repository](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite) and run inference of these models using a sample .jpg image. Firstly, I checked model's architecture and some details using Netron tool and decided to pick first Conv2D layer as my case study.\r\n\r\n![netron](https://user-images.githubusercontent.com/57605047/145392585-5a4393f8-5ab5-4e36-864a-9927c9782a63.png)\r\n\r\nI started with fp32 model inference as I thought it will be simplier and used the code below for preprocess the image and for the inference of the model.\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport PIL.Image as Image\r\n\r\n\r\nMEAN_RGB = 127.0\r\nSTDDEV_RGB = 128.0\r\nCROP_PADDING = 32\r\nIMAGE_SIZE = 224\r\n\r\ndef _decode_and_center_crop(image, image_size, resize_method=Image.BICUBIC):\r\n    \"\"\"Crops to center of image with padding then scales image_size.\"\"\"\r\n    image_width, image_height = image.size\r\n    padded_center_crop_size = int((image_size / (image_size + CROP_PADDING)) * min(image_height, image_width))\r\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\r\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\r\n    crop_window = [offset_width, offset_height, \r\n                   offset_width + padded_center_crop_size, \r\n                   offset_height + padded_center_crop_size]\r\n    resized_image = image.crop(crop_window)\r\n    resized_image = resized_image.resize((image_size, image_size), resize_method)\r\n    return resized_image\r\n\r\nwith open('image_net_classes.txt') as f:\r\n    lines = f.readlines()\r\n\r\nimage = Image.open('beagle.jpg')\r\nresized_image = _decode_and_center_crop(image, IMAGE_SIZE)\r\nresized_image = np.array(resized_image).astype(np.float32)\r\nresized_image -= MEAN_RGB\r\nresized_image /= STDDEV_RGB\r\nresized_image = np.expand_dims(resized_image, axis=0)\r\n\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"efficientnet-lite0-fp32.tflite\", experimental_preserve_all_tensors=True)\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninput_shape = input_details[0]['shape']\r\ninterpreter.set_tensor(input_details[0]['index'], resized_image)\r\n\r\ninterpreter.invoke()\r\n\r\nfor t in interpreter.get_tensor_details():\r\n  if t['index'] == 102:\r\n    test = interpreter.get_tensor(t['index'])\r\nprint(test.shape)\r\n\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nstring = str(np.argmax(output_data))\r\n\r\nfor line in lines:\r\n    if string in line:\r\n        print('The image is a', line)\r\n        break\r\n```\r\nSo after I downloaded Conv2D layer's parameters(kernel) I implented fused Relu6 Conv2D layer using simple Python and later came back to compare results and everything was working pretty good.\r\n\r\nSo the next step was to implement quantized Conv2D layer of EfficientNet-lite0-int8 and used the code below.\r\n\r\n```\r\nMEAN_RGB = 127.0\r\nSTDDEV_RGB = 128.0\r\nIMAGE_SIZE = 224\r\nscale = 0.012566016986966133\r\nzero_point = 131\r\n\r\nimage = Image.open('beagle.jpg')\r\nresized_image = _decode_and_center_crop(image, IMAGE_SIZE)\r\nresized_image = np.array(resized_image).astype(np.float32)\r\nresized_image -= MEAN_RGB\r\nresized_image /= STDDEV_RGB\r\nresized_image = np.expand_dims(resized_image, axis=0)\r\nresized_image = resized_image / scale + zero_point\r\nresized_image = np.array(resized_image).astype(np.uint8)\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=\"efficientnet-lite0-int8.tflite\", experimental_preserve_all_tensors=True)\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninput_shape = input_details[0]['shape']\r\ninterpreter.set_tensor(input_details[0]['index'], resized_image)\r\n\r\n\r\ninterpreter.invoke()\r\nfor t in interpreter.get_tensor_details():\r\n  if t['index'] == 102:\r\n    test = interpreter.get_tensor(t['index'])\r\nprint(test.shape)\r\n\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n\r\nfor line in lines:\r\n    if string in line:\r\n        print('The image is a', line)\r\n        break\r\n```\r\nI also studied this [paper](https://arxiv.org/pdf/1712.05877.pdf) that provided this Conv layer's implemetation [here](https://github.com/tensorflow/tensorflow/blob/4952f981be07b8bf508f8226f83c10cdafa3f0c4/tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h#L248-L314). So in my understanding, the quantized Convolution Operation is the same as the full precision one but you have also to take into account offsets and scales. \r\n\r\nI was able to extract input,output,kernel and biases scales and offsets and managed to transform double multipliers to quantized multipliers and right shift using fuctions defined [here](https://github.com/tensorflow/tensorflow/blob/4952f981be07b8bf508f8226f83c10cdafa3f0c4/tensorflow/contrib/lite/kernels/internal/quantization_util.cc) to be able to do the needed operation below.\r\n```\r\nacc = MultiplyByQuantizedMultiplierSmallerThanOne(\r\n              acc, output_multiplier, output_shift);\r\n```\r\n\r\nThat I assume is a per-axis operation. **So my question here, is that for this operation the multiplier we use is a quantized multiplier that equals Mo = (Sinput * Skernel) / Soutput or it's something else?**\r\n\r\nSo after I wrote a Python implementation of the above quantized Con2D layer and checked the results using\r\n```\r\nfor t in interpreter.get_tensor_details():\r\n  if t['index'] == 102:\r\n    test = interpreter.get_tensor(t['index'])\r\n```\r\nthere was a big deviation. Firstly, I thought that the above layer was a fused Relu6 Conv2D layer so I was expecting output tensor's values to be between 0 and 6 but that was not the case.\r\n\r\n**Could you please provide me a more detailed description of a quantized fused Relu6 Conv2D layer's operation?**\r\n\r\n### Parameters defined\r\n\r\nThe sample image I used for inference.\r\n![beagle](https://user-images.githubusercontent.com/57605047/145397592-f2d5b484-d91e-4dbd-be53-93f1135a7844.jpg)\r\n\r\nI am using Google Collaboratory to run the above code snippets.\r\n\r\n**Tensorflow Version:** 2.7.0\r\n**Python version:**  3.7.12\r\n**Numpy version:** 1.19.5", "comments": ["@ThanasisGiak ,\r\nOn running the given code snippet, I am facing an different error. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/7dcc05e1d0a6b2307583254e8d2813ad/untitled138.ipynb).Also please, fill issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) and the tensorflow version you are using.\r\n", "Hello @tilakrayal , thanks a lot for your immediate response. I forgot to attach [image_net_classes.txt](https://github.com/tensorflow/tensorflow/files/7691512/image_net_classes.txt) file. As you can see I updated my post with the information asked\r\n\r\n", "After, studying carefully  Tensorflow's github repository I found kernel_util.cc file and [CalculateActivationRangeUint8](https://github.com/tensorflow/tensorflow/blob/4952f981be07b8bf508f8226f83c10cdafa3f0c4/tensorflow/contrib/lite/kernels/kernel_util.cc#L41) function. So using this function, I managed to understand why quantized fused ReLu6 Conv2D layer's output tensor is not clipped between [0, 6] but between [-128, 127] values. For the record, I managed to implement a Conv2D layer's operation in Python with some simple steps. \r\n\r\n- Firstly, you got to take layer's parameters(kernel, bias, scales, offsets) using _interpreter.get_tensor_details()_  command and calculate _output_multiplier_  using [GetQuantizedConvolutionMultipler](https://github.com/tensorflow/tensorflow/blob/4952f981be07b8bf508f8226f83c10cdafa3f0c4/tensorflow/contrib/lite/kernels/kernel_util.cc#L22) and [QuantizeMultiplierSmallerThanOne](https://github.com/tensorflow/tensorflow/blob/4952f981be07b8bf508f8226f83c10cdafa3f0c4/tensorflow/contrib/lite/kernels/internal/quantization_util.cc#L25) functions. \r\n- After that, subtract input offset from the input layer before padding it and implement a simple convolution. \r\n- Later, you need to use [MultiplyByQuantizedMultiplierSmallerThanOne](https://github.com/tensorflow/tensorflow/blob/4952f981be07b8bf508f8226f83c10cdafa3f0c4/tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h#L36) function that uses [SaturatingRoundingDoublingHighMul](https://github.com/google/gemmlowp/blob/master/fixedpoint/fixedpoint.h#L340) and [RoundingDivideByPOT](https://github.com/google/gemmlowp/blob/master/fixedpoint/fixedpoint.h#L368) from _gemmlowp/fixedpoint.h_ library. \r\n- Finally, add _output_offset_ to the result and clip it using the values taken from _CalculateActivationRangeUint8_ function.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53370\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53370\">No</a>\n", "Hello @ThanasisGiak,\r\nI'm also trying to learn to implement quantized convolution. I have been trying to find a resource (a blog, a book, or anything) on this matter and stumbled on this issue. Can you please share the resources that you have found helpful? Any guidance on this is deeply appreciated. \r\nThank you.", "Hello @mrtpk123 ,\r\nFirstly, a good paper to understand the basic operations behind quantized Tensorflow layer's operations is [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](https://arxiv.org/pdf/1712.05877.pdf).\r\nAfter that for the implementation, you have to study in detail Tensorflow's [repo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/kernels) and follow the steps I mentioned above. Be careful, because for the third step some layer's use [Single-rounding MultiplyByQuantizedMultiplier function](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/common.h#L144) and other layers use [Double-rounding MultiplyByQuantizedMultiplier function](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/common.h#L236).", "Thank you @ThanasisGiak "]}, {"number": 53369, "title": "Update Compute Library to 21.11 release", "body": "Updates the version of Compute Library for the Arm architecture to 21.11\r\nThis release adds support for bf16 fast-maths-mode to additional\r\noneDNN primitives - inner product and matmul.\r\n\r\noneDNN 2.4 build is patched to expose this functionality to TensorFlow\r\n\r\n`TF_ENABLE_ONEDNN_OPTS` flag (runtime selection of oneDNN backend) is\r\nenabled on AArch64 for builds with `--config=mkl_aarch64` set.", "comments": ["Hi @penpornk,\r\n\r\nThis PR updates the Compute Library build and enables the `TF_ENABLE_ONEDNN_OPTS` flag (Note: this doesn't impact the 'stock' builds on AArch64, but will allow runtime selection of the oneDNN+ACL backend for `mkl_aarch64` build.\r\n\r\nNow that oneDNN 2.5 is out, we will probably need another patch to move the oneDNN version for the mkl_aarch64 build from 2.4 to 2.5, assuming that's the intent for the x86 build.\r\n\r\nI'm hoping to be able to get this all done for the 2.8 release"]}, {"number": 53368, "title": "[Grappler] Add enforced_layout parameter for LayoutOptimizer", "body": "This PR adds an option to enforce a specific layout in the grappler `LayoutOptimizer`.\r\n\r\nThe `enforce_layout` can be set to either `NHCW` or `NCHW`.  When set, the layout optimizer will attempt to convert all the layout sensitive operations to the specified layout (thus, not relying on `NumConvOnDeviceWithDataTypeOverThreshold` anymore).", "comments": ["@Kh4L Can you please check @bixia1's comments and keep us posted ? Thanks!"]}, {"number": 53367, "title": "Fix wrong output of tf.stack with 0-dimension tensor", "body": "This PR tries to address the issue raised in #53300 where\r\ntf.stack will silently output wrong result with 0-dimension tensor.\r\nThe issue was that the shape check was skipped when num of output elements\r\nwas zero. This PR fixed the issue.\r\n\r\nThis PR fixes #53300.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["This has landed but GitHub fails to notice that.", "> This has landed but GitHub fails to notice that.\r\n\r\nIf it fails the detection do we need to manually close the linked issue https://github.com/tensorflow/tensorflow/issues/53300 ?", "@bhack Let's do that. I'll close it. Thank you!"]}, {"number": 53366, "title": "tensorflow/c/c_api_experimental.cc:698:41: error: 'class tensorflow::EagerContext' has no member named 'StoreCollectiveOpsServer'", "body": "\u7f16\u8bd1  tensorflow 1.14\u7248\u672c\uff0c\u63d0\u793a\r\ntensorflow/c/c_api_experimental.cc:698:41: error: 'class tensorflow::EagerContext' has no member named 'StoreCollectiveOpsServer'", "comments": ["Hi @luojuan2020 ! \r\nCould you please update the template too as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53366\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53366\">No</a>\n"]}, {"number": 53365, "title": "Interpreter->invoke() calls Segmentation Fault", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 with USB Coral. \r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): v2.5.0, v2.6.0\r\n\r\n**Description**\r\nWhen i calls `initTfLiteInterpreter()`, tfLite work is correct, output has all information (boxes, labels and classes), time of invoke is good (15 ms). But when i calls `processingFrame(cv::Mat)` in other classes with equalent code (i get Seg fault on `Interpreter->invoke()`):\r\n```\r\n        // From other class \r\nfor(int i = 0; i< 10; i++)\r\n    {\r\n        cv::Mat testImage = cv::imread(TestClass->EXAMPLE_FRAME);\r\n        TestClass->processingFrame(testImage);\r\n    }\r\n```\r\nI get this error with TF 2.5.0, 2.6.0. \r\n\r\n**Source of my programm**\r\n**TestClass.h:**\r\n```\r\n/// Build EDGE Interpreter for Coral\r\n    void BuildEdgeTpuInterpreter(const tflite::FlatBufferModel &model,\r\n                                edgetpu::EdgeTpuContext *edgetpu_context);\r\n\r\n    // Load graph to coral\r\n    void initTfLiteInterpreter();\r\n\r\n    // Processing the received frame\r\n    void processingFrame(cv::Mat& frame);\r\n\r\n    int num_threads = 1;\r\n    std::unique_ptr<tflite::Interpreter> interpreter;\r\n    std::shared_ptr<edgetpu::EdgeTpuContext> tpu_context;\r\n\r\n    TfLiteTensor* input_tensor;\r\n    TfLiteTensor* output_locations;\r\n    TfLiteTensor* output_classes;\r\n    TfLiteTensor* output_scores;\r\n    TfLiteTensor* num_detections_;\r\n\r\n    int height;\r\n    int width;\r\n    int channels;\r\n    int row_elems;\r\n```\r\n**TestClass.cxx:**\r\n```\r\nvoid TestClass::BuildEdgeTpuInterpreter(const tflite::FlatBufferModel &model,\r\n                                                               edgetpu::EdgeTpuContext *edgetpu_context)\r\n{\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    resolver.AddCustom(edgetpu::kCustomOp, edgetpu::RegisterCustomOp());\r\n    if (tflite::InterpreterBuilder(model, resolver)(&interpreter) != kTfLiteOk) {\r\n        std::cerr << \"Failed to build interpreter.\" << std::endl;\r\n        return;\r\n    }\r\n    // Allocate tensor buffers.\r\n    // Bind given context with interpreter.\r\n    interpreter->SetExternalContext(kTfLiteEdgeTpuContext, edgetpu_context);\r\n    interpreter->SetNumThreads(1);\r\n    if (interpreter->AllocateTensors() != kTfLiteOk)\r\n    {\r\n      std::cerr << \"Failed to allocate tensors.\" << std::endl;\r\n    }\r\n}\r\n\r\nvoid TestClass::initTfLiteInterpreter(void)\r\n{\r\n    auto model = tflite::FlatBufferModel::BuildFromFile(GRAPH.c_str());\r\n\r\n    tpu_context = edgetpu::EdgeTpuManager::GetSingleton()->OpenDevice();\r\n    std::cout << \"Checking readiness of Coral device\" << std::endl;\r\n    if(!tpu_context->IsReady())\r\n    {\r\n        std::cout << \"Coral device is not ready\" << std::endl;\r\n        throw -1;\r\n    }\r\n    std::cout << \"EDGE TPU path: \" << tpu_context->GetDeviceEnumRecord().path << std::endl;\r\n    BuildEdgeTpuInterpreter(*model, tpu_context.get());\r\n\r\n    input_tensor = interpreter->tensor(interpreter->inputs()[0]);\r\n    output_locations = interpreter->tensor(interpreter->outputs()[0]);\r\n    output_classes = interpreter->tensor(interpreter->outputs()[1]);\r\n    output_scores = interpreter->tensor(interpreter->outputs()[2]);\r\n    num_detections_ = interpreter->tensor(interpreter->outputs()[3]);\r\n\r\n    height = input_tensor->dims->data[1];\r\n    width = input_tensor->dims->data[2];\r\n    channels = input_tensor->dims->data[3];\r\n    row_elems = width * channels;\r\n\r\n    for(int i = 0; i< 10; i++)\r\n    {\r\n        cv::Mat testImage = cv::imread(EXAMPLE_FRAME);\r\n        processingFrame(testImage);\r\n    }\r\n    \r\n    Utils::dual_write(\"CNN is ready, example frame was processed\");\r\n    m_readyFlag.store(true);\r\n}\r\n\r\nvoid TestClass::processingFrame(cv::Mat& frame)\r\n{\r\n    Q_ASSERT(q_ptr);\r\n    const clock_t begin_time = clock();\r\n    QMutexLocker locker(&m_mutex);\r\n    qDebug() << \"cv mat size: \" << width << height;\r\n    cvtColor(frame, frame, cv::COLOR_BGR2RGB);\r\n    // Resize for model input\r\n    cv::resize(frame, frame, cv::Size(width, height));\r\n\r\n    if (input_tensor->type != kTfLiteUInt8 ||           //\r\n        input_tensor->dims->data[0] != 1 ||             //\r\n        input_tensor->dims->data[1] != height ||  //\r\n        input_tensor->dims->data[2] != width ||   //\r\n        input_tensor->dims->data[3] != channels) {\r\n    std::cerr << \"Input tensor shape does not match input image\" << std::endl;\r\n    return;\r\n    }\r\n\r\n    uint8_t* dst = input_tensor->data.uint8;\r\n    for (int row = 0; row < height; row++) {\r\n        memcpy(dst, frame.ptr(row), row_elems);\r\n        dst += row_elems;\r\n    }\r\n\r\n    if(interpreter->Invoke() != kTfLiteOk)\r\n        qDebug() << \"Invoke is broken\";\r\n    qDebug() << \"Invoke is done!\";\r\n    const float* detection_locations = output_locations->data.f;\r\n    const float* detection_classes = output_classes->data.f;\r\n    const float* detection_scores = output_scores->data.f;\r\n    const int num_detections = *(num_detections_->data.f);\r\n    for (int i = 0; i < num_detections; i++) {\r\n        const float score = detection_scores[i];\r\n        const std::string label = std::to_string(uint8_t(detection_classes[i]));\r\n        const float yMin = detection_locations[4 * i + 0];\r\n        const float xMin = detection_locations[4 * i + 1];\r\n        const float yMax = detection_locations[4 * i + 2];\r\n        const float xMax = detection_locations[4 * i + 3];\r\n        if (score > thresholdScore) {\r\n            std::cout << label << \" score:\" << score << std::endl;\r\n            emit q_ptr->returnBoundingBoxes(frame, yMin, xMin, yMax, xMax, score, label, true);\r\n        }\r\n    }\r\n    std::cout << \"time: \" << float( clock () - begin_time ) /  CLOCKS_PER_SEC << std::endl;\r\n    emit q_ptr->finishedCNNProcessing(frame);\r\n}\r\n```\r\n**LOGS**\r\n```\r\nChecking readiness of Coral device\r\nEDGE TPU path: /sys/bus/usb/devices/2-1\r\ncv mat size:  640 480\r\nInvoke is done!\r\n1 score:0.902344\r\ntime: 0.022215\r\ncv mat size:  640 480\r\nInvoke is done!\r\n1 score:0.902344\r\ntime: 0.012465\r\ncv mat size:  640 480\r\nInvoke is done!\r\n1 score:0.902344\r\ntime: 0.011841\r\ncv mat size:  640 480\r\nInvoke is done!\r\n1 score:0.902344\r\ntime: 0.011659\r\ncv mat size:  640 480\r\nInvoke is done!\r\n1 score:0.902344\r\ntime: 0.014413\r\ncv mat size:  640 480\r\nInvoke is done!\r\n1 score:0.902344\r\ntime: 0.011502\r\ncv mat size:  640 480\r\nInvoke is done!\r\n1 score:0.902344\r\ntime: 0.012496\r\ncv mat size:  640 480\r\nInvoke is done!\r\n1 score:0.902344\r\ntime: 0.012136\r\ncv mat size:  640 480\r\nInvoke is done!\r\n1 score:0.902344\r\ntime: 0.012898\r\ncv mat size:  640 480\r\nInvoke is done!\r\n1 score:0.902344\r\ntime: 0.012129\r\nThu Dec  9 11:52:59 2021:  CNN is ready, example frame was processed\r\ncv mat size:  640 480\r\nSegmentation fault (core dumped)\r\n```\r\n**GDB out**\r\n```\r\n0x000000000067d47c in tflite::ops::custom::detection_postprocess::DecodeCenterSizeBoxes(TfLiteContext*, TfLiteNode*, tflite::ops::custom::detection_postprocess::OpData*)\r\n```\r\n", "comments": ["I find problem, it is uncorrect model.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53365\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53365\">No</a>\n"]}, {"number": 53363, "title": "build: bump bazel to 4.2.2", "body": null, "comments": ["We'll do this change after the branch cut for the next release given that the last Bazel bump caused breakages to MacOS and we still haven't fixed that build. Apologies for the delay."]}, {"number": 53362, "title": "Could not load dynamic library 'libcudart.so.11.0' cannot open shared object file: No such file or directory", "body": "- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.7.0\r\n- Python version: 3.9.7\r\n- Installed using virtualenv? pip? conda?: pip3\r\n- CUDA/cuDNN version: 11.5\r\n- GPU model and memory: Gtx 1060 \r\ni have tensorflow and tensorflow-gpu installed\r\n\r\ncode source : \r\n\r\n```\r\nimport numpy as np\r\nimport keras\r\nfrom keras.preprocessing.image import load_img\r\nfrom keras.preprocessing.image import img_to_array\r\nfrom keras.applications.vgg16 import preprocess_input\r\nfrom keras.applications.vgg16 import decode_predictions\r\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\r\nfrom keras.applications.vgg16 import VGG16\r\nimport tensorflow as tf\r\n\r\n# def predict(img_path):\r\ndef getPrediction(filename):\r\n     model = tf.keras.models.load_model(\"/classrepo/HomeWork_out/Project3_ManuelaClone/UCF-PROJECT-03/final_model_weights.hdf5\")\r\n     img = load_img('/classrepo/HomeWork_out/Project3_ManuelaClone/UCF-PROJECT-03/static/'+filename, target_size=(180, 180))\r\n     img = img_to_array(img)\r\n     img = img / 255\r\n     img = np.expand_dims(img,axis=0)\r\n     category = model.predict_classes(img)\r\n     answer = category[0]\r\n     probability = model.predict(img)\r\n     probability_results = 0\r\n\r\n     if answer == 1:\r\n          answer = \"Recycle\"\r\n          probability_results = probability[0][1]\r\n     else:\r\n          answer = \"Organic\"\r\n          probability_results = probability[0][0]\r\n\r\n     answer = str(answer)\r\n     probability_results=str(probability_results)\r\n\r\n     values = [answer, probability_results, filename]\r\n     return values[0], values[1], values[2]\r\n```\r\n\r\n\r\n\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 495.44       Driver Version: 495.44       CUDA Version: 11.5     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   33C    P8     2W /  N/A |      6MiB /  6078MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A       833      G   /usr/lib/Xorg                       4MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n**i'm using manjaro and i get this error when i want to run my code by python3 main.py :**\r\n\r\n```\r\n2021-12-09 00:35:36.425837: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-12-09 00:35:36.425863: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n```\r\n\r\nany help please ??\r\n", "comments": ["Hi @3kba! \r\nCould you please update the template too as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].     \r\n  \r\nTested Cuda/CudNN config for each version can be found [here.](https://www.tensorflow.org/install/source_windows#gpu) Attaching relevant [thread](https://github.com/tensorflow/tensorflow/issues/45930#issuecomment-770342299) for reference. Thanks!", "> Hi @3kba! Could you please update the template too as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].\r\n> \r\n> Tested Cuda/CudNN config for each version can be found [here.](https://www.tensorflow.org/install/source_windows#gpu) Attaching relevant [thread](https://github.com/tensorflow/tensorflow/issues/45930#issuecomment-770342299) for reference. Thanks!\r\n\r\nupdated you can see and tell me if u want me to add another information it can help thanks.\r\ndidn't get any results with `sudo find / -name 'libcudart.so.11.0'`\r\n```\r\nsudo find / -name 'libcudart.so.11.0'\r\n\r\n[sudo] password for devokba: \r\nfind: \u2018/run/user/1000/doc\u2019: Permission denied\r\nfind: \u2018/run/user/1000/gvfs\u2019: Permission denied\r\n```\r\nwhere the problem should be ?", "Ok @3kba! You can install Cuda files either through [conda](https://anaconda.org/anaconda/cudatoolkit) command or follow instructions for ubuntu/others[ here](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#ubuntu-installation). After that You can point your bin file from [Cuda installation](url) in your environment path as above [thread ](https://github.com/tensorflow/tensorflow/issues/45930#issuecomment-770342299)suggests . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53362\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53362\">No</a>\n"]}, {"number": 53360, "title": "Autoformat entire codebase?", "body": "There are two popular ways of applying and enforcing automatic source code formatting a codebase like yours, these are:\r\n- [clang-format](https://clang.llvm.org/docs/ClangFormat.html) (e.g., for your C++ and protobuf files)\r\n- [black](https://github.com/psf/black) (for your Python and Jupyter Notebook files)\r\n\r\nWhich is as easy as:\r\n```sh\r\n$ python -m black .\r\n$ fd -eh -ec -ehpp -ecpp -eproto -x clang-format -i --style=Google {} \\;\r\n```\r\n\r\nReview changes:\r\n\r\n  - https://github.com/offscale/tensorflow/tree/clang-format-apply from `clang-format` 13.0.0 at 9ebfeab\r\n  - https://github.com/offscale/tensorflow/tree/black from `python -m black` 21.12b0 also at 9ebfeab\r\n\r\n---\r\n\r\n**Related**:\r\n\r\n   - https://github.com/tensorflow/addons does a bunch of auto format and auto linting\r\n   - My recently merged PR: #53329\r\n\r\n**Disadvantages**:\r\n\r\n  - Impact just about every file in the codebase\r\n  - Require open PRs to be modified and forks to be updated\r\n\r\n**Mitigation of disadvantages**:\r\n\r\n  - To avoid stepping on toes, one can write a pull request scanner to avoid touching any file that has a current pull request lodged against it. I'm happy to write this script, but will also need someone to run this internally so it knows what files are being affected by non public PRs\r\n\r\n**Advantages**:\r\n\r\n  - Easier for automated tooling to make automated fixes (whitespace / prettyprinting issues don't get in the way)\r\n  - More consistent codebase (which is somewhat of a quality metric)", "comments": ["@SamuelMarks ,\r\nPlease feel free to submit a PR for the requested change or share the link where requested change is to be made in respective repo.Thanks!", "See my comment on the PR. \r\n\r\nWe have internal systems for this, and generally we only apply autoformatting to code when we're updating it for other reasons. \r\n\r\nFormatting just to format makes code history harder to follow. ", "@MarkDaoust Yeah I agree formatting makes code history harder to follow. Which is why generally all these niceties (autolinters, autoformatters, &etc.) are done at the beginning of a project (and not 6 years later).\r\n\r\nStill though, I wanted to start the discussion and see if there was any appetite for such.\r\n\r\nNaturally autoformatters greatly simplify automated code modification tooling, and the consistency aids in readability/grokking.", "Yeah, we have them. Some as presubmit checks in github, some as internal presubmits. But typically we don't apply them to existing code unless it's something we're touching for other reasons."]}, {"number": 53359, "title": "Update go documenting comment", "body": "Working with tensorflow version 2.6.0 it seems that the toy python program mentioned in the comment no longer works.\r\n`tensorflow.ConfigProto()` seems to have been moved to `tensorflow.compat.v1.ConfigProto()`\r\nAnd the byteStream shown bellow `(\\x01\"` seems to correspond to setting the `inter_op` parameter not the `intra_op`", "comments": ["I have signed the CLA, could we rescan this?"]}, {"number": 53357, "title": "Tutorial for autoencoder might be applying denoising to the wrong input data", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/generative/autoencoder#second_example_image_denoising\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe model was trained to remove noise using the `x_train_noisy` as input and `x_train` as target output.\r\nLater when demonstrating the denoising capabilities `x_test` is used as input, which is the original (**not noisy**) test data. In the plot it is also presented as `original + noise` (noisy images) vs the reconstruction [*from the noisy images*]. \r\n\r\nIf I am not missing something, right now the autoencoder is used to denoise the already **not noisy** images instead of denoising the **noisy** images which are also shown as a comparison besides the reconstructed images.\r\n\r\n### Clear description\r\n\r\nUse the autoencoder to denoise the actually noisy images, in the same way as it is presented below and the same way as the model is trained and validated before.\r\n\r\nChange this\r\n```python\r\nencoded_imgs = autoencoder.encoder(x_test).numpy()\r\ndecoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\r\n```\r\nto this\r\n```python\r\nencoded_imgs = autoencoder.encoder(x_test_noisy).numpy()\r\ndecoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\r\n```", "comments": ["Yes, you are correct.\r\nThe input to the encoder should be x_test_noisy instead of x_test.\r\n", "@dnlmlr Sorry for the late response!\r\nThe description in the tutorial is correct since it is unsupervised learning so we require only input to learn patterns from it.Here we define two things one is **Encoder** which compresses the image that means conversion to a smaller dimension and second one is **Decoder** which takes the small image and reconstruct it back. In this case we call the  fit function to train the model and we use arguments (x_train as input and x_train as output ).The input and output will be the same as the entire role of the model is that it has to construct the same image as input . \r\n\r\nIn the below code we are passing some test images(x_test) to see how it works.We are passing it to the encoder and saving it as encoded images (encoded_imgs) then we take these encoded images and pass it to the decoder (decoded_imgs) again.\r\n`encoded_imgs = autoencoder.encoder(x_test).numpy()  \r\n`\r\n`decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()`\r\n\r\nI am able to run one example of autoencoder successfully  using TF v2.7.0 on colab using another data set,Please refer to this [gist](https://colab.research.google.com/gist/sushreebarsa/c99d303e689e72e8f00113776e668b95/autoencoder.ipynb) .\r\nThank you!", "I think there is a misunderstanding here. I was talking specifically about the **denoising** (second) example as you can see in the title and also the issue text. \r\nYou seem to be talking about the first example instead, which just tries to reconstruct the input image.\r\n\r\nAlso I **never said** that any of the examples **didn't work**, I just said that I think the wrong input images were used in the demonstration. \r\nThe second example was trained to remove noise from the given images, but the demonstration used the **not** noisy images as input. So the output of course seems to be correct if you run it, because it is just denoising images that are not noisy to begin with. \r\n\r\nBut I already wrote all that in the initial issue, so I would ask you to please read it again, especially the **Clear description** part where you can see the current code and what I would assume to be the correct demonstration code. You can see that I replaced the `x_test` with `x_test_noisy` which are the noisy images.", "@dnlmlr Thank you for the quick response!\r\nWould you like to submit a PR to fix this issue else I can submit a PR on your behalf ? ", "Feel free to submit the PR on my behalf. The change should literally be only 6 characters", "@dnlmlr This issue will be closed once the [PR](https://github.com/tensorflow/docs/pull/1986) is merged.Thanks!", "PR merged, thank you @dnlmlr @sushreebarsa !"]}, {"number": 53356, "title": "Passing empty tensors to TFLite converted signatures fails with an exception", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): tf-nightly 2.8.0-dev20211203 and 2.7 are both affected\r\n\r\n### 2. Code\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass TestModel(tf.keras.models.Model):\r\n  @tf.function\r\n  def test(self, x):\r\n    return x\r\n\r\ntest_model = TestModel()\r\nsignatures = [test_model.test.get_concrete_function(tf.TensorSpec([None], tf.float32))]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions(signatures, test_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\ntflite_model = converter.convert()\r\n\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\n\r\n# This raises \"ValueError: Cannot set tensor: Tensor is unallocated. Try calling allocate_tensors() first\"\r\nresult = interpreter.get_signature_runner()(x=tf.zeros([0], tf.float32))\r\n```\r\n\r\n### 3. Failure after conversion\r\nConversion seems to work fine, but trying to run the signature fails with the following exception if an empty tensor is passed.\r\n\r\n`ValueError: Cannot set tensor: Tensor is unallocated. Try calling allocate_tensors() first`\r\n\r\nDespite the message, calling `allocate_tensors()` before has no effect. Passing a tensor that is not empty does not cause the failure.\r\n\r\nWhile passing an empty tensor might seem absurd at first, I'm hitting this issue in much more subtle scenarios like passing shapes of tensors (it fails if the shape is for a scalar because it's empty), or when using ragged tensors where one of its internal row splits happens to be empty.\r\n\r\nNote that returning empty tensors from within a TFLite converted function also fails. In that case you get this other exception:\r\n\r\n`ValueError: Invalid tensor size.`\r\n\r\nA workaround would be much appreciated if possible, though it seems unlikely without a proper fix.", "comments": ["@sachinprasadhs ,\r\nI was able to reproduce the issue in tf v2.5 , v2.7 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/b097f87fda554821341bb11b6c278d56/53356.ipynb).", "@leandro-gracia-gil Can you please retry from master branch ?", "If easier wait for a new nightly and then retry.\r\n\r\nThanks", "> If easier wait for a new nightly and then retry.\r\n\r\nI'll do that, thanks!", "Seems to work fine now. Thanks for the fix! I'll be closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53356\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53356\">No</a>\n"]}, {"number": 53355, "title": "tf.random.uniform can't generate random tensor of type tf.int32", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nv2.7.0-rc1-69-gc256c071bb2 2.7.0\r\n\r\n**Describe the current behavior**\r\n\r\ntf.random.uniform can generate random tensor of type tf.float32 with distinct minvals and maxvals, but can't do it for tf.int32.\r\n\r\n**Describe the expected behavior**\r\n\r\ntf.random.uniform should generate random tensor for both tf.float32 and tf.int32 with distinct minvals and maxvals.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\n#!/usr/bin/python3\r\nimport tensorflow as tf;\r\n# generate random tensor of tf.float32 is OK\r\na = tf.random.uniform(minval = (1,2,3,4), maxval = (3,4,5,6), shape = (4,), dtype = tf.float32);\r\n# generate random tensor of tf.int32 is not OK\r\na = tf.random.uniform(minval = (1,2,3,4), maxval = (3,4,5,6), shape = (4,), dtype = tf.int32);\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\", line 7107, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: minval must be 0-D, got shape [4] [Op:RandomUniformInt]\r\n```", "comments": ["Hi @breadbread1984 ! As this [document](https://www.tensorflow.org/api_docs/python/tf/random/uniform#args) suggests ,You need to provide a scalar value for minval and maxval  in case of using dtype int32. Attaching [Gist](https://colab.research.google.com/gist/mohantym/7bfe663a5806b4ed119d9737671771f4/github_53355.ipynb) for reference. Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 53354, "title": "[Help] how to fix i tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] none of the mlir optimization passes are enabled (registered 2)", "body": "i used tf-cpu 2.6.0, python 3.9 run program in vscode env for conda find fault \r\n**i tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] none of the mlir optimization passes are enabled (registered 2)**\r\n\r\nhow this fix ??\r\nchatbot with LSTM\r\nthanks", "comments": ["@dekkuz \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose), and please refer this [thread](https://discuss.tensorflow.org/t/none-of-the-mlir-optimization-passes-are-enabled/2247/14) ?Let us know if it helps?\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53354\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53354\">No</a>\n"]}, {"number": 53353, "title": "How to build static library libtensorflowlite.a for arm?", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are\r\nnot verified bugs in TensorFlow, please go to\r\n[Discourse](https://discuss.tensorflow.org/).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@Eddy-Wu ,\r\nCan you please refer this [link](https://www.tensorflow.org/lite/guide/build_arm) for the installation of TensorFlow Lite for ARM boards.It helps.Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53353\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53353\">No</a>\n"]}, {"number": 53352, "title": "Abnormally long loading/calling time for Tensorflow", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.3 LTS\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version:CUDA: 11.1 / 8.1.4\r\n- GPU model and memory: RTX 3080, 10GB x 2\r\n\r\n**Describe the current behavior**\r\nWe've found that it takes an unexpectedly long loading time when calling Tensorflow or machine learning models. Once the library/model is loaded, it takes significantly less time afterwards. However, whenever we are restarting the kernel we run into the exact same problem. \r\n\r\n**Standalone code to reproduce the issue**\r\nThe reproducible code is provided below:\r\n```\r\nimport tensorflow as tf\r\nimport time\r\n\r\nprint(f\"TensorFlow version: {tf.__version__}\")\r\n\r\nstart = time.time()\r\nprint(tf.reduce_sum(tf.random.normal([1000, 1000])))\r\nend = time.time()\r\n\r\nprint(f\"it took = {end - start} seconds\")\r\n```\r\nBelow is the result of the above code block:\r\n```\r\nTensorFlow version: 2.4.1\r\ntf.Tensor(973.3261, shape=(), dtype=float32)\r\nit took = 273.7537660598755 seconds\r\n```\r\nAs you can see it takes way too much time for such a simple task. This issue applies the same when we are trying to call functions such as `model.fit` or `model.summary()`. The reproducible code below is another example of the problem. We used a simple CNN model with the cifar10 dataset. It took extensive time to just load the model for some reason. Additionally, when we are calling the `fit` function it takes forever time to actually run Epochs.\r\n```\r\nimport random\r\nimport numpy as np\r\n\r\nimport tensorflow as tf \r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.utils import to_categorical\r\nimport time\r\n\r\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\r\ny_train = to_categorical(y_train)\r\ny_test = to_categorical(y_test)\r\n\r\nX_train = X_train.reshape(50000, 32, 32, 3).astype(\"float32\") / 255\r\nX_test = X_test.reshape(10000, 32, 32, 3).astype(\"float32\") / 255\r\n\r\ndef CNN():\r\n    input_layer = keras.Input(shape=(32,32,3))\r\n    conv = keras.layers.Conv2D(32, (3,3), padding='same', activation = 'relu', kernel_initializer = keras.initializers.HeUniform(seed=1))(input_layer)\r\n    max_pool = keras.layers.MaxPooling2D((2, 2))(conv)\r\n    flatten = keras.layers.Flatten()(max_pool)\r\n    dense = keras.layers.Dense(128, activation='relu', kernel_initializer = keras.initializers.HeUniform(seed=1))(flatten)\r\n    output_layer = keras.layers.Dense(10, activation='softmax')(dense)\r\n    model = keras.Model(inputs=input_layer, outputs=output_layer, name = 'CNN')\r\n    model.compile(loss='categorical_crossentropy', optimizer= keras.optimizers.Adam(learning_rate=0.001),  metrics=['acc', 'AUC'])\r\n    return model \r\n\r\nstart = time.time()\r\nmodel = CNN()\r\nend = time.time()\r\nprint(f\"it took = {end - start} seconds\")\r\n>>> it took = 109.86318612098694 seconds\r\n\r\nstart = time.time()\r\nmodel.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\r\nend = time.time()\r\nprint(f\"it took = {end - start} seconds\")\r\n>>> it took = 809.008472442627 seconds\r\n```\r\n\r\nWe would love to figure out what is causing such a problem and a way to fix this. We greatly appreciate your help and support in advance. ", "comments": ["Hi @Irene-kim ! Could you try again in latest version with tested [configurations](https://www.tensorflow.org/install/source)?Could not replicate this issues in Colab  neither in CPU /GPU. Attaching Gist in [2.7_GPU](https://colab.research.google.com/gist/mohantym/5650123ba1bf5809615c763fdfb5d135/github_53352_2-7_gpu.ipynb#scrollTo=VVWfkQUpKmUg) and [2.7_CPU](https://colab.research.google.com/gist/mohantym/5650123ba1bf5809615c763fdfb5d135/github_53352_2-7_gpu.ipynb#scrollTo=cyVNqMY0Kzv5) for reference. Thank you!", "Hi @mohantym ! I'm a coworker with @Irene-kim, I have installed tensorflow-gpu=2.7.0 version. and ran the below commands but now the tensorflow cannot recognize my local gpu machine.\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nprint(tf.config.list_physical_devices())\r\n# 2.7.0\r\n# [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\r\n\r\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n# 0\r\n```", "Ok @jaeyung1001 ! Could you confirm whether you have properly [installed](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) Cuda 11.2 and CudNN 8.1 for TF 2.7 or not?", "hi @mohantym ! I reinstalled my CUDA11.2 & cuDNN 8.1 for TF2.7, but I still encounter same issue about cannot recognize my local gpu machine.\r\n\r\nwhen I ran below command & show result:\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nprint(tf.config.list_physical_devices())\r\n>>>\r\n2.7.0\r\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\r\n2021-12-10 13:28:55.407463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-10 13:28:55.407668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-10 13:28:55.410724: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/local/cuda-11/lib64\r\n2021-12-10 13:28:55.410733: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n```\r\n\r\nand\r\n- the libcudnn.so.8 file exists in /usr/local/cuda-11.2/lib64\r\n- the LD_LIBRARY_PATH(/usr/local/cuda-11.2/lib64) exists in ENV (~/.bashrc)\r\n- I ran the tensorflow2.7 in my virtualenv(conda env)", "Hi @chunduriv! Could you please look at this issue. Its not replicating in Colab environment though. Attaching [Gist](https://colab.sandbox.google.com/gist/mohantym/5650123ba1bf5809615c763fdfb5d135/github_53352_2-7_gpu.ipynb#scrollTo=VVWfkQUpKmUg) in 2.7  for reference. Thanks!", "@Irene-kim, Generally if there was any incompatible between TF, CUDA and cuDNN versions we can observe this behaviour. To get benefit we recommend to follow [tested build configurations](https://www.tensorflow.org/install/source#gpu).\r\n\r\n@jaeyung1001, Can you please let us know steps that you have followed to install Tensorflow?\r\n\r\n", "hi @chunduriv !. I follow the environment that you told me the link.\r\n- tensorflow-gpu 2.6.0, CUDA11.2, cuDNN8.1\r\n![image](https://user-images.githubusercontent.com/11284021/145748111-07c5bc65-cc79-4632-8f6f-268349191800.png)\r\n\r\nfirst I created the virtualenv using conda. and ran the command like below\r\n- conda create -n check python=3.8\r\n- pip install tensorflow-gpu==2.6.0\r\n\r\nbut it's still cannot recognize my local gpu machine. output like below\r\n```\r\n2.6.0\r\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\r\n2021-12-13 12:28:52.446773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-13 12:28:52.446958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-13 12:28:52.450091: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64/\r\n2021-12-13 12:28:52.450100: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n```\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@jaeyung1001, Sorry for late response.\r\n\r\nCan you please confirm that you have followed steps mentioned below or not?\r\n\r\n#Set Up Anaconda Environments\r\n`conda create --name tf_gpu python=3.8\r\n`\r\n#Activate the new Environment\r\n`source activate tf_gpu\r\n`\r\n#Install CUDA and cuDNN\r\n`tf_gpu$conda install -c anaconda cudatoolkit=11.2 cudnn=8.1\r\n`\r\n#Install Tensorflow\r\n`tf_gpu$ pip install tensorflow-gpu==2.6`\r\n\r\nAlso, can you take a look at this [link1](https://github.com/tensorflow/tensorflow/issues/45200), [link2](https://stackoverflow.com/questions/66977227/could-not-load-dynamic-library-libcudnn-so-8-when-running-tensorflow-on-ubun) which discusses about the similar issue and let us know if it helps? Thanks!\r\n\r\n", "hi @chunduriv , there is no same version of cudatoolkit and cudnn\r\n![image](https://user-images.githubusercontent.com/11284021/147191464-f2315c95-f6e6-4c28-8de9-e7f72865922c.png)\r\n![image](https://user-images.githubusercontent.com/11284021/147191489-d6aca22d-59c9-497a-a1cf-19cda899a6b8.png)\r\n-----\r\noh it's exists at conda-forge channel. let me check", "I think something go wrong with my system problem. So I used **docker image file** to resolved it. thank you.\r\n\r\nhttps://hub.docker.com/layers/tensorflow/serving/2.6.0-devel-gpu/images/sha256-f935ceb90139a7b12d61e1a13bc5e1cc0332728f7c223f469cb9b39b7a67b1d2?context=explore", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53352\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53352\">No</a>\n"]}, {"number": 53350, "title": "R2.7 cherry-picks to fix TFLite ARM PIP build", "body": "These cherry-picks are needed to fix TFLite ARM PIP build.", "comments": []}, {"number": 53349, "title": "[lite] Remove explicit #define for  TFLITE_ENABLE_HEXAGON. This shoul\u2026", "body": "\u2026d be enabled by separate build target.\r\n\r\nPiperOrigin-RevId: 414750624\r\nChange-Id: I0650e96f81e840f4f885c7f459a59d75620e3743", "comments": []}, {"number": 53348, "title": "[lite] Add flag TFLITE_ENABLE_HEXAGON and update build rules to only \u2026", "body": "\u2026define it if compiling for arm on non-apple devices.\r\n\r\nPiperOrigin-RevId: 414330792\r\nChange-Id: I88e534e506c03554c2cb6b079ba2bde07f1244e1", "comments": []}, {"number": 53347, "title": "meddling around", "body": null, "comments": ["Please elaborate the change you made and mention a issue if it is related. Also please sign CLA \r\n\r\ncc @mihaimaruseac ", "Spam"]}, {"number": 53346, "title": "Update grouping.py", "body": "Added an example and updated format", "comments": ["please check below error : \r\n\r\n```\r\n`\"tensorflow/python/data/experimental/ops/grouping.py\" | tr \" \" \"\\n\" | grep \".py$\" | xargs pylint --rcfile=tensorflow/tools/ci_build/pylintrc\r\n************* Module grouping\r\ntensorflow/python/data/experimental/ops/grouping.py:394:0: C0301: Line too long (96/80) (line-too-long)\r\n\r\n-----------------------------------\r\nYour code has been rated at 9.88/10`\r\n```"]}]