[{"number": 15618, "title": "Tensorflow does not build in a python3 only environment", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nFedora 27\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.4.1\r\n- **Python version**: \r\n3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n0.8.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\nUsing built-in specs.\r\nCOLLECT_GCC=/usr/bin/gcc\r\nCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/7/lto-wrapper\r\nOFFLOAD_TARGET_NAMES=nvptx-none\r\nOFFLOAD_TARGET_DEFAULT=1\r\nTarget: x86_64-redhat-linux\r\nConfigured with: ../configure --enable-bootstrap --enable-languages=c,c++,objc,obj-c++,fortran,ada,go,lto --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-gcc-major-version-only --with-linker-hash-style=gnu --enable-plugin --enable-initfini-array --with-isl --enable-libmpx --enable-offload-targets=nvptx-none --without-cuda-driver --enable-gnu-indirect-function --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux\r\nThread model: posix\r\ngcc version 7.2.1 20170915 (Red Hat 7.2.1-2) (GCC) \r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\n\r\nexport LANG=\"en_US.UTF-8\"\r\nexport PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nexport PYTHON_LIB_PATH=\"/usr/lib64/python3.6/site-packages\"\r\nexport TF_NEED_JEMALLOC=\"1\"\r\nexport TF_NEED_S3=\"0\"\r\nexport TF_NEED_GCP=\"0\"\r\nexport TF_NEED_OPENCL=\"0\"\r\nexport TF_ENABLE_XLA=\"0\"\r\nexport TF_NEED_GDR=\"0\"\r\nexport TF_NEED_VERBS=\"0\"\r\nexport TF_NEED_MPI=\"0\"\r\nexport TF_NEED_CUDA=\"0\"\r\nexport TF_NEED_HDFS=\"0\"\r\nexport CC_OPT_FLAGS=\"-march=native\"\r\n./configure\r\n\r\nbazel build --action_env PATH=\"$PATH\" --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhen building tensorflow in an environment with only python3 (e.g. no /usr/bin/python but instead /usr/bin/python3) build fails.  As this is likely to become increasingly the case as other distributions drop python2.7 from default installs (and, eventually drop it altogether), this ought to be addressed.\r\n\r\n### Source code / logs\r\n\r\nERROR: /builddir/build/BUILD/tensorflow-1.4.1/tensorflow/python/BUILD:4363:1: Executing genrule //tensorflow/python:framework_fast_tensor_util_cython_translation failed (Exit 127): bash failed: error executing command \r\n  (cd /builddir/.cache/bazel/_bazel_mockbuild/88de5ec7248fb8215eb84aeff796b606/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/sbin:/builddir/.local/bin:/builddir/bin \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib64/python3.6/site-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; PYTHONHASHSEED=0 bazel-out/host/bin/external/cython/cython_binary --cplus tensorflow/python/framework/fast_tensor_util.pyx && python -c '\\''import shutil, sys; n = len(sys.argv); [shutil.copyfile(src.split(\".\")[0] + \".cpp\", dst) for src, dst in zip(sys.argv[1:], sys.argv[1+n//2:])]'\\'' tensorflow/python/framework/fast_tensor_util.pyx bazel-out/k8-py3-opt/genfiles/tensorflow/python/framework/fast_tensor_util.cpp')\r\n/usr/bin/env: 'python': No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 575.085s, Critical Path: 26.26s\r\nFAILED: Build did NOT complete successfully\r\n\r\n", "comments": ["@gunan could you take a look or redirect? Thanks!", "I think the cause is in our bzl rules we defined, we try to execute python files like binaries. Here is my hacks to get around this problem, it is not working yet but I believe I touch all the necessary pieces on this commit:\r\nhttps://github.com/gunan/tensorflow/commit/77eaad46df1fb6532577c130a2508665c17de049\r\n\r\nThe files we need to edit are:\r\nRemove shebang in these files:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/git/gen_git_source.py#L1\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/build_info/gen_build_info.py\r\n\r\nEdit the following rules to avoid trying to execute the files, build them as py_binaries first and then execute the built pieces:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L1564\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L1578\r\nhttps://github.com/tensorflow/tensorflow/blob/master/third_party/git/git_configure.bzl#L15\r\n\r\nAnd finally fix this rule. From what I can see the python command here is a noop, but I will need to verify it later:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/build_config.bzl#L77\r\n\r\n+CC @case540 @jart I may need help from you to guide me in this endeavor into skylark.\r\n ", "@gunan @case540 Another 14 days has passed. Is there any other track thread(s) that I may have not known? Thanks a lot!", "@gunan I could be of some assistance with review. Last time I touched this code was #14467. From the looks of https://github.com/gunan/tensorflow/commit/77eaad46df1fb6532577c130a2508665c17de049 this seems like a tough nut to crack. I barely understand this git/python autoconf code and my intuition tells me it *should* or *could* be significantly simpler. But not something I have cycles to think too deeply about right now.", "@jart Thanks a lot and I will try to use Miniconda2 for invoking like \"/usr/bin env python\" in the meantime.", "I thought I fixed everything (not merged to master yet), where I realized that any `py_test` or `py_binary` rule outputs get `/usr/bin/env python` by bazel.\r\nSo this is blocked by a bazel bug.\r\nI am contacting bazel team to see how we can fix this.", "Looks like this is blocked on a bazel bug.\r\nBazel silently inserts the line \"/usr/bin/env python\" at the first line of each file.\r\nSometimes python3 does not get symlinked to \"python\" and without python2 build fails.", "This is still blocked on a bazel bug.", "@gunan is there a bug tracking Bazel's insertion of that line?  Looks like [it's here](https://github.com/bazelbuild/bazel/blob/7a3e39fc20f1fba810d8023ff1608e39f501492a/src/main/java/com/google/devtools/build/lib/bazel/rules/python/BazelPythonSemantics.java#L188)?", "It has been a while, I do not remember completely.\r\nBut I remember discussing this with @ulfjack \r\nUlf,  is there a bug for this in bazel?", "This is still a bazel bug, we still have not received confirmation that it is fixed.", "Looks like now python2 is an actual dependency of bazel apt package.\r\nI am quite surprised about that.\r\n@meteorcloudy do you know why python2 is now a bazel dependency?", "https://github.com/bazelbuild/bazel/blob/5e34534fe6108277843632e5fd206052ca296ce3/scripts/packages/debian/control#L13\r\n@ulfjack Do we still need `python` as a dependency for Bazel's apt package?", "We're still shipping a bunch of python files as part of the Bazel binary. There are some Android tools as well as a couple of packaging build_defs under tools/. They should all be unbundled, but I'm not sure what the timeline is:\r\n```\r\n$ unzip -l $(which bazel) | grep \".py\"\r\n...\r\n     3510  1980-01-01 00:00   embedded_tools/tools/android/aar_embedded_jars_extractor.py\r\n     3135  1980-01-01 00:00   embedded_tools/tools/android/aar_native_libs_zip_creator.py\r\n     5244  1980-01-01 00:00   embedded_tools/tools/android/aar_resources_extractor.py\r\n     3882  1980-01-01 00:00   embedded_tools/tools/android/build_incremental_dexmanifest.py\r\n     3416  1980-01-01 00:00   embedded_tools/tools/android/build_split_manifest.py\r\n    30331  1980-01-01 00:00   embedded_tools/tools/android/incremental_install.py\r\n     5031  1980-01-01 00:00   embedded_tools/tools/android/instrumentation_test_check.py\r\n     6254  1980-01-01 00:00   embedded_tools/tools/android/junction.py\r\n     3094  1980-01-01 00:00   embedded_tools/tools/android/resource_extractor.py\r\n     1825  1980-01-01 00:00   embedded_tools/tools/android/strip_resources.py\r\n     5725  1980-01-01 00:00   embedded_tools/tools/android/stubify_manifest.py\r\n     1225  1980-01-01 00:00   embedded_tools/tools/build_defs/hash/sha256.py\r\n    15830  1980-01-01 00:00   embedded_tools/tools/build_defs/pkg/archive.py\r\n    10986  1980-01-01 00:00   embedded_tools/tools/build_defs/pkg/archive_test.py\r\n    12708  1980-01-01 00:00   embedded_tools/tools/build_defs/pkg/build_tar.py\r\n    11617  1980-01-01 00:00   embedded_tools/tools/build_defs/pkg/make_deb.py\r\n     8457  1980-01-01 00:00   embedded_tools/tools/build_defs/pkg/make_rpm.py\r\n     4632  1980-01-01 00:00   embedded_tools/tools/build_defs/pkg/make_rpm_test.py\r\n     2548  1980-01-01 00:00   embedded_tools/tools/build_defs/pkg/path_test.py\r\n      748  1980-01-01 00:00   embedded_tools/tools/build_defs/pkg/testenv.py\r\n     4784  1980-01-01 00:00   embedded_tools/tools/j2objc/j2objc_header_map.py\r\n    19390  1980-01-01 00:00   embedded_tools/tools/j2objc/j2objc_wrapper.py\r\n     3055  1980-01-01 00:00   embedded_tools/tools/jdk/proguard_whitelister.py\r\n     2725  1980-01-01 00:00   embedded_tools/tools/jdk/proguard_whitelister_test.py\r\n    20019  1980-01-01 00:00   embedded_tools/tools/objc/j2objc_dead_code_pruner.py\r\n     2056  1980-01-01 00:00   embedded_tools/tools/objc/make_hashed_objlist.py\r\n      820  1980-01-01 00:00   embedded_tools/tools/objc/protobuf_compiler.py\r\n       20  1980-01-01 00:00   embedded_tools/tools/python/2to3.sh\r\n     2456  1980-01-01 00:00   embedded_tools/tools/python/BUILD\r\n     4060  1980-01-01 00:00   embedded_tools/tools/python/python_version.bzl\r\n      107  1980-01-01 00:00   embedded_tools/tools/python/runfiles/BUILD\r\n     9560  1980-01-01 00:00   embedded_tools/tools/python/runfiles/runfiles.py\r\n    11050  1980-01-01 00:00   embedded_tools/tools/python/srcs_version.bzl\r\n     3139  1980-01-01 00:00   embedded_tools/tools/python/toolchain.bzl\r\n```", "Solution: https://github.com/tensorflow/tensorflow/issues/10289#issuecomment-492102920", "Closing due to staleness. Please check with the latest version of TensorFlow. Feel free to reopen if the issue still persists. Thanks!", "Issue seems to persist. Symlinking **/usr/bin/python** to **/usr/bin/python3** seems to work.\r\n`link /usr/bin/python3 /usr/bin/python`\r\n\r\nHope it helps!", "overcame this by `sudo ln -sf /usr/bin/python3 /usr/bin/python`", "> Issue seems to persist. Symlinking **/usr/bin/python** to **/usr/bin/python3** seems to work. `link /usr/bin/python3 /usr/bin/python`\r\n> \r\n> Hope it helps!\r\n\r\nDoesn't helps. Same with sudo\r\n`link: /usr/bin/python: Read-only file system`"]}, {"number": 15617, "title": "Fix periodic resample", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "Just signed individual CLA.", "CLAs look good, thanks!\n\n<!-- ok -->", "@vchigrin I'm just catching up on this thread. Thanks for adding the check for the desired shape and fixing the MathJax (seems I forgot the double slash). Regarding the documentation, I notice that most of my indentation is ignored. Do you know how to remedy this so that the formatting in the doc string is more-or-less respected on the documentation webpage?", "@jhetherly  - Sorry, I am very novice in Tensorflow, so I also don't know much about this. Will search the repo, if I find example code, will fix it too.\r\n\r\nBy the way, I locally implemented gradient propagation in this operation, and made output shape fully defined, when possible. May be it will be interesting to you to look at these changes. I can make PR with them if it worth it. See last 2 commits in \r\nhttps://github.com/vchigrin/tensorflow/commits/periodic-resample-grad", "@vchigrin Ah, I'll also hunt for ways to make the docs formatted a bit better. As for the gradient operations, it seems like you refactored some of the common components for the op and essentially did identical operations for both the op and the grad. If this summary is correct, then I greatly appreciate this addition. This was next on my todo list, but I hadn't found the time. Thanks a lot!", "@jhetherly Seems I've found way to  fix line breaks - it's neccessary to use two newlines instead of one. Tried to build documentation from latest commit, then used  pandoc to generate HTML from markdown files. Here is how generated HTML looks like now, please, check is this  something you expect?\r\nhttps://jsfiddle.net/tLdougwr/\r\nBy  the way, please, check whether last expression for  hq is  correct, I'm a bit confused -  Sq present in it twice....\r\n\r\nAbout my changes with  gradient propagation - yes, you are absolutely correct - I tried touse existing code as  much as possible. Index computation is tricky and I was afraid to make some hard to debug errors...", "@vchigrin I now see that it's a bit unclear, but the first $S_q$ is capitalized and is define earlier while the second $s_q$ is lower case and represents the index of the $q$th component of the output tensor. One small typo is where I give the definition of $S_q$ I wrote \"defined as by.\" Could you remove the \"as\" in that sentence? Thanks again for this PR!", "@jhetherly , Fixed it. Thanks for explaining that equation!", "@rmlarsen - Could you review this PR, please?", "@rmlarsen , @drpngx : Already after creation of this PR I made a bunch of even more useful changes in this operation in my local repository:\r\n\r\n- Added support for back propagating gradients for this operation.\r\n- Optimized it to allow compute indices incrementally - that speed up it 2.5 times (single-thread version).\r\n- Made possible split this operation across mutliple threads - that make it work even faster on modern multi-core CPU.\r\n\r\nSince this review, unfortunately, is not approved yet, I have a question: What is the better way of contributing my improvements? Should I push them all in this PR so all of them will be reviewed simultaneously, or it is better to wait for this PR to be approved and then make another one? ", "@rmlarsen , @drpngx - could anybody look at this PR? This is my first PR to Tensorflow, if I missed something I need to get these changes reviewed, please, tell me. I will be happy to contribute \r\nmy improvements to the project.\r\n", "@jhetherly WDYT?", "@drpngx : From my conversation @vchigrin and reviewing the changes to the code he's made, the changes look quite beneficial.\r\n@vchigrin, just to be sure, do the python tests in still pass with your changes (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/periodic_resample/python/kernel_tests/periodic_resample_op_test.py)? If so, I think we're good to go. If it's possible, it may be nice to include additional python tests for the gradients since you added that info into the op.", "@vchigrin Thanks for the change.\r\n@jhetherly Thanks for reviewing and helping improve it.", "@vchigrin: unless I missed it, this merge doesn't seem to include your contributions of the gradient info, etc. Do you plan to create another PR for these (__very__ useful) additions?", "@jhetherly : Sorry for delay - TensorFlow is my hobby, not primary work area. Yes, I will upstream these changes too, hope on this weekand. About your earlier questoions -  this PR does not have changes for gradient, that's why there are no new tests for it. New PR will have them  - I already wrote it. As far as I know all tests for this operation passed successfully on my machine. \r\n\r\nBy the way, is it correct that infrastructure in this repository will not allow to merge PR with broken tests?  I am very new in TensorFlow development... but as far as I know some projects implement such policy...", "@vchigrin: Ah, I understand (no need for an apology ;) ).\r\nI'm not sure what the merge policy is for TF.", "@jhetherly : Made PR https://github.com/tensorflow/tensorflow/pull/16520 - could you look at it? I extracted most index computation logic to separate class, this made implementing intra-op parallelism much easier.\r\n\r\nI tested it locally by command\r\nbazel test --config=opt --test_output=all '//tensorflow/contrib/periodic_resample/...'\r\nall tests passed successfully."]}, {"number": 15616, "title": "[CMake] Include example compile script", "body": "@mrry Friendly ping.", "comments": ["Can one of the admins verify this patch?", "Is it possible to move the CMake artifacts out of tree?", "> Is it possible to move the CMake artifacts out of tree?\r\n\r\nWell, `cmake` generally puts them in the current directory.\r\n\r\nA work-around would have to look like this:\r\n```\r\nmkdir -p artifacts\r\n\r\n(\r\ncd artifacts\r\nrm -rf *\r\ncmake ..\r\n)\r\n```\r\nIt would be nice to have this as a script included in the repo.", "I would rather have a script example than to support the in-tree build, which is not a good way to go in general.", "The script assumes it's run from that directory, it's probably better to `cd` to something like `dirname $0`. I think it's also better to name the directory something like `_build` rather than artifact", "Okay, good catch. But why the underscore?", "Just to make sure it looks special and doesn't clash with anything else.", "You'll need that preamble instead:\r\n```\r\n#!/usr/bin/env bash\r\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n```", "(2017)", "Jenkins, test this please.", "Ignoring `math_grad_test` flake."]}, {"number": 15615, "title": "Branch 180008567", "body": "", "comments": []}, {"number": 15614, "title": "Support Negativo17 Fedora Packaging", "body": "Support the [Negativo17](https://negativo17.org/nvidia-driver/) Nvidia driver packaging for Fedora. `libdevice` libraries are under `/usr/share/cuda`, includes are under `/usr/include/cuda` and libraries are under `/usr/lib64`. This PR should help #8264 too.\r\n\r\nIn addition, the gcc 5.3 in the Negativo17 repository (installed as `/usr/bin/gcc53`) only has a static non-PIC version of `libgomp.a`, so I have this local patch to force Tensorflow to link to the global (`/usr/lib64`) shared version:\r\n\r\n````diff\r\ndiff --git a/tensorflow/contrib/cmake/tf_stream_executor.cmake b/tensorflow/contrib/cmake/tf_stream_executor.cmake\r\nindex 91ca33f4c4..7719ee096d 100644\r\n--- a/tensorflow/contrib/cmake/tf_stream_executor.cmake\r\n+++ b/tensorflow/contrib/cmake/tf_stream_executor.cmake\r\n@@ -75,7 +75,7 @@ endif()\r\n #list(REMOVE_ITEM tf_stream_executor_srcs ${tf_stream_executor_test_srcs})\r\n \r\n if (NOT WIN32)\r\n-  set (CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -lgomp\")\r\n+  set (CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -l:libgomp.so.1\")\r\n endif (NOT WIN32)\r\n add_library(tf_stream_executor OBJECT ${tf_stream_executor_srcs})\r\n \r\ndiff --git a/third_party/gpus/cuda/BUILD.tpl b/third_party/gpus/cuda/BUILD.tpl\r\nindex b752734a08..0ce972291e 100644\r\n--- a/third_party/gpus/cuda/BUILD.tpl\r\n+++ b/third_party/gpus/cuda/BUILD.tpl\r\n@@ -109,7 +109,7 @@ cc_library(\r\n         \".\",\r\n         \"cuda/include\",\r\n     ],\r\n-    linkopts = [\"-lgomp\"],\r\n+    linkopts = [\"-l:libgomp.so.1\"],\r\n     linkstatic = 1,\r\n     visibility = [\"//visibility:public\"],\r\n )\r\ndiff --git a/third_party/toolchains/gpus/cuda/BUILD b/third_party/toolchains/gpus/cuda/BUILD\r\nindex 39136de99c..6f697919fd 100644\r\n--- a/third_party/toolchains/gpus/cuda/BUILD\r\n+++ b/third_party/toolchains/gpus/cuda/BUILD\r\n@@ -114,7 +114,7 @@ cc_library(\r\n         \".\",\r\n         \"cuda/include\",\r\n     ],\r\n-    linkopts = [\"-lgomp\"],\r\n+    linkopts = [\"-l:libgomp.so.1\"],\r\n     linkstatic = 1,\r\n     visibility = [\"//visibility:public\"],\r\n )\r\n````\r\n\r\nBuilding with clang is a lot more difficult, as it'd require making Tensorflow's CUDA symlink repo look enough like the unpacked tarball to pass [this detection logic](https://github.com/jyknight/llvm-monorepo/blob/6a6c3cae76a0839429c0b552572c46af9b194b86/clang/lib/Driver/ToolChains/Cuda.cpp)!\r\n\r\nMy `.tf_configure.bazelrc` (for FC 26) looks like:\r\n\r\n````\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/nicholas/miniconda3/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/nicholas/miniconda3/lib/python3.6/site-packages\"\r\nbuild --force_python=py3\r\nbuild --host_force_python=py3\r\nbuild --python_path=\"/home/nicholas/miniconda3/bin/python\"\r\nbuild --define with_jemalloc=true\r\nbuild:gcp --define with_gcp_support=true\r\nbuild:hdfs --define with_hdfs_support=true\r\nbuild:s3 --define with_s3_support=true\r\nbuild:xla --define with_xla_support=true\r\nbuild:gdr --define with_gdr_support=true\r\nbuild:verbs --define with_verbs_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr\"\r\nbuild --action_env TF_CUDA_VERSION=\"8.0\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env NVVMIR_LIBRARY_DIR=\"/usr/share/cuda\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc53\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild --define grpc_no_ares=true\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\nbuild --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\nbuild:mkl --define using_mkl=true\r\nbuild:mkl -c opt\r\nbuild:monolithic --define framework_shared_object=false\r\nbuild --define framework_shared_object=true\r\nbuild:android --crosstool_top=//external:android/crosstool\r\nbuild:android --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nbuild:android_arm --config=android\r\nbuild:android_arm --cpu=armeabi-v7a\r\nbuild:android_arm64 --config=android\r\nbuild:android_arm64 --cpu=arm64-v8a\r\n````", "comments": ["Can one of the admins verify this patch?", "I am not sure about if the added complexity warrants the benefit we will get with the new code.\r\nWe can rely on `LD_LIBRARY_PATH` to find most of the binaries for cuda anyway, if you add `/usr/lib64` to your library path, do you still need all the patches?\r\n\r\nIf we are to add support for these drivers, I think this needs to be done the following way:\r\n * The code needs more comments (for example, what is nvvm).\r\n * It needs to be written into `cuda_configure.bzl`.\r\n * Most importantly, rather than adding specific logic for this driver, we should add support for this specific driver package by making `cuda_configure.bzl` more flexible. What I mean by this is rather than hardcoding all the locations for the specific package, the cuda configuration code should search for more alternate locations for the libraries it needs.\r\n\r\n@martinwicke @zheng-xq to comment on support for this unofficial driver package\r\n@davidzchen to comment on cuda_configure code.\r\n\r\nAfter we receive comments from CCd people above, I will probably clsoe this PR, and maybe we can start a github issue and continue conversation there.", "I just came across this issue. I can confirm that adding `/usr/lib64` to `LD_LIBRARY_PATH` does not solve the issue (on Fedora 27).  Tensorflow still fails to build when using the negativo17's cuda package because nvvm can't be found (it's assumed to be under `cuda/nvvm`)\r\n\r\nnegativo17's repository is attractive here because it also packages other dependencies required by cuda that don't ship with Fedora 27, notably, gcc v6 ( fedora27 ships with v7 by default).", "@gunan can we close this and continue the discussion in an issue?", "I would like to close this PR in favor of https://github.com/tensorflow/tensorflow/pull/16319.\r\nI think that is a better approach to this challenge."]}, {"number": 15613, "title": "Tensorflow doesn't show the Cuda import messages ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I'm on Windows 10 Enterprise 64-bit \r\nWith a i5 4200m and GT740M,  I have pip installed Tensorflow GPU version, when I go into python type \"Import Tensorflow\" in the previous versions it used to give me messages on which library it successfully imported but on the latest one it gives me nothing, absolutely nothing ", "What information do you need from those message?\r\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "In the Previous Versions it used to display\r\n\r\n\"successfully opened CUDA library cublas64_80.dll locally\r\nsuccessfully opened CUDA library cudnn64_5.dll locally\r\nsuccessfully opened CUDA library cufft64_80.dll locally\r\nsuccessfully opened CUDA library nvcuda.dll locally\r\nsuccessfully opened CUDA library curand64_80.dll locally\"", "@akashmishra23, Sorry for late response.\r\n\r\nThis issue was resolved. Please refer logging details in below  \r\n\r\n```\r\n>>> import tensorflow as tf\r\n2022-03-14 15:57:13.750527: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n>>> print (tf.__version__)\r\n2.8.0\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/15613\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/15613\">No</a>\n"]}, {"number": 15612, "title": "Optimize FusedBatchNormGrad.", "body": "Reuse the output buffer and allocate only one temporary tensor, when data format is NHWC and\r\nGPU is used. This is based on the observation that cudnn can perform the backward computation\r\nin place. The same idea is used in PR #15601 . \r\n\r\nThis lowers GPU memory consumption and may improve performance, because fewer distinct memory addresses are accessed. It also permits a higher batch size.\r\n\r\nI've also added a new test for the gradient computation.", "comments": ["Can one of the admins verify this patch?", "/CC @zheng-xq ", "@codrut3, thank you very much for the contribution! This is indeed a nice change. Unfortunately, as you noted, the change relies on un-documented/un-officially supported feature of cuDNN; the future cuDNN may change this behavior and would break users silently. The change is also difficult to maintain, as the TF testing infrastructure needs to thoroughly test all cuDNN versions and various input sizes.\r\n\r\nIt seems un-safe to me to check in this change; we can certainly revisit this issue after cuDNN has official support on this.\r\n\r\nAdding @zheng-xq ", "Thanks for the contributions!\r\n\r\nAdding @benbarsdell and @nluehr from NVIDIA here. How reasonable is it that future cudnn can follow this convention that backward batch-norm can happen in place?\r\n\r\nAlso for real models, such as resnet50 or inceptionv3, how much is the performance and maximum batch size improvement? If they are quite significant, we could condition this on cudnn-versions. But that is a lot of maintenance overhead, and we could only consider it if the improvement is sizable in real models. Thanks!", "Hi @zhangyaobit ,\r\n\r\nI understand. If you think it's too much trouble I don't mind if you close the PR.", "In-place backward batch norm certainly works by accident rather than design. Also, execution can follow a variety of code paths depending on input size, GPU architecture, etc. I wouldn't be surprised if in-place breaks down in some corner cases. As noted above, it certainly could in future releases.\r\n\r\nMy assumption is that the memory savings should be modest. The out-of-place buffer is only needed during the actual cudnn call, and would be the size of one batch of activations. I would be interested to learn about any significant gains in important use cases. We might be able to add an explicitly in-place-safe variant.", "Closing the PR for now. Please re-open if it turns out the issue needs to be re-visited."]}, {"number": 15611, "title": "'saved_model_cli.py' bug fix!", "body": "In file `python/tools/saved_model_cli.py`  at function `def _print_tensor_info(tensor_info):`\r\nThe first line should be:\r\n\r\n `  print('    dtype: ' + {value:key for (key,value) in types_pb2.DataType.items()}[tensor_info.dtype])`\r\n\r\nNot be : ` print('    dtype: ' + types_pb2.DataType.keyss()[tensor_info.dtype])`\r\n\r\nbecause `tensor_info.dtype`  is an Integer which is the value of types(not the index of type values).", "comments": ["It's not necessary to do that, `types_pb2.DataType` is not python `dict`. The order of `EnumTypeWrapper.keys()` is not arbitrary.\r\n\r\nSee the definition of `EnumTypeWrapper.keys()`:\r\n```Python\r\n  def keys(self):\r\n    \"\"\"Return a list of the string names in the enum.\r\n    These are returned in the order they were defined in the .proto file.\r\n    \"\"\"\r\n\r\n    return [value_descriptor.name\r\n            for value_descriptor in self._enum_type.values]\r\n```\r\n\r\nand `self._enum_type.values` is something like:\r\n```Python\r\n  values=[\r\n    _descriptor.EnumValueDescriptor(\r\n      name='DT_INVALID', index=0, number=0,\r\n      options=None,\r\n      type=None),\r\n    _descriptor.EnumValueDescriptor(\r\n      name='DT_FLOAT', index=1, number=1,\r\n      options=None,\r\n      type=None),\r\n    _descriptor.EnumValueDescriptor(\r\n      name='DT_DOUBLE', index=2, number=2,\r\n      options=None,\r\n      type=None),\r\n      ...\r\n  ]\r\n```", "When I run tfdbg\uff1a\r\n```\r\ngraph = tf.get_default_graph()\r\nwith  graph.as_default():\r\n    v1 = tf.get_variable('v1', initializer=[1, 2, 3])\r\n    v2 = tf.square(v1, name='v2')\r\n    sess = tf.Session()\r\n    sess = tf_dbg.LocalCLIDebugWrapperSession(sess)\r\n    with sess.as_default():\r\n      sess.run(tf.global_variables_initializer())\r\n      _v2 = sess.run(v2)\r\n      print(_v2)\r\n```\r\nThe type of `v1` is `DT_INT_REF`\uff0cand saved_model_cli.py throws exception.", "Yes, my mistake, for `REF` type, the value is not the same as its index. Can you add a minimum repuducible code and its error log to better describe this problem?", "This is a simple  example to show the usage of `saved_model_cli`\uff1a\r\n\r\n```\r\nexport_dir = 'out'\r\nbuilder = tf.saved_model.builder.SavedModelBuilder(export_dir)\r\ngraph = tf.get_default_graph()\r\nwith graph.as_default():\r\n  v1 = tf.get_variable('v1',shape=[3],\r\n                  initializer=tf.zeros_initializer)\r\n  v2 = tf.square(v1,name='v2')\r\n  with tf.Session(graph=graph) as sess:\r\n    sess.run(v1.initializer)\r\n    builder.add_meta_graph_and_variables(sess,['xxx'])\r\n    predict_signature_def = tf.saved_model.\\\r\n    \tsignature_def_utils.build_signature_def(\\\r\n             inputs={\r\n        \t\t 'input_x': \\\r\n                  tf.saved_model.utils.build_tensor_info(v1),\r\n             }, outputs={'pred_y':\\\r\n\t\t\t\t  tf.saved_model.utils.build_tensor_info(v2)},\\\r\n             method_name=tf.saved_model.\\\r\n \t\t\t signature_constants.PREDICT_OUTPUTS)\r\n    builder.add_meta_graph([tf.saved_model.\\\r\n             tag_constants.TRAINING,\r\n             tf.saved_model.tag_constants.SERVING],\\\r\n             signature_def_map={'predict_graph': predict_signature_def})\r\n    builder.save(as_text=True)\r\n```\r\n\r\nThen run `saved_model_cli` in `shell`\uff1a\r\n\r\n```\r\nsaved_model_cli show --dir out/ --tag_set serve,train --signature_def predict_graph\r\n```\r\n\r\nThe result is : \r\n\r\n```\r\nThe given SavedModel SignatureDef contains the following input(s):\r\ninputs['input_x'] tensor_info:\r\ntensor type: 101\r\n    dtype: DT_FLOAT_REF\r\n    shape: (3)\r\n    name: v1:0\r\nThe given SavedModel SignatureDef contains the following output(s):\r\noutputs['pred_y'] tensor_info:\r\ntensor type: 1\r\n    dtype: DT_FLOAT\r\n    shape: (3)\r\n    name: v2:0\r\nMethod name is: outputs\r\n```\r\n\r\nI change `tensorflow/python/tools/saved_model_cli._print_tensor_info()` like this:\r\n\r\n```\r\ndef _print_tensor_info(tensor_info):\r\n  \"\"\"Prints details of the given tensor_info.\r\n\r\n  Args:\r\n    tensor_info: TensorInfo object to be printed.\r\n  \"\"\"\r\n#####  this is what I do ####\r\n  print('tensor type:',tensor_info.dtype)\r\n  print('    dtype: ' + {value:key for (key,value) in types_pb2.DataType.items()}[tensor_info.dtype])\r\n##########\r\n  # Display shape as tuple.\r\n  if tensor_info.tensor_shape.unknown_rank:\r\n    shape = 'unknown_rank'\r\n  else:\r\n    dims = [str(dim.size) for dim in tensor_info.tensor_shape.dim]\r\n    shape = ', '.join(dims)\r\n    shape = '(' + shape + ')'\r\n  print('    shape: ' + shape)\r\n  print('    name: ' + tensor_info.name)\r\n```\r\n\r\nBy the way, I use `python3.6+ubuntu 16.04+ tensorflow 1.4`\r\n\r\n", "Thanks for the supplement. I am not sure who is responsible for this module, friendly ping @yifeif @drpngx ", "/CC @sukritiramesh ", "Thanks @huaxz1986! Will fix this internally!"]}, {"number": 15610, "title": "Cannot load pretrained models in Android via jcenter-provided library", "body": "It's a great step to support importing Android lib via jcenter. However, I'm having trouble in loading pretrained models into `TensorFlowInferenceInterface`, which says `NodeDef mentions attr 'dilations' not in Op...`. I think it's a compatibility issue, meaning that the model graph is not consistent with the graph interpreter. The models directly downloaded from `slim` and frozen by myself. Anyone can help me?", "comments": ["Yup, this seems like a version mismatch. Closing this as a duplicate of #15698 so that the discussion happens on a single thread. Thanks!"]}, {"number": 15609, "title": "The relationship between neural network depth and accuracy", "body": "Hi:\r\nI am learning to design a simple neural network, and try to identify 28x28 pixel greyscale images in the MNIST dataset.\r\nI find that the neural network depth is not directly proportional to the recognition rate,\r\n\r\none layer neural network recognition rate: 92%\r\ntwo layer neural network recognition rate: 94%\r\nfour layer neural network recognition rate: 91.8%\r\n\r\nCan someone help me analyze it?\r\n![6](https://user-images.githubusercontent.com/31270354/34325172-85768d48-e8c5-11e7-831a-6d11b9b02bf5.PNG)\r\n(Please click on the picture to see the complete picture information)\r\n\r\n**thank you very much!**", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 15608, "title": "update _DECORATED_OPS for each latest call", "body": "This fixes a race condition where the arg list is not being reliably extracted during `add_arg_scope` .\r\nThe simple solution is to always update `_DECORATED_OPS` on each latest call to `_add_op`.\r\n\r\nfixes #11923", "comments": ["Can one of the admins verify this patch?", "/CC @sguada ", "Please sync to head and resolve conflicts.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "I sign it", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "It is just my github account.", "Could we have a test for that?", "@drpngx I can try writing a test a little later in the week", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Could you pull rebase and push again? merge commits seem to confuse the cla bot.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I must have missed the last message, I will fix (rebase & push) later today", "CLAs look good, thanks!\n\n<!-- ok -->", "@drpngx @sguada I rebased and pushed again, let me know if you need any other changes. I think this PR will need to be approved again since it went stale.", "Jeckins test this", "oh, the `_key_op` fn was renamed in 2bc52cd2d481a89c9724d20e827097efa4ff3f1e so it broke when I rebased, will push up a fix shortly", "@sguada it's ready for approval again", "@accraze could you fix the tests?", "Nagging Reviewer @sguada: It has been 17 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @sguada: It has been 32 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @sguada: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 49 days with no activity and the `awaiting review` label has been applied.", "Nagging Reviewer @sguada: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 63 days with no activity and the `awaiting review` label has been applied.", "Ignoring tflite flaky profiler test."]}, {"number": 15607, "title": "Fix `tf.pow(x, y)` edge case with integer x and negative integer y", "body": "This fix tries to address the issue raised in #12156 and #9560 (and PR #11852) where pow(x, y) hangs with an integer x and a negative value of y.\r\n\r\nThis fix tries to throw out an error like numpy in this case:\r\n```\r\n>>> np.power([5, 5], [2, -2])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nValueError: Integers to negative integer powers are not allowed.\r\n```\r\n\r\nThis fix adds error to the C++ functor like safe div/mod so that and InvalidArgument error could be triggered if any one of the values of y is negative.\r\n\r\nNOTE: Similar to safe_div/mod this fix also does not cover GPU (not working yet).\r\n\r\nThis fix fix #12156. This fix is also related to #9560.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["I'd rather we fix this upstream in Eigen, as @codrut3 commented on #9560 ?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@yongtang I agree with @drpngx that it would be nice to fix this in upstream Eigen, but that is not trivial because it would require the template for integer to return a float. This is a more involved change that might break other users of Eigen, so I tend to favor your change that at least lets TensorFlow users know that they are performing an operation that is not supported.", "@yongtang can you please resolve the conflict?", "Thanks @rmlarsen. The PR has been pushed with conflict resolved. Please take a look."]}, {"number": 15606, "title": "Fix issue #15588 by simplifying the code", "body": "The allocator.h code tried to be clever and use 32 byte alignment for SSE/AVX2/etc use,\r\nand 64 byte alignment for AVX512.\r\n\r\nUnfortunately, the #ifdef in use (from EIGEN) is not useful; the bazel BUILD files do\r\nnot propagate the tf_copts() compiler flags when the allocator.cc/allocator.h files get\r\ncompiled, to EIGEN does not see the actual AVX512 using compiler flags...\r\n\r\nRather than changing compiler flag propagation throughout a whole bunch of code,\r\nthere's an opportunity to just simplify the code and always use 64 byte alignment.\r\nYes it wastes a bit of space, but on the other hand now these allocations are\r\ncache line aligned which isn't a bad thing... and an ifdef can be dropped\r\n\r\nSigned-off-by: Arjan van de Ven <arjan@linux.intel.com>", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "Did you sign the CLA? If so, you need to reply with \"I signed it!\". Thanks.", "On Sun, Dec 24, 2017 at 18:28 drpngx <notifications@github.com> wrote:\n\n> Did you sign the CLA? If so, you need to reply with \"I signed it!\". Thanks.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15606#issuecomment-353816735>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABPeFd6z_lEbLGBUq6vMSyg0da7tbRaEks5tDwhJgaJpZM4RLyej>\n> .\n>\nI am trying to get ahold of a corporate lawyer who knows the corporate cla\nowner to get me added to some list.\n\nChances of that happening on Christmas Eve? Not great.\n", "Haha no worries. Who knows, maybe it'll happen on Christmas!\n\nOn Dec 24, 2017 8:04 PM, \"Arjan van de Ven\" <notifications@github.com>\nwrote:\n\nOn Sun, Dec 24, 2017 at 18:28 drpngx <notifications@github.com> wrote:\n\n> Did you sign the CLA? If so, you need to reply with \"I signed it!\".\nThanks.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <\nhttps://github.com/tensorflow/tensorflow/pull/15606#issuecomment-353816735>,\n> or mute the thread\n> <\nhttps://github.com/notifications/unsubscribe-auth/ABPeFd6z_lEbLGBUq6vMSyg0da7tbRaEks5tDwhJgaJpZM4RLyej\n>\n> .\n>\nI am trying to get ahold of a corporate lawyer who knows the corporate cla\nowner to get me added to some list.\n\nChances of that happening on Christmas Eve? Not great.\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/pull/15606#issuecomment-353826916>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/AT_SbYcG7I0S-j-G7hU_qwlGEPPSrIBTks5tDx7jgaJpZM4RLyej>\n.\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@fenrus75 any luck with the lawyers?", "@fenrus75 any luck?", "CLAs look good, thanks!\n\n<!-- ok -->", "lawyer situation resolved", "Wooohoo!\r\n\r\n@rmlarsen could you take a look?", "My main concern is that this will increase memory usage on mobile. @petewarden WDYT?\r\n@benoitsteiner could you chime in as well, please?", "I think TF_AllocateTensor from the C API will still try to do [32/64 byte alignment based on EIGEN_MAX_ALIGN_BYTES](https://github.com/tensorflow/tensorflow/blob/a21eb81c2c9fe24f9d2f5d448aeb11858855c1ab/tensorflow/c/c_api.cc#L152). So it sounds like that's similarly stuck on 32? May be worth switching that too for consistency (and so AVX512 works).\r\n\r\nCC @gunan who I think would prefer alignments not depend on build flags", "on memory consumption; max growth is 32 bytes ... most tensors are much much larger than these and it's in the noise.", "What's the performance impact on mobile CPUs such as the Cortex A9 that have a cache line size of 32 ? It seems this this change has the potential to increase cache conflicts and/or decrease cache utilization.", "that's a bit of a stretch; if you have a large-ish (more than a few cache lines) data structure, where you start for such a case does not matter.\r\n\r\nfor example, lets say your data structure is 10 cache lines in size (which is pretty tiny for tensors)\r\n\r\nyou can do (O=odd E = even)\r\nOEOEOEOEOE\r\nor\r\nEOEOEOEOEO\r\n\r\nif you would do 32 bytes\r\n\r\nif you only do 64 bytes, sure, you only get\r\nEOEOEOEOEO\r\nbut you still have 5 E's and 5 O's... you don't get disproportional pressure.\r\n", "(or to put your concern in other words; going to 2x cache line size is a non-issue, but doing half cache line size will always leave half a cache line unused)", "@hawkinsp Can you look at the XLA failure? I appears XLA asserts that the alignment is equal to 32. Is this a guard against two constants diverging and we can simply change the one in XLA to match, or is this a more involved change?", "@eliben @jlebar also if you have input.", "@tatatodd wrote the test, but he's on vacation for the next while.  I think it's very likely fine to fix the test by changing the value in compiler/aot/runtime.h.  Please also update the verbiage in the test (s/32/64).  cc @sanjoy in case I missed something.", "@fenrus75 can you fix the XLA test in the manner @jlebar described? Thanks!", "+1 to @jlebar's suggestion.\r\n\r\nSpecifically, change `kAlign=64` in `compiler/aot/runtime.h`.  Then fix up the comments in `runtime_test.cc`.  You might also need to fix up the `AlignedBufferBytes` test to account for the new alignment.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "(I'm on vacation away from my Core i9 machine so not able to work on this\nfor at least another week)\n\nOn Wed, Mar 7, 2018 at 7:14 AM, Alfred Sorten Wolf <notifications@github.com\n> wrote:\n\n> Nagging Assignee @rmlarsen <https://github.com/rmlarsen>: It has been 14\n> days with no activity and this issue has an assignee. Please update the\n> label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15606#issuecomment-371133268>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABPeFZIngB_GA7W0nkhes5YAFAOCJ27Vks5tb90qgaJpZM4RLyej>\n> .\n>\n", "@fenrus75 sounds like we can proceed when you are back from vacation.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@fenrus75 should we proceed with this, or did you find a workaround?", "I've done some testing with an AVX512 enabled tensorflow build over the last few days.  I can confirm that this patch is still needed but it is not enough by itself to prevent alignment crashes.  Setting EIGEN_MAX_ALIGN_BYTES to 64, as mentioned above, is also required.  In addition, a third patch is needed to prevent some compiler unit test failures, as has also been mentioned above.  I've submitted a new PR ( https://github.com/tensorflow/tensorflow/pull/19121 ) that contains fenrus75's original commit + 2 new commits to set EIGEN_MAX_ALIGN_BYTES and to fix the unit tests.\r\n\r\nEven with these patches, AVX512 enabled builds still don't work very well.  I'm seeing a lot of unit tests failures.  These seem to be caused by errors in floating point calculations resulting in NaNs.  I haven't investigated the calculation errors yet but I have noticed that the [getting started example](https://github.com/tensorflow/models/tree/master/samples/core/get_started) also fails with reports of NaNs after the first iteration.  I'll take a look at this later this week.", "Closing because we have #19121 now."]}, {"number": 15605, "title": "Make `frame` positions configurable", "body": "This also fixes a bug in `get_root_dir_with_all_resources`.\r\n\r\nThe value of `sys._getframe(1)` is caller-sensitive.\r\nThis leads to unexpected bugs where it gives the path of the wrong file.\r\n\r\nFor example, `get_root_dir_with_all_resources` is supposed to find the `runfiles` directoy.\r\nBut because of the extra call to `get_data_files_path`, everything shifts by one and now it resolves all paths on the location of the `resource_loader.py` file loaded (repository or PIP, but not a `runfiles` path). In the places it's used currently, it doesn't break anything, tho.\r\n\r\nThis PR fixes the behavior of `get_root_dir_with_all_resources` and makes the `frame` positions configurable on all methods (with a default for compatibility).\r\n\r\n@martinwicke Friendly ping.", "comments": ["Can one of the admins verify this patch?", "Yes, the motivation behind this is about different files being de-referenced to different places.\r\nNo, it's not (purely) about references to PIP, I have already accounted for this.\r\n\r\nThe problem is rather that while the python file initially called via `bazel` is \"correctly\" reported to be in `runfiles`, any other files called have their symlinks resolved.\r\n\r\nDepending on a few circumstances, things might as well work out, but in my case, I needed a `runfiles` path to remove a `__init__.py` file to make python call the PIP package instead of `runfiles` which doesn't contain generated ops which leads to immediate failure.", "My use case might not need this anymore due to recent changes, but I think nonetheless that something so easily breakable should at least be given a parameter for control.\r\n\r\nAnd `get_root_dir_with_all_resources` is dedicated to deal with `runfiles` paths.", "If there isn't a strong need for it right now, let's abandon and revisit when there is one."]}, {"number": 15604, "title": "ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory", "body": "I installed tf-nightly build and I get the following error on import of tensorflow.\r\n`ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory`.\r\n\r\nIf I check for cuda 9, I get the following:\r\n```\r\nldconfig -v\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib:\r\n\tlibnvgraph.so.8.0 -> libnvgraph.so.8.0.61\r\n\tlibnppicom.so.8.0 -> libnppicom.so.8.0.61\r\n\tlibnppial.so.8.0 -> libnppial.so.8.0.61\r\n\tlibcufftw.so.8.0 -> libcufftw.so.8.0.61\r\n\tlibcufft.so.8.0 -> libcufft.so.8.0.61\r\n\tlibnppif.so.8.0 -> libnppif.so.8.0.61\r\n\tlibcublas.so.8.0 -> libcublas.so.8.0.88\r\n\tlibnvblas.so.8.0 -> libnvblas.so.8.0.88\r\n\tlibnppi.so.8.0 -> libnppi.so.8.0.61\r\n\tlibcusolver.so.8.0 -> libcusolver.so.8.0.61\r\n\tlibnppidei.so.8.0 -> libnppidei.so.8.0.61\r\n\tlibnvrtc-builtins.so.8.0 -> libnvrtc-builtins.so.8.0.61\r\n\tlibnvrtc.so.8.0 -> libnvrtc.so.8.0.61\r\n\tlibnpps.so.8.0 -> libnpps.so.8.0.61\r\n\tlibcuinj64.so.8.0 -> libcuinj64.so.8.0.61\r\n\tlibnppig.so.8.0 -> libnppig.so.8.0.61\r\n\tlibOpenCL.so.1 -> libOpenCL.so.1.0.0\r\n\tlibnppicc.so.8.0 -> libnppicc.so.8.0.61\r\n\tlibnppist.so.8.0 -> libnppist.so.8.0.61\r\n\tlibnppisu.so.8.0 -> libnppisu.so.8.0.61\r\n\tlibnppim.so.8.0 -> libnppim.so.8.0.61\r\n\tlibcurand.so.8.0 -> libcurand.so.8.0.61\r\n\tlibcudart.so.8.0 -> libcudart.so.8.0.61\r\n\tlibnvToolsExt.so.1 -> libnvToolsExt.so.1.0.0\r\n\tlibnppitc.so.8.0 -> libnppitc.so.8.0.61\r\n\tlibnppc.so.8.0 -> libnppc.so.8.0.61\r\n\tlibcusparse.so.8.0 -> libcusparse.so.8.0.61\r\n/usr/local/cuda-9.1/targets/x86_64-linux/lib:\r\n\tlibnppicc.so.9.1 -> libnppicc.so.9.1.85\r\n\tlibnppisu.so.9.1 -> libnppisu.so.9.1.85\r\n\tlibcufftw.so.9.1 -> libcufftw.so.9.1.85\r\n\tlibcufft.so.9.1 -> libcufft.so.9.1.85\r\n\tlibnppial.so.9.1 -> libnppial.so.9.1.85\r\n\tlibnppist.so.9.1 -> libnppist.so.9.1.85\r\n\tlibcublas.so.9.1 -> libcublas.so.9.1.85\r\n\tlibnvblas.so.9.1 -> libnvblas.so.9.1.85\r\n\tlibnppitc.so.9.1 -> libnppitc.so.9.1.85\r\n\tlibcusolver.so.9.1 -> libcusolver.so.9.1.85\r\n\tlibnvrtc.so.9.1 -> libnvrtc.so.9.1.85\r\n\tlibnvrtc-builtins.so.9.1 -> libnvrtc-builtins.so.9.1.85\r\n\tlibnppidei.so.9.1 -> libnppidei.so.9.1.85\r\n\tlibOpenCL.so.1 -> libOpenCL.so.1.0.0\r\n\tlibnppig.so.9.1 -> libnppig.so.9.1.85\r\n\tlibnppc.so.9.1 -> libnppc.so.9.1.85\r\n\tlibcudart.so.9.1 -> libcudart.so.9.1.85\r\n\tlibnvToolsExt.so.1 -> libnvToolsExt.so.1.0.0\r\n\tlibnvgraph.so.9.1 -> libnvgraph.so.9.1.85\r\n\tlibnppif.so.9.1 -> libnppif.so.9.1.85\r\n\tlibcusparse.so.9.1 -> libcusparse.so.9.1.85\r\n\tlibaccinj64.so.9.1 -> libaccinj64.so.9.1.85\r\n\tlibcuinj64.so.9.1 -> libcuinj64.so.9.1.85\r\n\tlibnppim.so.9.1 -> libnppim.so.9.1.85\r\n\tlibnppicom.so.9.1 -> libnppicom.so.9.1.85\r\n\tlibnpps.so.9.1 -> libnpps.so.9.1.85\r\n\tlibcurand.so.9.1 -> libcurand.so.9.1.85\r\n```\r\nI that due to a name mismatch. `libcublas.so.9.0 =! libcublas.so.9.1`? And if so how can we overcome this?", "comments": ["I think this is due to the fact that you have CUDA 9.1 and not 9.0, I am facing exactly the same issue.", "@Timonzimm I know and I think the whole issue is this f** naming libcublas.so.xxx that nvidia puts. This inherently is mismatch on linux systems whenever that number changes, so since it can not find the exact matches then it thinks the file doesn't exist and throws the error.", "I think you should use symbol link from ''cuda/''   to ''cuda/9.1\",or your cuda version is too new to tensorflow master branch", "@burui11087 I completely forgot about symlinking. Thanks for reminding me.", "Seems like this is resolved (and the root cause was a version mismatch of CUDA - 9.0 vs 9.1)?\r\nClosing this out since I understand it to be resolved, but please let me know if I'm mistaken.\r\n\r\nFYI @gunan @av8ramit (who are working on the upcoming 1.5 release)", "I also occur the exactly same problem with kirk86. For me, I installed cuda toolkit 8.0, and cudnn 5.1.\r\nThen I did what you guys said above, all of them does not work. ", "For using nightlies, you have to have CUDA 9.0 and cudnn 7 installed.\r\n@yangfengKAUST with the current version of cuda and cudnn installed TF is just complaining that it cannot find the versions it is expecting.", "@Timonzimm  I am facing the same issue. Have you figured it out? ", "I have 8.0, 9.0, 9.1 installed + cudnn versions which seem specific to each.  The sym  linking didn't work from the 9.1 libs.  I suspect that sometimes the symlink in the LD_LIBRARY_PATH doesn't work either when I switch versions on the /usr/local/cuda link.  I ended up just doing it the low tech way to get the libraries loaded into my java program until I can figure out a cleaner way to handle the paths inside of Eclipse. \r\n\r\n\t    try {\r\n\t        System.load(\"/usr/local/cuda/lib64/libcublas.so.9.0\");\r\n\t        System.load(\"/usr/local/cuda/lib64/libcusolver.so.9.0\");\r\n\t        System.load(\"/usr/local/cuda/lib64/libcudart.so.9.0\");\r\n\t        System.load(\"/usr/local/cuda/lib64/libcufft.so.9.0\");\r\n\t        System.load(\"/usr/local/cuda/lib64/libcurand.so.9.0\");\r\n\r\n\t        System.load(\"/home/greg/Desktop/platform/tensorbuilder/jni/libtensorflow_jni.so\");\r\n\t      } catch (UnsatisfiedLinkError e) {\r\n\t          System.err.println(\"Native code library failed to load.\\n\" + e);\r\n\t          System.exit(1);\r\n\t      }\r\n.  ", "@asimshankar Would like to know that in your above comment you mean that we should downgrade cuda to 9.0 and tensorflow 1.5 doesn't work with cuda 9.1 ? \r\n\r\nYou have closed this issue but its not clear what is the correct action that  we should take!\r\n\r\nNote: I also have cuda 9.1 installed instead of cuda 9.0.  ", "Just FYI, I have both installed.  Building from scratch will work w/ either, but the nightly binaries use 9.0. ", "@AwasthiMaddy - Yes TensorFlow 1.5 release binaries are built for CUDA 9.", "Have you solved it ? This problem is caused tensorflow-gpu-1.5 required cuda 9.0 ,so you should install tensorflow-gpu-1.4. And rember uninstall tensorflow-gpu-1.5. Please use this\"pip install --upgrade tensorflow-gpu==1.4\"", "@aipeteryao  - Thank you. ", "Someone needs to fix the https://www.tensorflow.org/install/install_linux page if this is true, I just followed its instructions exactly, and tells you to install CUDA 8.0 (specifically, not \"latest CUDA\").\r\n\r\nThen as soon as you're done, you get this error (it is looking for cublas 9.0, which, from what I can read here, would not have worked either, as CUDA 9.1 is the default you get from NVIDIA).\r\n\r\nEither the webpage instructions should work with the default latest of everything, or it should tell you explicitly to install tensorflow-gpu-1.4 (for example) and not tensorflow-gpu..", "Seconding bwesons's comment. I have CUDA 8.0 and Tensorflow 1.3. I followed the current install instructions for TF 1.5 (GPU, ubuntu, virtualenv) and it breaks as described above. Reverting to TF 1.3 until this is resolved.", "@aipeteryao This fixed it, thanks!  I ended up uninstalling the latest version and installing 1.4, in my virtualenv.\r\n\r\n```\r\npip3 uninstall tensorflow-gpu\r\npip3 install --upgrade tensorflow-gpu==1.4\r\n```\r\n\r\nThe install page for Ubuntu should be updated: https://www.tensorflow.org/install/install_linux\r\nSince TensorFlow 1.5 is expecting Cuda 9.0 ( NOT 9.1 ), as well as cuDNN 7", "In fact, we should view the official document of tensorflow ,it give tensorflow\u2018s envirment(include python,gcc,cuda,cudnn,an so on).", "@bwesen yes,you were right .My computer installed CUDA 8.0,cudnn 6.0 ,tensorflow 1.4.", "I think this issue should still be open.  @bwesen's [comment](https://github.com/tensorflow/tensorflow/issues/15604#issuecomment-362637994) is correct.  [The docs](http://archive.is/S1P6z) tell you to install Cuda 8.0 and use `pip install --upgrade tensorflow-gpu`.  Right now that gives you tensorflow 1.5 which does not work with Cuda 8.0\r\n\r\npinging @asimshankar ", "I have the same issue (with cuda 9.1 + tensorflow 1.5). I think to resolve it, one option is that to downgrade cuda to 9.0. The other option would be to downgrade both cuda to 8.0 and tensorflow to 1.4. If you have already installed cuda 8.0, you only need to modify `LD_LIBRARY_PATH` (and `CUDA_HOME`) environment variable to point to cuda 8.0 directory (i.e. `/usr/local/cuda-8.0`).", "I'm getting this issue (Cuda 9.1.85, cuDNN 7.05)\r\n\r\nTried with tensorflow 1.5, it broke. Uninstalled, installed 1.4 with ```pip3 install --upgrade tensorflow-gpu==1.4```, still broke.", "@DylanDmitri  1.5 expects Cuda 9.0, not 9.1\r\n\r\nHave you tried with Cuda 9.0 drivers?  ", "@DylanDmitri @mkaze You need Cuda 9.0.\r\n\r\nAlso, for anyone having trouble installing requirements, I suggest double checking your cuDNN installation.  The .deb file didn't work for me because it did not copy files to the right place.  I had to use the .tgz file and manually copy files according to nVidia's directions in order to get a working installation.", "Why not just install cuda-9-0?\r\n- Go here: https://developer.nvidia.com/cuda-90-download-archive\r\n- Then, for me: Download deb (network)\r\n```\r\nsudo dpkg -i\u00a0cuda-repo-ubuntu1604_9.0.176-1_amd64.deb\r\nsudo apt-key adv --fetch-keys \\\r\n     http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\r\nsudo apt-get update\r\nsudo apt-get install cuda-9-0\r\n```", "> Why not just install cuda-9-0?\r\n\r\n@abrahamrhoffman That's easy for anyone who has sudo privileges but what about people on a shared system like a cluster environment with simple user privileges. In those cases even if you ask from the sys admin to install any libraries most probably the answer is gonna be NO! Since they are afraid that might interfere with other users' settings and environments.\r\n\r\n@abrahamrhoffman Would you also mind providing a justification on the down vote?\r\n", "I installed cuda-9.0 and still it does not work. This is really irritating.", "Please make sure to set your PATH variable appropriately, such as described here: https://stackoverflow.com/questions/39287744/ubuntu-16-04-nvidia-toolkit-8-0-rc-darknet-compilation-error-expected-a/41290056#41290056\r\n\r\n```\r\nexport PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda9.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n```", "cuda 9.1 is the current version. I want TF to use it. How to?", "Even tf-nightly-gpu is not looking for cuda 9.1. Meh", "I tried symbolic links from all the 9.0 filenames to all the 9.1 filenames and it didn't work. In the end, TF knows the true version. The repo doesn't even have 9.0 anymore so I'm afraid I'll break my nvidia stuff if I remove 9.1 and then manually install 9.0.", "I fix him for now by:\r\n\r\nDownload deb (network) from: https://developer.nvidia.com/cuda-90-download-archive?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1604&target_type=debnetwork\r\n\r\nThen: `dpkg -i cuda-repo-ubuntu1604_9.0.176-1_amd64.deb`\r\n\r\nThen: aptitude update\r\n\r\nThen: aptitude install cuda-9-0", "First I've installed tensorflow 1.5, it broke, and I get the following error:\r\n`ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory`\r\nthen I uninstalled,  installed 1.4 with pip install --upgrade tensorflow-gpu==1.4, it did't work,  and I get the following error:\r\n`ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory`\r\n", "@xiezhongzhao What version of Cuda are you using? For tensorflow 1.5 you must have installed the Cuda 9.0 and for tensorflow 1.4 you must use cuda 8.0. If the the tensorflow version and cuda version are compatible, then check the environment variables i.e. `CUDA_HOME `and `LD_LIBRARY_PATH`.", "@mkaze I used Cuda9.1", "@xiezhongzhao Install Cuda 9.0 and you should be fine. Tensorflow 1.5 does not work with Cuda 9.1.", "@mkaze Thank you very much", "I am also getting this issue and struggling to resolve it.  \r\n```\r\n$ pip3 install tensorflow-gpu\r\n$ python3\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n```\r\nI installed following these instructions\r\nhttps://www.tensorflow.org/install/install_linux#nvidia_requirements_to_run_tensorflow_with_gpu_support\r\n\r\nI believe I installed the right versions from nvidia.  filenames were \r\n`cuda-repo-ubuntu1604_9.0.176-1_amd64.deb`\r\nand `cudnn-9.0-linux-x64-v7.tgz` (version 7.0.5)\r\n\r\nI set the path as per those instructions on the tensorflow docs and also tried the instructions that `abrahamrhoffman` gave above.\r\n\r\nWhen I run ldconfig -v I get some 9.0 libs, but do not see libcublas.so.9.0\r\n```\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib:\r\n\tlibaccinj64.so.9.0 -> libaccinj64.so.9.0.176\r\n\tlibOpenCL.so.1 -> libOpenCL.so.1.0.0\r\n\tlibnvToolsExt.so.1 -> libnvToolsExt.so.1.0.0\r\n\tlibcuinj64.so.9.0 -> libcuinj64.so.9.0.176\r\n\tlibcudart.so.9.0 -> libcudart.so.9.0.176\r\n/usr/local/cuda-9.1/targets/x86_64-linux/lib:\r\n\tlibnppif.so.9.1 -> libnppif.so.9.1.85\r\n\tlibcusparse.so.9.1 -> libcusparse.so.9.1.85\r\n\tlibcusolver.so.9.1 -> libcusolver.so.9.1.85\r\n\tlibnpps.so.9.1 -> libnpps.so.9.1.85\r\n\tlibnppial.so.9.1 -> libnppial.so.9.1.85\r\n\tlibnvgraph.so.9.1 -> libnvgraph.so.9.1.85\r\n\tlibcuinj64.so.9.1 -> libcuinj64.so.9.1.85\r\n\tlibaccinj64.so.9.1 -> libaccinj64.so.9.1.85\r\n\tlibnppicc.so.9.1 -> libnppicc.so.9.1.85\r\n\tlibcudart.so.9.1 -> libcudart.so.9.1.85\r\n\tlibnppc.so.9.1 -> libnppc.so.9.1.85\r\n\tlibnppicom.so.9.1 -> libnppicom.so.9.1.85\r\n\tlibOpenCL.so.1 -> libOpenCL.so.1.0.0\r\n\tlibnppig.so.9.1 -> libnppig.so.9.1.85\r\n\tlibnvToolsExt.so.1 -> libnvToolsExt.so.1.0.0\r\n\tlibnppidei.so.9.1 -> libnppidei.so.9.1.85\r\n\tlibcurand.so.9.1 -> libcurand.so.9.1.85\r\n\tlibnvblas.so.9.1 -> libnvblas.so.9.1.128\r\n\tlibnvrtc.so.9.1 -> libnvrtc.so.9.1.85\r\n\tlibnppitc.so.9.1 -> libnppitc.so.9.1.85\r\n\tlibnppist.so.9.1 -> libnppist.so.9.1.85\r\n\tlibcublas.so.9.1 -> libcublas.so.9.1.128\r\n\tlibnppim.so.9.1 -> libnppim.so.9.1.85\r\n/sbin/ldconfig.real: /usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudnn.so.7 is not a symbolic link\r\n\r\n\tlibcudnn.so.7 -> libcudnn.so.7.0.5\r\n\tlibcufftw.so.9.1 -> libcufftw.so.9.1.85\r\n\tlibcufft.so.9.1 -> libcufft.so.9.1.85\r\n\tlibnppisu.so.9.1 -> libnppisu.so.9.1.85\r\n\tlibnvrtc-builtins.so.9.1 -> libnvrtc-builtins.so.9.1.85\r\n```\r\n\r\nI did not install 9.1, at least not intentionally.  This is on a amazon ec2 instance with stock ubuntu 16.04.  \r\nnvidia-smi also returns a gpu, this is a g3.4xlarge instance\r\n\r\nany guidance is greatly appreciated. ", "Per the CUDNN guide at:\r\n\r\nhttp://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html\r\n\r\nYou need to copy the unpacked files (from the directory you ran `$ tar -xzvf cudnn-9.0-linux-x64-v7.tgz` or similar) into `/usr/local/cuda` subdirectories:\r\n\r\n`$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include`\r\n`$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64`\r\n`$ sudo chmod a+r /usr/local/cuda/include/cudnn.h`\r\n`/usr/local/cuda/lib64/libcudnn*`\r\n", "Thank you for the reply @entropy43.  \r\n\r\nI should have been more specific.  I did those two `cp` and the `chmod` commands after `tar`.  When I look in the folder cuda folder for where I ran the tar command like `ls cuda/lib64` I see\r\n```libcudnn.so  libcudnn.so.7  libcudnn.so.7.0.5  libcudnn_static.a```\r\n\r\nI tried this section as well from the nvidia [doc](http://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html)\r\n`2.3.2. Installing from a Debian File`\r\n\r\nand the tests pass here in `2.4. Verifying`\r\n\r\nAny advice to get that lib is appreciated ", "Could someone please summarize where this currently stands? I am using TF1.3 + cuda 8 with no problem. I'd  like to upgrade but it seems like the install process for newer versions is completely broken. Advice?", "Similar to what yazabazra is asking above:\r\nTF1.6 Ubuntu 16.04\r\nnvcc -V\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Nov__3_21:07:56_CDT_2017\r\nCuda compilation tools, release 9.1, V9.1.85\r\nWhich requited a Nvidia display driver 390+\r\nCritical to see: https://devtalk.nvidia.com/default/topic/1000340/cuda-setup-and-installation/-quot-nvidia-smi-has-failed-because-it-couldn-t-communicate-with-the-nvidia-driver-quot-ubuntu-16-04/post/5243047/#5243047\r\n\r\nwhelp to add to it all,, After a major amount of hassle I got the Nvidia updated to the newest release see above, as the TF doc indicated that there were bugs in an earlier release.. \r\n\r\nNow I'm getting the : \r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\nFailed to load the native TensorFlow runtime.\r\n\r\nWhich appears to be a mismatch between 9.0 (TF wants) vs 9.1 Which is most current Nvidia.\r\nIt would seem better to run with 9.1 but I'd rather avoid building TF from source and it seems that may not fix it anyhow..\r\n\r\nCan this combo be made to work with a binary package?\r\nTF 1.6 Cuda 9.1 ??\r\n", "Further note and caution to those looking here.. after upgrading my Nvidia stuff my older versions of TF in separate conda env's no longer work as the older TF wants : ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\nso 9.1 won't cut it.. how about specifying greater than?? vs specific versions? just a suggestion.. In the meantime I'm dead in the water..\r\n", "And this is why availability of a binary that supports 9.1 would be nice: (from the TF1.6 release notes)\r\n\r\nUsing XLA:GPU with CUDA 9 and CUDA 9.1 results in garbage results and/or\r\nCUDA_ILLEGAL_ADDRESS failures.\r\n\r\nGoogle discovered in mid-December 2017 that the PTX-to-SASS compiler in CUDA 9\r\nand CUDA 9.1 sometimes does not properly compute the carry bit when\r\ndecomposing 64-bit address calculations with large offsets (e.g. load [x + large_constant]) into 32-bit arithmetic in SASS.\r\n\r\nAs a result, these versions of ptxas miscompile most XLA programs which use\r\nmore than 4GB of temp memory. This results in garbage results and/or\r\nCUDA_ERROR_ILLEGAL_ADDRESS failures.\r\n\r\nA fix in CUDA 9.1.121 is expected in late February 2018. We do not expect a\r\nfix for CUDA 9.0.x. Until the fix is available, the only workaround is to\r\ndowngrade to CUDA 8.0.x\r\nor disable XLA:GPU.\r\n\r\nMaybe one of the nightlies does it?", "Another solution? can one install multiple revisions of Cuda since  TF seems to search for specific Rev's? If so, any advice as to how to? \r\n\r\nSo Ideally I'd be able to to run TF 1.4(which currently requires Cuda 8.0) in one conda environment and TF 1.6 (which currently requires Cuda 9.0) in another?", "So I  just added sudo apt-get -y install cuda-toolkit-9.0 and I'm up and running with TF1.6", "@dartdog after installing cuda-toolkit-9.0, did you face the issue `ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory` ? ", "@dartdog \r\n```\r\nsudo apt-get install cuda-7-0\r\nvim ~/.bashrc\r\nexport PATH=/usr/local/cuda-7.0/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda7.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\nexport PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda9.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n```", "This is definitely supposed to be included in the tensorflow documentation, as said by @bwesen. It should also be included in the errors list. Is it possible for us do this in anyway? I think I am going to open a new issue, requesting to either add support for cuda 9.1, or mention that you need cuda 9.0 specifically in the docs, and also include this in the errors list", "Ok, guys. I have now opened a new issue at https://github.com/tensorflow/tensorflow/issues/17629.", "I was handling with this issue as well. What worked for me with tensorflow-gpu 1.6:\r\n- I downloaded the toolkit from the [archive](https://developer.nvidia.com/cuda-toolkit-archive) as 9.0 but it got installed as 9.1 (I do not know why...)\r\n- Still not found libcublas.so.9.0\r\n- Run: `sudo apt-get install cuda-libraries-9-0` as suggested at the end of the installation instructions.\r\n- The issue seems to have been solved.", "If you want to have tensorflow work with your CUDA version, you need to first uninstall it then compile it from source and specify the CUDA version while running ./configure\r\nDetailed information can be found here: [https://www.tensorflow.org/install/install_sources](url)", "I am trying this (which builds tensorflow manually)[link](http://www.python36.com/install-tensorflow141-gpu/)\r\n\r\nMight take a while longer but you can define the minor versions this way. ", "@mldm4 actually, the command `sudo apt-get install cuda` probably installed 9.1 for you because you also had that in your system. I had the same problem, and I did `sudo apt-get install cuda-9-0` to install a specific version (I had also downloaded from the archive).\r\nI think the commad you did (`sudo apt-get install cuda-libraries-9-0`) also downloads cuda 9.0.\r\nThis issue is not solved, as I mentioned at issue 17629, I know the problem is that TensorFlow expects version 9.0, while I had 9.1. The issue is to mention this more clearly in the docs and include it in the common installation problems in the bottom of the docs, or update tensorflow to accept Cuda 9.1.", "so no solution yet?", "@thread :\r\n\r\n****Please read through the posts carefully! The answer is posted.****\r\n\r\n It is your job to read the thread, and discover the solution; not simply scroll to the end. ", "@abrahamrhoffman that's rude.\r\n\r\nI just changed my batchrc from cuda-9.1 to just cuda. Then my tensorflow is able of finding the libcublas.so.9.0", "just fyi: nvidia website for downloading cuda-9.0 is actually downloading cuda-9.1. https://developer.nvidia.com/cuda-90-download-archive?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1604&target_type=deblocal", "@DanlanChen That is probably because you also have 9.1 installed. To install 9.0, in the steps to download, do `sudo apt-get install cuda-9-0` instead of `sudo apt-get install cuda`.", "@cubetastic33  I downgraded my tensorflow version and use cuda8.0, thank you anyway.", "@DanlanChen but then, I guess it is preffered to use the latest version. So, if you ever want to upgrade, you now know what to do! :smile:", "Im facing the same issue, but I am trying to run tensorflow using nvidia-docker. I have cuda-9-0 installed on the host, but when I try to run my docker container I get\r\n\r\n`ImportError: libcuda.so.1: cannot open shared object file: No such file or directory`", "@magick93 and all that turn up here!\r\n\r\nLISTEN! Anything you need is downgrade your cuda 9.1 -> cuda 9.0. That's it! Just do it (if you downloaded cuda 9.1 before that you can execute following command in your terminal):\r\n\r\n`sudo apt-get install cuda-9-0` and remove cuda 9.1 by rm -rf.\r\n\r\nBtw, don't forget to change $PATH in your `~/.bashrc` (9.1 -> 9.0).", "Hi @Oktai15 \r\n\r\n> LISTEN! Anything you need is downgrade your cuda 9.1 -> cuda 9.0. That's it! Just do it \r\n\r\nYes, I have done this - many times. \r\n\r\n```\r\nsudo apt-get install cuda-9-0\r\n[sudo] password for anton: \r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\ncuda-9-0 is already the newest version (9.0.176-1).\r\n0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\r\n```\r\n\r\n```\r\n$ ldconfig -p | grep cuda\r\n\tlibnvrtc.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc.so.9.0\r\n\tlibnvrtc.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc.so\r\n\tlibnvrtc-builtins.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc-builtins.so.9.0\r\n\tlibnvrtc-builtins.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc-builtins.so\r\n\tlibnvgraph.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvgraph.so.9.0\r\n\tlibnvgraph.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvgraph.so\r\n\tlibnvblas.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvblas.so.9.0\r\n\tlibnvblas.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvblas.so\r\n\tlibnvToolsExt.so.1 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvToolsExt.so.1\r\n\tlibnvToolsExt.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvToolsExt.so\r\n\tlibnpps.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnpps.so.9.0\r\n\tlibnpps.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnpps.so\r\n\tlibnppitc.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppitc.so.9.0\r\n\tlibnppitc.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppitc.so\r\n\tlibnppisu.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppisu.so.9.0\r\n\tlibnppisu.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppisu.so\r\n\tlibnppist.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppist.so.9.0\r\n\tlibnppist.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppist.so\r\n\tlibnppim.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppim.so.9.0\r\n\tlibnppim.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppim.so\r\n\tlibnppig.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppig.so.9.0\r\n\tlibnppig.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppig.so\r\n\tlibnppif.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppif.so.9.0\r\n\tlibnppif.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppif.so\r\n\tlibnppidei.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppidei.so.9.0\r\n\tlibnppidei.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppidei.so\r\n\tlibnppicom.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppicom.so.9.0\r\n\tlibnppicom.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppicom.so\r\n\tlibnppicc.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppicc.so.9.0\r\n\tlibnppicc.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppicc.so\r\n\tlibnppial.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppial.so.9.0\r\n\tlibnppial.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppial.so\r\n\tlibnppc.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppc.so.9.0\r\n\tlibnppc.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppc.so\r\n\tlibicudata.so.55 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libicudata.so.55\r\n\tlibcusparse.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcusparse.so.9.0\r\n\tlibcusparse.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcusparse.so\r\n\tlibcusolver.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcusolver.so.9.0\r\n\tlibcusolver.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcusolver.so\r\n\tlibcurand.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcurand.so.9.0\r\n\tlibcurand.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcurand.so\r\n\tlibcuinj64.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcuinj64.so.9.0\r\n\tlibcuinj64.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcuinj64.so\r\n\tlibcufftw.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcufftw.so.9.0\r\n\tlibcufftw.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcufftw.so\r\n\tlibcufft.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcufft.so.9.0\r\n\tlibcufft.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcufft.so\r\n\tlibcudnn.so.7 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7\r\n\tlibcudart.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0\r\n\tlibcudart.so.7.5 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudart.so.7.5\r\n\tlibcudart.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so\r\n\tlibcudart.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudart.so\r\n\tlibcuda.so.1 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n\tlibcuda.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcuda.so\r\n\tlibcublas.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcublas.so.9.0\r\n\tlibcublas.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcublas.so\r\n\tlibaccinj64.so.9.0 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libaccinj64.so.9.0\r\n\tlibaccinj64.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libaccinj64.so\r\n\tlibOpenCL.so.1 (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libOpenCL.so.1\r\n\tlibOpenCL.so (libc6,x86-64) => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libOpenCL.so\r\n\r\n```\r\n\r\nYet I cant even run this simple tensorflow script, as it results I get `ImportError: No module named tensorflow.python.client`\r\n\r\n```\r\nfrom tensorflow.python.client import device_lib\r\n\r\ndef get_available_gpus():\r\n    local_device_protos = device_lib.list_local_devices()\r\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\r\n\r\nget_available_gpus()\r\n```\r\n", "I had the same issue everybody is having here, installing 9.0 did help but then I got other issues.\r\nThen I followed [this](https://github.com/williamFalcon/tensorflow-gpu-install-ubuntu-16.04) and it finally worked for me. ", "@Oktai15 doesn't `rm -rf` delete your system? Please be more clear here, because people might try it without going into the required directory, and end up emptying their home folder.", "@magick93 your issue seems to be something else, not the CUDA version.", "I had same issue, I think I solved by some changing, this combination works for me\r\n\r\nUbuntu 16, cuda 9.0, cudnn 7.0, python 3,5, tensorflow  1.6\r\n\r\nIf you install new cuda while you still have previous version please make sure to specify the path  like this\r\n\r\n```\r\nexport PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n```", "@Abduoit your Ubuntu version doesn't really matter. The thing is, TensorFlow 1.6 expects CUDA to be version 9.0, and cuDNN to be version 7.0.4 (yes, the 0.4 _does_ matter)", "Traceback (most recent call last):\r\n  File \"utils.py\", line 15, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/sagar/miniconda2/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/sagar/miniconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/sagar/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/sagar/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/sagar/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/sagar/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\nsudo apt-get install cuda-7-0\r\nvim ~/.bashrc\r\nexport PATH=/usr/local/cuda-7.0/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda7.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\nexport PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda9.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PA\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "As @pascalwhoop mentioned, I followed the instructions in here [http://www.python36.com/install-tensorflow141-gpu/](http://www.python36.com/install-tensorflow141-gpu/) to build Tensorflow from source. \r\n\r\nWhenever it said cuda 9.0 I changed to 9.1, and cudnn 7.0 I put 7.1.2.\r\n\r\nWorked fine, so far!", "@SAGGSOC why are you installing cuda 7.0? You need CUDA 9.0 and CuDNN 7.0.4", "Everyone who's been having issues: who's up for turning this into a solid docker image that's shared with the community. Rather do a 6gb image pull once that works than DLing 5 versions of CuDNN before stuff works.. \n\nhttps://github.com/pascalwhoop/tf_openailab_gpu_docker\n\nI started a while back but stopped because of shifting project focuses. But I think it's worth pursuing. Keeps the whole trouble of finding a the right combination of 17 moving parts away from most ppl. ", "Just to clarify a few things for anyone who might stumble on this post. I have in my system installed cuda-8.0, cuda-9.0, cuda-9.1. You don't have to remove anything to make it work with tensorflow. Instead if you are missing cuda-9.0 from your system, as other have already pointed then you'll need to install it that is a prerequisite for tensorflow to work properly.\r\n\r\nIf you have cuda-9.0 installed on you system and tensorflow is complaining about `libcublas.so.9.0` again as others have said expose that during runtime through your  `LD_LIBRARY_PATH` environment variable in your `.bashrc` make it point to `/usr/local/cuda-9.0/lib64`. This should be working even for tensorflow 1.7.\r\n\r\nWhat I have tried and failed to accomplish is build from source. For some reason bazel always exits with an error. If you try to build with cuda-9.0/cuda-9.1 and cudnn7 it complains about gcc7. Using gcc5 compilation seems to be working fine but then at the end I always get an error and the build is unsuccessful. \r\n\r\nMy question is if anyone has managed to compile from source with cuda-9.1/cuda-9.0 without problems?", "This worked for me:\r\n\r\nDownload CUDA Toolkit 9.0 from NVidia previous releases section.\r\nThen:\r\n\r\n```\r\nsudo dpkg -i\u00a0cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb\r\nsudo apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub \r\nsudo apt-get update\r\nsudo apt-get install cuda-9.0\r\n```\r\n\r\nNotice 9.0 at the last line above.\r\n\r\n`export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64\r\n`\r\n\r\nMy setup\r\nTensorflow 1.7\r\ncuDNN 7.1.2\r\nUbuntu 16.04\r\n", "etemiz, \r\nI've got the same setup and it works for me too. \r\n\r\nThank you for your post :)\r\n", "I had teh same situation. I had cuda 9.1, and tensorflow would not find libraries for cuda 9.0.\r\n\r\nI have installed cuda 9.0 with command: sudo apt-get install cuda-libraries-9-0\r\n\r\nThat solved my problem.", "With cuda 8 and 9.0 installed, setting `LD_LIBRARY_PATH` in `.bashrc` and `.profile` not work. So, I set \r\n`LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64` in pycharm Environment variable field and it works. ", "the above worked for installing on Ubuntu Server 17. namely,\r\n\r\n- installing cuda-9.0 (NOT 9.1)\r\n- cuDNN v7.1.2 (Mar 21, 2018) for CUDA 9.0\r\n- everything else according to the official tf installation instructions\r\n\r\nmuch easier than compiling.", "be careful conda users. i hit the same problem and was scratching my head for two days, until finally i discovered that local copy of libcudnn.so was used by conda, under:\r\n/miniconda3/lib/libcudnn.so which pointed to libcudnn.so.7 which pointed to libcudnn.so.7.0.5\r\ni don't remember who and how placed it there but pretty much it overloaded the system default libcudnn.so.7.1.2 !!!\r\n\r\nonce removed, everything works like a charm:\r\ntensorflow 1.7 or 1.8-nightly, cuda-9.1, cudnn-7.1.2 on ubuntu 16.04\r\n", "Somehow, I solved this by installing:\r\ncuda 9.1 (from package manager), \r\ncudnn 7.1 for 9.1\r\nand from anaconda:\r\nby using this default command 'conda install -c anaconda tensorflow-gpu'\r\ncudatoolkit 9.0, \r\ntensorflow 1.7,\r\ntensorflow-gpu 1.7\r\n\r\nI used Antergos linux, GTX 1060 in my PC. It worked as well in my notebook (Xubuntu 18.04, GT 840m). In my notebook i used :\r\ncuda 9.1 (from nvidia ppa), cudnn 7.1 for 9.1 (from nvidia web), and the rest was the same", "Thanks @Suananda! It works like magic.", "If you have old version of CUDA, the library link may point to the old library even you install the newer CUDA especially if you install it manually. Try delete your old installation, and then sudo ldconfig to update the dynamic links.", "No solution yet!?", "For anyone that might stumble on this I have released a community wheel of latest tensorflow 1.8.0-rc1 built with cuda 9.1. You can find it [here](https://github.com/yaroslavvb/tensorflow-community-wheels/issues/64)!", "I have find the reason is ldconf,  ldconfig is a dynamic link library management command whose purpose is to allow the dynamic link library to be usedby the system.\r\n\r\nThe default ldconf only search /lib and /usr/lib, as well as the library file under the directory listed in the configuration file /etc/ld. so. conf.\r\n\r\nso all of this is caused by the  dynamic library of CUDA in the installed CUDA path such as : /path/cuda-9.0/lib64  or /path/cuda-9.0/lib. (for eample my CUDA is installed in /usr/local/cuda-9.0)\r\n\r\n1. if you install the CUDA manual,  then after install, you should add the  path of cuda/lib64 to /etc/ld.so.conf file \r\n  `sudo echo \"/usr/local/cuda-9.0/lib64/\" >> /etc/ld.so.conf`\r\nthen \r\n  `sudo ldconfig`\r\n\r\n   of course , you can add the path manual, like:\r\n   `vim /etc/ld.so.conf`\r\n   then add the path  '/usr/local/cuda-9.0/lib64' at the end. \r\n   `sudo ldconfig`\r\n   after the operation, reopen the ipython or pycharm , \r\n   import tensorflow as  tf \r\n   wow, you will enjoy it!\r\n\r\n2. if you install the CUDA by command such as 'dpkg -i cuda-repo-ubuntu1604_9.0.176-1_amd64.deb' or others, it may add the cuda lib path to the /etc/ld.so.conf  automatically . but to be on the safe side, check the /etc/ld.so.conf and see if the path add to it .\r\n\r\n", "@NYcleaner \r\nOn Ubuntu, there is a `/etc/ld.so.conf.d/cuda-9-1.conf` file containing : \r\n`/usr/local/cuda-9.1/targets/x86_64-linux/lib`\r\n\r\nIs this enough or do I need to add the directory `/usr/local/cuda-9.1/lib64/` to it ?", "@sebma \r\nyou  should add  the `/usr/local/cuda-9.1/lib64/` path  to it\uff0c the ***.so files  are in the  lib64", "```\r\nsudo bash -c \"echo /usr/local/cuda/lib64/ >/etc/ld.so.conf.d/cuda.conf\"\r\nsudo ldconfig\r\n```\r\nfrom https://gist.github.com/zhanwenchen/e520767a409325d9961072f666815bb8", "@mashu Well the other option is that the community provides pre-built [wheels](https://github.com/yaroslavvb/tensorflow-community-wheels). If you read 2-3 threads above you'll also see that mentioned again.", "@Suananda  Thanks, it works for me in a tensorflow conda environment. \r\nSuggest the official guys to modify the installation guide \"https://www.tensorflow.org/install/install_linux#InstallingAnaconda\", step 4 of Anaconda installing, from \"pip install --ignore-installed --upgrade tfBinaryURL\" to \"conda install -c anaconda tensorflow-gpu\"", "Softlink seems not solve this issue:\r\n\r\n```\r\n\u279c  cuda ls lib64 \r\nlibaccinj64.so                libcufftw_static.a     libnppial.so.9.2      libnppig_static.a    libnvblas.so.9.2\r\nlibaccinj64.so.9.2            libcuinj64.so          libnppial.so.9.2.88   libnppim.so          libnvblas.so.9.2.88\r\nlibaccinj64.so.9.2.88         libcuinj64.so.9.2      libnppial_static.a    libnppim.so.9.2      libnvgraph.so\r\nlibcublas_device.a            libcuinj64.so.9.2.88   libnppicc.so          libnppim.so.9.2.88   libnvgraph.so.9.2\r\nlibcublas.so                  libculibos.a           libnppicc.so.9.2      libnppim_static.a    libnvgraph.so.9.2.88\r\nlibcublas.so.9.0              libcurand.so           libnppicc.so.9.2.88   libnppist.so         libnvgraph_static.a\r\nlibcublas.so.9.2              libcurand.so.9.2       libnppicc_static.a    libnppist.so.9.2     libnvrtc-builtins.so\r\nlibcublas.so.9.2.88           libcurand.so.9.2.88    libnppicom.so         libnppist.so.9.2.88  libnvrtc-builtins.so.9.2\r\nlibcublas_static.a            libcurand_static.a     libnppicom.so.9.2     libnppist_static.a   libnvrtc-builtins.so.9.2.88\r\nlibcudadevrt.a                libcusolver.so         libnppicom.so.9.2.88  libnppisu.so         libnvrtc.so\r\nlibcudart.so                  libcusolver.so.9.2     libnppicom_static.a   libnppisu.so.9.2     libnvrtc.so.9.2\r\nlibcudart.so.9.2              libcusolver.so.9.2.88  libnppidei.so         libnppisu.so.9.2.88  libnvrtc.so.9.2.88\r\nlibcudart.so.9.2.88           libcusolver_static.a   libnppidei.so.9.2     libnppisu_static.a   libnvToolsExt.so\r\nlibcudart_static.a            libcusparse.so         libnppidei.so.9.2.88  libnppitc.so         libnvToolsExt.so.1\r\nlibcufft.so                   libcusparse.so.9.2     libnppidei_static.a   libnppitc.so.9.2     libnvToolsExt.so.1.0.0\r\nlibcufft.so.9.2               libcusparse.so.9.2.88  libnppif.so           libnppitc.so.9.2.88  libOpenCL.so\r\nlibcufft.so.9.2.88            libcusparse_static.a   libnppif.so.9.2       libnppitc_static.a   libOpenCL.so.1\r\nlibcufft_static.a             libnppc.so             libnppif.so.9.2.88    libnpps.so           libOpenCL.so.1.0\r\nlibcufft_static_nocallback.a  libnppc.so.9.2         libnppif_static.a     libnpps.so.9.2       libOpenCL.so.1.0.0\r\nlibcufftw.so                  libnppc.so.9.2.88      libnppig.so           libnpps.so.9.2.88    stubs\r\nlibcufftw.so.9.2              libnppc_static.a       libnppig.so.9.2       libnpps_static.a\r\nlibcufftw.so.9.2.88           libnppial.so           libnppig.so.9.2.88    libnvblas.so\r\n```\r\n\r\nStill got:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n```", "when I run my code on the linux environment directly, everything is OK. But when I run on the local pycharm through the remote interpreter, I encounter the problem: `ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory.        \"Failed to load the native TensorFlow runtime.\"`", "export PATH=${PATH}:/usr/local/cuda-9.0/bin\r\nexport CUDA_HOME=${CUDA_HOME}:/usr/local/cuda:/usr/local/cuda-9.0\r\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda-9.0/lib64\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64\r\n\r\nif use pycharm - add it to interpreter", "what about f** I just only want to use tensorflow1.8 and cuda9.1?", "@dongzhuoyao So what's the problem? look at my comment 6 threads above and you'll find your solution there!", "I guess the problem has cropped up again with `cuda 9.2` and `tensorflow-gpu 1.8`. In a virtualenv, I get:\r\n```\r\n>>> import tensorflow as tf\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n```\r\nMy `/usr/local/cuda-9.2/lib64` folder has the following libcublas:\r\n```\r\n$ ls /usr/local/cuda-9.2/lib64 | grep libcublas\r\nlibcublas_device.a\r\nlibcublas.so\r\nlibcublas.so.9.2\r\nlibcublas.so.9.2.113\r\nlibcublas.so.9.2.88\r\nlibcublas_static.a\r\n```\r\nVersions:\r\n```\r\nUbuntu 16.04\r\ncuda 9.2\r\ncudnn 7.1.4\r\ntensorflow-gpu 1.8.0\r\n```\r\nWe'll need an update to tensorflow-gpu to use cuda 9.2.\r\n\r\nAlso, if I were to downgrade to cuda 9.0, would I have to first remove cuda 9.2 or just install 9.0 straight away? Would I have conflicting installations?", "@mebble Here you go: [link](https://we.tl/b7yAL9rYrO). That's tf 1.8 wheel for cuda 9.2.\r\n\r\n> if I were to downgrade to cuda 9.0, would I have to first remove cuda 9.2\r\n\r\nDon't downgrade. Install whatever other version you want they'll get installed at `/usr/local/cuda-x.x`", "Thanks! I forgot to mention that im using `python 3.5.2` and pip `10.0.1`. I think the wheel is for python 3.6 so the install doesn't work. Do you have one for 3.5 as well?", "@kirk86 after installing the whl you gave it throws a similar error for libmpi.so.40\r\nI'm on CentOS and K80 GPU, cuda 9.2 and cudnn v7.1", "### Suggestion\r\nAs far as I know you can have sub-packages xxx,yyy,zzz etc.. and install them as follow\r\n```\r\npip install mainpackage[xxx]\r\n```\r\nThis way different co-existing back-ends can be provided. Tensorflow can be build with different options, so at least a couple of cuda-toolkit builds could be provided this way.\r\n\r\nThe whole point of package is to save time of building, but package build for very specific set of libraries that installs fine, but does not work is counter-productive. It would be better off not to have such package in the first place.", "@mebble just make a conda virtual environment for python 3.6. Make sure that you also have installed on your system openmpi.", "@pavan-08 Install openmpi on your system also nccl 2.x whatever is the latest from nvidia. I've compiled tf with most of the packages and libraries, so it can be used hdfs, kafta, aws, etc. That's why is asking libmpi.so because it's from openmpi library.", "With cuda 9.2 and tensorflow-gpu 1.8 I cannot build tensorflow\r\n\r\ndeclared output 'external/local_config_cuda/cuda/cuda/lib/libcudnn.so.7' is a dangling symbolic link\r\n\r\n\r\nThe symlink exists\r\n\r\nVersions:\r\n\r\nUbuntu 17.10\r\ncuda 9.2\r\ncudnn 7.1.4\r\ntensorflow-gpu 1.8.0\r\n\r\n", "This works for me (tensorflow-gpu==1.8.0 and cuda version is 9.0, install in anaconda)\r\n```\r\nexport LD_LIBRARY_PATH=LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64/\r\n```\r\nsuggestion from: https://stackoverflow.com/questions/48428415/importerror-libcublas-so-9-0-cannot-open-shared-object-file", "@Jackiexiao Yea, that's what I was going to say. I'm using a workstation, so for me all I needed to do was use CUDA 9.0 instead of 9.2 (since multiple versions of CUDA were installed)", "This worked for me:\r\nBecause Tensorflow > 1.4  requires CUDA 9, I uninstalled all CUDA versions : \r\n1. sudo rm -rf /usr/local/cuda*\r\n2. dpkg -l | grep cuda- | awk '{print $2}' | xargs -n1 sudo dpkg --purge\r\n\r\nInstall excatly CUDA 9.0 and CuDNN 7.0.x and reboot. ", "Please refer [combinations of CUDA, CuDNN and Tensorflow](https://www.tensorflow.org/install/install_sources#tested_source_configurations).\r\n\r\nThis error happens majorly due to incorrect version combinations of Nvidia-driver, CUDA, CuDNN and Tensorflow-gpu\r\n![image](https://user-images.githubusercontent.com/15031475/44293189-4d9a5c00-a2a7-11e8-9e5f-13463cb4f60c.png)\r\n", "Thank u\uff01\n\n\n| |\n\u6768\u5eb7\n|\n|\n\u90ae\u7bb1\uff1a18375320027@163.com\n|\n\n\u7b7e\u540d\u7531 \u7f51\u6613\u90ae\u7bb1\u5927\u5e08 \u5b9a\u5236\n\nOn 08/18/2018 08:00, Dhruv Srivastava wrote:\n\nPlease refer combinations of CUDA and CuDNN.\n\nThis error happens majorly due to incorrect version combinations of Nvidia-driver, CUDA, CuDNN and Tensorflow-gpu\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.", "@dhruvhacks Good. I think people should install the right version of tensorflow-gpu with the right version cuda.", "I am on Ubuntu 18.08, and had Cuda 9.0 installed at:\r\n\r\n```\r\n/usr/local/cuda-9.0\r\n```\r\n\r\nI decided to look for the `libcublas.so.9.0`:\r\n\r\n```\r\n# build search index\r\nupdatedb\r\n# find the \"missing\" file\r\nlocate libcublas.so.9.0\r\n```\r\n\r\nThat told me the file was in `/usr/local/cuda-9.0/lib64/stubs/libcublas.so.9.0`, which helped me realize I had two problems: In `~/.bash_profile` I had set: `export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64/`, but my file was in the `stubs` directory, so I needed to use:\r\n\r\n```\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64/stubs/\r\n```\r\n\r\nFinally, `libcublas.so.9.0` was owned by root, so I changed the permissions and the owner of the file:\r\n\r\n```\r\nsudo chown -R MY_USER:MY_GROUP /usr/local/cuda-9.0/\r\nsudo chmod -R 777 /usr/local/cuda-9.0/\r\n```\r\n\r\nAnd all was well!", "> With cuda 8 and 9.0 installed, setting `LD_LIBRARY_PATH` in `.bashrc` and `.profile` not work. So, I set\r\n> `LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64` in pycharm Environment variable field and it works.\r\n\r\nExactly, Thanks so much for solving the problem, I have spent days to crack this up. really appreciate mate", "I encounter this problem as well when I use **PyCharm** to debug my code at remote server(aws deep learning ami). The cuda version is 9.0 and the cudnn is 7.1.4 and tensorflow is 1.10.0(for gpu). I can import tensorflow normally by terminal connected to my server but fail to import tensorflow in the python console of PyCharm. The problem occurs at this time. Therefore, I cannot debug with PyCharm...\r\n**My solution is**: input this line in terminal after connecting to your server\r\n```\r\nsudo ldconfig /usr/local/cuda/lib64\r\n```\r\nAnd I referenced [thisLink](https://www.cnblogs.com/xiaojianliu/p/9312098.html#_label2)", "@NYcleaner Thanks a lot! It helps! and i found just use symbolic link also works. \r\nsudo echo \"/usr/local/cuda/lib64/\" >> /etc/ld.so.conf\r\nsudo ldconfig", "This is still an issue with Cuda 9.1 and Tensorflow 1.11", "cuda 10.0  tensorflow 1.11", "Even better solution - remove tensorflow, install pytorch\r\nWhy tensorflow can brake so easily?\r\nWhy can't I just install it and run? \r\nWhy is it so easy with pytorch and unpredictable with tf? Wtf is wrong with it?", "> Why not just install cuda-9-0?\r\n> \r\n> * Go here: https://developer.nvidia.com/cuda-90-download-archive\r\n> * Then, for me: Download deb (network)\r\n> \r\n> ```\r\n> sudo dpkg -i\u00a0cuda-repo-ubuntu1604_9.0.176-1_amd64.deb\r\n> sudo apt-key adv --fetch-keys \\\r\n>      http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\r\n> sudo apt-get update\r\n> sudo apt-get install cuda-9-0\r\n> ```\r\n\r\nUnder ubuntu 18.04 cuda 9-0 is not (officially) available. It works, though.\r\n", "Hi all, \r\nI use PyTorch as much as possible, but for a particular project where i need to export a (Keras) model to tensorflowjs, I'm forced to use tf. The only solution which has worked well for me has been to build from source, after installing CUDA from the Ubuntu multiverse, as described here:\r\n\r\nhttps://medium.com/@asmello/how-to-install-tensorflow-cuda-9-1-into-ubuntu-18-04-b645e769f01d\r\n\r\nBonne chance!", "[CUDA 9 isn't supported on Ubuntu 18.04 according to official website](https://developer.nvidia.com/cuda-90-download-archive?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu)\r\n\r\n[Tensorflow doesn't support anything above CUDA 9 according to the official website](https://www.tensorflow.org/install/gpu)\r\n\r\nCan we get an official CUDA 10 support for tensorflow soon?", "> Even better solution - remove tensorflow, install pytorch\r\n> Why tensorflow can brake so easily?\r\n> Why can't I just install it and run?\r\n> Why is it so easy with pytorch and unpredictable with tf? Wtf is wrong with it?\r\n\r\nStupid solution", "I have installed Cuda 10 and the latest version of Tensor Flow, but I have received `ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory`, I have made the downgrade and I still receiving this message...", "> I have installed Cuda 10 and the latest version of Tensor Flow, but I have received `ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory`, I have made the downgrade and I still receiving this message...\r\n\r\nCould you tell us the OS version and the exact command you issued to \"downgrade\"?", "I tried Deepin 15.8 and Mint 19.1, I'm going to test Windows 10 later. The command `pip install --upgrade tensorflow-gpu==version`, I tried the [master](https://pypi.org/project/tensorflow-gpu/#history) versions: 1.9.0, 1.10.1, 1.11.0 and 1.12.0. The [cuda](https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1804) version and the drivers:\r\n\r\n![image](https://user-images.githubusercontent.com/19755014/50491207-51c64b00-09f8-11e9-8755-086a0f64eac3.png)", "@Sphinxs, in my opinion you have two ways: a) to recompile the python wheel locally so it points to the installed cuda version or b) [install cuda-9.0](https://github.com/tensorflow/tensorflow/issues/15604#issuecomment-439601784) as I did for my 18.04 installation and it's still working after months.", "> Even better solution - remove tensorflow, install pytorch\r\n> Why tensorflow can brake so easily?\r\n> Why can't I just install it and run?\r\n> Why is it so easy with pytorch and unpredictable with tf? Wtf is wrong with it?\r\n\r\nthe man has got a point. TF CAN be compiled to work with more modern CUDA versions... why not just offer that to most people with a `tensorflow-gpu-cuda110` package or something temporary until the library can figure out the underlying cuda version dynamically?", "> Even better solution - remove tensorflow, install pytorch\r\n> Why tensorflow can brake so easily?\r\n> Why can't I just install it and run?\r\n> Why is it so easy with pytorch and unpredictable with tf? Wtf is wrong with it?\r\n\r\nI am sick of it!!!!", "> I am sick of it!!!!\r\n\r\nThis comment does not add anything to the issue. Also no one forces anyone to use TF, but If you really need to and can't get pip packages working, why not to build it yourself? That's what I ended up doing at some point. I saw there are also community supported builds here\r\nhttps://github.com/tensorflow/tensorflow just scroll down.\r\nMaybe build for your GPU and contribute?\r\n", "> > Even better solution - remove tensorflow, install pytorch\r\n> > Why tensorflow can brake so easily?\r\n> > Why can't I just install it and run?\r\n> > Why is it so easy with pytorch and unpredictable with tf? Wtf is wrong with it?\r\n> \r\n> I am sick of it!!!!\r\n\r\nThis is an issue page. Please, state your issues so they can be fixed, or, even better, help fixing them.\r\n", "@raphaunix may I ask where this chart comes from?", "I got Tensorflow 1.11.0 working by running the following commands:\r\n\r\n```\r\nconda create -n tf python=2\r\nconda activate tf\r\npip install tensorflow-gpu==1.11\r\nconda install cudatoolkit==9.0\r\n```\r\n", "I have find the reason is ldconf, ldconfig is a dynamic link library management command whose purpose is to allow the dynamic link library to be usedby the system.\r\n\r\nThe default ldconf only search /lib and /usr/lib, as well as the library file under the directory listed in the configuration file /etc/ld. so. conf.\r\n\r\nso all of this is caused by the dynamic library of CUDA in the installed CUDA path such as : /path/cuda-9.0/lib64 or /path/cuda-9.0/lib. (for example my CUDA is installed in /usr/local/cuda-9.0)\r\n\r\n1.if you install the CUDA manual, then after install, you should add the path of cuda/lib64 to /etc/ld.so.conf file\r\n`sudo echo \"/usr/local/cuda-9.0/lib64/\" >> /etc/ld.so.conf\r\n`\r\nthen\r\n`sudo ldconfig\r\n`\r\nof course , you can add the path manual, like:\r\nvim /etc/ld.so.conf\r\nthen add the path '/usr/local/cuda-9.0' at the end.\r\nthen update it\r\n`sudo ldconfig\r\n`\r\nafter the operation, reopen the ipython or pycharm ,\r\n`import tensorflow as tf\r\n`\r\nwow, you will enjoy it!\r\n\r\nif you install the CUDA by command such as 'dpkg -i cuda-repo-ubuntu1604_9.0.176-1_amd64.deb' or others, it may add the cuda lib path to the /etc/ld.so.conf automatically . but to be on the safe side, check the /etc/ld.so.conf and see if the path add to it .", "@jabalazs Instead of `conda install cudatoolkit==9.0` I tried `pip install cudatoolkit==9.0` but this didn't work, do you know why?", "I found that my runtime on colab was not using GPU that's how come I got my error", "> Even better solution - remove tensorflow, install pytorch\r\n> Why tensorflow can brake so easily?\r\n> Why can't I just install it and run?\r\n> Why is it so easy with pytorch and unpredictable with tf? Wtf is wrong with it?\r\n\r\nGood solution @dodler ! but I think you are missing the commands to make this great advice easy to follow through xD.\r\n\r\n```\r\npip uninstall tensorflow\r\npip install torch\r\n```\r\n\r\nHappy coding!", "I faced this same error trying to use Thundersvm to speedup NuSVR with GPUs on Google  Colab. \r\nInstalling Cuda 9.0 solved the problem to me.\r\nJust follow:\r\n\r\n`!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb`\r\n\r\n`!ls  # Check if required cuda 9.0 amd64-deb file is downloaded`\r\n\r\n`!dpkg -i cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb`\r\n\r\n`!ls /var/cuda-repo-9-0-local | grep .pub`\r\n\r\n`!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub`\r\n\r\n`!apt-get update`\r\n\r\n`!sudo apt-get install cuda-9.0`\r\n", "    conda install cudatoolkit=8.0\r\n\r\nsolves my problem.", "`conda install -c anaconda cudatoolkit=9.0 cudnn=7` solved it for me!", "> With cuda 8 and 9.0 installed, setting `LD_LIBRARY_PATH` in `.bashrc` and `.profile` not work. So, I set\r\n> `LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64` in pycharm Environment variable field and it works.\r\n\r\nThis solution worked for me. Thanks for this solution. ", "My TensorFlow and TensorFlow-gpu are 1.12.0 and CUDA 11.0 and I am facing this issue -\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/megh/anaconda3/envs/bne/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/megh/anaconda3/envs/bne/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/megh/anaconda3/envs/bne/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/megh/anaconda3/envs/bne/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/megh/anaconda3/envs/bne/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"bne.py\", line 21, in <module>\r\n    from encoder import *\r\n  File \"/home/megh/projects/entity-norm/BNE/encoder.py\", line 2, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/megh/anaconda3/envs/bne/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/megh/anaconda3/envs/bne/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/megh/anaconda3/envs/bne/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/megh/anaconda3/envs/bne/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/megh/anaconda3/envs/bne/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/megh/anaconda3/envs/bne/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/megh/anaconda3/envs/bne/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/megh/anaconda3/envs/bne/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\nSomeone please help. I have been struggling for days! :'(", "@meghbhalerao I guess your problem's cause is tensorflow and cuda version mismatch. Check [compatible versions here](https://stackoverflow.com/a/50622526/4379029). Have you tried to installed different cuda or tensorflow versions?", "Trying to install thundersvm on Colab instance, hit this error:\r\n```\r\npip install thundersvm\r\n\r\nCollecting thundersvm\r\n  Downloading thundersvm-0.3.12-py3-none-any.whl (507 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 507 kB 7.5 MB/s \r\nRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from thundersvm) (1.19.5)\r\nRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from thundersvm) (1.4.1)\r\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from thundersvm) (0.22.2.post1)\r\nRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->thundersvm) (1.1.0)\r\nInstalling collected packages: thundersvm\r\nSuccessfully installed thundersvm-0.3.12\r\n\r\n#python wheel available in working directory\r\nls\r\nsample_data/  thundersvm-cpu-0.2.0-py3-none-linux_x86_64.whl\r\n\r\npip install thundersvm-cu90-0.2.0-py3-none-linux_x86_64.whl\r\nWARNING: Requirement 'thundersvm-cu90-0.2.0-py3-none-linux_x86_64.whl' looks like a filename, but the file does not exist\r\nProcessing ./thundersvm-cu90-0.2.0-py3-none-linux_x86_64.whl\r\nERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/content/thundersvm-cu90-0.2.0-py3-none-linux_x86_64.whl'\r\n\r\n#full filepath\r\npwd\r\n'/content'\r\npip install content/thundersvm-cu90-0.2.0-py3-none-linux_x86_64.whl\r\nWARNING: Requirement 'content/thundersvm-cu90-0.2.0-py3-none-linux_x86_64.whl' looks like a filename, but the file does not exist\r\nProcessing ./content/thundersvm-cu90-0.2.0-py3-none-linux_x86_64.whl\r\nERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/content/content/thundersvm-cu90-0.2.0-py3-none-linux_x86_64.whl'\r\n```"]}, {"number": 15603, "title": "[CMake] Remove invalid python modules", "body": "Split from #15368\r\n@mrry Friendly ping.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "The last CI build failed due to newly included `tensorflow/contrib/lite/python` importing `tensorflow/contrib/lite/toco/python`. @drpngx Please re-run the tests.", "Jenkins, test this please.", "How is this import supposed to be resolved?\r\n`ImportError: No module named 'tensorflow.contrib.lite.toco.python.tensorflow_wrap_toco'`", "You need a dependency to `//tensorflow/contrib/lite/toco/python:tensorflow_wrap_toco`. I suggest not adding them just yet, and adding them in a separate PR.", "(The `_wrap` library, as its name implies, is a SWIG wrapped library. It links against protobuf, which gets statically linked, so adding it is probably unsafe to add it.)", "Aren't these two lines supposed to include `python:tensorflow_wrap_toco`?\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/BUILD#L60\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/python/BUILD#L18", "Yes, but loading that library runtime is inherently unsafe, particularly on MacOS. I'd do that in another PR.", "Okay, I have commented them out and left a TODO.", "Jenkins, test this please.\r\n\r\nThanks!", "The dreaded `ERROR: Error fetching remote repo 'origin'` again.\r\n\r\nJenkins, test this please."]}, {"number": 15602, "title": "[CMake] Test existence of python entries", "body": "Split from #15166\r\n@mrry Friendly ping,", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "@Androbin could you pull rebase and push again?", "Jenkins, test this please.", "`file_block_cache_test` is flaky, ignoring.", "Jenkins, test this please.\n\nYeah, it looks like there is some problem with Windows cmake. @yifeif might\nknow something about it\n\nOn Fri, Dec 29, 2017, 4:20 AM Robin Richtsfeld <notifications@github.com>\nwrote:\n\n> @drpngx <https://github.com/drpngx> Seems like the \"Sanity Checks\" and\n> the \"Windows CMake Tests\" are stuck.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15602#issuecomment-354439130>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbUWoFu3jhwmUqeMecNkgoLoqXx3kks5tFNkngaJpZM4RLq2d>\n> .\n>\n", "Jenkins, test this please.", "Jenkins, test this please.", "All tests pass (including sanity), ignoring `ci.tensorflow.org` status."]}, {"number": 15601, "title": "Optimize FusedBatchNorm (and fix a bug).", "body": "I discovered experimentally that `cudnn` computations can be performed in place. Therefore there is no need to allocate two temporary tensors in `FusedBatchNorm` for GPU and data format NHWC. One is enough. This lowers memory consumption, and hence increases the maximum possible batch size.\r\n\r\nThis might seem risky (because NVIDIA doesn't mention the property), but in fact the current implementation already uses it: by doing forward_input_or_allocate_output in `FusedBatchNormOp`.\r\nIf data format is NCHW, and the input is forwarded, then `cudnn` would be forced\r\nto do the computation in place (see line 247). This is how I discovered that the whole approach works:\r\nI was trying to see if forwarding the input is a bug or not.\r\n\r\nI added several tests to ensure that the change is correct.\r\n\r\nWhile doing this, I discovered that ops_testutil does not properly synchronize at the end.\r\nThe reason seems to be the call `context_->eigen_gpu_device().synchronize()`. Somehow\r\nit does nothing. I think the problem is that Eigen is not compiled with the flag `EIGEN_CUDACC`.\r\nSo I changed it to `GPUUtil::Sync(device_.get())`.", "comments": ["Can one of the admins verify this patch?", "/CC @zheng-xq ", "@codrut3, as in-place optimization is potentially unsafe (see discussions here: https://github.com/tensorflow/tensorflow/pull/15612 ), maybe just keep the \"fix a bug\" part of the PR? Thanks!", "@zhangyaobit , I think it's better to close this PR and open a separate one for the bug fix. I am afraid it would otherwise get committed with the wrong message and create confusion.", "Sure, feel free top open a new one and reference this one. Please CC @zhangyaobit on it. Thanks!"]}, {"number": 15600, "title": "Exclude tf_stream_executor.cmake for CPU only", "body": "Bug introduced in #15099\r\nFixes #3996", "comments": ["Can one of the admins verify this patch?", "@mrry I think things went like this:\r\n* `tf_stream_executor.cmake` was always enabled\r\n* `tf_stream_executor.cmake` was commented out\r\n* `tf_stream_executor.cmake` was enabled on GPU\r\n* Nobody bothered cleaning the comments", "Jenkins, test this please."]}, {"number": 15599, "title": "Freeze pb model will not remove is_training flag of bn attr and it is moreover still set as true", "body": "Using /tensorflow/python/framework/graph_util_impl.py # **convert_variables_to_constants** to freeze graph to pb model will not remove **is_training** flag of **batch_normalization** as well as **fused batch_normalization** ops (tf.layers.batch_normalziation) in the inference graph.  Moreover, the **is_training** flag is still set as true. This issue will not cause any errors to use the pb model on Android. However, it will cause converting errors to tflite model by toco. Is it possible to optimize this conversion tool to resolve the problem in further versions. \r\n\r\n_Following is the node output of the pb model during loading:\r\nname: \"convolution_layer/block_layer1/batch_normalization/FusedBatchNorm\"\r\nop: \"FusedBatchNorm\"\r\ninput: \"convolution_layer/block_layer1/conv2d/Conv2D\"\r\ninput: \"batch_normalization/gamma/read\"\r\ninput: \"batch_normalization/beta/read\"\r\ninput: \"convolution_layer/block_layer1/batch_normalization/Const\"\r\ninput: \"convolution_layer/block_layer1/batch_normalization/Const_1\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"data_format\"\r\n  value {\r\n    s: \"NHWC\"\r\n  }\r\n}\r\nattr {\r\n  key: \"epsilon\"\r\n  value {\r\n    f: 0.0010000000475\r\n  }\r\n}\r\n**attr {\r\n  key: \"is_training\"\r\n  value {\r\n    b: true\r\n  }**\r\n}_", "comments": ["Can one of the admins verify this patch?", "Can you pull rebase and push again? It's hard to tell what you're trying to merge.", "Could you pull rebase and push again?", "Sorry, you have to send this against master, not our release branch."]}, {"number": 15598, "title": "R1.4", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "Not sure what this does, please resubmit with a well-formed PR."]}, {"number": 15597, "title": "Adding installation dependencies for python3.6 based on failures from\u2026", "body": "\u2026 the release job.", "comments": ["Testing: http://ci.tensorflow.org/view/Release/job/test_36_fix/", "Shouldn't we link against editline or linenoise instead?"]}, {"number": 15596, "title": "Update tf-learn reference to TF Estimators", "body": "", "comments": ["Thanks, @terrytangyuan !"]}, {"number": 15595, "title": "Issue #15572, support for unspecified spatial dimensions in layers.Conv3d_transposed", "body": "Patch to 3d transposed convolution to use the same code for adding biases that regular 3d conv uses.  This allows transposed convolution to add biases if the spatial dimensions are not specified and the dimension ordering is channel last.  The case with channel first ordering still requires spatial dimensions be specified for both conv3d and conv3d_transposed because add_bias doesn't support 5d dimension ordering specifications.", "comments": ["Can one of the admins verify this patch?", "Added tests for 2D and 3D transposed conv with unspecified spatial dimensions.", "Could you fix the sanity checks?\r\n\r\n```\r\nFAIL: Found 8 non-whitelited pylint errors:\r\ntensorflow/python/layers/convolutional_test.py:612: [E0001(syntax-error), ] inconsistent use of tabs and spaces in indentation\r\ntensorflow/python/layers/convolutional.py:1625: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 6\r\ntensorflow/python/layers/convolutional.py:1626: [W0311(bad-indentation), ] Bad indentation. Found 12 spaces, expected 8\r\ntensorflow/python/layers/convolutional.py:1627: [W0311(bad-indentation), ] Bad indentation. Found 12 spaces, expected 8\r\ntensorflow/python/layers/convolutional.py:1631: [W0311(bad-indentation), ] Bad indentation. Found 12 spaces, expected 8\r\ntensorflow/python/layers/convolutional.py:1632: [W0311(bad-indentation), ] Bad indentation. Found 12 spaces, expected 8\r\ntensorflow/python/layers/convolutional.py:1633: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 6\r\ntensorflow/python/layers/convolutional.py:1634: [W0311(bad-indentation), ] Bad indentation. Found 12 spaces, expected 8\r\n```", "I think that's got it.  I don't think I've ever seen different numbers of spaces used for indentation before.  That appears to be contrary to Google's style guide... maybe a good idea to update the TF CONTRIBUTING.md?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@robb-brown any progress on fixing the formatting errors?", "@rmlarsen : https://github.com/tensorflow/tensorflow/pull/15595/commits/ce0b0eeb7c352534361bdeccce2c46cde4788673\r\n\r\nAs far as I can tell my push a month ago fixed everything.  Your sanity checks appear to show green.", "Please fix the tests. In TF code, you cannot import `from tensorflow`. \r\n\r\n```\r\nRunning test tensorflow/python/layers_convolutional_test on GPU 7\r\nTraceback (most recent call last):\r\n  File \"/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/python/layers_convolutional_test.runfiles/org_tensorflow/tensorflow/python/layers/convolutional_test.py\", line 33, in <module>\r\n    from tensorflow import placeholder, float32\r\nImportError: cannot import name 'placeholder'\r\n```", "Also please pull rebase and push again.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@robb-brown Any luck with this? Could you pull rebase and push again?", "It has been 28 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 15594, "title": "MKL: Adding MKL-DNN support for LRN op", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 15593, "title": "Branch 179953488", "body": "push", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@martinwicke looks like the PR we pulled in failed the cla check even though the original PR passed. Do you know if there is any way around this?", "Hmm... I fear this will be a persistent problem. Let's talk to CLABot team about how to fix this. Until then, the way around this is admin privileges.", "@tensorflow-jenkins test this please"]}, {"number": 15592, "title": "Refactor methods for path calculation", "body": "@jart I really appreciate your advice. I've rewritten #15315 to be more \"boring\":\r\n* `get_grandparent(path, degree)` -> a files grandparent of the given degree\r\n* `get_abs_data_path(path, depth, frame=0)` -> path relative to a file upwards the callstack\r\n* `get_data_files_path(frame=0)` -> no change in behavior, but now shared with other methods\r\n\r\nThey should be about boring enough to be useful and to be shared.\r\n\r\n* This should resolve the issues with `get_data_files_path` by making it less brittle...\r\n* And I think `get_abs_data_path` is how `get_path_to_datafile` should really look like?\r\n\r\nFurther thoughts on this are appreciated!", "comments": ["Can one of the admins verify this patch?", "In #15315 I was offering advice on #15166. We're not able to introduce an API here, for the reasons mentioned in #15315. The test code that's being written, shouldn't need libraries and API changes.", "@jart I agree, it shouldn't. The documentation of `get_path_to_datafile` says:\r\n> The path is relative to `tensorflow/`\r\n\r\nbut it's not. Instead, it is relative to the file calling this method.\r\nIf it were to do what it says it does, I would leave it as is, but unfortunately, it doesn't.", "@martinwicke What do you suggest?\r\n* Change the documentation of `get_path_to_datafile` ?\r\n* Change the behavior of `get_path_to_datafile` ?\r\n* Add a new method to do what `get_path_to_datafile` is supposed to do?"]}, {"number": 15591, "title": "Apply rc0 cherry-picks", "body": "Excludes https://github.com/tensorflow/tensorflow/pull/15458.", "comments": []}, {"number": 15590, "title": " [CMake] Extract more file lists", "body": "like #14877\r\n@mrry /cc", "comments": ["Can one of the admins verify this patch?", "Can you explain the benefit of this change? As far as I can tell, all it does is add an indirection, and the file lists aren't shared with other parts of the build. (This is unlike the list of Python modules, which is useful to extract so that it can be used in a consistency-checking test.)", "@mrry I [discussed this](https://github.com/tensorflow/tensorflow/issues/10296#issuecomment-353749235) with @gunan\r\nBoth `tf_python.cmake` and `tf_tests.cmake` will benefit, other won't.\r\nI reverted all non-beneficial changes.", "Feel free to bounce.", "Added tests (like #15670)", "My comments on the issue were just comparative values for extracting lists from the cmakefiles. Not a comment on the absolute value of extracting the file lists. So, as @mrry asked, if we can explain the added value we get from extracting the lists to txt files, we can review this. Otherwise, I usually like erring on the side of lesser code complexity.", "I guess what @gunan is rightfully asking: what does this buy us? Does it lead to\r\n- less/easier modification of CMAKE files in cases of changes to the code\r\n- better tests/errors in case of changes that would break the CMAKE build\r\nor some other advantage?\r\n", "* separates concerns of declaration and use (less need to care about the other)\r\n* changes don't affect each other (data format (`\"${tensorflow_source_dir}/`), line indentation)\r\n* provides clean access to other code (e.g. sharing with other build scripts)\r\n* enables testing (in this case, helps [cleaning up outdated references](https://github.com/tensorflow/tensorflow/issues/10296#issuecomment-353749963))", "Well, maybe points 1 and 3 don't quite hold for the `exclude` lists.", "The point is, as @gunan stated, issues with Python files go unnoticed.\r\nFor example, [this line](https://github.com/tensorflow/tensorflow/pull/15590/files#diff-154d7a6e7930f8e143fbd4b8329c67d0L248) should ideally have broken the build.\r\nAnd as the last commit on this PR shows, there were invalid entries in 3 of 4 file lists.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@gunan are you convinced by the arguments & changes that we should proceed with this?", "Seems like we hit two more invalid exclude paths:\r\n`tensorflow/contrib/tfprof/python/tools/tfprof/pprof_profiler_test.py`\r\n`tensorflow/contrib/data/python/kernel_tests/iterator_ops_test.py`\r\n(unnoticed since removal last week)", "After @mrry and @martinwicke's comments, I think it may be better to drop this PR. These lists do not need to be shared across different things, so removing them from the cmake files are just added complexity.\r\n"]}, {"number": 15589, "title": "Segmentation fault on get_session_handle after 1.4.1 upgrade", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 (from 1.4.1 pip install)\r\n- **Python version**: 2.7.6\r\n- **Bazel version (if compiling from source)**: N/a\r\n- **GCC/Compiler version (if compiling from source)**: N/a\r\n- **CUDA/cuDNN version**: N/a\r\n- **GPU model and memory**: N/a\r\n- **Exact command to reproduce**: \r\n\r\nAfter upgrading from 1.3 to 1.4.1, running the following code produces a segmentation fault:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.Session() as S: S.run( tf.get_session_handle(tf.constant(1, dtype=tf.float32)) )\r\n```\r\n\r\n\r\n### Source code / logs\r\n\r\nGDB stack traces:\r\n\r\n```\r\n(gdb) py-bt \r\n0x00007fffb765b307 in nsync::nsync_mu_lock(nsync::nsync_mu_s_*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n(gdb) py-bt\r\n#22 Frame 0xfea2d0, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 1302, in _run_fn (session=<SwigPyObject at remote 0x7fff6e664330>, feed_dict={}, fetch_list=['GetSessionHandle:0'], target_list=[], options=None, run_metadata=None, status=<SwigPyObject at remote 0x7fff6e670f00>)\r\n    status, run_metadata)\r\n#27 Frame 0xff8a00, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 1323, in _do_call (self=<Session(_config=None, _graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={}, _name_stack='', _gradient_override_map={}, _seed=None, _handle_movers={}, _op_to_kernel_label_map={}, _nodes_by_id={1: <Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fffeb2f2150>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=1) at remote 0x7ffff377d950>, _consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fff6e612810>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=7) at remote 0x7ffff377db10>, _consumers=[], _id=1L) at remote 0x7fff6e612750>], _control_flow_context=None, _id_value=2, _original_op=None, _traceback=[('minimal_core_dump.py', 3, '<module>', {'__builtins__': <module at remote 0x7ffff7f9ab08>, '__fi...(truncated)\r\n    return fn(*args)\r\n#31 Frame 0xfe2fe0, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 1317, in _do_run (self=<Session(_config=None, _graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={}, _name_stack='', _gradient_override_map={}, _seed=None, _handle_movers={}, _op_to_kernel_label_map={}, _nodes_by_id={1: <Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fffeb2f2150>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=1) at remote 0x7ffff377d950>, _consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fff6e612810>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=7) at remote 0x7ffff377db10>, _consumers=[], _id=1L) at remote 0x7fff6e612750>], _control_flow_context=None, _id_value=2, _original_op=None, _traceback=[('minimal_core_dump.py', 3, '<module>', {'__builtins__': <module at remote 0x7ffff7f9ab08>, '__fil...(truncated)\r\n    options, run_metadata)\r\n#35 Frame 0xf12bc0, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 1120, in _run (self=<Session(_config=None, _graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={}, _name_stack='', _gradient_override_map={}, _seed=None, _handle_movers={}, _op_to_kernel_label_map={}, _nodes_by_id={1: <Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fffeb2f2150>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=1) at remote 0x7ffff377d950>, _consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fff6e612810>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=7) at remote 0x7ffff377db10>, _consumers=[], _id=1L) at remote 0x7fff6e612750>], _control_flow_context=None, _id_value=2, _original_op=None, _traceback=[('minimal_core_dump.py', 3, '<module>', {'__builtins__': <module at remote 0x7ffff7f9ab08>, '__file__...(truncated)\r\n    feed_dict_tensor, options, run_metadata)\r\n#39 Frame 0xfa91e0, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 889, in run (self=<Session(_config=None, _graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={}, _name_stack='', _gradient_override_map={}, _seed=None, _handle_movers={}, _op_to_kernel_label_map={}, _nodes_by_id={1: <Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fffeb2f2150>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=1) at remote 0x7ffff377d950>, _consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fff6e612810>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=7) at remote 0x7ffff377db10>, _consumers=[], _id=1L) at remote 0x7fff6e612750>], _control_flow_context=None, _id_value=2, _original_op=None, _traceback=[('minimal_core_dump.py', 3, '<module>', {'__builtins__': <module at remote 0x7ffff7f9ab08>, '__file__':...(truncated)\r\n    run_metadata_ptr)\r\n#43 Frame 0x7ffff7f4da00, for file minimal_core_dump.py, line 3, in <module> ()\r\n    with tf.Session() as S: S.run( tf.get_session_handle(tf.constant(1, dtype=tf.float32)) )\r\n```\r\n\r\n\r\n```\r\n(gdb) bt  \r\n#0  0x00007fffb765b307 in nsync::nsync_mu_lock(nsync::nsync_mu_s_*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#1  0x00007fffb494de1f in tensorflow::SessionState::GetNewId() () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#2  0x00007fffb636bd24 in tensorflow::GetSessionHandleOp::Compute(tensorflow::OpKernelContext*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fffb7852729 in tensorflow::grappler::ConstantFolding::EvaluateNode(tensorflow::NodeDef const&, tensorflow::gtl::InlinedVector<tensorflow::TensorValue, 4> const&, tensorflow::gtl::InlinedVector<tensorflow::TensorValue, 4>*) const () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fffb7858a03 in tensorflow::grappler::ConstantFolding::EvaluateOneFoldable(tensorflow::NodeDef const&, std::vector<tensorflow::NodeDef, std::allocator<tensorflow::NodeDef> >*) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fffb7859486 in tensorflow::grappler::ConstantFolding::FoldNode(tensorflow::NodeDef*, tensorflow::GraphDef*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fffb785ab03 in tensorflow::grappler::ConstantFolding::FoldGraph(tensorflow::GraphDef*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007fffb785b526 in tensorflow::grappler::ConstantFolding::RunOptimizationPass(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007fffb785b9c9 in tensorflow::grappler::ConstantFolding::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007fffb7843657 in tensorflow::grappler::MetaOptimizer::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007fffb7844925 in tensorflow::grappler::RunMetaOptimizer(tensorflow::grappler::GrapplerItem const&, tensorflow::RewriterConfig const&, tensorflow::DeviceBase*, tensorflow::grappler::Cluster*, tensorflow::GraphDef*)\r\n    () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007fffb782f316 in tensorflow::GraphExecutionState::OptimizeGraph(tensorflow::BuildGraphOptions const&, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >*) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#12 0x00007fffb782fe80 in tensorflow::GraphExecutionState::BuildGraph(tensorflow::BuildGraphOptions const&, std::unique_ptr<tensorflow::ClientGraph, std::default_delete<tensorflow::ClientGraph> >*) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#13 0x00007fffb76678ae in tensorflow::DirectSession::CreateGraphs(tensorflow::BuildGraphOptions const&, std::unordered_map<std::string, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> > > > >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*, tensorflow::DirectSession::RunStateArgs*, tensorflow::gtl::InlinedVector<tensorflow::DataType, 4>*, tensorflow::gtl::InlinedVector<tensorflow::DataType, 4>*)\r\n    () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#14 0x00007fffb7669daa in tensorflow::DirectSession::GetOrCreateExecutors(tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#15 0x00007fffb766b5bb in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#16 0x00007fffb5d5e0fa in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#17 0x00007fffb5d5e434 in TF_Run () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#18 0x00007fffb5a7c9da in tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#19 0x00007fffb5a7cdd1 in tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#20 0x00007fffb5a410b1 in _wrap_TF_Run () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#21 0x00000000004c7f54 in call_function (oparg=<optimized out>, pp_stack=0x7fffffffd1a0) at ../Python/ceval.c:4020\r\n#22 PyEval_EvalFrameEx (\r\n    f=f@entry=Frame 0xfea2d0, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 1302, in _run_fn (session=<SwigPyObject at remote 0x7fff6e664330>, feed_dict={}, fetch_list=['GetSessionHandle:0'], target_list=[], options=None, run_metadata=None, status=<SwigPyObject at remote 0x7fff6e670f00>), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\r\n#23 0x00000000004704ea in PyEval_EvalCodeEx (closure=<optimized out>, defcount=<optimized out>, defs=0x0, kwcount=<optimized out>, kws=<optimized out>, argcount=<optimized out>, args=<optimized out>, locals=0x0,\r\n    globals=<optimized out>, co=<optimized out>) at ../Python/ceval.c:3252\r\n#24 function_call.15337 (func=<optimized out>, arg=<optimized out>, kw=<optimized out>) at ../Objects/funcobject.c:526\r\n#25 0x00000000004c9aa5 in PyObject_Call (kw=0x0, arg=(<SwigPyObject at remote 0x7fff6e664330>, {}, ['GetSessionHandle:0'], [], None, None), func=<function at remote 0x7fff6e610d70>) at ../Objects/abstract.c:2529\r\n#26 ext_do_call (nk=<optimized out>, na=<optimized out>, flags=<optimized out>, pp_stack=0x7fffffffd3e0, func=<function at remote 0x7fff6e610d70>) at ../Python/ceval.c:4333\r\n... # partial output\r\n```\r\n", "comments": ["I'm not sure what's going on. I know `tf.get_session_handle` is marked experimental in the documentation. @girving touched GetSessionHandleOp last and might have a better understanding of why it'd have a mutex segfault. I also know they say things like, \"it's never a compiler bug\" but I'm going to cc: @m3bm3b since this does trace into `nsync::nsync_mu_lock` knowing that the library has seen a lot of work this year.", "My guess is that the session state pointer in the context is invalid or corrupt.\r\nI don't know why that would be, but maybe @girving has some thoughts.\r\n\r\nHere's what I see from the stack trace and the source:\r\n\r\nWe know that GetSessionHandleOp::Compute() got a SessionState pointer\r\nout of the context ctx, and it called SessionState::GetNewId() passing that pointer.\r\nGetNewId() called nsync_mu_lock() passing a pointer that's a constant \r\noffset from the SessionState pointer.   Thus, the first routine to indirect through that\r\npointer was nsync_mu_lock().\r\n\r\nIt's likely that nsync_mu_lock() is crashing because that SessionState\r\npointer is bad.  The evidence is:\r\n- The mutex acquistion code crashed in nsync_mu_lock(), rather than in one\r\n  of the routines it calls.  The routine is straightforward, and aside from its stack frame, \r\n  it accesses only one word, in the mutex itself.  Thus, to get a segfault in that routine \r\n  directly, either the stack pointer is bad, or the pointer to the mutex is bad. \r\n- Routines on this platform are 16-byte aligned.\r\n  The PC of the faulting instruction is 0x00007fffb765b307, which is 7 mod 16.\r\n  That's what you'd expect if nsync_mu_lock() failed on the first attempt to touch the\r\n   mutex, which (as we normally compile it) is at offset 0x17 from the start of the routine.\r\n   If the stack pointer were bad, we might expect a crash at an address 0 mod 16\r\n   (or indeed at an earlier call)  because the first instruction of the routine pushes to the stack.\r\n\r\nTo confirm this, I recommend that you get to the crash in gdb, \r\nthen report the output of\r\n    disas\r\nThis will disassemble nsync::nsync_mu_lock(nsync::nsync_mu_s_*) ,\r\nand indicate which instruction failed.\r\nI also recommend reporting the output of \r\n    info reg\r\nwhich will indicate the value of the register being used for the indirection.\r\nThe pointer value may give a clue as to how it became corrupt.\r\n\r\n", "I collected the requested data. This time I had to reproduce the bug in a docker container running ubuntu 16.04 instead of in our production environment as I've already downgraded. The same segfault with the same stacktrace occurs.\r\n\r\nHere is the output of disas:\r\n\r\n```\r\n(gdb) disas\r\nDump of assembler code for function _ZN5nsync13nsync_mu_lockEPNS_11nsync_mu_s_E:\r\n   0x00007fffececc2f0 <+0>:\tpush   %rbp\r\n   0x00007fffececc2f1 <+1>:\tmov    $0x1,%edx\r\n   0x00007fffececc2f6 <+6>:\txor    %eax,%eax\r\n   0x00007fffececc2f8 <+8>:\tmov    %rsp,%rbp\r\n   0x00007fffececc2fb <+11>:\tpush   %rbx\r\n   0x00007fffececc2fc <+12>:\tsub    $0x28,%rsp\r\n   0x00007fffececc300 <+16>:\tmovl   $0x0,-0x18(%rbp)\r\n=> 0x00007fffececc307 <+23>:\tlock cmpxchg %edx,(%rdi)\r\n   0x00007fffececc30b <+27>:\tje     0x7fffececc348 <_ZN5nsync13nsync_mu_lockEPNS_11nsync_mu_s_E+88>\r\n   0x00007fffececc30d <+29>:\tmov    %eax,-0x18(%rbp)\r\n   0x00007fffececc310 <+32>:\tmov    (%rdi),%eax\r\n   0x00007fffececc312 <+34>:\ttest   $0xffffff41,%eax\r\n   0x00007fffececc317 <+39>:\tje     0x7fffececc34f <_ZN5nsync13nsync_mu_lockEPNS_11nsync_mu_s_E+95>\r\n   0x00007fffececc319 <+41>:\tmov    %rdi,-0x28(%rbp)\r\n   0x00007fffececc31d <+45>:\tcallq  0x7fffeb1c2b50 <_ZN5nsync17nsync_waiter_new_Ev@plt>\r\n   0x00007fffececc322 <+50>:\tmov    0x10cbe27(%rip),%rdx        # 0x7fffedf98150\r\n   0x00007fffececc329 <+57>:\tmov    -0x28(%rbp),%rdi\r\n---Type <return> to continue, or q <return> to quit---\r\n   0x00007fffececc32d <+61>:\tmov    %rax,%rbx\r\n   0x00007fffececc330 <+64>:\tmov    %rax,%rsi\r\n   0x00007fffececc333 <+67>:\tmov    (%rdx),%rcx\r\n   0x00007fffececc336 <+70>:\txor    %edx,%edx\r\n   0x00007fffececc338 <+72>:\tcallq  0x7fffeb1c2b90 <_ZN5nsync19nsync_mu_lock_slow_EPNS_11nsync_mu_s_EPNS_6waiterEjPNS_11lock_type_sE@plt>\r\n   0x00007fffececc33d <+77>:\tmov    %rbx,%rdi\r\n   0x00007fffececc340 <+80>:\tcallq  0x7fffeb1c2b60 <_ZN5nsync18nsync_waiter_free_EPNS_6waiterE@plt>\r\n   0x00007fffececc345 <+85>:\tnopl   (%rax)\r\n   0x00007fffececc348 <+88>:\tadd    $0x28,%rsp\r\n   0x00007fffececc34c <+92>:\tpop    %rbx\r\n   0x00007fffececc34d <+93>:\tpop    %rbp\r\n   0x00007fffececc34e <+94>:\tretq\r\n   0x00007fffececc34f <+95>:\tlea    0x1(%rax),%edx\r\n   0x00007fffececc352 <+98>:\tmov    %eax,-0x14(%rbp)\r\n   0x00007fffececc355 <+101>:\tand    $0xffffffdf,%edx\r\n   0x00007fffececc358 <+104>:\tlock cmpxchg %edx,(%rdi)\r\n   0x00007fffececc35c <+108>:\tje     0x7fffececc348 <_ZN5nsync13nsync_mu_lockEPNS_11nsync_mu_s_E+88>\r\n   0x00007fffececc35e <+110>:\tmov    %eax,-0x14(%rbp)\r\n   0x00007fffececc361 <+113>:\tjmp    0x7fffececc319 <_ZN5nsync13nsync_mu_lockEPNS_11nsync_mu_s_E+41>\r\n---Type <return> to continue, or q <return> to quit---\r\nEnd of assembler dump.\r\n```\r\n\r\nAnd info reg:\r\n\r\n```\r\n(gdb) info reg\r\nrax            0x0\t0\r\nrbx            0x0\t0\r\nrcx            0x0\t0\r\nrdx            0x1\t1\r\nrsi            0x0\t0\r\nrdi            0x0\t0\r\nrbp            0x7fffffffa630\t0x7fffffffa630\r\nrsp            0x7fffffffa600\t0x7fffffffa600\r\nr8             0x555555a19960\t93824997235040\r\nr9             0x7fffea514dd0\t140737124584912\r\nr10            0x0\t0\r\nr11            0x7ffff6e97f50\t140737335885648\r\nr12            0x7fffffffaa60\t140737488333408\r\nr13            0x555555bc1280\t93824998969984\r\nr14            0x555555de5560\t93825001215328\r\nr15            0x7fffffffb0e0\t140737488335072\r\nrip            0x7fffececc307\t0x7fffececc307 <nsync::nsync_mu_lock(nsync::nsync_mu_s_*)+23>\r\neflags         0x10206\t[ PF IF RF ]\r\ncs             0x33\t51\r\nss             0x2b\t43\r\nds             0x0\t0\r\nes             0x0\t0\r\n---Type <return> to continue, or q <return> to quit---\r\nfs             0x0\t0\r\ngs             0x0\t0\r\n```", "Thanks.  That confirms that the pointer to the SessionState is bad: it is nil (the value of register rdi),\r\nthus causing the mutex code to fail as soon as it is used.\r\n(The mutex is the first field in the SessionState, so its address is the same as the SessionState object.)\r\n\r\nAs far as I can see, nothing is setting the session_state anywhere in ConstantFolding::EvaluateNode\r\nin tensorflow/core/grappler/optimizers/constant_folding.cc\r\n\r\nAlas, I don't know whether the problem is that the session state pointer isn't valid,\r\nor that the GetSessionHandleOp code is assuming that the value is valid without\r\nfirst testing it.  The spec for the OpKernelContext in op_kernel.h is not especially clear.\r\n@jart Do you know?  Or know who would know?", "On the advice of a colleague, I have submitted a change\r\nthat registers the operations\r\n   GetSessionHandle GetSessionHandleV2 GetSessionTensor DeleteSessionTensor\r\nas stateful.   That's good, because they are.   But in addition it should fix this problem,\r\nbecause it will also tell the system to avoid constant folding on these operations.\r\nIt was the combination of the constant folding together with the experimental\r\nGetSessionHandle op.\r\n\r\nSo in a few days when the change hits github, this should be fixed.\r\n", "This seems to still be a problem in the new 1.5.0 release. Any idea when this fix will be included in a release?", "@mkmatlock If you would look at the commit that closed this issue, you would see that it is in `v1.6.0-rc1` and `v1.6.0-rc0`."]}]