[{"number": 5724, "title": "enabling VocabularyProcessor.transform to also return document length.", "body": "Currently, `VocabularyProcessor.transform()` ( and `VocabularyProcessor.fit_transform()` ) do not return the sequence length of ( number of tokens in )  individual raw documents. sequence lengths are required when dealing with variable length sequences ( eg. like `dynamic_rnn` where we need to also pass the lengths of individual sequences).\r\n\r\nAfter the changes in PR, both `fit_transform()` and `fit()` has following signature - \r\n- `VocabularyProcessor.fit_transform( raw_documents, return_docs_len=False, unused_y=None)`\r\n- `VocabularyProcessor.transform( raw_documents, return_docs_len=False)`\r\n\r\nSetting `return_docs_len=True` makes above function to also yield the number of token in individual documents. ", "comments": ["@abhitopia, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @vrv and @martinwicke to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "According to `fit_transform(X, y)` semantic, you should place your flag after `unused_y`.", "@enizhibitsky - Thanks for pointing out. Made the changes.", "@martinwicke , @ilblackdragon - Have you had a chance to look at it yet? It's a very minor patch.", "@martinwicke - Any comments on this?", "@ilblackdragon I suppose we're keeping VocabularyProcessor around for lack of an alternative, would you agree? In that case, I think this is sensible, but you know the code much better.", "@ilblackdragon - what do you think?", "@martinwicke , @ilblackdragon , @danmane - been a while, please someone review.", "@martinwicke - It seems that @ilblackdragon is unavailable, can you assign someone else please?", "Sorry, everybody's at NIPS.", "@abhitopia Thanks for the contribution. Just a question - what do you think about `return_docs_len` to be argument of the `VocabularyProcessor` constructor? As I would expect that if `fit` returns then `transform` should return as well. Also how are you passing this (x, doc_length) into Estimator? Probably quick example would be great.\r\n\r\n@martinwicke Yes, there is no real alternative (even after we get feature_to_id op, we still need vocabulary generator).", "@abhitopia could you address @ilblackdragon concerns?", "Closing due to inactivity. Feel free to reopen when you're able to address review comments."]}, {"number": 5723, "title": "Swap dictionary and tensor for readability", "body": null, "comments": ["@raviqqe, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jhseu and @tensorflower-gardener to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!!!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please", "The Windows cmake build failure looks unrelated. @mrry and @gunan, can you take a look?", "@caisq Yes, it looks unrelated.", "PR merged. Thanks, @raviqqe !"]}, {"number": 5722, "title": "Async prefetching queue data on GPU", "body": "For example look at te following code:\r\n```python\r\nX = fifo_queue.dequeue_many(100) # Some queue\r\n...  # some processing\r\nf  # reslut of computation\r\n```\r\n\r\n```fifo_queue``` holds all its variable and data in RAM, that is on CPU since there's no GPU kernel for it (nor for any other queue that I'm aware of.\r\n\r\nHowever upon running:\r\n```python\r\nsess.run(f)\r\n```\r\n\r\nThere's a huge bottleneck for dequeuing and copying data from CPU to GPU which could be done async. Here is example of timeline: http://imgur.com/a/1cGHf\r\n\r\nHow can I tell tensorflow to async prefetch data on GPU (for example 3xbatch size examples or something like that) so that I avoid waiting for QueueDequeuMany and MEMCPYHtoD operations? Closest I could find for an answer is [this](http://stackoverflow.com/questions/38751736/understanding-tensorflow-queues-and-cpu-gpu-transfer) stackoverflow post, but it didn't shed enough light on the problem.\r\n", "comments": ["Relevant: https://github.com/tensorflow/tensorflow/issues/3009\n", "As you say, queues are currently only implemented on CPU and cannot cross devices.\r\n\r\nIf you don't have any useful computation which can happen while the *async* transfer D2H is in progress then the only way to overlap this transfer would be to do it in a different call to session.run and store the result in some sort of variable.  However, this would rely on coordination between steps \r\n\r\nThe only stateful ops available on GPU are Variables and TensorArrays.  \r\n\r\nYou *could* imagine manually building some sort of pipeline or double-buffering where the session.run call computed on one Variable and simultaneously transferred the input for the next step into a second Variable.\r\n\r\nAlternatively, you could look at how the dynamic while loop speculatively executes multiple loop iterations in parallel, overlapping transfers for subsequent steps.\r\n\r\n", "Four options:\n\n1. Don't use queues; batch input yourself each iteration.\n2. Run parallel training threads in parallel; while one is reading another is training\n3.  Modify a queue to use GPU; not an impossible task but also not fun\n4. Improve the performance of existing queues by optimizing the c++ code\n\nTensorArrays are per step objects, thus not useful for this case ", "Would be very awesome to have this feature!\r\nI've compared a resnet18 on two different input method:\r\n1. Use a constant variable on GPU (not useful in practice)\r\n2. Use a dequeued tensor, and having a Python thread running enqueue op in a loop.\r\n\r\nIn my experiment (1) is about 10% faster than (2).\r\n\r\nbtw, [convnet-benchmarks](https://github.com/soumith/convnet-benchmarks) use (1) to benchmark tensorflow.", "@mrry Do you have any ideas about how best to mask GPU transfer latency of large input tensors?", "In principle one could write a version of the `FIFOQueue` that managed GPU buffers rather than CPU buffers, and then use the existing mechanisms (like queue runners) to drive a subgraph that copies the data from CPU to GPU and enqueues it in this queue. I'm not sure what the relative latency is between these copies and the framework overhead to run a step that performs the enqueuing, but I suspect it could be profitable in some cases.", "@nmiculinic a way to provide evidence that GPU-resident queue is useful could be to implement it on the client side using a rotating buffer of GPU-pinned variables and measure the speed-up, ie https://github.com/tensorflow/tensorflow/issues/4526#issuecomment-249021613", "@yaroslavvb  I've tried to implement double buffering and run some test: Full code is here: http://pastebin.com/Rcg76XPM\r\n\r\n1) CPU only\r\n\r\nWithout buffering, that is async prefetching:\r\nhttps://i.imgur.com/DWS3OKZ.png\r\n\r\nWith buffering:\r\nhttps://i.imgur.com/gi4wgUu.png\r\n\r\nAs you can see there's performance improvement when queue element is already loaded in memory even for CPU... not sure why\r\n\r\n2) GPU\r\n\r\nWith GPU this is even more pronounced due to data copying:\r\nWithout buffer:\r\nhttps://i.imgur.com/v7sIKF8.png\r\nWith Buffer:\r\nhttps://i.imgur.com/j6bF67e.png\r\n\r\nI'm not sure why is only dequeue async in my code, not also data transfer. If somebody could give me some feedback I'd much appreciate it. Nevertheless speed improvement is obvious and immediate.", "@yaroslavvb what do you think of @mniculinic's adove code? Is that what you intended? I have tried to hunt down this copy overhead problem, and have found you have suggested this solution in half a dozen cases. Might be nice to nail it down. I'm willing and able to help.", "Two comments on the code:\r\n1) Measuring the execution time of a step when you are tracing and retrieving a large RunMetadata proto back to Python is never a good idea - especially if you haven't installed the C++ protobuf library.  \r\n2) I *think* there's probably a simpler way to do this:\r\n   * Place a (non-trainable) Variable on the GPU to hold a single cached value.  \r\n   * Each step, in parallel: \r\n       * Take a snapshot of the current value of the GPU variable using an Identity op\r\n       * Dequeue from the FIFO (i.e. kick off the copy from the CPU)\r\n   * Serialized after the read, Assign the value from the FIFO into the variable for use in the next step.\r\n   * Compute with the snapshot value.\r\n\r\nThis may be equivalent to your code... but I admit I got a little lost in all the X's ;-)\r\nI'm also not 100% sure whether this would play well with auto-gradient code though...\r\n", "OK @prb12 @yaroslavvb @nmiculinic, I implemented the rotating buffer with a size of 2. One is where the GPUs pull from, the other is where the feeder pushes to.\r\nI took me a while to get the most optimal implementation while avoiding R/W race conditions.\r\nI made a Gist for this: https://gist.github.com/TimZaman/639576c25693ca4890457efa5f67d103\r\n\r\n1. init stage: `sess.run([put_in_buffer])`\r\n2. init stage: `sess.run([move_buffer])`\r\n3. loop: `sess.run([pull])`\r\n\r\nDefitinion of `pull`:\r\n```py\r\nwith tf.control_dependencies([move_buffer]):\r\n        pull = tf.group(put_in_buffer, test_out)\r\n```\r\nSo `pull` first refreshes the buffer, and then simultaneously puts new data in the buffer, and reads for GPU computation from the other buffer element.\r\n\r\nI have a lot of trouble with these `control_dependencies`: the above yields accurate results, but if you read carefully i (1) push (2) move (3) loop(move, put_in_buffer, test_output). As you can see, i move 2x on the first iteration, but the value stays the same.\r\nThe Gist is setup such that i can monitor the values of the both buffers and the output. Here is an output where the first 20 buffer values are 0:1:20:\r\n```\r\n[BUF_IN][BUF_OUT][RESULT]:[[[1 1 1]]...][[[0 0 0]]...][0]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[2 2 2]]...][[[1 1 1]]...][0.99999082]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[3 3 3]]...][[[2 2 2]]...][1.9999816]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[4 4 4]]...][[[3 3 3]]...][3.0000162]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[5 5 5]]...][[[4 4 4]]...][3.9999633]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[6 6 6]]...][[[5 5 5]]...][5.0000668]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[7 7 7]]...][[[6 6 6]]...][6.0000324]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[8 8 8]]...][[[7 7 7]]...][6.9999766]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[9 9 9]]...][[[8 8 8]]...][7.9999266]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[10 10 10]]...][[[9 9 9]]...][8.99991]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[11 11 11]]...][[[10 10 10]]...][10.000134]\r\n```\r\nThe above reveals that the input buffer is always +1 with respect to the output buffer, and the output buffers value is the same as the result: so it's working.\r\nFor this particular example, you can see the amazing benefit of a GPU resident queue:\r\n\r\n![image](https://cloud.githubusercontent.com/assets/7721540/21082114/1cfdbe10-bfdd-11e6-8cb0-69bc352f6963.png)\r\n\r\nNow, I would like to reiterate this comment https://github.com/tensorflow/tensorflow/issues/4526#issuecomment-249026120 from @mkolod:\r\n\r\n> I think though that it would be good for this to be baked into the framework rather than shifting flow control workarounds to the user. Data prefetch is pretty much a universal need, since the input has to come from somewhere, hopefully in an efficient manner to leverage available compute cycles.\r\n\r\nI would like to suggest opening up this issue: https://github.com/tensorflow/tensorflow/issues/4526#issuecomment-249026120\r\nIt would bring amazing benefit from operations that are heavy on data, and light on compute.\r\n\r\n", "I wonder if it's possible to build the prefetch into the scheduler? Consider the following diagram:\r\n\r\n![img_20161212_134953255](https://cloud.githubusercontent.com/assets/3530212/21098743/4774b4dc-c073-11e6-9fa1-f90ecb4a213b.jpg)\r\n\r\nWe have three separate sub-graphs [(foo->bar, qux->baz, plug->thud)] that require execution on a (GPU?) device, depending on inputs [(a,b,c),(f,g,h,i,k), (l,m)] located across a device boundary and whose outputs [(d,e),(k),(n)] must be transmitted across the boundary once they are complete.\r\n\r\nAs it currently stands, tensorflow appears (to me) *to treat transfers and compute operations as the same \"type\" of operation, rather than as operations that can be concurrently scheduled*.  As @prb12  notes  in https://github.com/tensorflow/tensorflow/issues/2848, Recvs are scheduled immediately and Sends are scheduled once the input data is available.  So on a rough timeline the above sub-graphs seem to currently be scheduled as (see https://github.com/tensorflow/tensorflow/issues/2848 for e.g. where sends are scheduled immediately)\r\n\r\n![img_20161212_141704007](https://cloud.githubusercontent.com/assets/3530212/21099282/05f56d00-c076-11e6-9983-4919a9cce0da.jpg)\r\n\r\nThe problem with this scheduling is that it wastes both I/O bandwidth and device compute. Given that these operations *can* be concurrently scheduled an optimal order of transfer and execution might be:\r\n\r\n![img_20161212_135248066](https://cloud.githubusercontent.com/assets/3530212/21098800/945d2130-c073-11e6-8eef-b3d28ebeb4f9.jpg)\r\n\r\n(a,b) must be transferred before foo can execute, (c) must be transferred before bar can and so on. The output transfers for [(d,e), (k), (n)] can be scheduled last.\r\n\r\nThe point is that scheduling the ops on the (GPU?) device necessarily imposes an ordering on how the inputs should be transferred. While my understanding of how ops are currently scheduled is limited  I would ideally expect it to be something like [a, b, foo, c, bar, f, g, h, qux, i, j, baz, l, plug, m, thud, d, e, k, n].\r\n\r\nRunning the ops *as soon* as their inputs are available should use the device optimally. If your compute beats your I/O then the device is used optimally. If not, then you've done the best you can given the algorithms in the ops.\r\n\r\nAll this information is available in the graph. I think the tricky part is estimating how many sub-graphs can be scheduled in one go -- this would involve balancing device memory vs the size of the input , temporary result and output tensors. But looking at https://github.com/tensorflow/tensorflow/issues/2848 and other issues suggests that tensorflow does this already. It just doesn't currently schedule transfers and executions optimally.\r\n\r\nFurther optimisations based on the number of copy engines on the GPU are possible.\r\n\r\n\r\n\r\n\r\n", "@sjperkins Thanks for your suggestions.  Here are some brief comments: (I don't really have time to go into this at length)\r\n\r\n>  Recvs are scheduled immediately and Sends are scheduled once the input data is available. So on a rough timeline the above sub-graphs seem to currently be scheduled as (see #2848 for e.g. where sends are scheduled immediately)\r\n\r\nI think you are confused about what `Send` and `Recv` *ops* actually do.   \r\n\r\nEach transfer between devices involves two ops - the Send op on the source device is executed as soon as its input tensor is \"ready\". (subtlety - on GPU this means when the op which computes the Tensor has been added to the GPU compute stream.)  The 'Recv' op has no inputs, an hence is ready to be dispatched immediately, but the Send and Recv ops actually execute a _rendezvous_ and the  copy/DMA between devices is only scheduled once the rendezvous is complete.   Many Send and Recv ops can be dispatched simultaneously (`inter_op_parallelism_threads`), and they do not (at this point) have any ordering constraints with respect to other computation ops which have no dataflow dependency.\r\n\r\n> The point is that scheduling the ops on the (GPU?) device necessarily imposes an ordering on how the inputs should be transferred.\r\n\r\nI strongly encourage you to read `executor.cc`.  The above described mechanism is there purely to capture these ordering dependencies in  a way which is safe for async memory allocation and kernel execution.\r\n\r\n> appears (to me) to treat transfers and compute operations as the same \"type\" of operation, rather than as operations that can be concurrently scheduled. \r\n\r\nThis isn't really true. Transfers to and from GPU are executed on their own CUDA streams - and hence can happen in parallel with compute.  The ops dependent on Tensors which are being transferred to GPU cause the compute strream to take a dependency on the memcpy_host_to_device stream using the CUDA `RecordEvent` and `StreamWaitEvent` primitives.   There are some awkward case to deal with here and the current implementation is suboptimal in a couple of places (mainly due to there being a single compute stream), but it's a lot better than what you describe.\r\n\r\n> While my understanding of how ops are currently scheduled is limited I would ideally expect it to be something like [a, b, foo, c, bar, f, g, h, qux, i, j, baz, l, plug, m, thud, d, e, k, n].\r\n\r\nIn fact, what currently happens is that a, c, c, f, g, h will all get scheduled on some non-deterministic order on the device_to_host stream shortly after their inputs are available on the CPU device.  As soon as the **async**  MemcpyH2D has been enqueued for all of the inputs for foo, the compute stream will be serialized on the copy stream (using an event) and the CUDA kernel for `foo` will be enqueued on the compute stream.  Once `foo` and `bar` have been enqueued, the device_to_host stream is made to depend on the compute stream using another event, and an async MemCopyD2H is ennqueued on the device_to_host stream.  This can execute concurrently wrt quz, baz, plug, thd et al.\r\n\r\nThe sub-optimal behavior, which *sometimes* matters, especially with device-to-device transfers is that unrelated compute ops can get added to the compute stream *before* the event which we use to serialize the copy back to the host (or other GPU).  The fix for this is not *too* bad, and involves adding an event to the compute stream eagerly as part of enqueueing and compute op which feeds into a `Send`.  However, this comes at the cost of needing quite a large number of CUDA Events, and we didn't deem this performance tradeoff worthwhile.  I suspect this decision will be revisited soon since the impact on multi-GPU configurations can be significant.\r\n\r\n@TimZaman \r\n> It would bring amazing benefit from operations that are heavy on data, and light on compute.\r\n\r\nThis is really not a very common case in my experience... e.g. when training an inception model on a single GPU the step time is around 2 seconds and DMA transfers are largely in the noise.  Latency sensitive inference steps may be more of an issue.  I would be open to seeing more evidence of where this matters for non-trivial workloads.\r\n\r\n", "@prb12 \r\n> This is really not a very common case in my experience... e.g. when training an inception model on a single GPU the step time is around 2 seconds and DMA transfers are largely in the noise. Latency sensitive inference steps may be more of an issue. I would be open to seeing more evidence of where this matters for non-trivial workloads.\r\n\r\nWhat about the shallow model (such as LR, deep and wide) workloads? I think this should be one of the heavy on data case. The H2D transfer may not be the major bottleneck compared to IO, but indeed it can improve if async transfer is supported.", "The Inception model example and a 2 second step time doesn't sound lightweight. Another example like given by @llhe; anyone who's doing light, realtime things can't generally use models that heavy. My particular model takes steps in the order of tens of ms. A GPU queue would make perfect sense to me.\r\n", "@prb12 Thanks for taking the time to write your response, the information is very useful information for those coming from a CUDA background.\r\n\r\n> Each transfer between devices involves two ops - the Send op on the source device is executed as soon as its input tensor is \"ready\". (subtlety - on GPU this means when the op which computes the Tensor has been added to the GPU compute stream.) The 'Recv' op has no inputs, an hence is ready to be dispatched immediately, but the Send and Recv ops actually execute a rendezvous and the actual copy/DMA between devices is scheduled once the rendezvous is complete.\r\n\r\nI browsed through the code, but was unable to follow through all the Send/Recv/Rendezvouz/CUDA abstractions. Briefly, is this similar to the following pattern for a GPU to CPU transfer? In one thread, asynchronously issue:\r\n\r\n```cpp\r\n// for some kernel computing a result placed in d_result\r\n// kernel<..., stream>(arg0, arg1, d_result);\r\n\r\n// \"Send op\" starts here\r\ncudaMemcpyAsync(s_result, d_result, stream);\r\ncudaEvent_t done;\r\ncudaEventRecord(done, stream);\r\n```\r\n\r\nand in some synchronisation thread:\r\n\r\n```cpp\r\n// \"Recv op\" starts here\r\ncudaEventSynchronise(done);\r\nrelease_to_pool(d_result);\r\nprobably_in_another_thread_use(s_result);\r\n```\r\n\r\n> This isn't really true. Transfers to and from GPU are executed on their own CUDA streams - and hence can happen in parallel with compute. The ops dependent on Tensors which are being transferred to GPU cause the compute strream to take a dependency on the memcpy_host_to_device stream using the CUDA RecordEvent and StreamWaitEvent primitives. There are some awkward case to deal with here and the current implementation is suboptimal in a couple of places (mainly due to there being a single compute stream), but it's a lot better than what you describe.\r\n\r\nOK I'm familiar with this pattern. Thanks for clarifying this, but I have an example below where I'm struggling to achieve this overlap through submitting separate compute to `tf.Session.run` in multiple threads.\r\n\r\n> In fact, what currently happens is that a, c, c, f, g, h will all get scheduled on some **non-deterministic order** on the device_to_host stream shortly after their inputs are available on the CPU device.\r\n\r\nThe non-determinism is what I want to niggle about. More below:\r\n\r\n> As soon as the async MemcpyH2D has been enqueued for all of the inputs for foo, the compute stream will be serialized on the copy stream (using an event) and the CUDA kernel for foo will be enqueued on the compute stream. Once foo and bar have been enqueued, the device_to_host stream is made to depend on the compute stream using another event, and an async MemCopyD2H is ennqueued on the device_to_host stream. This can execute concurrently wrt quz, baz, plug, thd et al.\r\n\r\n> Each transfer between devices involves two ops - the Send op on the source device is executed as soon as its input tensor is \"ready\"\r\n\r\nOK tensorflow is scheduling ops non-deterministically because its not assuming knowledge of when inputs will become available (network transfers, multiple ops running on multiple CPU cores). Yeah I can see why this is a difficult problem. I think this is perhaps where CUDA programmers struggle. e.g. thinks like \"I know all my inputs are on the CPU and can be scheduled in a specific order, why isn't tensorflow scheduling it like I know I could do?\"\r\n\r\n*BUT* this non-determinism does bite as it can result in input transfers for multiple sub-graphs being unnecessarily scheduled upfront before compute occurs (example below).  Perhaps a possible solution is to logically group inputs required by a GPU op for transfer? I don't know enough of the scheduling internals to ascertain whether this is a reasonable suggestion. A GPU queue would achieve the same effect\r\n\r\nI've another concrete example to demonstrate. We're developing radio telescope simulator and the challenge (in terms of data transfer) is that 550MB (!) voxel cubes are used to model how the beam of an antenna affects the signal from a radio source. I've sharded feeding (3 threads), compute (3 threads) and output dequeues (1 thread). I have 6 units of compute that I want to perform. The algorithm is compute bound.\r\n\r\n![titan_x_3_shards](https://cloud.githubusercontent.com/assets/3530212/21262989/ef9e90b6-c39c-11e6-98fd-c8e1630dd628.png)\r\n\r\nI've attached:\r\n-  [CUDA profile](https://github.com/tensorflow/tensorflow/files/657167/nvvp_prof.zip) on a Titan X. Relevant run from 20.65s to 21.99s. (merging RunMetadata objects for multiple threads produces a trace file that is too large for chrome).\r\n- Chrome [trace](https://github.com/tensorflow/tensorflow/files/657206/timeline.json.zip) a on laptop GTX960. Single thread feeding, single thread computing.\r\n\r\nThe above profile shows the 6 units of compute sheduled into two sections of 3 units each (presumably because the 3 compute threads submit work to `Session.run` at a similar time). As the above profile and image shows, the 3 voxel cubes (1.5GB!) associated with 3 work units (or, using the terminology in my previous post, sub-graphs) scheduled for transfer upfront. Additionally the 3 transfers for the last 3 work units/sub-graphs are not overlapped with the compute of the first 3.\r\n\r\nThis is what I was describing using the toy example in the previous post:\r\n\r\n![prev](https://cloud.githubusercontent.com/assets/3530212/21099282/05f56d00-c076-11e6-9983-4919a9cce0da.jpg)\r\n\r\nI think the associated code is too large to serve as a test case. Would a minimal example be useful here?\r\n\r\n\r\n\r\n", "+1 to built-in support for GPU-based data queues. Not all of us are running huge inception-style models with images. I definitely have a application with a smaller network but huge data sets, and got a graphics card with a large amount of RAM in order to queue up all the data in memory. I was really surprised that some people don't consider this a standard use case.", "Looks like there's `enqueue` like op with GPU support being added -- https://github.com/tensorflow/tensorflow/commit/85acf52a", "This is great news. Thanks to those who put this together. I can't wait to try it out.", "@yaroslavvb Can you suggest how to use this GPU support?", "@post2web -- @tfboyd is working on https://github.com/tensorflow/tensorflow/issues/7679 which should have an example of using these ops, once it's ready", "I am observing the issue where \"Host to Device\" data tx. does not overlap with GPU execution using the rotating buffer code (published by TimZaman on Dec 11, 2016). Is this a known issue? I also tried Stage/Unstage APIs but have the same issue. It looks like that there is a plan to publish the scripts in order to perfrom \"Async Host to Device transfers\" (as mentioned by zheng-xq on Feb 13 in  #4526)", "@agupta74 releasing of those scripts is tracked in https://github.com/tensorflow/tensorflow/issues/7679", "It appears this has been resolved. I'm gonna close, but please let me know if it should be reopened.", "Do you know when the latest version of StagingQueue (i.e. with capacity and memory_limit options) will be integrated in tensorflow releases? Is it planned to add examples of how to use?", "@EloiZ  Examples of how to StagingQueue is in [here](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py).  Depending on your experience with TensorFlow it may or may not be hard to get working.  There is some documentation [here](https://www.tensorflow.org/performance/performance_models)but it is far from step by step.  \r\n\r\nI cannot promise a date but in Q3 we are working on making DataSets as fast as our custom solution in the tf_cnn_benchmark, which may include suing StagingQueues.  Our goal is to use DataSets in the StagingQueues.  From my own personal testing [DataSets](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data) is much faster than queue readers and starts to show more significant gains when pushing multiple-gpus. \r\n\r\nDataSets instructions / guide should be on tf.org soon, I saw a CL moving the content.  The link I provided is to github so if it moves on me I am sorry.  That link is what I used to get up to speed myself.\r\n\r\nFinally, DataSets will move to Core likely with TF 1.4.  There are no rumors of major changes but the team wants to keep it in contrib for one more release to flush out any possible API changes before moving to core.  \r\n\r\n\r\n", "I wrote a simple script with Timeline API in order to understand if StagingQueue is infact overlapping data transfer with GPU processing but for some reason I did not see any overlap in computation and communication in the timelines shown below. Infact the gap between 2 GPU processing tasks increased on adding the StagingQueue. Any ideas on what could explain this behavior?  Is there anything wrong in my usage of StagingQueues? I am attaching a couple of Python scripts in order reproduce the behavior.\r\n\r\n**With StagingQueue (using 1 GPU)**\r\n\r\npython InferenceTest.py 1 1\r\n\r\n![staging](https://user-images.githubusercontent.com/21690396/29092688-118a996a-7c3c-11e7-9337-d4b8c3aeb9f7.png)\r\n\r\n**No StagingQueue (using 1 GPU)**\r\npython InferenceTest.py 1 0\r\n\r\n![nostaging](https://user-images.githubusercontent.com/21690396/29092782-59724174-7c3c-11e7-91d9-5b997a38a5da.png)\r\n\r\n[Inference.txt](https://github.com/tensorflow/tensorflow/files/1209463/Inference.txt)\r\n[InferenceTest.txt](https://github.com/tensorflow/tensorflow/files/1209462/InferenceTest.txt)\r\n"]}, {"number": 5721, "title": "R0.12", "body": "Update release version @asimshankar just to check if the new string complies with all our usecases.\r\nAdding windows GPU build script to the branch.", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @yifeif, @keveman and @danmane to be potential reviewers.\n", "You mean if the version string is semver compliant? \nLooking at semver.org, it seems that \"0rc\" is not a valid patch since it can't have leading zeros.\nAlso, the whl files are currently a bit funny - 0.12.0-0rc-0rc... (so 0rc twice).\n\nI think what we might want to do is to set `TF_PATCH_VERSION` in `tensorflow/core/public/version.h` to `0-rc0` but remove the hyphen from the filenames of the `.whl`s\n", "Thanks for the explanation.\nI made a change to have the version string as '0.12.0-rc0', but make pip convert this to '0.12.0rc0'\n", "You are right,\nlooks like we will need to handle version updates in a special way.\n"]}, {"number": 5720, "title": "freeze_graph not working as expected", "body": "I am currently attempting to export a protobuf file to Android. To do that, I generated a .pbtxt file using http://pastebin.com/HcWA0xMQ to run freeze_graph on. I saved the protobuf file as text because when I try to save it as binary and run it with freeze_graph, I get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 135, in <module>\r\n    tf.app.run()\r\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 132, in main\r\n    FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)\r\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 98, in freeze_graph\r\n    text_format.Merge(f.read().decode(\"utf-8\"), input_graph_def)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/encodings/utf_8.py\", line 16, in decode\r\n    return codecs.utf_8_decode(input, errors, True)\r\nUnicodeDecodeError: 'utf8' codec can't decode byte 0x96 in position 331: invalid start byte\r\n```\r\n\r\nAfter generating the frozen graph with the .pbtxt file, I tried to run it on Android but for some reason am getting the same output every time regardless of the input. I also noticed, that the frozen graph is actually smaller than the checkpoint file even though it's supposed to be bigger according to the pre-release documentation I got ([deploying.txt](https://github.com/tensorflow/tensorflow/files/601596/deploying.txt)).\r\n\r\n### Environment info\r\nOperating System:\r\nMac OS 10.12\r\nTensorflow 0.11.0\r\nAndroid Cyanogenmod 12\r\n\r\nCode to create .pb file for `freeze_graph`: http://pastebin.com/HcWA0xMQ\r\nMy checkpoint file: [Y6_1476978999.zip](https://github.com/tensorflow/tensorflow/files/601598/Y6_1476978999.zip)\r\n\r\n\r\n", "comments": ["Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 5719, "title": "[FEATURE REQUEST]Calculate topK on GPU with k-selection algos", "body": "Hi,\r\nI am currently using [tf.nn.top_k](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#top_k) in my code. However, I found this operation is calculated on CPU and it is quite slow.\r\nSo I am wondering if it is possible to calculate topK on GPU?\r\n\r\nHere are the papers and cuda codes about k-selection on GPU:\r\nhttp://www.math.grin.edu/~blanchaj/Research/ABGS_KSelection.pdf\r\nhttps://code.google.com/p/ggks/\r\n\r\nHope someday this feature will be added to tensorflow.\r\n", "comments": ["Hello, I'd love to contribute by working on an implementation of this. I'm considering the in-place radix k-select, pivot to cut problem size, then sort, or perhaps a full sort if that turns out to be quicker.\r\n\r\n@lagoon0o0, do you have a history of runtimes for certain size/shape tensors for this algorithm you'd like to see improvement on?\r\n\r\nI'm fairly new to TensorFlow, and would appreciate some insight into the shapes that tensors take in production applications. Thanks!", "+1", "This mixture of experts paper (https://arxiv.org/pdf/1701.06538.pdf) from google brain seems to heavily use topk, so maybe we can hope for an update soon?", "I'm also interested in implementing this as it's currently bottlenecking me. If there is already an implementation internally at Google brain (seems likely) or if @eth-n has already started then there probably isn't much point though.\r\n\r\nedit: The GGKS code linked above looks useful and is under the Apache 2.0 license - same as Tensorflow.   If, hypothetically, a contributor were to modify that code and sign the CLA would that be sufficient? I'm guessing not based on this wording: \"Only original source code from you and other people that have signed the CLA can be accepted into the repository.\"", "For those interested, I've started attempting an implementation of this. Unfortunately I'm having to learn CUDA as I go, so it's slow going. I figure if I make something suboptimal that avoids D2H/H2D transfers then that is still a good starting point.\r\n\r\nThe GGKS code would need some modification so it couldn't just be placed in the third_party folder. That presumably would mean we'd need to get the original authors to sign the CLA in addition to myself - perhaps @prb12 or @aselle can confirm my assumption there? Given that the paper referenced above has enough detail to implement it from scratch, I've decided to go that route for now (making sure not to look at the original code to avoid plagiarism).\r\n\r\nThe fact that this op also requires a sort at the end makes things a little messier. That is probably something that could best be achieved using a third_party lib such as Nvidia's [thrust](https://github.com/thrust/thrust) (licensed under Apache 2.0). This could also be used to make a dedicated sort op (see issue #288). I'll focus on the selection algorithm for now, perhaps others might have some input?\r\n\r\nedit: as I've progressed further I've also found it would be nice to have pre-canned minmax and cumulative sum (scan) algorithms which thrust also has. Perhaps Eigen is able to perform those operations on GPU if I can somehow create a tensor object from my pointer into device memory. Using Eigen still wouldn't solve the problem of sorting though.\r\n\r\nedit 2: As far as I can tell, thrust is installed along with the CUDA toolkit....woops! I'll have a go at using it.", "Update: I have a working GPU implementation but based on some very informal benchmarking, performance is only really better for huge tensors. Still need to clean it up a lot, but after that I might upload it to see if anyone has any tips on how to improve it.", "@ed-alertedh May I see your code? I'm doing something similar, for Theano. Thanks.", "You could use a GPU cuda radix sort sample that comes with standard CUDA distribution as a starting point for implementing topK (even though there's probably a simpler way to do it in CUDA)", "I've ported PyTorch implementation for Theano at [here](https://github.com/Theano/Theano/pull/5959), which uses radix selection. It mostly works but still need some cleanup.", "Well, it looks like we got what we wanted! @ebrevdo looks to have implemented this and it's in 1.3.0rc0!\r\nInterestingly, it's a heap-based algorithm: [code here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/topk_op_gpu.cu.cc)\r\n\r\nI'm a little sad that I didn't manage to release my code in time due to unforeseen \"red tape\" at work. But OTOH, I'm a newbie with CUDA and my thrust-based implementation wasn't really that great anyway :P\r\n\r\nedit: It's worth noting that the CPU benchmarks are probably not indicative of real-world performance, as it seems likely to me that most of us wanting a GPU implementation of this op have the rest of their graph running on GPU. To simulate the performance most people are currently getting, you need the inputs resident on the GPU and the TopK op running on CPU. For large input tensors, I've seen the DtoH transfer times start having a significant impact on throughput.", "@ebrevdo @ed-alertedh I tried running the new top_k operator as an intermediate layer on the gpu, and am still getting cpu-like speeds. Any idea what could be going on? I logged the device placement, and it does seem like the operator is still getting placed on the gpu. \r\n\r\nI tested a script with the operator in between two convolutional layers on both 1.3.0-rc2 (gpu) and 1.2.1 (cpu), and the cpu version was faster. Any idea what the bottleneck could be?", "@yliapis What shape are the tensors you're running top_k on? They're probably just too small to benefit from the GPU op. I've just been using the CPU op, although in my use-case I've found k=1 is often sufficient so in that case I can just use the max op instead.\r\n\r\nOn another note, I tried to run the new GPU op on a tensor of shape [1, 10,000] and got some obscure CUDA error. Haven't had time to track down possible cause or file a bug report yet. I know with thrust I had a lot of fun trying to integrate with TF's memory allocator - as far as I could tell, it doesn't allow kernels to dynamically allocate memory so if CUB is using malloc inside kernels it could just be running out of memory because TF has grabbed it all...", "@ed-alertedh The tensors I am using the op on are generally [256, 20] up to [3600, 20]. Note that I am using tf.map_fn to map a tensorflow operator using top_k to a batch (32-128 usually) of matrices, so the top_k function is getting called batch_sz times by map_fn. I haven't been getting any cuda errors, but the performance makes me question whether the gpu is being properly utilized.", "I guess it just isn't scaling well for that number of rows. Not really what you'd expect given it should be an \"embarassingly parallel\" problem, but maybe you just need to throw a huge number of rows at it to start seeing the benefit. Perhaps concatenating the matrices rather than using map_fn would change performance? Would likely eat up too much memory though....\r\n\r\nHard to say for sure because the benchmarks were all run with 128 rows.  Your case of [256,20] is roughly comparable to the benchmarks on [128,10] and [128,100] which all ran faster on CPU. In case you hadn't found them, benchmark results here: https://github.com/tensorflow/tensorflow/commit/0b5cce367cf95a5d7fbb6b037e7a8646f6c47b70", "@ed-alertedh @ebrevdo I tried top_k by itself on the gpu and the cpu with m=10**6, n=20. The cpu version was twice as fast. Are the rows being sorted in parallel?", "It does seem counter-intuitive that performance would be that much worse than CPU on that many rows...\r\nIf it helps, I was using the following to force a DtoH/HtoD transfer when benchmarking the CPU op so I could better compare CUDA overheads versus the overhead of transferring to CPU:\r\n```\r\nfrom tensorflow.python.platform import benchmark\r\nfrom tensorflow.python.platform import test\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass TopKBenchmark(test.Benchmark):\r\n    def benchmark_topk_gpu(self, dims, k, sorted=False):\r\n        with tf.device('/gpu:0'):\r\n            test_data = np.random.uniform(size=dims)\r\n            data = tf.constant(test_data, dtype=tf.float64)\r\n            k_in = tf.constant(k)\r\n\r\n        with tf.device('/gpu:0'):\r\n            op_under_test = tf.nn.top_k(data, k_in, sorted=sorted)\r\n\r\n        # Creates a session with log_device_placement set to True.\r\n        conf = tf.ConfigProto(log_device_placement=True, allow_soft_placement=False)\r\n        # Don't perform optimizations for tests so we don't inadvertently run\r\n        # gpu ops on cpu\r\n        conf.graph_options.optimizer_options.opt_level = -1\r\n        sess = tf.Session(config=conf)\r\n        with sess:\r\n            return self.run_op_benchmark(\r\n                sess, op_under_test, min_iters=1000, store_trace=False, name=\"op_benchmark\")\r\n\r\n    def benchmark_topk_cpu(self, dims, k, sorted=False):\r\n        with tf.device('/gpu:0'):\r\n            test_data = np.random.uniform(size=dims)\r\n            data = tf.constant(test_data, dtype=tf.float64)\r\n            k_in = tf.constant(k)\r\n\r\n        with tf.device('/cpu:0'):\r\n            op_under_test = tf.nn.top_k(data, k_in, sorted=sorted)\r\n\r\n        # Creates a session with log_device_placement set to True.\r\n        conf = tf.ConfigProto(log_device_placement=True, allow_soft_placement=False)\r\n        # Don't perform optimizations for tests so we don't inadvertently run\r\n        # gpu ops on cpu\r\n        conf.graph_options.optimizer_options.opt_level = -1\r\n        sess = tf.Session(config=conf)\r\n        with sess:\r\n            return self.run_op_benchmark(\r\n                sess, op_under_test, min_iters=1000, store_trace=False, name=\"op_benchmark\")\r\n\r\n\r\nbm = TopKBenchmark()\r\ndims = (10**6, 20)\r\nk=5\r\ncpu_res = bm.benchmark_topk_cpu(dims, k)\r\ngpu_res = bm.benchmark_topk_gpu(dims, k)\r\nprint('***** CPU TIME:')\r\nprint(cpu_res)\r\nprint('***** GPU TIME:')\r\nprint(gpu_res)\r\n```\r\nedit: fixed a couple of typos in code above\r\n\r\nI won't pretend to fully understand the current implementation since I've only skim-read it, but the sort implementation looks like it should be parallel on the rows: https://nvlabs.github.io/cub/structcub_1_1_device_segmented_radix_sort.html", "You don't have that many rows to be fair, you do have a big batch dimension, however.\r\n\r\nTry maybe `(20,10**6)`, so 20 batches a 1 million rows. The GPU topk implementation launches one thread block per batch, so with the original `(10**6,20)`, it will perform a topk on 20 elements over a 1 million batches. Probably not what you want. (Or at least I don't think that is the general use-case).\r\n\r\nThanks,\r\n Andreas", "Yeah, I think it is just inherent in how this problem scales to GPU - \"small\" inputs don't overcome the fixed costs of kernel launch. I hope it doesn't come across that we're criticising your implementation! Just figuring out what the scaling behaviour is.\r\n\r\nFor the sake of clarity, in my above statements I was using the convention [rows, columns] whereas @BlackHC seems to be using the convention [batch size, rows].\r\n\r\nIf it only launches one thread block per batch (=1024 threads on most devices), wouldn't that mean the implementation scales well to large batch sizes and doesn't scale well to large numbers of rows? (using your convention [batch size, rows]).", "There are two different implementations, depending on the input sizes.\nCode is here\n<https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/topk_op_gpu.cu.cc#L545>.\nWe use cub sort followed by copy of the top k for one range of parameters\nand we use @BlackHC's heap for another range.\n\nThere may actually be a bug where we decide to use cub for num_cols <=\n1000; but perhaps that should have been use cub for num_cols >= 1000\ninstead.\n\nOn Wed, Aug 23, 2017 at 6:03 PM, Edward Bordin <notifications@github.com>\nwrote:\n\n> Yeah, I think it is just inherent in how this problem scales to GPU -\n> \"small\" inputs don't overcome the fixed costs of kernel launch. I hope it\n> doesn't come across that we're criticising your implementation! Just\n> figuring out what the scaling behaviour is.\n>\n> For the sake of clarity, in my above statements I was using the convention\n> [rows, columns] whereas @BlackHC <https://github.com/blackhc> seems to be\n> using the convention [batch size, rows].\n>\n> If it only launches one thread block per batch (=1024 threads on most\n> devices), wouldn't that mean the implementation scales well to large batch\n> sizes and doesn't scale well to large numbers of rows? (using your\n> convention [batch size, rows]).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5719#issuecomment-324503437>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9nNvXbjQect7wv9UN3tBW7qt3A8ks5sbMvMgaJpZM4K3RGK>\n> .\n>\n", "Ah right, forgot about that detail. So for [20, 10 ** 6] the heap will be used whereas for [10**6, 20] the cub sort will be used.\r\n\r\nUsing cub for `num_cols<=1000` seems intuitive to me - use a heap for larger problem sizes to save the effort of sorting the whole thing. But once k gets large enough it approaches a heapsort, hence why you'd switch to a more performant sorting algo. For non-parallel versions of the algorithms I believe the complexity comparison is O(n + k log(n)) for heap versus O(n log(n)) for sort. Obviously you can't directly apply that comparison to the parallel algorithms, but you'd expect the heap to become faster at some point where  k << n.\r\n\r\nI think my previous comment still makes sense - the heap implementation currently shards into a maximum of 1024 threads per batch all in one thread block. Each thread builds its own heap on a subset of the data. If the shards were split over multiple blocks you could increase the parallelism. But that would require a parallel merge to gain any benefit and it would be merging between blocks which means you risk getting bottlenecked by global memory. I can see why you'd want to keep it within a single block. If you had large enough num_cols, it would eventually become worth it to use multiple blocks.", "I tried using top_k on multiple GPU's using TensorFlow 1.3.0, and it failed with this message:\r\n\r\n**Cannot assign a device for operation 'gradients/TopKV2_grad/sub_1': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.**\r\n\r\nThis suggests to me that top_k still does not have GPU kernel support, and hence must still run on the CPU. But this thread mentioned that the problem had been solved by @ebrevdo back in 1.3.0rc0. Did I misunderstand?", "Does it work in tf nightlies?\n\nOn Wed, Oct 11, 2017 at 2:26 PM, nitaidean <notifications@github.com> wrote:\n\n> I tried using top_k on multiple GPU's using TensorFlow 1.3.0, and it\n> failed with this message:\n>\n> *Cannot assign a device for operation 'gradients/TopKV2_grad/sub_1': Could\n> not satisfy explicit device specification '/device:GPU:0' because no\n> supported kernel for GPU devices is available.*\n>\n> This suggests to me that top_k still does not have GPU kernel support, and\n> hence must still run on the CPU. But this thread mentioned that the problem\n> had been solved by @ebrevdo <https://github.com/ebrevdo> back in\n> 1.3.0rc0. Did I misunderstand?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5719#issuecomment-335953353>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim3Qc38Gk8PCKhoQ7tcpe-sQz0ieyks5srTKTgaJpZM4K3RGK>\n> .\n>\n", "Just tried it - nope. Same error. Is it supposed to work?", "Peanut gallery chiming in: What datatype is the tensor you're trying to run top_k on? Practically none of the GPU ops are defined for int types (see #9506)", "Yeah, I read that, but no - it's a tensor of floats.", "After looking more closely at the error message, my guess would be that one or more of the ops in the gradient definition [here](https://github.com/tensorflow/tensorflow/blob/c1832374a6a311837f2738e08a9bff8480c8ede4/tensorflow/python/ops/nn_grad.py#L903) can't run on GPU due to the limitation on ints", "Ah, so the problem was never fixed in the first place then? Has anyone else succeeded in getting top_k to run on GPU under any scenario?", "It will still be useful in scenarios where you don't need the gradient. As a workaround I'd suggest enabling soft placement so the gradient ops can be placed on CPU.\r\n\r\nI did get some CUDA kernel launch errors with the new implementation but that would be a totally separate issue. Still haven't had time to prepare a proper bug report (it could have been fixed in the meantime anyway)", "It should work in the nightlies, if you have a full stack trace and minimal\ngraph/code to replicate that would be helpful.\n\nOn Sun, Oct 15, 2017, 8:54 PM Edward Bordin <notifications@github.com>\nwrote:\n\n> It will still be useful in scenarios where you don't need the gradient. As\n> a workaround I'd suggest enabling soft placement so the gradient ops can be\n> placed on CPU.\n>\n> I did get some CUDA kernel launch errors with the new implementation but\n> that would be a totally separate issue. Still haven't had time to prepare a\n> proper bug report (it could have been fixed in the meantime anyway)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5719#issuecomment-336774021>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimx_OTN9_wal9uCLxARrv6Uf42cihks5sstNzgaJpZM4K3RGK>\n> .\n>\n", "OK, looks like you've made significant changes since I last tried it, but I'm still seeing the same behaviour on tf-nightly-gpu==1.4.0.dev20171013. Info below but let me know if I can gather any more - sorry for not reporting earlier!\r\n\r\nMinimal working example for my machine:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nFLAGS = tf.flags.FLAGS\r\n\r\ntf.flags.DEFINE_integer('m', 1, 'batch size')\r\ntf.flags.DEFINE_integer('n', 1, 'data length')\r\ntf.flags.DEFINE_integer('k', 1, 'pick the top k')\r\n\r\ndims = (FLAGS.m, FLAGS.n)\r\n\r\nwith tf.device('/gpu:0'):\r\n    test_data = np.random.uniform(size=dims)\r\n    data = tf.constant(test_data, dtype=tf.float64)\r\n    k_in = tf.constant(FLAGS.k)\r\n    op_under_test = tf.nn.top_k(data, k_in, sorted=False)\r\n\r\nconf = tf.ConfigProto(log_device_placement=True, allow_soft_placement=False)\r\n# Don't perform optimizations for tests so we don't inadvertently run\r\n# gpu ops on cpu\r\nconf.graph_options.optimizer_options.opt_level = -1\r\nwith tf.Session(config=conf) as sess:\r\n    sess.run(op_under_test)\r\n```\r\n\r\nError log for input of size [1, 6140]\r\n[error_log.txt](https://github.com/tensorflow/tensorflow/files/1386529/error_log.txt)\r\nNormal execution for input of size [1,6139]\r\n[normal_execution.txt](https://github.com/tensorflow/tensorflow/files/1386530/normal_execution.txt)\r\n\r\n", "In a [Differentiable Neural Computer (DNC)](https://github.com/deepmind/dnc) model, top_k is used in memory write access, which is in the RNN while loop, and this degrades performance tremendously. I'm looking forward to the implementation of GPU top_k, too.", "I submitted a PR #21436 which attempts to workaround the issue of gradient execution on CPU, waiting for code review. If anybody is still interested...", "Thanks!\n\nOn Tue, Aug 7, 2018, 8:11 AM Jonathan Zhang <notifications@github.com>\nwrote:\n\n> I submitted a PR #21436\n> <https://github.com/tensorflow/tensorflow/pull/21436> which attempts to\n> workaround the issue of gradient execution on CPU, waiting for code review.\n> If anybody is still interested...\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5719#issuecomment-411091264>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0sD81DJzAD0caghdsiFfqMO8iApks5uOa4fgaJpZM4K3RGK>\n> .\n>\n", "FYI the error I reported above was addressed in https://github.com/tensorflow/tensorflow/commit/6bcc00668094be8daf8465b8689bbed5ab285b2d and running the script I posted in the same windows env with tf 1.10 no longer throws an error."]}, {"number": 5718, "title": "Return to initialize_all_variables()", "body": "Return to initialize_all_variables() instead of global_variables_initializer() \r\nGot the following error while using global_variables_initializer:\r\nAttributeError: 'module' object has no attribute 'global_variables_initializer'", "comments": ["Can one of the admins verify this patch?\n", "@roee058, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ilblackdragon, @vincentvanhoucke and @vrv to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "This code is correct at HEAD.  Check out the code at the branch you have installed otherwise.\n"]}, {"number": 5717, "title": "How to install Tensorflow on Python3 (using Bash on Ubuntu on Windows)", "body": "Hi,\r\n\r\nI have Bash on Ubuntu on Windows (on Windows 10) also I have python 2.7, 3.4 and 3.5 installed. When I follow below url to install tensor flow it installs in python 2.7 only.\r\n\r\nhttp://www.hanselman.com/blog/PlayingWithTensorFlowOnWindows.aspx\r\n\r\nI want to install tensorflow on Python 3.5. Please let me know how to install the same. To move to python 3.5, I use python3.5 on the Bash command prompt.\r\n\r\nMany Thanks, Thirumalai M", "comments": ["I'm afraid this is not a supported configuration.  \r\n\r\nPlease consider posting your question to StackOverlfow, or even to Scott Hanselmann's blog."]}, {"number": 5716, "title": "Universal dataset generation toolkit ?", "body": "Is it feasible to add a build-in universal tool to create a TF dataset from raw source data, e.g. `dataset = tf.CreateDataSet(..RAW_SOURCE_PATH..)`, and then can use it like `dataset.train.next_batch, dataset.validation.epochs_completed` etc..", "comments": ["This seems a bit too open ended, so I'm going to close.  I'd recommend using the new input pipelines: https://www.tensorflow.org/api_docs/python/tf/contrib/data."]}, {"number": 5715, "title": "Problems importing Tensorflow to PyCharm and IDLE", "body": "I am currently trying to use Tensorflow on PyCharm and IDLE. However, whenever I try to import Tensorflow (import tensorflow as tf), I get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/leslie/PycharmProjects/untitled/tensor.py\", line 2, in <module>\r\n    import tensorflow as tf\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n  Reason: image not found\r\n```\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nFrom looking around online, it seems that I need to install cuda on my machine. However, I do not have a NVIDIA graphics card nor a cuda-enabled card.\r\n\r\n### Environment info\r\nOperating System:\r\nMac OS 10.12\r\nTensorflow 0.11.0", "comments": ["Please ignore. Somehow I had managed to install the wrong version of Tensorflow. \n"]}, {"number": 5714, "title": "Update mnist_softmax.py", "body": "Move some comments to the right place.", "comments": ["@AfirSraftGarrier, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @zheng-xq to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please."]}, {"number": 5713, "title": "Update r0.12 branch", "body": "Merge important updates into the release branch.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "LGTM. Feel free to merge if you don't want to wait for Jenkins.\n", "Looks like mac0-slave needs portpicker installed.\n", "Please include https://github.com/tensorflow/tensorflow/pull/5645 as well, fully-connected layer in contrib.layers is not Python 3 compatible in 0.12 branch right now.\n", "#5645 is in the r0.12 branch, so it'll appear in the release.\n\nMerging:\n- CLAs are covered by the master branch\n- Mac failures happen only on that machine because it's missing portpicker, so I'm ignoring them\n", "mac0 is now taken care of.\nThanks for merging, Jonathan.\n\nNow 0.12 branch is locked. :)\n"]}, {"number": 5712, "title": "Documentation for NowSeonds and NowMillis methods have been updated.", "body": "Unlike the \"NowMillis\", the new method guarantees to return the time since the epoch. This is the fix for #5682 . Also, the fix should unblock the implementation of the feature request: #2076 .", "comments": ["Can one of the admins verify this patch?\n", "@b0noI, thanks for your PR! By analyzing the history of the files in this pull request, we identified @rinugun, @josh11b and @vinuraja to be potential reviewers.\n", "Not sure I'm the best person to review the core change here, which is env.h file. Reassigning to @jhseu ", "jhseu@ commit has been updated accordingly (with new documentation).", "Jenkins, test this please", "jhseu@ I hope breakage are not caused by my docs update and we still can merge it.", "Yeah, it can't be related. @danmane feel free to ignore the failures and merge.", "Merging based on jhseu's comment."]}, {"number": 5711, "title": "Add Pillow installation into Docker build file.", "body": "@caisq this is a new PR related to [PR #5693 install Pillow in Dockerfile](https://github.com/tensorflow/tensorflow/pull/5693). 2 commits are incorprated:\r\n1. add Pillow installation into tensorflow/tools/docker/Dockerfile\r\n2. add Pillow installation into tensorflow/tools/docker/Dockerfile.gpu\r\n\r\nthe following information for image size comparison:\r\n\r\n**gpu version image size drops, i guess latest-gpu tagged image is too old.**\r\n\r\n> REPOSITORY                     TAG                 IMAGE ID            CREATED             SIZE\r\n> gcr.io/tensorflow/tensorflow   gpu                 8ca00ec56643        8 minutes ago       2.658 GB\r\n> gcr.io/tensorflow/tensorflow   latest-gpu          dd645f420f1d        10 days ago         2.713 GB\r\n\r\n**cpu version image size increases about 22MB < 2.5% up**\r\n\r\n> REPOSITORY                     TAG                 IMAGE ID            CREATED             SIZE\r\n> gcr.io/tensorflow/tensorflow   Pillow              dff6ac3189bd        17 hours ago        981.6 MB\r\n> gcr.io/tensorflow/tensorflow   latest              5547120ff897        10 days ago         959.6 MB\r\n\r\nHao", "comments": ["Can one of the admins verify this patch?\n", "@foreverfaint, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @caisq and @vrv to be potential reviewers.\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@googlebot try again, hope it works now.\n", "@tensorflow-jenkins test this please\n", "@foreverfaint , can you look into the CLA issue again. I'll merge the PR as soon as the CLA is good. Thanks.\n", "@caisq, is it still not working for CLA? previously i typo a wrong name in my local git config for email part \" foreverfailt@gmail.com\". For resolving this issue, i did create a gmail for foreverfailt@gmail.com, and signed CLA by that email too. so for now i have the signed CLA for both author names:\n- foreverfaint@gmail.com\n- foreverfailt@gmail.com\n  so what is a further step i should follow?\n", "I guess the reason why CLA doesn't work is the second commit:\n\ninstall Pillow in Dockerfile\nroot committed ....\n\nYou used user root to do the commit, which the CLA bot doesn't recognize. Maybe open another PR to include one commit with the correct user name?\n", "sure. let me do it again. close this one\n"]}, {"number": 5710, "title": "Branch 139652213", "body": "", "comments": ["Jenkins, test this please\n", "@rmlarsen @gunan fyi\n"]}, {"number": 5709, "title": "Add a GPU build script for windows.", "body": "@guschmue FYI", "comments": ["Jenkins, test this please.\n"]}, {"number": 5708, "title": "Branch 139650649", "body": "", "comments": []}, {"number": 5707, "title": "AttributeError: type object 'NewBase' has no attribute 'is_abstract'  when I have update SIX", "body": "Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 98, in <module>\r\n    from tensorflow.python.platform import test\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/test.py\", line 63, in <module>\r\n    from tensorflow.python.framework import test_util\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/framework/test_util.py\", line 43, in <module>\r\n    from tensorflow.python.platform import googletest\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/googletest.py\", line 32, in <module>\r\n    from tensorflow.python.platform import benchmark  # pylint: disable=unused-import\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/benchmark.py\", line 122, in <module>\r\n    class Benchmark(six.with_metaclass(_BenchmarkRegistrar, object)):\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six.py\", line 566, in with_metaclass\r\n    return meta(\"NewBase\", bases, {})\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/benchmark.py\", line 117, in __new__\r\n    if not newclass.is_abstract():\r\nAttributeError: type object 'NewBase' has no attribute 'is_abstract'\r\n", "comments": ["Please could you provide the information requested in the issue reporting template.", "Closing due to lack of response."]}, {"number": 5706, "title": " change validation_metrics to MetricSpec", "body": "tensorflow:Please specify metrics using MetricSpec. Using bare functions or (key, fn) tuples is deprecated and support for it will be removed on Oct 1, 2016.\r\nso run the previous code will get a mistake!", "comments": ["@BoyuanJiang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @jart and @MrQianJinSi to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "@ispirmustafa  @tensorflow-jenkins  hey,it's my first time to use github PR,so what should I do next to complete those six checks bellow and merge the changes?", "@BoyuanJiang When tests pass it's good to merge. @tensorflow-jenkins Test this please", "I test the changes on my local machine using Python3.5 correctly,so I'm not sure why it failed in Linux CPU Tests (Python 3)  T_T", "@BoyuanJiang it seems like it's failed independently of your change.\r\nLet's retest and merge.\r\n@tensorflow-jenkins Test this please", "Pretty confident the failure is unrelated (looks like a platform-specific timing issue). Merging, thanks!"]}, {"number": 5705, "title": "bazel 0.4 support for tensorflow", "body": "I am trying to build the tensorflow from source. I get errors like \"bazel-out/host/bin/external/grpc/grpc_cpp_plugin: program not found or is not executable\". But builds successfully. If bazel 0.4 is not supported, its a documentation bug here https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html to mention that bazel latest version should be downloaded. I could successfully build with bazel 0.3.x.", "comments": ["I successfully built with 0.4 yesterday -- is this still a problem at head? Or is it 0.11 only?", "Sorry for the delayed reply. I was using a commit from head(October 10) https://github.com/tensorflow/tensorflow/commit/8e48ec6ea0492e2cb9fd19c0a2ccf41afc7b4dc6\r\n", "Building with bazel 0.4 works on tf 0.11."]}, {"number": 5704, "title": "null", "body": "set to null", "comments": []}, {"number": 5703, "title": "Untag scatter_nd_ops_test now it's been fixed by CL/139592087", "body": "", "comments": ["@caisq, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @ebrevdo and @gunan to be potential reviewers.\n"]}, {"number": 5702, "title": "Tensorboard interactive demo is down. ", "body": "https://www.tensorflow.org/tensorboard/index.html retuns:\r\n\r\n> Error: Not Found\r\n> The requested URL /tensorboard/index.html was not found on this server.", "comments": ["The demo was deliberately turned off.\r\nThe new website is supposed to display a nice message explaining that the demo is gone, but transition to the new website has been slow. I'll check if we're going to switch soon, and if not we'll put a better warning message."]}, {"number": 5701, "title": "Quantization tool is gone?", "body": "if you run **bazel build tensorflow/contrib/quantization/tools:quantize_graph** with the latest code, it report error: can't not find the files.\r\n\r\nif you go to **https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantization** folder, the files are not here already and there is no **tools** folder.\r\n\r\n", "comments": ["oh, I just see it goes to another folder, tensorflow/tensorflow/tools/quantization/ \n"]}, {"number": 5700, "title": "Definition of epoch_size in PTB model leads to skipping last sample batch", "body": "Not 100% sure this is a bug, please take a look:\r\n\r\nThe PTB RNN model (tensorflow/models/rnn/ptb/ptb_word_lm.py, line 92) defines a variable called epoch_size as \r\n\r\nself.epoch_size = ((len(data) // batch_size) - 1) // num_steps\r\n\r\n(This variable is also defined in the same way in tensorflow/models/rnn/ptb/reader.py, line 108.) The variable is used on line 278 in ptb_word_lm.py:\r\n\r\nfor step in range(model.input.epoch_size):\r\n\r\nI'm not sure why the \"- 1\" term is in the equation -- that is, I think it should be just\r\n\r\nself.epoch_size = (len(data) // batch_size) // num_steps\r\n\r\nThe equation already uses floor division to round down. Currently, if both batch_size and num_steps are 1, epoch_size will be equal to data length - 1, so the last data sample will be skipped.", "comments": ["Thanks for bringing this up, but I don't think there's a bug here: the issue is that the data set is split up so that (k-1) words predict the k'th word. So the -1 in that expression leaves room for the label."]}, {"number": 5699, "title": "Use passthrough args rather than wrapper functions", "body": "Googlers can look see [this doc](https://docs.google.com/document/d/1gVcn_r5njuEOEZDXXW3HubWqAGJDs4M5APX1d_JLr4o/edit#heading=h.mgiweq7i3oas) for an explanation of why this change is being made.\r\n\r\nThis change was made in a backwards compatible way. Other changes referenced in that doc will likely depend on this change but not be backwards compatible. ", "comments": ["Can one of the admins verify this patch?\n", "@elibixby, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @ilblackdragon and @martinwicke to be potential reviewers.\n", "@elibixby this will need to be rebased", "@elibixby is this still current? We need to merge conflicts.", "@elibixby Are we still doing this?", "We are in the middle of a redesign. You can close this, we're aware of it\nand can use it as a prototype.\n\nOn Sun, Jan 15, 2017 at 11:25 drpngx <notifications@github.com> wrote:\n\n> @elibixby <https://github.com/elibixby> Are we still doing this?\n>\n>\n>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/5699#issuecomment-272717256>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAjO_ZtXWKMWxxqkYvYLlSbsNCDOQBLQks5rSnKcgaJpZM4K20H5>\n> .\n>\n"]}, {"number": 5698, "title": "[Windows/CMake] Avoid file locking race in protoc.", "body": "In particular, we avoid regenerating the C++ protobuf implementations\r\nfor `tensorflow/core` proto files, because this can trigger a flaky\r\nrace when run on Windows.\r\n\r\nAnother reminder of why hermetic builds are delightful, I suppose :).", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @danmane and @martinwicke to be potential reviewers.\n"]}, {"number": 5697, "title": "Branch 139585378", "body": "Internal push.", "comments": ["@rmlarsen, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @langmore to be potential reviewers.\n", "@mrry @gunan would you mind taking a look at the Windows cmake test failure. I had a merge conflict when pushing and may have made a mistake.\n", "@rmlarsen Looks like a flake due to some file locking issues. I'm about to send a PR but in the meantime...\n\n@tensorflow-jenkins test this please.\n", "@mrry thanks for looking Derek. That particular test passed the second time around. I'm merging this.\n"]}, {"number": 5696, "title": "API pages could use some search optimization", "body": "When I search on Google for Tensorflow methods, the documentation is hard to distinguish in the search results. The API pages have irrelevant titles and aren't as highly ranked as they should be.\r\n\r\nTo reproduce:\r\nSearch for [tf extract_image_patches]\r\nExpected behavior: I get a result with the API documentation: https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html\r\nObserved behavior: this page shows up as result 3 with the mysterious title \"tf.pack - TensorFlow\"\r\n\r\nAnother example is EveryN. If I add enough keywords, I get the API page as result 4 (search on [tf EveryN api tf.contrib.learn]). But it has the inscrutable title \"tf.contrib.learn.monitors.ExportMonitor.run_on_all_workers - TensorFlow\"\r\n\r\nThis makes it difficult to find the documentation for methods.\r\n\r\nSuggested fix: maybe make the page titles more descriptive, and add the word \"API\"? Maybe have separate pages for each class, like JavaDocs?\r\n\r\n", "comments": ["I concur, search could use improvement. For example, I go to main API page and search for \"segment_max\" -- https://www.tensorflow.org/s/results/?q=segment_max&p=%2F\r\n\r\nThis gives \"segment_max\" as second result, but when you go there, it has broken images/broken links because it's actually for version 0.10", "Another example is \"dynamic_rnn\" -- [https://www.tensorflow.org/s/results/?q=dynamic_rnn&p=%2F](https://www.tensorflow.org/s/results/?q=dynamic_rnn&p=%2F)", "One more example:\r\n\r\n![image](https://cloud.githubusercontent.com/assets/15331/23146994/ad180d1c-f7a9-11e6-9804-7011a1143c12.png)\r\n\r\nEach option for `Initializers` links to a different API version.", "For some things, gitbook search works better. Latest gitbook is at https://haosdent.gitbooks.io/tensorflow-document/content/api_docs/ and tracking issue for gitbook updates is https://github.com/tensorflow/tensorflow/issues/3810", "Closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest documentation. Thank you."]}, {"number": 5695, "title": "strange output and jam while running \"train_setp.run()\" in tensorflow", "body": "I am a newcomer of Tensorflow and I encounter trouble while trying to train a convolutional neuralnetwork. I receive massive output information which seems initializing something while running:\r\n**train_step.run(session=sess,feed_dict={x:batch[0],y_:batch[1],keep_prob:0.5})**\r\nWhat is more, while testing the accuracy of my training data, the program output some information and stop working, so I have to stop it with \"ctrl+c\" \r\nI think there is something wrong with \"sess\"  but I do not know how to fix them. Part of my code are listed below:\r\nto emphasize the places I think are important and help you understand it quickly, I use # in my code and (  )in my output to express what  I think and mark some vital command by ->\r\n```\r\n->sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\r\n# I initialized my sess\r\nfor d in ['/gpu:0']:\r\n    with tf.device(d):\r\n        x=tf.placeholder(tf.float32,shape=[None, 224, 224, 1])\r\n        y_=tf.placeholder(tf.float32,shape=[None,3])\r\n      ->vggNet=vgg16( x, weights='vgg16_weights.npz', sess=sess)\r\n         # vgg16 is a Class to define and initialize neuralnetwork\r\n        cross_entropy = -tf.reduce_sum(y_*tf.log(tf.clip_by_value(vggNet.probs,1e-10,1.0)))\r\ntrain_step=tf.train.AdamOptimizer(l_rate).minimize(cross_entropy)\r\ncorrect_prediction=tf.equal(tf.argmax(vggNet.probs,1),tf.argmax(y_,1))\r\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,\"float\"))\r\n->sess.run(tf.initialize_all_variables())\r\nprint ('\\n\\n\\n session run successfully ! \\n\\n\\n')\r\n#I think after this, there should not be any initialize work\r\nfor i in range (20000):\r\n    batch=octData.train.next_batch(train_batch_size)\r\n    #load data to batch\r\n    if (i+1)%5==0:\r\n        print '#',\r\n        #this has been printed\r\n    if (i+1)%20==0:\r\n        print ('evaluating train dataset ...')\r\n        #this has been printed but the program get stuck after some output\r\n        train_accuracy = sess.run(accuracy,feed_dict={x:batch[0], y_: batch[1], vggNet.keep_prob: 1.0})\r\n        #after this my program jammed, the following print do not work\r\n        print \"  step %d, training accuracy %g\"%(i,train_accuracy)\r\n   ->train_step.run(session=sess,feed_dict={x:batch[0],y_:batch[1],vggNet.keep_prob:0.5})\r\n    #I think all the strange output are printed by this,vggNet.keep_prob is a parameter to control drop out in fc layer\r\n```\r\n\r\nAccording to the tutorial of tensorflow, the output in the loop **for i in range (20000):** should be what I print in the program. However, the following information are presented to me.\r\n\r\n```\r\nsession run successfully !   \r\n(I believe this is the printed info at previous line before the loop **for i in range (20000):**)\r\n\r\n\r\nAdam/epsilon: /job:localhost/replica:0/task:0/gpu:0\r\n. . .  . . . (I omit some similar output there )\r\ngradients/clip_by_value/Minimum_grad/Shape_1: /job:localhost/replica:0/task:0/gpu:0\r\n. . .  . . . \r\nfc3/weights/read: /job:localhost/replica:0/task:0/gpu:0\r\n. . .  . . .\r\nI tensorflow/core/common_runtime/simple_placer.cc:819] conv5_3/biases/read: /job:localhost/replica:0/task:0/gpu:0\r\n. . .  . . .\r\nconv5_3/biases/read: /job:localhost/replica:0/task:0/gpu:0\r\n. . . . . . \r\n. . . . . . \r\n# # # # evaluating train dataset ...\r\n(I think the program is about to calculate the accuracy of training set)\r\n. . . . . .\r\nfc3/biases/read: /job:localhost/replica:0/task:0/gpu:0\r\n. , . . . .\r\nI tensorflow/core/common_runtime/simple_placer.cc:819] fc1/Reshape/shape: /job:localhost/replica:0/task:0/gpu:0\r\n```\r\nAnd after that my program got jammed. There is nothing wrong with the batch, which is used to read data and it has nothing to do with tensorflow. I think **sess** is the most likely cause of the problem. Do anybody know what is going on and how to fix it?", "comments": ["This is a general TensorFlow usage question best suited to StackOverflow.  Please can you re-ask your question there.\r\n"]}]