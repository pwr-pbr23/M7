[{"number": 45579, "title": "\"SAME\" padding for avg_pool2d yields different results than explicitly 0-padded input with \"VALID\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.8.5\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using padding \"SAME\" with average pooling, the results on a small input look wrong. If I use average pooling on a completely empty matrix with only 1 value of the matrix at  1, using the \"SAME\" padding algorithm I don't get a correct average value. If I explicitly 0-pad the input and use \"VALID\", I get the correct value. \r\n\r\n**Describe the expected behavior**\r\n\r\nThe behavior should be the same using :\r\n* an explicitly 0-padded matrix with `tf.pad` and the \"VALID\" algorithm\r\n* The \"SAME\" padding algorithm\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n**\"SAME\"**\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ngenerated = np.zeros((4,4))\r\ngenerated[2,1] = 1\r\ngen_nhwc = tf.constant(generated[np.newaxis,:,:,np.newaxis])\r\npool = 3\r\nres = tf.nn.avg_pool2d(gen_nhwc, (pool,pool), (1,1),\"SAME\")\r\nresult = np.squeeze(res.numpy())\r\nprint(\"Input:\\n\",generated)\r\nprint(\"Output\\n\",result)\r\n```\r\n```\r\nInput:\r\n [[0. 0. 0. 0.]\r\n [0. 0. 0. 0.]\r\n [0. 1. 0. 0.]\r\n [0. 0. 0. 0.]]\r\nOutput\r\n [[0.         0.         0.         0.        ]\r\n [0.16666667 0.11111111 0.11111111 0.        ]\r\n [0.16666667 0.11111111 0.11111111 0.        ]\r\n [0.25       0.16666667 0.16666667 0.        ]]\r\n```\r\n\r\n**`tf.pad` and \"VALID\" results:** \r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ngenerated = np.zeros((4,4))\r\ngenerated[2,1] = 1\r\ngen_nhwc = tf.constant(generated[np.newaxis,:,:,np.newaxis])\r\npool = 3\r\npaddings = [[0,0],[pool//2,pool//2],[pool//2,pool//2],[0,0]]\r\ngen_pad = tf.pad(gen_nhwc, paddings, \"CONSTANT\")\r\nres = tf.nn.avg_pool2d(gen_pad, (pool,pool), (1,1),\"VALID\")\r\nresult = np.squeeze(res.numpy())\r\nprint(generated)\r\nprint(result)\r\n```\r\n\r\n```\r\nInput:\r\n [[0. 0. 0. 0.]\r\n [0. 0. 0. 0.]\r\n [0. 1. 0. 0.]\r\n [0. 0. 0. 0.]]\r\nOutput\r\n [[0.         0.         0.         0.        ]\r\n [0.11111111 0.11111111 0.11111111 0.        ]\r\n [0.11111111 0.11111111 0.11111111 0.        ]\r\n [0.11111111 0.11111111 0.11111111 0.        ]]\r\n```\r\n", "comments": ["I have tried in colab with TF 2.2, 2.3 and nightly version(`2.5.0-dev20201210`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d100d4b85685c6ccf0ac70182aeab3e9/untitled574.ipynb).Thanks!", "Thanks for creating issue, thought about doing so as well according to my yesterdays question on SO ([Question](https://stackoverflow.com/questions/65231630/understanding-average-sum-pooling-padding-in-keras/65237472)). This happened to me in colab with TF 2.3 as well as at windows 10, python 3.6 with TF 2.2 and 1.14, seem to be a bug across all versions, can somebody confirm?", "Was able to Reproduce the issue in TF 2.5 as well. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/bccaa47ea61ecd98ce5dfa6d15e4d6ed/untitled88.ipynb).Thanks!"]}, {"number": 45568, "title": "Using mixed precision causes incorrect loss, metric values", "body": "Tensorflow version : 2.4.0-rc3, compiled from source\r\nGPU : RTX 3080 10GB\r\nCUDA / CUDNN : 11.1 / 8\r\nbazel version : 3.1.0\r\nWindows 10\r\n\r\nI decided to mixed precision to speed up the training, but some issues were.\r\nI used this code:\r\n```\r\npolicy = keras.mixed_precision.Policy('mixed_float16')\r\nkeras.mixed_precision.set_global_policy(policy)\r\n\r\n...\r\n\r\nmodel.compile(optimizer = keras.mixed_precision.LossScaleOptimizer(optimizers.Adam(learning_rate=1e-2))\r\n                        loss = dice_loss\r\n                        metric = [dice_coefficient]\r\n                                                )\r\n```\r\n**Training didn't show any nan loss and metric. But when evaluating the model or each epoch ended, val loss and metric were incorrect(they were fixed over epochs).** The colab notebook is **[here](https://colab.research.google.com/drive/1kLpn88HSn7WOnH5vN5l-GlLRTPVArsDw?usp=sharing)**\r\n\r\netc:\r\n- The values of `model.predict(x)` and `model(x)` are different. When the former was nan, the latter seemed normal.", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45568\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45568\">No</a>\n", "On running the code with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/f32019942de7a6cba160e8ffb5b2dfd1/45568-2-2.ipynb#scrollTo=kM-cUF1sR-_k), the `val_loss` value is `1.0001`.\r\n\r\nWas able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/25afe63903c66dc02a2a1a90b6482ca3/45568.ipynb#scrollTo=kM-cUF1sR-_k), with which the `val_loss` is `nan`.\r\n\r\nWhereas with TF v2.4.0-rc3 and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/e3aa6bab29586bb0449e56d48ebeeb27/45568-tf-nightly.ipynb#scrollTo=kM-cUF1sR-_k), the code runs indefinitely. Please check the linked gist for reference. Thanks!", "same issue here. Please let me know once this is resolved.", "@Crispy13 I have figured out the bug. You need to explicitly cast the softmax Dense layer to dtype=tf.float32 to avoid numerical instability. Thanks to the source here: https://wandb.ai/site/articles/mixed-precision-training-with-tf-keras", "As @amahendrakar  mentioned above, the code runs indefinitely in TF 2.5. Please check the linked gist for [reference](https://colab.research.google.com/gist/saikumarchalla/b125e6a6224324e84a5385dc2e07d3d7/45568-tf-nightly.ipynb#scrollTo=kM-cUF1sR-_k&uniqifier=1). Thanks!", "I have the same issue:\r\nTF 2.5\r\nRTX A5000 GPU\r\nUbuntu 20.04\r\n\r\nI'm training a Keras implementation of Tiramisu, when using a Dice loss, I have to use float 32 in all inputs and outputs and I cannot use float16 mixed precision training. I tried things like lower lr and gradient normalisation.  Also tried the solution above of casting my softmax to float32.  But as soon as I go to float16 for the inputs , training loss and metric go to nan.  It's not a fatal error. But not using the tensor cores slows down the training by a factor of almost 2.\r\n\r\nIs there a solution to this?"]}, {"number": 45557, "title": "Cannot load SavedModel when model trained with tfa.optimizers.NovoGrad: ValueError: Shapes (3, 8) and () are incompatible", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.6.10\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI'm training a model with the `tfa.optimizer.NovoGrad` (https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/NovoGrad). The model appears to train fine, but when I try to load the model using `tf.keras.models.load_model(\"my_model\")`, I get a `ValueError` regarding the shapes. A minimal example to replicate this behavior is provided below:\r\n\r\n```python\r\n# Train a small model and save it as a SavedModel\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(8))\r\nmodel.add(tf.keras.layers.Dense(2))\r\nmodel.compile(optimizer=tfa.optimizers.NovoGrad(), loss=tf.keras.losses.sparse_categorical_crossentropy)\r\ncallbacks = [tf.keras.callbacks.ModelCheckpoint(\"scratch\")]\r\n\r\nx = np.random.random((2, 3))\r\ny = np.random.randint(0, 2, (2,))\r\nmodel.fit(x, y, batch_size=1, epochs=1, callbacks=callbacks)\r\n```\r\nNow close the python session, open a new one, and try to load the model:\r\n```python\r\nimport tensorflow as tf\r\nmodel = tf.keras.models.load_model(\"scratch\")\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\", line 187, in load_model\r\n    return saved_model_load.load(filepath, compile, options)\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 121, in load\r\n    path, options=options, loader_cls=KerasObjectLoader)\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 633, in load_internal\r\n    ckpt_options)\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 194, in __init__\r\n    super(KerasObjectLoader, self).__init__(*args, **kwargs)\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 131, in __init__\r\n    self._restore_checkpoint()\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 328, in _restore_checkpoint\r\n    self._checkpoint_options).expect_partial()\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py\", line 1320, in restore\r\n    checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 209, in restore\r\n    restore_ops = trackable._restore_from_checkpoint_position(self)  # pylint: disable=protected-access\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 908, in _restore_from_checkpoint_position\r\n    visit_queue=visit_queue))\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 943, in _single_restoration_from_checkpoint_position\r\n    if child_position.bind_object(trackable=local_object):\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 258, in bind_object \r\n    slot_name=slot_restoration.slot_name)\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 1236, in _create_or_restore_slot_variable\r\n    slot_variable_position.restore(slot_variable)\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 209, in restore\r\n    restore_ops = trackable._restore_from_checkpoint_position(self)  # pylint: disable=protected-access\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 914, in _restore_from_checkpoint_position\r\n    tensor_saveables, python_saveables))\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py\", line 297, in restore_saveables\r\n    validated_saveables).restore(self.save_path_tensor, self.options)\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/saving/functional_saver.py\", line 340, in restore\r\n    restore_ops = restore_fn()\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/saving/functional_saver.py\", line 316, in restore_fn\r\n    restore_ops.update(saver.restore(file_prefix, options))\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/saving/functional_saver.py\", line 111, in restore\r\n    restored_tensors, restored_shapes=None)\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 127, in restore\r\n    self.handle_op, self._var_shape, restored_tensor)\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 311, in shape_safe_assign_variable_handle\r\n    shape.assert_is_compatible_with(value_tensor.shape)\r\n  File \"/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 1134, in assert_is_compatible_with\r\n    raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\nValueError: Shapes (3, 8) and () are incompatible\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe saved model should be loaded without error. \r\n\r\n**Standalone code to reproduce the issue**\r\nStandalone code provided above.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I am able to replicate the issue reported, please find the gist here for [nightly](https://colab.research.google.com/gist/Saduf2019/4493bdcf3fc650096ae65c57beeb6254/untitled480.ipynb),  [tf 2.4](https://colab.research.google.com/gist/Saduf2019/1012ccdc5616fcef7353733e2b118e4c/untitled481.ipynb) and [tf 2.3](https://colab.research.google.com/gist/Saduf2019/1106d41b11f8fcf33b634e8c2ddc17c6/untitled481.ipynb)", "@Feynman27  I tried to reproduce the issue but I am facing error while importing tensorflow add on. Please check this [gist](https://colab.research.google.com/gist/saikumarchalla/ccb52fcf065ff069f8484c91205bc618/untitled88.ipynb).Thanks!", "@Feynman27 Sorry, this is still an issue. [Here](https://colab.research.google.com/gist/jvishnuvardhan/372dc291720c6c66e0cf52350a5070c0/untitled88.ipynb) is the updated gist for our reference. Thanks!", "I'm having the same issue. Is there any update to this?", "@k-w-w Please take a look at this issue? Thanks!", "A workaround is to load the checkpoint compile with no optimizer and save the weights back to the checkpoint file. If you load from it after that it won't crash. It does delete the optimizer state though."]}, {"number": 45481, "title": "TF-TRT Dynamic Shapes Feature Tracker", "body": "### Introduction\r\nThe dynamic shape mode in TF-TRT utilizes TensorRT\u2019s [dynamic shape feature](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work_dynamic_shapes) to improve the conversion rate of networks and handle networks with unknown input shapes efficiently. This issue tracks the ongoing development to enable TRT's dynamic shape mode trough TF-TRT.\r\n\r\n**Who will benefit with this feature?**\r\nThe conversion rate and therefore the performance will improve for the following inference problems:\r\n- Network with unknown input shapes (e.g. fully convolutional object detection networks)\r\n- Networks where the first (batch) dimension of a tensor changes within the graph (e.g. BERT)\r\n- Networks that have subgraphs where the tensors have non identical first dimension\r\n\r\nAdditionally the memory usage will improve: to handle input tensors with different shapes (eg image size, sequence length) currently requires separate TRT engine creation for each input. With dynamic shape mode a single engine can handle various input shapes.\r\n\r\n**Will this change the current api? How?**\r\n\r\nSome change in the [conversion parameters](https://github.com/tensorflow/tensorflow/blob/b34e7144ed6dd0ab51c03df783aa14009589e800/tensorflow/python/compiler/tensorrt/trt_convert.py#L118) will be necessary to enable/disable dynamic shape mode and provide a way to select optimization profiles.\r\n\r\n### Phase 1\r\nThe first phase of this work is the basic scaffolding to enable the TF-TRT converter to use TRT's dynami shape API.\r\n\r\n0. Add implicit batch experimental #34293\r\n1. Enable explicit batch mode #36379\r\n2. Improve binding index query #36434\r\n3. Add binding size specification #36435\r\n4. Define networks with dynamic shapes #36439\r\n5. Add optimizaton profiles #36660\r\n6. Execution context managment #36664\r\n7. TensorRT profile generation mode #36729\r\n\r\n### Phase 2\r\nEnable dynamic shape mode for ops used in MobileNet, ResNet, Bert. This includes improvement in the converters plus increasing their unit test coverage. Note that at this stage dynamic shape mode is still experimental.\r\n\r\n- Refactor ExecuteTrtEngine #38118 \r\n- Use Unified Memory in TRT opconverter tests  #38124\r\n- ConvertSqueeze #38146\r\n- ConvertUnary, ConvertRsqrt #39153\r\n- ConvertTranspose #39151 \r\n- ConvertActivation, ConvertLeakyRelu #39155 \r\n- BiasAdd - ConvertBiasAdd #39156\r\n- Conv2D, DepthWiseConv2dNative #39204\r\n- ConvertConv2dBackPropInput #47840\r\n- ConvertPack #39859\r\n- ConvertReshape #40545\r\n- ConvertExpandDims  #39282\r\n- ConvertSlice, ConvertStridedSlice #40736\r\n- ConvertGather #39848\r\n- ConvertMatMul and ConvertBatchMatMul #47215\r\n- ConvertBinary #39785\r\n- ConvertSquaredDifference #39758\r\n- ConvertReduce #40201\r\n- ConvertFusedBatchNorm #40179\r\n- ConvertSoftMax  #47039\r\n- ConvertShape #39990\r\n- ConvertPool #40184\r\n- Update related python integration tests: Unary_test.py, Batch_matmul_test.py, biasadd_matmul_test.py, Conv2d_test.py, reshape_traspose_test.py\r\n\r\n### Phase 3\r\nThis is a direct continuation of Phase 2. Ensure that all op converter support dynamic shape mode and test it. We have almost 20 converters to update and test. The bulk of this work is improving the test coverage. \r\n- ConvertSquare #40483\r\n- ConvertClipByValue #45589 \r\n- ConvertPad #45597\r\n- ConvertResize #46376\r\n- ConvertArgMinMax #45862 \r\n- ConvertQuantize #45599\r\n- ConvertTopK #46299\r\n- ConvertUnpack #48049\r\n- ConvertAddN #46675 \r\n- ConvertConcat #46382\r\n- ConvertConv3D #46940\r\n- ConvertDepthSpaceShuffle #47590\r\n- ConvertSplit #48246\r\n- ConvertCombinedNMS #40062\r\n- Enable calibration if static input shapes are used #48244\r\n\r\n### Phase 3+\r\nSome converters in phase 3 were updated only for explicit batch support with static shape. Enable dynamic shape mode for them:\r\n- ConvertCombinedNMS\r\n- ConvertResize #51462\r\n- ConvertStridedSlice #51475\r\n- ConvertConv2DBackpropInput #51468\r\n\r\nAdditionally:\r\n- Fix shape output bug https://github.com/tensorflow/tensorrt/issues/251 \r\n  #52181 \r\n  #52186\r\n- Update python integration tests #51411 #51471\r\n\r\n### Phase 4\r\n Implement calibration in dynamic shape mode. Using TRT 7.1 one can  run calibration in [dynamic shape mode](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#int8-calib-dynamic-shapes).\r\n- TRTEngineOp: allow profile collection before calibration\r\n- Refine APIs: build mode + calibration + lazy calibration\r\n\r\n### Phase 5\r\nTest performance of dynamic shape mode\r\n\r\n### Phase 6\r\nDefine API to enable dynamic shape and specify optimization profiles.\r\n- Finalize API - [implemeted here](https://github.com/tensorflow/tensorflow/blob/e52ccf29b87d7262078040b86a0fb4669149cb05/tensorflow/python/compiler/tensorrt/trt_convert.py#L944-L987)\r\n- Implement additional optimization profiles\r\n  - basic profiles for testing #45588 \r\n  - Pass profile params to TrtShapeOptimizationProfiles class #48060\r\n  - Implement Range, Range+Optimal profiles #48414\r\n\r\n### Phase 7\r\n- C++ conversion API #52012\r\nOptional elements from Phase 6 are moved here.\r\n- Implement UserDefined profile \r\n- Change default conversion param from implicit batch mode to dynamic shape mode\r\n\r\nTagging @DEKHTIARJonathan and @bixia1 ", "comments": ["Hi tfeher, About 'Networks where the first (batch) dimension of a tensor changes within the graph (e.g. BERT)'.\r\nWhy would the first dimension change within the graph? I would assume that once the batch size is determined, the shape won't change throughout graph of BERT. \r\n", "> Why would the first dimension change within the graph?\r\n\r\nSome networks do reshape operations that change the first dim. It is done internally to express some operations more conveniently, the output is usually reshaped again to have the expected batch size. An example is [BERT TF1 model](https://github.com/NVIDIA/DeepLearningExamples/blob/c481324031ecf0f70f8939516c02e16cac60446d/TensorFlow/LanguageModeling/BERT/modeling.py#L674). \r\n\r\nThe dynamic shape feature of TF-TRT improves the TRT conversion of such networks.", "Hi, @tfeher can tf-trt dynamic_batch support the situation of being divided into multiple trt subgraphs now? \r\nAnd how should we set the shape information of the min, max, and opt of the internal subgraph?\r\n\r\n![image](https://user-images.githubusercontent.com/26377421/127456355-b87ee9f3-2ccf-4bc9-a92f-c98aaf0fc4fe.png)\r\n", "> can tf-trt dynamic_batch support the situation of being divided into multiple trt subgraphs now?\r\n\r\nYes, dynamic shape mode supports graphs that have multiple TRT subgraphs. There is a known issue https://github.com/tensorflow/tensorrt/issues/251, which occurs if the trt_engine_op is trying to output shape tensors. Otherwise it should work.\r\n\r\n> And how should we set the shape information of the min, max, and opt of the internal subgraph?\r\n\r\nYou only need to set the shape information for the inputs of the model. From these we calculate the size of any tensor in the graph, and set input shape information for the internal TRT subgraphs (`trt_engine_ops`).\r\n\r\n\r\n", "PR for Conv2dBackpropInput https://github.com/tensorflow/tensorflow/pull/51468"]}, {"number": 45465, "title": "How to unset changes made by tf.debugging.set_log_device_placement?", "body": "\r\n## URL(s) with the issue:  https://www.tensorflow.org/api_docs/python/tf/debugging/set_log_device_placement\r\n\r\n\r\n\r\n## Description of issue (what needs changing): \r\nIt is not specified as to how to unset the changes made by this function.  Therefore it's becoming difficult to suppress the info level messages and a long trail of info messages gets printed leading to difficulty in analyzing actual results.\r\n\r\n![image](https://user-images.githubusercontent.com/9129069/101432051-265f7100-392e-11eb-9987-f5b1de11e71f.png)\r\n\r\n### Clear description\r\n\r\nMaybe a feature needs to be developed to provide functionality within the function to disable its behaviour.\r\n\r\n`tf.debugging.set_log_device_placement(False)`\r\n`tf.get_logger().setLevel(3)`\r\n`os.environ[\"KMP_WARNINGS\"] = \"FALSE\" `\r\n`tf.get_logger().setLevel('INFO')`\r\n\r\n\r\n\r\nNone of the above methods work.\r\n### Correct links\r\n\r\nIs the link to the source code correct? Yes\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? Yes\r\n \r\n### Returns defined\r\n\r\nAre return values defined? Yes\r\n\r\n### Raises listed and defined\r\n\r\nNA\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nYes\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?  NA\r\n\r\n### Submit a pull request? \r\n\r\nUnclear how to slove the issue yet.\r\n\r\n", "comments": ["@adityagupta679,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!\r\n", "> None of the above methods work.\r\n\r\nAlso, try setting the log level before importing TensorFlow as shown below and check if it works.\r\n\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\r\nimport tensorflow as tf\r\n...\r\n```\r\nThanks!", "> @adityagupta679,\r\n> In order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!\r\n\r\n\r\n> > None of the above methods work.\r\n> \r\n> Also, try setting the log level before importing TensorFlow as shown below and check if it works.\r\n> \r\n> ```\r\n> import os\r\n> os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\r\n> import tensorflow as tf\r\n> ...\r\n> ```\r\n> \r\n> Thanks!\r\n\r\nHey , This code can regenerate the issue. GPU environment is required for the messages to be shown at all. I ran on Google colab using Tensorflow version 2.3.0 . Notice that in the second run of the matmul operation, the messages should not be displayed but are still displayed. \r\nTrying the workaround you suggest also does not work.  Additionally, the issue is that I wish to be able to turn the messages on/off at the requirement. Maybe a feature needs to be added for that. Till then if possible a workaround can be displayed in the documentation. \r\n \r\n```\r\nimport tensorflow as tf\r\nimport os\r\ntf.config.set_soft_device_placement(True)\r\ntf.debugging.set_log_device_placement(True)\r\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\nb = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\r\nc = tf.matmul(a, b)\r\nprint(c)\r\ntf.config.set_soft_device_placement(False)\r\ntf.debugging.set_log_device_placement(False)\r\ntf.get_logger().setLevel(3)\r\nos.environ[\"KMP_WARNINGS\"] = \"FALSE\" \r\ntf.get_logger().setLevel('WARNING')\r\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\nb = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\r\nc = tf.matmul(a, b)\r\nprint(c)\r\n```\r\n\r\n> O/P\r\n\r\n> Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\r\n> tf.Tensor(\r\n> [[22. 28.]\r\n>  [49. 64.]], shape=(2, 2), dtype=float32)\r\n> Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\r\n> tf.Tensor(\r\n> [[22. 28.]\r\n>  [49. 64.]], shape=(2, 2), dtype=float32)\r\n\r\n", "These configuration flags are for the entire program and cannot be changed after startup. I agree that we should document this aspect and throw even an error message if it is called post initialization. "]}, {"number": 45459, "title": "\"keras predict\" uses one more batch than specified by \"steps\" argument with generator x", "body": "\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: pip\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.1\r\n\r\n**Describe the current behavior**\r\nWhen providing a python generator to the keras.Model.predict function with a number of steps=N then, the generator is called N+1 times whereas the output of the predict function is the expected result (of size N x batch_size).\r\n\r\n**Describe the expected behavior**\r\nI expect the generator to be called only N times. It makes it impossible to resume the prediction just after the last called batch. Beside, the last batch is taken from the generator but not used for the prediction.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# a model that simply take a float as input and return it Model(x) = x\r\ninputs = tf.keras.Input(shape=(1,))\r\noutputs = tf.keras.layers.Dense(\r\n    1, \r\n    activation=None, \r\n    use_bias=False, \r\n    kernel_initializer=tf.keras.initializers.Ones()\r\n)(inputs)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\ndef get_generator(batch_size=32, max_value=100):\r\n    for begin in np.arange(start=0, stop=max_value, step=batch_size):\r\n        end = min(max_value, begin + batch_size)\r\n        print(\"Call generator {} to {}\".format(begin, end))\r\n        yield np.arange(begin, end)[:,None]\r\n\r\nbatch_size = 32\r\nmax_value = 100\r\ndata_generator = get_generator(batch_size=batch_size, max_value=max_value)\r\n\r\nN = 3\r\n# 3 steps but generator called 4 times...\r\nres = model.predict(x=data_generator, steps=N)\r\n```\r\n![image](https://user-images.githubusercontent.com/7781975/101401829-2da56100-38d3-11eb-8013-7ed0d1470226.png)\r\n\r\n```python\r\nexpected_result = np.arange(0, N * batch_size)[:, None]\r\nprint(np.allclose(res, expected_result))\r\n```\r\n![image](https://user-images.githubusercontent.com/7781975/101401914-5168a700-38d3-11eb-8dc4-4d4eaf25e3dc.png)\r\n\r\n", "comments": ["I am able to replicate the issue reported, please fin the [gist for nightly](https://colab.research.google.com/gist/Saduf2019/552f6674aa5f547a6f2898c1a4edf9c2/untitled.ipynb), [tf 2.3](https://colab.research.google.com/gist/Saduf2019/05229ccff799bcd67db6d4cc3cbb3014/untitled480.ipynb)", "Was able to reproduce the issue using TF 2.5. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/1f755dc8ae5731f627e58c5608f1f997/untitled88.ipynb).Thanks!"]}, {"number": 45446, "title": "Source compilation fails on Musl system on multiple places", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Alpine Linux edge\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.1\r\n- Python version: 3.8.6\r\n- Installed using virtualenv? pip? conda?: source\r\n- Bazel version (if compiling from source): 3.5.0\r\n- GCC/Compiler version (if compiling from source): 10.2.1_pre1\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to compile TensorFlow from source on Alpine Linux which is a Musl based system. It fails on several points however:\r\n\r\n```\r\nERROR: /builds/PureTryOut/aports/testing/tensorflow/src/tensorflow-2.3.1/tensorflow/core/platform/default/BUILD:75:11: Couldn't build file tensorflow/core/platform/default/_objs/env/0/env.pic.o: C++ compilation of rule '//tensorflow/core/platform/default:env' failed (Exit 1)\r\ntensorflow/core/platform/default/env.cc: In member function 'virtual bool tensorflow::{anonymous}::PosixEnv::GetCurrentThreadName(std::string*)':\r\ntensorflow/core/platform/default/env.cc:160:15: error: 'pthread_getname_np' was not declared in this scope; did you mean 'pthread_setname_np'?\r\n  160 |     int res = pthread_getname_np(pthread_self(), buf, static_cast<size_t>(100));\r\n      |               ^~~~~~~~~~~~~~~~~~\r\n      |               pthread_setname_np\r\n```\r\n\r\n```\r\nERROR: /builds/PureTryOut/aports/testing/tensorflow/src/tensorflow-2.3.1/tensorflow/core/platform/s3/BUILD:44:11: Couldn't build file tensorflow/core/platform/s3/_objs/aws_crypto/aws_crypto.pic.o: C++ compilation of rule '//tensorflow/core/platform/s3:aws_crypto' failed (Exit 1)\r\ntensorflow/core/platform/s3/aws_crypto.cc: In member function 'virtual Aws::Utils::Crypto::HashResult tensorflow::AWSSha256HMACOpenSSLImpl::Calculate(const ByteBuffer&, const ByteBuffer&)':\r\ntensorflow/core/platform/s3/aws_crypto.cc:38:14: error: aggregate 'HMAC_CTX ctx' has incomplete type and cannot be defined\r\n   38 |     HMAC_CTX ctx;\r\n      |              ^~~\r\ntensorflow/core/platform/s3/aws_crypto.cc:39:5: error: 'HMAC_CTX_init' was not declared in this scope; did you mean 'HMAC_CTX_new'?\r\n   39 |     HMAC_CTX_init(&ctx);\r\n      |     ^~~~~~~~~~~~~\r\n      |     HMAC_CTX_new\r\ntensorflow/core/platform/s3/aws_crypto.cc:45:5: error: 'HMAC_CTX_cleanup' was not declared in this scope\r\n   45 |     HMAC_CTX_cleanup(&ctx);\r\n      |     ^~~~~~~~~~~~~~~~\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nThe build script can be found at https://gitlab.alpinelinux.org/alpine/aports/-/raw/4cf626b10d2f4700cc5e5e9e7536061137c8c6a1/testing/tensorflow/APKBUILD. Note that `prepare()` there is called before `build()` and the environment variables set carry over.", "comments": ["@PureTryOut,\r\nCould you please install the pip package dependencies as mentioned in [the guide](https://www.tensorflow.org/install/source#install_python_and_the_tensorflow_package_dependencies) and check if you are still facing the same issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Well, I get different errors that way.\r\n\r\n```\r\nERROR: /home/bart/.cache/bazel/_bazel_bart/f5cee94bd62e1be0d824a974250af519/external/llvm-project/llvm/BUILD:3766:11: C++ compilation of rule '@llvm-project//llvm:Support' failed (Exit 1)\r\nIn file included from external/llvm-project/llvm/lib/Support/Process.cpp:101:\r\nexternal/llvm-project/llvm/lib/Support/Unix/Process.inc: In static member function 'static size_t llvm::sys::Process::GetMallocUsage()':\r\nexternal/llvm-project/llvm/lib/Support/Unix/Process.inc:93:19: error: aggregate 'llvm::sys::Process::GetMallocUsage()::mallinfo mi' has incomplete type and cannot be defined\r\n   93 |   struct mallinfo mi;\r\n      |                   ^~\r\nexternal/llvm-project/llvm/lib/Support/Unix/Process.inc:94:10: error: '::mallinfo' has not been declared\r\n   94 |   mi = ::mallinfo();\r\n      |          ^~~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```", "This looks like some C/C++/system dependencies are not installed/found.\r\n\r\nIt seems issues come from LLVM. Are you able to compile LLVM on the system?", "Well, right now it doesn't even want to compile that as it keeps trying to exec `python` while it _should_ be using `python3` instead.\r\n\r\n```\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (405 packages loaded, 29314 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/bart/Documents/Git/alpine/aports/testing/tensorflow/src/tensorflow-2.4.0/tensorflow/core/util/BUILD:370:24: error executing shell command: '/bin/bash -c bazel-out/host/bin/tensorflow/tools/git/gen_git_source --generate \"$@\" --git_tag_override=${GIT_TAG_OVERRIDE:-}  external/local_config_git/gen/spec.json external/local_config_git/gen/h...' failed (Exit 127): bash failed: error executing command /bin/bash -c 'bazel-out/host/bin/tensorflow/tools/git/gen_git_source --generate \"$@\" --git_tag_override=${GIT_TAG_OVERRIDE:-}' '' external/local_config_git/gen/spec.json ... (remaining 3 argument(s) skipped)\r\nenv: can't execute 'python': No such file or directory\r\n```\r\n\r\nI have `PYTHON_BIN_PATH=/usr/bin/python3` set, but it seems that is only used by `./configure` and not by bazel?", "That again is a Bazel bug.", "Found a workaround luckily.\r\n\r\nSo, from source install with instructions from https://www.tensorflow.org/install/source#install_python_and_the_tensorflow_package_dependencies, I now get a different error with TF 2.4.0, but that's fixed with https://github.com/tensorflow/tensorflow/pull/46138.\r\n\r\nNow I'm back to the mallinfo error. LLVM is actually available on this distribution, is it not possible to use the system variant for TF?", "Unfortunately not really. TF does a lot of JIT and since LLVM does not provide a real stable ABI using the system variant might result in broken code.", "Well with hacks I get further and further.\r\n\r\nTo workaround the Python interpreter problem as mentioned in https://github.com/tensorflow/tensorflow/issues/15618#issuecomment-541406050:\r\n```\r\nln /usr/bin/python3 /usr/bin/python\r\n```\r\n\r\nIt seems I got rid of the LLVM problem by adding deps as used in the LLVM packages from the distribution.\r\n\r\nHowever I'm now back to one of the original problems:\r\n\r\n```\r\nERROR: /home/bart/Documents/Git/alpine/aports/testing/tensorflow/src/tensorflow-2.4.0/tensorflow/core/platform/s3/BUILD:44:11: C++ compilation of rule '//tensorflow/core/platform/s3:aws_crypto' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 75 argument(s) skipped)\r\ntensorflow/core/platform/s3/aws_crypto.cc: In member function 'virtual Aws::Utils::Crypto::HashResult tensorflow::AWSSha256HMACOpenSSLImpl::Calculate(const ByteBuffer&, const ByteBuffer&)':\r\ntensorflow/core/platform/s3/aws_crypto.cc:38:14: error: aggregate 'HMAC_CTX ctx' has incomplete type and cannot be defined\r\n   38 |     HMAC_CTX ctx;\r\n      |              ^~~\r\ntensorflow/core/platform/s3/aws_crypto.cc:39:5: error: 'HMAC_CTX_init' was not declared in this scope; did you mean 'HMAC_CTX_new'?\r\n   39 |     HMAC_CTX_init(&ctx);\r\n      |     ^~~~~~~~~~~~~\r\n      |     HMAC_CTX_new\r\ntensorflow/core/platform/s3/aws_crypto.cc:45:5: error: 'HMAC_CTX_cleanup' was not declared in this scope\r\n   45 |     HMAC_CTX_cleanup(&ctx);\r\n      |     ^~~~~~~~~~~~~~~~\r\nERROR: /home/bart/Documents/Git/alpine/aports/testing/tensorflow/src/tensorflow-2.4.0/tensorflow/tools/pip_package/BUILD:69:10 C++ compilation of rule '//tensorflow/core/platform/s3:aws_crypto' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 75 argument(s) skipped)\r\n```\r\n\r\nIt seems those failing files are part of S3 support, but I actually tried to disable that:\r\n\r\n```\r\nexport TF_NEED_AWS=0\r\nexport TF_NEED_S3=0\r\n```\r\n\r\nIt still tries to compile those files however.", "After setting those exports you have to run `./configure` (better, `python configure.py`). i think this might solve.", "That is exactly what I'm doing and what I have always done, yes. Doesn't make a difference.\r\n\r\nPlease check my build script, should be nothing wrong. https://gitlab.alpinelinux.org/PureTryOut/aports/-/raw/mycroft-precise/testing/tensorflow/APKBUILD", "I took a change from Gentoo, seems `echo \"build --config=noaws\" >> .bazelrc` does the trick as well to stop building AWS support.\r\n\r\nSo just stuck on the LLVM issue now. I really don't understand why, the failing code is guarded by a same conditional statement as one that's used to include the required header file. https://github.com/llvm/llvm-project/blob/main/llvm/lib/Support/Unix/Process.inc#L92 and https://github.com/llvm/llvm-project/blob/main/llvm/lib/Support/Unix/Process.inc#L34\r\n\r\nThere really is no reason this should fail...\r\n\r\n**EDIT:** So Musl does not have support for mallinfo, https://www.openwall.com/lists/musl/2018/01/17/2\r\n\r\nHowever, that shouldn't be a problem as support for it is checked in https://github.com/llvm/llvm-project/blob/main/llvm/cmake/config-ix.cmake#L234 which _should_ just return false. However, for some reason in the case of Tensorflow this line reports that the system does support it?", "IREE uses LLVM and Bazel in the same way as TF (almost). Can you try compiling that? This should give us an indication on whether the issue is from TF or from LLVM.\r\n\r\nRegarding having to manually write to `.bazelrc` to disable AWS, I think that is a bug in the `./configure` process.", "Following these instructions https://google.github.io/iree/get-started/getting-started-linux-bazel, LLVM fails the same way yes.\r\n\r\nAlthough interestingly enough it also fails on `pthread_getname_np` there with LLVM, which it (so far) doesn't for Tensorflow LLVM.", "Does Bazel/Tensorflow call CMake for LLVM differently somehow? The thing it's failing on currently _is_ guarded properly by CMake, and it works for the distribution packages in Alpine Linux. So what is different in Tensorflow that it somehow passes the condition while it shouldn't?", "Bazel doesn't use cmake to configure the build.\r\n\r\nIt seems the issue comes from the BUILD files that LLVM uses. A copy of them is at https://github.com/google/llvm-bazel/", "Oh, it doesn't use LLVM's own build system at all? How strange, why would you ever do that?\r\n\r\nIn that case, is there anyway to replicate what https://github.com/llvm/llvm-project/blob/main/llvm/cmake/config-ix.cmake#L234 does in https://github.com/google/llvm-bazel/ then? As that is basically what should be happening but isn't currently and would fix the issue.", "Bazel has a different way of representing the build dependencies than CMake.\r\n\r\nNote that llvm-bazel only has the BUILD files, not the LLVM source code. To properly build, you need to import both the source code and the BUILD files and merge them together.", "Sure, in case of Tensorflow already handles that merging together luckily. But how would I replicate the CMake check LLVM does in Bazel? This Bazel build system is completely new to me and so far hasn't been fun...\r\n\r\nBazel needs to do the same as https://github.com/llvm/llvm-project/blob/main/llvm/cmake/config-ix.cmake#L234 does", "I think it's better to raise a bug on Bazel's repo.", "Hey there. The version of LLVM build files used by TF is *not* the same as the one at https://github.com/google/llvm-bazel. Both versions hardcode `HAVE_MALLINFO=1`, however, so yes unsurprising this doesn't work. Since the goal of those build files does not include building LLVM in every configuration it supports, configurability is added in an ad-hoc manner. If you want to be able to build TF on this system, you'll need to edit their build files. In particular at https://github.com/tensorflow/tensorflow/blob/a5c4882d6c953c24f8451b866b984abe98a8fa7b/third_party/llvm/llvm.bzl#L248\r\n\r\nIf you'd like to send a patch to llvm-bazel, that would also be welcome. The relevant configuration is here: https://github.com/google/llvm-bazel/blob/4c8b546e53eebc708c77ba19a2110926a8732642/llvm-bazel/llvm-project-overlay/llvm/config.bzl#L39. We do not yet have autodetection of library availability.\r\n\r\nYes having all these different copies and versions of the build files is awful, and we're trying to consolidate them.\r\n\r\nIf something being alpine is detectable with Bazel config settings, that would be the easiest way. https://github.com/bazelbuild/bazel/blob/master/src/conditions/BUILD shows the ones already available as select targets in bazel_tools. If not, then if it's detectable with https://docs.bazel.build/versions/master/configurable-attributes.html that would be the next easiest. After that, the options are basically creating your own flag to indicate musl usage or getting a bazel repo rule to do proper detection of system requirements. Bazel tends to be explicit rather than implicit, wanting to control things via the build invocation, not through environment detection because it aims for reproducibility (and prioritizes that way above ease of use, in my experience).\r\n\r\nSo all that said, I think you might be able to pass `--copt=-UMALLINFO` to Bazel. Not sure whether library defines or commandline copts take precedent.\r\n\r\n> Oh, it doesn't use LLVM's own build system at all? How strange, why would you ever do that?\r\n\r\n:grin: great question. Because getting a Bazel project to interact with a non-Bazel build system is...not great. In addition, Bazel is happier if it manages all the source files. With those issues combined, it turns out to be easier to just create a Bazel build configuration for LLVM :confused: ", "Thanks for the detailed answer! Sadly passing `--copt=-UMALLINFO` doesn't seem to work. For now I've just patched that line to report 0. Alpine it self isn't detectable, and shouldn't need to really. Musl follows standards very strictly: if something fails to compile with Musl but works on glibc, then the code is assuming the exception rather than the standard. Code has to check for glibc rather than others.\r\n\r\nPreferably it would properly detect system requirements rather than just `if (os == \"Alpine\") { }`. I don't know Bazel though so I have no clue how to do that :sweat_smile: \r\n\r\nSo, I patched it for now. I get further, this time it fails differently (and seemingly not on LLVM anymore):\r\n\r\n```\r\nERROR: Skipping '       ': Bad target pattern ' ': package names may contain A-Z, a-z, 0-9, or any of ' !\"#$%&'()*+,-./;<=>?[]^_`{|}~' (most 7-bit ascii characters except 0-31, 127, ':', or '\\')\r\nWARNING: Target pattern parsing failed.\r\nERROR: Bad target pattern '     ': package names may contain A-Z, a-z, 0-9, or any of ' !\"#$%&'()*+,-./;<=>?[]^_`{|}~' (most 7-bit ascii characters except 0-31, 127, ':', or '\\')\r\n``", "Thanks @GMNGeoffrey  for the additional context and the help.\r\n\r\n@PureTryOut I think the new error comes from an invalid BUILD file edit. Do you have more lines of context around that error?", "Well there is some stuff, but it doesn't seem related.\r\n\r\n```\r\nINFO: Reading rc options for 'build' from /home/bart/Documents/Git/alpine/aports/testing/tensorflow/src/tensorflow-2.4.0/.bazelrc:\r\n  'build' options: --jobs=8 --compilation_mode=opt --host_compilation_mode=opt --repository_cache=/home/bart/Documents/Git/alpine/aports/testing/tensorflow/src/tensorflow-2.4.0/bazel-cache/ --distdir=/home/bart/Documents/Git/alpine/aports/testing/tensorflow/src/tensorflow-2.4.0/bazel-distdir/ --define=PREFIX=/home/bart/Documents/Git/alpine/aports/testing/tensorflow/pkg/tensorflow/usr --config=noaws --config=nohdfs --config=mkl\r\nINFO: Found applicable config definition build:short_logs in file /home/bart/Documents/Git/alpine/aports/testing/tensorflow/src/tensorflow-2.4.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/bart/Documents/Git/alpine/aports/testing/tensorflow/src/tensorflow-2.4.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/bart/Documents/Git/alpine/aports/testing/tensorflow/src/tensorflow-2.4.0/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:noaws in file /home/bart/Documents/Git/alpine/aports/testing/tensorflow/src/tensorflow-2.4.0/.bazelrc: --define=no_aws_support=true\r\nINFO: Found applicable config definition build:nohdfs in file /home/bart/Documents/Git/alpine/aports/testing/tensorflow/src/tensorflow-2.4.0/.bazelrc: --define=no_hdfs_support=true\r\nINFO: Found applicable config definition build:mkl in file /home/bart/Documents/Git/alpine/aports/testing/tensorflow/src/tensorflow-2.4.0/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt\r\nINFO: Found applicable config definition build:linux in file /home/bart/Documents/Git/alpine/aports/testing/tensorflow/src/tensorflow-2.4.0/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/bart/Documents/Git/alpine/aports/testing/tensorflow/src/tensorflow-2.4.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\n```", "There should be some lines that should print the path to a malformed BUILD target.\r\n\r\nAlternatively, you can use `git` commands to see where `'       '` gets inserted, in case this was from an accidental edit.", "It seems I found the cause of that particular issue. I did the following:\r\n\r\n```\r\nbazel build \\\r\n    //tensorflow:libtensorflow_framework.so \\\r\n    //tensorflow:libtensorflow.so \\\r\n    //tensorflow:libtensorflow_cc.so \\\r\n    //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nHowever, it seems Bazel doesn't like new lines and the whitespace. I put it all on one line instead, and the error was gone. Annoying :shrug: \r\n\r\nNew error:\r\n\r\n```\r\nERROR: /home/bart/.cache/bazel/_bazel_bart/fa04af0b72c1747dcd0d716042f56ed5/external/com_github_grpc_grpc/src/compiler/BUILD:66:8: declared output 'external/com_github_grpc_grpc/src/compiler/grpc_python_plugin.bin' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\r\nERROR: /home/bart/.cache/bazel/_bazel_bart/fa04af0b72c1747dcd0d716042f56ed5/external/com_github_grpc_grpc/src/compiler/BUILD:66:8: not all outputs were created or valid\r\nERROR: /home/bart/Documents/Git/alpine/aports/testing/tensorflow/src/tensorflow-2.4.0/tensorflow/python/tools/BUILD:143:10 not all outputs were created or valid\r\n```", "Hmm, now this is a grpc issue.\r\n\r\nThey also use Bazel, can you file an issue at https://github.com/grpc/grpc please?", "Sorry it took a while. I filed an issue, https://github.com/grpc/grpc/issues/25188", "> It seems I got rid of the LLVM problem by adding deps as used in the LLVM packages from the distribution.\r\n\r\nCan you please elaborate on that? \r\nI managed to compile 2.5 with Bazel 3.7.2, Python 3.9.5 running: \r\n\r\n`bazel build --config=noaws --config=nogcp --config=nonccl --config=mkl //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nThe only problem I had with TF itself was that I had to disable stacktrace using the method from this patch:\r\nhttps://github.com/ribalda/meta-tensorflow/blob/master/recipes-framework/tensorflow/files/0001-support-musl.patch\r\n\r\n\r\nLLVM refused to compile for me even with all the build dependencies. I took a similar approach for mallinfo by adding a GLIBC condition in\r\n `~/.cache/bazel/ ... /external/llvm-project/llvm/lib/Support/Unix/Process.inc`\r\n\r\n```\r\n 96 #elif defined(HAVE_MALLINFO) && defined(__GLIBC__)\r\n 97   struct mallinfo mi;\r\n 98   mi = ::mallinfo();\r\n```\r\nand had to remove backtrace in\r\n`~/.cache/bazel/ ... /external/llvm-project/llvm/include/llvm/Config/config.h.cmake`\r\n```\r\n 22 /* Define to 1 if you have the `backtrace' function. */\r\n 23 /*#define HAVE_BACKTRACE 0 */\r\n 24 \r\n 25 /*#define BACKTRACE_HEADER <${BACKTRACE_HEADER}>*/\r\n```\r\nYou can actually keep ENABLE_BACKTRACE defined to 1... but just setting HAVE_BACKTRACE to 0 doesn't work because the troublesome header uses an ifdef... which seems like a mistake on llvm's part.\r\n\r\nThere's probably a more elegant solution - it's a matter of musl lacking execinfo.h discussed here:\r\nhttps://www.openwall.com/lists/musl/2015/04/09/3\r\nAs mentioned, a viable solution is finding an alternative backtrace lib, if you care about that. ", "@PureTryOut Could you please let us know if this issue still persists ? Thanks!", "Oof I haven't tried it in a while, I kinda lost interest because of all the issues. I'll give it another shot early in the new year.", "Well the good news is that there are fewer copies of these build files now. They've been upstreamed at https://github.com/llvm/llvm-project/tree/main/utils/bazel. Additionally, we've started defining things based on C preprocessor macros in [config.h](https://github.com/llvm/llvm-project/blob/587bdb3772333763dd739021cd08bc44bcd8485d/utils/bazel/llvm-project-overlay/llvm/include/llvm/Config/config.h#L372), which are generally way easier to use than Bazel platform selects (with the limitation that we can't execute arbitrary code like try-compile), so if you want to move `HAVE_MALLINFO` and the like out of [config.bzl](https://github.com/llvm/llvm-project/blob/main/utils/bazel/llvm-project-overlay/llvm/config.bzl) and into the config headers themselves, then go for it.\r\n\r\n", "Hey there. I successfully built TF 2.8.0 on Alpine Linux with these changes:  https://github.com/grebaza/tensorflow/commit/39381a1f453da0f37b7b1f94c2e64ea2736de4e1. I included the backtrace library (`libexecinfo`) and hardcoded POSIX defines. Hope this helps.\r\n", "That is awesome! Any chance you could upstream that to the main Tensorflow repo (this one)?", "Sure thing. I will also upload changes into tensorflow-io's repo (as tensorflow requires io during its installation)."]}, {"number": 45432, "title": "tf.keras.model.Model-subclassed models do not serialise tf.function-ed methods", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Have not tested\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.5.0-dev20201206\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 11.0, 8.0.4\r\n- GPU model and memory: NVidia Quatro 5000\r\n\r\n**Describe the current behavior**\r\nI have a model which implements a custom training step (tf.keras.models.Model.train_step), which all works fine until serialisation. I understand that because the `train_step` is python code, it needs to be saved as a `tf.function` (graph), and for that it should be decorated including an `input_signature`. However, saving and then reloading models decorated in this way does not seem to recover the custom `train_step`.\r\n\r\n**Describe the expected behavior**\r\n\r\nMethods decorated by `tf.function` including an `input_signature` should be seralised and loaded along with the weights, optimisers and losses.\r\n\r\n**Standalone code to reproduce this issue** \r\n\r\n[Gist](https://colab.research.google.com/drive/1hGqE_s1YFKEFjovEO9Kb-US-5nLsCx6H?usp=sharing)\r\n\r\nThanks you in advance.", "comments": ["I am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/7b0f4b070dd2eda6e0d68e9035e7b703/untitled478.ipynb)", "Still an issue in TF 2.5 and TF-Nightly(2.6). Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/a6d5d9f229848715dcb417ae8cd443ae/untitled478.ipynb#scrollTo=XbCGn9Rs2J_l).Thanks!"]}, {"number": 45425, "title": "XLA dumped llvm IR: how to convert the ir to assembly code?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nHi,\r\n\r\nI want to inspect the assembly code that compiled by llvm, is there any method to do that?\r\nI have tried to using llvm-as tool to conver the .ll file to .bc file but failed:(\r\n\r\nBest Regards,\r\nGary\r\n\r\n**Will this change the current api? How?**\r\n\r\nmaybe\r\n\r\n**Who will benefit with this feature?**\r\n\r\ncompiler engineer\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 45406, "title": "Op to convert numeric tensor to string type faster ", "body": "\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n\r\nWhen we want to convert a tensor with numeric value to string type, there is `tf.as_string` available for us. However, its performance will become very poor along with the input tensor swelling, which is caused due to its implement by `vsnprintf` in my opinion. `vsnprintf` indeed provides a quite rich formatting features but in my case I only want to convert number to string..\r\n\r\nAccording to this [benchmark](http://www.zverovich.net/2020/06/13/fast-int-to-string-revisited.html), `fmt::to_string` is 4x faster than `sprintf`. Therefore, I want to inroduce it into third_patry of tensorflow and create a new function (e.g. `tf.number_to_string`) which can simplely and fast convert numeric tensor to string type based on [`fmtlib`](https://github.com/fmtlib/fmt).\r\n\r\nIn fact, I have implemented a basic version for `tf.number_to_string`, whose performance in my case has been significantly improved than `tf.as_string`. Considering the versatility and the overall design of the framework, I am looking forward to your suggestions.\r\n", "comments": ["I think if we write a specific function for conversion between number and string, we should use a different library which is dedicated for that conversion, since `fmt` is still a formatting library. I found a [benchmark](https://github.com/miloyip/dtoa-benchmark) here and I think [double-conversion](https://github.com/google/double-conversion) is a good choice ( it is fast and it supports bazel out-of-the-box ). WDYT ?", "@vnvo2409 Thanks for your reply!\r\n\r\n> `fmt` is still a formatting library.\r\n\r\nAlthough `fmt` is definitely a formatting library, `fmt::to_string` is a function only for conversion between number and string in fact.\r\n\r\n> I think double-conversion is a good choice ( it is fast and it supports bazel out-of-the-box ).\r\n\r\n`double-conversion` is also a good choice of implementation library worth considering. However, it seems to focus only on the conversion between floating-point numbers and strings, and as for integer numbers, I am currently not sure whether the library also supports it? \r\n\r\nIf my concern doesn't actually constitute a issue, then I am going to get more familiar with this library and try to transform my implementation based it. But before that, since the reasons now seem to be incomplete, we might be able to further discuss why we should use `double-conversion` instead of `fmt`...\r\n\r\n", ">  `fmt::to_string` is a function only for conversion between number and string in fact.\r\n\r\nIn that case, `fmt::to_string` is good to me. `double-conversion` does not support integer indeed."]}, {"number": 45371, "title": "TimeDistributed Layer Applied to Masked Layer (Functional API) not working under TF 2.2+", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GForce GTX 1060 (6BG)\r\n\r\n**Describe the current behavior**\r\nThe following implementation of a model using the Keras functional API worked under TF2.0 and TF2.1 (reduced example from a bigger project, reproduces error though)\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.layers import TimeDistributed\r\n\r\nshared = False\r\nbs = 1\r\nlayer_sizes = (64, 64)\r\n\r\nstate_dimensionality, n_actions = 8, 4\r\nrnn_choice = tf.keras.layers.SimpleRNN\r\n\r\nsequence_length = None\r\ninputs = tf.keras.Input(shape=(sequence_length, state_dimensionality,), batch_size=bs)\r\nmasked = tf.keras.layers.Masking()(inputs)\r\n\r\n# build encoder\r\nencoder_inputs = tf.keras.Input(shape=state_dimensionality, batch_size=bs)\r\nencoder_x = encoder_inputs\r\nfor i in range(len(layer_sizes)):\r\n    encoder_x = tf.keras.layers.Dense(layer_sizes[i],\r\n                                      kernel_initializer=tf.keras.initializers.Orthogonal(gain=tf.sqrt(2.0)),\r\n                                      bias_initializer=tf.constant_initializer(0.0))(encoder_x)\r\n    encoder_x = tf.keras.layers.Activation(\"tanh\")(encoder_x)\r\n\r\nencoder = tf.keras.Model(inputs=encoder_inputs, outputs=encoder_x, name=\"policy_encoder\")\r\n\r\n# policy network; stateful, so batch size needs to be known\r\nx = TimeDistributed(encoder, name=\"TD_policy\")(masked)\r\nx.set_shape([bs] + x.shape[1:])\r\n\r\nx, *_ = rnn_choice(layer_sizes[-1],\r\n                   stateful=True,\r\n                   return_sequences=True,\r\n                   return_state=True,\r\n                   batch_size=bs,\r\n                   name=\"policy_recurrent_layer\")(x)\r\n\r\nmeans = tf.keras.layers.Dense(n_actions, name=\"means\",\r\n                              kernel_initializer=tf.keras.initializers.Orthogonal(0.01),\r\n                              bias_initializer=tf.keras.initializers.Constant(0.0))(x)\r\n\r\npolicy = tf.keras.Model(inputs=inputs, outputs=means, name=\"simple_rnn_policy\")\r\n```\r\n\r\nHowever when I run this under TF2.2.0+ I receive the following error: \r\n`ValueError: Input tensor 'policy_recurrent_layer/zeros_like:0' enters the loop with shape (1, 64), but has shape (None, 64) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.`\r\n\r\nI can prevent this error in the following ways:\r\n- not using `stateful=True` and removing the line `x.set_shape([bs] + x.shape[1:])`\r\n- removing the Masking layer\r\n- setting `bs=2` or higher\r\n\r\nSadly, none of these options would work with the remainder of our project.\r\n\r\n**Describe the expected behavior**\r\nGiven that this was working under TF2.0 I would also expect this to work under 2.3.1 or at least to throw a useful error message hinting towards a breaking change.\r\n", "comments": ["i am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/69427352b1acdf76e61b4a9bf1dddc56/untitled477.ipynb)", "This remains to be an issue in TF2.4.0. ", "Is this being worked on? Or can someone recommend a workaround?\r\n", "Sorry for the very late reply. Keras team is heavily under staffed at the moment and this issue hasn't been prioritized for now. We will take a closer look when we have time. Meanwhile, contribution from community are welcome.", "Thank you for the reply @qlzh727. I do of course understand that you have to set priorities. \r\nI will try to find a solution myself then. If I find one, I will post it here. ", "Was able to to reproduce the issue using TF 2.5. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/aab50160cd5c8a0a4aee4f8f9221f2dd/untitled477.ipynb).Thanks!"]}, {"number": 45361, "title": "TFLite Converter for BiLSTM based architecture", "body": "**System information**: Default configuration of Google Colab. I ran this experiment on Colab and I am attaching the self-contained notebook.\r\n\r\nI have been working on the conversion of [EasyOCR](https://github.com/JaidedAI/EasyOCR) to TFLite . I have been getting this error while converting to TFLite.\r\n> ConverterError: input resource[0] expected type resource != float, the type of assignvariableop_resource_0[0]\r\n\tIn {{node AssignVariableOp}}\r\n\r\nI successfully converted the PyTorch Model to ONNX and ran the inference with sample data and the results are matching correctly. So I hope there are no issues in PyTorch --> ONNX conversion. Even there are no issues in converting to TensorFlow SavedModel. I am getting the error while converting the SavedModel to TFLite. \r\n\r\n**FYI:** Model consists of 2 layer Bi-Directional LSTM cells.\r\n\r\nI have attached all the mentioned details in this [Notebook](https://colab.research.google.com/github/tulasiram58827/ocr_tflite/blob/main/colabs/easyocr_tflite_report.ipynb). To reproduce the above error you can use the same [Notebook](https://colab.research.google.com/github/tulasiram58827/ocr_tflite/blob/main/colabs/easyocr_tflite_report.ipynb).\r\n\r\nThe notebook provided itself is self-contained and will download and install all the necessary modules and files.\r\n\r\n@amahendrakar ", "comments": ["I have tried in TF nightly version(`2.5.0-dev20201202`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/f3f533b14704175abbde6f5fbc1b480e/untitled561.ipynb). Thanks!", "@ymodak ", "Sorry for encountering this issue. TFLite runtime is not able to handle mutable variable cases yet. We are working on improving this area.", "Thanks @abattery for replying. Can I expect the support of mutable variables in further nightly releases of this month or is it planned for long term?", "@tulasiram58827 we are working on support for mutable variables. However, it will take some times. I will update this thread when the feature is available to use.", "Thanks @abattery ", "Hi @abattery \r\n\r\nCan I expect the support for mutable variables in Q1"]}, {"number": 45346, "title": "\"bluepill\" target build fails if repository pathname contains spaces", "body": "@tensorflow/micro\r\n@petewarden \r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n  - Linux Ubuntu 18.04.5\r\n- TensorFlow installed from (source or binary):\r\n  - source\r\n- Tensorflow version (commit SHA if source):\r\n  - commit 753786571eb24b08f06506e2ff397358974a8c1d (HEAD -> master, origin/master, origin/HEAD)\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n  - bluepill (ARM emulator)\r\n\r\n**Describe the problem**\r\n\r\nBuilding a test for use with the emulator (TARGET=bluepill) fails in the shell script.  This is due to the repository pathname having a space character in it.  The following scripts do not have sufficient quote escapes:\r\n- line 33 of tensorflow/lite/micro/testing/test_bluepill_binary.sh\r\n- line 12 of tensorflow/lite/micro/tools/make/downloads/renode/test.sh\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill test_kernel_mul_test\r\ntensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.\r\ntensorflow/lite/micro/tools/make/Makefile:476: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/Makefile:476: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/testing/test_bluepill_binary.sh tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/bin/kernel_mul_test '~~~ALL TESTS PASSED~~~'\r\nThe following command failed: /media/ddavis/WETU Repo/git/tensorflow/tensorflow/lite/micro/testing/../tools/make/downloads/renode/test.sh. Please  make sure that you have correctly installed Renode for TFLM. See  tensorflow/lite/micro/docs/renode.md for more details.\r\ntensorflow/lite/micro/tools/make/Makefile:532: recipe for target 'test_kernel_mul_test' failed\r\nmake: *** [test_kernel_mul_test] Error 1\r\n```", "comments": ["@ddavis-2015 Could you please let us know if this is still an issue in latest stable TF v2.6.0 ?Thank you!", "@ddavis-2015 ! \r\nWe are checking to see whether you still need help in this issue . Have you checked [this thread ](https://github.com/tensorflow/tensorflow/issues/45348#issuecomment-777661482)yet ? Please post this query on Stackoverflow/ [TF forum](https://discuss.tensorflow.org/) for further assistance?Thanks!", "@mohantym I have not yet had a chance to test this with TF v2.6.0 as I am currently swamped with work in the tflite-micro and tflite-micro-arduino-examples repos.  But I do intend to re-test this in the coming weeks.\r\n"]}, {"number": 45330, "title": "Post-training quantization same model get different result for tf.concat", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04 in docker\r\n- TensorFlow installed from (source or binary): binary pip install\r\n- TensorFlow version (or github SHA if from source):2.3.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\ndef representative_dataset_gen_img(imgCnt=20):\r\n    imgDir = os.path.abspath(FLAGS.quan_images)\r\n    imgNameList = glob.glob(os.path.join(imgDir, '*.jpg'))\r\n    imgIndex = random.sample(range(len(imgNameList)), len(imgNameList) if imgCnt == 0 else imgCnt)\r\n    for i in imgIndex:\r\n        imgMat = cv2.imread(imgNameList[i])\r\n        imgMat = utils.image_preporcess(np.copy(imgMat), [FLAGS.input_size[0], FLAGS.input_size[1]])\r\n        imgMat = imgMat[np.newaxis,:,:,:]\r\n        yield [imgMat.astype(np.float32)]\r\n\r\ndef convert_tflite_from_keras():\r\n\t#converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\tconverter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(kerasH5File)\r\n\tconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\tconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n\tconverter.inference_input_type = tf.uint8\r\n\tconverter.inference_output_type = tf.uint8\r\n\tconverter.representative_dataset = representative_dataset_gen_img\r\n\tTFLiteModel = converter.convert()\r\n\topen(TFLiteFile, \"wb\").write(TFLiteModel)\r\n\r\n```\r\n**Also, please include a link to the saved model or GraphDef**\r\n[bad.h5](https://drive.google.com/file/d/11301633Tl5pUjq6DXC8p-VPPECVGYjM3/view?usp=sharing)\r\n[good.h5](https://drive.google.com/file/d/1VsQZ-tfaxmouYo3fNdYFZT-mG_FSEqmx/view?usp=sharing)\r\n[bad.tflite](https://drive.google.com/file/d/15ReLcnSGqz67j8KSZdHCMG-hTRj-KWPO/view?usp=sharing)\r\n[good.tflite](https://drive.google.com/file/d/1910F-erSpyLUXIO6m6nvvxfXJCacn-2f/view?usp=sharing)\r\n\r\n**Failure details**\r\nI use the same tiny-yolov3 network to train two different target detection tasks, and use the same tflite Python API to quantify the model to get the tflite model.However, one result of the two models after quantization is correct and the other is wrong. After comparison, I found that the branch of \"Quantize \"node for concat OP quantization is different (as shown in the figure below).letf is ok, and right is bad.\r\n![1](https://user-images.githubusercontent.com/20400418/100845353-0c4e0a80-34b8-11eb-836f-8ae8ad9411b7.png)\r\n--Why does the same model have different quantization results?\r\n--I've tried several versions of tensorflow, such as tf2.3, tf-nightly-2.5.0 and the problem remains", "comments": ["It is not clear how you are getting different results on quantization perhaps you can explain more.\r\nAre you observing incorrect results with `tf.compat.v1.lite.TFLiteConverter.from_keras_model_file` and correct results with `tf.lite.TFLiteConverter.from_keras_model` ?", "> It is not clear how you are getting different results on quantization perhaps you can explain more.\r\n> Are you observing incorrect results with `tf.compat.v1.lite.TFLiteConverter.from_keras_model_file` and correct results with `tf.lite.TFLiteConverter.from_keras_model` ?\r\n\r\nNo, I used the same Tiny-yolov3 network to train two different object detection tasks, and got two H5 models (good.h5 and bad.h5). also I used the same tflite python API `tf.compat.v1.lite.TFLiteConverter.from_keras_model_file` to convert the two H5 models, and got two tflite models(good.tflite and bad.tflite). The 'bad.tflite' is incorrect, After comparison, I found that the branch of \"Quantize \"node  is different for the two tflite models. I think that's what caused the incorrect results."]}, {"number": 45328, "title": "RuntimeError: Quantization not yet supported for op: 'DEQUANTIZE', after QAT quantize_apply()", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.5.0.dev20201130\r\n\r\nI'm trying to apply quantization on a keras model for quantization aware training and then int8 quantize the model with tflite.\r\nI successfully annotated only the supported layers and use tfmot.quantization.keras.quantize_apply. But, when I try to quantize the quant aware model I get this error multiple times:\r\nRuntimeError: Quantization not yet supported for op: 'DEQUANTIZE'.\r\nAm I doing something wrong?\r\n\r\n**Code**\r\n(Just need to change ```directory``` to a folder with some calibration images from COCO)\r\n```\r\nimport cv2\r\nimport numpy as np\r\nimport os\r\nimport tensorflow as tf\r\nimport tensorflow_model_optimization as tfmot\r\nfrom tensorflow import keras\r\n\r\n\r\ndef apply_quantization_to_supported_layers(layer):\r\n    if isinstance(layer, tf.keras.layers.ReLU):\r\n        return tfmot.quantization.keras.quantize_annotate_layer(layer)\r\n    elif isinstance(layer, tf.keras.layers.Activation):\r\n        if not layer.activation.__name__ == 'sigmoid':\r\n            return tfmot.quantization.keras.quantize_annotate_layer(layer)\r\n    elif isinstance(layer, tf.keras.layers.DepthwiseConv2D):\r\n        return tfmot.quantization.keras.quantize_annotate_layer(layer)\r\n    elif isinstance(layer, tf.keras.layers.Conv2D) and not isinstance(layer, tf.keras.layers.Conv2DTranspose):\r\n        # Conv2DTranspose inherits from Conv2D\r\n        return tfmot.quantization.keras.quantize_annotate_layer(layer)\r\n    return layer\r\n\r\n\r\ndef read_and_process_image(fname):\r\n    img = cv2.imread(fname)\r\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n    img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_LANCZOS4)\r\n    img = img / 127.5\r\n    img = img - 1\r\n    img = img.astype(np.float32)\r\n    img = img.reshape(1, 224, 224, 3)\r\n    return img\r\n\r\n\r\ndef representative_data_gen():\r\n    directory = r'C:\\user\\calibrationsImages'\r\n    for filename in os.listdir(directory):\r\n        if filename.endswith(\".jpg\"):\r\n            image_path = os.path.join(directory, filename)\r\n            img = read_and_process_image(image_path)\r\n            img_in = np.array(img)\r\n            yield [img_in]\r\n        else:\r\n            continue\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    print(tf.version.VERSION)\r\n    orig_model_path = \"model.hdf5\"\r\n\r\n    # load regularly trained model\r\n    model = keras.models.load_model(orig_model_path)\r\n    model.summary()  # print model layers\r\n\r\n    # convert supported layers to quantization aware\r\n    annotated_model = tf.keras.models.clone_model(\r\n        model,\r\n        clone_function=apply_quantization_to_supported_layers,\r\n    )\r\n\r\n    quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\r\n    quant_aware_model.summary()\r\n\r\n    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0, amsgrad=False)\r\n    quant_aware_model.compile(optimizer=adam,\r\n                              loss=\"binary_crossentropy\")\r\n\r\n    quant_aware_model.summary()\r\n\r\n    # quantize quantization aware model to int8\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = representative_data_gen\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n\r\n    tflite_quant_model = converter.convert()\r\n\r\n    open('model.tflite', 'wb').write(tflite_quant_model)\r\n\r\n```\r\n\r\n**The error I'm getting**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:/REPOSITORIES/QuantizationAwareTraining/forTfIssue.py\", line 78, in <module>\r\n    tflite_quant_model = converter.convert()\r\n  File \"C:\\Users\\limorb\\Anaconda3\\envs\\QuantizationAwareTraining\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 895, in convert\r\n    self).convert(graph_def, input_tensors, output_tensors)\r\n  File \"C:\\Users\\limorb\\Anaconda3\\envs\\QuantizationAwareTraining\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 653, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"C:\\Users\\limorb\\Anaconda3\\envs\\QuantizationAwareTraining\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 481, in _calibrate_quantize_model\r\n    inference_output_type, allow_float, activations_type)\r\n  File \"C:\\Users\\limorb\\Anaconda3\\envs\\QuantizationAwareTraining\\lib\\site-packages\\tensorflow\\lite\\python\\optimize\\calibrator.py\", line 104, in calibrate_and_quantize\r\n    np.dtype(activations_type.as_numpy_dtype()).num)\r\nRuntimeError: Quantization not yet supported for op: 'DEQUANTIZE'.\r\nQuantization not yet supported for op: 'DEQUANTIZE'.\r\nQuantization not yet supported for op: 'DEQUANTIZE'.\r\nQuantization not yet supported for op: 'DEQUANTIZE'.\r\nQuantization not yet supported for op: 'DEQUANTIZE'.\r\n. \r\n.\r\n.\r\n```\r\n\r\n**Link to model**\r\n\r\n[model.zip](https://drive.google.com/file/d/1eopi5IdNKPTJimK0Bb1q-8TYZc7IY5sp/view?usp=sharing)", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/b4f2d8570c93f6612ecbb451c86df8a3/45328.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/3354c2211a47b12df9348599cdb49589/45328-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Hi, is there any update on this matter?", "Hi, I was able to quant-aware the model, but when I'm trying to convert it to tflite using this code:\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    tflite_quant_model = converter.convert()\r\n\r\nI get the following error:\r\nerror: 'tfl.pad' op quantization parameters violate the same scale constraint: !quant.uniform<i8:f32, 0.16100097357058057:-128> vs. !quant.uniform<i8:f32, 0.023529411764705882:-128>\r\n\r\ntf version: 2.5.0-dev20210204\r\nHere is the quant-aware model\r\n[qat_model.zip](https://drive.google.com/file/d/1CgLDjrq2lZfgtp0U9mkvN2ZSuicwXjHO/view?usp=sharing)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45328\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45328\">No</a>\n", "Hi, I was able to quant-aware the model, but when I'm trying to convert it to tflite using this code:\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    tflite_quant_model = converter.convert()\r\n\r\nI get the following error:\r\nerror: 'tfl.pad' op quantization parameters violate the same scale constraint: !quant.uniform<i8:f32, 0.16100097357058057:-128> vs. !quant.uniform<i8:f32, 0.023529411764705882:-128>\r\n\r\ntf version: 2.5.0-dev20210204\r\nHere is the quant-aware model\r\n[qat_model.zip](https://drive.google.com/file/d/1CgLDjrq2lZfgtp0U9mkvN2ZSuicwXjHO/view?usp=sharing)", "> Hi, I was able to quant-aware the model, but when I'm trying to convert it to tflite using this code:\r\n> \r\n> ```\r\n> converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\r\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n> tflite_quant_model = converter.convert()\r\n> ```\r\n> \r\n> I get the following error:\r\n> error: 'tfl.pad' op quantization parameters violate the same scale constraint: !quant.uniform<i8:f32, 0.16100097357058057:-128> vs. !quant.uniform<i8:f32, 0.023529411764705882:-128>\r\n> \r\n> tf version: 2.5.0-dev20210204\r\n> Here is the quant-aware model\r\n> [qat_model.zip](https://drive.google.com/file/d/1CgLDjrq2lZfgtp0U9mkvN2ZSuicwXjHO/view?usp=sharing)\r\n\r\nhello, i have the same error when quant my model to uint8 model. tf version 2.5.0.  Have you solved the problem?"]}, {"number": 45326, "title": "tf.keras.preprocessing.image.random_shear raises TypeError when it is mapped to a tf dataset.", "body": "```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-5-3819cfbfb025> in <module>()\r\n----> 1 for i in tfd.map(test):\r\n      2   print(i.shape)\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    256       except Exception as e:  # pylint:disable=broad-except\r\n    257         if hasattr(e, 'ag_error_metadata'):\r\n--> 258           raise e.ag_error_metadata.to_exception(e)\r\n    259         else:\r\n    260           raise\r\n\r\nTypeError: in user code:\r\n\r\n    <ipython-input-4-bf71d3ba553f>:3 test  *\r\n        image_5 = tf.keras.preprocessing.image.random_shear(image_4, intensity = shear_angle, row_axis=0, col_axis=1, channel_axis=2)\r\n    /usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py:114 random_shear  *\r\n        shear = np.random.uniform(-intensity, intensity)\r\n    mtrand.pyx:1083 numpy.random.mtrand.RandomState.uniform  **\r\n        \r\n\r\n    TypeError: __array__() takes 1 positional argument but 2 were given\r\n```\r\n\r\nPlease check **this Colab notebook:**\r\nhttps://colab.research.google.com/drive/1SWl5cGiSTa_ML2zmQFpTvs6SIqOH7qNW?usp=sharing\r\n\r\nWhen editing some part of code got it work, but it can't be used for map method of tfdataset.", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/edc528ea775af52e7aa90afbad637d01/45326.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/5b55c740767fca2cb532c169306ac5c7/45326-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@Crispy13 the problem I think is that `random_shear` is implemented as numpy/scipy function:\r\nhttps://github.com/keras-team/keras-preprocessing/blob/58df11e1145b2088092252c4dba02168c6da2b13/keras_preprocessing/image/affine_transformations.py#L99\r\n\r\nWith map you are passing a Tensor cause you are in graph mode. Probably you could to use map with `py_function` or explore shear augmentations from Tensorflow addons in @tanzhenyu https://github.com/tanzhenyu/image_augmentation/blob/master/image_augmentation/image/README.md\r\n\r\n@tanzhenyu I think that probably we need to add an extra checks to this old scipy/PIL functions for the input type to give a better error message to developers.", "@bhack\r\nSo is it intended to not use tensor with the `random_shear` function?", "Check the input at:\n\nhttps://github.com/keras-team/keras-preprocessing/blob/58df11e1145b2088092252c4dba02168c6da2b13/keras_preprocessing/image/affine_transformations.py#L281", "Was able to replicate the issue in TF v2.5 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/0aa6d6de6bf542f23dc7c10df2c4cea4/45326-tf-nightly.ipynb#scrollTo=o0dEQAmaUikk)..Thanks !", "While trying to reproduce your issue in `tf-nightly(2.8.0-dev20211009)` facing different error, please find the gist [here](https://colab.research.google.com/gist/chunduriv/b94010827e1cd34414b39fd99432fe89/45326.ipynb),Thanks!\r\n\r\n"]}, {"number": 45322, "title": "Memory leak using nested name_scope in eager mode", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Both Windows 10 and Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Using desktop only\r\n- TensorFlow installed from (source or binary): tf-nightly\r\n- TensorFlow version (use command below): 2.5.0-dev20201201\r\n- Python version: Both 3.7 and 3.8\r\n- Bazel version (if compiling from source): Not compiling from source\r\n- GCC/Compiler version (if compiling from source): Not compiling from source\r\n- CUDA/cuDNN version: 11\r\n- GPU model and memory: RTX 2070\r\n\r\n**Describe the current behavior**\r\nIt seems using nested name_scope can lead to memory leak. For example, in my setup, the memory grows indefinitely using this code.\r\n```\r\nimport tensorflow as tf\r\nfor i in range(int(1e7)):\r\n    with tf.name_scope(\"first\"):\r\n        with tf.name_scope(\"second\"):\r\n            pass\r\n```\r\nI'm actually not sure this is valid usage of name_scope. However, it seems this king of nested usage of name_scope is used in Tensorflow Probability ([here for example](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/distributions/distribution.py#L1542)). Another minimal example leading to memory leak is:\r\n```\r\nimport tensorflow_probability as tfp\r\ndistr = tfp.distributions.Normal(loc=1.0, scale=2)\r\nfor i in range(int(1e7)):\r\n    distr.sample()\r\n```\r\n\r\nI don't know why, but appending \"/\" to the name of the scope seems to fix the leak. Here are samples without memory leaks:\r\n```\r\nimport tensorflow as tf\r\nfor i in range(int(1e7)):\r\n    with tf.name_scope(\"first/\"):\r\n        with tf.name_scope(\"second\"):\r\n            pass\r\n```\r\nor\r\n```\r\nimport tensorflow_probability as tfp\r\ndistr = tfp.distributions.Normal(loc=1.0, scale=2, name=\"foo/\")\r\ndistr._name += \"/\"  # Force \"/\" because it's removed in the constructor\r\nfor i in range(int(1e7)):\r\n    distr.sample(name=\"sample/\")\r\n```\r\nIt seems like ctx.name_scope is now using a C++ Safe_PyObjectPtr. I'm not sure why but maybe the reference counter is incorrect.\r\n", "comments": ["Hi ,can I work on this issue?\r\n ", "@matthieucoquet,\r\n> It seems using nested name_scope can lead to memory leak. For example, in my setup, the memory grows indefinitely using this code.\r\n\r\nI did not observe any memory leaks while running this given code snippet. Although second example runs indefinitely, the memory consumption is constant. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/eb2309eed05d0cbe8cb9e873eb380bf3/45322-tf-nightly.ipynb). Thanks!\r\n ", "@amahendrakar \r\nI see the RAM usage going up in the colab notebook you linked (first sample, using only name_scope).\r\n\r\nUsing the tracemalloc module, I also see increasing memory usage in the colab notebook.\r\n```\r\nimport tensorflow as tf\r\nimport tracemalloc\r\ntracemalloc.start()\r\n\r\nsnapshot1 = tracemalloc.take_snapshot()\r\nfor i in range(int(1e7)):\r\n    with tf.name_scope(\"first\"):\r\n        with tf.name_scope(\"second\"):\r\n            pass\r\nsnapshot2 = tracemalloc.take_snapshot()\r\ntop_stats = snapshot2.compare_to(snapshot1, 'lineno')\r\nprint(\"[ Top 10 differences ]\")\r\nfor stat in top_stats[:10]:\r\n    print(stat)\r\n```\r\n\r\nEdit: Here is the [gist](https://colab.research.google.com/gist/matthieucoquet/527952beccf61a3bc3656ef00fdc2f29) link.", "Was able to reproduce the issue with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/2e47c65e4cb30ab59a3fc83d3e288137/45322-tf-nightly.ipynb), i.e. v2.5.0-dev20201208.\r\n\r\nHowever, I did not observe much increase in the memory usage with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/03b1a1862050fc7013ad3e217f828d3a/45322-2-2.ipynb) and [TF v2.3](https://colab.research.google.com/gist/amahendrakar/f5b75de4bef3971db0edd28effa642a7/45322-2-3.ipynb). Please check the linked gist for reference. Thanks!\r\n"]}, {"number": 45285, "title": "Tensorflow gpu does not work with RTX 3000 series card.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary (pip install tf-nightly-gpu)\r\n- TensorFlow version (use command below): tf-nightly-gpu==2.5.0.dev20201111\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 11.1 CUDNN 8.0.5.39\r\n- GPU model and memory: RTX 3070 8GB\r\n\r\n**Describe the current behavior**\r\nThe GPU is successfully detected and the anaconda prompt does not show any errors or warnings but whenever the model starts training it uses the CPU. \r\nWhen running ```tf.config.experimental.list_physical_devices('GPU')``` I get ```[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]```\r\nWhen running ```print(device_lib.list_local_devices())``` I get \r\n```\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 12958882941219838237\r\n, name: \"/device:GPU:0\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 6910041152\r\nlocality {\r\n  bus_id: 1\r\n  links {\r\n  }\r\n}\r\nincarnation: 16296908220344189432\r\nphysical_device_desc: \"device: 0, name: GeForce RTX 3070, pci bus id: 0000:08:00.0, compute capability: 8.6\"\r\n]\r\n```\r\nwhen running ```tf.config.list_physical_devices('GPU')``` I get\r\n```[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]```\r\nand when running ```tf.test.is_gpu_available()```  I get \r\n```True```\r\n**Describe the expected behavior**\r\nThe model should use the GPU to train.\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n**My Model Code**\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.preprocessing.text import Tokenizer\r\n\r\nfor device in tf.config.experimental.list_physical_devices(\"GPU\"):\r\n    tf.config.experimental.set_memory_growth(device, True)\r\n\r\nmax_features = 1000000\r\nmaxlen = 200\r\ntrain_size=442598\r\nupdatedtrainsize = 5;\r\n\r\nmy_data = pd.read_csv('mydata.csv')\r\ny = my_data[\"label\"]\r\nx = my_data[\"url\"]\r\nz = np.array(x)\r\nw = np.array(y)\r\nx_train = z[0:train_size]\r\nx_val = z[train_size:]\r\ny_train = w[0:train_size]\r\ny_val = w[train_size:]\r\n\r\nfor x in range(len(y_train)): \r\n  if \"good\" in y_train[x]:\r\n    y_train[x] = 0\r\n  else:\r\n    y_train[x] = 1\r\n\r\nfor x in range(len(y_val)): \r\n  if \"good\" in y_val[x]:\r\n    y_val[x] = 0\r\n  else:\r\n    y_val[x] = 1\r\n\r\ntokenizer = Tokenizer(filters='/-.+',\r\n                      lower=True,\r\n                      split=' ',\r\n                      char_level=False,\r\n                      oov_token='<OOV>')\r\ntokenizer.fit_on_texts(x_train)\r\ntokenizer.fit_on_texts(x_val)\r\nword_index = tokenizer.word_index\r\n\r\nx_train = tokenizer.texts_to_sequences(x_train)\r\nx_val = tokenizer.texts_to_sequences(x_val)\r\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\r\nx_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\r\nx_train = np.array(x_train).astype('float32')\r\nx_val = np.array(x_val).astype('float32')\r\ny_train = np.array(y_train).astype('float32')\r\ny_val = np.array(y_val).astype('float32')\r\n\r\ninputs = keras.Input(shape=(None,), dtype=\"int32\")\r\nx = layers.Embedding(max_features, 128)(inputs)\r\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\r\nx = layers.Bidirectional(layers.LSTM(64))(x)\r\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\r\nmodel = keras.Model(inputs, outputs)\r\n\r\nmodel.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\r\n\r\n# with tf.device(\"/GPU:0\"):\r\nmodel.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n**My Anaconda Prompt**\r\n```\r\n2020-11-30 17:47:00.576296: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-11-30 17:47:01.757077: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-30 17:47:01.757826: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-11-30 17:47:01.778881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:08:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.725GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-11-30 17:47:01.778947: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-11-30 17:47:01.784571: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-11-30 17:47:01.784619: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-11-30 17:47:01.787759: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-30 17:47:01.788668: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-11-30 17:47:01.795255: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-30 17:47:01.797702: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-11-30 17:47:01.798214: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-11-30 17:47:01.798304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-11-30 17:47:09.856239: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-11-30 17:47:09.856839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:08:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.725GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-11-30 17:47:09.857285: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-11-30 17:47:09.857609: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-11-30 17:47:09.857858: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-11-30 17:47:09.858083: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-30 17:47:09.858310: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-11-30 17:47:09.858534: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-30 17:47:09.858766: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-11-30 17:47:09.858991: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-11-30 17:47:09.859247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-11-30 17:47:10.265547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-30 17:47:10.265630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2020-11-30 17:47:10.266448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2020-11-30 17:47:10.266885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6589 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:08:00.0, compute capability: 8.6)\r\n2020-11-30 17:47:10.267361: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-30 17:47:11.146901: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2020-11-30 17:47:13.898501: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-11-30 17:47:14.736090: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-11-30 17:47:14.881003: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-11-30 17:47:21.167001: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\n```\r\n**Additional Note**\r\nI have already tried compiling tf-nightly-gpu from source on Ubuntu 20.04, using tensorflow docker images, and other versions of tensorflow (2.4 rcs) but all of them result in the same situation I am stuck in now. \r\n**GPU utilization during training: 0%** **CPU utilization during training: 50-60%**\r\nThe CPU I am using is the AMD Ryzen 3800x.\r\nLink to my [stack question](https://stackoverflow.com/questions/65056018/tensorflow-trains-on-cpu-instead-of-rtx-3000-series-gpu)", "comments": ["@linuxmaster0312,\r\nOn running the code, I am facing an error stating `FileNotFoundError: [Errno 2] No such file or directory: 'mydata.csv'`. Could you please share all the supporting files required to run the code?\r\n\r\n> ```\r\n> # with tf.device(\"/GPU:0\"):\r\n> model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))\r\n> ```\r\nAlso, please uncomment the `with tf.device(\"/GPU:0\"):` line and check if you are facing the same issue. Thanks!", "@amahendrakar Thanks for getting back to me.\r\n[Link to the data](https://drive.google.com/file/d/1IwxJiPw5Yj18jmWUi73UVOlHht5cUFvV/view?usp=sharing)\r\nuncommenting the line does not change anything unfortunately.", "i have exactly the same issue with an identical setup. i am using an intel cpu, windows 10 pro, pip, tried several python versions, tried cuda 11.0, several driver versions, many precompiled tensorflow versions. My rtx 3070 always gets recognized but doesn't get used\r\n\r\n", "@rmothukuru Any updates on the issue?", "The logs look fine unfortunately, so I don't have much to go by.  Are you able to run normal CUDA programs on your machine?", "I have the same problem when trying to run a CNN on GPU. For Multilayer Perceptron it runs fine. But it crashes the kernel when trying to run a CNN using NVIDIA 3000x GPU.  On PyTorch it runs just fine on the GPU.", "@leandrolopez can you attach logs for your run?  You could be running into a different issue.\r\n\r\nAlso, getting logs with `TF_CPP_VMODULE=gpu_device=1` in the environment will be useful.  It will tell us for sure which ops are running on the GPU.", "@sanjoy here are is the log. Fortunately it is not too big. The problem happens only when using a CNN model with GPU, I noticed it get stuck on the first epoch and then it crashes the kernel (shown here in the logs). If I force it to run on CPU it works well except it takes longer. \r\n`\r\n\r\n\r\n2021-01-07 18:10:32.665777: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-07 18:10:35.933930: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-07 18:10:35.934819: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-01-07 18:10:35.965538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: GeForce RTX 3090 computeCapability: 8.6\r\ncoreClock: 1.8GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s\r\n2021-01-07 18:10:35.965595: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-07 18:10:35.975963: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-07 18:10:35.976012: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-07 18:10:35.982167: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-07 18:10:35.984237: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-07 18:10:35.998264: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-07 18:10:36.002831: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-07 18:10:36.004355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-07 18:10:36.004511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-07 18:11:03.194153: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-01-07 18:11:03.195405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: GeForce RTX 3090 computeCapability: 8.6\r\ncoreClock: 1.8GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s\r\n2021-01-07 18:11:03.195467: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-07 18:11:03.195495: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-07 18:11:03.195522: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-07 18:11:03.195544: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-07 18:11:03.195569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-07 18:11:03.195594: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-07 18:11:03.195617: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-07 18:11:03.195639: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-07 18:11:03.195808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n\r\n2021-01-07 18:10:32.665777: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-07 18:10:35.933930: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-07 18:10:35.934819: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-01-07 18:10:35.965538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: GeForce RTX 3090 computeCapability: 8.6\r\ncoreClock: 1.8GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s\r\n2021-01-07 18:10:35.965595: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-07 18:10:35.975963: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-07 18:10:35.976012: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-07 18:10:35.982167: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-07 18:10:35.984237: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-07 18:10:35.998264: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-07 18:10:36.002831: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-07 18:10:36.004355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-07 18:10:36.004511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-07 18:11:03.194153: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-01-07 18:11:03.195405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: GeForce RTX 3090 computeCapability: 8.6\r\ncoreClock: 1.8GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s\r\n2021-01-07 18:11:03.195467: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-07 18:11:03.195495: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-07 18:11:03.195522: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-07 18:11:03.195544: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-07 18:11:03.195569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-07 18:11:03.195594: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-07 18:11:03.195617: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-07 18:11:03.195639: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-07 18:11:03.195808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-07 18:11:04.044884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-07 18:11:04.044940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-01-07 18:11:04.045072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-01-07 18:11:04.045352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21821 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:27:00.0, compute capability: 8.6)\r\n2021-01-07 18:11:04.045945: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nEpoch 1/15\r\n\r\n2021-01-07 18:10:32.665777: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-07 18:10:35.933930: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-07 18:10:35.934819: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-01-07 18:10:35.965538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: GeForce RTX 3090 computeCapability: 8.6\r\ncoreClock: 1.8GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s\r\n2021-01-07 18:10:35.965595: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-07 18:10:35.975963: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-07 18:10:35.976012: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-07 18:10:35.982167: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-07 18:10:35.984237: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-07 18:10:35.998264: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-07 18:10:36.002831: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-07 18:10:36.004355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-07 18:10:36.004511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-07 18:11:03.194153: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-01-07 18:11:03.195405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: GeForce RTX 3090 computeCapability: 8.6\r\ncoreClock: 1.8GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s\r\n2021-01-07 18:11:03.195467: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-07 18:11:03.195495: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-07 18:11:03.195522: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-07 18:11:03.195544: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-07 18:11:03.195569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-07 18:11:03.195594: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-07 18:11:03.195617: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-07 18:11:03.195639: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-07 18:11:03.195808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-07 18:11:04.044884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-07 18:11:04.044940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-01-07 18:11:04.045072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-01-07 18:11:04.045352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21821 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:27:00.0, compute capability: 8.6)\r\n2021-01-07 18:11:04.045945: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-07 18:11:05.369270: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n\r\n2021-01-07 18:10:32.665777: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-07 18:10:35.933930: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-07 18:10:35.934819: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-01-07 18:10:35.965538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: GeForce RTX 3090 computeCapability: 8.6\r\ncoreClock: 1.8GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s\r\n2021-01-07 18:10:35.965595: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-07 18:10:35.975963: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-07 18:10:35.976012: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-07 18:10:35.982167: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-07 18:10:35.984237: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-07 18:10:35.998264: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-07 18:10:36.002831: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-07 18:10:36.004355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-07 18:10:36.004511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-07 18:11:03.194153: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-01-07 18:11:03.195405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: GeForce RTX 3090 computeCapability: 8.6\r\ncoreClock: 1.8GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s\r\n2021-01-07 18:11:03.195467: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-07 18:11:03.195495: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-07 18:11:03.195522: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-07 18:11:03.195544: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-07 18:11:03.195569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-07 18:11:03.195594: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-07 18:11:03.195617: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-07 18:11:03.195639: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-07 18:11:03.195808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-07 18:11:04.044884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-07 18:11:04.044940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-01-07 18:11:04.045072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-01-07 18:11:04.045352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21821 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:27:00.0, compute capability: 8.6)\r\n2021-01-07 18:11:04.045945: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-07 18:11:05.369270: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-01-07 18:11:06.877333: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n\r\n2021-01-07 18:10:32.665777: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-07 18:10:35.933930: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-07 18:10:35.934819: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-01-07 18:10:35.965538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: GeForce RTX 3090 computeCapability: 8.6\r\ncoreClock: 1.8GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s\r\n2021-01-07 18:10:35.965595: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-07 18:10:35.975963: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-07 18:10:35.976012: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-07 18:10:35.982167: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-07 18:10:35.984237: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-07 18:10:35.998264: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-07 18:10:36.002831: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-07 18:10:36.004355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-07 18:10:36.004511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-07 18:11:03.194153: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-01-07 18:11:03.195405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: GeForce RTX 3090 computeCapability: 8.6\r\ncoreClock: 1.8GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s\r\n2021-01-07 18:11:03.195467: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-07 18:11:03.195495: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-07 18:11:03.195522: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-07 18:11:03.195544: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-07 18:11:03.195569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-07 18:11:03.195594: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-07 18:11:03.195617: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-07 18:11:03.195639: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-07 18:11:03.195808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-07 18:11:04.044884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-07 18:11:04.044940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-01-07 18:11:04.045072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-01-07 18:11:04.045352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21821 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:27:00.0, compute capability: 8.6)\r\n2021-01-07 18:11:04.045945: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-07 18:11:05.369270: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-01-07 18:11:06.877333: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-07 18:11:08.023218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-07 18:11:08.037238: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n\r\nKernel died, restarting\r\n\r\n`", "> Kernel died, restarting\r\n\r\nDid the machine reboot after this?  If yes, this looks like a bug in the GPU driver, TensorFlow should not be able to do things that kill the OS kernel.", "@sanjoy yes I can run normal cuda programs on my machine.", "> \r\n> \r\n> > Kernel died, restarting\r\n> \r\n> Did the machine reboot after this? If yes, this looks like a bug in the GPU driver, TensorFlow should not be able to do things that kill the OS kernel.\r\n\r\n@sanjoy The machine did not reboot. It just restarts the kernel when using Spyder but if using the VSCode notebooks it just hangs. I am able to run other CUDA dependent program. For example I play chess and the Fat Fritz engine requires the GPU and it works. I rewrote the same CNN model on PyTorch and it works too. Only issue is when using an RTX 3000x GPU on a CNN tensorflow model.  ", "I have same issue, I'm using a Ryzen 7, Nvidia 3070. My card gets recognized but does not execute any code. \r\n\r\nTry installing on windows and ubuntu different versions of cuda, cuDNN. \r\n\r\nI don't understand what is wrong. Some say these 30 series are not yet supported which is very surprising. \r\n\r\nPlease let me know if you find a fix.", "> > > Kernel died, restarting\r\n> > \r\n> > \r\n> > Did the machine reboot after this? If yes, this looks like a bug in the GPU driver, TensorFlow should not be able to do things that kill the OS kernel.\r\n> \r\n> @sanjoy The machine did not reboot. It just restarts the kernel when using Spyder but if using the VSCode notebooks it just hangs. I am able to run other CUDA dependent program. For example I play chess and the Fat Fritz engine requires the GPU and it works. I rewrote the same CNN model on PyTorch and it works too. Only issue is when using an RTX 3000x GPU on a CNN tensorflow model.\r\n\r\nSame problem for me. Only work with Dense layers, with LSTM or CONV crash on first epoch after few seconds. Get one moment of 100% use of gpu and crash.", "Same problem here with tf-nightly, last line I see is:\r\n`2021-03-01 15:30:30.757086: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.`\r\nAfter this program just exits without any error message, If I run it on jupyter I get kernel died error there. \r\nI'm using Ryzen 5 + Nvidia 3080.", "I am facing the same problem\r\nRTX 3070 \r\ncUda 11.0 (tried ll 10, 11.2 ...)\r\n\r\nGPU and Tensorflow work well in other algorithms where CNN not used.\r\ncrash if CNN added to the network.", "Any working solution for rtx 3070 laptops card for tensorflow using full potential of GPU?", "When this will solve ??\r\nAnybody who can solve this please share the information.", "Facing the same issue", "Now I used rtx 3060 and installed tf-nightly-gpu (intread tensorflow-gpu) with cuda core 11.1.0 and it work for me", "> Now I used rtx 3060 and installed tf-nightly-gpu (intread tensorflow-gpu) with cuda core 11.1.0 and it work for me\r\n\r\nCan you mention exact versions you installed. Of tensorflow, cuda and cudnn. Also can you confirm that you able to train CNN models with it?", "I am working in RTX 3070 and tf causing the error and tf-nightly is\ncrashing instead of showing any error.\nbut the other algorithm run well in GPU except CNN\nsame CNN model run well in CPU mode.\n\n@ ashitpatel did you tested in the CNN model ??\n\nOn Sun, Mar 28, 2021 at 10:00 AM ashitpatel2496 ***@***.***>\nwrote:\n\n> Now I used rtx 3060 and installed tf-nightly-gpu (intread tensorflow-gpu)\n> with cuda core 11.1.0 and it work for me\n>\n> Can you mention exact versions you installed. Of tensorflow, cuda and\n> cudnn. Also can you confirm that you able to train CNN models with it?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/45285#issuecomment-808825402>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AK65C6FOCD7UWLS3BCAMTBDTFZ5UBANCNFSM4UIGYMGQ>\n> .\n>\n\n\n-- \n[image: photo]\n*ADHIKARI KRISHNA P.*\nAssistant Manager(\ub300\ub9ac), THERAGEN GENOME CARE\n\n+82-1544-9771 | +82-1022734018\n\nhttps://www.genomecare.net/\n***@***.***\n\nCreate your own email signature\n<https://www.wisestamp.com/create-own-signature/?utm_source=promotion&utm_medium=signature&utm_campaign=create_your_own&srcid=>\n", "> > Now I used rtx 3060 and installed tf-nightly-gpu (intread tensorflow-gpu) with cuda core 11.1.0 and it work for me\r\n> \r\n> Can you mention exact versions you installed. Of tensorflow, cuda and cudnn. Also can you confirm that you able to train CNN models with it?\r\n\r\n@ashitpatel2496 \r\nsorry I use  CUDA Toolkit 11.2.2 (March 2021) and  tf-nightly-gpu==2.6.0.dev20210327\r\nI training Yolov3 model of [https://github.com/pythonlessons/TensorFlow-2.x-YOLOv3](https://github.com/pythonlessons/TensorFlow-2.x-YOLOv3)\r\n![Untitled](https://user-images.githubusercontent.com/51964193/112739600-d3263280-8f9f-11eb-9453-7a03917346ab.jpg)\r\n\r\n", "> > > Now I used rtx 3060 and installed tf-nightly-gpu (intread tensorflow-gpu) with cuda core 11.1.0 and it work for me\r\n> > \r\n> > \r\n> > Can you mention exact versions you installed. Of tensorflow, cuda and cudnn. Also can you confirm that you able to train CNN models with it?\r\n> \r\n> @ashitpatel2496\r\n> sorry I use CUDA Toolkit 11.2.2 (March 2021) and tf-nightly-gpu==2.6.0.dev20210327\r\n> I training Yolov3 model of https://github.com/pythonlessons/TensorFlow-2.x-YOLOv3\r\n> ![Untitled](https://user-images.githubusercontent.com/51964193/112739600-d3263280-8f9f-11eb-9453-7a03917346ab.jpg)\r\n\r\nWhat version was Cudnn?", "> > > > Now I used rtx 3060 and installed tf-nightly-gpu (intread tensorflow-gpu) with cuda core 11.1.0 and it work for me\r\n> > > \r\n> > > \r\n> > > Can you mention exact versions you installed. Of tensorflow, cuda and cudnn. Also can you confirm that you able to train CNN models with it?\r\n> > \r\n> > \r\n> > @ashitpatel2496\r\n> > sorry I use CUDA Toolkit 11.2.2 (March 2021) and tf-nightly-gpu==2.6.0.dev20210327\r\n> > I training Yolov3 model of https://github.com/pythonlessons/TensorFlow-2.x-YOLOv3\r\n> > ![Untitled](https://user-images.githubusercontent.com/51964193/112739600-d3263280-8f9f-11eb-9453-7a03917346ab.jpg)\r\n> \r\n> What version was Cudnn?\r\n\r\n cuDNN v8.1.0 (January 26th, 2021), for CUDA 11.0,11.1 and 11.2", "> > > Now I used rtx 3060 and installed tf-nightly-gpu (intread tensorflow-gpu) with cuda core 11.1.0 and it work for me\r\n> > \r\n> > \r\n> > Can you mention exact versions you installed. Of tensorflow, cuda and cudnn. Also can you confirm that you able to train CNN models with it?\r\n> \r\n> @ashitpatel2496\r\n> sorry I use CUDA Toolkit 11.2.2 (March 2021) and tf-nightly-gpu==2.6.0.dev20210327\r\n> I training Yolov3 model of https://github.com/pythonlessons/TensorFlow-2.x-YOLOv3\r\n> ![Untitled](https://user-images.githubusercontent.com/51964193/112739600-d3263280-8f9f-11eb-9453-7a03917346ab.jpg)\r\n\r\nThis versions works for me in a rtx 3070 with CNN layers. The problems of crashes disappears. \r\nThanks a lot", "I am also successfully running now with the above same version\r\nTill now I am facing some speed problems.\r\n\r\nI think my GPU is not working fully. somewhere I need to do setup inside TensorFlow.\r\n\r\nThank you very much for sharing the information.\r\n\r\n![rtx_3070_cpu](https://user-images.githubusercontent.com/45994360/113145610-98571f80-9269-11eb-96e5-77f811ce3040.PNG)\r\n\r\n![rtx_3070_gpu](https://user-images.githubusercontent.com/45994360/113145626-9d1bd380-9269-11eb-90ca-c032270bf783.PNG)\r\n![rtx_3070_2](https://user-images.githubusercontent.com/45994360/113145649-a311b480-9269-11eb-89fa-1cd2c67682ba.PNG)\r\n![rtx_3070](https://user-images.githubusercontent.com/45994360/113145671-a9a02c00-9269-11eb-9073-f5e078f963b3.PNG)\r\n\r\n", "I had the similar issue. GPU was detected by tensorflow but not used during the prediction (I was using only for the prediction, already have the trained model).\r\n\r\nI managed to fix the problem using the following steps:\r\n1. installed cudatoolkit11.0.221 (conda install -c anaconda cudatoolkit)\r\n2.  downloaded **cudnnv8.1.0.77**\r\n3. copy **cudnnv8.1.0.77/bin** to **envs/env_name/Libarary/bin**\r\n4. Rename **cusolver64_10.dll** to **cusolver64_11.dll**\r\n5. installed **tf-nightly-gpu2.6.0.dev20210413** (pip install tf-nightly-gpu) \r\n\r\nAttaching a screenshot of GPU usage, the spikes are appearing during the prediction, in between I am using CPU only for the other tasks.  \r\n![Screenshot 2021-04-19 18 44 31](https://user-images.githubusercontent.com/53816219/115277460-997ebc80-a144-11eb-8f15-a7c09a86de47.png)\r\n\r\n", "to add to the quagmire of misconfigurations - you can specify nvidia channel in conda which will return more recent results.\r\n\r\nconda install --channel nvidia cudatoolkit cudnn\r\n\r\n\r\n```shell\r\nconda search cudatoolkit\r\n conda search cudnn\r\n conda search --channel nvidia cudnn\r\n  conda search --channel nvidia cudatoolkit\r\n\r\n\r\n(nvidia-cuda) \u279c  stylegan2-ada git:(digressions) \u2717 conda search --channel anaconda cudatoolkit\r\nLoading channels: done\r\n# Name                       Version           Build  Channel             \r\ncudatoolkit                      9.0      h13b8566_0  anaconda            \r\ncudatoolkit                      9.0      h13b8566_0  pkgs/main           \r\ncudatoolkit                      9.2               0  anaconda            \r\ncudatoolkit                      9.2               0  pkgs/main           \r\ncudatoolkit                 10.0.130               0  anaconda            \r\ncudatoolkit                 10.0.130               0  pkgs/main           \r\ncudatoolkit                 10.1.168               0  anaconda            \r\ncudatoolkit                 10.1.168               0  pkgs/main           \r\ncudatoolkit                 10.1.243      h6bb024c_0  anaconda            \r\ncudatoolkit                 10.1.243      h6bb024c_0  pkgs/main           \r\ncudatoolkit                  10.2.89      hfd86e86_0  anaconda            \r\ncudatoolkit                  10.2.89      hfd86e86_0  pkgs/main           \r\ncudatoolkit                  10.2.89      hfd86e86_1  anaconda            \r\ncudatoolkit                  10.2.89      hfd86e86_1  pkgs/main           \r\ncudatoolkit                 11.0.221      h6bb024c_0  anaconda            \r\ncudatoolkit                 11.0.221      h6bb024c_0  pkgs/main           \r\n(nvidia-cuda) \u279c  stylegan2-ada git:(digressions) \u2717 conda search --channel nvidia cudatoolkit  \r\nLoading channels: done\r\n# Name                       Version           Build  Channel             \r\ncudatoolkit                      9.0      h13b8566_0  pkgs/main           \r\ncudatoolkit                      9.2               0  nvidia              \r\ncudatoolkit                      9.2               0  pkgs/main           \r\ncudatoolkit                 10.0.130               0  nvidia              \r\ncudatoolkit                 10.0.130               0  pkgs/main           \r\ncudatoolkit                 10.1.168               0  pkgs/main           \r\ncudatoolkit                 10.1.243      h036e899_8  nvidia              \r\ncudatoolkit                 10.1.243      h6bb024c_0  nvidia              \r\ncudatoolkit                 10.1.243      h6bb024c_0  pkgs/main           \r\ncudatoolkit                  10.2.89      h6bb024c_0  nvidia              \r\ncudatoolkit                  10.2.89      h8f6ccaa_8  nvidia              \r\ncudatoolkit                  10.2.89      hfd86e86_0  pkgs/main           \r\ncudatoolkit                  10.2.89      hfd86e86_1  pkgs/main           \r\ncudatoolkit                   11.0.3      h15472ef_8  nvidia              \r\ncudatoolkit                 11.0.221      h6bb024c_0  nvidia              \r\ncudatoolkit                 11.0.221      h6bb024c_0  pkgs/main           \r\ncudatoolkit                   11.1.1      h6406543_8  nvidia              \r\ncudatoolkit                  11.1.74      h6bb024c_0  nvidia              \r\n**cudatoolkit                   11.2.0      h73cb219_8  nvidia              \r\ncudatoolkit                   11.2.1      h8204236_8  nvidia              \r\ncudatoolkit                   11.2.2      he111cf0_8  nvidia              \r\ncudatoolkit                  11.2.72      h2bc3f7f_0  nvidia**         \r\n```\r\n\r\nnot working though - I will create a separate issue. \r\n", "> I am also successfully running now with the above same version\r\n> Till now I am facing some speed problems.\r\n> \r\n> I think my GPU is not working fully. somewhere I need to do setup inside TensorFlow.\r\n> \r\n> Thank you very much for sharing the information.\r\n> \r\n> ![rtx_3070_cpu](https://user-images.githubusercontent.com/45994360/113145610-98571f80-9269-11eb-96e5-77f811ce3040.PNG)\r\n> \r\n> ![rtx_3070_gpu](https://user-images.githubusercontent.com/45994360/113145626-9d1bd380-9269-11eb-90ca-c032270bf783.PNG)\r\n> ![rtx_3070_2](https://user-images.githubusercontent.com/45994360/113145649-a311b480-9269-11eb-89fa-1cd2c67682ba.PNG)\r\n> ![rtx_3070](https://user-images.githubusercontent.com/45994360/113145671-a9a02c00-9269-11eb-9073-f5e078f963b3.PNG)\r\n\r\nHey @krishdb38,\r\n\r\nWhat is the model size & why it is occupying so much of GPU Memory? Can you tell me the reason behind it? I am having a similar memory usage issue with my 3080 gpu. The same model is occupying less gpu memory while working on 2080ti.", "> > > Now I used rtx 3060 and installed tf-nightly-gpu (intread tensorflow-gpu) with cuda core 11.1.0 and it work for me\r\n> > \r\n> > \r\n> > Can you mention exact versions you installed. Of tensorflow, cuda and cudnn. Also can you confirm that you able to train CNN models with it?\r\n> \r\n> @ashitpatel2496 sorry I use CUDA Toolkit 11.2.2 (March 2021) and tf-nightly-gpu==2.6.0.dev20210327 I training Yolov3 model of https://github.com/pythonlessons/TensorFlow-2.x-YOLOv3 ![Untitled](https://user-images.githubusercontent.com/51964193/112739600-d3263280-8f9f-11eb-9453-7a03917346ab.jpg)\r\nHi. There were a couple of questions about the cuda 10.1 drivers in conjunction with tensorflow 2.3.1 on Nvidia 30 series (rtx 3060) video cards in ubuntu 18.04 operating system. There were no problems with installing the drivers, all the necessary files for the neural network are there (like libcublas, libcudart and others), but for some reason the neural network does not want to process the image (before that, I tested it on nvidia 1060, everything worked without problems). What could be the problem? Does it make sense to upgrade to cuda 11.x and tensorflow above? Can you tell me which version is better to use cuda and tensorflow for the nvidia 3060 video card? thanks in advance for your reply. ", "Hi, have the same problem here, it crashes after less than 1 epoch.\r\n\r\nEnvironment: windows11 WSL2 Ubuntu18.04/Ubuntu20.04\r\nVersion: tensorflow-gpu2.4.0, python3.6\r\nGPU: GTX 3060\r\n\r\nI tried training in the docker environment and it crashed. I also tried to install it myself and it was the version mentioned by @tondangerza , @stronquens , but the result was still crashing.\r\n\r\nIt is worth mentioning that the simple CNN model works fine, but when the number of model parameters reaches millions, it will crash\r\n\r\nAny suggestions here?\r\nThanks in advance!", "@Minxiangliu \r\nI solved the same problem by uninstalling Cuda tool kits and installing again & \r\npersonally, I find Cuda toolkits & cudnn versions affect highly.\r\nwhat about trying some lower version of Cuda tool kits & matching driver.\r\n![image](https://user-images.githubusercontent.com/45994360/151315331-1c6bd386-e6a9-4fd0-af56-4464edb4070c.png)\r\n\r\nI am using it without any problem.\r\nin the beginning, it was very difficult for me to set up.\r\n\r\nin the window how  about running in power shell \r\n\r\nI think changing the driver version & Cuda tool kit is also not a bad idea.\r\n\r\n"]}, {"number": 45255, "title": "Universal support for bfloat16 data, ie saving results of float32 operations as bfloat16 to memory by truncating", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2 (google colab)\r\n- Are you willing to contribute it (Yes/No): ?\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nbfloat16 is a truncated float32. That means that any float32 value can be cast to bfloat16 by simply discarding the high two bytes, for example by a 16bit-shift or mask and saving the remaining bits into a 2-byte type, or packing two truncated floats into a 4-byte type. Such operations are available universally. Currently, bfloat16 is only supported on TPUs and some CPUs, which makes sense because those also accelerate computation on bfloat16 values.\r\n\r\nBut all devices can benefit from bfloat16 data even when there is no half-precision computation support of any form: By doing calculations in float32, but truncating the results to 16bits (\"casting\" to bfloat16) before saving to memory, and expanding back to 32bits after loading from memory, one can improve memory utilization (allowing larger models) and also increase the effective memory bandwidth.\r\nThis would not utilize fast half-precision _processing_, but that is effectively unusable for training anyways when there is no fast mixed-precision support.\r\n\r\nBecause bfloat16 has the same range of values as float32 but at a lower precision, it can solve the problem of underruns/overflows arising when using pure float16 when training on devices that do not support mixed precision, while still providing the memory size and bandwidth benefits of 16-bit data.\r\n\r\nCurrently, setting the dtype policy to \"bfloat16\" like so:\r\n`policy = tf.keras.mixed_precision.experimental.Policy('bfloat16')\r\ntf.keras.mixed_precision.experimental.set_policy(policy)`\r\nDoes not raise errors immediately, but results in type errors when building the model:\r\n`Value for attr 'T' of bfloat16 is not in the list of allowed values: double, float, half, complex64, complex128 [...]`\r\nSome tensorflow operations and layers do not support bfloat16 on the Tesla T4 used in colab, notably LSTMs. Removing such layers allows building the model, but will raise another type error when attempting to train the model via model.fit().\r\n\r\n**Will this change the current api? How?**\r\nNo change, support for bfloat16 already exists for certain devices. Implementing this feature could be done via creating another mixed-precision policy, like \"data-bfloat16\".\r\n\r\n**Who will benefit from this feature?**\r\nEverybody that uses devices without mixed precision or even any half precision support (for reference, only Volta, Turing, and Ampere GPUs support fast mixed precision. The still widely-used Pascal GPUs like the P100 don't support it), by allowing to take advantage of improved memory utilization and effective memory bandwidth without the disabling underrun issues when attempting to train using pure float16. CPUs that are often bandwidth-limited in vector operations might benefit the most from 16-bit data.\r\n\r\n**Any Other info.**\r\n\r\nThis feature would not benefit anyone that uses the most cutting-edge hardware. But for reference, even the P100 used in google colab services would benefit from a feature like this. While it would make no difference to researchers and companies with access to expensive cutting-edge hardware, those make up only a part of tensorflow's users. Enabling everyone to experiment with models with up to twice the size given their current hardware would be an obvious benefit for most users of tensorflow.", "comments": ["I would be very interested in this feature!"]}, {"number": 45249, "title": "Validation data generator yields 2 times, but should only 1", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\nTrain process yields validation_data more than I defined in `.fit` function in each epoch. I'm using `.fit` function with two generators for train and validation datasets, which length is the same. I defined learn process for 10 epochs, but after 5 I had warning about there is not more validation_data. You can see more details in colab link below.\r\n\r\n**Describe the expected behavior**\r\nTrain process should yields `validation_data` only `validation_steps` times. \r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1INO5UfPDr_7Jvx6xM4FVBYkGnLG6IFUf?usp=sharing\r\n\r\n**Logs**\r\n```\r\nEpoch 1/10\r\ntrain [0 1 2 3]\r\n1/1 [==============================] - ETA: 0s - loss: 17.7931 - mae: 2.8934\r\nval [4]\r\n1/1 [==============================] - 0s 105ms/step - loss: 17.7931 - mae: 2.8934 - val_loss: 205.8619 - val_mae: 14.3479\r\n\r\nEpoch 2/10\r\ntrain [5 6 7 8]\r\n1/1 [==============================] - ETA: 0s - loss: 1865.1838 - mae: 40.8180\r\nval [9]\r\nval [14]\r\n1/1 [==============================] - 0s 37ms/step - loss: 1865.1838 - mae: 40.8180 - val_loss: 36154.7812 - val_mae: 190.1441\r\n\r\nEpoch 3/10\r\ntrain [10 11 12 13]\r\n1/1 [==============================] - ETA: 0s - loss: 17199.1582 - mae: 128.6884\r\nval [19]\r\nval [24]\r\n1/1 [==============================] - 0s 32ms/step - loss: 17199.1582 - mae: 128.6884 - val_loss: 320162.5000 - val_mae: 565.8290\r\n\r\nEpoch 4/10\r\ntrain [15 16 17 18]\r\n1/1 [==============================] - ETA: 0s - loss: 72351.9141 - mae: 266.5040\r\nval [29]\r\nval [34]\r\n1/1 [==============================] - 0s 40ms/step - loss: 72351.9141 - mae: 266.5040 - val_loss: 1302790.6250 - val_mae: 1141.3986\r\n\r\nEpoch 5/10\r\ntrain [20 21 22 23]\r\n 1/1 [==============================] - ETA: 0s - loss: 208619.7188 - mae: 454.2613\r\nval [39]\r\nval [44]\r\n1/1 [==============================] - 0s 36ms/step - loss: 208619.7188 - mae: 454.2613 - val_loss: 3674302.5000 - val_mae: 1916.8470\r\n\r\nEpoch 6/10\r\ntrain [25 26 27 28]\r\n1/1 [==============================] - ETA: 0s - loss: 482259.9688 - mae: 691.9575\r\nval [49]\r\nWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1 batches). You may need to use the repeat() function when building your dataset.\r\n1/1 [==============================] - 0s 44ms/step - loss: 482259.9688 - mae: 691.9575\r\n\r\nEpoch 7/10\r\ntrain [30 31 32 33]\r\n1/1 [==============================] - 0s 18ms/step - loss: 964487.8750 - mae: 979.5891\r\n\r\nEpoch 8/10\r\ntrain [35 36 37 38]\r\n1/1 [==============================] - 0s 25ms/step - loss: 1741474.2500 - mae: 1317.1534\r\n\r\nEpoch 9/10\r\ntrain [40 41 42 43]\r\n1/1 [==============================] - 0s 16ms/step - loss: 2914341.0000 - mae: 1704.6472\r\n\r\nEpoch 10/10\r\ntrain [45 46 47 48]\r\n1/1 [==============================] - 0s 16ms/step - loss: 4599159.5000 - mae: 2142.0679\r\n```", "comments": ["I am able to rpelicate th eissue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/618dc7a808b6ad44707277454915ab9b/untitled476.ipynb).", "This is similar to this issue https://github.com/tensorflow/tensorflow/issues/45459 (both uses generator object for training/validation). Thanks!", "Was able to reproduce the issue using TF 2.5 . Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/aa0a95c7a0bdfcc772278045d366ddd4/untitled87.ipynb).Thanks!", "Thanks for opening this issue. Development of keras moved to separate repository https://github.com/keras-team/keras/issues\r\n\r\nPlease post this issue on keras-team/keras repo.\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThank you!"]}, {"number": 45238, "title": "Resource exhausted: MemoryError: Unable to allocate", "body": "For similar questions see: #38414 \r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): `pip install tensorflow-gpu==2.4.0rc3`\r\n- TensorFlow version (use command below): v2.4.0-rc2-20-g68f236364c 2.4.0-rc3\r\n- Python version: 3.7.9\r\n- CUDA/cuDNN version: CUDA 11.1 \r\n- GPU model and memory: GeForce RTX 3090 24GiB\r\n\r\n**Describe the current behavior**\r\n\r\nAn error occurs when training to the second epoch, `MemoryError: Unable to allocate 184. MiB for an array with shape (64, 26, 26, 3, 371) and data type float32`\r\nWhen the problem occurred, I had 70GB of RAM and 5GB of video memory left on my system. But \"Unable to allocate 184. MiB\".\r\nTo hide this problem, simply reduce frozen_batch_size from 64 to 32, i.e., reduce the batch size.\r\n\r\n**Describe the expected behavior**\r\n\r\nThere should be no errors.\r\n\r\n**Standalone code to reproduce the issue**\r\nFor code and data, please see: https://github.com/liasece/tf-38414\r\n\r\n**Other info / logs** \r\n\r\n15/15 [==============================] - ETA: 0s - loss: 7711.74822020-11-28 09:40:12.434912: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] layout failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\r\n2020-11-28 09:40:12.449082: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] remapper failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\r\n2020-11-28 09:40:12.582863: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] remapper failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\r\n2020-11-28 09:40:14.685870: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[1,512,647,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.686045: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at image_resizer_state.h:142 : Resource exhausted: OOM when allocating tensor with shape[1,416,416,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.686110: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at image_resizer_state.h:142 : Resource exhausted: OOM when allocating tensor with shape[1,416,416,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.686801: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at image_resizer_state.h:142 : Resource exhausted: OOM when allocating tensor with shape[1,416,416,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.687007: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[1,512,826,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.686816: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at image_resizer_state.h:142 : Resource exhausted: OOM when allocating tensor with shape[1,416,416,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.687972: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[1,769,512,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.689583: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[1,768,512,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.689721: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[1,512,678,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.690045: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[1,512,768,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.690171: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[1,512,683,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.772685: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[1,512,770,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.775723: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[1,640,640,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.776161: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at image_resizer_state.h:142 : Resource exhausted: OOM when allocating tensor with shape[1,416,416,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.777899: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[1,512,776,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.782664: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[1,512,683,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.791474: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[1,512,932,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.791955: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at image_resizer_state.h:142 : Resource exhausted: OOM when allocating tensor with shape[1,416,416,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.792808: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at image_resizer_state.h:142 : Resource exhausted: OOM when allocating tensor with shape[1,416,416,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.793188: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[1,559,512,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.793469: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[1,640,494,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.793868: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at image_resizer_state.h:142 : Resource exhausted: OOM when allocating tensor with shape[1,416,416,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-11-28 09:40:14.816077: W tensorflow/core/framework/op_kernel.cc:1751] Resource exhausted: MemoryError: Unable to allocate 184. MiB for an array with shape (64, 26, 26, 3, 371) and data type float32\r\nTraceback (most recent call last):\r\n\r\n  File \"R:\\ProgramData\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 247, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"R:\\ProgramData\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 135, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"R:\\ProgramData\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 620, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"R:\\ml\\bug-test\\xyolo\\yolo3\\utils.py\", line 358, in preprocess_true_boxes_xyolo\r\n    dtype='float32') for l in range(num_layers)]\r\n\r\n  File \"R:\\ml\\bug-test\\xyolo\\yolo3\\utils.py\", line 358, in <listcomp>\r\n    dtype='float32') for l in range(num_layers)]\r\n\r\nMemoryError: Unable to allocate 184. MiB for an array with shape (64, 26, 26, 3, 371) and data type float32", "comments": ["@liasece \r\nMemory error indicates that the process has consumed all of the ram memory. You may want to reduce the batch size/image size and try again.", "> @liasece\r\n> Memory error indicates that the process has consumed all of the ram memory. You may want to reduce the batch size/image size and try again.\r\n\r\nThis is what is not normal, I have enough memory in my system, you should probably pay attention to what I am describing.", "@liasece \r\nI ran the code shared and face a different error, please find [gist here](https://colab.research.google.com/gist/Saduf2019/9756c6ec6bcbb1ccd63d9497a1c92022/untitled472.ipynb). please share a colab gist with the error reported.", "@Saduf2019 \r\nThis problem does not seem to be reproducible in your colab environment, so perhaps you need Windows and a GPU, properly tuned for frozen_batch_size, to reproduce the problem.", "> This is what is not normal, I have enough memory in my system\r\n\r\nWhen it says \"Unable to allocate 184. MiB\", it means that it was not able to allocate 184M on top of everything it has allocated.  So this isn't surprising by itself -- imagine that all other tensors consume 5G-10M and so the 184M memory allocation fails.\r\n\r\nDo you have other reasons to believe that the 184M allocation should succeed?", "> > This is what is not normal, I have enough memory in my system\r\n> \r\n> When it says \"Unable to allocate 184. MiB\", it means that it was not able to allocate 184M on top of everything it has allocated. So this isn't surprising by itself -- imagine that all other tensors consume 5G-10M and so the 184M memory allocation fails.\r\n> \r\n> Do you have other reasons to believe that the 184M allocation should succeed?\r\n\r\nIndeed, I can't rule out that my performance monitoring tool is missing memory usage samples and that the real graph may have spikes that it doesn't catch.\r\n\r\nI'm getting a ResourceExhaustedError in the middle of a training and I'm assuming that shouldn't be possible. I.e., either it happens in the very first iteration, showing me my GPU doesn't have enough memory to train my model or it works until the end.\r\n\r\nThe reality is that my RAM and GPU RAM footprint is flat during training, and my inputs are the same size for each training step, so there should be no particular step that requires a particular amount of memory.\r\n\r\nIt should not fail with 70GB (70%) of RAM and 5GB (20%) of GPU RAM remaining on my system.", "> I'm getting a ResourceExhaustedError in the middle of a training and I'm assuming that shouldn't be possible.\r\n\r\nThat is surprising, but it is not a logical impossibility.  Maybe there is a memory leak somewhere, have you tried using the [TF memory profiler](https://www.tensorflow.org/guide/profiler#memory_profile_tool)?\r\n\r\nAs for why TF is OOMing with 70G left of CPU memory left over, can you try running with `TF_CPP_VMODULE=<module_name>=process_state=2` and attach the logs?  That might give us a clue.\r\n\r\nCC @ezhulenev in case has seen this before.", "I am having a similar situation on LSTMs, however I (knowing it might be bad idea): I lowered my sequence length from 32 -> 16 -> 8 -> 4, 4 finally gets me to epoch 60 and beyond (still running)\r\n\r\nIt was certainly a RAM (16GB) resource exhaust.\r\n\r\nIt's interesting to see that 70GB still managed to resource exhaust.\r\n\r\nHas this problem been solved yet?", "The problem still presists"]}, {"number": 45233, "title": "Trying to deploy hello_world on KW41Z (M0) device", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): `pip install tensorflow`\r\n- Tensorflow version (commit SHA if source): 2.1.0 (py2.7) 2.3.1 (py3.7)\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): mbed\r\n\r\n- gcc-arm-none-eabi v9.3.1\r\n- mbed-cli v1.10.5\r\n\r\n**Describe the problem**\r\n\r\nTrying to deploy the hello_world example project to the [KW41Z](https://os.mbed.com/platforms/FRDM-KW41Z/) device using mbed and cannot get the project to compile for various reasons.\r\n\r\nI have a hunch that mbed is not configured correctly, but I'm unsure how to fix the issues properly. I'll continue working on this but any advice is appreciated!\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nUnder Python 2.7 as recommended, but I also tried under Python 3.7 with similar results.\r\n\r\n```\r\n# setup env\r\nconda create -n tflm-2.7 \"python=2.7\"\r\nconda activate tflm-2.7\r\npip install tensorflow mbed-cli\r\n\r\n# setup project\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET_ARCH=cortex-m0plus TARGET=mbed TAGS=\"CMSIS\" generate_hello_world_mbed_project\r\ncd tensorflow/lite/micro/tools/make/gen/mbed_cortex-m0plus/prj/hello_world/mbed\r\nmbed config root .\r\nmbed deploy\r\n\r\n# Note all the json files have -std=gnu++14 so I omitted the 98 -> 11 update\r\nmbed compile -m KW41Z -t GCC_ARM -v\r\n```\r\n(partial) output:\r\n```\r\n[Fatal Error] NanostackRfPhyKw41z.cpp@17,10: common_functions.h: No such file or directory \r\n[DEBUG] Return: 1 \r\n[DEBUG] Output: ./mbed-os/features/nanostack/targets/TARGET_NXP/TARGET_KW41Z/NanostackRfPhyKw41z.cpp:17:10: fatal error: common_functions.h: No such file or directory \r\n[DEBUG] Output:    17 | #include \"common_functions.h\" \r\n[DEBUG] Output:       |          ^~~~~~~~~~~~~~~~~~~~ \r\n[DEBUG] Output: compilation terminated. \r\nTraceback (most recent call last): \r\n  File \"/home/ben/git/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m0plus/prj/hello_world/mbed/mbed-os/tools/make.py\", line 78, in wrapped_build_project \r\n    *args, **kwargs \r\n  File \"/home/ben/git/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m0plus/prj/hello_world/mbed/mbed-os/tools/build_api.py\", line 607, in build_project \r\n    objects = toolchain.compile_sources(resources, sorted(resources.get_file_paths(FileType.INC_DIR))) \r\n  File \"/home/ben/git/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m0plus/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 418, in compile_sources \r\n    return self._compile_sources(resources, inc_dirs=inc_dirs) \r\n  File \"/home/ben/git/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m0plus/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 495, in _compile_sources \r\n    return self.compile_queue(queue, objects) \r\n  File \"/home/ben/git/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m0plus/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 566, in compile_queue \r\n    raise ToolException(err) \r\nToolException: ./mbed-os/features/nanostack/targets/TARGET_NXP/TARGET_KW41Z/NanostackRfPhyKw41z.cpp:17:10: fatal error: common_functions.h: No such file or directory \r\n   17 | #include \"common_functions.h\" \r\n      |          ^~~~~~~~~~~~~~~~~~~~ \r\ncompilation terminated. \r\n```\r\nI located this file: `find . -name \"common_functions.h\"` and tried again:\r\n```\r\nmbed compile -m KW41Z -t GCC_ARM -v \\\r\n  --source ./mbed-os/features/frameworks/nanostack-libservice/mbed-client-libservice\r\n```\r\n(partial) output:\r\n```\r\nCould not compile for KW41Z: No Linker Script found \r\n```\r\nI located the KW41Z linker script: `find . -name \"*KW41Z*.ld\"`\r\n```\r\nmbed compile -m KW41Z -t GCC_ARM -v \\\r\n  --source ./mbed-os/features/frameworks/nanostack-libservice/mbed-client-libservice \\\r\n  --source ./mbed-os/targets/TARGET_Freescale/TARGET_MCUXpresso_MCUS/TARGET_KW41Z/device/TOOLCHAIN_GCC_ARM \r\n```\r\n(partial) output:\r\n```\r\n[DEBUG] Errors: /usr/bin/../lib/gcc/arm-none-eabi/9.3.1/../../../../arm-none-eabi/bin/ld: BUILD/KW41Z/GCC_ARM/mbed-os/targets/TARGET_Freescale/TARGET_MCUXpresso_MCUS/TARGET_KW41Z/device/TOOLCHAIN_GCC_ARM/startup_MKW41Z4.o: in function `Reset_Handler': \r\n[DEBUG] Errors: /home/ben/git/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m0plus/prj/hello_world/mbed/./mbed-os/targets/TARGET_Freescale/TARGET_MCUXpresso_MCUS/TARGET_KW41Z/device/TOOLCHAIN_GCC_ARM/startup_MKW41Z4.S:124: undefined reference to `SystemInit' \r\n[DEBUG] Errors: /usr/bin/../lib/gcc/arm-none-eabi/9.3.1/../../../../arm-none-eabi/bin/ld: /usr/bin/../lib/gcc/arm-none-eabi/9.3.1/../../../../arm-none-eabi/lib/thumb/v6-m/nofp/crt0.o: in function `_mainCRTStartup': \r\n[DEBUG] Errors: (.text+0x56): undefined reference to `__wrap_main' \r\n[DEBUG] Errors: /usr/bin/../lib/gcc/arm-none-eabi/9.3.1/../../../../arm-none-eabi/bin/ld: (.text+0x5a): undefined reference to `__wrap_exit' \r\n[DEBUG] Errors: collect2: error: ld returned 1 exit status \r\nTraceback (most recent call last): \r\n  File \"/home/ben/git/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m0plus/prj/hello_world/mbed/mbed-os/tools/make.py\", line 78, in wrapped_build_project \r\n    *args, **kwargs \r\n  File \"/home/ben/git/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m0plus/prj/hello_world/mbed/mbed-os/tools/build_api.py\", line 610, in build_project \r\n    res = toolchain.link_program(resources, build_path, name) \r\n  File \"/home/ben/git/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m0plus/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 778, in link_program \r\n    self.link(elf, objects, libraries, lib_dirs, linker_script) \r\n  File \"/home/ben/git/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m0plus/prj/hello_world/mbed/mbed-os/tools/toolchains/gcc.py\", line 357, in link \r\n    self.default_cmd(cmd) \r\n  File \"/home/ben/git/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m0plus/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 830, in default_cmd \r\n    raise ToolException(stderr) \r\nToolException: /usr/bin/../lib/gcc/arm-none-eabi/9.3.1/../../../../arm-none-eabi/bin/ld: BUILD/KW41Z/GCC_ARM/mbed-os/targets/TARGET_Freescale/TARGET_MCUXpresso_MCUS/TARGET_KW41Z/device/TOOLCHAIN_GCC_ARM/startup_MKW41Z4.o: in function `Reset_Handler': \r\n/home/ben/git/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m0plus/prj/hello_world/mbed/./mbed-os/targets/TARGET_Freescale/TARGET_MCUXpresso_MCUS/TARGET_KW41Z/device/TOOLCHAIN_GCC_ARM/startup_MKW41Z4.S:124: undefined reference to `SystemInit' \r\n/usr/bin/../lib/gcc/arm-none-eabi/9.3.1/../../../../arm-none-eabi/bin/ld: /usr/bin/../lib/gcc/arm-none-eabi/9.3.1/../../../../arm-none-eabi/lib/thumb/v6-m/nofp/crt0.o: in function `_mainCRTStartup': \r\n(.text+0x56): undefined reference to `__wrap_main' \r\n/usr/bin/../lib/gcc/arm-none-eabi/9.3.1/../../../../arm-none-eabi/bin/ld: (.text+0x5a): undefined reference to `__wrap_exit' \r\n```\r\n", "comments": ["I finally got this to compile this morning.\r\n\r\nI found some related mbed issues/PRs, specifically this one: ARMmbed/mbed-os#13119, which led me to a fix. By default TFMicro downloaded mbed-os at tag `mbed-os-6.0.0-alpha-3` and it looks like the fix from that PR was merged after that tag  (along with some other breaking changes noted below).\r\n\r\n\r\n**Hacky workaround:**\r\n\r\nMoving the `targets` dir into `nanostack-interface` as indicated by the mbed PR allows things to compile now:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET_ARCH=cortex-m0plus TARGET=mbed TAGS=\"CMSIS\" generate_hello_world_mbed_project\r\ncd tensorflow/lite/micro/tools/make/gen/mbed_cortex-m0plus/prj/hello_world/mbed\r\nmbed config root .\r\nmbed deploy\r\n# this is the fix\r\nmv mbed-os/features/nanostack/targets mbed-os/features/nanostack/nanostack-interface\r\nmbed compile -m KW41Z -t GCC_ARM\r\n```\r\n\r\n**Trying to update mbed-os as a better fix:**\r\n- mbed-os versions `6.2.0`, `6.3.0`, `6.4.0`, and `6.5.0` (latest at the time of writing) have a different serial interface, so there are issues with print statements, which are resolvable but beyond the scope of this issue I think.\r\n- mbed-os version `6.1.0` still has the same issue with `common_functions.h` as noted originally.\r\n\r\nSo this is working now and the issue and quick fix exist here for posterity, but it might be nice to update the demos to use a newer version of mbed-os :man_shrugging:  ", "@bmorcos It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "I tried again with the following:\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- Tensorflow version (commit SHA if source): a4dfb8d1a71 (v2.5.0)\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): mbed\r\n- gcc-arm-none-eabi v10.2.1\r\n- mbed-cli v1.10.5\r\n- Python 3.7.9\r\n\r\nI had to make two small adjustments from the original issue to accommodate the new version:\r\n1. Update the `TAGS=\"cmsis\"` argument to `OPTIMIZED_KERNEL_DIR=cmsis_nn`:\r\n```bash\r\nmake -f tensorflow/lite/micro/tools/make/Makefile \\\r\n  TARGET_ARCH=cortex-m0plus \\\r\n  TARGET=mbed \\\r\n  OPTIMIZED_KERNEL_DIR=cmsis_nn \\\r\n  generate_hello_world_mbed_project\r\n```\r\n2. Add '_default' to the generated project dir name:\r\n```bash\r\ncd tensorflow/lite/micro/tools/make/gen/mbed_cortex-m0plus_default/prj/hello_world/mbed\r\n```\r\n\r\nI got identical results to the original issue posted and the same workaround allows the build to succeed.\r\n\r\nI still believe this is an issue with the mbed-os version and not necessarily dependant on the TF version. TF 2.5.0 still grabs `mbed-os-6.0.0-alpha-3` when generating the mbed project."]}, {"number": 45216, "title": "Groups parameter of Conv2d and Conv2dTranpose ('deconvolution') not Working ?", "body": "Hello Authors\r\n\r\nI'm a user of tensorflow. Someday i have use Deconvolution and i know it called \"Conv2dTranpose\" in tensorflow. But I want to reduce parameters of model by groups. I found groups property at Pytorch and I see it in Conv2D class (Conv2dTranspose inheritance Conv2d). But when i use i get result which i don't want \r\nThis is my code \r\n\r\n`\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, ReLU, MaxPool2D\r\nmodel = tf.keras.Sequential([\r\n    Conv2D(512,\r\n           kernel_size=(3, 3),\r\n           strides=(1, 1),\r\n           padding='same',\r\n           use_bias=False,\r\n           name='conv'),\r\n    BatchNormalization(),\r\n    ReLU(),\r\n    Conv2DTranspose(256, kernel_size=(4, 4), strides=(2, 2),\r\n                    padding='same', use_bias=False,\r\n                    groups=256,\r\n                    kernel_initializer='he_normal'),\r\n    BatchNormalization(),\r\n    ReLU()\r\n])\r\n\r\nmodel.build((32, 256, 192, 3))\r\nmodel.summary()\r\n`\r\n\r\nThis is summary when i received \r\n>\r\nLayer (type)                 Output Shape              Param #   \r\nconv (Conv2D)                (32, 256, 192, 512)       13824     \r\nbatch_normalization (BatchNo (32, 256, 192, 512)       2048      \r\nre_lu (ReLU)                 (32, 256, 192, 512)       0         \r\nconv2d_transpose (Conv2DTran (32, 512, 384, 256)       2097152   \r\nbatch_normalization_1 (Batch (32, 512, 384, 256)       1024      \r\nre_lu_1 (ReLU)               (32, 512, 384, 256)       0         \r\nTotal params: 2,114,048\r\nTrainable params: 2,112,512\r\nNon-trainable params: 1,536\r\n\r\nI think conv2d_transpose will: 2097152 / 256(groups) = 8192 params ? \r\n\r\nSorry for my writer is not good \r\nThanks you for reading \r\nThai Hoc\r\n", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/d8410424a44ce181b1e9fca6bd70e1fe/45216.ipynb). Thanks!", "Indeed, I get\r\n```python\r\n>>> conv_t = Conv2DTranspose(32, 3, 1, groups=32)\r\n>>> conv_t(tf.ones([1, 64, 64, 32]))\r\n>>> print(conv_t.kernel.shape)\r\n(3, 3, 32, 32)\r\n```\r\nwhile\r\n```python\r\n>>> conv = Conv2D(32, 3, 1, groups=32)\r\n>>> conv(tf.ones([1, 64, 64, 32]))\r\n>>> print(conv_t.kernel.shape)\r\n(3, 3, 1, 32)\r\n```\r\nYou could try this:\r\n```python\r\ndef grouped_conv2d_transpose(inputs, filters, kernel_size, strides, groups):\r\n    \"\"\"Performs grouped transposed convolution.\r\n\r\n    Args:\r\n        inputs: A `Tensor` of shape `[batch_size, h, w, c]`.\r\n        filters: The number of convolutional filters.\r\n        kernel_size: The spatial size of the convolutional kernel.\r\n        strides: The convolutional stride.\r\n        groups: The number of groups to use in the grouped convolution step.\r\n            The input channel count needs to be evenly divisible by `groups`.\r\n    Returns:\r\n        A `Tensor` of shape `[batch_size, new_h, new_w, filters]`.\r\n    \"\"\"\r\n    splits = tf.split(inputs, groups, axis=-1)\r\n    convolved_splits = [\r\n        tf.keras.layers.Conv2DTranspose(\r\n            filters // groups,\r\n            kernel_size,\r\n            strides\r\n        )(split) for split in splits\r\n    ]\r\n    return tf.concat(convolved_splits, -1)\r\n```\r\n", "@NguyenThaiHoc1,\r\nCan you please let us know if the above workaround worked for you? Thanks! ", "@NguyenThaiHoc1,\r\nCan you please respond to the above comment. Thanks! ", "Note that the workaround will be considerably slower on GPUs than regular grouped `conv2d` since it does not make use of the grouped CUDA kernel. Maybe this should be flagged as a bug.", "Can we set this as `contribution welcome`?", "I was able to replicate this issue in TF 2.7 .Attaching [Gist](https://colab.research.google.com/gist/mohantym/9434e0602d92b1167766ed3940b4599c/45216.ipynb#scrollTo=ceJRIVOPfZVJ) for reference. Thanks!"]}, {"number": 45202, "title": "model.inputs and model.outputs are None when creating a sub-classed model", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: no\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux 4.19.112+ x86_64 - running in Google Colab\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: N/A\r\n-   **TensorFlow installed from (source or binary)**: binary (pip)\r\n-   **TensorFlow version (use command below)**: v2.3.0-0-gb36436b087 2.3.0\r\n-   **Python version**: 3.6.9\r\n-   **Bazel version (if compiling from source)**: N/A\r\n-   **GCC/Compiler version (if compiling from source)**: N/A\r\n-   **CUDA/cuDNN version**: N/A\r\n-   **GPU model and memory**: N/A\r\n\r\n### Describe the problem\r\nI created a model using the [sub-classing method](https://www.tensorflow.org/guide/keras/custom_layers_and_models). As I wanted to examine the model inputs and outputs (using `model.inputs` and `model.outputs`), I found them to be `None`. On the contrary, when I created the same model using the [Sequential API](https://www.tensorflow.org/guide/keras/sequential_model), I do manage to see the inputs and outputs (using the aforementioned `inputs/outputs` attributes). \r\nI also checked these attributes after predicting with the model, and calling to `model.compile` - nothing works, the outputs are still `None`. \r\n\r\nI'd like to know how (or if?) it's possible to examine a sub-classed model's inputs and outputs this way.\r\n\r\n### Source code / logs\r\nYou can play with the code in this Colab notebook: \r\nhttps://colab.research.google.com/drive/1Oq0oF9aITXNGI9iI3mTsJkmDBoKacDB8?usp=sharing\r\n\r\nThe model creating code is taken from the official TF 2 quickstart guides (with some minor modifications):\r\nhttps://www.tensorflow.org/tutorials/quickstart/beginner\r\nhttps://www.tensorflow.org/tutorials/quickstart/advanced\r\n\r\n#### example 1 - Sequential API:\r\n```python\r\nimport TensorFlow as tf\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dense(10)\r\n])\r\n\r\nprint(model.inputs)\r\nprint(model.outputs)\r\n```\r\nThe output:\r\n```\r\n[<tf.Tensor 'flatten_5_input:0' shape=(None, 28, 28, 1) dtype=float32>]\r\n[<tf.Tensor 'dense_11/BiasAdd:0' shape=(None, 10) dtype=float32>]\r\n```\r\n\r\n#### example 2 - Sub-Classed Model:\r\n```python\r\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\r\nfrom tensorflow.keras import Model\r\n\r\nclass MyModel(Model):\r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n    self.flatten = Flatten(input_shape=(28, 28, 1))\r\n    self.d1 = Dense(128, activation='relu')\r\n    self.d2 = Dense(10)\r\n\r\n  def call(self, x):\r\n    x = self.flatten(x)\r\n    x = self.d1(x)\r\n    return self.d2(x)\r\n\r\n# Create an instance of the model\r\nmodel = MyModel()\r\nprint(model.inputs)\r\nprint(model.outputs)\r\n```\r\nThe output:\r\n```\r\nNone\r\nNone\r\n```\r\n\r\nNote: \r\nI noticed that when I run this code locally, I get `[]` instead of `None`, but the principle still exists. ", "comments": ["I have tried in colab with TF version 2.3,nightly version(`2.5.0-dev20201126`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/9e9bc1464748f43adb32be1135ce385a/untitled544.ipynb#scrollTo=aq5iXZRYiO4z). Thanks!", "Issue still exists in TF-Nightly 2.6 as well. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/df9e145f02310b54f89aadf6112689ef/untitled95.ipynb).", "Came across the same issue. Any suggested means to access the input list of a given subclassing model?"]}, {"number": 45179, "title": "Lamda Layer seems to ignore output_shape parameter", "body": "Hello Tensorflow/Keras - Team,\r\n\r\nI noticed some strange behaviour when I tried to create a simple sequential model with a lambda layer in it. In my example below I tried to make the code as simple as possible to clearify the problem.\r\n\r\nMy Lambda function is as simple as the minimal example from keras page:\r\n\r\n```\r\ndef square_it(x):\r\n    return (x ** 2)\r\n```\r\n\r\nTo calculate the output shape I defined the following function (please note that I tried to explicit set an output shape that is wrong in my example. But from my understanding this value should overwrite any shape calculation from the lambda layer and return the output shape I defined):\r\n\r\n```\r\ndef output_of_lambda(input_shape):\r\n    return (None, None, 10, 20)\r\n```\r\n\r\nCreating the lambda layer as follows:\r\n\r\n`lambda_layer = layers.Lambda(square_it, output_shape=output_of_lambda)`\r\n\r\n\r\ncreating an example model:\r\n\r\n```\r\nmodel_2 = keras.Sequential(\r\n    [\r\n        layers.Input(shape=(784)),\r\n        layers.Dense(2, activation=\"relu\", name=\"layer1\"),\r\n        lambda_layer,\r\n        layers.Dense(4, name=\"layer5\"),\r\n    ]\r\n)\r\n```\r\n\r\n\r\nAnd finally call the summary\r\n\r\n`model_2.summary()`\r\n\r\n\r\n\r\nNow I would expect to find my customized shape inside the summary or at least an error that the shape is wrong but instead it gives me:\r\n\r\n```\r\n\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nlayer1 (Dense)               (None, 2)                 1570      \r\n_________________________________________________________________\r\nlambda (Lambda)              (None, 2)                 0         \r\n_________________________________________________________________\r\nlayer5 (Dense)               (None, 4)                 12        \r\n=================================================================\r\n```\r\n\r\n\r\n\r\nBut I would have expected something like this:\r\n\r\n```\r\n\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nlayer1 (Dense)               (None, 2)                 1570      \r\n_________________________________________________________________\r\nlambda (Lambda)              (None, None, 10, 20)                 0         \r\n_________________________________________________________________\r\nlayer5 (Dense)               (None, 4)                 12        \r\n=================================================================\r\n```\r\n\r\nFor me it seems that the output_shape parameter has no effect at all but instead the value will always be automatically calculated.\r\n\r\nI am using POP OS 20.04 \r\nTensorflow: 2.3.1\r\nPython: 3.8.2\r\nNo Cuda was used in this example.", "comments": ["I have tried in colab with TF version 2.3, 2.4, nightly version (`2.5.0-dev20201129`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/14f878fbec332ce3416a0f04ceb8b9e8/untitled552.ipynb).Thanks!", "Issue still exist in TF 2.5 stable version and Nightly 2.6 as well. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/ba5c05c34cd98432fd4c622788801dbc/untitled552.ipynb#scrollTo=4DjxNEJh_euy).Thanks!"]}, {"number": 45171, "title": "GFile does not create file when nothing is written.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary (pip3)\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.io.gfile.GFile` does nothing to the filesystem when it is closed without any write operations:\r\n```python\r\nwith tf.io.gfile.GFile('foo', 'wb') as fp:\r\n  fp.write('test')  # 'foo' is created.\r\n\r\nwith tf.io.gfile.GFile('bar', 'wb'):\r\n  pass  # 'bar' is not created.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe same behavior with builtin `open`: creates a file even if no write operation is invoked.\r\n\r\n**Standalone code to reproduce the issue**\r\nAlready described above.\r\n\r\n**Other info / logs**\r\nNA", "comments": ["I have tried in colab with TF 2.3, nightly version( `2.5.0-dev20201125`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d483ed1238245b325cc6b63b8c5a10c4/untitled537.ipynb). Thanks!", "@odashi,\r\nAs per the [Test Code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/ram_file_system_test.py#L88-L93), it works if we add `fp2.write('')` instead of `pass`.\r\n\r\nPlease find the [Gist](https://colab.research.google.com/gist/rmothukuru/dd8482e088c90957bac4ced1e82055bd/gh_45171.ipynb#scrollTo=PAN41kIO2tb6).", "@rmothukuru \r\nThanks for pointing it out. Calling `FGile.write()` with an empty string seems to do something to the inner filesystem explicitly by a user. It is still not a desired solution to me -- matching the behavior with `open`.", "Still an issue in TF 2.5 and Nightly.Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/5fd782be1477e16ec68223ad564606ba/untitled87.ipynb).Thanks!"]}, {"number": 45155, "title": "TFLite - Java - Executing models on JVM not Android", "body": "**System information**\r\n- Linux Ubuntu 16.04, Windows\r\n- Not mobile device. JDK or JRE.\r\n\r\n**Example application using maven to compile and run an tensorflow lite project on JVM**\r\n\r\nIn the examples I only find bindings for Android. Which jar files (with maven coordinates) would I need\r\nto run / evaluate tensorflow models on a JVM.\r\n\r\nThanks,\r\nDieter", "comments": ["It would be great to have such an example. It would allow to test common ml code against a huge dataset without data copying and android as dependency.", "Thanks for the request. Are you planning to deploy your ML workload in a desktop Java environment? Or you primarily want to test it (but ultimately deploy on a mobile device)?", "Thanks for the response! \r\n\r\nWe are going to test on a not-Android host ml-code base then deploy full app on an Android device. We have several Gb test videos so one test takes too much time. We use tf with *.pb models for now but it is not 100% correct because the app uses tflite. ", "I see, thanks. I'm assuming your test harness won't be using bazel for execution? If it were, this gets a lot easier.\r\n\r\nHistorically, we haven't hosted desktop-compatible targets on Maven, as there hasn't been strong demand. Would it be sufficient to provide bazel commands and/or a script which generates the necessary .jar + shared libraries?", "Yes. For us, it would be sufficient to have a script or a list bazel commands.\r\nThanks!", "Same here.\r\n\r\nI would like to have the same Java code running on both desktop and Android devices, and having tflite libraries working on regular JVM would be very helpful.\r\n\r\nAlso, the lower overhead of tflite is desirable anyway ;)\r\n\r\nHaving it as a just-works maven dependencies would be ideal, but the bazel commands you suggested would work too :)", "I really would like to have them as maven dependency. That's the way how you share code on the JVM, ain't it?", "@xunkai55 what are your thoughts on hosting desktop-specific TFLite builds on MavenCentral?", "@jdduke I don't see any technical blocker to have a new project (maybe \"tensorflow-lite-java\" or \"tensorflow-lite-jvm\") on MavenCentral.", "Not really a problem, but it this necessary to have 2 distinct project names?\r\nMost libraries just-work when used on either desktop or Android.", "@paulo-raca can you point to a Maven library that works on both desktop and Android, and also bundles native (C++) libraries? As you noted, if we can keep the library names the same, great, but the packaging around native libraries and resources is different with Android, so I'd be curious if there's an example project structure we can follow.", "Ah, good point :man_facepalming: \r\n\r\nMaybe you could take advantage on the fact that android uses the AAR artifact and desktop uses JAR? :thinking: \r\nBut I really haven't seem this done anywhere :sweat_smile: ", "Hi there,\r\n\r\nWe're visiting this and we plan to provide something like \"tensorflow-lite-java\" in the future. But due to priority limits, it's unlikely to happen in the next 3 months.", "@dibog \r\nIs this still an issue,\r\nCould you please let us know if this is still an issue in latest stable TF v2.6.0 ?Thank you!", "Hi @Saduf2019 ,\r\n\r\nwhere would I find the required libraries to run my TFLite tests on a JVM (best using Maven)?\r\nAs soon as I know that I could run my test.\r\n\r\nWith best regards,\r\nDieter", "any update on when `tensorflow-lite-java` would be developed/released?\r\n\r\n> Hi there,\r\n> \r\n> We're visiting this and we plan to provide something like \"tensorflow-lite-java\" in the future. But due to priority limits, it's unlikely to happen in the next 3 months.\r\n\r\n", "as a workaround, i extracted the aar package to grab the jars, build the jni binary by executing\r\n```\r\nbazel build -c opt --fat_apk_cpu=x86_64  //tensorflow/lite/java:libtensorflowlite_jni\r\n```\r\nand put this jni lib with `-Djava.library.path=yourPathOfLib`\r\nbut yes. this could be provided by a all in one dependency (see also various javacpp project for opencv/ffpmeg..)\r\n\r\nmy use case: Running inference on a x64 barebone with a coral tpu. You need `tflite` for, but cpu and memory must be more powerful to track multiple cams. Using a gpu on a barebone would be the alternative. So tflite is not only good for embedded/small devices. \r\n\r\n", "I have the same issue as well, I try to test a small chatbot model I have created and it would be perfect to be able to test it on Desktop,  especially now when Jetpac Compose can be used to create desktop applications, I think many Android developers will find it exciting to build small tools on desktop environment and having TFLite for such situations will be great!"]}, {"number": 45151, "title": "Allow --cpu=x64_arm64_windows as a compilation target on Windows", "body": "Compilation of TensorFlow on Windows currently assumes x64 as a target CPU architecture. Searching .bzl files for \"x64_windows\" shows the many places in TF where this assumption is made. \r\n\r\nIn principle, though, nothing should prevent compiling TensorFlow to target ARM64 on Windows (--cpu=x64_arm64_windows, see the [bazel doc](https://github.com/bazelbuild/bazel/blob/master/site/docs/windows.md)).\r\n\r\nThis feature request is about adding support for '--cpu=x64_arm64_windows' to produce ARM64 binaries. The goal is to be able to use TF on Windows on ARM 64 bits.", "comments": ["Will happily review and approve a PR", "I would happily work on a PR, but unfortunately my knowledge of Bazel and TF build system is nearly zero. Passing --cpu=x64_arm64_windows causes the TF build system to download LLVM instead of using MSVC. \r\n@mihaimaruseac : Can you hint where to start fixing/patching? The initial goal is to get the MSVC toolchain invoked, then I should be able to work on the required code changes.", "TF needs to download LLVM to build as it contains a lot of code that uses LLV libraries directly (MLIR)"]}, {"number": 45114, "title": "GPU and CPU utilization slashed after first epoch using tf.data and tf.keras", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.12.1-46274-g410a6d8f46 2.5.0-dev20201121\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: CUDA 11.1/cuDNN-11.1-8.0.5.39\r\n- GPU model and memory: Nvidia RTX 3090 24GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nTraining performance of a 16-layer CNN (VGG16) decreases dramatically after the first epoch. I am using tf.data to load ImageNet. The first epoch performs as well as can be expected on my machine (training time ~1hr), with CPU utilization of 65% and GPU at 88%. \r\n\r\nEpoch 2 has CPU utilization of 40.5% with the GPU at ~35% (usage appears spiky, indicating some sort of bottle neck that is idling the GPU). Data is being loaded from an NVMe SSD and there is no discernable reason for performance to be reduced on the second epoch (i.e., SSD temperature is not abnormally high). I suspect tf.data is the culprit but I don't know why -- this code works on a different system (read on below).\r\n\r\nThe dataset is ImageNet. \r\n\r\n**Describe the expected behavior**\r\n\r\nThere should be no performance difference between Epoch 1 and subsequent epochs (subject to thermal throttling, which I have confirmed is not an issue on my system). When running this same code on a different system (w/ Titan V GPU and HDD), performance is sustained. The version of TF there is different: v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n\r\nThe official TF2 package does not appear to support RTX 3090 GPUs on Windows, which require CUDA 11.1. Hence, I used a tf-nightly-gpu release.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nComplete code is attached.\r\n\r\nThe relevant bit is probably the tf.data dataset construction and how the fit command is executed:\r\n\r\n```\r\n# Construct tf.data.Dataset\r\npaths_ds = tf.data.Dataset.from_tensor_slices(image_paths)\r\npaths_ds = paths_ds.shuffle(buffer_size=len(image_paths), reshuffle_each_iteration=True)  # reshuffle each epoch\r\ndataset = paths_ds.map(\r\n  lambda path: (self.load_image(path), self.get_onehot_label(path, tf_class_index_by_image, self.num_classes)),\r\n  num_parallel_calls=num_threads\r\n)\r\ndataset = dataset.batch(batch_size)\r\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)  #TODO: why does this sometimes slow things down?\r\nself.dataset = dataset\r\n```\r\n\r\nThe intent of this pipeline is to shuffle the dataset completely each epoch by shuffling the file names. Then, I would like to prefetch and load -- in parallel -- as many images as possible.\r\n\r\n```\r\nmodel.fit(callbacks=callbacks_list, x=data.dataset, epochs=options.epochs)\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nWindows performance monitoring screenshots are attached. Please note the difference between CPU_Epoch1 / GPU_Epoch1 and CPU_Epoch2 / GPU_Epoch2 image sets.\r\n\r\n\r\nSample code: [VGG_Bug_Sample_Code.txt](https://github.com/tensorflow/tensorflow/files/5585368/VGG_Bug_Sample_Code.txt)\r\n\r\nCPU Epoch 1:\r\n\r\n![CPU_Epoch1](https://user-images.githubusercontent.com/1285573/100008084-d3a88580-2d81-11eb-85bf-0f85b413ceff.jpg)\r\n\r\nGPU Epoch 1:\r\n\r\n![GPU_Epoch1](https://user-images.githubusercontent.com/1285573/100008115-dc00c080-2d81-11eb-8eff-0db5be3e4297.jpg)\r\n\r\nCPU Epoch 2 (reduced utilization):\r\n\r\n![CPU_Epoch2](https://user-images.githubusercontent.com/1285573/100008098-d73c0c80-2d81-11eb-99cc-bdf9026ca071.jpg)\r\n\r\nGPU Epoch 2 (substantial bottleneck visible):\r\n\r\n![GPU_Epoch2](https://user-images.githubusercontent.com/1285573/100008137-e0c57480-2d81-11eb-9d34-3eae6ba67d59.jpg)\r\n", "comments": ["@trzy \r\nI ran the code shared on tf 2.3 and face a [different error](https://colab.research.google.com/gist/Saduf2019/35e3437143aea16a3bdaec2f843057cb/untitled471.ipynb), can you please share a colab gist with the error reported.", "This error occurs if the dataset directory is incorrect. Do you have access to ImageNet (it needs to be preprocessed a bit) or any other image data set? If not I will try to see if I can generate synthetic data in the loading stage.\n\nSent from my iPhone\n\n> On Nov 24, 2020, at 6:22 AM, Saduf2019 <notifications@github.com> wrote:\n> \n> \ufeff\n> @trzy\n> I ran the code shared on tf 2.3 and face a different error, can you please share a colab gist with the error reported.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "I tried generating synthetic data to replicate the problem but was unable to (1.2M simple JPEGs at 6GB total is not equivalent to 1.2M JPEGs occupying 160GB as with the ImageNet dataset). \r\n\r\nHowever, I got Tensorboard profiling up and captured some very interesting traces that highlight the problem.\r\n\r\n<b>Epoch 1 overview</b> (most of the time is spent computing on the GPU):\r\n\r\n![Epoch1_overview](https://user-images.githubusercontent.com/1285573/100284946-dabbc900-2f24-11eb-8b59-068cb3281ca8.jpg)\r\n\r\n<b>Epoch 2 overview (input bottleneck appears)</b>:\r\n\r\n![Epoch2_overview](https://user-images.githubusercontent.com/1285573/100285006-f3c47a00-2f24-11eb-87d8-1989d3cfa9c0.jpg)\r\n\r\nWhat's going on?\r\n\r\nTrace view indicates that in Epoch 1, data loading for the subsequent batch happens across four threads during training of the current batch. Loading a batch takes <50% of the training time, so by the time the current batch is completed, it is ready to proceed to the next.\r\n\r\n<b>Here is Epoch 1's trace</b>:\r\n\r\n![Epoch1_prefetch](https://user-images.githubusercontent.com/1285573/100285181-4aca4f00-2f25-11eb-85cf-f0ca57f01e96.jpg)\r\n\r\nZooming in, we can see that the loading operation consists of reading, JPEG decoding, a cast, and a resize. They happen one after another.\r\n\r\n![Epoch1_prefetch_mid_training](https://user-images.githubusercontent.com/1285573/100285237-633a6980-2f25-11eb-8a4f-72b7b17ef891.jpg)\r\n\r\n<b>Now compare Epoch 2</b>:\r\n\r\n![Epoch2_saturated](https://user-images.githubusercontent.com/1285573/100285287-7a795700-2f25-11eb-8659-75d3549ce997.jpg)\r\n\r\nWe can see the input pipeline has slowed down dramatically and it takes longer than a training step to load a batch.\r\n\r\n<b>Why are there gaps between the different input pipeline steps in Epoch 2?</b> There is idle time between reading the file, decoding the JPEG, resizing, etc. I suspect this is why Iterator::MapAndBatch is now visible in the trace (these calls take virtually no time in the first epoch).\r\n\r\nHere is a clearer view of what's going on in Epoch 2 (note gaps in the timeline on each tf_Compute thread):\r\n\r\n![Epoch2_gaps](https://user-images.githubusercontent.com/1285573/100293860-e8c81480-2f39-11eb-8cdc-f332ffaf33dd.jpg)\r\n\r\nI hope this is more helpful. ", "Just popping in to add some more information. This problem appears to be very similar to mine: https://github.com/fizyr/keras-retinanet/issues/322\r\n\r\nIt appears that in their case, a driver update fixed the problem. Googling around has confirmed that Nvidia 30-series GPUs require CUDA 11.1, which Tensorflow is not built against. Hence, tf-nightly-gpu. This may be a 3090-specific issue and it would be nice to understand whether the problem could be within CUDA or Tensorflow."]}, {"number": 45110, "title": "Perhaps TF graph connectivity issues in keras-based Model containing Masking layers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n[gist](https://gist.github.com/klausk55/b2a9117989d32b8f55ad6aeb99532728)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu-20.04 on a Windows Subsystem for Linux (WSL)\r\n- TensorFlow version (use command below):\r\n2.3.1 v2.3.0-54-gfcc4b966f1\r\n- Python version:\r\n3.8.2 64-bit using a venv\r\n- Ray[rllib] version (RL framework):\r\n1.0.1.post1\r\n- Text editor (development environment):\r\nVisual Studio Code\r\n\r\n**Describe the current behavior**\r\nGet `InvalidArgumentError: You must feed a value for placeholder tensor` if line 73 in _articleSchedulingModel.py_ is active, instead comment in line 74 and comment out line 73 then there is no error.\r\nIt seems to me that perhaps there might be an issue with connectivity of the TF graph similar to a former related issue mentioned in this [annotation](https://github.com/ray-project/ray/blob/084f03797b7a88e4ba491b329eb6e1634905fb72/rllib/examples/models/batch_norm_model.py#L81):\r\n\r\n> IMPORTANT NOTE: This model will not work with PPO due to a bug in keras\r\n    that surfaces when having more than one input placeholder (here: `inputs`\r\n    and `is_training`) AND using the `make_tf_callable` helper (e.g. used by\r\n    PPO), in which auto-placeholders are generated, then passed through the\r\n    tf.keras. models.Model. In this last step, the connection between 1) the\r\n    provided value in the auto-placeholder and 2) the keras `is_training`\r\n    Input is broken and keras complains.\r\n\r\nMy assumption is that some connection gets lost in the tf.keras.Model caused by the masking layer defined in line 64 and further connected to the concatenate layer defined in line 73 (in _articleSchedulingModel.py_).\r\n\r\nIs the assumption appropriate or what else might cause this error?\r\n\r\n**Standalone code to reproduce the issue**\r\nLink to [gist](https://gist.github.com/klausk55/b2a9117989d32b8f55ad6aeb99532728)\r\n\r\n**Other info / logs**\r\nTraceback also in [gist](https://gist.github.com/klausk55/b2a9117989d32b8f55ad6aeb99532728)\r\n", "comments": ["Update: For now, I am quite sure that the masking is causing the issue, but I do not understand why.\r\nThe masking layer in line 58 works fine, but including further masking layers (e.g. in line 64) leads to the error above.", "@klausk55 \r\nPlease provide with \"simple stand alone code\" to replicate the issue or if possible share a colab gist with the error reported.", "@Saduf2019\r\nAdditional to the shared gist from above, I share a [colab gist](https://colab.research.google.com/gist/klausk55/7c5c1a87fee088e233bec7a0181620e8/invalidargerr.ipynb)", "A question regarding masking which might cause my reported issue:\r\nFirst, can a TF keras model only have one mask or is it feasible to have more different masks? If so, must all present masks have at least the same shape and otherwise connections in TF graph gets broken?", "I think I know what triggers the error. For example, the computation graph of a keras model as depicted on the left-hand side works fine, but the example on the right-hand side raises the InvalidArgumentError from above.\r\nThe first concatenation of graph parts which contain connections to layers.Masking is, in my opinion, okay, but a later further concatenation of graph parts which again contain connections to layers.Masking seems to trigger the error.\r\n\r\nI do not know the reason behind, but it seems to be not feasible or not supported yet. Any suggestions?\r\n\r\n![Masking_concatenate](https://user-images.githubusercontent.com/63732956/100236695-49e9eb00-2f2e-11eb-9231-0b05f616ff93.png)\r\n", "I am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e13fd5583fb59e3ef896536fa51415ec/untitled473.ipynb).", "Any feedback or ideas?", "While trying to reproduce your issue in Tf Nightly 2.6 facing different error, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/fad1c0a7582c51a53214efcbb246f37c/45110.ipynb), Thanks!", "Was able to replicate issue in `tf-nightly(2.8.0-dev20211009)`, please find the gist [here](https://colab.research.google.com/gist/chunduriv/aae1f965adf9cb29ba70036aab46bf87/45110.ipynb#scrollTo=OhLgtaOO6UbF).Thanks!"]}]