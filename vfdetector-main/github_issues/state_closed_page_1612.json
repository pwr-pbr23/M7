[{"number": 4551, "title": "Feature request: add TensorFlow Serving to conda-forge as well", "body": "[TensorFlow made it into conda-forge](https://github.com/conda-forge/tensorflow-feedstock) which is awesome, but could TensorFlow Serving be provided as well for easier CI? Installing bazel just for building Serving during a CI run is a bit excessive.\n", "comments": ["@nfiedel - could you take a look?  Thanks!\n", "Thanks for the ping. Community contributions are welcome.\n\n@kirilg can provide guidance or may be able to get to this eventually...\n", "Marking as \"contributions welcome\" as per @nfiedel .", "We do not maintain the conda-forge repository for tensorflow.\r\nTherefore I will close this issue."]}, {"number": 4550, "title": "Multilabel image classification with sparse labels?", "body": "I want to perform a multilabel image classification task for n classes.\nI've got sparse label vectors for each image and each dimension of each label vector is currently encoded in this way:\n-   1.0 ->Label true / Image belongs to this class\n- -1.0 ->Label false / Image does not contain to this class.\n-  0.0 ->missing value/label\n\nE.g.: V= {1.0,-1.0,1.0, 0.0}\n\nFor this example V the model should learn, that the corresponding image should be classified in the first and third class.\n\nMy problem is currently how to handle the missing values/labels. I've searched through the issues and found this issue:\n[https://github.com/tensorflow/skflow/issues/113](https://github.com/tensorflow/skflow/issues/113)\n\nSo could do multilable image classification with:\n[tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard5/tf.nn.sigmoid_cross_entropy_with_logits.md)\n\nbut TensorFlow has this error function for sparse softmax, which is used for exclusive classification:\n[tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard4/tf.nn.sparse_softmax_cross_entropy_with_logits.md)\n\nSo is there something like sparse sigmoid cross entropy? (Couldn't find something) or any suggestions how can I handle my multilabel classification problem with sparse labels.\n", "comments": ["Thanks for the question @zorancupic!\n\nWe primarily use github issues to track bugs and installation problems.  This is a question better suited for StackOverflow.  Please ask it there and tag it with the `tensorflow` tag.  Thanks!\n", "@tatatodd But what about feature requests? It seems to me that @zorancupic is asking for a cost functional `tf.nn.sparse_sigmoid_cross_entropy_with_logits(logits, labels, name=None)` analogously to the already provided ``tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None)`.\n", "@tatatodd I asked this question on Stackoverflow and didn't get the answer, that this cost function is provided in TensorFlow. But like @FlorianWilhelm mentioned, this would be a good Feature Request. Sparse labels and the multilabel image classification task often occur. \n\nHere is the link to the StackOverflow question: [http://stackoverflow.com/questions/39697216/multilabel-image-classification-with-sparse-labels-in-tensorflow/39737057?noredirect=1#comment66817206_39737057](http://stackoverflow.com/questions/39697216/multilabel-image-classification-with-sparse-labels-in-tensorflow)\n", "@zorancupic @FlorianWilhelm My apologies, I missed the feature request for adding a `tf.nn.sparse_sigmoid_cross_entropy_with_logits` function.\n\nWe definitely welcome pull requests for new functionality!\n", "So looking at this issue and the documentation of `tf.nn.sparse_softmax_cross_entropy_with_logits`, again I think there might be some confusion here about the word \"sparse\" here. This function is actually only a shortcut for a one-hot encoding and applying `tf.nn.softmax_cross_entropy_with_logits` afterwards as explained [here](http://stackoverflow.com/questions/34240703/difference-between-tensorflow-tf-nn-softmax-and-tf-nn-softmax-cross-entropy-with). I think @zorancupic understanding and also mine was that \"sparse\" allows handling of missing labels. Since this is not the case, a method like `tf.nn.sparse_sigmoid_cross_entropy_with_logits` makes no sense at all I guess. \n\nRegarding the problem of missing labels, I think one could easily solve the problem by applying one of TensorFlow's loss functions that has a `weight` parameter, e.g. `tf.contrib.losses.log_loss`. Using a binary vector as weight that is 0 for missing values and 1 for known labels should do the trick. \n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 4549, "title": "tensorboard can not work", "body": "when i run tensorboard demo:\n1. python mnist_with_summaries.py   OK\n2. tensorboard --logdir=/tmp/mnist_logs/  \nprint like this \nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/webcomponentsjs/webcomponents-lite.min.js' on path /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/webcomponentsjs/webcomponents-lite.min.js\n127.0.0.1 - - [23/Sep/2016 16:55:12] \"GET /lib/css/global.css HTTP/1.1\" 200 -\n127.0.0.1 - - [23/Sep/2016 16:55:12] \"GET /webcomponentsjs/webcomponents-lite.min.js HTTP/1.1\" 200 -\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/dist/bazel-html-imports.html' on path /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/dist/bazel-html-imports.html\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/external/dist/bazel-html-imports.html' on path /usr/local/lib/python2.7/dist-packages/external/dist/bazel-html-imports.html\n127.0.0.1 - - [23/Sep/2016 16:55:12] code 404, message Not Found\n127.0.0.1 - - [23/Sep/2016 16:55:12] \"GET /dist/bazel-html-imports.html HTTP/1.1\" 404 -\n127.0.0.1 - - [23/Sep/2016 16:55:12] \"GET /dist/tf-tensorboard.html HTTP/1.1\" 200 -\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/polymer/polymer.html' on path /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/polymer/polymer.html\n127.0.0.1 - - [23/Sep/2016 16:55:12] \"GET /polymer/polymer.html HTTP/1.1\" 200 -\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/iron-icons/iron-icons.html' on path /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/iron-icons/iron-icons.html\n.......\n\nNo such file or directory error \n\nhow can i install these file ?\n", "comments": ["change firefox to chrome can slove this problem.\n", "I had this problem too with 0.11rc.  0.10 doesn't have any issues with firefox though.\n", "Please @cdmajie reopen this issue, firefox is important enough to take a closer look.\n"]}, {"number": 4548, "title": "r0.10: ImportError: No module named tensorflow", "body": "My code runs well on r0.9, and after I upgrade to r0.10, and it runs failed. And my TF r0.10 is built from source also.\n", "comments": ["More information: since r0.10 needs setuptool > 1.7, and the installed setuptool is 0.98, then I update it to 27.3. And then I switch back to TF r0.9, encounter the same error. \n\nAnyone use r0.10, and what's the version of setuptool used?\n", "How did you tried to revert it back to previous build ?\n", "I can't revert it back, it seems the python issue. Now, my yum can't work after I delete /usr/lib/python2.7/site-packages/. I am not sure whether such packages, like python, setuptools are OS related, and I use centos 7. I dowload the python 7.5.12 source code to rebuild it, and install. But it still can't work.\n\nAfter I dowload setuptool27.3 and install it, even r0.9 can't work, and it can work well before...\n\nroot@hal-dev share]# yum\nThere was a problem importing one of the Python modules\nrequired to run yum. The error leading to this problem was:\n\n   No module named yum\n\nPlease install a package which provides this module, or\nverify that the module is installed correctly.\n\nIt's possible that the above module doesn't match the\ncurrent version of Python, which is:\n2.7.5 (default, Sep 15 2016, 22:37:39) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)]\n\nIf you cannot solve this problem yourself, please go to \nthe yum faq at:\n  http://yum.baseurl.org/wiki/Faq\n\n[root@hal-dev share]# \n\n[alvin@hal-dev mnist]$ \n[alvin@hal-dev mnist]$ python convolutional.py \nTraceback (most recent call last):\n  File \"convolutional.py\", line 31, in <module>\n    import numpy\nImportError: No module named numpy\n[alvin@hal-dev mnist]$ \n", "Here we have multiple issues !!\n\nBecause it's conflicting with YUM package manager and python modules, if possible could you provide cat convolutional.py \n\nOne more doubt have you installed numpy module inside pyhton ?\n", "Firstly, my original r0.9 works well, then I want to upgrade to r0.10, since r0.10 need setuptool > 1.7, and I updated setuptools to 27.3 manually, then both r0.10 and r0.9 can't work. Then I do a lot of OPs to python, then yum can't work after I delete /usr/lib/python2.7/site-packages since I try to update it by re-install pip.\n'convolutional.py' is the example of mnist in repo, and I haven't change it.\n", "@vigneshstack I copy the 'site-packages' from another machine, and now the yum can work. But my r0.9 still can't work for the example 'tensorflow-r0.9/tensorflow/models/image/mnist'.. I follow the steps:\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n\nmkdir _python_build\ncd _python_build\nln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/\\* .\nln -s ../tensorflow/tools/pip_package/\\* .\npython setup.py develop\n", "BTW, the following works well:\n\n$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n\n$ bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n", "@jart Could you take a look at this? Thanks.\n", "The python issues are solved. \n\nHold on this issue, although I still can't run r0.10 due to my network connection while do configuring. \n", "4 nodes get 4x performance improving, and the network is the bottleneck.\n"]}, {"number": 4547, "title": "import tensorflow on apache2 cgi module", "body": "I try to run tensorflow on web server python cgi\nI succeed in running python in apache2 web server\n\nbut I stoped by environment variables :LD_LIBRARY_PATH set in Python Code\n\nI run os.environ:function and result print check\nThis result is correct value\nI don't know what can i do\n\nI need you help\n\n**my code is** \n\n```\n\nimport cgi\nimport os\nimport cgitb\ncgitb.enable()\n\nimport requests\n\nprint(\"Content-type: text/html;charset=utf-8\\r\\n\")\n\nprint(\"<html>\")\nprint(\"<head>\")\nprint(\"\\t<title>Python CGI Test</title>\")\nprint(\"</head>\")\nprint(\"<body>\")\nprint(\"\\t<h1>Hello, Python CGI!</h1>\")\nprint(\"</body>\")\nprint(\"</html>\")\n\n\nos.environ[\"LD_LIBRARY_PATH\"] = '$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64'\n\nprint os.environ\nprint (\"\")\n\n\nimport tensorflow as tf\nhello = tf.constant('Hello, TensorFlow!')\nsess = tf.Session()\nprint(sess.run(hello))\n```\n\n**and result**\n\n> Hello, Python CGI!\n> \n> {'CONTEXT_DOCUMENT_ROOT': '/var/www/html', 'SERVER_SOFTWARE': 'Apache/2.4.18 (Ubuntu)', 'CONTEXT_PREFIX': '', 'SERVER_SIGNATURE': '\n> Apache/2.4.18 (Ubuntu) Server at 192.168.0.115 Port 80\n> \\n', 'REQUEST_METHOD': 'GET', 'SERVER_PROTOCOL': 'HTTP/1.1', 'QUERY_STRING': '', 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'LD_LIBRARY_PATH': '$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64', 'HTTP_USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36', 'HTTP_CONNECTION': 'keep-alive', 'SERVER_NAME': '192.168.0.115', 'REMOTE_PORT': '57930', 'SERVER_PORT': '80', 'SERVER_ADDR': '192.168.0.115', 'DOCUMENT_ROOT': '/var/www/html', 'SCRIPT_FILENAME': '/var/www/html/123.py', 'SERVER_ADMIN': 'webmaster@localhost', 'HTTP_HOST': '192.168.0.115', 'SCRIPT_NAME': '/123.py', 'HTTP_UPGRADE_INSECURE_REQUESTS': '1', 'HTTP_CACHE_CONTROL': 'max-age=0', 'REQUEST_URI': '/123.py', 'HTTP_ACCEPT': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,_/_;q=0.8', 'GATEWAY_INTERFACE': 'CGI/1.1', 'REMOTE_ADDR': '192.168.0.115', 'HTTP_ACCEPT_LANGUAGE': 'ko-KR,ko;q=0.8,en-US;q=0.6,en;q=0.4', 'REQUEST_SCHEME': 'http', 'HTTP_ACCEPT_ENCODING': 'gzip, deflate, sdch'} --> -->\n> \n> <type 'exceptions.ImportError'>   Python 2.7.12: /home/mutation/anaconda2/bin/python2.7\n> Fri Sep 23 15:28:53 2016\n> A problem occurred in a Python script. Here is the sequence of function calls leading up to the error, in the order they occurred.\n> \n>  /var/www/html/123.py in ()\n>      41 \n>      42 \n> =>   43 import tensorflow as tf\n>      44 hello = tf.constant('Hello, TensorFlow!')\n>      45 sess = tf.Session()\n> tensorflow undefined, tf undefined\n>  /home/mutation/anaconda2/lib/python2.7/site-packages/tensorflow/**init**.py in ()\n>      19 from **future** import absolute_import\n>      20 from **future** import division\n>      21 from **future** import print_function\n>      22 \n> =>   23 from tensorflow.python import *\n> tensorflow undefined\n>  /home/mutation/anaconda2/lib/python2.7/site-packages/tensorflow/python/**init**.py in ()\n>      46 _default_dlopen_flags = sys.getdlopenflags()\n>      47 sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)\n> =>   48 from tensorflow.python import pywrap_tensorflow\n>      49 sys.setdlopenflags(_default_dlopen_flags)\n>      50 \n> tensorflow undefined, pywrap_tensorflow undefined\n>  /home/mutation/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py in ()\n>      26                 fp.close()\n>      27             return _mod\n> =>   28     _pywrap_tensorflow = swig_import_helper()\n>      29     del swig_import_helper\n>      30 else:\n> _pywrap_tensorflow undefined, swig_import_helper = None\n>  /home/mutation/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py in swig_import_helper()\n>      22         if fp is not None:\n>      23             try:\n> =>   24                 _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n>      25             finally:\n>      26                 fp.close()\n> _mod undefined, imp = <module 'imp' (built-in)>, imp.load_module = <built-in function load_module>, fp = <closed file '/home/mutation/anaconda2/lib/pytho...sorflow/python/_pywrap_tensorflow.so', mode 'rb'>, pathname = '/home/mutation/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so', description = ('.so', 'rb', 3)\n> <type 'exceptions.ImportError'>: libcudart.so.8.0: cannot open shared object file: No such file or directory \n>       args = ('libcudart.so.8.0: cannot open shared object file: No such file or directory',) \n>       message = 'libcudart.so.8.0: cannot open shared object file: No such file or directory'\n", "comments": ["Hi, were you able to find a solution to this?", "I'm sorry my answer too late\r\n\r\nI found solution \r\n\r\nThe solution is that set environment value in apache config", "Can you elaborate on the solution @dotdotgod ? "]}, {"number": 4546, "title": "Update os_setup.md cuDNN install instructions", "body": "To install cuDNN:\n\nThe file `cudnn.h` is included in the destination for `cp` command since `/usr/local/cuda/include` is a directory.\n", "comments": ["Can one of the admins verify this patch?\n", "@rowjay, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @keveman and @vrv to be potential reviewers\n", "Sorry, I don't understand the need for this. What does this do?\n", "@martinwicke thanks, this was unnecessary; the directory is created by the previous step; installing the CUDA toolkit into `/usr/local/cuda`\n"]}, {"number": 4545, "title": "TypeError: Expected int for argument 'num' not <tf.Tensor 'Squeeze:0' shape=() dtype=int32>.", "body": "I run the following code, but it has one error at    \"tf.unpack(inputs,self.bathsize)\",the error is as follows:\nTypeError: Expected int for argument 'num' not <tf.Tensor 'Squeeze:0' shape=() dtype=int32>.\n\n```\n def build(n_dict, word_dim):\n       self.word_ids = tf.placeholder(tf.int32, shape=(None, None, 7), name='word_ids')\n        self.bathsize = tf.shape(self.word_ids)[0]\n        self.word_len=tf.shape(self.word_ids)[1]\n        with tf.device(\"/cpu:0\"):\n            self.embedding = tf.Variable(tf.random_uniform([n_dict, word_dim], -1.0, 1.0), name='embedding', dtype='float32')\n            self.embedded_input = tf.nn.embedding_lookup(self.embedding, self.word_ids)\n        inputs = tf.reshape(self.embedded_input, [self.bathsize, self.word_len, -1])\n        # inputs=tf.transpose(inputs,[1,0,2])\n        inputs = tf.nn.dropout(inputs, self.dropout)\n        self.inputs = inputs\n\n        self.input_length = tf.fill([self.word_len], self.bathsize)\n        self.input_length = tf.cast(self.input_length, dtype=tf.int64)\n        with tf.variable_scope('forward'):\n            self.lstm_fw_cell = rnn_cell.BasicLSTMCell(n_hidden)\n        with tf.variable_scope('backward'):\n            self.lstm_bw_cell = rnn_cell.BasicLSTMCell(n_hidden)\n        lstm_state_fw = self.lstm_fw_cell.zero_state(self.word_len, tf.float32)\n        lstm_state_bw = self.lstm_bw_cell.zero_state(self.word_len, tf.float32)\n        # sequence_length=self.input_length\n\n        outputs, output_state_fw, output_state_bw = rnn.bidirectional_rnn(\n            self.lstm_fw_cell, self.lstm_bw_cell,\n            tf.unpack(inputs,self.bathsize),\n            initial_state_fw=lstm_state_fw,\n            initial_state_bw=lstm_state_bw,\n            sequence_length=self.input_length\n        )\n```\n\nAs I know, tf.unpack have one parameter num, when the code run to \"tf.unpack(inputs,self.bathsize)\", num=None.  When the bachsize if fixed, it succeed to run. But  I want the model inputs have variable bacthsize, do you have any idea to solve this?@Geoffrey Irving\n", "comments": ["Here is the documentation for `tf.unpack`:\nhttps://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#unpack\n\nNote this line in particular:\nIf num is not specified (the default), it is inferred from value's shape\n\nTry changing your code above to just the following:\n\n```\n  tf.unpack(inputs)\n```\n", "Automatically closing due to lack of recent response. Please reopen when additional information becomes available. Thanks!\n", "@tatatodd removing the parameter `num` will cause a `ValueError: Cannot infer num from shape` and cannot solve anything.\r\nI'm now having the same problem when trying to use `tf.unpack`. Could anyone give an effective solution to  this?"]}, {"number": 4544, "title": "A problem when indexing tensor, I'm not sure if it is related to gather_nd issue", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nMight be related to https://github.com/tensorflow/tensorflow/issues/206\n### Environment info\n\ncuda-7.5\ntensorflow 0.10.0rc0\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n```\n# logit_t is tensor of shape (batch_size, number_of_label)\n# correct_t is targets, of size  (batch_size)\n\nlogit_corrects = []\n# I loop through batch_size because gather_nd doesn't support gradient\nfor i in xrange(batch_size):\n    logit_correct = tf.gather( logit_t[ i , : ], correct_t[ i ] ) \n    logit_corrects.append( logit_correct )\n\nlogit_corrects = tf.pack( logit_corrects )\nself._cost = cost = tf.reduce_mean( -logit_correct )\n```\n\nHowever, it complained that there is no supported kernel for GPU device. \n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n```\nE tensorflow/core/client/tensor_c_api.cc:485] Cannot assign a device to node 'model/gradients/model/Squeeze_159_grad/Res\nhape/tensor': Could not satisfy explicit device specification '/device:GPU:3' because no supported kernel for GPU device\ns is available.\n         [[Node: model/gradients/model/Squeeze_159_grad/Reshape/tensor = UnsortedSegmentSum[T=DT_FLOAT, Tindices=DT_INT3\n2, _device=\"/device:GPU:3\"](model/gradients/model/Gather_39_grad/Reshape, model/gradients/model/Gather_39_grad/Reshape_1\n, model/gradients/model/Squeeze_159_grad/Reshape/Squeeze)]]\nTraceback (most recent call last):\n  File \"learning3.py\", line 976, in <module>\n    tf.initialize_all_variables().run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1553, in run\n    _run_using_default_session(self, feed_dict, self.graph, session)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3684, in _run_using_default_ses\nsion\n    session.run(operation, feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 382, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 655, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 723, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 743, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'model/gradients/model/Squeeze_1\n59_grad/Reshape/tensor': Could not satisfy explicit device specification '/device:GPU:3' because no supported kernel for\n GPU devices is available.\n         [[Node: model/gradients/model/Squeeze_159_grad/Reshape/tensor = UnsortedSegmentSum[T=DT_FLOAT, Tindices=DT_INT3\n2, _device=\"/device:GPU:3\"](model/gradients/model/Gather_39_grad/Reshape, model/gradients/model/Gather_39_grad/Reshape_1\n, model/gradients/model/Squeeze_159_grad/Reshape/Squeeze)]]\nCaused by op u'model/gradients/model/Squeeze_159_grad/Reshape/tensor', defined at:\n  File \"learning3.py\", line 967, in <module>\n    m = Recognizer(is_training=True, config=config)\n  File \"learning3.py\", line 501, in __init__\n    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 478, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_grad.py\", line 291, in _SqueezeGrad\n    return _ReshapeToInput(op, grad)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_grad.py\", line 281, in _ReshapeToInput\n    return array_ops.reshape(grad, array_ops.shape(op.inputs[0]))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1750, in reshape\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 454, in apply_op\n    as_ref=input_arg.is_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 621, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 95, in _IndexedSlicesToTensor\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2350, in unsorted_segment_su\nm\n    num_segments=num_segments, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'model/Squeeze_159', defined at:\n  File \"learning3.py\", line 967, in <module>\n    m = Recognizer(is_training=True, config=config)\n  File \"learning3.py\", line 480, in __init__\n    tf.gather(logit_t[i,:], correct_t[i])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 340, in _SliceHelper\n    return squeeze(sliced, squeeze_dims=squeeze_dims)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2273, in squeeze\n    squeeze_dims=squeeze_dims, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n```\n", "comments": ["I can't immediately tell which Op is missing a GPU kernel.\n\nIn the mean time, can you set `allow_soft_placement` to True?  See tensorflow/g3doc/how_tos/using_gpu/index.md as an example. \n", "When I switched to CPU, it did run, but slower than I want it to. I thought that you can run gradient on tf.gather using GPU, but it seems like it couldn't. Am I correct? \n", "@tuandnvn can you try version 0.11:\nhttps://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#download-and-setup\n\nBased on the error it looks like the op with a missing GPU kernel is UnsortedSegmentSum, but that support was added in this commit:\nhttps://github.com/tensorflow/tensorflow/commit/88686b308146887a88903c883bed7affc650e076\n\nSo I'm expecting it to work in 0.11, or at least you'll run into a different problem.  :)\n", "I would try that, thanks tataodd for your new update.\n", "@tuandnvn, did this work for you, could I close the issue?\n", "Cleaning up issues mentioning 0.11.\nClosing due to lack of activity.\n"]}, {"number": 4543, "title": "ValidationMonitor + readers lead to: Coordinator didn't stop cleanly: Attempted to use a closed Session.", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nhttp://andyljones.tumblr.com/post/133267887103/tensorflow-placeholder-queue-errors\n\nAbove not totally relevant, but couldn't find much.\n### Environment info\n\nOperating System:\n\nRelatively recently updated Arch.\nLinux terrapin 4.6.5-4-ck #1 SMP PREEMPT Mon Aug 8 20:47:19 EDT 2016 x86_64 GNU/Linux\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n$ ls -l /usr/local/cuda/lib/libcud*\n-rw-r--r-- 1 root root 189170 Sep 17 01:09 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Sep 17 01:09 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Sep 17 01:09 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Sep 17 01:09 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Sep 17 01:09 /usr/local/cuda/lib/libcudart_static.a\n```\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n   https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\n$ python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.10.0\n```\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nWould be a bit of a hassle, and I think I already found the bug so...\n\nI've got a fairly simple model, it is fed by a `string_input_queue()` and then batched up by `shuffle_batch()` for training and `batch()` for evaluation. I'm using an `estimator.Estimator()` for the model and then calling `fit()` on the estimator with a `ValidationMonitor`.\n\nI get a bunch of errors in the output when trying this. Here are the relevant parts:\n\n```\nWARNING:tensorflow:Coordinator didn't stop cleanly: Attempted to use a closed Session.\nWARNING:tensorflow:Given features: {'clips': <tf.Tensor 'input_test-11/batch:0' shape=(100, 3840) dtype=float32>}, required signatures: {'clips': TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(100), Dimension(3840)]), is_sparse=False)}.\nWARNING:tensorflow:Given targets: Tensor(\"input_test-11/batch:1\", shape=(100,), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(100)]), is_sparse=False).\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:03:00.0)\nW tensorflow/core/kernels/queue_base.cc:294] _9_input_test-11/input_producer: Skipping cancelled enqueue attempt with queue not closed\nWARNING:tensorflow:Coordinator didn't stop cleanly: Enqueue operation was cancelled\n     [[Node: input_test-11/input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_test-11/input_producer\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_test-11/input_producer, input_test-11/input_producer/Identity)]]\nCaused by op 'input_test-11/input_producer/input_producer_EnqueueMany', defined at:\n  File \"./train.py\", line 320, in <module>\n    tf.app.run()\n```\n\nWithout the `ValidationMonitor`, everything is fine, but with it, most of these errors happen on every validate, and sometimes it throws the exception shown.\n\nOf particular relevance is the error `Coordinator didn't stop cleanly: Attempted to use a closed Session.` which pretty clearly points out the problem.\n\nYou can see the exact situation the error is complaining about here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/graph_actions.py#L757\nThere's even a comment saying that exactly that is about to be done, although I don't know why. If I move the `session.close()` after the `coord.join(...)` call, then all my problems go away.\n\nSeems like a pretty clear problem with a pretty clear fix. Let me know if you need more details.\n", "comments": ["@ilblackdragon seems to have written that code - can you respond to this?  Thanks!\n", "@ispirmustafa  is re-working the `evaluate` graph_actions function to make it use `MonitoredSession` which will make it robust to stopping before queue is closed or Coordinator finalized.\n", "Great! @ilblackdragon feel free to post here when the code is merged. Closing, feel free to reopen if the problem persists after our fixes.\n"]}, {"number": 4542, "title": "\"Convolution\" across two matrices", "body": "Are there any plans to implement \"convolutions\" wherein we have a moving patch over a second matrix that acts as a changing filter?\n\nThis would be useful for implementing kernels such as this one:\nhttps://papers.nips.cc/paper/5348-convolutional-kernel-networks.pdf\n\nI asked whether a restricted version of this could be achieved by using already existing tf ops on [stack overflow](http://stackoverflow.com/questions/39632849/dot-product-of-patches-in-tensorflow). After thinking about it for a while though I don't think there's a way to do this in an efficient manner\n\nFor a simple convolution with no strides I could do something like:\n`filters = tf.extract_image_patches(filter_matrix, [filter_height, filter_width], [1, 1, 1, 1], [1, 1, 1, 1], padding)`\n`filters = tf.reshape(filters, [out_rows * out_cols, filter_height, filter_width, n_channels])`\n`filters = tf.transpose(filters, [1, 2, 3, 0])`\n`tf.conv2d(image, filters, [1, 1, 1, 1], padding)`\nbut it seems inefficient since I'd be generating an intermediate tensor that is going to be about filter_height \\* filter_width larger than filter_matrix.\n\nIf there is a need for such an op but no plans to implement it in the immediate future, I'm happy to look into adding it.\n", "comments": ["Actually, the final output of the operation should be much larger than the tensor generated by extract_image_patches. Can't really see a reason to implement the op closing the issue.\n"]}, {"number": 4541, "title": "Fix recently-introduced bug in ci_build.sh re. cmake builds", "body": "Commit 4316aeb inadvertently made cmake builds depend on bazel. This PR removes this dependency.\n", "comments": ["@caisq, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv, @martinwicke and @ebrevdo to be potential reviewers\n", "Thanks, @vrv\n"]}, {"number": 4540, "title": "error when using embedding_lookup with random initial embeddings", "body": "Hi all. I'm working on variable-length sequence data. eg. a sequence `data = np.array([4,2,3,2,0,-1,-1])`(padded to length 7, the real sequence length is 5). They are word ids. So I can get word embeddings through `tf.nn.embedding_lookup`. \n\n```\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport cPickle\n\n\nvocab_size = 105374\nembed_dim = 200\ninput_x = tf.placeholder(dtype=tf.int32, shape=[7], name=\"x\")\nembedding_matrix = tf.placeholder(dtype=tf.float32, shape=[vocab_size, embed_dim], name=\"embeddings\")\nwith tf.device(\"/cpu:0\"), tf.variable_scope(\"embedding\"):\n    embedded_inputs = tf.nn.embedding_lookup(embedding_matrix, input_x)\n\n\n## Test\ndata = np.array([4,2,3,2,0,-1,-1])\nembedding_path = os.path.join(\"/home/lan/data/dataset-TSU\", \"IMDB\", \"embinit.save\")\nwith open(embedding_path, 'rb') as f:\n    embed_init = cPickle.load(f)\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print sess.run(embedded_inputs, feed_dict={input_x:data, embedding_matrix:embed_init})\n```\n\nIf I use `placeholder` to hold my pre-trained word vectors `embinit.save`, `id = -1` is ok to work.\nI got result from the code above like that. As I expected, the last two ids are both -1, which cannot be found in word embedding matrix. So it returned all zeros.\n\n```\n[[ 0.26877201  0.239695   -0.08297    ..., -0.030947    0.199618\n  -0.24129499]\n [ 0.115232    0.123859   -0.055312   ..., -0.040216    0.176268   -0.259417  ]\n [ 0.132403    0.134068   -0.059557   ..., -0.010268    0.16242801\n  -0.240173  ]\n ..., \n [ 0.20500401  0.125486   -0.116052   ...,  0.066659    0.15658499\n  -0.227872  ]\n [ 0.          0.          0.         ...,  0.          0.          0.        ]\n [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n```\n\nBut if I randomly initialized word embedding matrix, like that \n\n```\nvocab_size = 105374\nembed_dim = 200\ninput_x = tf.placeholder(dtype=tf.int32, shape=[7], name=\"x\")\nwith tf.device(\"/cpu:0\"), tf.variable_scope(\"embedding\"):\n    embedding_matrix = tf.get_variable(name=\"embedding_matrix\",\n                                            dtype=tf.float32,\n                                            shape=[vocab_size,embed_dim],\n                                            initializer=tf.random_uniform_initializer(-1.0, 1.0))\n    embedded_inputs = tf.nn.embedding_lookup(embedding_matrix, input_x)\n```\n\nI got such an error:\n\n```\ntensorflow.python.framework.errors.InvalidArgumentError: indices[5] = -1 is not in [0, 105374)\n```\n\nThat's to say I cannot use id=-1 in this way. Why?? Is there any solution?\n", "comments": ["@a-dai Could you take a look at this? Thanks.\n", "same problem here.\n\nIis '-1' indexing as 'PAD' not supported anymore? \n\ni'm currently avoiding this problem with concatenating 0-vector in embedding weights :\n\n```\ndef embedding(inputs, in_dim, out_dim, name=''):    \n    with tf.variable_scope('Embedding'+name):\n        W_emb = tf.get_variable('W', shape=[in_dim-1, out_dim], \n                                initializer=tf.random_normal_initializer())\n        padding_zeros = tf.zeros(shape=[out_dim])\n        W_emb = tf.concat(0, [[padding_zeros], W_emb])\n    return embedding_ops.embedding_lookup(W_emb, inputs, validate_indices=False)\n```\n", "You want `tf.random_initializer(0, [BATCH_SIZE], 0, num_vectors, tf.int64)`, I think.\r\n\r\nThis is a question that is better answered in stackoverflow, where we monitor all issues with the tag `tensorflow`.", "Is this problem well solved nowadays? \r\nWhen using embedding_lookup, how can I return a 0 tensor if the id is not within the valid range? \r\nWhere could I find the related question in stackoverflow.\r\n\r\nThanks!\r\n", "> Is this problem well solved nowadays?\r\n> When using embedding_lookup, how can I return a 0 tensor if the id is not within the valid range?\r\n> Where could I find the related question in stackoverflow.\r\n> \r\n> Thanks!\r\n\r\nI have the same problem,\r\nIs there a way to solve it?"]}, {"number": 4539, "title": "Clarification about embedding_attention_seq2seq", "body": "This would qualify as a bug depending on what was the intention behind writing the code, the way it was written and whether I understand the workings of Tensorflow well enough.\n\nThe [`cell` input](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L755) given to\n`embedding_attention_seq2seq` function in [seq2seq.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py)  seems to be used both by [the encoder](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L811) and by [the decoder](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L832). Internally, the EmbeddingWrapper class just does [`self._cell = cell`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L717)  which means that it does not create a copy of the cell. Doesn't this mean that the decoder and encoder end up sharing the parameters of their respective LSTMs? is this intentional?\n\nEDIT: realized why they're different cells: because of the scopes.\n", "comments": []}, {"number": 4538, "title": "Branch 134001275", "body": "", "comments": ["There was a conflict in the gif.BUILD. I basically took @jart's version which is much cleaner, but since there was a fix, I'm not sure it strictly supercedes it, and I'm also not sure whether our CI would catch a problem if there was one with my attempt at a merge. Please let me know if I did it wrong.\n", "@meteorcloudy: FYI about the `gif.BUILD` change.\n", "Jenkins, test this please\n", "The MacOS CPU tests appear to have failed because the GitHub server timed out on the download. Maybe I should really prioritize figuring out that mirror situation.\n\nThe cmake CI might be configured incorrectly, since it's saying the bazel command was not found. \n", "I'm retesting, but cmake may not work still. Derek?\n", "According to @caisq on #4537, the CI is misconfigured to run `bazel` even when it's not necessary. He said he'd take a look soon.\n", "See https://github.com/tensorflow/tensorflow/pull/4541 regarding the cmake-bazel issue.\n", "Jenkins, test this please.\n", "@mrry I also reviewed the `gif.BUILD` change internally, Looks good. :)\n", "There's still an issue with the CMAKE tests -- something about SHA and ALGO. @caisq, @mrry any idea?\n", "Looks like https://github.com/tensorflow/tensorflow/commit/c5983f87f0402f2cb8c627807917ebdf8e4d4bb6 introduced a dependency between `eigen.cmake` and `workspace.bzl`, and the following code was deleted somewhere in this PR:\n\n```\n  # These lines need to be changed when updating Eigen. They are parsed from\n  # this file by the cmake and make builds to determine the eigen version and hash.\n  eigen_version = \"6bcd74d2fa40\"\n  eigen_sha256 = \"df3ca8a395fb615003762b8748c03e3aa7a8932b5674dbb5a6bd3343cc3f408d\"\n\n  native.new_http_archive(\n    name = \"eigen_archive\",\n    url = \"https://bitbucket.org/eigen/eigen/get/\" + eigen_version + \".tar.gz\",\n    sha256 = eigen_sha256,\n    strip_prefix = \"eigen-eigen-\" + eigen_version,\n    build_file = str(Label(\"//:eigen.BUILD\")),\n  )\n```\n", "I think it was the 'simplify bazel' commit. look at what the makefile did in that PR and maybe the same thing can be done in cmake?\n", "Ok. Killing this and making a new one.\n", "I've reinstated the dependent lines. We can sort it out later, but I want this push out.\n"]}, {"number": 4537, "title": "Portability improvements for CMake build.", "body": "- Adds WIN32 overrides for many of the external projects.\n- Makes building boringssl optional, to speed up the build.\n- Cleans up various comments in the build files.\n", "comments": ["@mrry, thanks for your PR! By analyzing the annotation information on this pull request, we identified @lilac, @ebrevdo and @benoitsteiner to be potential reviewers\n", "@gunan @caisq Looks like the CMake presubmit is trying (and failing) to run `bazel`?! Did something change recently to make this the case?\n\n(On the other hand, this builds locally for me without using Docker, so if you want to ignore the check and merge it anyway, that'd be splendid....)\n", "@mrry The build has always called bazel, unnecessarily. But it has been working fine until recently, when some environment changes may have made bazel unavailable. I'll look into it soon.\n", "This is caused by https://github.com/tensorflow/tensorflow/commit/4316aeb4cb3f7867cf40f74020bd5d12f72d9f0f. Will send in a fix soon.\n", "See https://github.com/tensorflow/tensorflow/pull/4541, @mrry \n", "Jenkins, test this please.\n", "Rebased to get the CI fix (thanks @caisq!) and fixed an incorrect path. PTAL.\n", "Jenkins, test this please.\n", "Wait, the tests already pass, I was looking at the wrong thing.\n", "Ach if they passed once they'll pass again :).\n", "Using old passing tests as an excuse to merge.\n"]}, {"number": 4536, "title": "Convolutional RNN/LSTM", "body": "Hi all,\n\nThis is a feature request for Convolutional RNNs such as [Convolutional LSTMS](http://arxiv.org/pdf/1506.04214v2.pdf). Is there any plan to support them in tensorflow?\n\nI have an implementation working in [here](https://github.com/loliverhennigh/Convolutional-LSTM-in-Tensorflow) and some basic results indicating how well they do. I went about implementing them by making a new class called `ConvRNNCell` and `BasicVONVLSTMCell` and following what is seen in the tensorflow `rnn_cell.py` file. It seems I could redo the whole `rnn_cell.py` file like this. \n\nIf this is of interest I would be more then happy to implement it for all the rnn functions.\n", "comments": ["@ebrevdo could you take a look?  Thanks.\n", "I haven't tried it yet, but @iwyoo did share an implementation of a ConvLSTM variant of the BasicLSTMCell : https://github.com/iwyoo/ConvLSTMCell-tensorflow\n", "What's the benefit of having a dedicated cell? I've been running a convolutional layer followed by a recurrent layer with the existing modules.\n", "@loliverhennigh   sounds good!  Thank you for your work.\n", "@davek44, it's mostly applicable to seq2seq where you only want the final RNN output from the encoder, but it's hard to say really, because a ConvLSTM also has convolutions for the hidden states, which you're missing by sequentially applying a conv3d and then a RNN.", "@davek44 do you have any working example?", "This seems useful to add. We're still reworking rnns a little. May take until late January to get to this.", "So, is this feature somewhere in master already?", "I just put a pull request here #8891.  Im a bit new to contributing to tensorflow so hopefully I did everything ok.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 4535, "title": "Feature: Ability to restart, reopen or reset a queue.", "body": "Taking into account what @mrry said on http://stackoverflow.com/questions/39204335/can-a-tensorflow-queue-be-reopened-after-it-is-closed/39209186#39209186 ;\n\nIt's not possible to reset a queue unless you have multiple sessions. How about resetting it when you have a single session?\n\n**Use case:**\nIn a single session, if one would want to run _exactly_ one epoch (where epoch_size%batch_size!=0) of a model _x_ times and report it's result exactly after one full and single epoch. It would be convenient if one could restart the queues.\n", "comments": ["@suharshs is working on many different improvements to the Session code, and I think making `Session::Reset()` sensible for a single session is on his list.\n", "This is perfect! 0.11? Thanks for your valuable contributions on tf all over the interwebs, @mrry .\n", "Has this implemented for single session in 0.11.0? The documentation implies that the queue reset mechanism suggested on [SO](http://stackoverflow.com/questions/39204335/can-a-tensorflow-queue-be-reopened-after-it-is-closed) (reset queues in a resource container, needs distributed session) is still the way to do it.", "@suharshs Any updates on this? Cheers.", "Apologies, I haven't gotten a chance to look into this yet.", "I'm _really_ looking forward to an implementation of in-session queue resetting, and hope this will be coming soon.\r\n\r\nThis would enable repeated evaluation of an enqueued validation set, where (epoch_size % batch_size > 0) in a single session. I thought this was a rather common case, and am a bit surprised there is no good support in TensorFlow for this scenario.\r\n\r\nCurrently we are resorting to complicated hacks that [emit undefined behavior](http://stackoverflow.com/questions/41187745/tensorflow-how-can-i-evaluate-a-validation-data-queue-multiple-times-during-tra), and I cannot get the proposed workaround using multiple sessions to work.", "+1, this would be incredibly useful. \r\n\r\nDoes anyone have the multiple session workaround going by any chance? ", "Any updates on this? Thanks!", "@mrry is working on some input pipeline stuff that should make this obsolete. So assigning to him. For now , continue to use the workaround provided here: http://stackoverflow.com/questions/39204335/can-a-tensorflow-queue-be-reopened-after-it-is-closed", "Thanks @suharshs! We're planning to move away from queues and provide first-class support for multi-epoch processing in the redesigned input pipeline API. As a result, I doubt that we will add these features to the queues directly, but instead we'll make it easy to get a signal between epochs in the new API.\r\n\r\nI'm going to close this issue for now. Please feel free to comment on #7951 if there are particular features that you'd particularly like to see or other use cases that you'd like us to support in the new API!", "@mrry That sounds interesting. Can I already try it? Or when is this planned to be released?", "@albertz Good timing: we just released the initial version as `tf.contrib.data` in the 1.2rc0 release.\r\n\r\nHere are some docs to get started: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/README.md", "That's interesting. I also extended my [StackOverflow answer here](https://stackoverflow.com/questions/41187745/tensorflow-how-can-i-evaluate-a-validation-data-queue-multiple-times-during-tra/44067467#44067467) to address this.\r\n\r\nSmall questions (all also put on StackOverflow, maybe you can answer there):\r\n\r\nWhen I use `repeat` (for multiple epochs) together with `shuffle` (as `read_batch_features` does internally), how will I notice when some epochs ends, and what the current epoch is? Also, when the epoch ends, will the `ShuffleDataset` wait first to dequeue everything or will it already be filled with more data from the next epoch? In the last epoch, or if I don't use `repeat`, will the `ShuffleDataset` dequeue all remaining data, like `tf.RandomShuffleQueue` dequeueing does after close?\r\nI asked this on [StackOverflow here](https://stackoverflow.com/questions/44132307/tf-contrib-data-dataset-repeat-with-shuffle-notice-epoch-end-mixed-epochs).\r\n\r\nIf I want to have more control, I would not use `repeat` but go once over the data and use `ShuffleDataset` to get shuffling like `RandomShuffleQueue`, and then at some point I get `OutOfRangeError` and I know that I reached the end of the epoch. Then I reinitializable the iterator, like it is described.\r\n\r\nIf I use `PaddedBatchDataset`, how could I recover the lengths of each sequence in the batch?\r\nI asked this on [StackOverflow here](https://stackoverflow.com/questions/40675692/get-dynamic-sequence-length-from-paddingfifoqueue).\r\n\r\nThe way how to get data doesn't really fit any way how I get the data usually. In my case, I have a thread and I receive data there and I don't know in advance when it will end but I see when it ends. Then I wait until I processed all the buffers and then I have finished one epoch. How can I get this logic with the `Dataset`? I asked this on [StackOverflow here](https://stackoverflow.com/questions/44132579/feed-data-into-a-tf-contrib-data-dataset-like-a-queue).\r\n\r\nHow can I wrap around a `Dataset` over a queue? I have some thread with reads some data from somewhere and which can feed it and queue it somehow. How do I get the data into the `Dataset`? I could repeat some dummy tensor infinite times and then use `map` to just return my `queue.dequeue()` but that really only gets me back to all the original problems with the queue, i.e. how to reopen the queue. The second part of my [StackOverflow question here](https://stackoverflow.com/questions/44132579/feed-data-into-a-tf-contrib-data-dataset-like-a-queue).\r\n", "I agree with @albertz . Queues have one important feature -- it can take data from Python side and that's important for a lot of use cases where data cannot be easily preprocessed with TF operators. Also it's much easier to write for anyone.\r\nTherefore dataset API at least today doesn't seem to be strong enough to replace Queues. I still hope to see some methods to close & reopen the queue.", "Queues take data from the Python side?", "Hmm - technically, you cannot do that. You can feed_dict into a placeholder. Then _that_ can go into a queue. This holds up for anything, so also for the Dataset API:\r\nhttps://www.tensorflow.org/versions/r1.3/programmers_guide/datasets\r\nThe dataset API can be _feedable_, just like a queue.", "You're right -- almost anything is feedable. I was only thinking about the typical cases I work with. I'll correct my point:\r\nQueues have one feature -- prefetch. And that seemed to be missing from dataset API. Therefore dataset API cannot replace queues now.\r\nOr to summarize, what I want is some reader that can: \r\n1. take data from Python \r\n2. prefetch \r\n3. Let me know I finished 1 pass of the dataset and start another (it can be reset for dataset API, or close-reopen for queues, whatever)\r\n\r\n1 may not be a problem -- btw, does the link you post suggest that \"feedable iterators\" are not \"reinitializable\"? If that's true, then feed_dict to dataset API isn't really useful.", "(1) and (2) seem independent points to me. That's probably not correct. Look for capacity* or buffer* through the docs: https://www.tensorflow.org/api_docs/python/tf/contrib/data/Dataset.", "They seem independent to me as well.\r\nAll \"capacity*\" or \"buffer*\" I found is about shufflequeue. That's not for prefetch.", "@ppwwyyxx as far as I can tell it's not feedable. I constructed [this example](https://gist.github.com/samwhitlock/bfafc793c6b7589d12514e741c06eea1) and [modified it](https://gist.github.com/samwhitlock/251aaa04a1c64a929f20657b53662e2b) to try rerunning the initializer with a different feed value and it caused an error. However, the `tf.Errors.OutOfRangeError` was thrown on all `sess.run()` calls after the first.\r\n\r\nThis seems like a pretty common use case to support, where one input is fed but produces multiple outputs per input.\r\n\r\nIs there a way to do this with the existing `Dataset` API that I'm not seeing?", "Hi folks! You might be interested in my [answer](https://stackoverflow.com/a/45928467/3574081) to @albertz's question on Stack Overflow. In short, there have been a couple of developments since the 1.3 release that might be useful to people on this thread, and I'd encourage you to try them out:\r\n\r\n* [`Dataset.from_generator()`](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/data/Dataset#from_generator) makes it easy to create a `Dataset` from a Python generator. This gives you a way to wrap \"feeding\" code by writing a Python function that uses `yield` (rather than `feed_dict`) to pass NumPy arrays into the dataset.\r\n* [`Dataset.prefetch()`](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/data/Dataset#prefetch) makes it easy to create a background thread that preprocesses elements. (You could do this in 1.3, using `Dataset.map(lambda x, y, z: (x, y, z), num_threads=1, output_buffer_size=N)`, but the new version is simpler and a little more efficient.)\r\n\r\nAs an aside, we've upgraded `Dataset.map(..., num_threads=N)` for `N > 1` to use a more efficient representation that doesn't actually create a huge number of threads. (The new argument is called `num_parallel_calls`, but we've kept `num_threads` for now.) This should make it more efficient to process large numbers of elements in parallel, and we've seen encouraging improvements on our benchmarks for CNN image input pipelines."]}, {"number": 4534, "title": "Enhancement: Locally Connected layers ", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nhttp://stackoverflow.com/questions/34858459/tensorflow-not-sharing-variables\r\nhttps://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/S5_xohNLwgs\r\n\r\nI need to implement a network with locally connected layers (convolution without weight sharing) with 2D inputs (images).\r\n\r\nI was wondering if the implementation of this feature is on the roadmap or if there exists some clever hacks using existing TF functions to do it.\r\n\r\n**EDIT:** It seems this kind of layer is often used for face classification for instance in [DeepFace](https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf). \r\n", "comments": ["@mrry Since you replied in SO. Do you know if anyone is working on this?\n", "@jmchen-g I've no idea.\n", "I thought about trying to add the op in C++ myself but thinking about it it looks fairly difficult: 6D tensors, not even sure if CUDA convolutions would be better optimization-wise than matrix multiplications.\nAny update on this ?\n", "Anyone with workaround / clever hacks for this instead of brute force? I came across this issue today. 6D kernel seems intimidating ...", "Even if this feature does not get included in the Tensorflow code, I would like to know if anyone has any working code (or link to working code) for implementing Locally Connected Layers in tensorflow.", "I found this for a time. Only find keras implements https://www.tensorflow.org/api_docs/python/tf/contrib/keras/layers/LocallyConnected2D"]}, {"number": 4533, "title": "Update gitignore.", "body": "`/tensorflow/tools/git/gen/` would be generated during build and it\nshould be ignored.\n", "comments": ["@haosdent, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @rasbt and @girving to be potential reviewers\n", "Can one of the admins verify this patch?\n", "LGTM\n", "did you mean to merge, not close?\n", "Can one of the admins verify this patch?\n", "Yes sorry.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 4532, "title": "Update seq2seq tutorial", "body": "The seq2seq tutorial still refers to models/rnn/seq2seq.py, but it has been moved to tf.nn.seq2seq (python/ops/seq2seq.py) on master\n", "comments": ["@amlankar, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @vrv and @ebrevdo to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!  \n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please\n"]}, {"number": 4531, "title": "Error while updating tensorflow installed from source", "body": "### Environment info\n\nOperating System: Ubuntu 14.04.4\n\nInstalled version of CUDA and cuDNN: CUDA 8.0, cuDNN 5\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n-rw-r--r-- 1 root root   560184  7\uc6d4 29 09:12 /usr/local/cuda-8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16  7\uc6d4 29 09:12 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19  7\uc6d4 29 09:12 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472  7\uc6d4 29 09:12 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516  7\uc6d4 29 09:12 /usr/local/cuda-8.0/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 78065952  7\uc6d4 29 09:43 /usr/local/cuda-8.0/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 78065952  7\uc6d4 29 09:43 /usr/local/cuda-8.0/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 78065952  7\uc6d4 29 09:43 /usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5\n-rw-r--r-- 1 root root 68709594  7\uc6d4 29 09:43 /usr/local/cuda-8.0/lib64/libcudnn_static.a\n```\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n\n`ca1aa4ad2d4e011e8479319e10d73281a50f7560`\n1. The output of `bazel version`\n   \n    Build label: 0.2.3\n    Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n    Build time: Tue May 17 14:21:13 2016 (1463494873)\n    Build timestamp: 1463494873\n    Build timestamp as int: 1463494873\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nI installed tensorflow around two monthes ago and I've used it.\nThen, I tried to update by following [Stackoverflow](http://stackoverflow.com/questions/34239537/how-to-update-tensorflow-from-source).\n\nI first tried 'git pull', it could not be merged since I have some changes in source.\nSo I reset the source by\n\n```\ngit fetch --all\ngit reset --hard origin/master\n```\n\nThen I did `./configure`. But it gives errors. I attached the output of the configure below.\nIt's weird because I successfully update with this way before.\nActually, I also tried same way in another system (Cuda 7.0 cuDNN 4), but failed with similar error message...\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\n```\nPlease specify the location of python. [Default is /home/jinhyung/anaconda2/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \nNo Hadoop File System support will be enabled for TensorFlow\nFound possible Python library paths:\n  /home/jinhyung/anaconda2/lib/python2.7/site-packages\nPlease input the desired Python library path to use.  Default is [/home/jinhyung/anaconda2/lib/python2.7/site-packages]\n\n/home/jinhyung/anaconda2/lib/python2.7/site-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n.\nERROR: /home/jinhyung/tensorflow/tensorflow/tensorflow.bzl:567:26: Traceback (most recent call last):\n    File \"/home/jinhyung/tensorflow/tensorflow/tensorflow.bzl\", line 561\n        rule(attrs = {\"srcs\": attr.label_list...\"), <3 more arguments>)}, <2 more arguments>)\n    File \"/home/jinhyung/tensorflow/tensorflow/tensorflow.bzl\", line 567, in rule\n        attr.label_list(cfg = \"data\", allow_files = True)\nexpected ConfigurationTransition or NoneType for 'cfg' while calling label_list but got string instead:     data.\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'tensorflow/tensorflow.bzl' has errors.\nConfiguration finished\n```\n", "comments": ["I was able to fix it locally for me by reverting\nhttps://github.com/tensorflow/tensorflow/commit/7bcdcbbf60fc08346fd8016270a0563f4b51362b\nand changing in the lines 567 and following: \n\n`\n            cfg = DATA_CFG,\n...\n            cfg = HOST_CFG,\n\n`\n", "@Oryak Same observation for the commit that causes the problem. However, the change you propose depends on the Bazel version. That will work before 0.3.1, if I am not mistaken (it may even be the 0.3.x series). From 0.3.1, `cfg` should be a string. See #4371 for related and recent activity.\n", "@jart Could you take a look at this? Thanks.\n", "@Oryak I could install without error after I update the bazel version 0.3.1. Problem solved.\n", "@kkjh0723 Problem not solved on linux. The [following correction](https://github.com/tensorflow/tensorflow/issues/4371#issuecomment-249340251) was needed. Bazel still pulls from an outdated protobuf archive \n", "The first solution is to upgrade Bazel to 0.3.1. But we have another problem @kamal94 identified where we need to update our protobuf dependency. See #4561.\n", "I also ran into this issue just now. I initially used homebrew on Mac to install bazel v0.2.3. After uninstall v0.2.3 and manually downloading and installing v0.3.1 it appears that the configuration finishes correctly.\n", "@ccravens  How did you uninstall bazel that was installed brew.. Thanks\n"]}, {"number": 4530, "title": "distribute tensorflow chief node OOM", "body": "sorry to trouble.\nmy model has 6 conv layer and a softmax layer. kernel size is 3*3, channel is 192, num class is 300.\nI run a distribute model with 20 Tesla K40m.\nwhen I run one ps job and 5 worker job, everything is fine. But when I run a ps job and 19 worker jobs in 20 PC, when the chief node Initialize for node 15, it gets the OOM error.\nI don't know how to deal with it.\nAnyone can help me?\n", "comments": ["Using 1 ps for 19 workers is very likely to make PS the system's bottleneck. Can you use say 5 ps instead?\n", "Ok,I will try 5 ps job and response soon \n", "@jmchen-g sad ,it also out of memory:\n\nRan out of memory trying to allocate 33.84MiB.  See logs for memory state.\nResource exhausted: OOM when allocating tensor with shape[128,32,32,192]\n", "and I tried : config.gpu_options.allocator_type='BFC'\nbut also OOM occured.\n\nLimit:                 11325259776\nInUse:                 11303057152\nMaxInUse:              11324129024\nNumAllocs:                  684755\nMaxAllocSize:           1065501696\n\nso, how could I fixed this problem in distribute tensorflow?\n", "when you say when the chief initialize node 15, what do you mean by that? The chief should not know the existence of other workers at all. All the workers only communicate with ps.\n", "Automatically closing due to lack of recent activity. Please reopen when additional information becomes available. Thanks!\n"]}, {"number": 4529, "title": "can't bazel build benchmark_model with quantization deps due to -lpthread linking error", "body": "Trying to build //tensorflow/tools/benchmark:benchmark_model per instructions at this stackoveflow post http://stackoverflow.com/questions/37953369/tensorflow-quantized-graph-for-android. Am getting error that cannot find -lpthread despite my belief that lib is unneeded and best efforts to remove it.\n\n```\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/../lib/gcc/arm-linux-androideabi/4.9/../../../../arm-linux-androideabi/bin/ld: error: cannot find -lpthread\ncollect2: error: ld returned 1 exit status\n```\n\n//tensorflow/tools/benchmark/BUILD cc_binary\n\n```\ndeps = [\":benchmark_model_lib\",\n        \"//tensorflow/contrib/quantization/kernels:quantized_ops\",\n            ],\n```\n\n//tensorflow/contrib/quantization/kernels/BUILD:\n\n```\ndeps = [\n    \"//tensorflow/contrib/quantization:cc_array_ops\",\n    \"//tensorflow/contrib/quantization:cc_math_ops\",\n    \"//tensorflow/contrib/quantization:cc_nn_ops\",\n    #\"//tensorflow/core\",\n    #\"//tensorflow/core:framework\",\n    #\"//tensorflow/core:lib\",\n    #\"//tensorflow/core/kernels:concat_lib_hdrs\",\n    #\"//tensorflow/core/kernels:conv_ops\",\n    #\"//tensorflow/core/kernels:eigen_helpers\",\n    #\"//tensorflow/core/kernels:ops_util\",\n    #\"//tensorflow/core/kernels:pooling_ops\",\n    \"//third_party/eigen3\",\n    \"@gemmlowp//:eight_bit_int_gemm\",\n],\n```\n\nThen run:\n`bazel build -c opt --cxxopt='-std=gnu++11'--crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain tensorflow/tools/benchmark:benchmark_model --verbose_failures`\n\nWhich (with following all other instructions in linked post) succeeds with the exception that it fails to link against pthread.\n\nI've tried removing -lpthread in `tensorflow/tensorflow.bzl`,  `tensorflow/tools/proto_text/BUILD`, `tensorflow/tools/proto_text/BUILD`, and `tensorflow/cc/BUILD` but this still results in -lpthread being linked to in final compilation. \n\nI've also looked at the fixes such as this one https://github.com/tensorflow/tensorflow/issues/419 which either remove -lpthread from google protobuf or link to the dummy lib but these seem to only apply to earlier versions of TensorFlow.\n\nApologies if this is a basic config error, I'm new to Bazel and am pretty stuck. benchmark_model and quantize_graph both compile well independently but not yet for me when combined.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nhttp://stackoverflow.com/questions/37953369/tensorflow-quantized-graph-for-android\nhttps://github.com/tensorflow/tensorflow/issues/419\nhttps://github.com/google/protobuf/issues/1373\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: \n(ml) socialh@socialh:~$ echo $LD_LIBRARY_PATH\n/home/socialh/cuda/lib64:/usr/local/cuda/lib64:\n\n(ml) socialh@socialh:~/otf$ ll /usr/local/cuda/lib/libcud*\n-rw-r--r-- 1 root root 189170 Aug 31 15:22 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Aug 31 15:22 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5*\nlrwxrwxrwx 1 root root     19 Aug 31 15:22 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18*\n-rwxr-xr-x 1 root root 311596 Aug 31 15:22 /usr/local/cuda/lib/libcudart.so.7.5.18*\n-rw-r--r-- 1 root root 558020 Aug 31 15:22 /usr/local/cuda/lib/libcudart_static.a\n\n(ml) socialh@socialh:~$ ll /home/socialh/cuda/lib64/\nlrwxrwxrwx 1 socialh socialh       13 Jun 10 01:20 libcudnn.so -> libcudnn.so.5*\nlrwxrwxrwx 1 socialh socialh       17 Jun 10 01:20 libcudnn.so.5 -> libcudnn.so.5.1.3*\n-rwxr-xr-x 1 socialh socialh 60696704 Jun 10 01:18 libcudnn.so.5.1.3*\n-rw-r--r-- 1 socialh socialh 59715990 Jun 10 01:18 libcudnn_static.a\n\nIf installed from source, provide \n`git rev-parse head`\n640353d5d1db50d6601f9410b9d06462a8a71ce4\n`bazel version`\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n`bazel build -c opt --cxxopt '-std=gnu++11' --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain   tensorflow/tools/benchmark:benchmark_model --verbose_failures\n`\nwith other steps taken in first StackOverflow example\n### What other attempted solutions have you tried?\n\nTried removing all -lpthreads references, also tried removing lpthreads in bazel-out files but did not help. \n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["To work around the pthread issue, use the `@gemmlowp//:eight_bit_int_gemm_sources` filegroup in your srcs rather than the cc_library target as a dep.\n\ne.g.:\n\n```\ncc_library(\n    name = \"android_tensorflow_lib\",\n    srcs = if_android([\":android_op_registrations_and_gradients\",\n        \"//tensorflow/contrib/quantization:android_ops\",\n        \"//tensorflow/contrib/quantization/kernels:android_ops\",\n        \"@gemmlowp//:eight_bit_int_gemm_sources\"]),\n    copts = tf_copts() + [\"-Iexternal/gemmlowp\"],\n    linkopts = [\"-lz\"],\n    tags = [\n        \"manual\",\n        \"notap\",\n    ],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":android_tensorflow_lib_lite\",\n        \":protos_cc\",\n        \"//tensorflow/core/kernels:android_tensorflow_kernels\",\n        \"//third_party/eigen3\",\n    ],\n    alwayslink = 1,\n)\n```\n", "Worked great, thanks so much @andrewharp!!!!\n\nReally appreciate prompt reply, transcribed answer (along with git pull --recurse-submodules and removing all earlier \"fixes\") to StackOverflow so hopefully will prevent other n00bs hitting you with same question.\n\nThanks again!\n"]}, {"number": 4528, "title": "How could wide_n_deep tutorial apply for muti-class categoring task?", "body": "### Environment info\n\nOperating System: Linux\n### What other attempted solutions have you tried?\n\ndelete the lines: \n\n```\ndf_train[LABEL_COLUMN] = (df_train[\"label\"].apply(lambda x: '>50K' in x)).astype(int)\ndf_test[LABEL_COLUMN] = (df_test[\"label\"].apply(lambda x: '>50K' in x)).astype(int)\n```\n\nwhich caused 'tensorflow.contrib.learn.python.learn.monitors.NanLossDuringTrainingError: NaN loss during training'. \n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["This is a question better suited for StackOverflow. Please ask it there and tag it with the `tensorflow` tag.\n"]}, {"number": 4527, "title": "Error occurs when a client accesses a server on different machine", "body": "I just get started to use distributed version of tensorflow and start by simply run a server on comp1:2222:\n\n\"\nimport tensorflow as tf\nworker1 = \"192.168.217.227:2222\"\nworker_hosts = [worker1]\ncluster_spec = tf.train.ClusterSpec({\"worker\" : worker_hosts})\nserver = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=0)\nserver.join()\n\"\n\nand run a client (\"192.168.217.205\") :\n\n\"\nimport tensorflow as tf\nc = tf.constant(\"hello\")\nwith tf.Session(\"grpc://192.168.217.227:2222\") as sess:\n    print(sess.run(c))\n\"\nAlways got the below error:\n\"\nUnimplementedError                        Traceback (most recent call last)\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, _args)\n    729     try:\n--> 730       return fn(_args)\n    731     except errors.OpError as e:\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\n    707       # Ensure any changes to the graph are reflected in the runtime.\n--> 708       self._extend_graph()\n    709       with errors.raise_exception_on_not_ok_status() as status:\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _extend_graph(self)\n    756           tf_session.TF_ExtendGraph(\n--> 757               self._session, graph_def.SerializeToString(), status)\n    758         self._opened = True\n\n/usr/lib/python3.5/contextlib.py in **exit**(self, type, value, traceback)\n     65             try:\n---> 66                 next(self.gen)\n     67             except StopIteration:\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors.py in raise_exception_on_not_ok_status()\n    449           compat.as_text(pywrap_tensorflow.TF_Message(status)),\n--> 450           pywrap_tensorflow.TF_GetCode(status))\n    451   finally:\n\nUnimplementedError: \n\"\n\nIf run the client code in the same machine as the server, I can see the correct result.\n### Environment info\n\nOperating System: Ubuntu 16.04\n\nInstalled version of CUDA and cuDNN: no\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n   pip 8.1.2 from /home/***/.local/lib/python3.5/site-packages (python 3.5)\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   $ python3 -c \"import tensorflow; print(tensorflow.**version**)\"\n   0.10.0rc0\n", "comments": []}, {"number": 4526, "title": "GPU-resident queue for prefetching over PCIe", "body": "It would improve performance in some cases to be able to asynchronously prefetch data over the PCIe bus while GPU computation is taking place. A GPU-resident queue seems like the natural way to achieve this.\n\nIn the SO thread below, @yaroslavvb mentions using Variables pinned to the GPU to achieve the same effect, but I was unable to find a way to get this to work.\n### Related threads:\n\nhttps://stackoverflow.com/questions/38751736/understanding-tensorflow-queues-and-cpu-gpu-transfer\nhttps://github.com/tensorflow/tensorflow/issues/3009#issuecomment-242235654\nhttps://github.com/tensorflow/tensorflow/issues/3377#issuecomment-239966932\n", "comments": ["@zheng-xq can tf.assign copy data to GPU at the same time as the GPU runs computations?\n\nBTW, transfers between TensorFlow and Python runtimes can be pretty slow, so if you are getting data from feed_dict/numpy as in the SO question, that's likely to be the bottleneck and GPU-resident queue won't help you. Recently I found a [case](https://github.com/tensorflow/tensorflow/issues/4498) where data transfer rate was limited to 65 MB/second at the TF/Python boundary\n", "tf.assign itself doesn't do the copy. The send/recv pair after the graph partition does that. And it runs in parallel with the computation. \n\nThe data transfer between TF and Python through feed_dict is often hidden through input queues. A GPU-resident queue could save the additional overhead from CPU to GPU once the data is in TF. \n\nHowever, we had discussed the possibility of a GPU-resident queue, or GPU-cached queue in this context in the past few days. However, a naive implementation might be problematic since it might consume too much GPU memory, which is often much less than its CPU counterpart. If we go with a GPU-cached queue, we have to be very careful, since there are a number of tricky issues to iron out so it doesn't interfere with the rest of the system. \n", "Will close this for now. Feel free to reopen this with more updates. Thanks.\n", "A GPU queue would only need to double-buffer the input; this doesn't seem like a big deal with respect to memory use(?).\n\nIs a GPU queue more complicated than a CPU queue besides the different allocator?\n", "Modern models are very aggressive in using the queue capacities. In order to hide the latency, they tend to use a very large queue capacity. \n", "I think the issues with H2D transfers are quite real. Here's an example from AlexNet where the TF queues are feeding the GPUs with data for every mini-batch. The lack of overlap is demonstrated here for 2 GPUs (see screenshots). Because AlexNet has a low compute-to-I/O ratio, the problem is more clearly visible than in say Inception v3 or ResNet, so even though AlexNet isn't interesting for academic or business application reasons anymore, it's interesting for infrastructural/engineering reasons. Note how for 2 GPUs, H2D transfers are causing the GPU to wait for >2 seconds at each iteration. Hence, GPU compute efficiency drops to about 26%. This is on two GTX 1080s with batch sizes at 1024 per GPU. With smaller batch sizes, the problem is more pronounced. For a single GPU, the GPU compute utilization is significantly better (80%) but the kernel pipeline still blocks on H2D transfer, even though the transfer is taking place in a separate CUDA stream. I'd like to support @benbarsdell's point here. This doesn't look like an optimized prefetch - the compute/memcpy overlap for H2D is rather hard to see.\n\n![alexnet_2_gpus](https://cloud.githubusercontent.com/assets/476135/18764266/7220b368-80c5-11e6-9d09-8dfcb65943c0.png)\n", "Here's the single-GPU example (I previously posted a screenshot for 2 GPUs).\n\n![alexnet_1_gpu](https://cloud.githubusercontent.com/assets/476135/18764262/7090fbca-80c5-11e6-9ea2-86078a8d6926.png)\n", "Here's what I meant when I said that GPU-resident queue could be doing using existing ops:\n- Keep a buffer of examples on GPU in several variables\n- Regular FIFO queue is enqueued with example indices, while actual example data lies on GPU\n- A global counter keeps track of number of examples that have been consumed\n- A separate thread periodically checks the counter, and replaces stale variables with new example data using `tf.assign`\n\nEssentially you would have a rotating buffer of examples on GPU, and load that data asynchronously\n", "@yaroslavvb Yes, basically a backpressure pattern (some common tech doing this type of stuff these days being Akka, RxJava, and Flink, and for that matter, even flow control in TCP for a more dated reference). I think though that it would be good for this to be baked into the framework rather than shifting flow control workarounds to the user. Data prefetch is pretty much a universal need, since the input has to come from somewhere, hopefully in an efficient manner to leverage available compute cycles.\n", "We had some discussions about this. It seems a good idea to have a separate GPU queue in this case. We still need a larger CPU queue to hide latency between Python and TF. But this introduces a separate transfer stage from CPU to each GPU. We expect the GPU queue on each device to be much smaller: one or two in queue capacity. \n\nThe down-side is that this introduces more client side threads to drive the new data transfer. But there is a separate effort to migrate them into TF itself. So we will ignore that problem for now. \n", "I also agree with @mkolod and @benbarsdell  that the H2D (and D2H) transfers can be problematic. Related to #2848 where there is further discussion on tensorflow's GPU transfer scheduling. Part of the problem is that the Send/Recv ops are scheduled immediately and not necessarily in the order of their dependent operations. One suggested solution is to use `tf.identity` to order transfer to the GPU.\n\nThinking on this some more, would multiple `towers` (in the cifar10 example terminology) per GPU be a solution? Is the scheduler intelligent enough to interleave the GPU transfers and compute for both towers in parallel? I'm trying to come up with something \n", "Also, the following quote from the [documentation](https://www.tensorflow.org/versions/r0.11/how_tos/threading_and_queues/index.html) suggests one can pin queues to a GPU?\n\n> N.B. Queue methods (such as q.enqueue(...)) must run on the same device as the queue. Incompatible device placement directives will be ignored when creating these operations.\n\nEDIT: Its not possible to pin queues to GPUs, yet.\n", "@zheng-xq \r\n\r\n> The down-side is that this introduces more client side threads to drive the new data transfer. But there is a separate effort to migrate them into TF itself. So we will ignore that problem for now.\r\n\r\nNow the stage/unstage operators are added to TF, but they seem to require an extra client side thread indeed. Can you share what was the plan you mentioned (or if there is a solution already)?", "The plan is not to have separate python threads driving them, but to embed the parallelism in the graph. We will publish some scripts demonstrate how to do this in near future. Stay tuned. ", "Still waiting!", "@Neltherion `from tensorflow.python.ops.data_flow_ops import StagingArea` has been there for a year."]}, {"number": 4525, "title": "Python Exceptions lead to CUDA Memory Leak", "body": "I've been using hyperopt in it's MondoTrials configuration to do a grid search of my hyperparameters and the process has been slow going since every time my python program crashes for whatever reason, my CUDA memory is not freed\n\nHere is the output i see from nvidia-smi:\n\n```\n$ nvidia-smi\nWed Sep 21 20:09:42 2016       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 367.44                 Driver Version: 367.44                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX TIT...  Off  | 0000:03:00.0      On |                  N/A |\n| 22%   32C    P8    15W / 250W |  11848MiB / 12198MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      3475    G   /usr/lib/xorg/Xorg                             207MiB |\n|    0      4579    G   compiz                                          52MiB |\n|    0      5258    G   /opt/google/chrom                               86MiB |\n|    0      6144    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    37MiB |\n+-----------------------------------------------------------------------------+\n```\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nLots of people asking questions, no-one with answers.\n### Environment info\n\nOperating System: Ubuntu 16.04\n\nInstalled version of CUDA and cuDNN: CUDA 7.5 cuDNN 5.1\n\nIf installed from binary pip package, provide:\n1. https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl\n2. 0.10.0\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nI don't have a minimal sample yet.\n### What other attempted solutions have you tried?\n\nRebooting works, but is painful\n### Logs or other output that would be helpful\n\nPlease let me know what logs would be useful.\n", "comments": ["FWIW, hyperopt does some terrible things where it serializes a function that you define for running a trial, stores that (dill) pickle in mongodb and then deserializes that pickle from a module and runs it there.\n\nI feel like there's a high likelihood that setup is what is causing issues.\n", "We don't really directly support those third party tools so please post this on stackoverflow. Thanks.\n"]}, {"number": 4524, "title": "bidirectional_rnn not taking sequence_length [batch_size]", "body": "Trying to do variable sequence length with brinn and not able to use placeholder of batch_size as sequence length, instead wants num_steps size even though function description says [batch_size].  \n\nIn the code below setting up the graph gives an dimension error. (I am using the latest tensorflow version 0.10.0.)\n\n```\nimport tensorflow as tf\nflags = tf.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_bool(\"use_fp16\", False,\n                  \"Train using 16-bit floats instead of 32bit floats\")\n\ndef data_type():\n    return tf.float16 if FLAGS.use_fp16 else tf.float32\n\nbatch_size = 20\nnum_steps = 40\nhidden_size = 64\nnum_layers = 2\nvocab_size = 1000000\n\nembedding_input = tf.placeholder(tf.int32, [batch_size, num_steps])\nsequence_length = tf.placeholder(tf.int32, [batch_size])\nembeddings = tf.get_variable(\"embedding\",\n                             [vocab_size, 300],dtype=data_type(),trainable=False)\n\nembedding_input = tf.nn.embedding_lookup(embeddings, embedding_input)\n\n\ninitializer = tf.random_uniform_initializer(-1,1)\n\nlstm_fw_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size,\n                                       initializer=initializer)\nlstm_bw_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size,\n                                       initializer=initializer)\n\nlstm_fw_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_fw_cell] * num_layers)\nlstm_bw_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_bw_cell] * num_layers)\n\nlstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=0.9)\nlstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=0.9)\ninputs = tf.nn.dropout(embedding_input, 0.9, noise_shape=None, seed=None)\n\ninputs = [tf.squeeze(x) for x in tf.split(0, batch_size, inputs)]\noutput, _, _ = tf.nn.bidirectional_rnn(lstm_fw_cell,\n                                       lstm_bw_cell,inputs,\n                                       sequence_length=sequence_length,\n                                       dtype=tf.float32)\n```\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 566, in merge_with\n    new_dims.append(dim.merge_with(other[i]))\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 133, in merge_with\n    self.assert_is_compatible_with(other)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 108, in assert_is_compatible_with\n    % (self, other))\nValueError: Dimensions 20 and 40 are not compatible\n", "comments": ["Please post this on stackoverflow instead. Thanks.\n"]}, {"number": 4523, "title": "make text in word2vec tutorial match image", "body": "The embedded image has male-female, verb tense, and country-capital, so the text should too.\n", "comments": ["Can one of the admins verify this patch?\n", "@hrldcpr, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @vrv and @ilblackdragon to be potential reviewers\n", "Thanks!\n"]}, {"number": 4522, "title": "Added colocate gradients argument to optimize_loss", "body": "This PR adds a colocate gradients argument to optimize_loss which is useful for multi GPU training where the model is split across multiple GPUs.\n", "comments": ["Can one of the admins verify this patch?\n", "@alexgkendall, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @ilblackdragon and @itsmeolivia to be potential reviewers\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "@martinwicke do you know what is causing these tests to fail?\n", "The Linux/Py3 failure is unrelated. \n\nThe Mac failure was a network problem, and should be just a flake. I re-ran it, with results eventually showing up at \nhttp://ci.tensorflow.org/job/tensorflow-pull-requests-mac/2023\n\nOnce that's clean we can merge. @jhseu, can you merge this if the above test comes back clean?\n", "Yeah. The mac flakes are fixed internally, so I'll rerun the tests and merge this once I push the internal changes.\n"]}]