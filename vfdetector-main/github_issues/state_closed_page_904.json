[{"number": 26349, "title": "[TF 2.0] Checkpoint breaking change for all optimizers.", "body": "**Type of breakage**: Breakage with changing code.\r\n\r\nAPIs that are affected: \r\n\r\n1. [`tf.train.GradientDescentOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer)\r\n2. [`tf.train.MomentumOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer)\r\n3. [`tf.train.RMSPropOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer)\r\n4. [`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)\r\n5. [`tf.train.AdadeltaOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdadeltaOptimizer)\r\n6. [`tf.train.AdagradOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer)\r\n7. [`tf.train.FtrlOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer)\r\n\r\n**Side Note**: there are two extra `tf.contrib`-owned optimizers that will also have breaking changes: \r\n\r\n1. [`tf.contrib.opt.NadamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/contrib/opt/NadamOptimizer)\r\n2. [`tf.contrib.opt.AdaMaxOptimizer`](https://www.tensorflow.org/api_docs/python/tf/contrib/opt/AdaMaxOptimizer)\r\n\r\n**Description of change**: The current endpoint `tf.train.xxxOptimizer()` is being deprecated in favor of `tf.keras.optimizers.xxx()`. Specifically:\r\n\r\n1. `tf.train.GradientDescentOptimizer(lr=???)` -> `tf.keras.optimizers.SGD(learning_rate=???)`\r\n2. `tf.train.MomentumOptimizer(lr=???, momentum=???)` -> `tf.keras.optimizers.SGD(learning_rate=???, momentum=???)`\r\n3. `tf.train.RMSPropOptimizer(lr=???)` -> `tf.keras.optimizers.RMSprop(learning_rate=???)`\r\n4. `tf.train.AdamOptimizer(lr=???, beta1=???, beta2=???)` -> `tf.keras.optimizers.Adam(learning_rate=???, beta_1=???, beta_2=???)`\r\n5. `tf.train.AdadeltaOptimizer(lr=???)` -> `tf.keras.optimizers.Adadelta(learning_rate=???)`\r\n6. `tf.train.AdagradOptimizer(lr=???)` -> `tf.keras.optimizers.Adagrad(learning_rate=???)`\r\n7. `tf.train.FtrlOptimizer(lr=???)` -> `tf.keras.Ftrl(learning_rate=???)`\r\n\r\nTensorFlow users of `tf.train.xxxOptimizer()` will be updated to `tf.keras.optimizers.xxx()`, and checkpoints from the old `tf.train.xxxOptimizer()` calls will no longer work.\r\n\r\n**Variable name change map**:  The `tf.keras.optimizers.xxx` weights are in different format than existing `tf.train.xxxOptimizer`. There isn't direct mapping for that.\r\n\r\n**Target time window**: Undecided since the update requires non-trivial user side change.", "comments": ["@yhliang2018 -- can you link to the docs for converters/guides here? For canned estimators for now, and the others when ready.", "The conversion guide for premade estimators is [here](https://www.tensorflow.org/beta/guide/migration_guide#premade_estimators).\r\n\r\nFor checkpoint converter tool, you can find it [here](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/tools/checkpoint_converter.py). So far the converter supports canned estimator only, and we are working on extending it to custom estimators as well.", "This issue was reported in 2.0 and moved to keras now this issue does not exist, the above comment is deprecated hence moving this to closed status. Please create this is keras repo in case of any queries."]}, {"number": 26348, "title": "TF_CPP_MIN_LOG_LEVEL=3 does not work after upgrading to 1.13.1", "body": "After upgrading to 1.13.1, the `TF_CPP_MIN_LOG_LEVEL` does not seem to work for certain warnings.\r\nFor example, I get the following warnings even if log level is set to 3:\r\n\r\n```\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\n\r\n```\r\n", "comments": ["You can try this instead, \r\n```python\r\nimport tensorflow as tf\r\ntf.logging.info('TensorFlow')\r\nINFO:tensorflow:TensorFlow\r\ntf.logging.set_verbosity(tf.logging.ERROR)\r\ntf.logging.info('TensorFlow')\r\n```", "@ymodak , apparently, doing it this way worked! \r\n\r\nthanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)", "> You can try this instead,\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> tf.logging.info('TensorFlow')\r\n> INFO:tensorflow:TensorFlow\r\n> tf.logging.set_verbosity(tf.logging.ERROR)\r\n> tf.logging.info('TensorFlow')\r\n> ```\r\n\r\nCan you explain these statements??", "Does anyone know how to do this in C++?"]}, {"number": 26347, "title": "2019-03-05 12:24:39.106 18565-18583/org.tensorflow.lite.demo W/System.err: java.lang.ArrayIndexOutOfBoundsException: length=10; index=-2147483648", "body": "When i run the tfLite demo app in my mobile I am getting the following error:\r\n\r\n2019-03-05 12:24:39.106 18565-18583/org.tensorflow.lite.demo W/System.err: java.lang.ArrayIndexOutOfBoundsException: length=10; index=-2147483648\r\n\r\n", "comments": ["Please provide following applicable information. Thanks!\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@aravindchaluvadi \r\nI'm also facing this issue while running custom model with the app.\r\n\r\nHave you resolve this ?\r\n"]}, {"number": 26346, "title": "ref type problem merging simple graphs", "body": "Hi guys, the following simple graph is compiled correctly on my computer:\r\n\r\n\r\ngraph_unique = tf.Graph()\r\n\r\nwith graph_unique.as_default():\r\n    v = tf.get_variable(name='v', shape=[2], dtype=tf.float64)\r\n    x = v[:1]\r\n    y = v[1:]\r\n    c = tf.add(x, y, name='c')\r\n    gra = tf.gradients([c], [x])\r\n    \r\n\r\nWhen I try to write it as a stack of two graphs, though, I get an error.\r\nI guess it is related with this post and I beg for some help.\r\nHere my procedure:\r\n\r\n\r\n\r\nadr_big = ''   # please add a valid addres \r\nadr_small = '' #       \"          \"\r\n\r\n\r\ngraph_small = tf.Graph()\r\n\r\nwith graph_small.as_default():\r\n    v = tf.get_variable(name='v', shape=[2], dtype=tf.float64)\r\n    x = tf.identity(v[:1], name='x')\r\n    y = tf.identity(v[1:], name='y')\r\n    s_small = tf.train.Saver()\r\n    \r\nwith tf.Session(graph=graph_small) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    s_small.save(sess, adr_small)\r\n    \r\n    \r\ngraph_big = tf.Graph()\r\n\r\nwith graph_big.as_default():\r\n    a = tf.get_variable(name='a', shape=[1], dtype=tf.float64)\r\n    b = tf.get_variable(name='b', shape=[1], dtype=tf.float64)\r\n    c = tf.add(a, b, name='c')\r\n    s = tf.train.Saver()\r\n    \r\nwith tf.Session(graph=graph_big) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    s.save(sess, adr_big)\r\n    \r\n    \r\n    \r\ngraph_together = tf.Graph()\r\n\r\nwith graph_together.as_default():\r\n    tf.train.import_meta_graph(adr_small+'.meta', import_scope='g_small')\r\n    x = graph_together.get_tensor_by_name('g_small/x:0')\r\n    y = graph_together.get_tensor_by_name('g_small/y:0')\r\n    tf.train.import_meta_graph(adr_big+'.meta', import_scope='g_big', input_map={'a:0':x, 'b:0':y})\r\n    c = graph_together.get_tensor_by_name('g_big/c:0')\r\n    gra = tf.gradients([c],[x])\r\n    \r\n\r\nTensorflow says:\r\n[...]\r\nInvalidArgumentError: Input 0 of node gg/a/Assign was passed double from g_small/x:0 incompatible with expected double_ref.\r\nDuring handling of the above exception, another exception occurred:\r\n[...]\r\nValueError: Input 0 of node gg/a/Assign was passed double from g_small/x:0 incompatible with expected double_ref.\r\n\r\nPlease notice: everything works correctly if the definition of graph_small above is replaced with the following one:  \r\n\r\nwith graph_small.as_default():\r\n    tf.get_variable(name='x', shape=[1], dtype=tf.float64)\r\n    tf.get_variable(name='y', shape=[1], dtype=tf.float64)\r\n    s_small = tf.train.Saver()  \r\n    \r\nThanx a lot!\r\n", "comments": ["PS: Here above is just an attempt to obtain what I actually need.\r\nMaybe there are better ways. Here under I describe my esential\r\ngoal.\r\n\r\n\r\nGiven the graph\r\n\r\nwith graph_small.as_default():\r\n    v = tf.get_variable(name='v', shape=[2], dtype=tf.float64)\r\n    x = tf.identity(v[:1], name='x')\r\n    y = tf.identity(v[1:], name='y')\r\n    \r\nit is easy to build it up to the following one by simply adding \r\nvariables:\r\n\r\nwith graph_together.as_default():\r\n    tf.train.import_meta_graph(adr_small+'.meta', import_scope='g_small')\r\n    x = graph_together.get_tensor_by_name('g_small/x:0')\r\n    y = graph_together.get_tensor_by_name('g_small/y:0')\r\n    tf.train.import_meta_graph(adr_big+'.meta', import_scope='g_big', input_map={'a:0':x, 'b:0':y})\r\n    c = graph_together.get_tensor_by_name('g_big/c:0')\r\n    gra = tf.gradients([c],[x])\r\n    \r\n\r\nSuppose now, that we start from the following graph, instead:\r\n\r\nwith graph_big.as_default():\r\n    a = tf.get_variable(name='a', shape=[1], dtype=tf.float64)\r\n    b = tf.get_variable(name='b', shape=[1], dtype=tf.float64)\r\n    c = tf.add(a, b, name='c')\r\n    s = tf.train.Saver()\r\n    \r\nHow can we the get to \"graph_together\" defined above? In other words, \r\nhow can we add variables not \"downstream\" but \"upstream\"? \r\nIt would be eesential for me, that everything than works correctly with\r\ngradients and hessians. ", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}, {"number": 26345, "title": "Fix AttributeError on LocalBuffer.__delete__ call in python xla client", "body": "While playing around with some custom JAX primitives, I was noticing the python xla client throwing exceptions like below towards the end (probably when LocalBuffer is gc'd or in a separate cleanup operation). The exceptions are ignored, but they are logged many times in the code that I am working with. This adds a small check (taken from the Executable class) to ensure that `c_api` hasn't already been freed in the `LocalBuffer.delete()` method.\r\n\r\n```\r\nException ignored in: <bound method LocalBuffer.__del__ of <jaxlib.xla_client.LocalBuffer object at 0x1267a55c0>>\r\nTraceback (most recent call last):\r\n  File \"/Users/npradhan/miniconda3/envs/numpyro/lib/python3.6/site-packages/jaxlib/xla_client.py\", line 393, in __del__\r\n  File \"/Users/npradhan/miniconda3/envs/numpyro/lib/python3.6/site-packages/jaxlib/xla_client.py\", line 377, in delete\r\n  File \"/Users/npradhan/miniconda3/envs/numpyro/lib/python3.6/site-packages/jaxlib/xla_client.py\", line 104, in delete_buffer\r\nAttributeError: 'NoneType' object has no attribute 'DeleteLocalShapedBuffer'\r\n```\r\n\r\ncc. @hawkinsp ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26345) for more info**.\n\n<!-- need_sender_cla -->", "> Once you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\r\n\r\nSigned! ", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26345) for more info**.\n\n<!-- ok -->"]}, {"number": 26344, "title": "How to retrain with custom object and get frozen_inference.pb model file with Tensorflow.", "body": "Hi,\r\n\r\nI am working in a project for eye-lid detection. My eye-lid detection is based on landmark points in face. it uses tensor-flow library. i was stuck in a procedure how to retrain the algorithm with new data-set and generate frozen_inference.pb file. I have tried with procedure given in tensor-flow page but i cannot able to get frozen_inference.pb file. Please help me in the procedure how to retrain and get the frozen_inference,pb file.\r\n\r\nKindly do the needful..\r\n\r\nRegards,\r\nNiranjan B", "comments": ["1)In order to train the TensorFlow model, we will need to two files\u200a\u2014\u200acustom object Model files (.pb) and object names file (.pbtxt)\r\n2)Inorder to train our model, we need to provide it with some training data. In our case, since we want it to detect eye-lid, we need to collect images of various tools. Make sure you collect a ton of images and you can also datasets online for this.\r\n3)Once images are ready, we need to annotate them to describe the object in the image. To do this we can use a powerful graphical annotation tool called labellmg which generates an XML file for you.\r\n4)TensorFlow expects training data as TFRecord file. Inorder to convert the XML file we obtained from labelimg, we first need to convert it to CSV using xml_to_csv.py [link](https://github.com/datitran/raccoon_dataset/blob/master/xml_to_csv.py)\r\nnd then use generate_tfrecord.py [link](https://github.com/datitran/raccoon_dataset/blob/master/generate_tfrecord.py)\r\nto get the TFRecord file.\r\nWhile using the xml_to_csv.py script, make sure you change path to your images folder and the output csv folder.\r\nCommand\r\nimage_path = os.path.join(os.getcwd(),\u2019your image path\u2019.format(directory))\r\nxml_df.to_csv(\u2018your csv folder \u2019.format(directory), index=None)\r\n\r\nHere\u2019s how you use generate_tfrecord.py to convert your CSV to TFRecord.\r\n\r\npython generate_tfrecord.py --csv_input=screwdriver_data/train_labels.csv --output_path=screwdriver_data/train.record\r\n\r\nTo set the object name in TFRecord you can change the generate_tfrecord.py file in this place \u2014\r\n\r\n# TO-DO replace this with label map\r\ndef class_text_to_int(row_label):\r\n print(row_label)\r\n if row_label == \u2018Your object name\u2019:\r\n return 1\r\n else:\r\n None\r\n\r\n5)Clone the Tensorflow model repository and navigate to the research/object_detection folder and then execute the below commands in this path.\r\n\r\npython train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config\r\n6)All files will create in training folder. You can visualise the training progress by using TensorBoard by executing the below command in object_detection folder\u2014\r\n\r\n$ tensorbord --logdir='training'\r\n\r\n6)Finally, execute the below command in same path which will generate the model file(.bp) in the specified output folder. You should also use the latest \u201cTrained Checkpoint Prefix\u201d filename which is of the format\u200a\u2014\u200amodel.ckpt-XX in the below command.\r\n\r\npython export_inference_graph.py \\\r\n \u2014 input_type image_tensor \\\r\n \u2014 pipeline_config_path training/ssd_mobilenet_v1_pets.config \\\r\n \u2014 trained_checkpoint_prefix training/model.ckpt-XX\\\r\n \u2014 output_directory \u201cgraphfoldername\u201d\r\n\r\nNow your frozen_inference graph is generated and you are ready to rock", "hello Niranjan B,\r\nare you working on tensorflow object detection api?\r\nif so, add the new dataset with the old data, generate csv ,convert it to tfrecord\r\nthis command should begin the training process  Eg: python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_resnet101_coco.config\r\n\r\nTo generate inference graph Eg: python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/faster_rcnn_resnet101_coco.config --trained_checkpoint_prefix training/model.ckpt-10575 --output_directory inference_graph2019", "Hello, \r\n\r\nThanks for your reply.\r\n\r\ni have tried your method with racoon data-set and can successfully able to freeze my variables into frozen_inference_graph.pb.\r\n\r\nbut after getting the model file i have loaded it into my eye-detection algorithm and tested it..but its showing me the below error\r\n----------------------------------------------------------------------------------------------------------\r\npython Face_estimate_head_pose.py \r\nLinux is fine! Python multiprocessing works.\r\n2019-03-05 17:59:41.237918: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"Face_estimate_head_pose.py\", line 184, in <module>\r\n    main()\r\n  File \"Face_estimate_head_pose.py\", line 78, in main\r\n    marks = mark_detector.detect_marks(face_img)\r\n  File \"/home/user/Desktop/head-pose-estimation-master/mark_detector.py\", line 165, in detect_marks\r\n    logits_tensor = self.graph.get_tensor_by_name('logits/BiasAdd:0')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3666, in get_tensor_by_name\r\n    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3490, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3532, in _as_graph_element_locked\r\n    \"graph.\" % (repr(name), repr(op_name)))\r\nKeyError: \"The name 'logits/BiasAdd:0' refers to a Tensor which does not exist. The operation, 'logits/BiasAdd', does not exist in the graph.\"\r\n----------------------------------------------------------------------------------------------------------------\r\n\r\nPls find the code it is eyelid detection based on detected land marks in face. it uses landmark annotation method.\r\n ---------------------------------------------------------------------------------------------------------------\r\nclass MarkDetector:\r\n    \"\"\"Facial landmark detector by Convolutional Neural Network\"\"\"\r\n\r\n    def __init__(self, mark_model='/home/user/Desktop/eye/raccoon_dataset-master/training/output_inference_graph.pb'):\r\n        \"\"\"Initialization\"\"\"\r\n        # A face detector is required for mark detection.\r\n        self.face_detector = FaceDetector()\r\n\r\n        self.cnn_input_size = 128\r\n        self.marks = None\r\n\r\n        # Get a TensorFlow session ready to do landmark detection\r\n        # Load a (frozen) Tensorflow model into memory.\r\n        detection_graph = tf.Graph()\r\n        with detection_graph.as_default():\r\n            od_graph_def = tf.GraphDef()\r\n            with tf.gfile.GFile(mark_model, 'rb') as fid:\r\n                serialized_graph = fid.read()\r\n                od_graph_def.ParseFromString(serialized_graph)\r\n                tf.import_graph_def(od_graph_def, name='')\r\n        self.graph = detection_graph\r\n        self.sess = tf.Session(graph=detection_graph)\r\n\r\n    @staticmethod\r\n    def draw_box(image, boxes, box_color=(255, 255, 255)):\r\n        \"\"\"Draw square boxes on image\"\"\"\r\n        for box in boxes:\r\n            cv2.rectangle(image,\r\n                          (box[0], box[1]),\r\n                          (box[2], box[3]), box_color)\r\n\r\n    @staticmethod\r\n    def move_box(box, offset):\r\n        \"\"\"Move the box to direction specified by vector offset\"\"\"\r\n        left_x = box[0] + offset[0]\r\n        top_y = box[1] + offset[1]\r\n        right_x = box[2] + offset[0]\r\n        bottom_y = box[3] + offset[1]\r\n        return [left_x, top_y, right_x, bottom_y]\r\n\r\n    @staticmethod\r\n    def get_square_box(box):\r\n        \"\"\"Get a square box out of the given box, by expanding it.\"\"\"\r\n        left_x = box[0]\r\n        top_y = box[1]\r\n        right_x = box[2]\r\n        bottom_y = box[3]\r\n\r\n        box_width = right_x - left_x  \r\n        box_height = bottom_y - top_y \r\n\r\n        # Check if box is already a square. If not, make it a square.\r\n        diff = box_height - box_width\r\n        delta = int(abs(diff) / 2)\r\n\r\n        if diff == 0:                   # Already a square.\r\n            return box\r\n        elif diff > 0:                  # Height > width, a slim box.\r\n            left_x -= delta\r\n            right_x += delta\r\n            if diff % 2== 1:\r\n                right_x += 1\r\n        else:                           # Width > height, a short box.\r\n            top_y -= delta\r\n            bottom_y += delta\r\n            if diff % 2 == 1:\r\n                bottom_y += 1\r\n\r\n        # Make sure box is always square.\r\n        assert ((right_x - left_x) == (bottom_y - top_y)), 'Box is not square.'\r\n\r\n        return [left_x, top_y, right_x, bottom_y]\r\n\r\n    @staticmethod\r\n    def box_in_image(box, image):\r\n        \"\"\"Check if the box is in image\"\"\"\r\n        rows = image.shape[0]\r\n        cols = image.shape[1]\r\n        return box[0] >= 0 and box[1] >= 0 and box[2] <= cols and box[3] <= rows\r\n\r\n    def extract_cnn_facebox(self, image):\r\n        \"\"\"Extract face area from image.\"\"\"\r\n        _, raw_boxes = self.face_detector.get_faceboxes(\r\n            image=image, threshold=0.9)\r\n\r\n        for box in raw_boxes:\r\n            # Move box down.\r\n            diff_height_width = (box[3] - box[1]) - (box[2] - box[0])\r\n            offset_y = int(abs(diff_height_width / 2))\r\n            box_moved = self.move_box(box, [0, offset_y])\r\n\r\n            # Make box square.\r\n            facebox = self.get_square_box(box_moved)\r\n\r\n            if self.box_in_image(facebox, image):\r\n                return facebox\r\n\r\n        return None\r\n\r\n    def detect_marks(self, image_np):\r\n        \"\"\"Detect marks from image\"\"\"\r\n        # Get result tensor by its name.\r\n        logits_tensor = self.graph.get_tensor_by_name('logits/BiasAdd:0')\r\n\r\n        # Actual detection.\r\n        predictions = self.sess.run(\r\n            logits_tensor,\r\n            feed_dict={'input_image_tensor:0': image_np})\r\n\r\n        # Convert predictions to landmarks.\r\n        marks = np.array(predictions).flatten()\r\n        marks = np.reshape(marks, (-1, 2))\r\n\r\n        return marks\r\n\r\n    @staticmethod\r\n    def draw_marks(image, marks, color=(255, 255, 255)):\r\n        \"\"\"Draw mark points on image\"\"\"\r\n        for mark in marks:\r\n            cv2.circle(image, (int(mark[0]), int(\r\nmark[1])), 1, color, -1, cv2.LINE_AA)\r\n-----------------------------------------------------------------------------------------------------------------\r\nAm stuck with this issue for past one month..Please help with the procedure how to retrain and get the frozen_inference.pb file for eyelid detection.\r\n\r\nThanks & Regards,\r\nNiranjan B\r\n", "@NiranjanAntany Did you feed your annotated images to the model,along with seperate annotation file\r\ni.e \".pbtxt\"file\"?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 26343, "title": "cherrypicks for test and builds fixes for TF 2.0 alpha0", "body": "", "comments": ["Just the backwards compatible test. Is the new file in? ", "> Just the backwards compatible test. Is the new file in?\r\n\r\nyes that is in the list of cherrypicks", "Ubuntu CC is still failing with the old \"file not found\": https://source.cloud.google.com/results/invocations/2a2317c2-2a38-4851-b9ec-fa695e0b5f9f/targets/%2F%2Ftensorflow%2Fcore%2Fops%2Fcompat:backwards_compatibility_test/tests", "> Ubuntu CC is still failing with the old \"file not found\": https://source.cloud.google.com/results/invocations/2a2317c2-2a38-4851-b9ec-fa695e0b5f9f/targets/%2F%2Ftensorflow%2Fcore%2Fops%2Fcompat:backwards_compatibility_test/tests\r\n\r\nadded the build file dependancy"]}, {"number": 26342, "title": "Mixed precision Grappler optimizer", "body": "Adds an opt-in Grappler pass to convert parts of a graph from float32 to float16.\r\n\r\nThanks to @minminsun, @Dido0o0, and @yangjunpro of Alibaba for their useful collaboration and feedback on this work.\r\n\r\nAttention @azaks2 and @reedwm, and FYI @jlebar.", "comments": ["@rthadur, since this doesn't touch XLA, I don't think I am the right reviewer for this PR.", "@rthadur I think @reedwm and @rmlarsen are the more appropriate people for this PR, so I'm removing myself as well.", "We're looking into the compilation errors now.", "Thanks for the thorough review @reedwm. PTAL at my changes/comments in response.\r\n\r\nThere were a few very small things that we discovered after submitting the PR, so I've added those in too (see the last 4 commits).", "@reedwm @rmlarsen -- is there any special context we should have about the _ordering_ of the optimizers that run through MetaOptimizer?\r\n\r\nI've been doing perf testing and noticed that we can run into unfortunate cases with `Sum` ops generated for the backward graph. As an example, consider a graph that runs `Dense+Relu -> Dropout -> Dense+Relu -> LossFn`. We'd like to run everything between the matmuls in fp16, both forward and backward.\r\n\r\nBut the (naive) backward graph for Dropout contains `Sum` ops (due to `BroadcastGradientArgs`). This breaks the span-finding behavior of AutoMixedPrecision, and it keeps dropout backward in fp32.\r\n\r\nThe constant folding optimizer correctly recognizes that these broadcasts aren't necessary and strips everything out. So if we run AutoMixedPrecision after constant folding, we get a better set of casts. However, we don't want to make AutoMixedPrecision a \"run once at the end\" pass (like xla-fusion), since there's some benefit to running things like constant folding _after_ -- for example it can push casts of constants to be constants of the casted type.\r\n\r\nSo, what I'd like to do is move AutoMixedPrecision toward the end of the (implicit) ordering in MetaOptimizer and keep it \"run once,\" so that it can process an already-simplified graph but also benefit from downstream optimization.\r\n\r\nDoes that make sense? I'm anxious there is secret sauce in the existing ordering and don't want to break what's there.", "@cbcase We normally run all grappler passes twice:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L369\r\n\r\nso I think you can achieve the effect you desire by placing AutoMixedPrecision towards the end of the first iteration and  adding it to the set of run-once optimizers here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L80\r\n\r\nI agree that it would be nice to make the ordering more flexible and user defined.", "@rmlarsen, can you take a quick look at this PR now?", "@MattConley can you please check failed builds", "Looks like this change to VirtualPlacer's constructor is causing the break: https://github.com/tensorflow/tensorflow/commit/f089b3180ec7ddecd503d6d499cd6977c22b8f11\r\n\r\nValidating the fix now", "After the VirtualPlacer update, the build looks good on my end", "@reedwm @rmlarsen This should be ready for final review; let me know if I should change anything for the internal builds.", "@MattConley The default behavior is to have mixed precision on by default?\r\nI couldn't find a flag to toggle it on/off so just making sure.", "@rahulpalamuttam no, it defaults to off:\r\nhttps://github.com/tensorflow/tensorflow/pull/26342/files#diff-5b48a50cc60c18bb944e33e9c039f1c7R113\r\nThis toggle must be set to ON to enable it:\r\nhttps://github.com/tensorflow/tensorflow/pull/26342/files#diff-280f21be5641238b9220b8d5df758555R88", "To make use of Mixed precision, do I need to set any environment variable similarly what NVIDIA has `TF_ENABLE_AUTO_MIXED_PRECISION`?\r\n\r\nOR \r\n\r\nIs the variable different here?\r\n\r\nThanks?", "@asispatra   This week we will have a full example added to \r\nhttps://github.com/tensorflow/models/tree/master/official/resnet\r\n\r\nI am not sure how to alert you as to when it is finished.  We are adding it there to compare perf with the custom rolled solution and for continuous integration testing.  I am pretty sure you do not need the ENV VAR at all just using the auto loss scaling optimizer.  Feel free to ping me on this PR here WED this week or after assuming I have not gotten back to this thread to provide the link and more info.", "The recommended way to use mixed precision is to wrap your optimizer with [enable_mixed_precision_graph_rewrite](https://github.com/tensorflow/tensorflow/blob/66d4f9e6d8f65c56eda1091384c4f0967abd0fdf/tensorflow/python/training/experimental/mixed_precision.py#L79):\r\n\r\n```\r\noptimizer = ...\r\noptimizer = tf.train.experimental.enable_mixed_precision_graph_rewrite(optimizer)\r\n```\r\n\r\nAs Toby said, there is no environmental variable in upstream TensorFlow, and we will update the official model with this change this week. \r\n\r\nNvidia did publish a fork of TensorFlow in their [NDG container](https://devblogs.nvidia.com/nvidia-automatic-mixed-precision-tensorflow/), where mixed precision can be enabled with an environmental variable, but in upstream you instead wrap the optimizer.", "@tfboyd and @reedwm, Thanks for the information. I will also keep watching this PR till WED to get more update.\r\n\r\nFor this PR, with there be any change in the [Tensorflow Benchmarks](https://github.com/tensorflow/benchmarks) code? I am asking this, because while I was trying NVIDIA's Tensorflow AutomaticMixedPrecision with Benchmark code, it was failing. \r\n\r\nThanks.", "@asispatra we have partial stepped away from tensorflow/benchmarks for the moment.  Our focus is bringing the examples in tensorflow/models/official up to speed to benefit users of the high level APIs.  That said, I suspect @reedwm would address issues you are having and it would be nice to have \"AMP\" in the tensorflow/benchmark.  \r\n\r\nThe long story shortened is we are using tf_cnn_benchmark as our high level benchmark for ResNet but beyond that we are not doing much with it currently.  But we do not want it broken for TF 1.x", "I didn't plan on adding support to tf_cnn_benchmarks, but I might change my mind. One issue is that tf_cnn_benchmarks uses tf.gradients to compute gradients, while `enable_mixed_precision_graph_rewrite` requires you to call optimizer.compute_gradients.\r\n\r\nNote tf_cnn_benchmarks already supports fp16 with manual casts. This is very ugly and difficult compared to `enable_mixed_precision_graph_rewrite`, but the rest of tf_cnn_benchmarks is fairly messy as well.", "@reedwm \r\nCan we use mixed precision without loss_scaling (via enable_mixed_precision_graph_rewrite) i.e. a value of None instead of 'dynamic'?\r\nOr could we just give it a value of 1.0?", "You should keep loss_scale as 'dynamic'. Giving it 1.0 is equivalent to not doing loss scaling, but without loss scaling, your model may converge to a worse accuracy. You cannot pass None.", "has anybody tested this with horovod?\r\n\r\nI would be replacing this line basically to wrap a horovod optimizer\r\n\r\nhttps://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow/LanguageModeling/BERT/optimization.py#L80", "No but Nvidia did the PR so I am pretty sure it works.  They have a Bert\nwith FP16 and XLA that is very fast in a blog.  It may predate this exact\ncode path.\n\nOn Fri, May 24, 2019, 12:21 PM Randall Lin <notifications@github.com> wrote:\n\n> has anybody tested this with horovod?\n>\n> I would be replacing this line basically to wrap a horovod optimizer\n>\n>\n> https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow/LanguageModeling/BERT/optimization.py#L80\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/26342?email_source=notifications&email_token=AFTF5MWTAYOCGK7MLDGHQU3PXA5Z7A5CNFSM4G3W4WM2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGK62I#issuecomment-495759209>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFTF5MTLRMW2EZ6DPS4OHDLPXA5Z7ANCNFSM4G3W4WMQ>\n> .\n>\n", "@tfboyd yea I'm basically trying to replicate their squad benchmarks right now but on an image with tf nightly + horovod based on https://gitlab.com/nvidia/cuda/tree/ubuntu18.04 (vs using the NGC image which is a blackbox)", "I have reproduced the original Nvidia benchmark on SQUAD on 1x V100 16Gi using tf nightly based off this (open) image https://gitlab.com/nvidia/cuda (the original benchmark uses https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel_19.03.html#rel_19.03 which is closed source)\r\n\r\nSee https://github.com/NVIDIA/DeepLearningExamples/issues/53#issuecomment-495808640 for details.\r\n\r\nHowever I run into an OOM issue with 8x V100 16Gi.  More info here https://github.com/NVIDIA/DeepLearningExamples/issues/53#issuecomment-495808640\r\n\r\nAny insights or assistance would be appreciated, as I haven't found an open source reproduction of multi-gpu with fp16 (AMP) with xla on horovod.\r\n\r\nEDIT: I don't get OOM anymore, I had left out a horovod flag.  However, I do get a segfault on a horovod allreduce call now.", "@rllin-fathom it's possible that we're using different definitions of closed source, but as I mentioned in the other thread, the NGC images do contain full source code for the TensorFlow build that they include, plus instructions for how to rebuild with your own customizations.", "@benbarsdell mm yes you're right it does, I was speaking towards the fact that it's unclear if the image has any other customizations not apparent in the release notes.  I've (for now, based on confirmation from you and @nluehr (thanks!)) assumed that the tf packaged in the image is functionally identical to tf nightly (since i've replicated fp16 AMP with XLA speeds for 1x V100 on my image).\r\n\r\nThus the remaining unknown for me is if there are image build customizations, that I am not cloning.  But I did try to follow the release notes at https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel_19.03.html#rel_19.03 and horovod installation instructions at https://github.com/horovod/horovod/blob/master/docs/gpus.md  and https://github.com/horovod/horovod/blob/master/Dockerfile.test.gpu\r\n\r\nEDIT: mainly concerned with the horovod installation piece given I did replicate on 1x V100", "e.g.\r\n\r\nwhich any flags during horovod installation were turned on https://github.com/horovod/horovod/blob/master/docs/gpus.md#advanced-have-a-proprietary-mpi-implementation-with-gpu-support-optimized-for-your-network", "How do i enable auto mixed precision in keras with tensorflow?", "@xpirad, you can use auto mixed precision by wrapping your optimizer as follows\r\n\r\n```\r\nopt = ... # A TensorFlow optimizer\r\nopt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\r\n```\r\n\r\nThis works regardless of whether you use tf.keras or not. See the [enable_mixed_precision_graph_rewrite docstring](https://github.com/tensorflow/tensorflow/blob/165a535d07750de0d12747649423259107ec6539/tensorflow/python/training/experimental/mixed_precision.py#L75) for details.\r\n\r\nWe are also working on a pure tf.keras API for TF 2, but this API is not yet ready.", "@reedwm what is the lowest supported Tensorflow version? ", "@yselivonchyk, this is available in TF 1.14 or higher.", "Is Mixed Precision only  supported for optimization in TF 1.14 ?  Is there a way to enable Mixed Precision  for inference ? ", "You can enable the graph rewrite pass by setting the following session config option.\r\n\r\n`config.graph_options.rewrite_options.rewrite_options.auto_mixed_precision = 1`", "Thanks. So the NVIDIA recommended way of setting  TF_ENABLE_AUTO_MIXED_PRECISION=1 in the environment does not apply ?   Also, Mixed Precision has to be specifically enabled , not auto-detected ? \r\n", "TF_ENABLE_AUTO_MIXED_PRECISION only applies to the NVIDIA NGC TensorFlow Docker container. And yes, enabling mixed precision must be enabled by setting the config option above (to enable only the graph rewrite pass) or calling enable_mixed_precision_graph_rewrite() to enable both the graph rewrite and wrap the optimizer with loss scaling.", "Will we have TF_ENABLE_NHWC soon for official tensorflow? TF_ENABLE_NHWC is important to help tftrt optimize graph written in NHWC layout, otherwise there will be a lot of NHWC/NCHW conversion inserted by tftrt on volta/turing gpus. Since NHWC layout has the best performance for tensor core, tftrt need to support graphes in NHWC layout.\r\n\r\nI also observed that if the graph is written in NCHW layout, tftrt still inserted a lot of alternating NCHW/NHWC conversions on volta gpus. Such conversions are redundant and time consuming. They need to be automatically removed.\r\n\r\n@aaroey  @pooyadavoodi ", "@gpx1000 \r\nAs of tf-nightly-gpu and f-nightly-gpu-2.0-preview 2.0.0-dev20190723  there is an automatic pass with what is known as \"grappler\" that will rewrite NCHW to NHWC if you are using FP16.  The only model I am testing for the scenario is ResNet50 V1.5 and perf with real data improved from 767 to 1,008 image/sec without using XLA.  The tests are done with the data format in the code of channels first (NCHW); and \"grappler\" does the change.  It should work for any model; but I want to be clear I am only testing the one.  \r\n\r\nLooking toward the foggy future.  The XLA number is 1,313; but multi-GPU (even single node) is not well supported.  FYI if you were using FP16 with XLA before it would have already been flipping to NHWC and it has a few more tricks.  Again, not ready for production\r\n\r\nThis was all done using the high level APIs with these models.\r\n\r\n- [Keras](https://github.com/tensorflow/models/tree/master/official/resnet/keras)\r\n- [Old Skool Estimator and tf.layers](https://github.com/tensorflow/models/tree/master/official/r1/resnet)\r\n- tf_cnn_benchmarks with lower level API usage experiences the same gains as well.", "@tfboyd Actually my model was manually coded in NHWC and FP16 at the first time, but tftrt didn't optimize well for this model since it generated too many redundant conversions between NHWC/NCHW. Then I manually modify the model to use NCHW, and tftrt works better now, but the generated tft graph still have many redundant conversions. All tests are done on turing gpus. So I think a grappler that rewrites NCHW to NHWC doesn't help much, since this could be easily done manually.\r\n\r\nI have also compared XLA and tftrt for tf 1.14. For my model, tftrt with NCHW input graph (which has many redundant NCHW/NHWC conversions inserted to the trt graph) has better inference speed than XLA with NHWC, both on turing gpu with FP16 precision used.", "> @xpirad, you can use auto mixed precision by wrapping your optimizer as follows\r\n> \r\n> ```\r\n> opt = ... # A TensorFlow optimizer\r\n> opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\r\n> ```\r\n> \r\n> This works regardless of whether you use tf.keras or not. See the [enable_mixed_precision_graph_rewrite docstring](https://github.com/tensorflow/tensorflow/blob/165a535d07750de0d12747649423259107ec6539/tensorflow/python/training/experimental/mixed_precision.py#L75) for details.\r\n> \r\n> We are also working on a pure tf.keras API for TF 2, but this API is not yet ready.\r\n\r\nThanks for the solution. Not sure if I'm doing it wrong, but I tried it and got:\r\n\r\n```\r\nWARNING:tensorflow:You already have existing Sessions that do not use mixed precision. enable_mixed_precision_graph_rewrite() will not affect these Sessions.\r\n```\r\n\r\nHere is the code to reproduce this warning:\r\n```\r\nimport os\r\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]= '1'\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nimport numpy as np\r\nimport tensorflow\r\nfrom tensorflow.keras.applications.vgg16 import VGG16\r\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\n\r\nfrom tensorflow.python.framework.ops import disable_eager_execution\r\ndisable_eager_execution()\r\n\r\nbatch_size = 256\r\nimagenet_test = 'some image directory'\r\n\r\nmodel = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\r\n\r\nopt = Adam(lr=0.0003)\r\nopt_mixed = tensorflow.train.experimental.enable_mixed_precision_graph_rewrite(opt)\r\nmodel.compile(opt_mixed, loss='sparse_categorical_crossentropy', metrics=['acc'])\r\n\r\ndatagen = ImageDataGenerator(preprocessing_function=preprocess_input)\r\ntest_generator = datagen.flow_from_directory(directory=imagenet_test,\r\n                                            batch_size=batch_size,\r\n                                            class_mode='sparse',\r\n                                            target_size=(224, 224))\r\nsteps = np.ceil(len(test_generator.classes) / batch_size)\r\n\r\nmodel.evaluate_generator(test_generator, steps, verbose=1)\r\n```\r\n\r\nI think the warning only appears when I set eager execution off and the reason I turned it off is due to another issue I had with OOM #33174 ", "@don-tpanic, try calling `enable_mixed_precision_graph_rewrite` before creating the VGG16 model.", "> You can enable the graph rewrite pass by setting the following session config option.\r\n> \r\n> `config.graph_options.rewrite_options.rewrite_options.auto_mixed_precision = 1`\r\n\r\ntwo \"rewrite_options\" can't work, correct it to be:\r\n`config.graph_options.rewrite_options.auto_mixed_precision = 1`"]}, {"number": 26341, "title": "A function decorated with tf.function cannot return a variable by reference", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOSX 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\ntf.version.VERSION: '2.0.0-dev20190304'\r\ntf.version.GIT_VERSION: 'v1.12.0-9475-gc1487a9c93'\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nA function decorated with `tf.function` cannot return a variable by reference. This makes it impossible to write accessors, so we end up having to access variables directly, which leads to ugly code & hard to maintain. For example, this is the root cause of #25754.\r\nThis might be considered a feature request, rather than an issue: I grant you that it's debatable! :)\r\n\r\n**Describe the expected behavior**\r\nI expect `return variable` to return a reference, not a copy of the variable. \r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\ncounter = tf.Variable(0)\r\n    \r\n@tf.function\r\ndef increment():\r\n    return counter.assign_add(1) # ugly, direct access, but it works\r\n    \r\nprint(increment().numpy()) # prints 1\r\nprint(increment().numpy()) # prints 2\r\n\r\n@tf.function\r\ndef get_counter():\r\n    return counter   # actually returns a Tensor (a copy), not a reference to the variable\r\n\r\n@tf.function\r\ndef increment():\r\n    return get_counter().assign_add(1)  # access through an accessor\r\n\r\nincrement() # raises AttributeError: 'Tensor' object has no attribute 'assign_add'\r\n```\r\n\r\n**Other info / logs**\r\nHere is the stacktrace:\r\n\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-1-dfbc861847ab> in <module>\r\n     18     return get_counter().assign_add(1)\r\n     19\r\n---> 20 increment() # raises AttributeError: '[...].EagerTensor' object has no attribute 'assign_add'\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    424     # This is the first call of __call__, so we have to initialize.\r\n    425     initializer_map = {}\r\n--> 426     self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n    427     if self._created_variables:\r\n    428       try:\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    368     self._concrete_stateful_fn = (\r\n    369         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 370             *args, **kwds))\r\n    371\r\n    372     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1311     if self._input_signature:\r\n   1312       args, kwargs = None, None\r\n-> 1313     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1314     return graph_function\r\n   1315\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   1578           or call_context_key not in self._function_cache.missed):\r\n   1579         self._function_cache.missed.add(call_context_key)\r\n-> 1580         graph_function = self._create_graph_function(args, kwargs)\r\n   1581         self._function_cache.primary[cache_key] = graph_function\r\n   1582         return graph_function, args, kwargs\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   1510             arg_names=arg_names,\r\n   1511             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 1512             capture_by_value=self._capture_by_value),\r\n   1513         self._function_attributes)\r\n   1514\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    692                                           converted_func)\r\n    693\r\n--> 694       func_outputs = python_func(*func_args, **func_kwargs)\r\n    695\r\n    696       # invariant: `func_outputs` contains only Tensors, IndexedSlices,\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    315         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    316         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    318     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    319\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    684                   optional_features=autograph_options,\r\n    685                   force_conversion=True,\r\n--> 686               ), args, kwargs)\r\n    687\r\n    688         # Wrapping around a decorator allows checks like tf_inspect.getargspec\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)\r\n    390     return _call_unconverted(f, args, kwargs)\r\n    391\r\n--> 392   result = converted_f(*effective_args, **kwargs)\r\n    393\r\n    394   # The converted function's closure is simply inserted into the function's\r\n\r\n/var/folders/wy/h39t6kb11pnbb0pzhksd_fqh0000gn/T/tmpfo1eq6i2.py in tf__increment()\r\n      4   retval_ = None\r\n      5   do_return = True\r\n----> 6   retval_ = ag__.converted_call('assign_add', get_counter(), ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (1,), {})\r\n      7   return retval_\r\n      8\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)\r\n    234       owner = inspect_utils.SuperWrapperForDynamicAttrs(owner)\r\n    235\r\n--> 236     f = getattr(owner, f)\r\n    237\r\n    238   if inspect_utils.isbuiltin(f):\r\n\r\nAttributeError: 'Tensor' object has no attribute 'assign_add'\r\n```\r\n\r\n", "comments": ["This is intended behavior for now because returning variables by reference can lead to different semantics in legacy graphs vs tf.function graphs."]}, {"number": 26340, "title": "Timeline not reporting compute times for some GPU ops", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: Python 2.7.15rc1\r\n- Bazel version (if compiling from source): 0.21\r\n- GCC/Compiler version (if compiling from source): (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: 10.0/7.4.2\r\n- GPU model and memory: NVIDIA Corporation GP104GL [Quadro P5000] (2 GPUs)\r\n\r\n\r\n**Describe the current behavior**\r\nI am running the sample [RNNLM code](https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb). I just added timeline logging code in the given tutorial for training ([Modified code here](https://github.com/xilenteyex/RNNLM_ptb_timeline)) to analyze the sequence of low level operations being executed as well as the time each operation is taking. There are a lot of operations which are placed on GPU and they have just kernel queue/launch times recorded in timeline in `/job:localhost/replica:0/task:0/device:GPU:.* Compute` streams. I am unable to find actual time to execute those operations in `/device:GPU:0/.*Compute` streams. To identify these operations, I used [this script](https://github.com/xilenteyex/RNNLM_ptb_timeline/blob/master/get_unexec_ops.py). Is this a bug, or am I missing something in my understanding of Timeline? [List of operations with kernel queue times reported, but no execution times can be found here.](https://github.com/xilenteyex/RNNLM_ptb_timeline/blob/master/un_exec_tmp.txt)\r\nI want to do this because I am trying to do model parallelism for this RNNLM (Reccurrent Neural Network for Language Modeling) code and I want to place ops after running a heuristic graph partitioning algorithm which takes as input the estimated compute time as well as dependencies for each operation.\r\n\r\n**Describe the expected behavior**\r\nFor every kernel that is queued on a GPU I expect it to be executed at some point and logged in the Timeline\r\n\r\n**Code to reproduce the issue**\r\n[tutorial code for RNNLM using ptb_data_set can be found here](https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb)\r\n[modified tutorial code with Timline logging added for training loop can be found here](https://github.com/xilenteyex/RNNLM_ptb_timeline)\r\n\r\nexecute the following commands to get the list of operations which have kernel queue times in timeline and execution times are not logged. A file named `new_unexecute_ops.txt` will be created and each line contains name of one of these operations.\r\n \r\n```\r\ngit clone https://github.com/xilenteyex/RNNLM_ptb_timeline\r\npython ptb_word_lm_timeline.py --data_path=simple-examples/data/ --model=medium\r\npython get_unexec_ops.py logs/timeline_logs/timeline_rnnlm_med_0_100.ctf.json new_unexecute_ops.txt\r\n```\r\n\r\n\r\n\r\n**Other info / logs**\r\n[Sample timeline file can be found here](https://github.com/xilenteyex/RNNLM_ptb_timeline/tree/master/logs/timeline_logs)\r\n[List of ops with missing execution times in the timeline file can be found here](https://github.com/xilenteyex/RNNLM_ptb_timeline/blob/master/un_exec_tmp.txt)\r\n", "comments": ["I am trying to write a custom graph partitioner and am stuck on this issue for a very long time. If someone can help me figure this out, it will be great. Thanks!", "+Paul Barham <pbar@google.com>\n\nOn Thu, May 2, 2019, 7:46 AM xilenteyex <notifications@github.com> wrote:\n\n> I am trying to write a custom graph partitioner and am stuck on this issue\n> for a very long time. If someone can help me figure this out, it will be\n> great. Thanks!\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26340#issuecomment-488701987>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AANWFG2X6XSEESXAV7XZAXLPTL5EPANCNFSM4G3WUCPA>\n> .\n>\n", "@prb12 , can you please have a look at this ?\r\nThanks!", "It's a *long* time since I worked on any of this code... but from a quick scan of the 'list of ops with missing exection times' it looks like the vast majority are what I would call 'metadata-only' ops ... i.e. ops that do not need to launch a CUDA kernel to compute their results.   Examples include things like `Shape`, `Rank`, `BroadcastGradientArgs`, which all deal with the *shapes* of tensors and return integer results in host memory.  Other ops in this list are `Const`s ... which, even if they are float tensors are already in GPU memory and all the op needs to do is return a pointer.  ", "Thanks for the Explanation!", "@prb12 , for a Shape type op for example, will it copy data to GPU or it will just keep the tensor in host memory and return pointer to the reshaped tensor ?", "A `Shape` op does not need to look at any of the values in a tensor.  The metadata lives in the hosts memory, no matter what TF device the op runs on.  It returns a tiny int32 (or int64?) tensor whose values are the dimension sizes  - these always live in host memory, even on the GPU device (one of TF's strange foibles - but it exists precisely because of things like this, since much TF gradient code does a lot of shape 'computation' and it would be dreadful to launch tiny GPU ops to do all of this.  Also conditionals and control flow decisions need to be done on the host and mainly use int32/bool tensors).  You can look at the `OpKernel` registrations to confirm this: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/shape_ops.cc#L25-L29\r\n\r\n```\r\n  REGISTER_KERNEL_BUILDER(Name(\"Shape\")                          \\\r\n                              .Device(DEVICE_GPU)                \\\r\n                              .HostMemory(\"output\")              \\\r\n                              .TypeConstraint<int32>(\"out_type\") \\\r\n                              .TypeConstraint<type>(\"T\"),        \\\r\n                          ShapeOp<int32>);                       \\\r\n```\r\n\r\nAnother thing to bear in mind is that a lot of these 'metadata' operations are applied to tensors whose shapes are known statically - they can frequently be optimized out completely.\r\n\r\nSince you mentioned \"reshaped tensor\" ... perhaps you meant the `Reshape` op? ... in which case, the following happens:  \r\n\r\nAssuming a float tensor input on the GPU device, the Tensor metadata object is in the host's DRAM.   This contains a pointer to a buffer in GPU memory where the tensor's data lives.  `Reshape` will make a new Tensor object whose metadata contains the new shape/dimensions.  The buffer pointer can just point at the same GPU memory since no arguments to `Reshape` ever require the data to be shuffled.  (this is true of TF, pytorch, numpy etc)\r\n\r\nNow, `Transpose` would be a different story...    \r\n\r\n\r\n\r\n\r\n"]}, {"number": 26339, "title": "Build r1.12 from source with CUDA10.0 sucessfully, but still need CUDA9.0 when importing", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: r1.12\r\n- Python version: py3.6\r\n- Installed using virtualenv? pip? conda?: source and pip \r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 4.9.3\r\n- CUDA/cuDNN version: CUDA10.0/cuDNN7.5.0\r\n- GPU model and memory: TitanXP\r\n\r\n**Describe the problem**\r\nSince I want to use CUDA10.0, I tried to build TF1.12 from source with above versions.\r\nThe build is successfully done, and my .tf_configure.bazelrc is correct.\r\nAfter install the pip wheel package, I import the tensorflow but it still look for CUDA9.0 which is not provided in environment path.\r\n\r\nERROR:\r\n```\r\nPython 3.6.4 (default, Mar 23 2018, 22:59:35) \r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\nI tried the pip wheel that built by other people but still get the same error.\r\nSeems somehow CUDA9.0 is needed by some components? \r\nAny hint will be welcome\r\n\r\nConfiguration:\r\n```\r\n$ ./configure \r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.15.0 installed.\r\nDo you wish to build TensorFlow with Apache Ignite support? [Y/n]: n\r\nNo Apache Ignite support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 10.0\r\n\r\n\r\nPlease specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /home/web_server/xiaolun/cuda-10.0\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: \r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /home/web_server/xiaolun/cuda-10.0]: \r\n\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify the location where TensorRT is installed. [Default is /usr/lib/x86_64-linux-gnu]:/home/web_server/xiaolun/TensorRT-5.0.2.6-10.0\r\n\r\n\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 2.3\r\n\r\n\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /home/web_server/xiaolun/cuda-10.0]:\r\n\r\n\r\nNCCL found at /media/disk1/fordata/web_server/project/xiaolun/cuda-10.0/lib64/libnccl.so.2\r\nAssuming NCCL header path is /media/disk1/fordata/web_server/project/xiaolun/cuda-10.0/lib64/../include/nccl.h\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 3.5\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /home/web_server/gcc-4.9.3/bin/gcc]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=gdr         \t# Build with GDR support.\r\n\t--config=verbs       \t# Build with libverbs support.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\nConfiguration finished\r\n\r\n```", "comments": ["Solved by compile a new python3.6.8"]}, {"number": 26338, "title": "CPU build error on OSX Mojave in hwloc dependency", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX - 10.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master branch\r\n- Python version: Python 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.22\r\n- GCC/Compiler version (if compiling from source): clang 10.0.1\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the problem**\r\nI have followed the instructions in the [installation docs](https://www.tensorflow.org/install/source#macos) for Mac, but when I try to build the source using bazel:\r\n\r\n```\r\n bazel build --verbose_failures --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nI see the following errors when trying to build `hwloc`:\r\n```\r\nexternal/hwloc/hwloc/distances.c:32:3: error: use of undeclared identifier 'locale_t'\r\n```\r\n\r\n<Details> <summary><em>trace</em></summary>\r\n```\r\n\r\nERROR: /private/var/tmp/_bazel_npradhan/36fac8838dc4eaff04ef63d531bc51a3/external/hwloc/BUILD.bazel:214:1: C++ compilation of rule '@hwloc//:hwloc' failed (Exit 1): wrapped_clang failed: error executing command\r\n  (cd /private/var/tmp/_bazel_npradhan/36fac8838dc4eaff04ef63d531bc51a3/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM='' \\\r\n    APPLE_SDK_VERSION_OVERRIDE='' \\\r\n    PATH='/Users/npradhan/miniconda3/envs/numpyro/bin:/Users/npradhan/miniconda3/bin:/Users/npradhan/.cargo/bin:/usr/local/opt/python/libexec/bin:/Users/npradhan/.nvm/v0.10.32/bin:/bin:/usr/local/sbin:/usr/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/usr/local/munki:/Library/TeX/texbin:/Users/npradhan/bin:/Users/npradhan/gocode/bin:/Users/npradhan/.opus/bin:/Users/npradhan/miniconda2/bin:/Users/npradhan/.opus/bin:~/Library/Python/3.6/bin' \\\r\n    XCODE_VERSION_OVERRIDE=10.2.0 \\\r\n  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -iquote external/hwloc -iquote bazel-out/host/genfiles/external/hwloc -iquote bazel-out/host/bin/external/hwloc -isystem external/hwloc/hwloc -isystem bazel-out/host/genfiles/external/hwloc/hwloc -isystem bazel-out/host/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/host/genfiles/external/hwloc/include -isystem bazel-out/host/bin/external/hwloc/include -MD -MF bazel-out/host/bin/external/hwloc/_objs/hwloc/distances.d '-frandom-seed=bazel-out/host/bin/external/hwloc/_objs/hwloc/distances.o' -isysroot __BAZEL_XCODE_SDKROOT__ -g0 '-march=native' -I. -Ihwloc -Iinclude -Wno-vla '-DHWLOC_DUMPED_HWDATA_DIR=' '-DRUNSTATEDIR=' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/hwloc/hwloc/distances.c -o bazel-out/host/bin/external/hwloc/_objs/hwloc/distances.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nexternal/hwloc/hwloc/distances.c:32:3: error: use of undeclared identifier 'locale_t'\r\n  hwloc_localeswitch_declare;\r\n  ^\r\nexternal/hwloc/include/private/misc.h:493:36: note: expanded from macro 'hwloc_localeswitch_declare'\r\n#define hwloc_localeswitch_declare locale_t __old_locale = (locale_t)0, __new_locale\r\n                                   ^\r\nexternal/hwloc/hwloc/distances.c:52:5: error: use of undeclared identifier '__new_locale'\r\n    hwloc_localeswitch_init();\r\n    ^\r\nexternal/hwloc/include/private/misc.h:495:3: note: expanded from macro 'hwloc_localeswitch_init'\r\n  __new_locale = newlocale(LC_ALL_MASK, \"C\", (locale_t)0); \\\r\n  ^\r\nexternal/hwloc/hwloc/distances.c:52:5: warning: implicit declaration of function 'newlocale' is invalid in C99 [-Wimplicit-function-declaration]\r\nexternal/hwloc/include/private/misc.h:495:18: note: expanded from macro 'hwloc_localeswitch_init'\r\n  __new_locale = newlocale(LC_ALL_MASK, \"C\", (locale_t)0); \\\r\n                 ^\r\nexternal/hwloc/hwloc/distances.c:52:5: error: use of undeclared identifier 'locale_t'\r\nexternal/hwloc/include/private/misc.h:495:47: note: expanded from macro 'hwloc_localeswitch_init'\r\n  __new_locale = newlocale(LC_ALL_MASK, \"C\", (locale_t)0); \\\r\n                                              ^\r\nexternal/hwloc/hwloc/distances.c:52:5: error: use of undeclared identifier 'LC_ALL_MASK'\r\nexternal/hwloc/include/private/misc.h:495:28: note: expanded from macro 'hwloc_localeswitch_init'\r\n  __new_locale = newlocale(LC_ALL_MASK, \"C\", (locale_t)0); \\\r\n                           ^\r\nexternal/hwloc/hwloc/distances.c:52:5: error: use of undeclared identifier 'locale_t'\r\nexternal/hwloc/include/private/misc.h:496:24: note: expanded from macro 'hwloc_localeswitch_init'\r\n  if (__new_locale != (locale_t)0)                         \\\r\n                       ^\r\nexternal/hwloc/hwloc/distances.c:52:5: error: use of undeclared identifier '__new_locale'; did you mean 'newlocale'?\r\nexternal/hwloc/include/private/misc.h:496:7: note: expanded from macro 'hwloc_localeswitch_init'\r\n  if (__new_locale != (locale_t)0)                         \\\r\n      ^\r\nexternal/hwloc/hwloc/distances.c:52:5: note: 'newlocale' declared here\r\nexternal/hwloc/include/private/misc.h:495:18: note: expanded from macro 'hwloc_localeswitch_init'\r\n  __new_locale = newlocale(LC_ALL_MASK, \"C\", (locale_t)0); \\\r\n                 ^\r\nexternal/hwloc/hwloc/distances.c:52:5: error: use of undeclared identifier '__old_locale'\r\n    hwloc_localeswitch_init();\r\n    ^\r\nexternal/hwloc/include/private/misc.h:497:5: note: expanded from macro 'hwloc_localeswitch_init'\r\n    __old_locale = uselocale(__new_locale);                \\\r\n    ^\r\nexternal/hwloc/hwloc/distances.c:52:5: warning: implicit declaration of function 'uselocale' is invalid in C99 [-Wimplicit-function-declaration]\r\nexternal/hwloc/include/private/misc.h:497:20: note: expanded from macro 'hwloc_localeswitch_init'\r\n    __old_locale = uselocale(__new_locale);                \\\r\n                   ^\r\nexternal/hwloc/hwloc/distances.c:52:5: error: use of undeclared identifier '__new_locale'; did you mean 'newlocale'?\r\nexternal/hwloc/include/private/misc.h:497:30: note: expanded from macro 'hwloc_localeswitch_init'\r\n    __old_locale = uselocale(__new_locale);                \\\r\n                             ^\r\nexternal/hwloc/hwloc/distances.c:52:5: note: 'newlocale' declared here\r\nexternal/hwloc/include/private/misc.h:495:18: note: expanded from macro 'hwloc_localeswitch_init'\r\n  __new_locale = newlocale(LC_ALL_MASK, \"C\", (locale_t)0); \\\r\n                 ^\r\nexternal/hwloc/hwloc/distances.c:62:5: error: use of undeclared identifier 'locale_t'\r\n    hwloc_localeswitch_fini();\r\n    ^\r\nexternal/hwloc/include/private/misc.h:500:24: note: expanded from macro 'hwloc_localeswitch_fini'\r\n  if (__new_locale != (locale_t)0) {   \\\r\n                       ^\r\nexternal/hwloc/hwloc/distances.c:62:5: error: use of undeclared identifier '__new_locale'\r\nexternal/hwloc/include/private/misc.h:500:7: note: expanded from macro 'hwloc_localeswitch_fini'\r\n  if (__new_locale != (locale_t)0) {   \\\r\n      ^\r\nexternal/hwloc/hwloc/distances.c:62:5: warning: implicit declaration of function 'uselocale' is invalid in C99 [-Wimplicit-function-declaration]\r\nexternal/hwloc/include/private/misc.h:501:5: note: expanded from macro 'hwloc_localeswitch_fini'\r\n    uselocale(__old_locale);           \\\r\n    ^\r\nexternal/hwloc/hwloc/distances.c:62:5: error: use of undeclared identifier '__old_locale'\r\nexternal/hwloc/include/private/misc.h:501:15: note: expanded from macro 'hwloc_localeswitch_fini'\r\n    uselocale(__old_locale);           \\\r\n              ^\r\nexternal/hwloc/hwloc/distances.c:62:5: warning: implicit declaration of function 'freelocale' is invalid in C99 [-Wimplicit-function-declaration]\r\nexternal/hwloc/include/private/misc.h:502:5: note: expanded from macro 'hwloc_localeswitch_fini'\r\n    freelocale(__new_locale);          \\\r\n    ^\r\nexternal/hwloc/hwloc/distances.c:62:5: error: use of undeclared identifier '__new_locale'\r\nexternal/hwloc/include/private/misc.h:502:16: note: expanded from macro 'hwloc_localeswitch_fini'\r\n    freelocale(__new_locale);          \\\r\n               ^\r\n4 warnings and 12 errors generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 14.263s, Critical Path: 6.70s\r\nINFO: 158 processes: 158 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n</Details>", "comments": ["I also see another (different) error when I tried again, for the same dependency:\r\n\r\n```\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (379 packages loaded, 18380 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /private/var/tmp/_bazel_npradhan/36fac8838dc4eaff04ef63d531bc51a3/external/hwloc/BUILD.bazel:214:1: C++ compilation of rule '@hwloc//:hwloc' failed (Exit 1)\r\nexternal/hwloc/hwloc/topology-linux.c:39:10: fatal error: 'mntent.h' file not found\r\n#include <mntent.h>\r\n```\r\n\r\nIt seems suspect that is is trying to build `topology-linux.c`.", "@neerajprad  Can you please try \"xcode-select --install\" and then build for Bazel 0.15.0\r\nThanks.", "> @neerajprad Can you please try \"xcode-select --install\" and then build for Bazel 0.15.0\r\nThanks.\r\n\r\n`xcode-select --install` gives a \"error: command line tools are already installed\" so I am guessing we are good on that front. Could you elaborate on what you mean by building for Bazel 0.15.0? If I downgrade bazel, I get a complaint \"Please upgrade your bazel installation to version 0.19.0 or higher to build TensorFlow!\".", "@neerajprad Could you try not to enable NUMA support in your `./configure` script? It does not support macOS right now, as suggested in [here](https://github.com/tensorflow/tensorflow/issues/19391#issuecomment-456506914).", "I had NUMA disabled. I just pulled from master and I think this issue was recently resolved and I was able to build now without any changes at my end."]}, {"number": 26337, "title": "Make weighted_cross_entropy_with_logits consistent with sigmoid_cross_entropy_with_logits in signature", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, the signature of the two functions are:\r\n```python\r\n@tf_export(\"nn.sigmoid_cross_entropy_with_logits\")\r\ndef sigmoid_cross_entropy_with_logits(  # pylint: disable=invalid-name\r\n    _sentinel=None,\r\n    labels=None,\r\n    logits=None,\r\n    name=None):\r\n```\r\nAnd \r\n```python\r\n@tf_export(\"nn.weighted_cross_entropy_with_logits\")\r\ndef weighted_cross_entropy_with_logits(targets, logits, pos_weight, name=None):\r\n```\r\n\r\nTF2.0 is a good chance to make them consistent in the naming.\r\n\r\n**Will this change the current api? How?**\r\nYes.\r\n\r\n**Who will benefit with this feature?**\r\nUsers of the two functions will be less confused.\r\n\r\n**Any Other info.**\r\nA follow up of #7086", "comments": ["Created a PR #26374 for the fix. Will let api review to decide if it is ok.", "```python\r\ndef sigmoid_cross_entropy_with_logits_v2(  # pylint: disable=invalid-name\r\n    labels=None,\r\n    logits=None,\r\n    name=None):\r\n```\r\n\r\n```python\r\ndef weighted_cross_entropy_with_logits(targets, logits, pos_weight, name=None):\r\n```\r\n\r\n```python\r\n@tf_export(\"nn.sparse_softmax_cross_entropy_with_logits\")\r\ndef sparse_softmax_cross_entropy_with_logits(\r\n    _sentinel=None,  # pylint: disable=invalid-name\r\n    labels=None,\r\n    logits=None,\r\n    name=None):\r\n```\r\n\r\nThere are still inconsistencies (targets vs labels; _sentinel in `sparse_softmax_cross_entropy_with_logits`).", "Added another PR #26380 for the remaining `sparse_softmax_cross_entropy_with_logits` and `weighted_cross_entropy_with_logits` issue.", "Thanks to both of you! \r\nAnother small issue from @alextp's CL: after removing `_sentinel`, we should also remove the None default `labels=None, logits=None` from the signature. These meaningless defaults exist only because of sentinel."]}, {"number": 26336, "title": " Disable NNAPI api for Raspberry Pi", "body": "As there is a [build error](https://github.com/tensorflow/tensorflow/issues/25120#issue-402094367) on Raspberry Pi", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26336) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26336) for more info**.\n\n<!-- ok -->", "@xinpingwang, could you also please address the same issue in `build_aarch64_lib.sh`? Here's how it could be addressed: define something like `generic-aarch64` target type and handle it similarly in the makefile. Then switch the makefile to use this target. I can test it for you if you don't have aarch64 hardware to test on.", "@1e100 Thanks for your suggestion, I create a new [commit](https://github.com/tensorflow/tensorflow/pull/26336/commits/5702bb9fd7b6696cf55e7301f9ef9d6b6926c998), but I do not have  `aarch64` devices to test, hope it works fine.", "I'll test it for you and report back.", "Just remove `generic_aarch64` and disable NNAPI for `aarch64`", "I was able to build the following from this PR without issues:\r\n\r\n 1. aarch64 cross build from amd64\r\n 2. aarch64 native build directly on Odroid-C2\r\n 3. armhf cross build from amd64\r\n\r\nI did not try armhf build directly on armhf device since that takes quite a long time. Sorry for notification spam, I was inadvertently posting from a wrong account.", "Ok, thank you", "Yes, the [force pushed](https://github.com/tensorflow/tensorflow/compare/959906c88082a5f1e91eba6929307aa2f7b62f6b..5702bb9fd7b6696cf55e7301f9ef9d6b6926c998) is the rebased version", "@xinpingwang there are still conflicts , can you please rebase again "]}, {"number": 26335, "title": "error building on Windows 10: Python Configuration Error: Problem getting numpy include path.", "body": "\r\n- Windows 10\r\n- TensorFlow installed from source following these instructions: https://www.tensorflow.org/install/source_windows\r\n- TensorFlow version:\r\n- Python version: 3.7 installed with Anaconda, in ../Anaconda3/python\r\n- Bazel version (if compiling from source): 0.23.0\r\n- CUDA/cuDNN version: 10.1/7\r\n- GPU model and memory: Geoforce GTX 1080Ti\r\n\r\nRunning this command: \r\n\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\nc:\\Users\\Usertytell_admin\\tensorflow>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nDEBUG: C:/users/usertytell_admin/_bazel_tytell_admin/oimkit2z/external/build_bazel_rules_apple/apple/repositories.bzl:35:5:\r\nWARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.\r\n\r\nERROR: C:/users/usertytell_admin/tensorflow/third_party/py/numpy/BUILD:11:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n        File \"C:/users/usertytell_admin/tensorflow/third_party/py/python_configure.bzl\", line 344\r\n                _create_local_python_repository(repository_ctx)\r\n        File \"C:/users/usertytell_admin/tensorflow/third_party/py/python_configure.bzl\", line 296, in _create_local_python_repository\r\n                _get_numpy_include(repository_ctx, python_bin)\r\n        File \"C:/users/usertytell_admin/tensorflow/third_party/py/python_configure.bzl\", line 276, in _get_numpy_include\r\n                _execute(repository_ctx, [python_bin, \"-c\",...\"], <2 more arguments>)\r\n        File \"C:/users/usertytell_admin/tensorflow/third_party/py/python_configure.bzl\", line 56, in _execute\r\n                _fail(\"\\n\".join([error_msg.strip() if ... \"\"]))\r\n        File \"C:/users/usertytell_admin/tensorflow/third_party/py/python_configure.bzl\", line 27, in _fail\r\n                fail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: Problem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\Usertytell_admin\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py\", line 140, in <module>\r\n    from . import _distributor_init\r\n  File \"C:\\Users\\Usertytell_admin\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py\", line 34, in <module>\r\n    from . import _mklinit\r\nImportError: DLL load failed: The specified module could not be found.\r\nIs numpy installed?\r\n and referenced by '//third_party/py/numpy:headers'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 14.234s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (235 packages loaded, 3598 targets configured)\r\n    currently loading: tensorflow/lite/kernels\r\n    Fetching @eigen_archive; Restarting. 4s\r\n    Fetching @local_config_python; fetching\r\n    Fetching @highwayhash; fetching\r\n\r\n\r\n\r\n", "comments": ["@effiek9999 What is the version number of tensorflow and numpy? Can you check the requirement of TF1.13.1 [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py). TF1.13.1 was built against CUDA10. If you want to use CUDA 10.1, use latest version of TF and check whether CUDA and cuDNN are referencing correct paths. Best things is to install CUDA10 drivers. First, uninstall python and tensorflow and reinstall following the instructions [here](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12/blob/master/README.md) but use new version of CUDA and cuDNN. \r\n\r\nAs another user suggested [here](https://github.com/tensorflow/tensorflow/issues/26182#issuecomment-468882301), you can do\r\nconda install cudatoolkit\r\nconda install cudnn\r\n\r\nPlease let me know how it progresses. Thanks!", "My numpy version is 1.16.2, and I am getting TensorFlow directly from the github master branch to do a build according to these instructions: https://www.tensorflow.org/install/source_windows. I will try these suggestions, thank you.", "@jvishnuvardhan \r\nNo one responded to my issue since you assigned someone else to issue.\r\nCould you please take a look into it?\r\nThis is the link to my issue: [https://github.com/tensorflow/tensorflow/issues/25597](url)", "I had the same problem until I downgraded my Bazel to version 0.20.", "@effiek9999 Please check #26182 for any workarounds and solutions. Please close the ticker if your issue was resolved. Thanks!", "I think it was resolved. I am closing the issue. Please open a new ticket if you see similar issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26335\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26335\">No</a>\n"]}, {"number": 26334, "title": "Fix for #26294", "body": "", "comments": ["Will this work @dynamicwebpaige ", "It's been quite a while, I'd love to know if I should improve anything @rthadur @dynamicwebpaige ", "Looks like a duplicate of https://github.com/tensorflow/tensorflow/pull/26332. Since that came first, I'll close this one out.", "Hi @jhseu @kyscg , I am doing a research about \"how the labels such as 'good first issue' help newcomers to contribute\" , but I found many PR were rejected because of duplication, especially for popular projects. How do you think of this problem? I wonder whether mechanisms to avoid this problem are needed, for example, in addition to \"open\" and \"closed\", add a new status of issue: ongoing."]}, {"number": 26333, "title": "GPU Support build instructions suggest old Docker image", "body": "\r\n**System information**\r\n- Linux Ubuntu 18.04.2 LTS:\r\n- TensorFlow installed from Docker image: 1.13.1-gpu-py3-jupyter\r\n- TensorFlow version: Should be 1.13.1\r\n- Python version: From Docker, appears to be 3.5.2\r\n- Installed using pip: NA - don't get that far\r\n- Bazel version (if compiling from source): Missing in docker image?\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda-10 from Docker image\r\n- GPU model and memory:\r\n\r\nprocessor       : 11\r\nvendor_id       : GenuineIntel\r\ncpu family      : 6\r\nmodel           : 44\r\nmodel name      : Intel(R) Core(TM) i7 CPU       X 980  @ 3.33GHz\r\nstepping        : 2\r\nmicrocode       : 0x13\r\ncpu MHz         : 1630.792\r\ncache size      : 12288 KB\r\nphysical id     : 0\r\nsiblings        : 12\r\ncore id         : 10\r\ncpu cores       : 6\r\napicid          : 21\r\ninitial apicid  : 21\r\nfpu             : yes\r\nfpu_exception   : yes\r\ncpuid level     : 11\r\nwp              : yes\r\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt aes lahf_lm epb pti tpr_shadow vnmi flexpriority ept vpid dtherm ida arat\r\nbugs            : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf\r\nbogomips        : 6675.50\r\nclflush size    : 64\r\ncache_alignment : 64\r\naddress sizes   : 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nTHE PROBLEM: The Docker image tensorflow/tensorflow:1.13.1-gpu-py3-jupyter doesn't build as per the instructions here:\r\n\r\nhttps://www.tensorflow.org/install/source#gpu_support_2\r\n\r\nUsing the tensorflow/tensorflow:1.13.1-gpu-py3-jupyter Docker image the build fails at the first step. (./configure) There's nothing in the /tensorflow directory.\r\n\r\nUsing the tensorflow/tensorflow:nightly-devel-gpu-py3 Docker image everything works fine and after being built tensorflow can be imported in python within the Docker container. In this image there is source code in /tensorflow.\r\n\r\nALSO, in this Docker container /usr/local/lib has only python3.5. In the nightly-devel-gpu-py image there is a bazel directory\r\n\r\n\r\n", "comments": ["Hmm... those docs may be out of date; `nightly-devel-gpu-py3` is a legacy Docker image. `devel-gpu-py3` is the newer equivalent with `/tensorflow_src`. [Our Docker Hub page](https://hub.docker.com/r/tensorflow/tensorflow/) may help explain our available tags.\r\n\r\nPing @lamberta", "OK, devel-gpu-py3 appears to have all the pieces I was looking for. I've pulled a copy of the image and have started a build using the commands on the old possibly out-of-date page. I'll report back tomorrow as to the results.\r\n\r\nThanks for the help and I need to learn about this ping thing going on here! :-)", "devel-gpu-py3 appears to be Ubuntu 16.04 LTS, I'm 18.04 LTS which is python-3.6. After installing python-3.5 on my system I am able to run tensorflow in a virtual environment created using python-3.5 and it see my GTX 960 so it's a big win. Thanks! However is there a 18.04/python-3.6 Docker image to make it a more direct install on Ubuntu 18.04 LTS?", "The images are all based on Ubuntu 16.04 (some of the older images were updated to 18.04, but are no longer used) -- we just haven't had the time to update them (you can find the infrastructure in tensorflow/tools/dockerfiles, if you are interested in updating them). The 16.04-based images should still work for you if you don't mind using the older Python version.", "Hi. Can someone update these files with the correct info? Thanks\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/install/docker.md\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/install/source.md", "tensorflow:1.13.1-gpu-py3-jupyter has cudnn? \r\nHow to view\uff1f", "This issue has been resolved now that the docs have been updated.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26333\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26333\">No</a>\n"]}, {"number": 26332, "title": "using ipython to convert ipython magic issue: #26294", "body": "hey will this work for issue #26294?", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26332) for more info**.\n\n<!-- need_sender_cla -->", "I've signed it\n\nOn Tue, Mar 5, 2019 at 12:03 AM googlebot <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here (e.g. I\n> signed it!) and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google.com/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2139\ufe0f *Googlers: Go here\n> <https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26332>\n> for more info*.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/26332#issuecomment-469438713>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEKpMsV_G6QoIojVHpsfbypX8_HqmYjsks5vTZhHgaJpZM4bdS1d>\n> .\n>\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26332) for more info**.\n\n<!-- ok -->", "Looks good to me. @lc0 is there enough test coverage to be comfortable after tests pass or do we need to test this manually? ", "The original issue is 3 month old. It should be working fine with a current version.\r\nDoes anybody has an example of test that fails with nightly builds? @NikolaMandic \r\n\r\nOtherwise, we are adding additional code for something, that already works - more opportunities to break things :( \r\n\r\ncc @martinwicke ", "Also, unfortunately my changes I tested with a set of test notebooks, but these never made to the repo. Was also not completely sure where to store the fixtures.\r\n\r\nIf you point me out what is the preferred way to add such tests, I would add my old test cases.", "I would simply add a testdata folder right there, containing your test notebooks (which are hopefully very small). And then add a test that runs the converter on them. ", "@lc0 I just tested it manually the original issue was to convert magics in notebooks so it did not work since there was complaint about it", "> @lc0 I just tested it manually the original issue was to convert magics in notebooks so it did not work since there was complaint about it\r\n\r\n@NikolaMandic do you have an example, when it fails with current nightly? \r\n\r\n@martinwicke I am going to add these test files over the weekend", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26332) for more info**.\n\n<!-- need_author_cla -->", "@lc0 hm I just saw test that failed lets see if it will pass after correction I pushed now", "@NikolaMandic one more time, would you mind to provide an example where you magic does not work? ", "> I would simply add a testdata folder right there, containing your test notebooks (which are hopefully very small). And then add a test that runs the converter on them.\r\n\r\n@martinwicke, sorry my weekend was packed. I am going to add my test noebooks over the weekend so we have a bit better setup.\r\n\r\nWith a proposed change itself, I am still not sure what case we are trying to solve. And currently it looks like we do not have dependency, so we check `globals()` and at the end of the day do nothing, since it's not imported", "@lc0 I understood it as someone in tensorflow team wrote an actual parser to convert python files and issue with it was that it could not handle magics so this converts magics to python code before feeding that parser. This is original issue as I saw it.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26332) for more info**.\n\n<!-- ok -->", "@alextp the issue with magic was handled way before this. So far I can not see an example to reproduce a bug.\r\n\r\nBy merging this I am not sure what we are fixing.", "Additionally I do not see `Ipython` as a dependency, so it feels a bit of like a dead code, that never gets executed. Correct me if I am wrong cc @alextp ", "You're right, let's just close this.", "Do we need to remove `ready to pull` tag, or would copybara pick it up since it has a tag even when the PR is closed? cc @alextp "]}, {"number": 26331, "title": "tf.gradients returns incorrect results if used multiple times on CudnnLSTM", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):1.12.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): unknown\r\n- GCC/Compiler version (if compiling from source): unknown\r\n- CUDA/cuDNN version:  10.0 / 7.4\r\n- GPU model and memory: TI 1080, 12GB RAM\r\n\r\nI found some cases where fetching multiple gradient computations for the parameters of a CudnnLSTM seems to result in incorrect results. For example, the following code:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn import CudnnLSTM\r\n\r\nx = tf.fill((2, 1, 5), 0.5)\r\nlayer = CudnnLSTM(1, 5)\r\nlayer.build(x.shape)\r\noutput = layer.apply(x)[0]\r\n\r\nsess = tf.Session()\r\n\r\nloss = tf.reduce_sum(output)\r\nvars = tf.trainable_variables()\r\ngrad1 = tf.gradients(loss, vars)[0]\r\ngrad2 = tf.gradients(loss, vars)[0]\r\n\r\nsess.run(tf.global_variables_initializer())\r\ng1, g2 = sess.run([grad1, grad2])\r\nprint(g1[:10])\r\nprint(g2[:10])\r\nprint((g1 - g2)[:10])\r\n```\r\nShows the two gradient computation are far from equal, even though they are requesting exactly the same value. On my machine I saw:\r\n\r\n```\r\n[ 0.0290555   0.0290555   0.0290555   0.0290555   0.0290555  -0.00758932\r\n -0.00758932 -0.00758932 -0.00758932 -0.00758932]\r\n[1.7652620e-04 1.7652620e-04 1.7652620e-04 1.7652620e-04 1.7652620e-04\r\n 7.4650106e-06 7.4650106e-06 7.4650106e-06 7.4650106e-06 7.4650106e-06]\r\n[ 0.02887898  0.02887898  0.02887898  0.02887898  0.02887898 -0.00759678\r\n -0.00759678 -0.00759678 -0.00759678 -0.00759678]\r\n```\r\n\r\nwhich seems far too large for the difference to be a rounding issue.\r\n\r\nUsing `g1, g2 = [sess.run(grad1), sess.run(grad2)]` will fix the issue.\r\n\r\nThe issue also goes away if we use  `grads = sess.run([grad1, grad1])`, but seems to remain for other, non-trivial cases. For example, if we had done `grad2 = tf.gradients(loss*2, vars)[0]` `g2` should be twice as large as `g1`, but it will be way off.", "comments": ["Adding @protoget who previously worked on cudnn kernels.", "@chrisc36,\r\nSorry for the delayed response. As **`CudnnLSTM layer`** has been depcreated in **`Tensorflow 2.x`**, if possible, can you please try with [RNN or LSTM or GRU with CuDNN Kernels](https://www.tensorflow.org/guide/keras/rnn#performance_optimization_and_cudnn_kernels) and let us know if this issue still persists? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26331\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26331\">No</a>\n"]}, {"number": 26330, "title": "Disable  backwards_compatibility_test in r2.0 release branch.", "body": "", "comments": ["Not disabling the test."]}, {"number": 26329, "title": "Fix for -latomic flag not working for Mac OSX Mojave builds", "body": "This two line fix moves -latomic to be Android specific, allowing for libtensorflowlite.so to build for MacOS X 10.14.3, since it doesn't even need JNI.", "comments": ["Saw that this got fixed, closing the PR."]}, {"number": 26328, "title": "Added Median Filtering Functionality  1D", "body": "This is my test code. Its working fine at my end. \r\n\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot\r\nimport matplotlib.pyplot as plt\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.util.tf_export import tf_export\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import image_ops_impl\r\nfrom tensorflow.python.ops import script_ops\r\n\r\nfname = 'add noise  image.png'\r\nimg = matplotlib.pyplot.imread(fname)\r\nimport numpy as np\r\n\r\ntf_img = tf.convert_to_tensor(img)\r\n@tf_export('image.median_filter_1D')\r\ndef median_filter_1D(input,filter_shape=3):\r\n    \"\"\"  This methods takes 3D Tensor Images.\r\n         Other than Tensor it takes optional parameter filter_Size\r\n         Default Filter Shape = 3\r\n         This Median Filtering is done by using 1D filters of user's choice\r\n         Filter_size should be odd\r\n         This method takes both kind of images where pixel values lie between 0 to 255 and where it lies between 0.0 and 1.0\r\n    \"\"\"\r\n\r\n    input = image_ops_impl._Assert3DImage(input)\r\n    m,no,ch = int(input.shape[0]),int(input.shape[1]),int(input.shape[2])\r\n    filter_shapex = filter_shape\r\n    if m * no < filter_shapex :\r\n        raise ValueError(\"No of Pixels in each dimension of the image should be more than the filter size. Got filter_shape \"\r\n                         \"(%s)\"% filter_shape+\" Image Shape (%s)\"% input.shape)\r\n    if filter_shapex % 2 == 0 :\r\n        raise ValueError(\"Filter size should be odd. Got filter_shape (%s)\" % filter_shape )\r\n    input = math_ops.cast(input,dtypes.float64)\r\n    def my_func (input2):\r\n        tf_i = input2.reshape(m*no*ch)\r\n        maxi = max(tf_i)\r\n        if maxi == 1:\r\n            input2 /= maxi\r\n        else :\r\n            input2 /= 255\r\n        #k and l is the Zero-padding size\r\n        res = np.empty((m,no,ch))\r\n        for a in range(ch):\r\n            img = input2[:,:,a:a+1]\r\n            img = img.reshape(m * no)\r\n            k = filter_shapex - 1\r\n            img  = np.pad(img,((k / 2, k / 2)),'constant', constant_values=(0))\r\n            res1 = np.empty((m*no))\r\n            for i in range(img.shape[0] - k):\r\n                li = []\r\n                for b in range(i, i + filter_shapex):\r\n                    li.append(img[b])\r\n                li.sort()\r\n                res1[i] = li[len(li) / 2]\r\n            res1 = res1.reshape(m,no,1)\r\n            res[:,:,a:a+1] = res1\r\n        res *= 255\r\n        res = res.astype('int64')\r\n        return res\r\n\r\n    y = script_ops.py_func(my_func, [input], dtypes.int64)\r\n    return y\r\n\r\nsess = tf.InteractiveSession()\r\n\r\nmimage = median_filter_1D(tf_img,5)\r\n\r\nfig = plt.figure()\r\nfig.add_subplot()\r\nplt.imshow(img,cmap='gray')\r\nplt.show()\r\nmimage = mimage.eval()\r\nfig.add_subplot()\r\nif mimage.shape[2] == 1:\r\n    mimage = mimage.reshape(mimage.shape[0],mimage.shape[1])\r\nplt.imshow(mimage,cmap = 'gray')\r\nplt.show()", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26328) for more info**.\n\n<!-- need_sender_cla -->", "I have signed.\n\nOn Tue 5 Mar, 2019, 2:26 AM googlebot, <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here (e.g. I\n> signed it!) and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google.com/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2139\ufe0f *Googlers: Go here\n> <https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26328>\n> for more info*.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/26328#issuecomment-469416999>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AiI2QpZneb8kKYv8HjUjcwgGp7Mdl_YTks5vTYhxgaJpZM4bdLuM>\n> .\n>\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26328) for more info**.\n\n<!-- ok -->"]}, {"number": 26327, "title": "Merge pull request #26326 from tensorflow/2.0-ff", "body": "2.0 ff , Move r2.0 branch ahaed to pick up test and build fixes.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26327) for more info**.\n\n<!-- need_sender_cla -->", "@shalevy1 please sign CLA", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26327) for more info**.\n\n<!-- cla_yes -->", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26327) for more info**.\n\n<!-- need_sender_cla -->", "closing this issue as this has more merge conflicts, please reopen with new PR"]}, {"number": 26326, "title": "2.0 ff , Move r2.0 branch ahaed to pick up test and build fixes.", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26326) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26326) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 26325, "title": "[TF 2.0] Allow declaring dynamic tf.Variable shapes", "body": "In the current TensorFlow 2.0 beta, `tf.Variables` inherits its shape from the initial value that it's provided:\r\n\r\n```python\r\n>>> tf.Variable(tf.zeros([16, 32])).shape\r\nTensorShape([16, 32])\r\n```\r\n\r\nBut for some use-cases, I'd like to explicitly make the Variable have a dynamic (`None`) shape:\r\n\r\n```python\r\ntf.Variable(tf.zeros([16, 32]), shape=[None, 32])\r\n```\r\n\r\nIn the current beta, you can kind of do this via;\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass HasDynamicVariable(tf.Module):\r\n\r\n  def __init__(self):\r\n    super(HasDynamicVariable, self).__init__()\r\n    self.v = None\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec([None, 2], tf.float32)])\r\n  def make_dynamic_variable(self, initial_value):\r\n    if self.v is None:\r\n      self.v = tf.Variable(initial_value)\r\n    return self.v\r\n```\r\n(xref https://github.com/tensorflow/community/pull/34#issuecomment-467690616)\r\n\r\nBut allowing you to explicitly annotate `tf.Variable(..., shape=...)` is IMO a lot cleaner and easier to use than having to declare it via the input signature.\r\n\r\n(My particular use-case is to have something like a `tf.keras.layers.LSTM` with a dynamic batch size -- that means I'd like to do something like:\r\n\r\n```\r\nlstm = ...\r\nlstm.reset_state(batch_size=5)\r\nlstm(batch1)\r\nlstm.reset_state(batch_size=20)\r\nlstm(batch2)\r\n```\r\n)", "comments": ["Linking similar issue #26104", "Fixed by https://github.com/tensorflow/tensorflow/commit/f9f2547b085969c67dc140aaa230551b6471a956", "does release less than 2.0 has this feature\uff1f", "It's available since 1.14"]}, {"number": 26324, "title": "tf.image.grayscale_to_rgb is not taking a grayscale image going into bugs ", "body": "*System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip installed\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 2.7\r\n\r\nCode- \r\nimport tensorflow as tf\r\nimport matplotlib.pyplot\r\nimport matplotlib.pyplot as plt\r\n\r\nfname = '/media/titan/ACER DATA/Mainak/ytfvideo/resized/Aaron_Eckhartimage_00001.jpg'\r\nimg = matplotlib.pyplot.imread(fname)\r\n\r\ntf_img = tf.convert_to_tensor(img)\r\ntf_img = tf.image.grayscale_to_rgb(tf_img)\r\nbrght_img = tf.image.flip_left_right(tf_img)\r\nplt.imshow(img)\r\nplt.show()\r\n**Aaron_Eckhartimage_00001.jpg is a grayscale image\r\nError-\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 9, in <module>\r\n    tf_img = tf.image.grayscale_to_rgb(tf_img)\r\n  File \"/home/titan/Documents/cpuenv/local/lib/python2.7/site-packages/tensorflow/python/ops/image_ops_impl.py\", line 1530, in grayscale_to_rgb\r\n    rgb.set_shape(images.get_shape()[:-1].concatenate([3]))\r\n  File \"/home/titan/Documents/cpuenv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 561, in set_shape\r\n    raise ValueError(str(e))\r\nValueError: Dimension 1 in both shapes must be equal, but are 120 and 3. Shapes are [40,120] and [40,3].\r\n\r\n", "comments": ["You see this exception because your grayscale image does not have a valid shape. Inside tensorflow a grayscale image is defined as a `3D` tensor whose last dimension is size 1. This is different from `matplotlibs` definition. Therefore, you can fix this be using:\r\n    \r\n    tf_img = tf.convert_to_tensor(img.reshape((*img.shape, 1))\r\n\r\nI created a [pull request](https://github.com/tensorflow/tensorflow/pull/26529) to give users a better feedback _why_ the operation fails if you try using it on an input with invalid shapes.", "@FlashTek  Or we can update the function code to work both on 3D and 2D tensor. It will be a one line change. If you want i can do it separately in a pull request. "]}, {"number": 26323, "title": "Pruning example, ValueError: Could not find a checkpoint at: .", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No I have not\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Antergos Linux (Gnome Version 3.30.2)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: b'v1.13.1-0-g6612da8951' 1.13.1\r\n- **Python version**: 3.6.0\r\n- **Bazel version (if compiling from source)**: 0.22.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc (GCC) 8.2.1 20181127\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: \r\n``\r\n$ examples_dir=contrib/model_pruning/examples\r\n$ bazel build -c opt $examples_dir/cifar10:cifar10_{train,eval}\r\n$ bazel-bin/$examples_dir/cifar10/cifar10_train --pruning_hparams=name=cifar10_pruning,begin_pruning_step=10000,end_pruning_step=100000,target_sparsity=0.9,sparsity_function_begin_step=10000,sparsity_function_end_step=100000\r\n$ bazel build -c opt contrib/model_pruning:strip_pruning_vars\r\n$ bazel-bin/contrib/model_pruning/strip_pruning_vars --checkpoint_path=/tmp/cifar10_train --output_node_names=softmax_linear/softmax_linear_2 --filename=cifar_pruned.pb\r\n``\r\n\r\n### Describe the problem\r\nWhen I run the commands above, it gets to train the model and everything, but fails when it tries to strip the pruning vars. Traceback is:\r\n``\r\n$ bazel-bin/tensorflow/contrib/model_pruning/strip_pruning_vars --checkpoint_path=/tmp/cifar10_train --output_node_names=softmax_linear/softmax_linear_2 --filename=cifar_pruned.pb\r\nTraceback (most recent call last):\r\n  File \"/home/cenh/tensorflow/bazel-bin/tensorflow/contrib/model_pruning/strip_pruning_vars.runfiles/org_tensorflow/tensorflow/contrib/model_pruning/python/strip_pruning_vars.py\", line 103, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/cenh/tensorflow/bazel-bin/tensorflow/contrib/model_pruning/strip_pruning_vars.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/cenh/tensorflow/bazel-bin/tensorflow/contrib/model_pruning/strip_pruning_vars.runfiles/absl_py/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/cenh/tensorflow/bazel-bin/tensorflow/contrib/model_pruning/strip_pruning_vars.runfiles/absl_py/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/cenh/tensorflow/bazel-bin/tensorflow/contrib/model_pruning/strip_pruning_vars.runfiles/org_tensorflow/tensorflow/contrib/model_pruning/python/strip_pruning_vars.py\", line 78, in main\r\n    FLAGS.output_dir, FLAGS.filename)\r\n  File \"/home/cenh/tensorflow/bazel-bin/tensorflow/contrib/model_pruning/strip_pruning_vars.runfiles/org_tensorflow/tensorflow/contrib/model_pruning/python/strip_pruning_vars.py\", line 67, in strip_pruning_vars\r\n    checkpoint_dir, output_node_names)\r\n  File \"/home/cenh/tensorflow/bazel-bin/tensorflow/contrib/model_pruning/strip_pruning_vars.runfiles/org_tensorflow/tensorflow/contrib/model_pruning/python/strip_pruning_vars_lib.py\", line 132, in graph_def_from_checkpoint\r\n    .format(checkpoint_dir))\r\nValueError: Could not find a checkpoint at: .\r\n``\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["`tf.contrib` module is deprecated. \r\nPlease try latest pruning example from tensorflow model optimization tooklit.\r\nhttps://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras "]}, {"number": 26322, "title": "[Bug]: tf.keras update_op for computing running mean and variance for BatchNorm causes an error", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: CUDA 9.0\r\n- GPU model and memory: TITAN X (Pascal) \r\n\r\nI am using tf.keras API to build a ConvNet and I train it with the Tensorflow custom training loop, using graph API. The ConvNet contains BatchNorm layers and as I don't use model.fit() for training, I have to manage the updates to the moving mean and variance manually. However this causes an error. Here is a minimal reproducible case:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.keras import layers\r\nfrom tensorflow.python.keras import initializers\r\nfrom tensorflow.python.keras import models\r\n\r\ntfsum = tf.contrib.summary\r\n\r\nHEIGHT = 480\r\nWIDTH = 640\r\n\r\ndef conv(x, num_out_layers, kernel_size, stride, activation_fn=\"relu\"):\r\n    kernel_initializer = initializers.VarianceScaling()\r\n    x = layers.Conv2D(num_out_layers,\r\n                      (kernel_size, kernel_size),\r\n                      strides=stride,\r\n                      padding=\"SAME\",\r\n                      activation=None,\r\n                      kernel_initializer=kernel_initializer)(x)\r\n\r\n    x = layers.BatchNormalization()(x)\r\n    x = layers.Activation(activation_fn)(x)\r\n    return x\r\n\r\n\r\ndef build_cnn(img_shape):\r\n    inputs = tf.keras.layers.Input(img_shape)\r\n\r\n    conv1 = conv(inputs, 64, 7, 1)\r\n    outputs = conv(conv1, 3, 7, 1)\r\n\r\n    model = models.Model(inputs=[inputs], outputs=[outputs])\r\n    return model\r\n\r\n\r\ndef main_graph():\r\n    learning_rate = 0.001\r\n    batch_size = 1\r\n    num_steps = 100\r\n    train_dir = \".\"\r\n    dataset_size = 20\r\n\r\n    images = np.random.random_sample([dataset_size, HEIGHT, WIDTH, 3]).astype(np.float32)\r\n    dataset = tf.data.Dataset.from_tensor_slices(images)\r\n    dataset = dataset.repeat() \\\r\n                     .batch(batch_size)\r\n    iterator = dataset.make_one_shot_iterator()\r\n    image = iterator.get_next()\r\n    print(image)\r\n\r\n    global_step = tf.train.get_or_create_global_step()\r\n    summary_writer = tfsum.create_file_writer(\r\n        train_dir, flush_millis=10000)\r\n    with summary_writer.as_default(), tfsum.record_summaries_every_n_global_steps(10):\r\n        model = build_cnn([HEIGHT, WIDTH, 3])\r\n        prediction = model(image, training=True)\r\n        update_ops = model.updates\r\n        train_var_list = model.trainable_variables\r\n\r\n        prediction = tf.ensure_shape(prediction, [batch_size, HEIGHT, WIDTH, 3])\r\n\r\n        loss = tf.reduce_mean(tf.abs(prediction-image))\r\n\r\n        tf.contrib.summary.scalar(\"loss\", loss)\r\n\r\n        optimizer = tf.train.AdamOptimizer(learning_rate)\r\n        with tf.control_dependencies(update_ops):\r\n            train_op = optimizer.minimize(loss, global_step=global_step, var_list=train_var_list)\r\n\r\n    sess = tf.Session()\r\n    with sess, summary_writer.as_default():\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(tf.local_variables_initializer())\r\n        tf.contrib.summary.initialize(graph=tf.get_default_graph())\r\n\r\n        for i in range(num_steps+1):\r\n            _, global_step_val, loss_val, _, = sess.run([train_op, global_step, loss,\r\n                                                        tfsum.all_summary_ops()])\r\n            print(f\"iter:{global_step_val:06d} loss: {loss_val:04.4f}\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main_graph()\r\n```\r\n\r\nThis results in the following error message:\r\n\r\n```\r\nCaused by op 'input_1', defined at:\r\n  File \"../../train_bug.py\", line 104, in <module>\r\n    main_graph()\r\n  File \"../../train_bug.py\", line 72, in main_graph\r\n    model = build_cnn([HEIGHT, WIDTH, 3])\r\n  File \"../../train_bug.py\", line 44, in build_cnn\r\n    inputs = tf.keras.layers.Input(img_shape)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_layer.py\", line 229, in Input\r\n    input_tensor=tensor)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_layer.py\", line 112, in __init__\r\n    name=self.name)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1747, in placeholder\r\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5206, in placeholder\r\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,480,640,3]\r\n         [[node input_1 (defined at ../../train_bug.py:44)  = Placeholder[dtype=DT_FLOAT, shape=[?,480,640,3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n```\r\n\r\nRemoving `with tf.control_dependencies(update_ops)` makes it run, but then the batchnorm statistics are not being updated. How can I use Keras in this setting?", "comments": ["The problem is very much related to #23873", "@karmel can you comment or redirect as necessary? Thanks.", "@eldar please replace `model.updates` with `model.get_updates_for(image)`. \r\n\r\n`model.updates` includes all updates, including the ones created when constructing your model off of the `keras.Input`s"]}, {"number": 26320, "title": "Used StartForAxis and StopForAxis instead", "body": "Removed the unnecessary duplicate function.", "comments": ["@jdduke , thanks for pointing that  out i have updated the code .It now reserves the memory in advance so that heap allocation in every invocation does not happen. Kindly check and approve.", "Can one of the admins verify this patch?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 26319, "title": "Training parameter in Keras models passed as None in 1.13", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary(pip)\r\n- TensorFlow version (use command below): v1.13.1-0-g6612da8951 1.13.1\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 10, cuDNN 7.5\r\n- GPU model and memory: GTX 1050ti, 4GB\r\n\r\n**Describe the current behavior**\r\nAs described in the third example in [the documentation for Keras models](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#class_model), a boolean `training` parameter can be used in the `call` method of subclassed models. However, the parameter is passed as `None` when it should be `True`. \r\n\r\n**Describe the expected behavior**\r\nThe training parameter should be passed as True when the model is training.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nclass MyModel(tf.keras.Model):\r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n    self.dense = tf.keras.layers.Dense(4)\r\n\r\n  def call(self, inputs, training=False):\r\n    print('Training', training)\r\n    return self.dense(inputs)\r\n    \r\nmodel = MyModel()\r\nmodel.compile(optimizer=tf.train.AdagradOptimizer(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\r\n\r\ninp = np.ones((5, 3), dtype=np.float32)\r\nout = np.ones((5, 4), dtype=np.float32)\r\n\r\n# training should be False\r\nmodel(inp)\r\n\r\n# training should be True\r\nmodel.fit(inp, out)\r\n```\r\n\r\n**Other info / logs**\r\nThis only happens in TF 1.13 and not in 1.12. I also tried 2.0 alpha and the bug is still present", "comments": ["Hi @pavithrasv, has there been any progress in regard to this?", "@janhartman I was able to reproduce this issue. Will look into the fix. ", "@janhartman I think this was resolved in the latest TF versions (`TF1.14.0` and `tf-nightly`). I cannot reproduce the issue. Please check the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/4c95c00604552800c6201c651dd758c8/untitled428.ipynb) with `TF1.14.0`.\r\n\r\nAutomatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26319\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26319\">No</a>\n", "@jvishnuvardhan It's not completely solved. The parameter is now a bool tensor instead of a plain bool, but it's better than before. Thanks!"]}]