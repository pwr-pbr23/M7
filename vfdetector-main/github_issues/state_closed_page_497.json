[{"number": 38867, "title": "issue converting a model defined and compiled with Keras into tflite", "body": "**System information**\r\n- OS Platform and Distribution Windows 10 (10.0.18363 N/A Build 18363)\r\n- TensorFlow installed from: binary (conda)\r\n- TensorFlow version: 2.1.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n2020-04-24 14:07:48.857192: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-24 14:07:53.751094: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-04-24 14:07:53.926489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:17:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-04-24 14:07:53.942398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties:\r\npciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-04-24 14:07:53.954174: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-24 14:07:53.963600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-04-24 14:07:53.970994: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-04-24 14:07:53.975195: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-04-24 14:07:53.981892: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-04-24 14:07:53.987511: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-04-24 14:07:53.997545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-24 14:07:54.002301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1\r\n2020-04-24 14:07:54.005077: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-04-24 14:07:54.268811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:17:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-04-24 14:07:54.283187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties:\r\npciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-04-24 14:07:54.295258: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-24 14:07:54.300726: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-04-24 14:07:54.304884: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-04-24 14:07:54.308414: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-04-24 14:07:54.312322: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-04-24 14:07:54.315485: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-04-24 14:07:54.318772: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-24 14:07:54.322343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1\r\n2020-04-24 14:07:55.346759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-24 14:07:55.353942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1\r\n2020-04-24 14:07:55.360645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N N\r\n2020-04-24 14:07:55.365830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   N N\r\n2020-04-24 14:07:55.371746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8779 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n2020-04-24 14:07:55.387124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8778 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n2020-04-24 14:08:10.543940: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\r\n2020-04-24 14:08:10.551744: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-04-24 14:08:10.558479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:17:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-04-24 14:08:10.572134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties:\r\npciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-04-24 14:08:10.582103: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-24 14:08:10.585591: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-04-24 14:08:10.589055: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-04-24 14:08:10.592316: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-04-24 14:08:10.599428: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-04-24 14:08:10.602331: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-04-24 14:08:10.605021: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-24 14:08:10.608479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1\r\n2020-04-24 14:08:10.611352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-24 14:08:10.614149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1\r\n2020-04-24 14:08:10.615877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N N\r\n2020-04-24 14:08:10.617604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   N N\r\n2020-04-24 14:08:10.619906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8779 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n2020-04-24 14:08:10.626586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8778 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n2020-04-24 14:08:10.712742: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-04-24 14:08:10.719884: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 425 nodes (377), 829 edges (780), time = 12.367ms.\r\n2020-04-24 14:08:10.729092: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 425 nodes (0), 829 edges (0), time = 4.555ms.\r\n2020-04-24 14:08:10.739369: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_while_cond_449002_1018\r\n2020-04-24 14:08:10.746241: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-04-24 14:08:10.753806: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-04-24 14:08:10.758528: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_while_body_449003_9686\r\n2020-04-24 14:08:10.763111: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-04-24 14:08:10.769022: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-04-24 14:08:10.772939: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_while_body_448562_6255\r\n2020-04-24 14:08:10.777007: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-04-24 14:08:10.782953: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-04-24 14:08:10.786952: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_while_cond_448561_10216\r\n2020-04-24 14:08:10.791234: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-04-24 14:08:10.795335: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-04-24 14:08:19.431007: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\r\n2020-04-24 14:08:19.439743: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-04-24 14:08:19.447312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:17:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-04-24 14:08:19.464579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties:\r\npciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-04-24 14:08:19.479745: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-24 14:08:19.486868: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-04-24 14:08:19.491302: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-04-24 14:08:19.495128: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-04-24 14:08:19.501121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-04-24 14:08:19.505040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-04-24 14:08:19.508921: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-24 14:08:19.513748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1\r\n2020-04-24 14:08:19.519839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-24 14:08:19.524028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1\r\n2020-04-24 14:08:19.528399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N N\r\n2020-04-24 14:08:19.533433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   N N\r\n2020-04-24 14:08:19.537068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8779 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n2020-04-24 14:08:19.543946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8778 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n2020-04-24 14:08:31.567997: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-04-24 14:08:31.575109: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 294 nodes (-45), 587 edges (-84), time = 7337.03613ms.\r\n2020-04-24 14:08:31.586441: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 294 nodes (0), 587 edges (0), time = 2786.70288ms.\r\n2020-04-24 14:08:31.597388: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_while_cond_449002_1018_frozen\r\n2020-04-24 14:08:31.605102: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 1.129ms.\r\n2020-04-24 14:08:31.615276: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.159ms.\r\n2020-04-24 14:08:31.620189: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_while_body_448562_6255_frozen\r\n2020-04-24 14:08:31.628466: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 53 nodes (-1), 71 edges (0), time = 1.975ms.\r\n2020-04-24 14:08:31.632827: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 53 nodes (0), 71 edges (0), time = 0.657ms.\r\n2020-04-24 14:08:31.637156: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_while_cond_448561_10216_frozen\r\n2020-04-24 14:08:31.643592: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.932ms.\r\n2020-04-24 14:08:31.647864: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.154ms.\r\n2020-04-24 14:08:31.652114: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_while_body_449003_9686_frozen\r\n2020-04-24 14:08:31.657508: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 53 nodes (-1), 71 edges (0), time = 1.836ms.\r\n2020-04-24 14:08:31.661923: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 53 nodes (0), 71 edges (0), time = 0.594ms.\r\nTraceback (most recent call last):\r\n  File \"tflite_converter.py\", line 9, in <module>\r\n    liteModel = converter.convert()\r\n  File \"C:\\Users\\Eric\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow_core\\lite\\python\\lite.py\", line 464, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\Eric\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py\", line 457, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"C:\\Users\\Eric\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py\", line 203, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-04-24 14:12:51.450709: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-24 14:12:56.829530: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-04-24 14:12:56.838592: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-04-24 14:12:57.156479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:17:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-04-24 14:12:57.157270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties:\r\npciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-04-24 14:12:57.157878: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-24 14:12:57.161964: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-04-24 14:12:57.165048: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-04-24 14:12:57.166358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-04-24 14:12:57.169654: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-04-24 14:12:57.171887: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-04-24 14:12:57.177220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-24 14:12:57.178160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1\r\n2020-04-24 14:12:58.109700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-24 14:12:58.110013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1\r\n2020-04-24 14:12:58.110180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N N\r\n2020-04-24 14:12:58.110389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   N N\r\n2020-04-24 14:12:58.111497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8779 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n2020-04-24 14:12:58.113651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8778 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n2020-04-24 14:12:59.553038: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\r\n2020-04-24 14:12:59.553307: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-04-24 14:12:59.553612: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\r\n2020-04-24 14:12:59.553856: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-04-24 14:12:59.554132: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2020-04-24 14:12:59.554405: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-04-24 14:12:59.554652: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2020-04-24 14:12:59.554902: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-04-24 14:12:59.555149: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While\r\n2020-04-24 14:12:59.555403: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-04-24 14:12:59.555651: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-04-24 14:12:59.555922: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While\r\n2020-04-24 14:12:59.556169: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-04-24 14:12:59.556419: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-04-24 14:12:59.556686: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n2020-04-24 14:12:59.556952: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n2020-04-24 14:12:59.702664: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 119 operators, 258 arrays (0 quantized)\r\n2020-04-24 14:12:59.704661: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 119 operators, 258 arrays (0 quantized)\r\n2020-04-24 14:13:01.157666: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 61 operators, 174 arrays (0 quantized)\r\n2020-04-24 14:13:01.159147: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 56 operators, 168 arrays (0 quantized)\r\n2020-04-24 14:13:01.160554: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 53 operators, 162 arrays (0 quantized)\r\n2020-04-24 14:13:01.161927: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 53 operators, 162 arrays (0 quantized)\r\n2020-04-24 14:13:01.162962: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 53 operators, 162 arrays (0 quantized)\r\n2020-04-24 14:13:01.164011: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Identify nearest upsample.: 53 operators, 162 arrays (0 quantized)\r\n2020-04-24 14:13:01.165730: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 52352 bytes, theoretical optimal value: 49280 bytes.\r\n2020-04-24 14:13:01.166257: I tensorflow/lite/toco/toco_tooling.cc:471] Number of parameters: 133230663\r\n2020-04-24 14:13:01.167066: E tensorflow/lite/toco/toco_tooling.cc:498] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, DEPTHWISE_CONV_2D, FULLY_CONNECTED, PACK, RESHAPE, REVERSE_V2, SHAPE, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Eric\\anaconda3\\envs\\gpu\\Scripts\\toco_from_protos-script.py\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"C:\\Users\\Eric\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Users\\Eric\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\Eric\\anaconda3\\envs\\gpu\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\Eric\\anaconda3\\envs\\gpu\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"C:\\Users\\Eric\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, DEPTHWISE_CONV_2D, FULLY_CONNECTED, PACK, RESHAPE, REVERSE_V2, SHAPE, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\n\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf \r\nimport numpy as np\r\n\r\nINPATH = \"Path_to_saved_model\"\r\nOUTPATH = \"Path_to_lite_model\"\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(INPATH)\r\nliteModel = converter.convert()\r\n\r\nprint(\"saving converted model: \", OUTPATH)\r\nopen(OUTPATH, \"wb\").write(liteModel)\r\nprint(\"Completed\")\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_content=liteModel)\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test the TensorFlow Lite model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float16)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\nI am not exactly sure what that is, I hope thi is the output of the keras summary\r\n```\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninputs (InputLayer)             [(None, 250, 1)]     0                                            \r\n__________________________________________________________________________________________________\r\nbidirectional (Bidirectional)   (None, 250, 100)     20800       inputs[0][0]                     \r\n__________________________________________________________________________________________________\r\nconv1d (Conv1D)                 (None, 246, 25)      150         inputs[0][0]                     \r\n__________________________________________________________________________________________________\r\nflatten (Flatten)               (None, 25000)        0           bidirectional[0][0]              \r\n__________________________________________________________________________________________________\r\nflatten_1 (Flatten)             (None, 6150)         0           conv1d[0][0]                     \r\n__________________________________________________________________________________________________\r\nconcatenate (Concatenate)       (None, 31150)        0           flatten[0][0]                    \r\n                                                                 flatten_1[0][0]                  \r\n__________________________________________________________________________________________________\r\ndense (Dense)                   (None, 2048)         63797248    concatenate[0][0]                \r\n__________________________________________________________________________________________________\r\ndense_1 (Dense)                 (None, 1024)         2098176     dense[0][0]                      \r\n__________________________________________________________________________________________________\r\ndense_2 (Dense)                 (None, 512)          524800      dense_1[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_7 (Dense)                 (None, 2048)         63797248    concatenate[0][0]                \r\n__________________________________________________________________________________________________\r\ndense_3 (Dense)                 (None, 256)          131328      dense_2[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_8 (Dense)                 (None, 1024)         2098176     dense_7[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_4 (Dense)                 (None, 128)          32896       dense_3[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_9 (Dense)                 (None, 512)          524800      dense_8[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_5 (Dense)                 (None, 50)           6450        dense_4[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_10 (Dense)                (None, 256)          131328      dense_9[0][0]                    \r\n__________________________________________________________________________________________________\r\nreshape (Reshape)               (None, 50, 1)        0           dense_5[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_11 (Dense)                (None, 128)          32896       dense_10[0][0]                   \r\n__________________________________________________________________________________________________\r\ntime_distributed (TimeDistribut (None, 50, 1)        2           reshape[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_12 (Dense)                (None, 50)           6450        dense_11[0][0]                   \r\n__________________________________________________________________________________________________\r\nom (Reshape)                    (None, 50, 1)        0           time_distributed[0][0]           \r\n__________________________________________________________________________________________________\r\nreshape_1 (Reshape)             (None, 50, 1)        0           dense_12[0][0]                   \r\n__________________________________________________________________________________________________\r\nconcatenate_1 (Concatenate)     (None, 50, 2)        0           om[0][0]                         \r\n                                                                 reshape_1[0][0]                  \r\n__________________________________________________________________________________________________\r\nflatten_2 (Flatten)             (None, 100)          0           concatenate_1[0][0]              \r\n__________________________________________________________________________________________________\r\ndense_13 (Dense)                (None, 100)          10100       flatten_2[0][0]                  \r\n__________________________________________________________________________________________________\r\ndense_14 (Dense)                (None, 100)          10100       dense_13[0][0]                   \r\n__________________________________________________________________________________________________\r\ndense_15 (Dense)                (None, 50)           5050        dense_14[0][0]                   \r\n__________________________________________________________________________________________________\r\ndense_16 (Dense)                (None, 50)           2550        dense_15[0][0]                   \r\n__________________________________________________________________________________________________\r\nreshape_2 (Reshape)             (None, 50, 1)        0           dense_16[0][0]                   \r\n__________________________________________________________________________________________________\r\ntime_distributed_1 (TimeDistrib (None, 50, 1)        2           reshape_2[0][0]                  \r\n__________________________________________________________________________________________________\r\nof (Reshape)                    (None, 50, 1)        0           time_distributed_1[0][0]         \r\n```\r\n\r\n\r\n**Any other info / logs**\r\nI also tried setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter() \r\nby doing this:\r\n```python\r\ntf.lite.TFLiteConverter.target_ops = set([\"TFLITE_BUILTINS\",\"SELECT_TF_OPS\"])\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(INPATH)\r\n```\r\nbut I could not spot any difference.\r\n\r\nAny help would be much appreciated.\r\n\r\nSorry if the issue is illformed, I am new to the TF world, please let me know what additional info is needed.\r\n\r\n", "comments": ["@ericqu,\r\nI am unable to reproduce the issue as the input files are missing. Could you please share the saved model files stored in the `INPATH` path. Thanks!", "Well, I can zip the directory into a file of about 900Mb. However I don't know how to upload that; coudl you suggest a way to send that model to you?\r\n\r\n", "@amahendrakar \r\nInstead of sending you the model please find below the code compiling the model, then saving the model then converting the model. On my environment this create the same error as above. \r\nThis will create a directory with about 500Mb of data. \r\n\r\n```python\r\nimport gc\r\nimport sys\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow\r\nimport tensorflow.keras as keras\r\nfrom tensorflow.keras import metrics, regularizers\r\nfrom tensorflow.keras.layers import (GRU, LSTM, Activation, Bidirectional,\r\n                                     Conv1D, Dense, Dropout, Flatten, Input,\r\n                                     Reshape, SimpleRNN, TimeDistributed)\r\nfrom tensorflow.keras.models import Model\r\n\r\nprint(sys.version)\r\nprint(\"Tensor Flow:\",  tensorflow.__version__)\r\nprint(\"Keras: \", keras.__version__)\r\nprint(\"Numpy: \", np.__version__)\r\n\r\n\r\ndef define_model(length_of_sequences, batch_size=None, neurons=5, modelName=\"nonamegiven\"):\r\n    hidden_neurons = neurons\r\n\r\n    inp = Input(batch_shape=(\r\n        batch_size, length_of_sequences, 1), name=\"inputs\")\r\n    lstmM = Bidirectional(LSTM(50, name=\"lstm_m\", return_sequences=True))(inp)\r\n    flat = Flatten()(lstmM)\r\n    convM = Conv1D(25, 5, activation=\"relu\")(inp)\r\n    flatc = Flatten()(convM)\r\n    firstflat = tensorflow.keras.layers.concatenate([flat, flatc])\r\n    denseM = Dense(2048, kernel_regularizer=regularizers.l2(0.0001))(firstflat)\r\n    denseM = Dense(1024, kernel_regularizer=regularizers.l2(0.0001))(denseM)\r\n    denseM = Dense(512, kernel_regularizer=regularizers.l2(0.0001))(denseM)\r\n    denseM = Dense(256, kernel_regularizer=regularizers.l2(0.0001))(denseM)\r\n    denseM = Dense(128, kernel_regularizer=regularizers.l2(0.0001))(denseM)\r\n    denseM = Dense(50, kernel_regularizer=regularizers.l2(0.0001))(denseM)\r\n    reshapeM = Reshape((50, 1))(denseM)\r\n    denseM = TimeDistributed(\r\n        Dense(1, kernel_regularizer=regularizers.l2(\r\n            0.0001), bias_initializer='zeros'),\r\n        input_shape=(50, 1))(reshapeM)\r\n    out_M = Reshape((50, 1), name=\"om\")(denseM)\r\n    denseF = Dense(2048, kernel_regularizer=regularizers.l2(0.0001))(firstflat)\r\n    denseF = Dense(1024, kernel_regularizer=regularizers.l2(0.0001))(denseF)\r\n    denseF = Dense(512, kernel_regularizer=regularizers.l2(0.0001))(denseF)\r\n    denseF = Dense(256, kernel_regularizer=regularizers.l2(0.0001))(denseF)\r\n    denseF = Dense(128, kernel_regularizer=regularizers.l2(0.0001))(denseF)\r\n    denseF = Dense(50, kernel_regularizer=regularizers.l2(0.0001))(denseF)\r\n    denseF = Reshape((50, 1))(denseF)\r\n    merger = tensorflow.keras.layers.concatenate([out_M, denseF])\r\n    flatfi = Flatten()(merger)\r\n    denseF = Dense(100, kernel_regularizer=regularizers.l2(0.0001))(flatfi)\r\n    denseF = Dense(100, kernel_regularizer=regularizers.l2(0.0001))(denseF)\r\n    denseF = Dense(50, kernel_regularizer=regularizers.l2(0.0001))(denseF)\r\n    denseF = Dense(50, kernel_regularizer=regularizers.l2(\r\n        0.0001), bias_initializer='zeros')(denseF)\r\n    reshapeF = Reshape((50, 1))(denseF)\r\n    denseF = TimeDistributed(\r\n        Dense(1, kernel_regularizer=regularizers.l2(\r\n            0.0001), bias_initializer='zeros'),\r\n        input_shape=(50, 1))(reshapeF)\r\n    out_F = Reshape((50, 1), name=\"of\")(denseF)\r\n\r\n    model = Model(inputs=[inp], outputs=[out_M, out_F], name=modelName)\r\n\r\n    model.compile(\r\n        loss={\"om\": \"mean_squared_error\", \"of\": \"mean_squared_error\"},\r\n        optimizer=keras.optimizers.RMSprop(learning_rate=0.001),\r\n        metrics=['mae'])\r\n    return model\r\n\r\n\r\ntf_model = define_model(\r\n    length_of_sequences=250, neurons=5, modelName=\"TFliteSupport\")\r\ntf_model.summary()\r\nkeras.utils.plot_model(\r\n    tf_model, \"TFliteSupport_overview.png\", show_shapes=True)\r\n\r\ntf_model.save(\"TFliteSupport\")\r\n\r\nconverter = tensorflow.lite.TFLiteConverter.from_saved_model(\"TFliteSupport\")\r\nliteModel = converter.convert()\r\n\r\nprint(\"saving converted model...\")\r\nopen(\"TFliteSupport\", \"wb\").write(liteModel)\r\nprint(\"Completed\")\r\n\r\n```\r\n", "Hi @ericqu \r\nunfortunately, the TF ops you are using are not whitelisted from lite/delegates/flex/whitelisted_flex_ops.cc\r\nYou can find more information about using TF ops here https://www.tensorflow.org/lite/guide/ops_select\r\nWe are trying to add more op to the whitelist but it will take some time.\r\nIf you need it in urgent, i recommend to get the TF source, add those ops to the whitelist and build the lib and try it again.", "Hi @thaink \r\n\r\nThank you for your answer. looking a bit more in the error message:\r\n>  Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\n\r\nand in the source file you mentioned (whitelisted_flex_ops.cc) I note that there are whitelisted ops for TensorArray type of operations. Hence I am wondering if there is a way to reliably transform my model in order to use Array instead of List ?\r\n\r\nIf not then I will do as you suggested and follow the instructions in [Build TensorFlow Lite for ARM64 boards](https://www.tensorflow.org/lite/guide/build_arm64). but I feel that I should also build TF lite for windows (to do the conversion) is there a guide to do that?\r\nI am also wondering if there is any additional limitations that I should keep in mind as the converted model will eventually run on the [edgetpu](https://github.com/google-coral/edgetpu) (should I for instance make some modifications in that library as well to reflect the changes in operations?).\r\n", "@ericqu,\r\nI was able to run the code with TF v2.2.0-rc3 after making a few changes to the code.\r\n\r\nI had to comment out the line `keras.utils.plot_model(tf_model, \"TFliteSupport_overview.png\", show_shapes=True)`, as it was throwing an error\r\n\r\n\r\nand change `open(\"TFliteSupport\", \"wb\").write(liteModel)\r\n` to \r\n`open(\"TFliteSupport.tflite\", \"wb\").write(liteModel)\r\n`\r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/355bac906b4464d7de9511f3db7b7540/38867.ipynb). Thanks!", "Dear @amahendrakar , \r\n\r\nThank you for testing it on the upcoming version, the results looks indeed promising. \r\nDo you have an indicative view as by when the v2.2.0 will be release?\r\n\r\nIn parallel I am trying to follow the suggestion from @thaink however I did not reach a conclusive results as of yet. \r\n", "@ericqu we are trying to add more ops to the whitelist but it will take sometime. I think List-related op will work fine on mobile.", "Thank you @thaink and @amahendrakar for your support, much appreciated. \r\nI eventually managed to get it to work. \r\nI suppose I should close the issue. \r\n"]}, {"number": 38865, "title": "Error : tf.compat.v1.resource_loader.get_path_to_datafile", "body": "While implementing this code:\r\nimport tensorflow as tf\r\n\r\n_flow_warp_ops = tf.load_op_library(\r\n    tf.compat.v1.resource_loader.get_path_to_datafile(\"./lib/flow_warp.so\"))\r\n\r\n\r\ndef flow_warp(image, flow):\r\n    return _flow_warp_ops.flow_warp(image, flow)\r\n\r\n\r\n@tf.RegisterGradient(\"FlowWarp\")\r\ndef _flow_warp_grad(flow_warp_op, gradients):\r\n    return _flow_warp_ops.flow_warp_grad(flow_warp_op.inputs[0],\r\n                                         flow_warp_op.inputs[1],\r\n                                         gradients)\r\n\r\n\r\nI got the following error:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/akshat_suwalka/Fully-Automatic-Video-Colorization-with-Self-Regularization-and-Diversity-master/flow_warp.py\", line 4, in <module>\r\n    tf.compat.v1.resource_loader.get_path_to_datafile(\"./lib/flow_warp.so\"))\r\n\r\n  File \"/home/akshat_suwalka/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/load_library.py\", line 57, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\n\r\nNotFoundError: libcudart.so.8.0: cannot open shared object file: No such file or directory\r\n\r\nI am not able to deal with this error.\r\nCan anyone help?", "comments": ["@akshat-suwalka \r\n\r\nRequest you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Request you to share colab link or simple standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38864, "title": "Fix timeline fail to parse long kernel names when op_time is set", "body": "This is a follow-up pr on #37074 .\r\n\r\nI just found out that `re.match` is not good for multi-line text, while many XLA kernels do have a really long name or timeline_label. Therefore, this pr changed the regular expression in to manually parsing. \r\nAlso, the format of `timeline_label` seems to be changed in the lastest version of tf, and I change a little of the parsing rule according to the new format.\r\n\r\nThank you for your time on reviewing this pr and I'm sorry that the origin pr on this did not resolve these problems.", "comments": []}, {"number": 38863, "title": "Windows PC Don't Need GPU get could not retrieve CUDA device attribute error ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary downloaded from tf website\r\n- TensorFlow version: latest from site at 24/04/20 \r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: not applicable\r\n- GPU model and memory: not applicable\r\n\r\nRan Beginners Tensorflow Demo Code in local Python envt (Thonny)\r\n\r\nimport tensorflow as tf, got \r\n2020-04-24 09:32:36.144859: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-04-24 09:32:36.145180: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\nso as instructed ignored this\r\n\r\nthen did\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nran OK\r\nthen did\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10)\r\n])\r\nand got the following\r\n2020-04-24 09:32:38.363170: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n\r\n2020-04-24 09:32:38.575980: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: Could not retrieve CUDA device attribute (81: UNKNOWN ERROR (1)\r\n\r\nSo seems need a workaround to get it to continue to ignore the fact there is no GPU (I want it to run on the CPU only.\r\n\r\nThanks very much\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Are you able to do a `model.fit(x_train, y_train)`\r\nThese warnings occure because you do not have a NVIDIA GPU card or CuDNN installed. It's fine you can continue without it.", "Tensorflow warns of not having a GPU installed but you can work with CPU as well. To hide these errors and warnings reported by Tensorflow refer [here](https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information)\r\n\r\n", "> Tensorflow warns of not having a GPU installed but you can work with CPU as well. To hide these errors and warnings reported by Tensorflow refer [here](https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information)\r\n\r\n@DavidMulvey2,\r\nCould you please check @oke-aditya's comment and let us know if it helps. Thanks!", "Hi there\n\nThank you very much for looking into this but on the face of it, it doesn't seem to help because the reply seems to be about how to suppress error messages rather than resolving the underlying issue.\n\nDavid\n________________________________\nFrom: amahendrakar <notifications@github.com>\nSent: 24 April 2020 18:44\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Mulvey, David (PG/R - Elec Electronic Eng) <d.mulvey@surrey.ac.uk>; Mention <mention@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] Windows PC Don't Need GPU get could not retrieve CUDA device attribute error (#38863)\n\n\nTensorflow warns of not having a GPU installed but you can work with CPU as well. To hide these errors and warnings reported by Tensorflow refer here<https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F35911252%2Fdisable-tensorflow-debugging-information&data=02%7C01%7Cd.mulvey%40surrey.ac.uk%7C9c08caf7fa3e463896ff08d7e877352f%7C6b902693107440aa9e21d89446a2ebb5%7C0%7C0%7C637233470999565845&sdata=4jyoo7BZV%2BT5YES3%2BW2iPxArjyWKtdC0DHP3KDYTN5Y%3D&reserved=0>\n\n@DavidMulvey2<https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FDavidMulvey2&data=02%7C01%7Cd.mulvey%40surrey.ac.uk%7C9c08caf7fa3e463896ff08d7e877352f%7C6b902693107440aa9e21d89446a2ebb5%7C0%7C0%7C637233470999575841&sdata=sTjbPlOzF2IN5ruqYTtf8xQVRQqjUychN8%2BaXVpDDAQ%3D&reserved=0>,\nCould you please check @oke-aditya<https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Foke-aditya&data=02%7C01%7Cd.mulvey%40surrey.ac.uk%7C9c08caf7fa3e463896ff08d7e877352f%7C6b902693107440aa9e21d89446a2ebb5%7C0%7C0%7C637233470999575841&sdata=6VbUpxxZ8CekesHnAmmOCL14J2KEQcpTBlX%2FjiywmXg%3D&reserved=0>'s comment and let us know if it helps. Thanks!\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F38863%23issuecomment-619155478&data=02%7C01%7Cd.mulvey%40surrey.ac.uk%7C9c08caf7fa3e463896ff08d7e877352f%7C6b902693107440aa9e21d89446a2ebb5%7C0%7C0%7C637233470999585829&sdata=s208%2FZVLdpOeB8ss1YTS2%2FBMYmWnEPc4nblJZ%2BBDVf4%3D&reserved=0>, or unsubscribe<https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAPKHH7TX5IKERCV7CDW7NB3ROHFZTANCNFSM4MP6FZYA&data=02%7C01%7Cd.mulvey%40surrey.ac.uk%7C9c08caf7fa3e463896ff08d7e877352f%7C6b902693107440aa9e21d89446a2ebb5%7C0%7C0%7C637233470999585829&sdata=MN%2B%2FozbRM565MIvwsjvW0exAbc9f8oiw%2FdjS%2FdhLRYM%3D&reserved=0>.\n", "You may want to install tensorflow cpu package in this case.\r\n`pip install tensorflow-cpu`\r\n", "That's a good tip thanks.\n\nIn reply to Aditya thanks for the question but in fact I couldn't do a fit which is why I had to raise the issue.\n\nI have solved it myself temporarily by setting the GPU index to -1 but I would expect that Yasir's tip will provide a permanent solution if I need it.\n\nSo, I would be happy for you to close the issue now, most grateful for your assistance on this, stay safe everyone.\n\nThanks\n\nDavid\n\n\n________________________________\nFrom: Yasir Modak <notifications@github.com>\nSent: 30 April 2020 21:22\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Mulvey, David (PG/R - Elec Electronic Eng) <d.mulvey@surrey.ac.uk>; Mention <mention@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] Windows PC Don't Need GPU get could not retrieve CUDA device attribute error (#38863)\n\n\nYou may want to install tensorflow cpu package in this case.\npip install tensorflow-cpu\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F38863%23issuecomment-622089152&data=02%7C01%7Cd.mulvey%40surrey.ac.uk%7C605dc4debb9e417f924808d7ed444784%7C6b902693107440aa9e21d89446a2ebb5%7C0%7C0%7C637238749824430075&sdata=CY0Mvhb6Hehu7Clepq3EYgsRBA31V%2FQEeCc6cmYikYM%3D&reserved=0>, or unsubscribe<https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAPKHH7XPYSNDB4VQ5JV34ITRPHM2HANCNFSM4MP6FZYA&data=02%7C01%7Cd.mulvey%40surrey.ac.uk%7C605dc4debb9e417f924808d7ed444784%7C6b902693107440aa9e21d89446a2ebb5%7C0%7C0%7C637238749824430075&sdata=ehChs%2FEtdQqhGgRZJBXafO%2Br7cprHAhW%2B7BkU%2FDKo2U%3D&reserved=0>.\n", "CPU option is possible. Stay Safe and have a Happy Time working with Tensorflow. :+1: "]}, {"number": 38862, "title": "tf.ragged.stack may return tf.Tensor instead of tf.RaggedTensor", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): unknown 2.1.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nA call like \r\n```\r\nprint(tf.ragged.stack([tf.constant(5)], axis=0))\r\n>>> tf.Tensor([5], shape=(1,), dtype=int32)\r\n```\r\nreturns a `tf.Tensor` instead of a `RaggedTensor`\r\n\r\n**Describe the expected behavior**\r\nAccording to the documentation, the result should be a `RaggedTensor`. One might consider this a documentation error, but I think the return type of this function should not change depending on the values that are stacked.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tf.ragged.stack([tf.constant(5)], axis=0))\r\n```", "comments": ["@ngc92 \r\ni ran the code shared by you, please find the gist of [error faced](https://colab.sandbox.google.com/gist/Saduf2019/b6be3b971405f38e3c4f7b689d4f6a81/untitled152.ipynb)", "maybe I'm blind, but doesn't the gist show exactly what I claimed it would, i.e. a return type which is not a `RaggedTensor`? I'm not sure which error you mean.", "With the current ragged tensor implementation, there is no such thing as a 1D (or 0D) ragged tensor.  In particular, each RaggedTensor wraps a \u201cvalues\u201d tensor (which must be at least 1D) and a \u201crow partitioning tensor\u201d that describes how the outermost dimension of values should be partitioned into rows. ", "For more details see https://www.tensorflow.org/guide/ragged_tensor#raggedtensor_encoding. \r\n\r\nYou said \u201c According to the documentation, the result should be a RaggedTensor.\u201d which documentation are you referring to?", "https://www.tensorflow.org/api_docs/python/tf/ragged/stack\r\n```\r\nReturns: A RaggedTensor with rank R+1. result.ragged_rank=1+max(axis, max(rt.ragged_rank for rt in values])).\r\n```", "@ngc92,\r\nSorry for the delayed response. The [Example Code](https://www.tensorflow.org/api_docs/python/tf/ragged/stack#examples) mentioned in documentation returns a **`Ragged Tensor`**, which is line with [the documentation](https://www.tensorflow.org/api_docs/python/tf/ragged/stack#returns) provided. \r\n\r\nPlease find [the Gist](https://colab.research.google.com/gist/rmothukuru/9a6a789d7adc9e746535129ba9f6e3e7/gh_38862.ipynb) of the working code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38862\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38862\">No</a>\n"]}, {"number": 38861, "title": "TFLu: Add MVE flag to cmsis-nn glue for clarity", "body": "__ARM_FEATURE_MVE is now autodetected so ARM_MATH_MVEI is no longer\r\nneeded.\r\nAlso added ruy download to stm32f4 target so that it can be built.", "comments": ["@mansnils Can you please resolve conflicts? Thanks!", "Conflicts are resolved.", "@mansnils  Can you please resolve conflicts? Thanks!", "@gbaned Conflicts are resolved now."]}, {"number": 38860, "title": "A problem about \"bazel workspace\"", "body": "I want to use \"tensorflow/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/run_eval.cc\" to evaluate a .tflite model. I run the command \"bazel run -c opt ......\", but returned \"ERROR: The 'run' command is only supported from within a workspace\". What does workspace mean?", "comments": ["@fourierer \r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38860\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38860\">No</a>\n"]}, {"number": 38859, "title": "Wrong description in Digit classifier : Tensorflow Lite examples", "body": "## URL with the issue: https://www.tensorflow.org/lite/examples\r\n\r\n## Description of issue:\r\nThe digit classifier example card has the wrong description : \r\n\"Generate reply suggestions to input conversational chat messages.\"\r\n\r\n## Screenshot\r\n\r\n![image](https://user-images.githubusercontent.com/23613193/80188049-4441c500-862e-11ea-998d-b66c246de3d2.png)\r\n", "comments": ["Thanks. Filed an internal issue: b/154952252", "This has been fixed now. Thanks!"]}, {"number": 38858, "title": "[CPU] Best CPU Build found with MKL config in bazel build BUT with MKL Disabled in script, especially on LSTM, can't undestand why", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): Build from source\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 0.27.1\r\n- GCC/Compiler version (if compiling from source): msvc2019\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nHello there, I'm trying to find one of the best possible build option config while build tensorflow from source on my computers.\r\n\r\nFirst I thought (as my two coputers are using i7 CPUs) building with MKL was the best option. But tuning correctly intra and inter op thread is hard because it really depends on the model used and the size of data used.\r\n\r\nI created a micro benchmark (not really great I admit) but I found some huge discrepencies between build config options AND between Defines while running the script. I'm only benchmarking inference.\r\n\r\nAll builds were using /arch:AVX2\r\n\r\nHere are the results of the microbenchmark with each build config options and define pairs :\r\n\r\n### Build Command (With MKL) :\r\n```\r\nbazel --output_base=output_dir build --define=no_tensorflow_py_deps=true --config=v2 --config=mkl --config=opt -c opt --copt=\"/Ob3\" --copt=\"/fp:fast\" --copt=\"/O2\"  //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n### Define Config (MKL Disabled)\r\n```\r\nos.environ['TF_DISABLE_MKL'] = '1'\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \r\n\r\ntf.compat.v1.disable_eager_execution()\r\n```\r\n### Results\r\n```\r\nBig Dense pass 1 : it took 10.544710159301758 seconds\r\nBig Dense pass 2 : it took 9.63374948501587 seconds\r\nBig Dense pass 3 : it took 9.610066175460815 seconds\r\nXception pass 1 : it took 5.660003900527954 seconds\r\nXception pass 2 : it took 5.289057493209839 seconds\r\nXception pass 3 : it took 5.238063812255859 seconds\r\nLSTM pass 1 : it took 10.222216606140137 seconds\r\nLSTM pass 2 : it took 8.22239375114441 seconds\r\nLSTM pass 3 : it took 8.210553884506226 seconds\r\nLSTM unrolled pass 1 : it took 14.414194822311401 seconds\r\nLSTM unrolled  pass 2 : it took 7.88745903968811 seconds\r\nLSTM unrolled pass 3 : it took 8.34917140007019 seconds\r\nLinear Model pass 1 : it took 1.0033559799194336 seconds\r\nLinear Model pass 2 : it took 0.0069768428802490234 seconds\r\nLinear Model pass 3 : it took 0.010947227478027344 seconds\r\nLinear Model small data pass 1 : it took 1.0467493534088135 seconds\r\nLinear Model small data pass 2 : it took 0.08178114891052246 seconds\r\nLinear Model small data pass 3 : it took 0.15857625007629395 seconds\r\n```\r\n\r\n### Build Command (with MKL) :\r\n```\r\nbazel --output_base=output_dir build --define=no_tensorflow_py_deps=true --config=v2 --config=mkl --config=opt -c opt --copt=\"/Ob3\" --copt=\"/fp:fast\" --copt=\"/O2\"  //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n### Define Config (MKL Enabled)\r\n```\r\n#os.environ['TF_DISABLE_MKL'] = '1'\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \r\n\r\ntf.compat.v1.disable_eager_execution()\r\n```\r\n### Results\r\n```\r\nBig Dense pass 1 : it took 13.546119928359985 seconds\r\nBig Dense pass 2 : it took 12.425012588500977 seconds\r\nBig Dense pass 3 : it took 12.265166759490967 seconds\r\nXception pass 1 : it took 16.793854475021362 seconds\r\nXception pass 2 : it took 15.786102056503296 seconds\r\nXception pass 3 : it took 15.787992000579834 seconds\r\nLSTM pass 1 : it took 10.035675525665283 seconds\r\nLSTM pass 2 : it took 8.080043315887451 seconds\r\nLSTM pass 3 : it took 7.926990509033203 seconds\r\nLSTM unrolled pass 1 : it took 20.55500102043152 seconds\r\nLSTM unrolled  pass 2 : it took 7.7295191287994385 seconds\r\nLSTM unrolled pass 3 : it took 7.706652879714966 seconds\r\nLinear Model pass 1 : it took 1.0731301307678223 seconds\r\nLinear Model pass 2 : it took 0.007978677749633789 seconds\r\nLinear Model pass 3 : it took 0.01296544075012207 seconds\r\nLinear Model small data pass 1 : it took 1.1389260292053223 seconds\r\nLinear Model small data pass 2 : it took 0.07978606224060059 seconds\r\nLinear Model small data pass 3 : it took 0.1665961742401123 seconds\r\n```\r\n\r\n### Build Command (Without MKL)\r\n```\r\nbazel --output_base=output_dir build --define=no_tensorflow_py_deps=true --config=v2 --config=opt -c opt --copt=\"/Ob3\" --copt=\"/fp:fast\" --copt=\"/O2\"  //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n### Define Config (MKL Disabled/Enabled should not matter as there it is not an MKL build)\r\n```\r\nos.environ['TF_DISABLE_MKL'] = '1'\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \r\n\r\ntf.compat.v1.disable_eager_execution()\r\n```\r\n### Results\r\n```\r\nBig Dense pass 1 : it took 10.71854853630066 seconds\r\nBig Dense pass 2 : it took 9.549690961837769 seconds\r\nBig Dense pass 3 : it took 9.396085262298584 seconds\r\nXception pass 1 : it took 5.693288326263428 seconds\r\nXception pass 2 : it took 5.287402391433716 seconds\r\nXception pass 3 : it took 5.261985540390015 seconds\r\nLSTM pass 1 : it took 22.114120721817017 seconds\r\nLSTM pass 2 : it took 20.470056772232056 seconds\r\nLSTM pass 3 : it took 20.77280902862549 seconds\r\nLSTM unrolled pass 1 : it took 27.426076889038086 seconds\r\nLSTM unrolled  pass 2 : it took 21.209628105163574 seconds\r\nLSTM unrolled pass 3 : it took 21.19388747215271 seconds\r\nLinear Model pass 1 : it took 1.0153882503509521 seconds\r\nLinear Model pass 2 : it took 0.007978439331054688 seconds\r\nLinear Model pass 3 : it took 0.010976552963256836 seconds\r\nLinear Model small data pass 1 : it took 1.046147108078003 seconds\r\nLinear Model small data pass 2 : it took 0.08078360557556152 seconds\r\nLinear Model small data pass 3 : it took 0.1626284122467041 seconds\r\n```\r\n\r\nWhat we see is : \r\n- on XCeption model, MKL Enabled run is very slow, and any amount of intra/extra threads config didn't manage to put these numbers down\r\n- without BUILDING with MKL, LSTM benches are very slow !\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nI can't undestand why the best results were obtained with building with MKL **enabled** in build and then **disabling** its usage via script.\r\n\r\nBy the way when installing tensorflow via pip, we observe the same behaviour (bad results on LSTM) and performance is a little bit worse on all other benchmarks (as it is not build with AVX2 and other optimisation flags I guess)\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nThe benchmark naive code is attached below :\r\n[bench.zip](https://github.com/tensorflow/tensorflow/files/4527325/bench.zip)\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@NicolasVidal Do you have any update on this issue? I'm facing the same problem on intel. Official wheels with mkl enabled are very slow for LSTM networks.", "Hi @NicolasVidal , \r\nCould you please try on latest stable version TF 2.6  and let us know if this is still an issue.Thanks!", "hi, \r\n\r\nTensorflow w/MKL tuning requires additional param settings around OpenMP in addition to inter and intra settings. can you try these settings before running your script?\r\n\r\nexport OMP_NUM_THREADS=< no of physical cores>\r\nexport KMP_AFFINITY=\"granularity=fine,noverbose,compact,1,0\"\r\nexport KMP_BLOCKTIME=1\r\nexport KMP_SETTINGS=1\r\n\r\nor \r\n\r\nAlternatively,  you can try the latest Tensorflow WITHOUT mkl flag and set the env variable to just enable oneDNN ops (no OpenMP dependency)\r\nexport TF_ENABLE_ONEDNN_OPTS=1", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38858\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38858\">No</a>\n"]}, {"number": 38857, "title": "Tensorflow lite for unity", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.14\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 1.1.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla K80/ 12GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nNot able to find libraries while building tensorflow lite for unity\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build -c opt --config android_arm64 --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt -fvisibility=hidden --linkopt -s --strip always //tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nERROR: Skipping '//tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so': no such target '//tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so': target 'libtensorflowlite_gpu_delegate.so' not declared in package 'tensorflow/lite/delegates/gpu' defined by /home/ubuntu/tf_1.14/tensorflow/tensorflow/lite/delegates/gpu/BUILD\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such target '//tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so': target 'libtensorflowlite_gpu_delegate.so' not declared in package 'tensorflow/lite/delegates/gpu' defined by /home/ubuntu/tf_1.14/tensorflow/tensorflow/lite/delegates/gpu/BUILD\r\n", "comments": ["You are using a too old version of tensorflow source code. The target you are trying to build is not there at that version.\r\nCould you tried to use a recent version?", "Thanks Thaink,\r\n\r\nRequest you to please specify to which version of TF you are referring to as i have used 1.14 version of TF.", "Hi @Pallav56 \r\nI would recommend to use our latest version 2.2.0", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi @Pallav56\r\nCan you close this issue if it is solved?", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38857\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38857\">No</a>\n"]}, {"number": 38856, "title": "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):-Windows 10\r\n- TensorFlow installed from (source or binary): Pip Install Tensorflow\r\n- TensorFlow version:2.2.0\r\n- Python version:3.8.2 64 bit\r\n- Installed using virtualenv? pip? conda?: PIP\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: i=I don't have Nvidia Graphic Card\r\n- GPU model and memory: Intel R HD family (2160 MB)\r\n-Laptop- Hp EliteBook 8 GB ram ; core i5\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am having a problem as Import error. Here is the stack code.\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"f:/MACHINE LEARNING/chatbot/main.py\", line 9, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nIt happened only after I just imported TensorFlow. Is it because I don't have a graphic card?\r\n\r\nPlease Help.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@dhruv-colosus,\r\nCould you please check [this comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) on a similar issue and let us know if it helps. Thanks!\r\n", "> @dhruv-colosus,\r\n> Could you please check [this comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) on a similar issue and let us know if it helps. Thanks!\r\n\r\nOk, I have avx2 SUPPORT. I already have MSVC 2017 installed. Python is 64 BIT. ALL libraries are on the same location. I think the problem is with the 2017 version instead of 2019.", "I solved my issue by upgrading it to 2019. Thank You.Closing The Issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38856\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38856\">No</a>\n"]}, {"number": 38855, "title": "Batch normalization performs different in tf.kera.Model and tf.estimator.Estimator.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- **Python version**: 3.5\r\n- **CUDA/cuDNN version**: 10.1\r\n- **GPU model and memory**: GeForce RTX 2080Ti 11GB\r\n\r\n### Describe the problem\r\nBatch normalization performs different in tf.kera.Model and tf.estimator.Estimator.\r\nWhen run the following code, it is easy to get the output among \u2460Model \u2461Estimator transformed by tf.keras.estimator.model_to_estimator \u2462 Hand-made Estimator(pre-trained model is transformed to ckpt for ensuring the same weights).\r\n\r\nVGG16(No BN layers in VGG): Three methods performs the same, ranging from 0.0 ~ 9.249033.\r\nResNet50: \u24600.0 ~ 11.896655 \u24610.0 ~ 11.621841 \u24620.0 ~ 11.621841\r\nMobileNetV2(alpha=1.0): \u24600.0 ~ 3.871163 \u24610.0 ~ 0.7918996 \u24620.0 ~ 0.7918996\r\nConv2D + BN(Default initialization): \u2460 performs the same to \u2461\r\nConv2D + BN(Random initialization): \u2460 performs differ from \u2461\r\n\r\nThe above result means that all Keras pre-trained model cannot performs right in estimator.\r\nI do the same comparison in TF 1.14, and it results the same thing.\r\n\r\nSo, how can I train a model with BN by using estimator correctly?\r\n\r\n### Source code / logs\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport logging\r\n\r\nLOGGER = logging.getLogger(\"tensorflow\")\r\n\r\n# Build model.\r\ndef build_model(name, random_bn_params=None):\r\n    if name == \"ResNet50\":\r\n        model = tf.keras.applications.ResNet50(\r\n            input_shape=(224, 224, 3),\r\n            include_top=False,\r\n            pooling=\"avg\")\r\n    elif name == \"MobileNetV2_1.0\":\r\n        model = tf.keras.applications.MobileNetV2(\r\n            input_shape=(224, 224, 3),\r\n            include_top=False,\r\n            pooling=\"avg\")\r\n    elif name == \"VGG16\":\r\n        model = tf.keras.applications.VGG16(\r\n            input_shape=(224, 224, 3),\r\n            include_top=False,\r\n            pooling=\"avg\")\r\n    else:\r\n        if random_bn_params:\r\n            params = {\"beta_initializer\": tf.random_normal_initializer(),\r\n                      \"gamma_initializer\": tf.random_normal_initializer(),\r\n                      \"moving_mean_initializer\": tf.random_normal_initializer(),\r\n                      \"moving_variance_initializer\": tf.random_normal_initializer()}\r\n        else:\r\n            params = dict()\r\n        layer = tf.keras.layers.Conv2D(1, (3, 3), use_bias=False)\r\n        layer_bn = tf.compat.v1.keras.layers.BatchNormalization(**params)\r\n        inp = tf.keras.layers.Input((224, 224, 3))\r\n        out = layer(inp)\r\n        out = layer_bn(out)\r\n        model = tf.keras.Model(inputs=inp,\r\n                               outputs=out)\r\n    return model\r\n\r\n# Model params.\r\nMODEL_NAME = \"VGG161\"  # ResNet50\u3001VGG16\u3001MobileNetV2_1.0 are available, pre-trained models are loaded. Name not in the above three will build a Conv2D + BN network.\r\nRANDOM_BN_PARAMS = True  # Whether to initialize BN  by randomized params when using a customized Conv2D + BN network.\r\nINPUT_TENSOR_FUNC = lambda: tf.ones((1, 224, 224, 3))  # Fixed model input.\r\n\r\n# Use Keras model to predict.\r\nmodel = build_model(MODEL_NAME, RANDOM_BN_PARAMS)\r\nmodel.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0, decay=1.0),\r\n              loss='mean_squared_error')\r\nmodel_output = model.predict(INPUT_TENSOR_FUNC())\r\n\r\n# Use Estimator transformed from Keras model to predict.\r\nestimator = tf.keras.estimator.model_to_estimator(model)\r\nestimator_output = next(estimator.predict(INPUT_TENSOR_FUNC))\r\n\r\n# Use a handmake model equivalent to Keras model to predict.\r\ndef _init_variables_from_checkpoint(checkpoint_path, model_dir):\r\n    flags_checkpoint_path = checkpoint_path\r\n    # Warn the user if a checkpoint exists in the model_dir. Then ignore.\r\n    if tf.compat.v1.train.latest_checkpoint(model_dir):\r\n        LOGGER.info(\r\n            \"Ignoring model_init_name because a checkpoint already exists in %s.\" % model_dir)\r\n        return None\r\n    if flags_checkpoint_path is \"\":\r\n        return None\r\n\r\n    # Gather all trainable variables to initialize.\r\n    variables_to_init = tf.compat.v1.trainable_variables()\r\n\r\n    variables_to_init_dict = {var.name.rsplit(\":\", 1)[0]: var for var in variables_to_init}\r\n\r\n    if tf.compat.v1.gfile.IsDirectory(flags_checkpoint_path):\r\n        checkpoint_path = tf.compat.v1.train.latest_checkpoint(flags_checkpoint_path)\r\n    else:\r\n        checkpoint_path = flags_checkpoint_path\r\n\r\n    LOGGER.info(\"Fine-tuning from %s.\" % checkpoint_path)\r\n\r\n    # Gather all available variables to initialize.\r\n    available_var_map = _get_variables_available_in_checkpoint(variables_to_init_dict,\r\n                                                               checkpoint_path)\r\n\r\n    init_op = tf.compat.v1.train.init_from_checkpoint(checkpoint_path, available_var_map)\r\n    LOGGER.info(\"%d/%d variables in checkpoint has been restored.\" % (len(available_var_map),\r\n                                                                      len(variables_to_init)))\r\n\r\n    return tf.compat.v1.train.Scaffold(init_op=init_op)\r\n\r\n\r\ndef _get_variables_available_in_checkpoint(variables,\r\n                                           checkpoint_path,\r\n                                           include_global_step=False):\r\n    \"\"\"Returns the subset of variables in the checkpoint.\r\n\r\n    Inspects given checkpoint and returns the subset of variables that are\r\n    available in it.\r\n\r\n    Args:\r\n        variables: A dictionary of variables to find in checkpoint.\r\n        checkpoint_path: Path to the checkpoint to restore variables from.\r\n        include_global_step: Whether to include `global_step` variable, if it\r\n            exists. Default True.\r\n\r\n    Returns:\r\n        A dictionary of variables.\r\n\r\n    Raises:\r\n        ValueError: If `variables` is not a dict.\r\n    \"\"\"\r\n    if not isinstance(variables, dict):\r\n        raise ValueError(\"`variables` is expected to be a dict.\")\r\n\r\n    # Available variables\r\n    ckpt_reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path)\r\n    ckpt_vars_to_shape_map = ckpt_reader.get_variable_to_shape_map()\r\n    if not include_global_step:\r\n        ckpt_vars_to_shape_map.pop(tf.compat.v1.GraphKeys.GLOBAL_STEP, None)\r\n    vars_in_ckpt = {}\r\n\r\n    # for key in ckpt_vars_to_shape_map:\r\n    #     LOGGER.info(\"Available variable name: %s\", key)\r\n\r\n    for variable_name, variable in sorted(variables.items()):\r\n        if variable_name in ckpt_vars_to_shape_map:\r\n            if ckpt_vars_to_shape_map[variable_name] == variable.shape.as_list():\r\n                vars_in_ckpt[variable_name] = variable\r\n            else:\r\n                LOGGER.warning(\"Variable [%s] is available in checkpoint, but has an incompatible \"\r\n                               \"shape with model variable. Checkpoint shape: [%s], model variable \"\r\n                               \"shape: [%s]. This variable will not be initialized from the \"\r\n                               \"checkpoint.\",\r\n                               variable_name,\r\n                               ckpt_vars_to_shape_map[variable_name],\r\n                               variable.shape.as_list())\r\n        else:\r\n            LOGGER.warning(\"Variable [%s] is not available in checkpoint\", variable_name)\r\n    return vars_in_ckpt\r\n\r\ndef model_fn(features, labels, mode):\r\n    # Set Keras learning phase for alter BatchNorm and Dropout performance.\r\n    tf.keras.backend.set_learning_phase(mode == tf.estimator.ModeKeys.TRAIN)\r\n\r\n    m = build_model(MODEL_NAME, RANDOM_BN_PARAMS)\r\n    predictions = m(features)\r\n    \r\n    scaffold = None\r\n    if MODEL_NAME in {\"ResNet50\", \"VGG16\", \"MobileNetV2_1.0\"}:\r\n        scaffold = _init_variables_from_checkpoint(\"w:/keras/%s.ckpt\" % MODEL_NAME, \".\")\r\n\r\n    # Create estimator_spec for Estimator.\r\n    estimator_spec = tf.estimator.EstimatorSpec(\r\n        mode=mode,\r\n        predictions=predictions,\r\n        scaffold = scaffold\r\n    )\r\n    return estimator_spec\r\nestimator_handmake = tf.estimator.Estimator(model_fn=model_fn)\r\nestimator_handmake_output = next(estimator_handmake.predict(INPUT_TENSOR_FUNC))\r\n\r\n# Compare the output range.\r\nprint(np.max(model_output), np.min(model_output))\r\nprint(np.max(estimator_output[list(estimator_output.keys())[0]]), np.min(estimator_output[list(estimator_output.keys())[0]]))\r\nprint(np.max(estimator_handmake_output), np.min(estimator_handmake_output))\r\n```", "comments": ["@mysephi \r\ni ran the code shared, please confirm the error faced is the same in the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/34b0656199b34ca71ee216392c1551cf/38855.ipynb)", "> @mysephi\r\n> i ran the code shared, please confirm the error faced is the same in the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/34b0656199b34ca71ee216392c1551cf/38855.ipynb)\r\n\r\nYes. it is.", "> @mysephi\r\n> i ran the code shared, please confirm the error faced is the same in the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/34b0656199b34ca71ee216392c1551cf/38855.ipynb)\r\n\r\nYou can also change MODEL_NAME and RANDOM_BN_PARAMS for more error mentioned above.", "I was able to see some values getting generated from the code provided by you in Tf Nightly 2.6, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/ff25340a22ff15cce6e4faab9ad8cb1c/38855.ipynb) and confirm if your issue is resolved.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38855\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38855\">No</a>\n"]}, {"number": 38854, "title": "Fix MirroredStrategy cannot output RunMetadata", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n\r\nCurrently, we cannot get `run_metadata` (or timeline) in graph mode when using `MirroredStrategy` and keras together. The reason is that the current implementation failed to pass `run_options` and `run_metadata` to the distributed model. This pr is trying to fix it.\r\n\r\nThe main changed we did are:\r\n- Retain the kwargs in `self.session_kwargs` in `GraphExecutionFunction`, because it is used as the input of  the distributed `K.function` in `_make_graph_execution_function`.\r\n- Update the unwrapping of grouped kwargs to make them support `run_metadata` and `run_options`.\r\n\r\nThank you for your time on reviewing this pr.", "comments": ["when using `tf.distribute.experimental.MultiWorkerMirroredStrategy`  in multi worker training, still not output timeline after this fix.", "@Hacky-DH The difference between `MirroredStrategy` and the `MultiWorkerMirroredStrategy` is that the latter use `GrpcSession` while the former uses `DirectSession`, and base on my experiment, the timeline problem of `MultiWorkerMirroredStrategy`lies in the GrpcSession side and that is why this pr hasn't fixed it.", "> @Hacky-DH The difference between `MirroredStrategy` and the `MultiWorkerMirroredStrategy` is that the latter use `GrpcSession` while the former uses `DirectSession`, and base on my experiment, the timeline problem of `MultiWorkerMirroredStrategy`lies in the GrpcSession side and that is why this pr hasn't fixed it.\r\n\r\nThanks for reply. So how to fix this when using `MultiWorkerMirroredStrategy`?", "> Thanks for reply. So how to fix this when using `MultiWorkerMirroredStrategy`?\r\n\r\nI haven't dug deep into this problem yet. If you have interest in this problem, I believe the problem resides in the interface between python and c++.", "@zhuzilin  Can you please check @qlzh727's comments and keep us posted. Thanks!", "Close this pr because RunMetadata may not be supported in the future release of keras."]}, {"number": 38853, "title": "Fix error in deleting servers because module may be garbage collected ahead", "body": "When deleting grpc server at the end of the program, the `errors` module may already be garbage collected and trigger:\r\n```python\r\nException ignored in: <bound method Server.__del__ of <tensorflow.python.training.server_lib.Server object at 0x7f61a00e4668>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/training/server_lib.py\", line 161, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'UnimplementedError'\r\n```\r\nThis pr will just check if `errors` is none ahead.\r\n\r\nThank you for your time on this review.", "comments": []}, {"number": 38852, "title": "TFLite on Windows?  Efficient embedded TF2 x86-64 Windows CPU inference?", "body": "On our ARMv8 Android embedded platform, we are using the following workflow and it's working great:\r\n\r\n    Train on Linux servers using TF2 ---> embedded inference on Android using TFLite\r\n\r\nOn our x86-64 Windows embedded platform, we are using the following workflow:\r\n\r\n    Train on Linux servers using TF1 ---> embedded inference on Windows using OpenVINO\r\n\r\nWe are stuck with TF1 for Windows because Intel's OpenVINO does not yet support TF2; they may have support next fall or later.\r\n\r\nIn the meantime, this presents a problem for us because we have teams using both TF1 and TF2 for the same AI feature.   We would like to standardize on TF2:\r\n\r\n    Train on Linux servers using TF2 ---> embedded inference on Windows using ?????\r\n\r\nFor example, is there a way to run efficient TFLite CPU inference on x86-64 Windows?   Ideas:\r\n\r\n- try to compile the TFLite runtime for Windows with x86-64 intrinsics (e.g. AVX, FMA, SSE)\r\n- try the TFLite XNNPack Delegate which seems to have x86 support (but maybe not Windows)\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack\r\n\r\nWe tried going through an intermediate path like TF2 -> ONNX, but have not had success yet.\r\n\r\nAny advice would be appreciated, especially from anyone who may have already gone down this path.\r\n\r\n", "comments": ["TFLite on Windows is supported. You can build a target for windows using:\r\nbazel build -c opt --config=windows TARGET\r\nFor example:\r\nbazel build -c opt --config=windows  //tensorflow/lite:tensorflowlite", "Also see #33634 \r\nI'm able to successfully build and run tflite on Windows 10.", "Thanks @thaink and @Lotte1990 \r\nWe'll give your suggestions a try.  Cheers.", "XNNPACK both supports Windows and includes optimizations for x86 (SSE2/AVX/AVX2/AVX512 levels). Build with clang-cl for the best performance.\r\n\r\nNote that only a limited subset of floating-point operators are supported in the XNNPACK delegate (see [README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md) for details), the rest will use the default implementation which is limited to SSE2.", "Thanks @Maratyszcza !  Will also try to give XNNPack a try.  Cheers\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Maratyszcza is it possible to use XNNPACK in Python via tf.lite.experimental.load_delegate?", "@yaysummeriscoming There is no Python API to apply XNNPACK delegate, but if you build TensorFlow Lite with `--define tflite_with_xnnpack=true`, XNNPACK delegate will be used by default. However, keep in mind that this feature landed just hours ago in e480d8f7ff66dbab239019c9f202748f6fa1f661 and you need a fresh checkout of tensorflow repository to try it.", "@Maratyszcza thanks for the tip. I did some tests and found that XNNPACK was able to meet or exceed openVino's performance. Very impressive!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@Maratyszcza Any plans to add a Python API?  I am updating some TFLite 2.2 serving infrastructure to 2.3 and was hoping to take advantage of the XNNPACK backend.  Currently, I use the default [build_pip_package.sh](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/build_pip_package.sh) file to build from source in Docker for a linux instance.  Should I consider using [build_pip_package_with_bazel.sh](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh) instead and add the `--define tflite_with_xnnpack=true` flag to the `BAZEL_FLAGS` variables?", "There's no Python API to enable XNNPACK 2.3, and the only way to enable XNNPACK in Python binaries is through the `--define tflite_with_xnnpack=true` Bazel option.", "Hi,\r\njust came across this issue looking for a way to build Tensorflow Lite library for Windows x86 systems.\r\nI'm pretty new to TF and Bazel word, apart from building it I'm just following the C++ API documentation to implement what I need.\r\n\r\n**What I need:** the libraries I work with need to be able to run a simple inference using a pre-trained model for image segmentation. The problem for me is only that these libraries _must support Linux and Windows, x86 and x64._\r\n\r\nBuilding the .so and the x64 .dll is straightforward for me, I could already build and link my library and debug my demo code following the official Tensorflow Lite documentation. I'm just stuck with the .dll for x86 processors build :disappointed: \r\n\r\n@holokai-ai do you mind sharing which one of your two ideas solved your problem? Did the XNNPACK version worked fine for you? (i assume so from your previous comment)\r\n\r\n@Maratyszcza since I'm new to TF, would you mind sharing a good reference link to understand better how TF uses external interpreters / inference engines? Could you also please explain briefly if the command line from XNNPACK README should work out of the box? The --cpu=x86 would solve Clang/MSVC tools to build for windows x86 staight away? I found the following example:\r\n`bazel build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  --define tflite_with_xnnpack=true \\\r\n  //tensorflow/lite/java:tensorflow-lite\r\n`\r\n\r\nSorry for asking so many questions, I've spend couple of hours looking for a solutions and people seem to really struggle with x86 architectures.\r\n\r\nThanks in advance, hope you folks have a good one :smiley: ", "@tcervi XNNPACK and TFLite support bulding for Windows x86 (32-bit), but AFAIK Bazel supports only x86-64 Windows out of the box. You'd need to either write a custom toolchain file for Bazel, or build TFLite and XNNPACK with CMake.", "Thanks dude! Cool stuff\r\nI indeed came across this way of configuring custom toolchains for windows on Bazel, was just unsure about how much effort to put on it without knowing if the code would even be able to compile for x86 (with MSVC or Clagn) without changes. Tbh I've been reading about it yesterday, did not have time to test it so far anyway :smile: \r\n\r\nOnce I'm back to office will give it a try, at least for now I have a mental roadmap for my approach. Thanks a lot for your quick answer, really appreciate it. If I find any good result will come back here to let you now.\r\n\r\nCheers.\r\n\r\n_Disclaimer: windows is not my fav platform anyway, bust since the library is meant to be supported I have to make it work somehow (unless it's not currently possible and then I have to find another way)._"]}, {"number": 38851, "title": "Issues creating Frozen_graph for this model", "body": "I am using Windows 10\r\nTensorflow v 1.14\r\n\r\nI am trying to create a frozen graph for the checkpoint data in the following gitrepo:\r\nhttps://github.com/una-dinosauria/3d-pose-baseline \r\nusing the code below:\r\n`\r\nwith tf.Session() as sess:\r\n    \r\n    saver=tf.train.import_meta_graph(meta_path)\r\n    \r\n    saver.restore(sess,'C:\\\\Users\\\\alecd\\\\3d-pose-baseline\\\\checks\\\\checkpoint-4874200.meta')\r\n    \r\n    frozen_graph_def=tf.graph_util.convert_variables_to_constants(sess,sess.graph_def,output_node_names)\r\n    \r\n    with open('output_graph.pb', 'wb') as f:\r\n        f.write(frozen_graph_def.SerializeToString())\r\n    \r\n`\r\nNow, when I would try to serve the model in python I would get the following error: \r\n\r\n`\r\nValueError: Input 0 of node learning_rate/Assign was passed float from learning_rate:0 incompatible with expected float_ref.\r\n`\r\n\r\nI thought that one of the possible issues was that I was not passing a specific output node so I decided to try and visualize the graph inside tensorboard using:\r\n\r\n`\r\ntf.train.import_meta_graph(\"checkpoint-4874200.meta\")\r\nfor n in tf.get_default_graph().as_graph_def().node:\r\n   print(n)\r\nwith tf.Session() as sess:\r\n  writer = tf.summary.FileWriter(\"./output/\", sess.graph)\r\n  writer.close()\r\n`\r\nbut when I would run\r\n`\r\ntensorboard --logdir=data/ --host localhost --port 8088\r\n`\r\nI get a tensorboard page with no information which made me start to wonder whether or not the meta graph was corrupted or something else was going on. \r\n\r\nJust looking for general advice and or help here if anyone has any idea as to what is going on?\r\n\r\nAs a side note the frozen_graph I created can be found at \r\nhttps://github.com/alecda573/frozen_graph", "comments": ["@alecda573,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "@amahendrakar  I did provide the full code more or less. To reiterate, I am trying to convert the following checkpoint data to a frozen graph: https://drive.google.com/file/d/0BxWzojlLp259MF9qSFpiVjl0cU0/view\r\n\r\nI specifically want to convert the last checkpoint, i.e. the files with this tag: checkpoint-4874200\r\n\r\nThe code I am using to freeze the graph is the following: \r\n\r\n\r\n    trained_checkpoint_prefix='checkpoint-4874200'\r\n\r\n    export_dir=os.path.join('export_dir')\r\n\r\n    graph=tf.Graph()\r\n\r\n\r\n    with tf.Session(graph=graph) as sess:\r\n\r\n    loader=tf.train.import_meta_graph(trained_checkpoint_prefix+'.meta')\r\n    loader.restore(sess,trained_checkpoint_prefix)\r\n    \r\n    builder=tf.saved_model.builder.SavedModelBuilder(export_dir)\r\n    builder.add_meta_graph_and_variables(sess,\r\n                                         [tf.saved_model.SERVING],\r\n                                         strip_default_attrs=True)\r\n    \r\n    \r\n    builder.save()  \r\n    builder.save(as_text=True)\r\n    \r\n    frozen_graph_def=tf.graph_util.convert_variables_to_constants(sess,sess.graph_def,[n.name for n in tf.get_default_graph().as_graph_def().node])\r\n    \r\n    output=[n.name for n in tf.get_default_graph().as_graph_def().node]\r\n    print(len(output))\r\n    print(output)\r\n    with open('frozen_graph.pb','wb') as f:\r\n        f.write(frozen_graph_def.SerializeToString())\r\n        \r\n\r\nIf I try to serve this model in python I end up getting \r\n\r\n`\r\nValueError: Input 0 of node learning_rate/Assign was passed float from learning_rate:0 incompatible with expected float_ref.\r\n`\r\n\r\nAlso, when I try to view the graph using tensorboard, by importing the meta graph no data is written to tensorboard. The code I am using to write to tensorboard is:\r\n\r\n    tf.train.import_meta_graph(\"checkpoint-4874200.meta\") \r\n\r\n    for n in tf.get_default_graph().as_graph_def().node: \r\n\r\n        print(n) \r\n\r\n    with tf.Session() as sess: \r\n\r\n        writer = tf.summary.FileWriter(\"./output/\", sess.graph) writer.close()\r\n\r\nand then I call using the following command in my terminal:\r\n\r\n    tensorboard --logdir=data/ --host localhost --port 8088\r\n\r\n\r\nIf you want to see the frozen graph I generated (but cannot serve) it can be found at:\r\nhttps://github.com/alecda573/frozen_graph\r\n\r\nThanks in advance for your help!", "@amahendrakar Then try to serve the model using: \r\n\r\n    import tensorflow as tf\r\n    import sys\r\n    from tensorflow.python.platform import gfile\r\n\r\n    from tensorflow.core.protobuf import saved_model_pb2\r\n    from tensorflow.python.util import compat\r\n    with tf.Session() as persisted_sess:\r\n         print(\"load graph\")\r\n         with gfile.FastGFile(\"frozen_graph.pb\",'rb') as f:\r\n            graph_def = tf.GraphDef()\r\n            graph_def.ParseFromString(f.read())\r\n            persisted_sess.graph.as_default()\r\n    tf.import_graph_def(graph_def, name='')\r\n    writer = tf.summary.FileWriter(\"./tf_summary\", graph=persisted_sess.graph)\r\n    # Print all operation names\r\n    for op in persisted_sess.graph.get_operations():\r\n      print(op)\r\n    # next: do the following in bash:\r\n    # tensorboard --logdir ./tf_summary/\r\n    "]}, {"number": 38850, "title": "Mixed Precision + Gaussian Noise throws data type error float16 / float 32", "body": "I get the following error:\r\n\r\n```\r\n\r\n  File \"<ipython-input-1-447a869fb5df>\", line 171, in ae\r\n    encoder,decoder = AddLayers(neurons,setup['AFunction'],setup['BatchNorm'],setup['Dropout'],setup['Layers'],dim,setup['Noise'])\r\n\r\n  File \"C:\\MA\\RecommenderSystems\\code\\helper.py\", line 89, in AddLayers\r\n    encoder.add(GaussianNoise(noise))\r\n\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py\", line 196, in add\r\n    output_tensor = layer(self.outputs[0])\r\n\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 842, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\noise.py\", line 70, in call\r\n    return K.in_train_phase(noised, inputs, training=training)\r\n\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 4285, in in_train_phase\r\n    x = switch(training, x, alt)\r\n\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 4218, in switch\r\n    x = control_flow_ops.cond(condition, then_expression_fn, else_expression_fn)\r\n\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\control_flow_ops.py\", line 1174, in cond\r\n    return cond_v2.cond_v2(pred, true_fn, false_fn, name)\r\n\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\cond_v2.py\", line 84, in cond_v2\r\n    op_return_value=pred)\r\n\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\noise.py\", line 68, in noised\r\n    shape=array_ops.shape(inputs), mean=0., stddev=self.stddev)\r\n\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\", line 899, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\", line 1197, in _add_dispatch\r\n    return gen_math_ops.add_v2(x, y, name=name)\r\n\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\", line 549, in add_v2\r\n    \"AddV2\", x=x, y=y, name=name)\r\n\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 563, in _apply_op_helper\r\n    inferred_from[input_arg.type_attr]))\r\n\r\nTypeError: Input 'y' of 'AddV2' Op has type float32 that does not match type float16 of argument 'x'.\r\n```\r\n**System information**\r\n- Windows 10 [Version 10.0.18362.418]\r\n- TensorFlow version 2.0.0 via Conda\r\n- Python version: 3.6.10\r\n- CUDA/cuDNN version: 10.0.0130 / 7.6.5\r\n- GPU model and memory: NVIDIA 2060 Super 8GB\r\n\r\n**Describe the current behavior**\r\nThe error only occurs only if mixed precision **and** noise is applied.\r\n\r\nWhen mixed precision is applied, then:\r\n```\r\nos.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1' #using tensor cores\r\npolicy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\r\ntf.keras.mixed_precision.experimental.set_policy(policy)\r\n```\r\nand data are converted to float16.\r\n\r\nConverting the output of the previous dense layer to float16 couldn't solve the problem.\r\n```\r\nencoder.add(Dense(eneurons[0],input_dim=dim,dtype=tf.float16))\r\n\r\nif noise:\r\n        encoder.add(GaussianNoise(noise))\r\n```\r\n\r\nI cannot update to a higher TF version.", "comments": ["@Arktius \r\nwe see that the code shared is not sufficient for us to replicate the issue, could you please share a simple stand alone code to replicate.", "```\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense,GaussianNoise\r\nfrom tensorflow.keras.models import Sequential\r\n\r\n\r\nos.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1' #using tensor cores\r\npolicy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\r\ntf.keras.mixed_precision.experimental.set_policy(policy)\r\n\r\n\r\n#models\r\nencoder = Sequential()\r\nencoder.add(Dense(5,input_dim=5))\r\nencoder.add(GaussianNoise(0.01))\r\n    \r\n\r\n```", "With casting the data to float16, an error during backpropagation occurs...\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense,GaussianNoise,Activation\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input\r\nfrom sklearn.metrics import mean_squared_error as mse\r\n\r\n\r\nos.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1' #using tensor cores\r\npolicy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\r\ntf.keras.mixed_precision.experimental.set_policy(policy)\r\n\r\n\r\nencoder = Sequential()\r\nencoder.add(Dense(5,input_dim=5,dtype=tf.float16))\r\nencoder.add(GaussianNoise(0.01,dtype='float32'))\r\nencoder.add(Activation('relu'))\r\nencoder.add(Dense(5))    \r\n    \r\n\r\ninput_dim = Input(shape = (5, ),name='rating_in')\r\n\r\nmodel = Model(input_dim, encoder(input_dim))\r\n    \r\nmodel.compile(loss=tf.keras.losses.mean_squared_error)\r\n  \r\ndata = np.asarray(np.random.rand(20,5),dtype='float16') ######## cast to float16\r\nmodel.fit(data,data,batch_size=10)  \r\n    \r\n```\r\n\r\nEven if I cast the gradients in the custom training loop, the training doesn't get through.\r\n\r\n```\r\ndef ModelTrain(model,optim,x,w,y,train_loss):\r\n        with tf.GradientTape() as tape:\r\n            predictions = model(x) \r\n            loss = MMSE(y,w,predictions)\r\n            scaled_loss = optim.get_scaled_loss(loss)\r\n        \r\n        scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\r\n        gradients = optim.get_unscaled_gradients([tf.cast(x,tf.float32) for x in scaled_gradients])\r\n        optim.apply_gradients(zip((gradients), model.trainable_variables))\r\n```\r\n`TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.`\r\n\r\nAnd If I now cast the trainable_variables to float16, I'll get this:\r\n`AttributeError: 'Tensor' object has no attribute '_in_graph_mode'`", "Any suggestions?", "@Arktius \r\nCan you please provide simple stand alone code such that we can replicate the issue, if possible share a colab gist, with the error the code shared in incomplete, the error faced is not same as yours. please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/3d02bd5e25f035a97a4393df156e2763/untitled162.ipynb)", "https://colab.research.google.com/gist/Saduf2019/3d02bd5e25f035a97a4393df156e2763/untitled162.ipynb\r\n\r\nYou need `!pip install tensorflow-gpu==2.0`.", "I am able to replicate the issue reported, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/07c4c42cd8b49bc19355b4b39a59b9ee/untitled162.ipynb)", "This is fixed latest tf_nightly_gpu-2.2.0.dev20200503", "Could you please show me what has changed? I've just looked in the release notes, but I couldn't find something related to Gaussian noise. \r\n\r\nI know that TF 2.1 also works as well, but I don't know why and I cannot use versions higher than 2.0. ", "Take a look at [commit](f9e899854cc96db28564fa65f22d32a647268fc1).  A bunch of other fixes were made as well so its a but tricky to pin point exactly.", "The link is probably broken. Did you want me to see the [release notes](https://github.com/tensorflow/tensorflow/releases)?", "@Arktius \r\n\r\nI am not seeing any issue with TF 2.2 version.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/127f1a5aba37886a48251c5fbbdc1b4c/untitled971.ipynb).Please, verify once and close this issue.Thanks!", "> I know that TF 2.1 also works as well, but I don't know why and **I cannot use versions higher than 2.0.**\r\n\r\nThanks for your comment, but I cannot work with versions higher than 2.0 at the moment due to driver issues on the server.\r\n", "Sorry for the delay in response. Please refer this commit [f9e8998](https://github.com/tensorflow/tensorflow/commit/f9e899854cc96db28564fa65f22d32a647268fc1#diff-784ae2490bf11479ceb7c81d180ba79a) for the changes.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38850\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38850\">No</a>\n"]}, {"number": 38849, "title": "Declare member functions on TensorShapeIter const.", "body": "This fixes a compiler error in C++20 mode caused by ambiguities between reversed and negated comparison candidates.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38849) for more info**.\n\n<!-- need_sender_cla -->", "@davidstone Thank you for your contribution. Can you please sign CLA? Thanks!", "Working through the process to get myself approved", "@googlebot I signed it!", "@davidstone it still shows CLA is pending , can you please sign CLA. Thanks!", "> @davidstone it still shows CLA is pending , can you please sign CLA. Thanks!\r\n\r\nI am seeing:\r\n\r\nCorporate Agreements\r\nYou are covered by the following corporate agreements:\r\nAgreement | Name | Date Signed\r\n-- | -- | --\r\nGoogle Corporate CLA | Uber Technologies | Mar 20, 2016 17:00 PDT", "@davidstone it still shows CLA is pending , can you use please make sure to use same GitHub username and email-id associated with it.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n"]}, {"number": 38848, "title": "Use TF_CALL_GPU_ALL_TYPES for variable ops", "body": "\r\nThis PR tries to expand variable ops, as was discussed in TF_CALL_GPU_ALL_TYPES\r\n\r\nThis PR fixes #35994\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["/cc @alextp @sanjoy @dirktheeng  as you participates in the discussions in issue #35994", "This fails internal GPU tests with error similar to \r\n\r\n```\r\nI0504 14:50:24.853820    1577 gpu_device.cc:1230] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14149 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:bb:00.0, compute capability: 7.0)\r\nE0504 14:50:24.889745    1577 test_util.py:1657] 2 root error(s) found.\r\n  (0) Invalid argument: assertion failed: [Input error: exclusive=True: more than 1 conditions (Equal:0, Equal_1:0, Equal_2:0) evaluated as True:] [0 1 1]\r\n\t [[node case/Assert/AssertGuard/Assert (defined at tensorflow/python/ops/control_flow_ops_test.py:1235) ]]\r\n  (1) Invalid argument: assertion failed: [Input error: exclusive=True: more than 1 conditions (Equal:0, Equal_1:0, Equal_2:0) evaluated as True:] [0 1 1]\r\n\t [[node case/Assert/AssertGuard/Assert (defined at tensorflow/python/ops/control_flow_ops_test.py:1235) ]]\r\n\t [[case/cond/Merge/_21]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```", "@mihaimaruseac  I don't see the code of the internal test setup, though I searched through the current repo. Is the error line corresponds to the following check in `tensorflow/python/ops/control_flow_ops_test.py` (master branch) ?:\r\n```python\r\n\r\n  @test_util.run_deprecated_v1\r\n  def testCase_multiple_matches_exclusive(self):\r\n    x = array_ops.placeholder(dtype=dtypes.int32, shape=[])\r\n    conditions = [(math_ops.equal(x, 1), lambda: constant_op.constant(2)),\r\n                  (math_ops.equal(x, 2), lambda: constant_op.constant(4)),\r\n                  (math_ops.equal(x, 2), lambda: constant_op.constant(6))]\r\n    default = lambda: constant_op.constant(8)\r\n    output = control_flow_ops.case(conditions, default, exclusive=True)\r\n    with self.cached_session() as sess:\r\n      self.assertEqual(sess.run(output, feed_dict={x: 1}), 2) # <= failure?\r\n      self.assertEqual(sess.run(output, feed_dict={x: 3}), 8) # <= failure?\r\n      with self.assertRaisesRegexp(errors.InvalidArgumentError, \"Input error:\"):\r\n        sess.run(output, feed_dict={x: 2})\r\n\r\n```", "Hi. Yes, that is the test that fails on GPU. Looks like the ref variable changes are not ok? (That's what @alextp suggested on the rollback)", "@mihaimaruseac The changes is `TF_CALL_GPU_ALL_TYPES` and `TF_CALL_GPU_NUMBER_TYPES` which impacts `bool`, `complex64`, and `complex128`\r\n\r\n```c++\r\n#define TF_CALL_GPU_ALL_TYPES(m) \\\r\n  TF_CALL_GPU_NUMBER_TYPES(m)    \\\r\n  TF_CALL_bool(m) TF_CALL_complex64(m) TF_CALL_complex128(m)\r\n```\r\n\r\nI will be surprised if  `complex64` or `complex128` will cause an issue. So it might be related to `bool`.\r\n\r\nI think I can either create a PR to remove `bool`, or I can create a PR to revert then resubmit `complex64` and `complex128` only.", "@mihaimaruseac @alextp Created a PR #39172 to remove `bool`. But let me know if you prefer to revert first, I can update the PR to revert if needed."]}, {"number": 38847, "title": "Getting \u201cCUDA_ERROR_INVALID_VALUE: invalid argument\u201d in python with Tensorflow 1.14", "body": "When I run the snippet below, as `python test.py`\r\n\r\n```python\r\nimport os\r\n# Enable '0' or disable '-1' GPU use\r\n# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\r\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\r\nimport warnings\r\n\r\nwith warnings.catch_warnings():\r\n\twarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\timport tensorflow as tf\r\n\tconfig = tf.compat.v1.ConfigProto()\r\n\t# config.gpu_options.visible_device_list = \"0\"  # pylint: disable=no-member\r\n\tconfig.gpu_options.allow_growth = True  # pylint: disable=no-member\r\n\tsession = tf.compat.v1.Session(config=config)\r\n\r\n# check if successfully using GPU\r\nif tf.test.gpu_device_name():\r\n\tprint('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\r\nelse:\r\n\tprint('GPU not being used')\r\n```\r\n\r\nI get the following error\r\n\r\n```bash\r\n2020-04-23 13:13:15.969352: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2020-04-23 13:13:15.974088: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2020-04-23 13:13:15.990122: W tensorflow/compiler/xla/service/platform_util.cc:256] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE: invalid argument\r\n2020-04-23 13:13:15.990240: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\r\nAborted (core dumped)\r\n```\r\n\r\nWhen I set `os.environ['CUDA_VISIBLE_DEVICES'] = \"-1\"`(ie no GPU use), there is no error and the output is as expected shown below.\r\n\r\n```bash\r\n2020-04-23 13:18:24.911806: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2020-04-23 13:18:24.916849: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2020-04-23 13:18:24.920347: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2020-04-23 13:18:24.920384: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: vumacs\r\n2020-04-23 13:18:24.920389: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: vumacs\r\n2020-04-23 13:18:24.920456: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 440.64.0\r\n2020-04-23 13:18:24.920482: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 440.64.0\r\n2020-04-23 13:18:24.920489: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 440.64.0\r\n2020-04-23 13:18:24.938734: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz\r\n2020-04-23 13:18:24.939659: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4849f40 executing computations on platform Host. Devices:\r\n2020-04-23 13:18:24.939686: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nGPU not being used\r\n```\r\nIs there any way to resolve this erro since I previously used the same code by setting `CUDA_VISIBLE_DEVICES` to 0 both through the script as well as shell and there were no issues. The error seems to be occuring when setting the session with `tf.compat.v1.Session(config=config)`\r\n\r\nAdditional information\r\n\r\n`python: 3.6.9`\r\n`tensorflow-gpu==1.14.0`\r\n`protobuf==3.11.3`\r\n`tensorflow-estimator==1.14.0`\r\n\r\n```bash\r\n$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Sat_Aug_25_21:08:01_CDT_2018\r\nCuda compilation tools, release 10.0, V10.0.130\r\n\r\n$ nvidia-smi\r\nThu Apr 23 13:22:06 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.64       Driver Version: 440.64       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 208...  Off  | 00000000:B3:00.0 Off |                  N/A |\r\n| 26%   28C    P8    12W / 250W |    119MiB / 11019MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1277      G   /usr/lib/xorg/Xorg                            39MiB |\r\n|    0      1388      G   /usr/bin/gnome-shell                          77MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nI believe it is not a CUDA mismatch error as someone might believe (tf 1.14 does not work with CUDA 10.2) but here CUDA for tensorflow is 10.0. Also cuda is being loaded properly as you might notice in either of those cases.\r\n\r\nProviding a few further logs\r\n```python\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n```\r\n```bash\r\n2020-04-23 15:32:47.855593: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2020-04-23 15:32:47.884652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545\r\npciBusID: 0000:b3:00.0\r\n2020-04-23 15:32:47.885146: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2020-04-23 15:32:47.886730: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2020-04-23 15:32:47.888298: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2020-04-23 15:32:47.888855: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2020-04-23 15:32:47.890673: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-04-23 15:32:47.892068: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-04-23 15:32:47.895348: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-23 15:32:47.896233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\nNum GPUs Available:  1\r\n```\r\nBut then executing this line gives me the same error\r\n```python\r\ntf.test.gpu_device_name()\r\n```\r\n```bash\r\n2020-04-23 15:34:50.948097: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2020-04-23 15:34:50.983906: W tensorflow/compiler/xla/service/platform_util.cc:256] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE: invalid argument\r\n2020-04-23 15:34:50.984119: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\r\nAborted (core dumped)\r\n```", "comments": ["@AvisekNaug \r\n\r\nI have tried on colab with TF version 1.14 -gpu and i am not seeing any issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/a9affa7c434e4f248e42e44a3ba3bd35/untitled812.ipynb).Thanks!", "I saw the gist. Well, I am not sure how to resolve this since it seems local to my linux desktop in that case. Issue wasn't there earlier and suddenly it started happening after once or twice I started to use os.environ['CUDA_VISIBLE_DEVICES'] = \"-1\" and then when I again set it to os.environ['CUDA_VISIBLE_DEVICES'] = \"0\", the error would come up. It would happen even after I have restarted the bash and run the script.\r\n\r\nI guess I would have to fix something in my linux system. :(", "@AvisekNaug \r\n\r\nThis issue is not related to Tensorflow and it looks like it is related to your local system.Please, close this thread as the issue is not related to Tensorflow.Thanks!"]}, {"number": 38846, "title": "[INTEL MKL] fix a missing build dependency.", "body": "Commit a9c4cbb4244733e3c4451e12a41ab513b9ee5d3b did some refactoring of the common_runtime build file which also includes some mkl components. However, there was a dependency that was missed which resulted in build failures under --config=mkl. This PR adds the dependency.", "comments": []}, {"number": 38845, "title": "Quantized Conv2D op gives different result in TensorFlow and TFLite", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n`No`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n`Linux Ubuntu 18.04`\r\n- TensorFlow installed from (source or binary):\r\n`pip`\r\n- TensorFlow version (use command below):\r\n`2.0.0`\r\n- Python version:\r\n`3.7.7`\r\n- GPU model and memory:\r\n`CPU only`\r\n\r\n**Describe the current behavior**\r\nConv2D op from the quanitzation-aware training graph yields different results when converted to TensorFlow Lite and executed in uint8. Manual quantization has been done when comparing the float value to the uint8 value. Both TF and TFLite models are run on x86-64 CPU.\r\n**Describe the expected behavior**\r\nThe op should give exactly the same result when comparing the node output of the TFLite node to the output of the fake-quantization node in the TF graph to ensure the correctness of the TFLite model.\r\n\r\n**Standalone code to reproduce the issue**\r\n```import os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom imageio import imread\r\nfrom typing import Any, AnyStr, Mapping, Sequence\r\n\r\n\r\ndef saved_model_to_tflite(saved_model_dir: AnyStr, output_path: AnyStr):\r\n    converter = tf.compat.v1.lite.TFLiteConverter.from_saved_model(\r\n        saved_model_dir=saved_model_dir)\r\n    converter.inference_type = tf.uint8\r\n    converter.inference_input_type = tf.uint8\r\n\r\n    converter.quantized_input_stats = {\"ToFloat\": [127, 127.5]}\r\n    converter.optimizations = []\r\n    converter.change_concat_input_ranges = False\r\n    converter.reorder_across_fake_quant = False\r\n    tflite_model = converter.convert()\r\n\r\n    with open(output_path, 'wb') as f:\r\n        f.write(tflite_model)\r\n\r\n\r\ndef compare_models(saved_model_dir: AnyStr, tflite_path: AnyStr, image_path: AnyStr):\r\n    image = imread(image_path)\r\n    image = np.ascontiguousarray(image, dtype=np.float32)\r\n    image = image[np.newaxis, ...]\r\n    normalized_image = ((image - 127) / 127.5)\r\n    \r\n    imported_model = tf.saved_model.load(saved_model_dir)\r\n    imported_model.variables.trainable = False\r\n    model = imported_model.signatures[tf.compat.v1.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n    \r\n    interpreter = tf.lite.Interpreter(tflite_path)\r\n    interpreter.allocate_tensors()\r\n    interpreter.tensor(interpreter.get_input_details()[0][\"index\"])()[:, :, :, :] = image.astype(np.uint8)\r\n    \r\n    tf_result = list(model(tf.constant(normalized_image)).values())[0].numpy()\r\n    interpreter.invoke()\r\n    tflite_result = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])().copy()\r\n    # Manually apply de-quantization\r\n    tflite_result = (tflite_result.astype(np.float) - 107) * 0.1115700826048851\r\n \r\n    ae = np.abs(tf_result - tflite_result)\r\n    max_ae = ae.max()\r\n    max_diff_indices = np.argwhere(ae == np.max(ae))\r\n    print(max_diff_indices)\r\n    print(f\"Max absolute error: {max_ae}. Example values tf: {tf_result[ae==np.max(ae)][0]}; tflite: {tflite_result[ae==np.max(ae)][0]}\")\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nTensorFlow and TFLite model visualization of the minimum model containing only one Conv2d op:\r\n![Screenshot from 2020-04-23 14-37-54](https://user-images.githubusercontent.com/10414613/80136811-3af22300-8570-11ea-8eb5-e936e8e22251.png)\r\n![Screenshot from 2020-04-23 14-31-10](https://user-images.githubusercontent.com/10414613/80136822-3ded1380-8570-11ea-8ef7-f1eee7211c0d.png)\r\n\r\n\r\nA minimum example including a saved model a converted TFLite model and an image has been attached to help reproduce the issue.\r\n[tf_issue.zip](https://github.com/tensorflow/tensorflow/files/4524528/tf_issue.zip)\r\n", "comments": ["@ChanZou I think you are doing the opposite way by feeding the image to tflite and normalized_image to TF. Could you tried it again? And provide what is differences and max error between them?", "@thaink Thank you for your reply. I have run the requested test and here are the results:\r\n```\r\nTF normalized; TFLite not normalized: Max absolute error: 0.11157090216875076. Example values tf: 4.685942649841309; tflite: 4.797513552010059; Number of 'max difference' within 2e-6 range: 225\r\nTF not normalized; TFLite normalized: Max absolute error: 24.880128353834152. Example values tf: -11.93799877166748; tflite: 12.942129582166672; Number of 'max difference' within 2e-6 range: 149\r\n```\r\nAnd two more just in case:\r\n```\r\nTF normalized; TFlite normalized: Max absolute error: 15.396671414375305. Example values tf: -10.04130744934082; tflite: 5.355363965034485; Number of 'max difference' within 2e-6 range: 1\r\nTF not normalized; TFLite not normalized: Max absolute error: 20.86360428482294. Example values tf: 16.512371063232422; tflite: -4.351233221590519; Number of 'max difference' within 2e-6 range: 1\r\n```\r\nI might be misunderstanding it but I thought what `quantized_input_stats` does is feeding normalization information into the input node of the TFLite model so that raw image value (uint8 in TFLite model) can be correctly mapped to normalized input value (float in TF model)", "Your first usage here is correct: \r\n\r\n    TF normalized; TFLite not normalized: Max absolute error: 0.11157090216875076. Example values tf: 4.685942649841309; tflite: 4.797513552010059; Number of 'max difference' within 2e-6 range: 225\r\n\r\nFakeQuant may have some difference if the model is not trained sufficiently long to allow the weights to learn about the quantization error.", "Here I am trying to compare Conv2D op in TF and TFLite given identical input and weights. In other words, if we are comparing 1. original TF model 2. fully quantization-aware trained TF model 3. converted TFLite model for 1, and 4. converted TFLite model for 2, we should expect differences between 1 and 2 (3 and 4 as well) but not between 1 and 3 (or 2 and 4), which is what I am showing in this example. Again, thank you for your reply!", "I see, thanks for the explanation.\r\n\r\nIt seems we need to trace down if the quantization error is coming from the weight, or the output quantization, or when tflite quantizes the bias to int32 during conversion, to see if this is fundamental to the implementation of rescale, or a bug in how rescale is being performed.\r\n\r\nWe will take a look and update the issue, thanks! ", "Hi ChanZou, is the virtualization actually reflects your model? I couldn't find the FakeQuant op for the weight. \r\n\r\nOn the other hand, the max abs output error is `0.11157090216875076`, which is very close to the scale value of the output tensor (0.1115700826048851). So it is possible due to the some rounding errors.", "Hi liufengdb, my bad not expanding scopes exhaustively, here is the updated one. As shown in the graph both weights and the activate are quantized. Do I need to attach a fakquant node to the bias as well? While it might be reasonable to do so, the converter did not do that for me.\r\n![Screenshot from 2020-05-04 17-04-12](https://user-images.githubusercontent.com/10414613/81014706-5e985180-8e2b-11ea-9194-f40e8f4af46a.png)\r\n\r\nAnd I agree that possibly deep down this might be a rounding error, but as NNs nowadays tend to be very deep, perturbation like this will very likely lead to catastrophic performance degradation, which we have observed on several models.", "Thanks ChanZhou, I was able to run your code. \r\n\r\nThanks Feng, yes the value of the output tensor is off b y 0.115.. which is exactly the value of the scale. So the output is off by 1 LSB for some values.\r\n\r\nInterestingly, I was able to reproduce this error when running a floating point model (Which has the FakeQuant operations in the TFLite graph), but the error occurred in fewer values.\r\n\r\n\r\n    [[ 0 72 96  3]]\r\n    Max absolute error: 0.11157035827636719. Example values tf: 1.561981201171875; tflite: \r\n    1.4504108428955078\r\n\r\nThis makes me believe that there is a discrepancy between the constant folding of FakeQuant in TensorFlow and the constant folding of FakeQuant in TensorFlow Lite, that results in a mismatch before quantization even happens.\r\n\r\nWill verify that theory and get back.\r\n", "ChanZou, I checked the value range of the bias and also the output range, it is [-11.9, 16.5]. I would suggest applying Relu6 or on the output to reduce this range. It should help the accuracy for deep models.", "Feng, thanks for the suggestion. This is a minimal example helping us to understand what shall we expect from TFLite down to the 'atomic' level. I understand narrower range for the activation will help with the precision of both bias and the activation itself. Yet we are still seeing quite a significant drop in regression type of tasks.", "I have verified that the different in the floating point graphs is due to the converter changing FakeQuant to Quantize and Dequantize. Resulting in numerical difference.\r\n\r\nThat being said, this is unrelated to your original issue, which after looking into I think is fundamental differences in rounding / rescaling in FakeQuant vs the actual tflite kernels. \r\n\r\nIt seems that this particular model is very prone to these differences, the most practical approach to resolving some of these issues would be to explore the following model-level changes (from simplest to try to more difficult):\r\n\r\n1) Try post-training quantization which uses per-channel quantization which may resolve some issues.\r\n2) Try using a model with restricted activation ranges (as Feng suggested). I have personally seen this work both ways, but since your model has a such a large range for activations this may be very helpful.\r\n3) Maybe try stochastic rounding in the FakeQuant operation. (this isn't in the code)\r\n\r\nTo help with suggestions, couple questions:\r\n\r\n1) Without this rounding error from FakeQuant to real quant are you getting sufficient accuracy in you FakeQuant float TF model?\r\n2) How far are you from your target accuracy for your problem with FakeQuant?", "To answer your questions:\r\n  1. Yes, we are getting very close performance when comparing fake-quantized TF model with the vanilla one (without fake-quantization nodes) so it is particularly surprising to see such a discrepancy at the 'final' step.\r\n  2. We are seeing up to ~30% performance drop in some tasks. Can you provide some insights on how often do you/your team see this scale of discrepancy?\r\n\r\nThank you for your suggestions, and a couple of questions/comments on them if you don't mind:\r\n  1. Is there a more specific reason why per-channel quantization help other than potentially narrower range? Does it share the same kernel with the per-layer quantization and hence suffer from the same issue?\r\n  2. As this is just a toy example, we have activations attached to most Conv2D, and still seeing errors accumulate into quite significant discrepancies across layers.\r\n\r\nAgain thank you for spending time on this.", "Couple more things to confirm.\r\n\r\nCan you confirm that your trained float model without FakeQuant can be converted to TFLite and doesn't have the 30% drop? Just for sanity check. :)", "Hey @ChanZou, \r\n\r\nWhat is the status of this issue? I also am seeing quite big amount of discrepancy between a quantized model and its TFLITE converted version. My environment is similar to your except that I am using Tensorflow 1.15.2 and Python 3.6. I use the `tflite_convert` command line tool. My input float model is a quantized aware trained model (hence with FakeQuantize nodes).  The command line I run is this:\r\n\r\n```\r\ntflite_convert \\\r\n  --graph_def_file=my_frozen_graph.pb \\\r\n  --output_file=my_tflite_model.tflite \\\r\n  --output_format=TFLITE \\\r\n  --input_shape=1,257,257,3 \\\r\n  --input_arrays=\"MobilenetV2/MobilenetV2/input\" \\\r\n  --output_arrays=\"MobilenetV2/expanded_conv/input\" \\\r\n  --inference_type=QUANTIZED_UINT8\\\r\n  --inference_input_type=QUANTIZED_UINT8 \\\r\n  --std_dev_values=128 \\\r\n  --mean_values=128 \\\r\n  --change_concat_input_ranges=true\r\n```\r\n\r\nSimilar to what you mentioned above, the TFLITE model has the quantization statistics for converting the quantized values to float (scale and bias). When I use this statistic to compare with the float model, there is a big difference, I am seeing maximum of 0.2 difference and this is the output of only one convolution block. \r\n\r\n", "Hi @AmKhG \r\nIn our case, the discrepancy for individual images remains 'arbitrary', that is, it is hard to conclude an upper or lower bound for the discrepancy. However, if looking at the aggregated stats (mAP) for the test set as a whole, the performance will not be having a siginificant drop. Intuitively this might have something to do with the ergodicity. Hope this help.", "@ChanZou  Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38845\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38845\">No</a>\n"]}, {"number": 38844, "title": "[tf.data] Memory-safe implementation of sharing access to the memory \u2026", "body": "\u2026cache.\r\n\r\nPiperOrigin-RevId: 307736215\r\nChange-Id: If10ef65e6706a106e6bb4fc2d6fe4542bbe056cc\r\n\r\nfixes #38655", "comments": []}, {"number": 38843, "title": "Fixes typo in the comments for StrategyBase", "body": "building **`an`** executing -> building **`and`** executing", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38843) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38843) for more info**.\n\n<!-- ok -->", "We will not be encouraging one liner grammatical changes as this is expensive process, if possible please include more such changes in a single PR.Thank you\r\nCC @mihaimaruseac "]}, {"number": 38842, "title": "ValueError: Could not interpret optimizer identifier (tf.keras) ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- I have written a custom callback for learning_rate_scheduler in keras\r\n- Code is running on Google Colaboratory:\r\n- TensorFlow version (use command below): tensorflow 2.2.0-rc3.\r\nI need to be able to set and get my learning_rate and other params in my optimizer,\r\n\r\n I need to be able to use the constructor of optimizer to set the parameters in it\r\n\r\nUsed the sample code in the keras documentation \r\n\"Issue Reproducing steps\"\r\n1. Run this code in Google colab\r\nfrom keras import optimizers\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))\r\nmodel.add(Activation('softmax'))\r\n\r\nsgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\r\nmodel.compile(loss='mean_squared_error', optimizer=sgd)\r\n2. Throws Deserialization error\r\n3. Attaching the screenshot of the erro\r\n4.Please help with a resolution/workaround for this issue, as i am working on a critical course assignment which I need to submit soon .\r\n![error 2020-04-23 222914](https://user-images.githubusercontent.com/21074002/80129974-824ed280-85b5-11ea-9eac-5dd4791e8d6a.jpg)\r\n\r\nThanks\r\n\r\n\r\n", "comments": ["I have tried the above and have got no error. Please have a look at this [link](https://colab.research.google.com/gist/oke-aditya/b6f1cc3c292e1703b4e878f8748282d4/test.ipynb)", "Maybe there is import issue or I have just changed the loss argument to \"mse\" (mean_squared_error).", "@jayaBalaR \r\ni ran the code shared in nightly and there are no errors, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/daa6959f58f5a3701e17c398077a56a8/38842-ipynbpip-install-tf_nightly.ipynb)\r\nplease confirm if the code provided is complete.", "Hi @Saduf2019 , @oke-aditya \r\n\r\nThank you for the quick resolution.\r\nIt was an import error. \r\nI am a novice programmer to Keras/Tensorflow . So I had to refer the Keras API documentation to code. \r\nAfter referring the documentation for optimizers section as detailed above, there was \r\nfrom keras import optimizers\r\nand used the same import.\r\n\r\nCould you please help to update documentation with the right import , so it could help other novice programmers not stuck with their development due to erroneous import .\r\n\r\nThank you for the resolution.\r\n\r\n", "Yes. Can you please provide link where it needs to be fixed? ", "Hi\r\n\r\nThis is the link https://keras.io/optimizers/\r\nRefer the first section of code for optimizers.\r\n\r\nThanks once again.", "Tensorflow 2 uses Keras as high level API for model prototyping. Keras by itself is a different package. \n\nKeras API has their own documentation which is correct for their package. \n\nTensorflow has its own documentation under tf.keras. Documentation issue of keras.io should be raised under Keras GitHub repository.\n\nPlease refer to tensorflow docs [docs link](https://www.tensorflow.org/overview) It would be easier to work with tensorflow. Mixing Keras docs with tensorflow while learning can lead confusion \ud83d\ude05. \n\nAll the best. We all are new someday to Deep Learning. You will master it soon. \n\n", "Okay thank you for the link  and for the clarification.\r\n\r\n", "@jayaBalaR \r\nplease let us know if this is still an issue ", "@Saduf2019 . So far resolved. Thank you", "moving to resolved status with confirmation", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38842\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38842\">No</a>\n"]}, {"number": 38841, "title": "NotFoundError: No registered 'AssignSubVariableOp' OpKernel for 'GPU' devices compatible with node {{node AssignSubVariableOp}}", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n`no`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n`windows10 x64`\r\n- TensorFlow installed from (source or binary):\r\n`anaconda`\r\n- TensorFlow version (use command below):\r\n`2.1.0`\r\n- Python version:\r\n`3.7.7`\r\n- CUDA/cuDNN version:\r\n`10.1 update2`\r\n- GPU model and memory:\r\n`gtx1050ti 4GB`\r\n\r\nimport tensorflow as tf\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n\r\nresult:unknown 2.1.0\r\n\r\n**Describe the current behavior**\r\nsee the code \r\n\r\n**Describe the expected behavior**\r\nsee the error\r\n\r\n**Standalone code to reproduce the issue**\r\ncmd\uff1a\r\n```\r\nconda create -n tf2g python=3.7\r\nconda activate tf2g\r\nconda install tensorflow-gpu spyder\r\nspyder\r\n```\r\nspyder:\r\n```\r\nimport tensorflow as tf\r\n\r\nepoch = 2\r\n\r\nx = tf.Variable([[0. + 0.j]], tf.complex128)\r\ny = tf.constant([[1. + 1.j]], tf.complex128)\r\n\r\n#variables = [x]\r\n\r\nfor i in range(epoch):\r\n    with tf.GradientTape() as tape:\r\n        loss_value = (x - y)\r\n    #print(loss_value)\r\n    grad = tape.gradient(loss_value, x)\r\n    #print(grad)\r\n    x.assign_sub(0.1 * tf.math.conj(grad))\r\n\r\nprint(epoch)\r\n```\r\ndebug:\r\n```\r\nrunfile('E:/Projects/Python/tensorflow/temp/004/001.py', wdir='E:/Projects/Python/tensorflow/temp/004')\r\nTraceback (most recent call last):\r\n\r\n  File \"E:\\Projects\\Python\\tensorflow\\temp\\004\\001.py\", line 23, in <module>\r\n    x.assign_sub(0.1 * tf.math.conj(grad))\r\n\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\tf2g\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 762, in assign_sub\r\n    name=name)\r\n\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\tf2g\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_resource_variable_ops.py\", line 102, in assign_sub_variable_op\r\n    _ops.raise_from_not_ok_status(e, name)\r\n\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\tf2g\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 6606, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n\r\n  File \"<string>\", line 3, in raise_from\r\n\r\nNotFoundError: No registered 'AssignSubVariableOp' OpKernel for 'GPU' devices compatible with node {{node AssignSubVariableOp}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: dtype=DT_COMPLEX128\r\n\t.  Registered:  device='CPU'; dtype in [DT_INT64]\r\n  device='CPU'; dtype in [DT_INT32]\r\n  device='CPU'; dtype in [DT_UINT16]\r\n  device='CPU'; dtype in [DT_INT16]\r\n  device='CPU'; dtype in [DT_UINT8]\r\n  device='CPU'; dtype in [DT_INT8]\r\n  device='CPU'; dtype in [DT_HALF]\r\n  device='CPU'; dtype in [DT_BFLOAT16]\r\n  device='CPU'; dtype in [DT_FLOAT]\r\n  device='CPU'; dtype in [DT_DOUBLE]\r\n  device='CPU'; dtype in [DT_COMPLEX64]\r\n  device='CPU'; dtype in [DT_COMPLEX128]\r\n  device='GPU'; dtype in [DT_HALF]\r\n  device='GPU'; dtype in [DT_FLOAT]\r\n  device='GPU'; dtype in [DT_DOUBLE]\r\n  device='GPU'; dtype in [DT_INT64]\r\n [Op:AssignSubVariableOp]\r\n```\r\n", "comments": ["but the is no wrong with \r\n`conda install tensorflow spyder`\r\n\r\n(not tensorflow-gpu)\r\n", "@hustarbor \r\n\r\nSorry, but we don't provide support for issues with the conda environment.This issue is more suitable for anaconda-issues repo. Please post it on anaconda-issues repo from [here](https://github.com/ContinuumIO/anaconda-issues/issues).Thanks!", "> \r\n> \r\n> @hustarbor\r\n> \r\n> Sorry, but we don't provide support for issues with the conda environment.This issue is more suitable for anaconda-issues repo. Please post it on anaconda-issues repo from [here](https://github.com/ContinuumIO/anaconda-issues/issues).Thanks!\r\n\r\nSorry, I think this may be the difference between tensorflow and tensorflow-gpu", "I have tried in colab with TF 2.1 and was able to reproduce the issue. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/83f2cf35d7651673bb51c312e34676a8/untitled825.ipynb).Thanks!", "> \r\n> \r\n> I have tried in colab with TF 2.1 and was able to reproduce the issue. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/83f2cf35d7651673bb51c312e34676a8/untitled825.ipynb).Thanks!\r\n\r\ntf.Variable.assign_sub can't run on tensorflow-gpu", "@hustarbor Yes thats the issue. In order to implement this op on your own on GPU please go through the following [doc](https://www.tensorflow.org/guide/create_op#gpu_kernels)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this issue as it has been inactive for more than 2 weeks. Please add additional comments for us to pen this issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38841\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38841\">No</a>\n", "The same issue exists in ``tensorflow-macos`` since it's also based on GPU. Workaround here.\r\n\r\nReplace:\r\n\r\n```python\r\ntf.assign_sub(a, b)\r\n```\r\n\r\nWith:\r\n\r\n```python\r\na.assign(tf.subtract(a, b))\r\n```"]}, {"number": 38840, "title": "Build error: No matching toolchains on ppc64le", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ppc64le, CentOS 7 manylinux 2014 container\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master branch\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 3.0.0\r\n- GCC/Compiler version (if compiling from source): 8.3.1\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\n\r\nThis commit: https://github.com/tensorflow/tensorflow/commit/f5bb64336039646cae91884f7177bdcb6f29a7e2, broke the build on ppc64le.\r\n\r\nbuilds on ppc64le have been working fine until that commit. To do a build with that commit, I must modify `third_party/remote_config/BUILD.tpl` to replace with `platforms:x86_64` with `platforms:ppc`.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\n./configure (accept all defaults)\r\nbazel build -c opt --config=v2 --local_cpu_resources 4 --local_ram_resources 4096 //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nbuild fails with:\r\n```\r\nERROR: While resolving toolchains for target //tensorflow/tools/build_info:gen_build_info: No matching toolchains found for types @bazel_tools//tools/cpp:toolchain_type. Maybe --incompatible_use_cc_configure_from_rules_cc has been flipped and there is no default C++ toolchain added in the WORKSPACE file? See https://github.com/bazelbuild/bazel/issues/10134 for details and migration instructions.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: No matching toolchains found for types @bazel_tools//tools/cpp:toolchain_type. Maybe --incompatible_use_cc_configure_from_rules_cc has been flipped and there is no default C++ toolchain added in the WORKSPACE file? See https://github.com/bazelbuild/bazel/issues/10134 for details and migration instructions.\r\n```\r\n\r\nif I add `--toolchain_resolution_debug` to the command line:\r\n\r\n`bazel build -c opt --config=v2 --local_cpu_resources 4 --local_ram_resources 4096 --toolchain_resolution_debug //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nI get this additional debug:\r\n```\r\nINFO: Build option --toolchain_resolution_debug has changed, discarding analysis cache.\r\nINFO: ToolchainResolution: Selected execution platform @local_execution_config_platform//:platform,\r\nINFO: ToolchainResolution: Looking for toolchain of type @bazel_tools//tools/cpp:toolchain_type...\r\nINFO: ToolchainResolution:   Considering toolchain @local_config_cc//:cc-compiler-armeabi-v7a...\r\nINFO: ToolchainResolution:     Toolchain constraint @platforms//cpu:cpu has value @platforms//cpu:arm, which does not match value @platforms//cpu:ppc from the target platform @local_config_platform//:host\r\nINFO: ToolchainResolution:     Toolchain constraint @platforms//os:os has value @platforms//os:android, which does not match value @platforms//os:linux from the target platform @local_config_platform//:host\r\nINFO: ToolchainResolution:   Rejected toolchain @local_config_cc//:cc-compiler-armeabi-v7a, because of target platform mismatch\r\nINFO: ToolchainResolution:   Considering toolchain @local_config_cc//:cc-compiler-ppc...\r\nINFO: ToolchainResolution:     Toolchain constraint @platforms//cpu:cpu has value @platforms//cpu:ppc, which does not match value @platforms//cpu:x86_64 from the execution platform @local_execution_config_platform//:platform\r\nINFO: ToolchainResolution:   For toolchain type @bazel_tools//tools/cpp:toolchain_type, possible execution platforms and toolchains: {@local_config_platform//:host -> @local_config_cc//:cc-compiler-ppc}\r\nINFO: ToolchainResolution: Looking for toolchain of type @bazel_tools//tools/python:toolchain_type...\r\nINFO: ToolchainResolution: Looking for toolchain of type @bazel_tools//tools/cpp:toolchain_type...\r\nINFO: ToolchainResolution: Looking for toolchain of type @bazel_tools//tools/python:toolchain_type...\r\nINFO: ToolchainResolution:   Considering toolchain @local_execution_config_python//:py_runtime_pair...\r\nINFO: ToolchainResolution: Selected execution platform @local_execution_config_platform//:platform,\r\nINFO: ToolchainResolution:     Toolchain constraint @local_execution_config_platform//:platform_setting has value @local_execution_config_platform//:platform_constraint, which does not match value <missing> from the target platform @local_config_platform//:host\r\nINFO: ToolchainResolution:   Rejected toolchain @local_execution_config_python//:py_runtime_pair, because of target platform mismatch\r\nINFO: ToolchainResolution:   Considering toolchain @local_config_cc//:cc-compiler-armeabi-v7a...\r\nINFO: ToolchainResolution:   Considering toolchain @local_execution_config_python//:py_runtime_pair...\r\nINFO: ToolchainResolution:     Toolchain constraint @platforms//cpu:cpu has value @platforms//cpu:arm, which does not match value @platforms//cpu:ppc from the target platform @local_config_platform//:host\r\nINFO: ToolchainResolution:   Considering toolchain @local_config_python//:py_runtime_pair...\r\nINFO: ToolchainResolution: Selected execution platform @local_config_platform//:host, type @bazel_tools//tools/cpp:toolchain_type -> toolchain @local_config_cc//:cc-compiler-ppc\r\nINFO: ToolchainResolution:     Toolchain constraint @platforms//os:os has value @platforms//os:android, which does not match value @platforms//os:linux from the target platform @local_config_platform//:host\r\nINFO: ToolchainResolution:     Toolchain constraint @local_execution_config_platform//:platform_setting has value @local_execution_config_platform//:platform_constraint, which does not match value <missing> from the target platform @local_config_platform//:host\r\nINFO: ToolchainResolution:   Rejected toolchain @local_config_cc//:cc-compiler-armeabi-v7a, because of target platform mismatch\r\nINFO: ToolchainResolution:   Considering toolchain @bazel_tools//tools/python:_autodetecting_py_runtime_pair...\r\nINFO: ToolchainResolution:   Considering toolchain @local_config_cc//:cc-compiler-ppc...\r\nINFO: ToolchainResolution:   Rejected toolchain @local_execution_config_python//:py_runtime_pair, because of target platform mismatch\r\nINFO: ToolchainResolution:     Toolchain constraint @platforms//cpu:cpu has value @platforms//cpu:ppc, which does not match value @platforms//cpu:x86_64 from the execution platform @local_execution_config_platform//:platform\r\nINFO: ToolchainResolution:   For toolchain type @bazel_tools//tools/python:toolchain_type, possible execution platforms and toolchains: {@local_execution_config_platform//:platform -> @local_config_python//:py_runtime_pair, @local_config_platform//:host -> @local_config_python//:py_runtime_pair}\r\nINFO: ToolchainResolution: Selected execution platform @local_execution_config_platform//:platform,\r\nINFO: ToolchainResolution:   For toolchain type @bazel_tools//tools/cpp:toolchain_type, possible execution platforms and toolchains: {@local_config_platform//:host -> @local_config_cc//:cc-compiler-ppc}\r\nINFO: ToolchainResolution: Selected execution platform @local_config_platform//:host, type @bazel_tools//tools/cpp:toolchain_type -> toolchain @local_config_cc//:cc-compiler-ppc\r\nINFO: ToolchainResolution:   Considering toolchain @local_config_python//:py_runtime_pair...\r\nINFO: ToolchainResolution: Looking for toolchain of type @bazel_tools//tools/cpp:toolchain_type...\r\nINFO: ToolchainResolution: Selected execution platform @local_config_platform//:host, type @bazel_tools//tools/cpp:toolchain_type -> toolchain @local_config_cc//:cc-compiler-ppc, type @bazel_tools//tools/python:toolchain_type -> toolchain @local_config_python//:py_runtime_pair\r\nINFO: ToolchainResolution: Removed execution platform @local_config_platform//:host from available execution platforms, it is missing constraint @local_execution_config_platform//:platform_constraint\r\nINFO: ToolchainResolution:   Considering toolchain @local_config_cc//:cc-compiler-armeabi-v7a...\r\nINFO: ToolchainResolution:     Toolchain constraint @platforms//cpu:cpu has value @platforms//cpu:arm, which does not match value @platforms//cpu:x86_64 from the target platform @local_execution_config_platform//:platform\r\nINFO: ToolchainResolution:   Considering toolchain @bazel_tools//tools/python:_autodetecting_py_runtime_pair...\r\nINFO: ToolchainResolution:     Toolchain constraint @platforms//os:os has value @platforms//os:android, which does not match value @platforms//os:linux from the target platform @local_execution_config_platform//:platform\r\nINFO: ToolchainResolution:   Rejected toolchain @local_config_cc//:cc-compiler-armeabi-v7a, because of target platform mismatch\r\nINFO: ToolchainResolution: Looking for toolchain of type @bazel_tools//tools/python:toolchain_type...\r\nINFO: ToolchainResolution: Looking for toolchain of type @bazel_tools//tools/cpp:toolchain_type...\r\nINFO: ToolchainResolution:   Considering toolchain @local_config_cc//:cc-compiler-armeabi-v7a...\r\nINFO: ToolchainResolution:   Considering toolchain @local_execution_config_python//:py_runtime_pair...\r\nINFO: ToolchainResolution:   Considering toolchain @local_config_cc//:cc-compiler-ppc...\r\nINFO: ToolchainResolution:   For toolchain type @bazel_tools//tools/python:toolchain_type, possible execution platforms and toolchains: {@local_execution_config_platform//:platform -> @local_config_python//:py_runtime_pair, @local_config_platform//:host -> @local_config_python//:py_runtime_pair}\r\nINFO: ToolchainResolution:     Toolchain constraint @platforms//cpu:cpu has value @platforms//cpu:ppc, which does not match value @platforms//cpu:x86_64 from the target platform @local_execution_config_platform//:platform\r\nINFO: ToolchainResolution:   Considering toolchain @local_config_python//:py_runtime_pair...\r\nINFO: ToolchainResolution:     Toolchain constraint @platforms//cpu:cpu has value @platforms//cpu:arm, which does not match value @platforms//cpu:x86_64 from the target platform @local_execution_config_platform//:platform\r\nINFO: ToolchainResolution: Selected execution platform @local_config_platform//:host, type @bazel_tools//tools/cpp:toolchain_type -> toolchain @local_config_cc//:cc-compiler-ppc, type @bazel_tools//tools/python:toolchain_type -> toolchain @local_config_python//:py_runtime_pair\r\nINFO: ToolchainResolution:   Considering toolchain @bazel_tools//tools/python:_autodetecting_py_runtime_pair...\r\nINFO: ToolchainResolution:   Rejected toolchain @local_config_cc//:cc-compiler-ppc, because of target platform mismatch\r\nINFO: ToolchainResolution:   No toolchains found\r\nINFO: ToolchainResolution:   For toolchain type @bazel_tools//tools/python:toolchain_type, possible execution platforms and toolchains: {@local_execution_config_platform//:platform -> @local_execution_config_python//:py_runtime_pair}\r\nINFO: ToolchainResolution:     Toolchain constraint @platforms//os:os has value @platforms//os:android, which does not match value @platforms//os:linux from the target platform @local_execution_config_platform//:platform\r\n```\r\n\r\n", "comments": ["The commit has been rolledback in 40d3f089", "Does $MACHTYPE on your machine start with \"ppc\"? (trying to make sure to not break this use case on roll-forward)", "This commit also broke s390x arch builds.", "For ppc64le, $MACHTYPE starts with powerpc64le for both ubuntu and rhel host.\r\n\r\n```\r\n$ echo $MACHTYPE\r\npowerpc64le-unknown-linux-gnu\r\n\r\n$ echo $MACHTYPE\r\npowerpc64le-redhat-linux-gnu\r\n```\r\n\r\n@rposts , please provide the same information for s390x. ", "Here is s390x output:\r\n```\r\n# echo $MACHTYPE\r\ns390x-ibm-linux-gnu\r\n```", "I can confirm this issue is now fixed. Thank you! You may close this issue, if your not using it to track anything. ", "Closing as issue has been fixed. Thanks for confirming.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38840\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38840\">No</a>\n", "My roll-forward didn't fully address this yet - will work on another patch asap.", "https://github.com/tensorflow/tensorflow/commit/889b322261384c90ac165ddd1e8bf2944b3e7785\r\nshould address the cases outlined here so far - please let me know if that doesn't work.", "unfortunately this is still broke, I'm trying to identify why it is failing.\r\n\r\nI reviewed all the changes and they look fine to me.\r\n\r\nFYI, the builds are online here: https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/\r\nlinked from: https://github.com/tensorflow/tensorflow/#community-supported-builds", "I think the issue is MACHTYPE is a built in bash variable and not an environment variable.\r\n\r\nSo echo $MACHTYPE works, but printenv MACHTYPE doesn't. \r\n```\r\n# echo $MACHTYPE\r\npowerpc64le-redhat-linux-gnu\r\n# printenv MACHTYPE\r\n#\r\n```\r\n", "For now to get the builds to work I'm doing `export MACHTYPE=$MACHTYPE` before the build starts.", "Does this work for you?\r\n\r\n```\r\ndiff --git a/third_party/remote_config/remote_platform_configure.bzl \r\nb/third_party/remote_config/remote_platform_configure.bzl\r\nindex 185120f762..5122d090ae 100644\r\n--- a/third_party/remote_config/remote_platform_configure.bzl\r\n+++ b/third_party/remote_config/remote_platform_configure.bzl\r\n@@ -12,13 +12,12 @@ def _remote_platform_configure_impl(repository_ctx):\r\n             platform = \"linux\"\r\n\r\n     cpu = \"x86_64\"\r\n-    if \"MACHTYPE\" in repository_ctx.os.environ:\r\n-        machine_type = repository_ctx.os.environ[\"MACHTYPE\"]\r\n-        if (machine_type.startswith(\"ppc\") or\r\n-            machine_type.startswith(\"powerpc\")):\r\n-            cpu = \"ppc\"\r\n-        elif machine_type.startswith(\"s390x\"):\r\n-            cpu = \"s390x\"\r\n+    machine_type = repository_ctx.execute(['bash', '-c', 'echo $MACHTYPE']).stdout\r\n+    if (machine_type.startswith(\"ppc\") or\r\n+        machine_type.startswith(\"powerpc\")):\r\n+        cpu = \"ppc\"\r\n+    elif machine_type.startswith(\"s390x\"):\r\n+        cpu = \"s390x\"\r\n\r\n     exec_properties = repository_ctx.attr.platform_exec_properties\r\n```", "@r4nt, yes the above patch does work for me. Thank you.", "Running it through presubmits now.", "This fix has been merged and is working great. Thank you @r4nt . Closing issue now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38840\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38840\">No</a>\n"]}, {"number": 38839, "title": "tf.repeat AttributeError: module 'tensorflow' has no attribute 'repeat' on versions 2.0.0 and 2.0.1", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Tried on Windows 10 and on Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): tried 2.0.0 and 2.0.1\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\n>>> tf.repeat(['a', 'b', 'c'], repeats=[3, 0, 2], axis=0)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'repeat'\r\n```\r\n**Describe the expected behavior**\r\nI found that this works on 2.1.0 and 1.15.2\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["It works in tf 2.2 as well. Also as you mentioned in tf 2.1 and 1.15 as well. \r\nIt does not work in 2.0.1. I checked it. Here is gist to reproduce error. [error_test](https://colab.research.google.com/gist/oke-aditya/d7d29ab4362dbb67fff7149bcc795390/test2.ipynb)", "@miguelgfierro\r\nwe ran your code over nightly and it does not have any errors,please find the[ gist here](https://colab.sandbox.google.com/gist/Saduf2019/a54afe6ca3b701d71af0432083c642df/38839.ipynb)\r\nis there any particular reason to run on 2.0.1, can you upgrade to later versions.", "> is there any particular reason to run on 2.0.1, can you upgrade to later versions.\r\n\r\nI can definitely use other versions, up to you :-0. Just FYI, at the moment, version 2.0.1 and 2.0.0 don't work. Furthermore, 2.1.0 has `repeat` and work in Linux, however, if I try to install 2.1.0 on my local windows laptop, it fails. \r\n\r\nThe summary is that, given that I only use stable releases, from the recent versions I can only use 1.15.2. I'll do that for the time being.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38839\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38839\">No</a>\n"]}, {"number": 38838, "title": " 'module' object has no attribute 'tf'", "body": "its part of my code:\r\n\r\n```\r\n    def call(self, inputs):\r\n        s = K.tf.gather(inputs[0], self.sel_inds[0], axis=1)\r\n        p = K.tf.gather(inputs[1], self.sel_inds[1], axis=1)\r\n        o = K.tf.gather(inputs[2], self.sel_inds[2], axis=1)\r\n        return s*p*o\r\n```\r\n\r\nand then : \r\n\r\n```\r\n  File \"/content/VidVRD-helper-master/baseline/model.py\", line 177, in call\r\n    s = K.tf.gather(inputs[0], self.sel_inds[0], axis=1)\r\nAttributeError: 'module' object has no attribute 'tf'\r\n```\r\n\r\nI use tensorflow=1.8.0 python=2.7\r\nwhat should I do for fix it?", "comments": ["TensorFlow 1.8 is too old. Please switch to 1.15, 2.0, 2.1 or the rc for 2.2.\r\n\r\nIf the problem still manifests, please fill in issue template and show output of `pip list`\r\n\r\nWhen posting code, please use proper markdown formatting.", "You also seem to get TF from keras (`K`). This is no longer a pattern that's recommended.", "> You also seem to get TF from keras (`K`). This is no longer a pattern that's recommended.\r\n\r\nis it correct?\r\n\r\n       def call(self, inputs):\r\n        s = tf.gather(inputs[0], self.sel_inds[0], axis=1)\r\n        p = tf.gather(inputs[1], self.sel_inds[1], axis=1)\r\n        o = tf.gather(inputs[2], self.sel_inds[2], axis=1)\r\n        return s*p*o", "@rezamansourian77 \r\n\r\nWill it be possible to share simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38838\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38838\">No</a>\n"]}, {"number": 38837, "title": "Bad accuracy results after training speech_commands example", "body": "I trained two models with 1 wanted word (\"help\") as in this [example](https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md). I've had 500 samples of my word, in right format and prepared for hashing function. First model was trained with arguments `--silence_percentage=10 --unknown_percentage=10`, second with `--silence_percentage=30 --unknown_percentage=30`. \r\nOutput of `test_streaming_accuracy` gives following results: \r\n1st model - `78.0% matched, 35.5% correctly, 42.5% wrongly, 34.5% false positives`\r\n2nd model -`85.5% matched, 37.0% correctly, 48.5% wrongly, 24.0% false positives`\r\nIs it because of small number of samples?", "comments": ["I repeated the training with default parameters and wanted word LEFT:\r\n`75.5% matched, 72.5% correctly, 3.0% wrongly, 4.5% false positives`, so the problem is with my training data\r\n"]}]