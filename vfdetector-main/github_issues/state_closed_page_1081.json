[{"number": 20843, "title": "Performance decrease in TensorFlow 1.9 for large graphs", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9.0/1.8.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: GTX 980 Ti, 6GB\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport time\r\n\r\nimport tensorflow as tf\r\n\r\nx = tf.zeros((1, 10))\r\n\r\nfor i in range(5000):\r\n    x = tf.layers.dense(x, units=10)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    print(\"a\")\r\n    start = time.time()\r\n    sess.run(x)\r\n    print(time.time() - start)\r\n\r\n    print(\"b\")\r\n    start = time.time()\r\n    sess.run(x)\r\n    print(time.time() - start)\r\n```\r\n\r\n### Describe the problem\r\nOn TensorFlow 1.9, this shows:\r\n```\r\na\r\n17.57940149307251\r\nb\r\n0.09075760841369629\r\n```\r\nOn TensorFlow 1.8, this shows:\r\n```\r\na\r\n11.689541101455688\r\nb\r\n0.08776521682739258\r\n```\r\nSo roughly a 50% increase in run time in TensorFlow 1.9.  Note that this only occurs on the first call to `sess.run` (\"a\" rather than \"b\"), so I suspect this is some change in the graph optimization step.  So, a couple questions:\r\n\r\n1. What changed?  I can't find anything obviously related to this in the release notes.\r\n2. Is this intended/expected (e.g., some new graph optimization steps were added that increase the initial time, but provide benefits in the long run)?  Or is it just a bug?\r\n3. If it is intended, is there some way to optionally disable this new feature if we find that initial build time is dominating the run time?\r\n\r\n", "comments": ["I did a git bisect to figure out where the slowdowns occurred, and found 3 commits that contributed to the decreased performance:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/2c105ace934edce193669b55b13b64283caa24d7\r\n\r\nRuns the graph optimizer pass twice (up from previous value of once), so makes sense that this would take longer.  Can be disabled via `config.graph_options.rewrite_options.meta_optimizer_iterations=RewriterConfig.ONE`.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/7e5d24de55572741f10ee7e17247ca195eaadfae\r\n\r\nTurns on three new graph optimization steps: dead branch elimination, shape optimization, and remapping.  Shape optimization and remapping can be disabled via `config.graph_options.rewrite_options.shape_optimization/remapping=RewriterConfig.OFF`, but I don't think there is any way to selectively disable dead branch elimination (other than turning off all loop optimizations).\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/2273b62a769aa477f8d2ef02ca7dee253b8ea7b0\r\n\r\nThis adds some new logic to the `SymbolicShapeRefiner`, which is what is slowing things down I'm guessing.  I don't think there is any way to disable this.\r\n\r\nNote: all three commits are by @benoitsteiner, so he may be able to provide more detail.\r\n", "I'm experiencing a fairly large slowdown too (on Windows 7 and Python 3.6; CPU) with the release of TensorFlow 1.9 and 1.10. For example, in 1.8 and versions prior, each evaluation step for the [CNN MNIST example model](https://www.tensorflow.org/tutorials/estimators/cnn) takes an average of 13 seconds while in 1.9 and 1.10, it trains for an average of 380 seconds. I hope this is still getting looked over.", "I have the same issues after upgrading to tensorflow 1.9, 1.10 or nightly(1.11.0-rc1 e4c4b20) with docker images with Tesla V100 GPU. The global_step/sec slow down from 0.96 to 0.73 with the nasnet-large network trained with a customised version of [tensorflow/models/research/slim](https://github.com/tensorflow/models/tree/master/research/slim)  \r\n\r\nDisable `shape_optimization`, `remapping` and reduce `meta_optimizer_iterations` back to one  according to findings of @drasmuss does **not improve** the performance.", "@drasmuss  Thanks for your interesting investigation. Can you please test this for latest version of TensorFlow and post your findings?. We would like to know if this has been taken care of by latest version.", "I had the hope that improvements of `SymbolicShapeRefiner` in commit 9f847cb in the latest nightly fix the issue, but nasnet-large in training is still slow (0.96 -> 0.72 global_step/sec). ", "Performance is the same (or slightly worse) on 1.11.0:\r\n\r\n```\r\na\r\n18.11762547492981\r\nb\r\n0.08776497840881348\r\n```", "CC @rmlarsen, in case Rasmus has more context about the Grappler changes in https://github.com/tensorflow/tensorflow/issues/20843#issuecomment-406702409\r\n\r\n\r\n\r\n", "I see following computation times for various TensorFlow versions and they look pretty descent to me:\r\n\r\nTensorFlow Version 1.10 - \r\na\r\n14.6700410843\r\nb\r\n0.0304279327393\r\n\r\nTensorFlow Version 1.11 - \r\na\r\n14.541062355041504\r\nb\r\n0.0331730842590332\r\n\r\nTensorFlow Version 1.12.0 -\r\na\r\n13.787318706512451\r\nb\r\n0.03347063064575195", "The slowdown occurred in version 1.9, so you need to compare 1.9 to 1.8 if you want to see the difference (all the versions you are comparing are post-slowdown).", "I talked with @rmlarsen and he mentioned that the regression in https://github.com/tensorflow/tensorflow/commit/2273b62a769aa477f8d2ef02ca7dee253b8ea7b0 has been fixed in later versions but later versions also introduced a few more optimizations.\r\n\r\nI don't know details of your use-case but it is possible to disable all graph optimizations with the disable_meta_optimizer option and the time spent on optimizations can be restricted with the meta_optimizer_timeout_ms option.\r\n\r\nAnother approach could be to use a dummy \"warm-up requests\" as described in https://github.com/tensorflow/serving/issues/385#issuecomment-290443584.", "I created an [example script on gist](https://gist.github.com/DavidWiesner/c6af3238a90b3a8b1209543a9f8d7127#file-flowers-nasnet-sh) with tensorflow models and the large nasnet. This script were run on the official tensorflow docker images.\r\nOn my Tesla V100 for tensorflow 1.8-devel-gpu-py3 I got around \r\n`INFO:tensorflow:global_step/sec: 1.33334`\r\nWith tensorflow 1.9-devel-gpu-py3 a maximum at\r\n`INFO:tensorflow:global_step/sec: 0.89946`\r\n\r\n\r\n", "> I talked with @rmlarsen and he mentioned that the regression in 2273b62 has been fixed in later versions but later versions also introduced a few more optimizations.\r\n\r\nThat's good, that was the main one I was worried about since there was no way to disable it.\r\n\r\nIn general, is there any documentation or information you can provide on the benefits of the various graph optimizations, or tradeoffs between their optimization time versus run time improvement?  In my applications I've seen the optimization time increase, but there hasn't been a corresponding improvement in the run time.  But I'm reluctant to disable the graph optimizations, because I'm not sure in which situations they might provide benefits I'm not seeing, or whether they might provide more benefit in future releases.", "@DavidWiesner That seems like a different issue to me and not related to the graph optimization overhead. If the regression is present in the latest release as well, consider filing a separate issue for that.", "Nagging Assignees @sherryy, @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue as longer optimization time is expected with the presence of additional graph optimizations."]}, {"number": 20842, "title": "BUG: tf.keras.layers and tf.REUSE.AUTO fail to share parameters in 1.9.0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\nv1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nrun MWE under 1.9.0, which works fine for 1.8.0\r\n\r\n### Describe the problem\r\nI think a bug was introduced, where `tf.keras.layers.Conv3D` in context of `tf.REUSE.AUTO` no longer works as intended (.e.g no parameter-sharing is performed).\r\nThis might be caused by the bugfix mentioned in the [release notes](https://github.com/tensorflow/tensorflow/releases/tag/v1.9.0) (Using tf.keras.layers with custom variable scopes).\r\n\r\nThis bug seemingly does not affect the `tf.keras.layers.Lambda` layer, as outlined by the MWE, where `tf.contrib.layers.instance_norm` and `tf.layers.conv3d` share paramters as intended.\r\n\r\n### Source code / logs\r\nMWE which runs fine under 1.8.0 but fails to yield complete parameter sharing under 1.9.0.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nConv3D = tf.keras.layers.Conv3D\r\nLambda = tf.keras.layers.Lambda\r\nActivation = tf.keras.layers.Activation\r\ntf_instance_norm = tf.contrib.layers.instance_norm    \r\n\r\ndef Conv_block_keras(input_layer, n_filters, kernel=(3, 3, 3), \r\n               padding='same', strides=(1, 1, 1)):\r\n\r\n    with tf.variable_scope('conv_block'):\r\n        #layer = Conv3D(n_filters, kernel, padding=padding, strides=strides)(input_layer) # does not work\r\n        #layer = Lambda(lambda x: Conv3D(n_filters, kernel, padding=padding, strides=strides)(x))(input_layer) # does not work\r\n        layer = Lambda(lambda x: tf.layers.conv3d(inputs=x, filters=n_filters, padding=padding, strides=strides, kernel_size=kernel))(input_layer)\r\n        layer = Lambda(lambda x: tf_instance_norm(inputs=x, data_format='NCHW'))(layer)\r\n        return Activation('relu')(layer)\r\n\r\nwith tf.variable_scope('reuse', reuse=tf.AUTO_REUSE):\r\n    conv_1 = Conv_block_keras(tf.zeros(shape=[1,1]+[128]*3), 3)\r\n    vars_conv_1 = [x.name for x in tf.global_variables(scope='reuse')]\r\n    conv_2 = Conv_block_keras(tf.zeros(shape=[1,1]+[128]*3), 3)\r\n    vars_conv_2 = [x.name for x in tf.global_variables(scope='reuse')]\r\n    print(vars_conv_1 == vars_conv_2)\r\n```\r\n", "comments": ["Thanks for the code, I'm getting the same behavior on my end. It looks like `tf.keras` is incrementing the name of the layer unconditionally every time a layer of the same type is created. As per the release note you mentioned, this might be intended behavior, I'll check with other members of the team and get back. \r\n\r\nIn the meantime, one way to have variable sharing with `tf.keras` is to create the layer once and then call it multiple times:\r\n\r\n```python\r\nconv = tf.keras.Conv3D(32, (3, 3, 3))\r\n\r\nout = conv(x)\r\nout_2 = conv(y) # this will share variables with the first call\r\n```\r\n", "Confirmed that this is intended behavior going forward. Preferred way to reuse variables in `tf.keras` is to construct the object once and then call it multiple times", "Okay, but then shouldn't the name of the `Lambda` layer be incremented unconditionally as well?\r\nIt does not do so in the above MWE, and this then is ambiguous behavior to me, as `Lambda` should behave identical to `Conv3D` since both are `tf.keras.layers` layers", "The *name* of the ```Lambda``` layer is getting updated unconditionally, but ```Lambda``` layers are completely unaware of any variables you create inside of them, so the variable creation defaults to using ```variable_scope``` and thus gets reused. This is one of the reasons Keras recommends not creating variables or performing any stateful operations inside of ```Lambda``` layers. The best way to do that is to create your own ```Layer``` subclass ", "Got it thanks :)", "Okay, so I am still a bit confused of what the expected behavior is for `tf.keras.layers`, `tf.layers` and `variable_scopes` and `tf.get_variable` with regard to the future.\r\n\r\nAs of 1.9.0, the `tf.keras.layers.Layer` and all subclasses no longer allow for [variable reusing, but instead use a \"pared-down\"](https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/training/checkpointable/util.py#L193) version of `tf.get_variable`. Furthermore, the `tf.layers` as well as `tf.contrib.layers` still allow for reusing with `tf.variable_scopes` and perform all required checks.\r\n\r\nHowever, according to #14703, `tf.layers` is set to be removed in the future. \r\n\r\nI would be very delighted, if you could give a roadmap on what's the future of a `high-level layers API` for tensorflow (as additionally requested here #16182, where use of `tf.layers` together with estimators are suggested as best practice).", "```tf.keras.layers``` is the preferred high-level layers API for Tensorflow going forward. You're right, for historical reasons ```tf.layers``` still allows for reuse with ```tf.variable_scopes```, but Keras layers don't as of 1.9.0 and I don't expect they will going forward.\r\n\r\nSome roadmap decisions are still being made, most of the updates for these types of things are sent through our [developer mailing lists](https://www.tensorflow.org/community/lists), you can check out the link to subscribe\r\n\r\nHope that helps!", "Okay, one last thing:\r\nIs there a way to intermingle `tf.layers` and `tf.contrib.layers` with `tf.keras.layers` and get the expected `tf.keras.Model` behavior? Because i think this was broken upon in 1.9.0. I tried wrapping the above MWE by subclassing `tf.keras.Model()`. Code is attached, but expected behavior of `tf.keras`, which is parameter-sharing by sharing the `Model` object is not achieved.\r\n\r\nIn 1.8.0, it was possible to do this simply by wrapping `tf.layers` and `tf.contrib.layers` inside a `Lambda` layer, as I did in the initial comment.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nConv3D = tf.keras.layers.Conv3D\r\nLambda = tf.keras.layers.Lambda\r\nActivation = tf.keras.layers.Activation\r\ntf_instance_norm = tf.contrib.layers.instance_norm    \r\n\r\nclass MyModel(tf.keras.Model):\r\n\r\n  def __init__(self, n_filters=1, kernel=3, padding='same', strides=1):\r\n    super(MyModel, self).__init__(name='my_model')\r\n    self.conv = Conv3D(n_filters, kernel, padding=padding, strides=strides, data_format='channels_first')\r\n    self.instance_norm = Lambda(lambda x: tf_instance_norm(inputs=x, data_format='NCHW'))\r\n\r\n  def call(self, inputs):\r\n    x = self.conv(inputs)\r\n    x = self.instance_norm(x)\r\n    x = self.conv(x)\r\n    x = self.instance_norm(x)\r\n    return x\r\n\r\nwith tf.variable_scope('reuse', reuse=tf.AUTO_REUSE) as scope:\r\n\r\n    model = MyModel()\r\n    \r\n    outputs_1 = model(tf.zeros(shape=[1,1]+[128]*3))\r\n    vars_conv_1 = [x.name for x in tf.global_variables(scope='reuse')]\r\n    outputs_2 = model(tf.zeros(shape=[1,1]+[128]*3))\r\n    vars_conv_2 = [x.name for x in tf.global_variables(scope='reuse')]\r\n    print(vars_conv_1)\r\n    print(vars_conv_2)\r\n```", "So the new practice is to define layers and then call them with the inputs. All further calls to the same instance of a layer results in underlying variables being reused.\r\nBut I'd like to use predefined tf.keras models (e.g. `tf.keras.applications.resnet50`) and define them multiple times (using different devices, for multi-GPU). This simply doesn't work now as the predefined models themselves define new instances of each layer each time they are created.\r\n\r\nIs there a way out of this?"]}, {"number": 20841, "title": "toco tensorflow lite conversion fails with ParseFromStringEitherTextOrBinary(input_file_contents, tf_graph.get())", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.7.1-2-g9156fcc7a8 1.7.1\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: ---\r\n- **GCC/Compiler version (if compiling from source)**: ---\r\n- **CUDA/cuDNN version**: 9.0 / 7.1\r\n- **GPU model and memory**: NVIDIA GeForce GTX 1080 Ti\r\n- **Exact command to reproduce**: make\r\n\r\nI am trying to build a tensorflow model and export it as an tensorflow lite model using **toco**. It crashes with the issue below. I actually tried to run it on a far larger model, and build this just to reproduce it. \r\n\r\nI put the script into a zip file [simple_export_test.zip], if you want to run it quickly, using make.\r\nThe exported model on my system looks like this: [exported.zip]\r\n\r\n[exported.zip]: https://github.com/tensorflow/tensorflow/files/2197981/exported.zip\r\n\r\n\r\n[simple_export_test.zip]: https://github.com/tensorflow/tensorflow/files/2197923/simple_export_test.zip\r\n\r\n_simple_model_export.py_\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nscript_path = os.path.dirname(os.path.abspath(__file__))\r\n\r\nx = tf.placeholder(tf.float32, shape=[2])\r\ny = tf.Variable([2., 3.])\r\nz = tf.pow(x, y)\r\n\r\nexport_dir = script_path + '/export'\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    sess.run(y, feed_dict={x: np.array([5, 6])})\r\n    tf.saved_model.simple_save(session=sess, export_dir=export_dir, inputs={'x': x}, outputs={'y': y})\r\n```\r\n_Makefile_\r\n```make\r\n.PHONY: all standard_export tflite_export\r\n  \r\nall: standard_export tflite_export\r\n\r\nstandard_export:\r\n    python simple_model_export.py\r\n\r\ntflite_export:\r\n    toco \\\r\n        --input_file=export/saved_model.pb \\\r\n        --output_file=export/optimized_graph.lite \\\r\n        --input_format=TENSORFLOW_GRAPHDEF \\\r\n        --output_format=TFLITE \\\r\n        --input_shape=2 \\\r\n        --input_array=x \\\r\n        --output_array=y \\\r\n        --inference_type=FLOAT \\\r\n        --input_data_type=FLOAT\r\n```\r\n\r\nExecuting it leads to:\r\n\r\n```log\r\n[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/text_format.cc:288] Error parsing text-format tensorflow.GraphDef: 1:1: Invalid control characters encountered in text.\r\n[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/text_format.cc:288] Error parsing text-format tensorflow.GraphDef: 1:4: Interpreting non ascii codepoint 188.\r\n[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/text_format.cc:288] Error parsing text-format tensorflow.GraphDef: 1:4: Expected identifier, got:\r\n2018-07-16 15:58:04.116409: F tensorflow/contrib/lite/toco/import_tensorflow.cc:2189] Check failed: ParseFromStringEitherTextOrBinary(input_file_contents, tf_graph.get())\r\nMakefile:9: recipe for target 'tflite_export' failed\r\n```\r\n\r\n", "comments": ["@ShapingView : To convert a saved model use tflite_convert:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md#savedmodel\r\n\r\nIn fact the commandline is much simpler :) since saved model has inputs and outputs.\r\ntflite_convert  --saved_model_dir=export/   -output_file=export/optimized_graph.lite\r\n\r\nI was able to convert your graph using the command, btw do you want the output to be z instead of y.\r\n", "This solved my issue! Thanks!", "Hi @shashishekhar \r\nCan you please also look into this error. https://github.com/tensorflow/tensorflow/issues/20878\r\nThis is also related to tflite conversion and I am facing it issue as well.\r\nPlease share some insight."]}, {"number": 20840, "title": "Omit some operations if only one para is kLogZero", "body": "If log_prob_1 is kLogZero and log_prob_2 is not kLogZero or log_prob_2 is kLogZero and log_prob_1 is not kLogZero, just return another parameters, this judge can omit some operations(log1pf() and expf()).\r\nThe mathematical equation is:\r\nln(e^(-inf)+e^(x))=ln(0+e^(x))=x", "comments": []}, {"number": 20839, "title": "[feature request] differentiate through eager_py_func", "body": "According to the [`tf.contrib.eager.py_func`](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/py_func) api doc,\r\n\r\n> tf.contrib.eager.py_func is not differentiable, though a gradient may be implemented in the future; if you would like to differentiate through it, please file an issue on Github.\r\n\r\nso I open an issue here.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Looks like this was fixed in c2493ed5aa9eaf375d88331c7cdb70e428614dc8. That commit will be in TensorFlow 1.10."]}, {"number": 20838, "title": "[Feature request] Extending set of canned estimators", "body": "The `tensorflow.contrib.learn` module was deprecated and some estimators were moved to `tensorflow.estimator`. There are the algorithms (SVM, k-means, Random Forest) which were not covered by the new estimators API.\r\n\r\nI'm wondering is there a chance that remaining estimators from deprecated `contrib.learn` module will be moved to `tensorflow.estimator`? Any plans to extend the current set of estimators with new algorithms?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Yes, it is still an open question.", "Nagging Assignee @shivaniag: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20837, "title": "Does the order of Java api runner.feed(#1).feed(#2).feed(#3)... matters?", "body": "Hi, guys, I have a quesion about the Tensorflow Java api.\r\nWhen I feed the data to the model, the feed order matters ?\r\nFor example, runner.feed(#1).feed(#2).feed(#3)... is equal to runner.feed(#2).feed(#1).feed(#3) ? \r\nBesides, recently, I run some response generation work from github(https://github.com/tuxchow/ecm), which use the old seq2seq code (implement with buckets), and i use the python to train the model, java to predict, as a result, I feed the test sentence word by word into the model, e.x. runner.feed(word1).feed(word2).feed(word3)... , However, I found that except the first word's logits is correct,  from the second word it's wrong, why is that?\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows10\r\n- **TensorFlow installed from (source or binary)**:  binary\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:None", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "The order shouldn't matter as long as each call to `runner.feed()` is provided a unique `inputName`. As for why other outputs would be wrong, it's hard to say without access to the model. Any chance you can share the model in question?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 20836, "title": "tensorflow binary was not compile to use AVX2", "body": "I have a problem. What should I do?\r\n\r\n![adsiz](https://user-images.githubusercontent.com/41289364/42754121-ede8db66-88fb-11e8-8ece-d31cee69f63b.png\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20835, "title": "Get corrupted records file's name", "body": "### System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora Red Hat Enterprise Linux Server release 7.5 (Maipo)\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 1.3.0\r\nPython version: 2.7\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: CUDA9.0/ CUDNN7.0\r\nGPU model and memory: P40 16GB\r\nExact command to reproduce: bash run.sh\r\n\r\n### Describe the problem\r\nNow, I get many record files from raw data .  But when i train a model with records data,  i get : DataLossError(see above for traeback): corrupted record at 3289898.     Is a record file data is inlegal\uff1f How to get the file name? with iterator?  thanks!\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 20834, "title": "SyncReplicasOptimizer + MonitoredTrainingSession go through network many times with one session.run", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 7.0\r\n- **GPU model and memory**: 16G\r\n- **Exact command to reproduce**: None\r\n\r\n### Describe the problem\r\ntraining model by `tf.train.MonitoredTrainingSession` combined with `tf.train.SyncReplicasOptimizer`,\r\nI find the model network will be run twice with only once `session.run` calling, and many NaN values will be raised. More, this will update the batch normalization parameters.\r\nI do the initialization by `tf.train.SyncReplicasOptimizer.make_session_run_hook`. By the way, code will be right without `tf.train.SyncReplicasOptimizer`.\r\n\r\n\r\n### Source code / logs\r\n```\r\n# 1. model define\r\ninput = tf.placeholder_with_default()\r\nphase = tf.placeholder_with_default(True, shape=(), name='phase')\r\n... network define ...\r\nself.loss = xxx\r\n\r\n# 2. optimizer define\r\nself.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\r\nself.optimizer = tf.train.SyncReplicasOptimizer(self.optimizer, replicas_to_aggregate=worker_num, total_num_replicas=worker_num\uff09\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n     self.train_op = self.optimizer.minimize(self.loss, global_step=self.global_step)\r\n\r\n# 3. hook define\r\nsync_replicas_hook = self.optimizer.make_session_run_hook((flags.task_index == 0), num_tokens=0)\r\n\r\n# 4. train\r\nwith tf.train.MonitoredTrainingSession(master=server.target, hooks=[sync_replicas_hook], is_chief=is_chief):\r\n    outpus = sess.run([self.train_op, global_step], feed_dict={'phase:0':True, ...})\r\n```\r\n\r\nProblem A:\r\nI add `tf.Print` line when define in network, I find this line executing twice with once sess.run calling.\r\n\r\nProblem B: \r\nwhen `phase = True`, the BatchNorm Parameter update right, but when `phase = False`, `moving_mean` and `moving_variance` turn to NaN, so does the loss.\r\n\r\nEverything is OK when training with async optimizer.", "comments": ["Another problem\uff1a\r\nI am not sure whether someone has tried SyncReplicasOptimizer +  MonitoredTrainingSession + Placeholder of Model Input. After session created, during initializing variables in the hook function, it reports needing to feed value to the placeholder. I am confused what triggered off the network computing before session.run calling explicitly. Finally I change to use placeholder_with_default with a default value to work around this question. But I think it's not a good idea. \r\nLittle information can be found in the doc.\r\n ", "This issue is likely due to custom network definition rather than a general one. Please provide details.", "It has been 31 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20833, "title": "Coordinator stopped with threads still running", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: NV-P40\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nRunning distributed tensorflow using estimator in sync mode, there is always a exception after the last training step, as followed:\r\n```\r\nINFO:tensorflow:Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany\r\nException in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 268, in _run\r\n    coord.request_stop(e)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 213, in request_stop\r\n    six.reraise(*sys.exc_info())\r\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\r\n    enqueue_callable()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1244, in _single_operation_run\r\n    self._call_tf_sessionrun(None, {}, [], target_list, None)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to `Session::Close()`.\r\n``` \r\n\r\nIt seems there is a closing problem when using tf.train.SyncReplicasOptimizer.\r\nAny one know how to fix this?  \r\n\r\n### Source code / logs\r\nMain Code Fragment(model_fn) as followed:\r\n```\r\nwith tf.device('/job:worker/task:%d' % task_index):\r\n    with slim.arg_scope([slim.variables.variable, slim.variables.global_step],\r\n                 device=slim.variables.VariableDeviceChooser(num_parameter_servers)):\r\n             global_step = slim.variables.global_step()\r\n             # Calculate the learning rate schedule.\r\n             num_batches_per_epoch = (num_examples_per_epoch / FLAGS.batch_size)\r\n             decay_steps = int(num_batches_per_epoch * FLAGS.num_epochs_per_decay)\r\n\r\n             # Decay the learning rate exponentially based on the number of steps.\r\n             lr = tf.train.exponential_decay(FLAGS.initial_learning_rate,\r\n                                             global_step,\r\n                                             decay_steps,\r\n                                             FLAGS.learning_rate_decay_factor,\r\n                                             staircase=True)\r\n             opt = tf.train.RMSPropOptimizer(learning_rate=lr,\r\n                                                 decay=RMSPROP_DECAY,\r\n                                                 momentum=RMSPROP_MOMENTUM,\r\n                                                 epsilon=RMSPROP_EPSILON)\r\n            # forward\r\n             logits = inception.inference(features, num_classes, for_training=True)\r\n\r\n             # loss\r\n             inception.loss(logits, labels)\r\n             losses = tf.get_collection(slim.losses.LOSSES_COLLECTION)\r\n             losses += tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\r\n\r\n             total_loss = tf.add_n(losses, name='total_loss')\r\n\r\n              # Create synchronous replica optimizer.\r\n             opt = tf.train.SyncReplicasOptimizer(\r\n                     opt,\r\n                     replicas_to_aggregate=num_replicas_to_aggregate,\r\n                     total_num_replicas=num_workers)\r\n\r\n             # Compute gradients with respect to the loss.\r\n             grads = opt.compute_gradients(total_loss)\r\n             apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\r\n             sync_replicas_hook = opt.make_session_run_hook(is_chief, num_tokens=num_workers)\r\n             \r\n              return tf.estimator.EstimatorSpec(\r\n                 mode=mode,\r\n                 loss=total_loss,\r\n                 train_op=train_op,\r\n                 training_chief_hooks=[],\r\n                 training_hooks=[sync_replicas_hook])\r\n```\r\n--------------------\r\nAnd some  logs:\r\n```\r\nINFO:tensorflow:SyncReplicasV2: replicas_to_aggregate=2; total_num_replicas=2\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:tokens_needed = 0\r\nINFO:tensorflow:Graph was finalized.\r\n2018-07-16 09:13:43.014523: I tensorflow/core/distributed_runtime/master_session.cc:1142] Start master session a13a7f78ee358c84 with config: allow_soft_placement: true\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:loss = 13.082163, step = 0\r\nINFO:tensorflow:Saving checkpoints for 1 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.082163\r\nINFO:tensorflow:loss = 13.115122 (15.557 sec)\r\nINFO:tensorflow:Saving checkpoints for 2 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.103788 (7.314 sec)\r\nINFO:tensorflow:global_step/sec: 0.043722\r\nINFO:tensorflow:Saving checkpoints for 3 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.137239 (6.563 sec)\r\nINFO:tensorflow:global_step/sec: 0.152378\r\nINFO:tensorflow:Saving checkpoints for 4 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.200961 (6.763 sec)\r\nINFO:tensorflow:global_step/sec: 0.147865\r\nINFO:tensorflow:Saving checkpoints for 5 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.265466 (6.318 sec)\r\nINFO:tensorflow:global_step/sec: 0.158291\r\nINFO:tensorflow:Saving checkpoints for 6 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.165977 (6.270 sec)\r\nINFO:tensorflow:global_step/sec: 0.159476\r\nINFO:tensorflow:Saving checkpoints for 7 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.478054 (7.017 sec)\r\nINFO:tensorflow:global_step/sec: 0.14252\r\nINFO:tensorflow:Saving checkpoints for 8 into /tmp/aves/train_dir/model.ckpt.\r\nWARNING:tensorflow:Ignoring: /tmp/aves/train_dir/model.ckpt-3.meta; No such file or directory\r\nINFO:tensorflow:loss = 13.231509 (6.544 sec)\r\nINFO:tensorflow:global_step/sec: 0.152785\r\nINFO:tensorflow:Saving checkpoints for 9 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.367797 (6.686 sec)\r\nINFO:tensorflow:global_step/sec: 0.149593\r\nINFO:tensorflow:Saving checkpoints for 10 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.461123 (6.659 sec)\r\nINFO:tensorflow:global_step/sec: 0.150159\r\nINFO:tensorflow:Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany\r\nException in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 268, in _run\r\n    coord.request_stop(e)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 213, in request_stop\r\n    six.reraise(*sys.exc_info())\r\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\r\n    enqueue_callable()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1244, in _single_operation_run\r\n    self._call_tf_sessionrun(None, {}, [], target_list, None)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to `Session::Close()`.\r\n\r\nINFO:tensorflow:Loss for final step: 13.461123.\r\n```", "comments": ["@mrry is that related to this?   After a thread has called `coord.request_stop()` the other threads have a\r\n  fixed time to stop, this is called the 'stop grace period' and defaults to 2\r\n  minutes.  If any of the threads is still alive after the grace period expires\r\n  `coord.join()` raises a RuntimeError reporting the laggards.", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have you got any new ideas about fixing this problem? @allenzhang010 ", "@dcmrlee there's a hint in #13779 about adding an ignore_live_threads=True; would you please try that?", "Hi, I'm also experiencing this behavior.\r\nI'm using `TensorFlow` version `1.9.0` and I'm running a `distributed mnist` job using [tfoperator](https://github.com/kubeflow/tf-operator) and [mlt](https://github.com/IntelAI/mlt) on a `GKE` cluster.\r\n\r\nAfter my [main.py](https://github.com/IntelAI/mlt/blob/master/mlt-templates/tf-dist-mnist/main.py) finishes near line 221, I get:\r\n```\r\n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-1-bmbvb] INFO:root:worker 1, step 254 of 256: loss = 1.478 \r\n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-0-zlea4] INFO:root:worker 0, step 254 of 256: loss = 1.481 \r\n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-0-zlea4] INFO:root:worker 0, step 255 of 256: loss = 1.499 \r\n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-1-bmbvb] INFO:root:worker 1, step 255 of 256: loss = 1.47 \r\n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-1-bmbvb] INFO:root:Finished on task 1 in 98.08173823356628 seconds \r\n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-1-bmbvb] INFO:root:Session from worker 1 closed cleanly \r\n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-0-zlea4] INFO:tensorflow:Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany\r\n```\r\nand for that reason my `Pods` remain running indefinitely and the TfJob is never marked as `Succeeded`.\r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: Ubuntu 16.04\r\nTensorFlow installed from: https://hub.docker.com/r/intelaipg/intel-optimized-tensorflow/\r\nTensorFlow version: 1.9.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: \r\n```\r\nmlt init my-tf-dist --template=tf-dist-mnist --template-repo=.\r\nmlt deploy -l\r\n```\r\nfirst command instantiates a distributed `TensorFlow mnist` template to be used for deploying as a TfJob on a `GKE` cluster and then the second command deploys the job and uses [kubetail](https://github.com/johanhaleby/kubetail) to tail the aggregated logs for all the Pods.\r\n\r\nThis is currently blocking one of our PRs on `mlt` here: https://github.com/IntelAI/mlt/pull/387", "I even tried modifying [coordinator.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/coordinator.py#L322) as follows:\r\n```python\r\n  def join(self, threads=None, stop_grace_period_secs=120,\r\n           ignore_live_threads=True):\r\n    \"\"\"Wait for threads to terminate.\r\n```\r\nand deployed the TfJob again but still same behavior and I get:\r\n```\r\n[my-tf-dist-7bd9faec-bd67-41cc-a9c2-86e68-worker-g1qj-1-urvjt] INFO:root:Finished on task 1 in 99.67636966705322 seconds \r\n[my-tf-dist-7bd9faec-bd67-41cc-a9c2-86e68-worker-g1qj-1-urvjt] INFO:root:Session from worker 1 closed cleanly \r\n[my-tf-dist-7bd9faec-bd67-41cc-a9c2-86e68-worker-g1qj-0-3oxfi] INFO:tensorflow:Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany \r\n```", "I just verified that with version `1.8.0` I don't have this issue.", "We have the similar problem in `TensorFlow 1.8.0` when using `MonitoredSession ` and  `SyncReplicasOptimizer` in distributed training. The parameter `ignore_live_threads` is in Coordinator and I'm not sure if we can set in `MonitoredSession`.\r\n\r\nHere is the error log of trying to close the session.\r\n\r\n```\r\nINFO:tensorflow:Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany\r\n2018-08-15 15:50:02 INFO     Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany\r\nException in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:\r\nTraceback (most recent call last):\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py\", line 810, in __bootstrap_inner\r\n    self.run()\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py\", line 763, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 268, in _run\r\n    coord.request_stop(e)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 213, in request_stop\r\n    six.reraise(*sys.exc_info())\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\r\n    enqueue_callable()\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1244, in _single_operation_run\r\n    self._call_tf_sessionrun(None, {}, [], target_list, None)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\nCancelledError: Step was cancelled by an explicit call to `Session::Close()`.\r\n```", "@tobegit3hub Thanks for the update.\r\nI'm seeing that error in my `kubetail` logs too, but it seems like for some reason `TfOperator` won't see that as a show stopper and marks the jobs as `Succeeded`:\r\nSo on `1.9.0` I have (`State:                Running`):\r\n```\r\n$ kubectl describe tfjob my-tf-dist-7bd9faec-bd67-41cc-a9c2-86e685f157da \r\nName:         my-tf-dist-7bd9faec-bd67-41cc-a9c2-86e685f157da\r\nNamespace:    ashahba\r\nLabels:       mlt-app-name=my-tf-dist\r\nAnnotations:  kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"kubeflow.org/v1alpha1\",\"kind\":\"TFJob\",\"metadata\":{\"annotations\":{},\"labels\":{\"mlt-app-name\":\"my-tf-dist\"},\"name\":\"my-tf-dist-7bd9faec-bd...\r\nAPI Version:  kubeflow.org/v1alpha1\r\nKind:         TFJob\r\nMetadata:\r\n  Cluster Name:        \r\n  Creation Timestamp:  2018-08-15T06:43:54Z\r\n  Generation:          1\r\n  Resource Version:    36154858\r\n  Self Link:           /apis/kubeflow.org/v1alpha1/namespaces/ashahba/tfjobs/my-tf-dist-7bd9faec-bd67-41cc-a9c2-86e685f157da\r\n  UID:                 93d30628-a056-11e8-8f32-42010a8a0062\r\nSpec:\r\n  Runtime Id:  g1qj\r\n  Replica Specs:\r\n    Replicas:  1\r\n    Template:\r\n      Metadata:\r\n        Creation Timestamp:  <nil>\r\n      Spec:\r\n        Containers:\r\n          Image:  <MY_IMAGE_REPOSITORY>/my-tf-dist:ee01f210-692a-4f5f-9211-1b57ddc35250\r\n          Name:   tensorflow\r\n          Resources:\r\n        Restart Policy:  OnFailure\r\n    Tf Port:             2222\r\n    Tf Replica Type:     PS\r\n    Replicas:            2\r\n    Template:\r\n      Metadata:\r\n        Creation Timestamp:  <nil>\r\n      Spec:\r\n        Containers:\r\n          Image:  <MY_IMAGE_REPOSITORY>/my-tf-dist:ee01f210-692a-4f5f-9211-1b57ddc35250\r\n          Name:   tensorflow\r\n          Resources:\r\n        Restart Policy:  OnFailure\r\n    Tf Port:             2222\r\n    Tf Replica Type:     WORKER\r\n  Termination Policy:\r\n    Chief:\r\n      Replica Index:  0\r\n      Replica Name:   WORKER\r\n  Tf Image:           tensorflow/tensorflow:1.3.0\r\nStatus:\r\n  Phase:   Running\r\n  Reason:  \r\n  Replica Statuses:\r\n    Replicas States:\r\n      Running:            1\r\n    State:                Running\r\n    Tf _ Replica _ Type:  PS\r\n    Replicas States:\r\n      Running:            1\r\n      Succeeded:          1\r\n    State:                Running\r\n    Tf _ Replica _ Type:  WORKER\r\n  State:                  Running\r\nEvents:                   <none>\r\n```\r\nand on `1.8.0` we get (`State:                  Succeeded`):\r\n```\r\n$ kubectl describe tfjob my-tf-dist-edb61c5c-6ef4-4d90-baac-752956f59978\r\nName:         my-tf-dist-edb61c5c-6ef4-4d90-baac-752956f59978\r\nNamespace:    ashahba\r\nLabels:       mlt-app-name=my-tf-dist\r\nAnnotations:  kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"kubeflow.org/v1alpha1\",\"kind\":\"TFJob\",\"metadata\":{\"annotations\":{},\"labels\":{\"mlt-app-name\":\"my-tf-dist\"},\"name\":\"my-tf-dist-edb61c5c-6e...\r\nAPI Version:  kubeflow.org/v1alpha1\r\nKind:         TFJob\r\nMetadata:\r\n  Cluster Name:        \r\n  Creation Timestamp:  2018-08-15T07:03:24Z\r\n  Generation:          1\r\n  Resource Version:    36158966\r\n  Self Link:           /apis/kubeflow.org/v1alpha1/namespaces/ashahba/tfjobs/my-tf-dist-edb61c5c-6ef4-4d90-baac-752956f59978\r\n  UID:                 4d73f22e-a059-11e8-8f32-42010a8a0062\r\nSpec:\r\n  Runtime Id:  qt9l\r\n  Replica Specs:\r\n    Replicas:  1\r\n    Template:\r\n      Metadata:\r\n        Creation Timestamp:  <nil>\r\n      Spec:\r\n        Containers:\r\n          Image:  gcr.io/constant-cubist-173123/my-tf-dist:15e9dd11-22a0-45e0-9a8e-9f72bd445703\r\n          Name:   tensorflow\r\n          Resources:\r\n        Restart Policy:  OnFailure\r\n    Tf Port:             2222\r\n    Tf Replica Type:     PS\r\n    Replicas:            2\r\n    Template:\r\n      Metadata:\r\n        Creation Timestamp:  <nil>\r\n      Spec:\r\n        Containers:\r\n          Image:  <MY_IMAGE_REPOSITORY>/my-tf-dist:15e9dd11-22a0-45e0-9a8e-9f72bd445703\r\n          Name:   tensorflow\r\n          Resources:\r\n        Restart Policy:  OnFailure\r\n    Tf Port:             2222\r\n    Tf Replica Type:     WORKER\r\n  Termination Policy:\r\n    Chief:\r\n      Replica Index:  0\r\n      Replica Name:   WORKER\r\n  Tf Image:           tensorflow/tensorflow:1.3.0\r\nStatus:\r\n  Phase:   Done\r\n  Reason:  \r\n  Replica Statuses:\r\n    Replicas States:\r\n      Running:            1\r\n    State:                Running\r\n    Tf _ Replica _ Type:  PS\r\n    Replicas States:\r\n      Running:            1\r\n      Succeeded:          1\r\n    State:                Running\r\n    Tf _ Replica _ Type:  WORKER\r\n  State:                  Succeeded\r\nEvents:                   <none>\r\n```\r\n\r\nand we have the first one with ", "@tensorflowbutler I added the requested response, anything else is needed to get some traction on this ticket?", "@allenzhang010 Do you need more info on this?", "@ashahba Thanks, I think this ticket can be closed.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Please create a new issue, if you have a similar issue.", "@ashahba  I have the similar problem in TensorFlow 1.10.0 when using MonitoredSession and SyncReplicasOptimizer at chief worker in distributed training, and i still don't know how to solve it. can you give me some advice. thanks.", "@dcmrlee I have same problem with you.But i don't understand that how to solve the problem finally.Have you solve it?", "I have same problem too. But `SyncReplicasOptimizer ` was deprecated, and will not be fixed.", ">>this problem happens because you are training more steps the what you have put in configuration file.\r\n", "> I have same problem too. But `SyncReplicasOptimizer ` was deprecated, and will not be fixed.\r\n\r\n+1"]}, {"number": 20832, "title": "'tflite_convert' is not recognized as an internal or external command ", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: none\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**:  3.5.1-32 bit\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: nothing since i have AMD radeon\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n```\r\ntflite_convert \\\r\n  --output_file=/saved_model/maonani.tflite \\\r\n  --saved_model_dir=/saved_model/saved_model\r\n```\r\n\r\n### Describe the problem\r\n when i enter the command on cmd i get a response saying:\r\n\r\n**'tflite_convert'** is not recognized as an internal or external command,\r\noperable program or batch file.\r\n\r\ni dont know why im getting this since i already have cloned the git repository of tensorflow\r\n\r\n### Source code / logs\r\n\r\nC:\\Users\\LENOVO-PC\\tensorflow> tflite_convert \\ --output_file=/saved_model/maonani.tflite \\ --saved_model_dir=/saved_model/saved_model\r\n'tflite_convert' is not recognized as an internal or external command,\r\noperable program or batch file.\r\n", "comments": ["@inakaaay : Is tflite_convert in the path, this looks like it cannot find the binary. Can you check the format of the file as well.\r\nfile tflite_convert", "did you mean that it is on the folder of tensorflow? @shashishekhar ", "Can you use something like:\r\nbazel run //tensorflow/contrib/lite/python:tflite_convert --  --output_file=/saved_model/maonani.tflite \\ --saved_model_dir=/saved_model/saved_model", "I did but it has a lot of errors too. check this out #20804 @shashishekhar ", "@inakaaay , how about try Linux solutions at https://stackoverflow.com/questions/51349873/tflite-convert-is-not-recognized-as-an-internal-or-external-command-in-window/51354656#51354656", "did you mean like this? `C:\\Users\\LENOVO-PC\\tensorflow\\tensorflow\\contrib\\lite\\python> tflite_convert \\ --output_file=/saved_model/maonani.tflite \\ --saved_model_dir=/saved_model/saved_model`  im having different error, it says **ImportError: No module named 'tensorflow.contrib.lite.python.tflite_convert'** @weishengchong ", "No I meant:\r\ncd C:\\Users\\LENOVO-PC\\tensorflow\r\nthen run \r\nC:\\Users\\LENOVO-PC\\tensorflow> bazel run //tensorflow/contrib/lite/python:tflite_convert -- --output_file=/saved_model/maonani.tflite \\ --saved_model_dir=/saved_model/saved_model\r\n\r\n", "yes i did. it says ```ERROR: C:/users/lenovo-pc/tensorflow/tensorflow/python/BUILD:1598:1: Linking of rule '//tensorflow/python:gen_linalg_ops_py_wrappers_cc' failed (Exit 1181)\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : fatal error LNK1181: cannot open input file 'tensorflow_framework.obj'```  @shashishekhar ", "Hi @inakaaay Sorry for the problems your are encountering. Unfortunately TF Lite doesn't support windows at the moment. We are hoping to have some form of support soon.", "Duplicate of #20975.", "Nagging Assignee @poxvoculi: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I also have this problem which says \"**tflite_convert: command not found**\" . I'm running Ubuntu 18.04 and tensorflow 1.5\r\nI'm running the code lab code for getting tensorflow on android \r\n. Below is the code\r\ntflite_convert \\\r\n  --graph_def_file=tf_files/retrained_graph.pb \\\r\n  --output_file=tf_files/optimized_graph.lite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --input_shape=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \\\r\n  --input_array=input \\\r\n  --output_array=final_result \\\r\n  --inference_type=FLOAT \\\r\n  --input_data_type=FLOAT", "I am too not able to run tflite_convert on windows. Any help ?\r\n![image](https://user-images.githubusercontent.com/4861930/50545059-6855e780-0c2e-11e9-8f0b-4d60f3e2bde8.png)\r\n", "![image](https://user-images.githubusercontent.com/13518093/52372699-6718a580-2a8b-11e9-94b2-a7c907be2b21.png)\r\n"]}, {"number": 20831, "title": "[Feature request] Allow `tf.estimator.train_and_evaluate` to support chief and evaluator working with different `model_dir`", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Cent OS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n\r\n### Describe the problem\r\nCurrently `tf.estimator.train_and_evaluate` makes it easy to use an Estimator to perform both training and evaluation, possibly in a distributed environment. \r\n\r\n In my case, `evaluator` have to \r\n\r\n  (1) read from chief worker's  `model_dir`,\r\n  (2) do some extra processing,\r\n  (3) generate **new model_dir** , \r\n  (4) finally concrete evaluation is based on the **new model_dir**. \r\n\r\nHowever, `train_and_evaluate` only supports one estimator **with single `model_dir`**,  i.e. `chief worker` and `evaluator` shares the same `checkpoint path`, which makes above requirements imcompatible. It would be ideal if we could perhaps pass more than one `model_dir` to train_and_evaluate to handle the above scenes.\r\n\r\nOr maybe there are some workarounds to deal with this problem?", "comments": ["The train_and_evaluate interface does not seems to be flexible. Sometimes the configuration of training and evaluation must be different. Currently we construct two different estimators to do these. Does this interface will be changed to support more complex situations ? @xiejw @ispirmustafa ", "This is not part of the design goal of train_and_evaluate. My suggestion would writing your own loop to do that. "]}, {"number": 20830, "title": "Performance Tensorflow Lite with NDK 17", "body": "Hi , After moving to NDK 17 , our internal timers showing ~30-40% degradation in Tensorflow Lite  performace .\r\n\r\nAny suggestion why clang will create such difference?\r\n\r\nThanks.", "comments": ["@dimitryn : Can you give more details about the model and how you are measuring. Can you report the numbers using our benchmark tool:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/benchmark\r\n\r\nYou may need to apply a patch: https://github.com/tensorflow/tensorflow/pull/20330 for it to run.", "it looks like a more general problem-related NDK  17  (issue: https://github.com/android-ndk/ndk/issues/495 )\r\n\"Bad\" performance in all submodules and  3rdparty libraries compiled with NDK 17. \r\nI just wanted to know if someone else got this behaviour.", "@dimitryn are you still observing the ~30% performance degradation with NDK 17?", "No , it same as 16"]}, {"number": 20829, "title": "feeding back curr state to next state causing dynamic_rnn to crash if batch size is varying", "body": "Im feeding data to the graph with tf.train.batch (dynamic_pad=True,allow_smaller_final_batch=True).\r\nalso im feeding in the final state returned by tf.train.dynamic_rnn as the next state during the training process. (initial state configured to zeros)\r\n\r\nit was working fine, however when end of epoch is reached, due to allow_smaller_final_batch=True above, it is trying to fetch smaller batch sizes. this seems to crash the program as the last state is of standard batch size(say 100) and current batch is lesser than standard batch size (eg: 90)\r\n\r\nif i dont feed the last state, things are working fine as the state always initialized to zeros based on the current batch size. I could not find proper explanation for dynamic batch sizes (pls not not im not referring to variable sequence lenghts here)  assoiated with state feedback to ascertain if this is a bug or a coding issue\r\n\r\nkindly help", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Could you post the log for the crash?", "apologies for delayed response...please find the requested details as below\r\n\r\nOS: Windows, \r\nIDE: Pycharm, \r\nTensorflow Installed from: pip3\r\nTensorflow Version: 1.9.0\r\n\r\nRequirement:\r\n\r\nIm trying to train an RNN model for a continous data. so after each batch processing i would like the prev state to be passed on to the next batch processing. The same is being achieved in the below code snippets\r\n\r\n```\r\ncell_size = 512\r\nbatch_size = 50\r\nnum_layers = 3\r\nfeature_size = 26\r\nback_propagation_steps = 600\r\n```\r\n\r\nSteps to Reproduce:\r\n\r\n- Created dynamic_rnn processing as follows\r\n```\r\noutputs, self.final_state = tf.nn.dynamic_rnn(\r\n  cell_fw,self.rnn_inputs,sequence_length=self.steps,\r\n  initial_state=self.state_fw,time_major=False)\r\n```\r\n\r\n- Reading data from input pipelines as follows\r\n```\r\nself.steps, self.xs , self.ys = tf.train.batch(\r\n  tensors=decoder.dequeue(self.is_classifier), batch_size=50,\r\n  dynamic_pad=True,  # <---\r\n  allow_smaller_final_batch=True,  # <---\r\n  name='batch_processor')\r\n```\r\n\r\n- During Training Phase Feeding the last state as follows\r\n```\r\n _,loss,  accuracy, summary, final_state,     = \r\n  self.sess.run(\r\n    [self.train_step,self.loss, self.accuracy, self.summary, self.final_state,],\r\n    feed_dict)\r\nfeed_dict[self.state_fw] = final_state\r\n```\r\n\r\n\r\n- Error during processing of the **last smaller batch**\r\n```\r\n_Dimensions of inputs should match: shape[0] = [20,512] vs. shape[1] = [50,512]\r\n\t [[Node: rnn_layer/rnn/while/rnn/multi_rnn_cell/cell_0/LSTMCell/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](rnn_layer/rnn/while/rnn/multi_rnn_cell/cell_0/dropout/mul, rnn_layer/rnn/while/Identity_4, rnn_layer/rnn/while/rnn/multi_rnn_cell/cell_0/LSTMCell/concat/axis)]]_\r\n```\r\n\r\n", "It's not totally clear to me how you are calling this, but from what I gather, it looks like the last tensor is smaller. As you point out, you are using `allow_smaller_final_batch=True`, which is likely to produce this.\r\n\r\nWhy not set that to `False`? I'm not sure how it's connected to feeding back the last state.\r\n\r\nAlso, I don't exactly understand how you are going to process the continuous sequence unless you plan on truncating the gradient at each batch. (Just consider the initial state as a constant.)", "Yeah...currently im setting `allow_smaller_final_batch=False` to overcome this issue. As you have rightly pointed out, im having a trouble in feeding continous sequences. so ive truncated my data into shorter segments , and relying on state to carry forward to the next batch instead resetting the state. \r\n\r\ni realize this approach is not entirely correct as the batched data is no more continous as i expect to be.\r\nim struggling on this point for a while may you can suggest a better way to do this\r\n\r\nfor eg:\r\nlet us say, continous data resembles` 1,2,3........so on to 1000.`\r\ni have stored this data as follows\r\nsegment input data into smaller units of length 20\r\n\r\n```\r\n1,2,3,....20  (seg-1)\r\n21,22,,,,40  (seg-2)\r\n981,982,,,1000   (seg- 50)\r\n```\r\nall these segments are written to tfrecords as serialized sequence examples.\r\n\r\nwhile training im reading the tfrecord file of fixed batch size (say 3)\r\nRNN state S is of shape (batch size x cell size)\r\n\r\n```\r\nState     Batch -1                    New State\r\n0          1,2,3..20                      S1  \r\n0          21,22,...40                   S2\r\n0          41,42...60                    S3\r\n```\r\n\r\n```\r\nState     Batch -2                   New State\r\nS1         61,62,...80                       S4  \r\nS2          81,81...100                     S5\r\nS3         101,102,...120                 S6\r\n```\r\n\r\n\r\n`Question: `is state S is correct representation of continous data? As S1,S2,S3 are representing states for different batch streams, im having a slight confusion if i need to ever use batching here or just always use batch-size of 1 with a single state \r\n\r\n`Question: `Are RNN models mostly suitable for smaller length sequences rather learning patterns in continous streams? though i dont believe so, but most of the documentation are talking around mini batches with state resets, im trying to get a basic idea where RNN are perfect fit for. \r\n\r\n`Question:` In what scenarios do we exactly carry forward state from one batch to other\r\n\r\n", "So,\r\n\r\n1. I'm not sure what you mean by continuous. Is that just long sequences?\r\n2. RNNs are good for smaller sequences, they will not work well long sequences, even if you use LSTMs.\r\n3. Yes, you can carry state forward, as long as you don't care about the gradient. If the sequence is long, it's probably OK but not great.\r\n\r\nImagine you have two sequences, `s`, and `r`. The batch size will be 2.\r\n\r\nThe way you would truncate the gradient is to consider the ending state fixed, so\r\n```\r\nState    Batch 0                New State\r\n0        s1, s2, ..., s20           S1\r\n0        r1, r2, ..., r20           R1\r\n```\r\n\r\nThen for the next batch:\r\n```\r\nState    Batch 1                New State\r\nS1       s21, s22, ..., s40         S2\r\nR1       r21, r22, ..., r40         R2\r\n```\r\n\r\nIf you are intent on doing the right thing, you can use `tf.contrib.recurrent` to implement re-materialization (otherwise you will run out of memory). You would create a cell that performs 20 steps, then the outer cell will connect these steps together. When computing the gradient, the code will compute the forward by doing, notionally:\r\n```\r\nS[0] = zero_state\r\nfor k in xrange(0, T/20):\r\n  intra_S[0] = S[k]\r\n  for j in xrange(20):\r\n      intra_S[j+1] = LSTM(x[k * 20 + j], intra_S[j])\r\n  S[k+1] = intra_S[20]\r\n```\r\nThe memory required is for `S`, and `intra_S` (which get reset every 20 steps). During the backward step, the intermediate states are computed automatically for you.\r\n"]}, {"number": 20828, "title": "Could not find com.androidx.test.espresso:espresso-core:2.2.2", "body": "When I tried to build the project, I got this:\r\nError:A problem occurred configuring project ':app'.\r\n> Could not resolve all dependencies for configuration ':app:_debugAndroidTestApkCopy'.\r\n   > Could not find com.androidx.test.espresso:espresso-core:2.2.2.\r\n     Required by:\r\n         project :app\r\n\r\n", "comments": ["I think this is now fixed in master\r\nhttps://github.com/tensorflow/tensorflow/commits/master/tensorflow/contrib/lite/examples/android/app/build.gradle\r\nPlease try again and close if so.\r\n", "hmm, now I get this error when building tensorflow/contrib/lite/java/demo with Android Studio:\r\n\r\n> Failed to resolve: androidx.test.espresso:espresso-core:3.1.0-alpha3", "I get the same error.... :-(\r\n", "This solved the problem for me:\r\n```\r\ndiff --git a/tensorflow/contrib/lite/java/demo/app/build.gradle b/tensorflow/contrib/lite/java/demo/app/build.gradle\r\nindex 92f04c651c..2a91f59df8 100644\r\n--- a/tensorflow/contrib/lite/java/demo/app/build.gradle\r\n+++ b/tensorflow/contrib/lite/java/demo/app/build.gradle\r\n@@ -11,11 +11,6 @@ android {\r\n         versionCode 1\r\n         versionName \"1.0\"\r\n         testInstrumentationRunner \"androidx.test.runner.AndroidJUnitRunner\"\r\n-\r\n-        // Remove this block.\r\n-        jackOptions {\r\n-            enabled true\r\n-        }\r\n     }\r\n     lintOptions {\r\n         abortOnError false\r\n@@ -37,6 +32,7 @@ android {\r\n }\r\n \r\n repositories {\r\n+    google()\r\n     maven {\r\n         url 'https://google.bintray.com/tensorflow'\r\n     }\r\ndiff --git a/tensorflow/contrib/lite/java/demo/build.gradle b/tensorflow/contrib/lite/java/demo/build.gradle\r\nindex b78a0b86c9..f6533b4535 100644\r\n--- a/tensorflow/contrib/lite/java/demo/build.gradle\r\n+++ b/tensorflow/contrib/lite/java/demo/build.gradle\r\n@@ -3,9 +3,10 @@\r\n buildscript {\r\n     repositories {\r\n         jcenter()\r\n+        google()\r\n     }\r\n     dependencies {\r\n-        classpath 'com.android.tools.build:gradle:2.3.1'\r\n+        classpath 'com.android.tools.build:gradle:3.1.3'\r\n \r\n         // NOTE: Do not place your application dependencies here; they belong\r\n         // in the individual module build.gradle files\r\ndiff --git a/tensorflow/contrib/lite/java/demo/gradle/wrapper/gradle-wrapper.properties b/tensorflow/contrib/lite/java/demo/gradle/wrapper/gradle-wrapper.properties\r\nindex fa7a38a0e4..72c939c9ff 100644\r\n--- a/tensorflow/contrib/lite/java/demo/gradle/wrapper/gradle-wrapper.properties\r\n+++ b/tensorflow/contrib/lite/java/demo/gradle/wrapper/gradle-wrapper.properties\r\n@@ -1,6 +1,6 @@\r\n-#Thu Sep 28 09:01:41 PDT 2017\r\n+#Thu Aug 02 11:46:34 CDT 2018\r\n distributionBase=GRADLE_USER_HOME\r\n distributionPath=wrapper/dists\r\n zipStoreBase=GRADLE_USER_HOME\r\n zipStorePath=wrapper/dists\r\n-distributionUrl=https\\://services.gradle.org/distributions/gradle-3.3-all.zip\r\n+distributionUrl=https\\://services.gradle.org/distributions/gradle-4.4-all.zip\r\n```", "I use \"com.android.support.test.espresso:espresso-core:2.2.2\".\r\nIt works.", "Ok I could build it but unfortunately it crashes directly on the mobile... (Nokia 6.1 with Android 8.1)... ", "@shashishekhar , could you please take a look?", "@odaira : looks like you already have a fix :), Can you send a PR :).", "Nagging Assignee @shashishekhar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20827, "title": "Keras guide: input_shape needed when fitting example model using datasets (instead of numpy arrays)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux fedora 28\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\nBelow, I'm extracting code from the Keras guide (sequentially)\r\n\r\nhttps://www.tensorflow.org/guide/keras\r\n\r\nHere a model is being fit twice, to show you can use numpy or datasets. This works fine:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nmodel = keras.Sequential()\r\n# Adds a densely-connected layer with 64 units to the model:\r\nmodel.add(keras.layers.Dense(64, activation='relu'))\r\n# Add another:\r\nmodel.add(keras.layers.Dense(64, activation='relu'))\r\n# Add a softmax layer with 10 output units:\r\nmodel.add(keras.layers.Dense(10, activation='softmax'))\r\n\r\nmodel.compile(optimizer=tf.train.AdamOptimizer(0.001),\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nimport numpy as np\r\n\r\ndata = np.random.random((1000, 32))\r\nlabels = np.random.random((1000, 10))\r\n\r\nval_data = np.random.random((100, 32))\r\nval_labels = np.random.random((100, 10))\r\n\r\nmodel.fit(data, labels, epochs=10, batch_size=32)\r\n          \r\ndataset = tf.data.Dataset.from_tensor_slices((data, labels))\r\ndataset\r\ndataset = dataset.batch(32)\r\ndataset = dataset.repeat()\r\ndataset\r\n# Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset.\r\nmodel.fit(dataset, epochs=10, steps_per_epoch=30)\r\n```\r\n\r\nHowever,  if you comment the first fit, the second will fail:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nmodel = keras.Sequential()\r\n# Adds a densely-connected layer with 64 units to the model:\r\nmodel.add(keras.layers.Dense(64, activation='relu'))\r\n# Add another:\r\nmodel.add(keras.layers.Dense(64, activation='relu'))\r\n# Add a softmax layer with 10 output units:\r\nmodel.add(keras.layers.Dense(10, activation='softmax'))\r\n\r\nmodel.compile(optimizer=tf.train.AdamOptimizer(0.001),\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nimport numpy as np\r\n\r\ndata = np.random.random((1000, 32))\r\nlabels = np.random.random((1000, 10))\r\n\r\n#model.fit(data, labels, epochs=10, batch_size=32)\r\n          \r\ndataset = tf.data.Dataset.from_tensor_slices((data, labels))\r\ndataset\r\ndataset = dataset.batch(32)\r\ndataset = dataset.repeat()\r\ndataset\r\n# Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset.\r\nmodel.fit(dataset, epochs=10, steps_per_epoch=30)\r\n```\r\n\r\nwith\r\n\r\n```\r\n  TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float64 of argument 'x'.\r\n```\r\n\r\nWhen I add an `input_shape` to the model, the fit using datasets works fine standalone, too.\r\n\r\nSo if `input_shape` is required with datasets, the model in the guide should probably have it, so people can copy out and directly use the code?", "comments": ["I have this exact same issue.\r\nEverything skeydan has said, occurs in my system (Linux Ubuntu 18.04). \r\n\r\nStrangely (for me at least) compiling and running model.fit again after this error works:\r\n```\r\n#Following instructions on https://www.tensorflow.org/guide/keras\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\nmodel = keras.Sequential()\r\n# Adds a densely-connected layer with 64 units to the model:\r\nmodel.add(keras.layers.Dense(64, activation='relu'))\r\n# Add another:\r\nmodel.add(keras.layers.Dense(64, activation='relu'))\r\n# Add a softmax layer with 10 output units:\r\nmodel.add(keras.layers.Dense(10, activation='softmax'))\r\n\r\nmodel.compile(optimizer=tf.train.AdamOptimizer(0.001),\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\n\r\ndata = np.random.random((1000, 32))\r\nlabels = np.random.random((1000, 10))\r\n\r\nval_data = np.random.random((100, 32))\r\nval_labels = np.random.random((100, 10))\r\n\r\n#model.fit(data, labels, epochs=10, batch_size=32)\r\n\r\n# Instantiates a toy dataset instance:\r\ndataset = tf.data.Dataset.from_tensor_slices((data, labels))\r\ndataset = dataset.batch(32)\r\ndataset = dataset.repeat()\r\n\r\n# Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset.\r\ntry:\r\n    model.fit(dataset, epochs=10, steps_per_epoch=30)\r\nexcept:\r\n    print(\"Error - repeating\")\r\n\r\n    model.compile(optimizer=tf.train.AdamOptimizer(0.001),\r\n                  loss='categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\n\r\n    model.fit(dataset, epochs=10, steps_per_epoch=30)\r\n```", "Thank you @skeydan and @Sirplentifus for reporting this. The issue has been fixed by https://github.com/tensorflow/tensorflow/commit/efe370fcb367efd069c8166120858492dffa9a33#diff-2afc7c6acfd68474e1e8c56718431276\r\n\r\nThis will be available in the next nightly release. Please try and let us know if it works for you.", "thanks, with the nightly build it works for me!", "@pavithrasv I have just tested the issue, and I am being able to reproduce the error. That is, the code mentioned above, with two calls to fit, works well, but commenting the first fit gives error in the second. That is: \"TypeError: Input 'b' of 'MatMul' Op has type float32 that does not match type float64 of argument 'a'.\"\r\n\r\nI am in Windows 10 enterprise, not using GPUs, tensorflow version = 1.11.0-dev20180907\r\n\r\nPlease notice that I am working with a nightly built since 1.10 was giving other issues, namely https://github.com/tensorflow/tensorflow/issues/21894\r\n"]}, {"number": 20826, "title": "TOCO (TensorFlow 1.9.0) -- 'TocoConverter' has no attribute 'from_keras_model_file'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04 on Windows 10 VirtualBox\r\n- **TensorFlow installed from (source or binary)**: binary (pip)\r\n- **TensorFlow version (use command below)**: [Git] 'v1.9.0-0-g25c197e023'; [Version] '1.9.0'\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: \r\n```\r\nimport tensorflow as tf\r\n\r\nconverter = tf.contrib.lite.TocoConverter.from_keras_model_file(\"keras_model.h5\")\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI'm trying to convert a keras model to the TensorFlow Lite format via TOCO. For some reason, though, the keras conversion functions seem to be missing from the TocoConverter. I've also tried using the command line feature and the same thing happens when I use the command-line version of TOCO. Is there something wrong? I just followed the [Python API tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md) and the [command-line examples.](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md)\r\n### Source code / logs\r\nTraceback (most recent call last):\r\n  File \"modelconv.py\", line 3, in <module>\r\n    converter = tf.contrib.lite.TocoConverter.from_keras_model_file(\"model.h5\")\r\nAttributeError: type object 'TocoConverter' has no attribute 'from_keras_model_file'\r\n\r\n", "comments": ["This feature is not yet in 1.9.0. You need to use the nightly e.g. use\r\npip install --upgrade tf_nightly\r\n"]}, {"number": 20825, "title": "TFLite --- TypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(\"MobilenetV1/Conv2d_0/BatchNorm/beta:0\", shape=(32,), dtype=float32)", "body": "we can get mobilenet tflite&pb(Mobilenet_1.0_224) from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md\r\n\r\nwhen I use the command:\r\nbazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/path/to/mobilenet_v1_1.0_224_frozen.pb --input_checkpoint=/path/to/mobilenet_v1_1.0_224.ckpt --output_graph=/path/to/freezed_mobilenet.pb --output_node_names=MobilenetV1/Predictions/Reshape_1 --input_binary=true\r\n\r\nerror occured:\r\n/home/leve/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-07-16 11:04:48.613520: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-07-16 11:04:48.728926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-07-16 11:04:48.729264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7845\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 5.31GiB\r\n2018-07-16 11:04:48.729276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-07-16 11:04:48.873004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-16 11:04:48.873030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2018-07-16 11:04:48.873035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-07-16 11:04:48.873192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5086 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 382, in <module>\r\n    run_main()\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 379, in run_main\r\n    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 378, in <lambda>\r\n    my_main = lambda unused_args: main(unused_args, flags)\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 272, in main\r\n    flags.saved_model_tags, checkpoint_version)\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 254, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 128, in freeze_graph_with_def_protos\r\n    var_list=var_list, write_version=checkpoint_version)\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py\", line 1338, in __init__\r\n    self.build()\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py\", line 1347, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py\", line 1384, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py\", line 813, in _build_internal\r\n    saveables = self._ValidateAndSliceInputs(names_to_saveables)\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py\", line 714, in _ValidateAndSliceInputs\r\n    variable)\r\n**TypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(\"MobilenetV1/Conv2d_0/BatchNorm/beta:0\", shape=(32,), dtype=float32)**\r\n\r\n### System information\r\n- **Linux Ubuntu 16.04**:\r\n- **TensorFlow installed from source**:\r\n- **TensorFlow version 1.8.0**:\r\n- **Python version:3.6**: \r\n- **Bazel version 0.11.1**:\r\n- **GCC/Compiler version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) **:\r\n- **CUDA/cuDNN 9.0/7.0.5**:\r\n- **GPU  1060-6G**:\r\n- **Exact command to reproduce**:", "comments": ["I have a same issue", "@longchr123 why are you trying to freeze a frozen_graph?\r\n```\r\nbazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/path/to/mobilenet_v1_1.0_224_frozen.pb --input_checkpoint=/path/to/mobilenet_v1_1.0_224.ckpt --output_graph=/path/to/freezed_mobilenet.pb --output_node_names=MobilenetV1/Predictions/Reshape_1 --input_binary=true\r\n```\r\nIf what you want to do is to generate a new frozen graph from the mobilenet tflite&pb \r\n (Mobilenet_1.0_224) file, try \r\n```\r\nbazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/path/to/mobilenet_v1_1.0_224_eval.pbtxt --input_checkpoint=/path/to/mobilenet_v1_1.0_224.ckpt --output_graph=/path/to/freezed_mobilenet.pb --output_node_names=MobilenetV1/Predictions/Reshape_1\r\n```\r\ninstead.", "@freedomtan what you said is right, I have tested,but when I write a test code(#20824 ), convert pb to pbtxt, the problem also happend, when execute the command\r\n`bazel-bin/tensorflow/python/tools/freeze_graph` --input_graph=/path/to/mobilenet_v1_1.0_224_eval.pbtxt --input_checkpoint=/path/to/mobilenet_v1_1.0_224.ckpt --output_graph=/path/to/freezed_mobilenet.pb --output_node_names=MobilenetV1/Predictions/Reshape_1`\r\nI don't know why", "I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please fill out the issue template (https://github.com/tensorflow/tensorflow/issues/new), and please re-paste using the error from trying to freeze a not-frozen graph rather than an already frozen one. Thanks--", "@karmel  I should change mobilenet_v1_1.0_224_frozen.pb to mobilenet.pb  from\r\n```\r\nbazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/path/to/mobilenet_v1_1.0_224_frozen.pb --input_checkpoint=/path/to/mobilenet_v1_1.0_224.ckpt --output_graph=/path/to/freezed_mobilenet.pb --output_node_names=MobilenetV1/Predictions/Reshape_1 --input_binary=true\r\n```\r\n\r\nlike this:\r\n```\r\nbazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/path/to/mobilenet.pb --input_checkpoint=/path/to/mobilenet_v1_1.0_224.ckpt --output_graph=/path/to/freezed_mobilenet.pb --output_node_names=MobilenetV1/Predictions/Reshape_1 --input_binary=true\r\n```\r\n\r\nCode to generate mobilenet.pb file:\r\n```\r\ndef freeze_graph(ckpt_dir):\r\n    output_graph = os.path.join(MODEL_DIR, MODEL_NAME)\r\n    output_node_names = \"output\"\r\n    saver = tf.train.import_meta_graph(ckpt_dir + '.meta', clear_devices=True)\r\n\r\n    graph = tf.get_default_graph()\r\n    input_graph_def = graph.as_graph_def()\r\n\r\n    with tf.Session() as sess:\r\n        saver.restore(sess, ckpt_dir)\r\n\r\n        output_graph_def = graph_util.convert_variables_to_constants(\r\n            sess,\r\n            input_graph_def,\r\n            output_node_names.split(\",\")\r\n        )\r\n        with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n            f.write(output_graph_def.SerializeToString())\r\n        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n\r\n        for op in graph.get_operations():\r\n            print(op.name, op.values())\r\n```\r\n\r\nIs the action of graph_util.convert_variables_to_constants and command(bazel...freeze_graph ) the same?", "@longchr123 not exactly the same, but one of the important steps in [freeze_graph](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) is `graph_util.convert_variables_to_constants`", "Can you explain your motivation behind freezing the model `mobilenet_v1_1.0_224_frozen.pb`? The model is already frozen, so you do not need to run it through the `freeze_graph` tool again.\r\n\r\nHere are a few suggestions on next steps:\r\n1. If you are trying to use TensorFlow Lite with this model, then use the provided `mobilenet_v1_1.0_224.tflite` file.\r\n\r\n2. If you want to freeze the graph to understand how to use the tool, freeze the `pbtxt` file provided. You can use the following command after defining `PATH_TO_MOBILNET`.\r\n```\r\nblaze run third_party/tensorflow/python/tools/freeze_graph -- \\\r\n--input_graph=$PATH_TO_MOBILENET/mobilenet_v1_1.0_224_eval.pbtxt \\\r\n--input_checkpoint=$PATH_TO_MOBILENET/mobilenet_v1_1.0_224.ckpt \\\r\n--output_graph=$PATH_TO_MOBILENET/freezed_mobilenet.pb \\\r\n--output_node_names=MobilenetV1/Predictions/Reshape_1 \\\r\n--input_binary=false\r\n```\r\n\r\n3. If you want to be able to use TensorFlow Lite on a graph you trained, you have the following options, a few of which don't directly use freeze graph. You can use either [`TocoConverter`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md) (Python API) or [`tflite_convert`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md) (command line tool) with your model. `TocoConverter` accepts a `tf.Session`, frozen graph file, SavedModel directory, or a `tf.keras` model file. `tflite_convert` accepts the later three formats."]}, {"number": 20824, "title": "TF Lite ---- Not a variable: Tensor(\"w1:0\", shape=(1, 128, 128, 3), dtype=float32)", "body": "**Environment:**\r\ntensorflow 1.8,ubuntu16.0.4,cuda9.0,cudnn:7.0.5\r\n\r\n**Test Demo:**\r\n\r\ndef save_to_pb(sess):\r\n    constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['out'])\r\n    with tf.gfile.FastGFile(MODEL_DIR + 'expert-graph.pb', mode='wb') as f:\r\n        f.write(constant_graph.SerializeToString())\r\n\r\ntrain_model():\r\n    img = tf.placeholder(name=\"img\", dtype=tf.float32, shape=(1, 128, 128, 3))\r\n    b = tf.Variable(tf.truncated_normal((1, 128, 128, 3), seed=1),name='w1')\r\n    y_real = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])\r\n\r\n    val = tf.add(img, b)\r\n    val = batch_norm_lite(val)\r\n    out = tf.identity(val, name=\"out\")\r\n\r\n    MSE = tf.reduce_mean(tf.square(y_real - out), name='mse')\r\n    train_step = tf.train.GradientDescentOptimizer(0.9).minimize(MSE)\r\n\r\n    saver = tf.train.Saver(var_list={'w1':b},max_to_keep=10)\r\n\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.per_process_gpu_memory_fraction = 0.1\r\n    config.gpu_options.allow_growth = True\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.initialize_all_variables())\r\n        for epoch in range(100):\r\n            sess.run(train_step,feed_dict={img:read_data(sess)})\r\n            if epoch % 20 == 0:\r\n                save_path = saver.save(sess, MODEL_DIR + MODEL_NAME, global_step=epoch + 1)\r\n            print('step = ' + str(epoch))\r\n        save_to_pb(sess)\r\n\r\n**Terminal:**\r\nbazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/path/to/expert-graph.pb --input_checkpoint=/path/to/model.ckpt --output_node_names=output --output_graph=/path/to/frozen.pb --input_binary=true\r\n\r\n**Error\uff1a**\r\n/home/leve/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-07-13 18:37:04.467417: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-07-13 18:37:04.579724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-07-13 18:37:04.580044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7845\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 5.29GiB\r\n2018-07-13 18:37:04.580057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-07-13 18:37:04.747082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-13 18:37:04.747104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2018-07-13 18:37:04.747109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-07-13 18:37:04.747225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5066 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 382, in <module>\r\n    run_main()\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 379, in run_main\r\n    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 378, in <lambda>\r\n    my_main = lambda unused_args: main(unused_args, flags)\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 272, in main\r\n    flags.saved_model_tags, checkpoint_version)\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 254, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 128, in freeze_graph_with_def_protos\r\n    var_list=var_list, write_version=checkpoint_version)\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py\", line 1338, in __init__\r\n    self.build()\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py\", line 1347, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py\", line 1384, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py\", line 813, in _build_internal\r\n    saveables = self._ValidateAndSliceInputs(names_to_saveables)\r\n  File \"/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py\", line 714, in _ValidateAndSliceInputs\r\n    variable)\r\n**TypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(\"w1:0\", shape=(1, 128, 128, 3), dtype=float32)**\r\n", "comments": ["Does anyone know about this problem?", "@longchr123 : Can you used tf.saved_model to save the graph and tf_convert https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md#savedmodel\r\n\r\nto convert it. It is available with latest version of Tensorflow", "@longchr123 : Please reopen this issue, if you are still having trouble converting the model."]}, {"number": 20823, "title": "Grappler warning and segfault", "body": "### System information\r\n- **Have I written custom code**: I used this TensorFlow [patch](https://gist.github.com/Willian-Zhang/a3bd10da2d8b343875f3862b2a62eb3b).\r\n- **OS Platform and Distribution**: macOS 10.13.6 and 10.13.5 before\r\n- **TensorFlow installed from**: Source\r\n- **TensorFlow version**: ('v1.8.0-0-g93bc2e2072', '1.8.0')\r\n- **Python**: 2.7\r\n- **Bazel version**: 0.13.1\r\n- **GCC/Compiler**: from Xcode 9.2\r\n- **CUDA/cuDNN**: 9.1/7.0.5\r\n- **GPU model and memory**: NVIDIA 1080 8.00GiB\r\n- **Exact command to reproduce**: Cifar 10 training from examples\r\n\r\n\r\nHardware: \r\nMacBook Pro 13,3 eGPU 1080\r\nNVIDIA Web Driver 387.10.10.10.40.105 \r\nCUDA Driver 396.148 \r\nCUDA 9.1 Toolkit \r\ncuDNN 7.0.5 \r\nPython 2.7 \r\nNCCL 2.1.15 \r\nXcode 9.2\r\n\r\nimport tensorflow as tf\r\ntf.test.is_gpu_available():\r\n`tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:859] OS X does not support NUMA - returning NUMA node zero\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:46:00.0\r\ntotalMemory: 8.00GiB freeMemory: 2.32GiB\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:0 with 2025 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:46:00.0, compute capability: 6.1)\r\nTrue`\r\n\r\nWhen I try to run something I get this error messages with segfault in the end:\r\n\r\n`Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:859] OS X does not support NUMA - returning NUMA node zero\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:46:00.0\r\ntotalMemory: 8.00GiB freeMemory: 3.39GiB\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3118 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:46:00.0, compute capability: 6.1)\r\nE tensorflow/core/grappler/clusters/utils.cc:127] Not found: TF GPU device with id 0 was not registered\r\nSegmentation fault: 11`\r\n\r\nIn other program I tried to reduce per_process_gpu_memory_fraction and batch size so it crashed after first batch with the same error code. I have seen people encounter same grappler warning without segfault. \r\n\r\nAny idea how to resolve this? Any help appreciated!! Thank you!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nOS Platform and Distribution\nCUDA/cuDNN version", "same error with CUDA 9.2 and cuDNN 7.1", "@smoothdvd Could you share your hardware and software info?", "@megagosha \r\n```\r\nNVIDIA driver   [387.10.10.10.40.105]\r\nCUDA drivers    [396.148]\r\nCUDA developer drivers  [396.148]\r\nCUDA toolkit  [9.2.148]\r\nCUDA samples   [9.2.148]\r\n\r\nmacOS version  [10.13.6]\r\nmacOS build   [17G65]\r\n\r\neGPU  GTX 1080\r\n\r\npython 3.6.x\r\n\r\niMac (Retina 5K, 27-inch, Late 2014)\r\n```", "We don't support GPU's on MacOS as of now.  This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20822, "title": "Test and fix the timer in InMemoryEvaluatorHook", "body": "Removed the extraneous return statement inside `_evaluate` that prevented calling `update_last_triggered_step`. Now the hook will run at the appropriate iterations.", "comments": ["+b."]}, {"number": 20821, "title": "Use FastBoundsCheck in ArgMax kernel op", "body": "This fix updates ArgMax kernel implementation to\r\nuse FastBoundsCheck for improved performance, and\r\nkeep consistency with other places in tf.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20820, "title": "[tfgan] Fix assertion in regularization loss unittest", "body": "`self.assertTrue(3.0)` always evaluates to `True`.\r\nThis PR fixes assertion so it will correctly check the loss values as set [here](https://github.com/lgeiger/tensorflow/blob/006b8faeb79c8b9329bd600390dbda888e9df226/tensorflow/contrib/gan/python/train_test.py#L437-L442).", "comments": []}, {"number": 20819, "title": "Tensorflow 1.9: android build of 'libtensorflow_cc.so' fails using android-ndk-r14b & android-ndk-r12b", "body": "I'm trying to build tensorflow 1.9 for android but build always fails due to different reasons... \r\n\r\nFor android-ndk-r14b I've tried the following:\r\n\r\n**bazel --output_user_root=/mnt/d/ai/.madman/bazel build -c opt //tensorflow:libtensorflow_cc.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --cxxopt=\"-std=c++11\" --cxxopt=\"-DTENSORFLOW_DISABLE_META\" --jobs 1**\r\n\r\n_But got this:_ \r\n\r\n```\r\nINFO: Analysed target //tensorflow:libtensorflow_cc.so (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /mnt/d/ai/.madman/tensorflow_r1.9/tensorflow/core/distributed_runtime/rpc/BUILD:160:1: C++ compilation of rule '//tensorflow/core/distributed_runtime/rpc:grpc_worker_service' failed (Exit 1)\r\nIn file included from tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc:20:\r\nIn file included from external/grpc/include/grpc++/alarm.h:26:\r\nIn file included from external/grpc/include/grpcpp/alarm.h:25:\r\nIn file included from external/grpc/include/grpcpp/impl/codegen/completion_queue.h:37:\r\nIn file included from external/grpc/include/grpcpp/impl/codegen/core_codegen_interface.h:25:\r\nexternal/grpc/include/grpcpp/impl/codegen/config.h:37:12: error: no member named 'to_string' in namespace 'std'\r\nusing std::to_string;\r\n      ~~~~~^\r\n```\r\n\r\n_... also tried to switch to llvm's STL like this:_ \r\n\r\n**bazel --output_user_root=/mnt/d/ai/.madman/bazel build -c opt //tensorflow:libtensorflow_cc.so --crosstool_top=@androidndk//:toolchain-libcpp --android_crosstool_top=@androidndk//:toolchain-libcpp --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --cxxopt=\"-std=c++11\" --cxxopt=\"-DTENSORFLOW_DISABLE_META\" --jobs 1**\r\n\r\n_... and got another issue:_ \r\n\r\n```\r\nWARNING: /mnt/d/ai/.madman/bazel/cd3928a854e918c00cc4e280f6815890/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /mnt/d/ai/.madman/bazel/cd3928a854e918c00cc4e280f6815890/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nINFO: Analysed target //tensorflow:libtensorflow_cc.so (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /mnt/d/ai/.madman/tensorflow_r1.9/tensorflow/core/kernels/BUILD:340:1: C++ compilation of rule '//tensorflow/core/kernels:stage_op' failed (Exit 1)\r\nIn file included from tensorflow/core/kernels/stage_op.cc:22:\r\nIn file included from ./tensorflow/core/framework/op_kernel.h:23:\r\nIn file included from ./tensorflow/core/framework/allocator.h:23:\r\nIn file included from ./tensorflow/core/framework/numeric_types.h:19:\r\nIn file included from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/complex:246:\r\nIn file included from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cmath:305:\r\nIn file included from external/androidndk/ndk/sources/android/support/include/math.h:31:\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:661:91: error: use of undeclared identifier 'acosl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double acos(long double __lcpp_x) _NOEXCEPT {return acosl(__lcpp_x);}\r\n                                                                                          ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:673:91: error: use of undeclared identifier 'asinl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double asin(long double __lcpp_x) _NOEXCEPT {return asinl(__lcpp_x);}\r\n                                                                                          ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:685:91: error: use of undeclared identifier 'atanl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double atan(long double __lcpp_x) _NOEXCEPT {return atanl(__lcpp_x);}\r\n                                                                                          ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:697:114: error: use of undeclared identifier 'atan2l'\r\ninline _LIBCPP_INLINE_VISIBILITY long double atan2(long double __lcpp_y, long double __lcpp_x) _NOEXCEPT {return atan2l(__lcpp_y, __lcpp_x);}\r\n                                                                                                                 ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:732:90: error: use of undeclared identifier 'cosl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double cos(long double __lcpp_x) _NOEXCEPT {return cosl(__lcpp_x);}\r\n                                                                                         ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:744:91: error: use of undeclared identifier 'coshl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double cosh(long double __lcpp_x) _NOEXCEPT {return coshl(__lcpp_x);}\r\n                                                                                          ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:756:90: error: use of undeclared identifier 'expl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double exp(long double __lcpp_x) _NOEXCEPT {return expl(__lcpp_x);}\r\n                                                                                         ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:792:113: error: use of undeclared identifier 'fmodl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double fmod(long double __lcpp_x, long double __lcpp_y) _NOEXCEPT {return fmodl(__lcpp_x, __lcpp_y);}\r\n                                                                                                                ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:839:90: error: use of undeclared identifier 'logl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double log(long double __lcpp_x) _NOEXCEPT {return logl(__lcpp_x);}\r\n                                                                                         ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:851:92: error: use of undeclared identifier 'log10l'\r\ninline _LIBCPP_INLINE_VISIBILITY long double log10(long double __lcpp_x) _NOEXCEPT {return log10l(__lcpp_x);}\r\n                                                                                           ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:863:114: error: use of undeclared identifier 'modfl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double modf(long double __lcpp_x, long double* __lcpp_y) _NOEXCEPT {return modfl(__lcpp_x, __lcpp_y);}\r\n                                                                                                                 ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:870:112: error: use of undeclared identifier 'powl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double pow(long double __lcpp_x, long double __lcpp_y) _NOEXCEPT {return powl(__lcpp_x, __lcpp_y);}\r\n                                                                                                               ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:893:90: error: use of undeclared identifier 'sinl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double sin(long double __lcpp_x) _NOEXCEPT {return sinl(__lcpp_x);}\r\n                                                                                         ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:905:91: error: use of undeclared identifier 'sinhl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double sinh(long double __lcpp_x) _NOEXCEPT {return sinhl(__lcpp_x);}\r\n                                                                                          ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:917:91: error: use of undeclared identifier 'sqrtl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double sqrt(long double __lcpp_x) _NOEXCEPT {return sqrtl(__lcpp_x);}\r\n                                                                                          ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:931:90: error: use of undeclared identifier 'tanl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double tan(long double __lcpp_x) _NOEXCEPT {return tanl(__lcpp_x);}\r\n                                                                                         ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:943:91: error: use of undeclared identifier 'tanhl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double tanh(long double __lcpp_x) _NOEXCEPT {return tanhl(__lcpp_x);}\r\n                                                                                          ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:955:92: error: use of undeclared identifier 'acoshl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double acosh(long double __lcpp_x) _NOEXCEPT {return acoshl(__lcpp_x);}\r\n                                                                                           ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:967:92: error: use of undeclared identifier 'asinhl'\r\ninline _LIBCPP_INLINE_VISIBILITY long double asinh(long double __lcpp_x) _NOEXCEPT {return asinhl(__lcpp_x);}\r\n                                                                                           ^\r\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n20 errors generated.\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 7.012s, Critical Path: 1.84s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\nSo I've tried the same for android-ndk-r12b:\r\n\r\n_first in this way:_ \r\n\r\n**bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a  --jobs 1**\r\n\r\n_... but got another issue:_ \r\n\r\n```\r\nERROR: /mnt/d/ai/.madman/tensorflow_r1.9-gcc/tensorflow/core/BUILD:2891:1: C++ compilation of rule '//tensorflow/core:gpu_runtime_impl' failed (Exit 1)\r\ntensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc: In member function 'virtual void* tensorflow::GPUNanResetAllocator::AllocateRaw(size_t, size_t)':\r\ntensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:176:27: error: 'nanf' is not a member of 'std'\r\n                           std::nanf(\"\"));\r\n                           ^\r\ntensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:176:27: note: suggested alternative:\r\nIn file included from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:44:0,\r\n                 from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/complex:44,\r\n                 from ./tensorflow/core/framework/numeric_types.h:19,\r\n                 from ./tensorflow/core/framework/allocator.h:23,\r\n                 from ./tensorflow/core/common_runtime/visitable_allocator.h:20,\r\n                 from ./tensorflow/core/common_runtime/gpu/gpu_debug_allocator.h:24,\r\n                 from tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:16:\r\nexternal/androidndk/ndk/platforms/android-14/arch-arm/usr/include/math.h:359:7: note:   'nanf'\r\n float nanf(const char *) __NDK_FPABI_MATH__ __pure2;\r\n       ^\r\ntensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc: In member function 'virtual void tensorflow::GPUNanResetAllocator::DeallocateRaw(void*)':\r\ntensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:191:29: error: 'nanf' is not a member of 'std'\r\n                             std::nanf(\"\"));\r\n                             ^\r\ntensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:191:29: note: suggested alternative:\r\nIn file included from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:44:0,\r\n                 from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/complex:44,\r\n                 from ./tensorflow/core/framework/numeric_types.h:19,\r\n                 from ./tensorflow/core/framework/allocator.h:23,\r\n                 from ./tensorflow/core/common_runtime/visitable_allocator.h:20,\r\n                 from ./tensorflow/core/common_runtime/gpu/gpu_debug_allocator.h:24,\r\n                 from tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:16:\r\nexternal/androidndk/ndk/platforms/android-14/arch-arm/usr/include/math.h:359:7: note:   'nanf'\r\n float nanf(const char *) __NDK_FPABI_MATH__ __pure2;\r\n       ^\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\n```\r\n\r\n\r\n_... then again tried to switch to llvm:_ \r\n\r\n**bazel build tensorflow/core:android_tensorflow_lib --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --config monolithic**\r\n\r\n_and again no luck:_ \r\n\r\n```\r\nERROR: /mnt/d/ai/.madman/tensorflow_r1.9-gcc/tensorflow/core/BUILD:2014:1: C++ compilation of rule '//tensorflow/core:lib_internal_impl' failed (Exit 1)\r\nIn file included from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libcxx/include/memory:614:0,\r\n                 from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libcxx/include/functional:477,\r\n                 from ./tensorflow/core/lib/core/threadpool.h:19,\r\n                 from tensorflow/core/lib/core/threadpool.cc:16:\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/libcxx/include/atomic: In member function 'void Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*)':\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/libcxx/include/atomic:720:75: error: invalid failure memory model for '__atomic_compare_exchange'\r\n                                    __gcc_atomic::__to_gcc_order(__failure));\r\n                                                                           ^\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 617.731s, Critical Path: 11.03s\r\nINFO: 264 processes: 264 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\nWhat am I doing wrong? \r\n\r\nP.S. '--jobs 1' seems to be necessary for my environment, otherwise I'm getting different random build issues.", "comments": ["Forgot to add, android:libtensorflow_inference.so builds *relatively* fine with both, android-ndk-r14b and android-ndk-r12b (after adding --jobs 1)", "Also just in case here are my WORKSPACE configs: \r\n\r\n... for ndk14: \r\n\r\n```\r\n# Uncomment and update the paths in these entries to build the Android demo.\r\nandroid_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 23,\r\n    build_tools_version = \"27.0.3\",\r\n    # Replace with path to Android SDK on your system\r\n    path = \"/mnt/g/AndroidSdkLinux\",\r\n)\r\n\r\nandroid_ndk_repository(\r\n    name=\"androidndk\",\r\n#    path=\"/mnt/g/AndroidSdk/ndk-bundle\",\r\n    path=\"/mnt/g/AndroidSdkLinux/android-ndk-r14b\",\r\n    api_level=14\r\n)\r\n\r\n```\r\n... for ndk12\r\n\r\n```\r\n# Uncomment and update the paths in these entries to build the Android demo.\r\nandroid_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 23,\r\n    build_tools_version = \"27.0.3\",\r\n    # Replace with path to Android SDK on your system\r\n    path = \"/mnt/g/AndroidSdkLinux\",\r\n)\r\n\r\nandroid_ndk_repository(\r\n    name=\"androidndk\",\r\n    path=\"/mnt/g/AndroidSdkLinux/android-ndk-r12b\",\r\n    api_level=14\r\n)\r\n```\r\n", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Overseer 4.4.0-17134-Microsoft #137-Microsoft Thu Jun 14 18:46:00 PST 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n(Ubuntu under Win10 / Windows subsystem for Unix)\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nn/a\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nsouce\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.9\r\n\r\n- **Python version**:\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18)\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.15.2\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\narm-linux-androideabi-g++ (GCC) 4.9.x 20150123 (prerelease)\r\nCopyright (C) 2014 Free Software Foundation, Inc.\r\n\r\n(Android NDK r14b)\r\n\r\n- **CUDA/cuDNN version**:\r\nn/a\r\n- **GPU model and memory**:\r\nn/a\r\n\r\n- **Exact command to reproduce**:\r\n\r\nbazel --output_user_root=/mnt/d/ai/.madman/bazel build -c opt //tensorflow:libtensorflow_cc.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --cxxopt=\"-std=c++11\" --cxxopt=\"-DTENSORFLOW_DISABLE_META\" --jobs 1\r\n\r\n.... or \r\n\r\nbazel --output_user_root=/mnt/d/ai/.madman/bazel build -c opt //tensorflow:libtensorflow_cc.so --crosstool_top=@androidndk//:toolchain-libcpp --android_crosstool_top=@androidndk//:toolchain-libcpp --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --cxxopt=\"-std=c++11\" --cxxopt=\"-DTENSORFLOW_DISABLE_META\" --jobs 1", "[This issue](https://github.com/tensorflow/tensorflow/issues/21416) may be relevant here. The issue was mostly resolved by switching to NDK 17b.", "@ElderOrb can you please try @gibiansky 's suggestion, to see if your issue is a duplicate of their's? ", "@cy89  Where can I get tf version to NDK version table, both using bazel and makefile now", "I have different error when build tensorflow C++ r1.10 for Android with NDK r15c\r\n\r\nbazel build -c opt //tensorflow:libtensorflow_cc.so --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --verbose_failures --cxxopt=\"-std=c++11\" --cxxopt=\"-DTENSORFLOW_DISABLE_META\" --verbose_failures\r\n\r\nERROR: /home/rhuang/tensorflow/tensorflow/core/common_runtime/eager/BUILD:215:1: no such package 'util/hash': BUILD file not found on package path and referenced by '//tensorflow/core/common_runtime/eager:attr_builder'\r\nERROR: Analysis of target '//tensorflow:libtensorflow_cc.so' failed; build aborted: no such package 'util/hash': BUILD file not found on package path\r\nINFO: Elapsed time: 3.116s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (60 packages loaded)\r\n\r\nAny idea ? Thanks\r\nBTW, I am able to build libtensorflow_inference.so", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@tensorflowbutler , could you please help me? I posted a question above you. Thanks", "@robinqhuang I would like to encourage you to create a new issue and provide all the information asked by the template. This will help us to focus on your specific issue and keep track of our commitment.\r\n \r\nClosing due to staleness. Please use the latest version for TensorFlow and test again. Feel free to open a new issue if it still persists. Thanks!", "> I'm trying to build tensorflow 1.9 for android but build always fails due to different reasons...\r\n> \r\n> For android-ndk-r14b I've tried the following:\r\n> \r\n> **bazel --output_user_root=/mnt/d/ai/.madman/bazel build -c opt //tensorflow:libtensorflow_cc.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --cxxopt=\"-std=c++11\" --cxxopt=\"-DTENSORFLOW_DISABLE_META\" --jobs 1**\r\n> \r\n> _But got this:_\r\n> \r\n> ```\r\n> INFO: Analysed target //tensorflow:libtensorflow_cc.so (0 packages loaded).\r\n> INFO: Found 1 target...\r\n> ERROR: /mnt/d/ai/.madman/tensorflow_r1.9/tensorflow/core/distributed_runtime/rpc/BUILD:160:1: C++ compilation of rule '//tensorflow/core/distributed_runtime/rpc:grpc_worker_service' failed (Exit 1)\r\n> In file included from tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc:20:\r\n> In file included from external/grpc/include/grpc++/alarm.h:26:\r\n> In file included from external/grpc/include/grpcpp/alarm.h:25:\r\n> In file included from external/grpc/include/grpcpp/impl/codegen/completion_queue.h:37:\r\n> In file included from external/grpc/include/grpcpp/impl/codegen/core_codegen_interface.h:25:\r\n> external/grpc/include/grpcpp/impl/codegen/config.h:37:12: error: no member named 'to_string' in namespace 'std'\r\n> using std::to_string;\r\n>       ~~~~~^\r\n> ```\r\n> \r\n> _... also tried to switch to llvm's STL like this:_\r\n> \r\n> **bazel --output_user_root=/mnt/d/ai/.madman/bazel build -c opt //tensorflow:libtensorflow_cc.so --crosstool_top=@androidndk//:toolchain-libcpp --android_crosstool_top=@androidndk//:toolchain-libcpp --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --cxxopt=\"-std=c++11\" --cxxopt=\"-DTENSORFLOW_DISABLE_META\" --jobs 1**\r\n> \r\n> _... and got another issue:_\r\n> \r\n> ```\r\n> WARNING: /mnt/d/ai/.madman/bazel/cd3928a854e918c00cc4e280f6815890/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /mnt/d/ai/.madman/bazel/cd3928a854e918c00cc4e280f6815890/external/grpc/bazel/grpc_build_system.bzl:172:12\r\n> INFO: Analysed target //tensorflow:libtensorflow_cc.so (0 packages loaded).\r\n> INFO: Found 1 target...\r\n> ERROR: /mnt/d/ai/.madman/tensorflow_r1.9/tensorflow/core/kernels/BUILD:340:1: C++ compilation of rule '//tensorflow/core/kernels:stage_op' failed (Exit 1)\r\n> In file included from tensorflow/core/kernels/stage_op.cc:22:\r\n> In file included from ./tensorflow/core/framework/op_kernel.h:23:\r\n> In file included from ./tensorflow/core/framework/allocator.h:23:\r\n> In file included from ./tensorflow/core/framework/numeric_types.h:19:\r\n> In file included from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/complex:246:\r\n> In file included from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cmath:305:\r\n> In file included from external/androidndk/ndk/sources/android/support/include/math.h:31:\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:661:91: error: use of undeclared identifier 'acosl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double acos(long double __lcpp_x) _NOEXCEPT {return acosl(__lcpp_x);}\r\n>                                                                                           ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:673:91: error: use of undeclared identifier 'asinl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double asin(long double __lcpp_x) _NOEXCEPT {return asinl(__lcpp_x);}\r\n>                                                                                           ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:685:91: error: use of undeclared identifier 'atanl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double atan(long double __lcpp_x) _NOEXCEPT {return atanl(__lcpp_x);}\r\n>                                                                                           ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:697:114: error: use of undeclared identifier 'atan2l'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double atan2(long double __lcpp_y, long double __lcpp_x) _NOEXCEPT {return atan2l(__lcpp_y, __lcpp_x);}\r\n>                                                                                                                  ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:732:90: error: use of undeclared identifier 'cosl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double cos(long double __lcpp_x) _NOEXCEPT {return cosl(__lcpp_x);}\r\n>                                                                                          ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:744:91: error: use of undeclared identifier 'coshl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double cosh(long double __lcpp_x) _NOEXCEPT {return coshl(__lcpp_x);}\r\n>                                                                                           ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:756:90: error: use of undeclared identifier 'expl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double exp(long double __lcpp_x) _NOEXCEPT {return expl(__lcpp_x);}\r\n>                                                                                          ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:792:113: error: use of undeclared identifier 'fmodl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double fmod(long double __lcpp_x, long double __lcpp_y) _NOEXCEPT {return fmodl(__lcpp_x, __lcpp_y);}\r\n>                                                                                                                 ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:839:90: error: use of undeclared identifier 'logl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double log(long double __lcpp_x) _NOEXCEPT {return logl(__lcpp_x);}\r\n>                                                                                          ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:851:92: error: use of undeclared identifier 'log10l'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double log10(long double __lcpp_x) _NOEXCEPT {return log10l(__lcpp_x);}\r\n>                                                                                            ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:863:114: error: use of undeclared identifier 'modfl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double modf(long double __lcpp_x, long double* __lcpp_y) _NOEXCEPT {return modfl(__lcpp_x, __lcpp_y);}\r\n>                                                                                                                  ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:870:112: error: use of undeclared identifier 'powl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double pow(long double __lcpp_x, long double __lcpp_y) _NOEXCEPT {return powl(__lcpp_x, __lcpp_y);}\r\n>                                                                                                                ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:893:90: error: use of undeclared identifier 'sinl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double sin(long double __lcpp_x) _NOEXCEPT {return sinl(__lcpp_x);}\r\n>                                                                                          ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:905:91: error: use of undeclared identifier 'sinhl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double sinh(long double __lcpp_x) _NOEXCEPT {return sinhl(__lcpp_x);}\r\n>                                                                                           ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:917:91: error: use of undeclared identifier 'sqrtl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double sqrt(long double __lcpp_x) _NOEXCEPT {return sqrtl(__lcpp_x);}\r\n>                                                                                           ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:931:90: error: use of undeclared identifier 'tanl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double tan(long double __lcpp_x) _NOEXCEPT {return tanl(__lcpp_x);}\r\n>                                                                                          ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:943:91: error: use of undeclared identifier 'tanhl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double tanh(long double __lcpp_x) _NOEXCEPT {return tanhl(__lcpp_x);}\r\n>                                                                                           ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:955:92: error: use of undeclared identifier 'acoshl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double acosh(long double __lcpp_x) _NOEXCEPT {return acoshl(__lcpp_x);}\r\n>                                                                                            ^\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:967:92: error: use of undeclared identifier 'asinhl'\r\n> inline _LIBCPP_INLINE_VISIBILITY long double asinh(long double __lcpp_x) _NOEXCEPT {return asinhl(__lcpp_x);}\r\n>                                                                                            ^\r\n> fatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n> 20 errors generated.\r\n> Target //tensorflow:libtensorflow_cc.so failed to build\r\n> Use --verbose_failures to see the command lines of failed build steps.\r\n> INFO: Elapsed time: 7.012s, Critical Path: 1.84s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully\r\n> ```\r\n> \r\n> So I've tried the same for android-ndk-r12b:\r\n> \r\n> _first in this way:_\r\n> \r\n> **bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --jobs 1**\r\n> \r\n> _... but got another issue:_\r\n> \r\n> ```\r\n> ERROR: /mnt/d/ai/.madman/tensorflow_r1.9-gcc/tensorflow/core/BUILD:2891:1: C++ compilation of rule '//tensorflow/core:gpu_runtime_impl' failed (Exit 1)\r\n> tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc: In member function 'virtual void* tensorflow::GPUNanResetAllocator::AllocateRaw(size_t, size_t)':\r\n> tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:176:27: error: 'nanf' is not a member of 'std'\r\n>                            std::nanf(\"\"));\r\n>                            ^\r\n> tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:176:27: note: suggested alternative:\r\n> In file included from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:44:0,\r\n>                  from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/complex:44,\r\n>                  from ./tensorflow/core/framework/numeric_types.h:19,\r\n>                  from ./tensorflow/core/framework/allocator.h:23,\r\n>                  from ./tensorflow/core/common_runtime/visitable_allocator.h:20,\r\n>                  from ./tensorflow/core/common_runtime/gpu/gpu_debug_allocator.h:24,\r\n>                  from tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:16:\r\n> external/androidndk/ndk/platforms/android-14/arch-arm/usr/include/math.h:359:7: note:   'nanf'\r\n>  float nanf(const char *) __NDK_FPABI_MATH__ __pure2;\r\n>        ^\r\n> tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc: In member function 'virtual void tensorflow::GPUNanResetAllocator::DeallocateRaw(void*)':\r\n> tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:191:29: error: 'nanf' is not a member of 'std'\r\n>                              std::nanf(\"\"));\r\n>                              ^\r\n> tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:191:29: note: suggested alternative:\r\n> In file included from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:44:0,\r\n>                  from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/complex:44,\r\n>                  from ./tensorflow/core/framework/numeric_types.h:19,\r\n>                  from ./tensorflow/core/framework/allocator.h:23,\r\n>                  from ./tensorflow/core/common_runtime/visitable_allocator.h:20,\r\n>                  from ./tensorflow/core/common_runtime/gpu/gpu_debug_allocator.h:24,\r\n>                  from tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:16:\r\n> external/androidndk/ndk/platforms/android-14/arch-arm/usr/include/math.h:359:7: note:   'nanf'\r\n>  float nanf(const char *) __NDK_FPABI_MATH__ __pure2;\r\n>        ^\r\n> Target //tensorflow:libtensorflow_cc.so failed to build\r\n> ```\r\n> \r\n> _... then again tried to switch to llvm:_\r\n> \r\n> **bazel build tensorflow/core:android_tensorflow_lib --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --config monolithic**\r\n> \r\n> _and again no luck:_\r\n> \r\n> ```\r\n> ERROR: /mnt/d/ai/.madman/tensorflow_r1.9-gcc/tensorflow/core/BUILD:2014:1: C++ compilation of rule '//tensorflow/core:lib_internal_impl' failed (Exit 1)\r\n> In file included from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libcxx/include/memory:614:0,\r\n>                  from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libcxx/include/functional:477,\r\n>                  from ./tensorflow/core/lib/core/threadpool.h:19,\r\n>                  from tensorflow/core/lib/core/threadpool.cc:16:\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libcxx/include/atomic: In member function 'void Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*)':\r\n> external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libcxx/include/atomic:720:75: error: invalid failure memory model for '__atomic_compare_exchange'\r\n>                                     __gcc_atomic::__to_gcc_order(__failure));\r\n>                                                                            ^\r\n> Target //tensorflow:libtensorflow_cc.so failed to build\r\n> Use --verbose_failures to see the command lines of failed build steps.\r\n> INFO: Elapsed time: 617.731s, Critical Path: 11.03s\r\n> INFO: 264 processes: 264 local.\r\n> FAILED: Build did NOT complete successfully\r\n> ```\r\n> \r\n> What am I doing wrong?\r\n> \r\n> P.S. '--jobs 1' seems to be necessary for my environment, otherwise I'm getting different random build issues.\r\n\r\n@ElderOrb Where you able to compile libtensorflow_cc.so for android arm64 ?"]}, {"number": 20818, "title": "[WIP] fix: InMemoryEvaluatorHook timer", "body": "There is an extraneous return statement inside `_evaluate` that prevents `update_last_triggered_step` from being called. This causes the hook trigger at every iteration.\r\n\r\nBesides this minor bug I am grateful for this new hook, it has sped up our training / evaluation loop tremendously. Thank you!\r\n\r\nTODO:\r\n- [ ] Add a test to prevent future regression (will do later today)", "comments": ["Going to reopen a new PR with tests", "Updated with a test in https://github.com/tensorflow/tensorflow/pull/20822"]}, {"number": 20817, "title": "Java SavedModelBundle.load do not support long path in windows", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10 64bit.\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n### Describe the problem\r\nJava SavedModelBundle.load do not support long path in windows.\r\nIt will throw something like:\r\n```\r\norg.tensorflow.TensorFlowException: NewRandomAccessFile failed to Create/Open: C:\\Projects\\veryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryverylongpath/variables/variables.data-00000-of-00001 : The system cannot find the path specified.\r\n; No such process\r\n\t [[Node: save_1/RestoreV2 = RestoreV2[_output_shapes=[<unknown>, <unknown>, <unknown>, <unknown>, <unknown>, ..., <unknown>, <unknown>, <unknown>, <unknown>, <unknown>], dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]\r\n\tat org.tensorflow.SavedModelBundle.load(Native Method)\r\n```\r\nI shorten the model path, everything works fine.\r\nI enabled long path based on this [doc](https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file). It still has this issue.\r\n\r\nI know it's a limit of Windows OS. I just wandering is there any way to let the C++ part bypass this limit?\r\n", "comments": ["Thanks for the report @iron9light , I'm guessing you're talking about the path size limitations described in https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation ?\r\n\r\nThe TensorFlow maintainers are unlikely to have the cycles to investigate a fix for this in the short term, so I'm marking this as \"Contributions Welcome\" in case anyone else in the community would like to give it a shot.\r\n\r\nThanks!", "Fine!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20816, "title": "Feature request: tfe.Variblae should support Python's float casting", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8.0 and 19.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nIn short, `tfe.Variable` should support:\r\n\r\n```\r\na = tfe.Variable(1.0)\r\nfloat(a)\r\n```\r\n\r\nIt is useful in my case, but to state my case it would be very convoluted. If contributors deem appropriate then plaese add a support to this.", "comments": ["It is also useful in my case.", "I don't think that would be practical. (Since we can't control how a builtin behaved. If you need to change dtypes you will need to use `tf.cast`. Feel free to reopen this issue with more explanation if that doesn't satisfy your use case.", "I think this could be supported by implementing the `__float__` method."]}, {"number": 20815, "title": "java.lang.RuntimeException: Unrecoverable error while evaluating node '@local_config_cc//:cc-compiler-darwin_x86_64'", "body": "## System Information\r\n\r\n**OS VERSION:** OS X 0.13.6\r\n**Bazel Verion:** release 0.15.0-homebrew\r\n**CPU:** Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz\r\n**GPU:**   Radeon Pro 555 Ram: 2048 MB\r\n**tensorflow version:** 1.9.0\r\n**CUDA/cuDNN version:** n/a\r\n**Python version:** Python 2.7.15\r\n\r\n#### Commands\r\n\r\n```go get -d github.com/tensorflow/tensorflow/tensorflow/go ```\r\n\r\n```\r\ncd ${GOPATH}/src/github.com/tensorflow/tensorflow\r\n./configure\r\nbazel build --config opt //tensorflow:libtensorflow.so\r\n```\r\n\r\nStarting local Bazel server and connecting to it...\r\n...........\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/util/tensor_bundle/BUILD:64:1: In rule 'tensor_bundle_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/util/tensor_bundle/BUILD:64:1: In rule 'tensor_bundle_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:40:1: In rule 'static_schedule_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:40:1: In rule 'static_schedule_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:40:1: In rule 'static_schedule_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:40:1: In rule 'static_schedule_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:75:1: In rule 'auto_parallel_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:75:1: In rule 'auto_parallel_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:75:1: In rule 'auto_parallel_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:75:1: In rule 'auto_parallel_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:112:1: In rule 'constant_folding_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:112:1: In rule 'constant_folding_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:156:1: In rule 'function_optimizer_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:156:1: In rule 'function_optimizer_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:156:1: In rule 'function_optimizer_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:156:1: In rule 'function_optimizer_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:360:1: In rule 'model_pruner_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:360:1: In rule 'model_pruner_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:360:1: In rule 'model_pruner_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:360:1: In rule 'model_pruner_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:433:1: In rule 'memory_optimizer_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:433:1: In rule 'memory_optimizer_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:477:1: In rule 'layout_optimizer_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:477:1: In rule 'layout_optimizer_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:477:1: In rule 'layout_optimizer_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:477:1: In rule 'layout_optimizer_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:536:1: In rule 'meta_optimizer_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:536:1: In rule 'meta_optimizer_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:536:1: In rule 'meta_optimizer_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:536:1: In rule 'meta_optimizer_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:620:1: In rule 'loop_optimizer_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:620:1: In rule 'loop_optimizer_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:620:1: In rule 'loop_optimizer_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:620:1: In rule 'loop_optimizer_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:658:1: In rule 'shape_optimizer_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:658:1: In rule 'shape_optimizer_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:693:1: In rule 'remapper_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:693:1: In rule 'remapper_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:693:1: In rule 'remapper_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:693:1: In rule 'remapper_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:723:1: In rule 'symbolic_shapes_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:723:1: In rule 'symbolic_shapes_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3231:1: In rule 'lib_random_random_distributions_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3231:1: In rule 'lib_random_random_distributions_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3356:1: In rule 'lib_jpeg_jpeg_mem_unittest', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3356:1: In rule 'lib_jpeg_jpeg_mem_unittest', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3369:1: In rule 'lib_strings_ordered_code_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3369:1: In rule 'lib_strings_ordered_code_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3381:1: In rule 'lib_random_weighted_picker_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3381:1: In rule 'lib_random_weighted_picker_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3405:1: In rule 'quantize_training_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3405:1: In rule 'quantize_training_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3596:1: In rule 'common_runtime_ring_reducer_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3596:1: In rule 'common_runtime_ring_reducer_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3806:1: In rule 'util_cuda_kernel_helper_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3806:1: In rule 'util_cuda_kernel_helper_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:4235:1: In rule 'gpu_allocator_retry_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:4235:1: In rule 'gpu_allocator_retry_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:4258:1: In rule 'gpu_debug_allocator_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:4258:1: In rule 'gpu_debug_allocator_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:1086:1: In rule 'conv_ops_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:1086:1: In rule 'conv_ops_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:1715:1: In rule 'scoped_allocator_ops_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:1715:1: In rule 'scoped_allocator_ops_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:1715:1: In rule 'scoped_allocator_ops_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:1715:1: In rule 'scoped_allocator_ops_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'adjust_contrast_op_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'adjust_contrast_op_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'colorspace_op_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'colorspace_op_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'crop_and_resize_op_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'crop_and_resize_op_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'non_max_suppression_op_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'non_max_suppression_op_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'resize_area_op_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'resize_area_op_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'resize_bicubic_op_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'resize_bicubic_op_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'resize_bilinear_op_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'resize_bilinear_op_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'resize_nearest_neighbor_op_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'resize_nearest_neighbor_op_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2309:1: In rule 'adjust_contrast_op_benchmark_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2309:1: In rule 'adjust_contrast_op_benchmark_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2309:1: In rule 'adjust_contrast_op_benchmark_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2309:1: In rule 'adjust_contrast_op_benchmark_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2325:1: In rule 'resize_benchmark_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2325:1: In rule 'resize_benchmark_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2325:1: In rule 'resize_benchmark_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2325:1: In rule 'resize_benchmark_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3148:1: In rule 'immutable_constant_op_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3148:1: In rule 'immutable_constant_op_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3170:1: In rule 'shape_op_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3170:1: In rule 'shape_op_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3534:1: In rule 'lrn_op_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3534:1: In rule 'lrn_op_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3534:1: In rule 'lrn_op_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3534:1: In rule 'lrn_op_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3554:1: In rule 'xent_op_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3554:1: In rule 'xent_op_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3554:1: In rule 'xent_op_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3554:1: In rule 'xent_op_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3574:1: In rule 'nn_ops_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3574:1: In rule 'nn_ops_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3574:1: In rule 'nn_ops_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3574:1: In rule 'nn_ops_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3720:1: In rule 'spacetobatch_benchmark_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3720:1: In rule 'spacetobatch_benchmark_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3720:1: In rule 'spacetobatch_benchmark_test_gpu', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3720:1: In rule 'spacetobatch_benchmark_test_gpu', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3812:1: In rule 'parse_tensor_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3812:1: In rule 'parse_tensor_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3913:1: In rule 'sendrecv_ops_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3913:1: In rule 'sendrecv_ops_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:4580:1: In rule 'spectrogram_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:4580:1: In rule 'spectrogram_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:5419:1: In rule 'quantization_utils_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:5419:1: In rule 'quantization_utils_test', timeout 'illegal' is not a valid timeout.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:5480:1: In rule 'quantized_activation_ops_test', size 'medium' is not a valid size.\r\nERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:5480:1: In rule 'quantized_activation_ops_test', timeout 'illegal' is not a valid timeout.\r\nUnhandled exception thrown during build; message: Unrecoverable error while evaluating node '@local_config_cc//:cc-compiler-darwin_x86_64 BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false' (requested by nodes '//tensorflow:libtensorflow.so BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '@bazel_tools//tools/cpp:malloc BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '@bazel_tools//tools/cpp:stl BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '//tensorflow:libtensorflow_framework.so BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false')\r\nINFO: Elapsed time: 5,309s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (18 packages loaded)\r\njava.lang.RuntimeException: Unrecoverable error while evaluating node '@local_config_cc//:cc-compiler-darwin_x86_64 BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false' (requested by nodes '//tensorflow:libtensorflow.so BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '@bazel_tools//tools/cpp:malloc BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '@bazel_tools//tools/cpp:stl BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '//tensorflow:libtensorflow_framework.so BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false')\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:460)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:355)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalArgumentException: No supported apple platform registered for target cpu ?os_x86_64\r\n\tat com.google.devtools.build.lib.rules.apple.ApplePlatform.forTargetCpu(ApplePlatform.java:165)\r\n\tat com.google.devtools.build.lib.rules.apple.ApplePlatform.forTarget(ApplePlatform.java:151)\r\n\tat com.google.devtools.build.lib.rules.apple.AppleConfiguration.getSingleArchPlatform(AppleConfiguration.java:260)\r\n\tat com.google.devtools.build.lib.rules.apple.cpp.AppleCcToolchain.addBuildVariables(AppleCcToolchain.java:70)\r\n\tat com.google.devtools.build.lib.rules.cpp.CcToolchain.getBuildVariables(CcToolchain.java:901)\r\n\tat com.google.devtools.build.lib.rules.cpp.CcToolchain.create(CcToolchain.java:596)\r\n\tat com.google.devtools.build.lib.rules.cpp.CcToolchain.create(CcToolchain.java:79)\r\n\tat com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:380)\r\n\tat com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:250)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:534)\r\n\tat com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:747)\r\n\tat com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:311)\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:382)\r\n\t... 4 more\r\njava.lang.RuntimeException: Unrecoverable error while evaluating node '@local_config_cc//:cc-compiler-darwin_x86_64 BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false' (requested by nodes '//tensorflow:libtensorflow.so BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '@bazel_tools//tools/cpp:malloc BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '@bazel_tools//tools/cpp:stl BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '//tensorflow:libtensorflow_framework.so BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false')\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:460)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:355)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalArgumentException: No supported apple platform registered for target cpu ?os_x86_64\r\n\tat com.google.devtools.build.lib.rules.apple.ApplePlatform.forTargetCpu(ApplePlatform.java:165)\r\n\tat com.google.devtools.build.lib.rules.apple.ApplePlatform.forTarget(ApplePlatform.java:151)\r\n\tat com.google.devtools.build.lib.rules.apple.AppleConfiguration.getSingleArchPlatform(AppleConfiguration.java:260)\r\n\tat com.google.devtools.build.lib.rules.apple.cpp.AppleCcToolchain.addBuildVariables(AppleCcToolchain.java:70)\r\n\tat com.google.devtools.build.lib.rules.cpp.CcToolchain.getBuildVariables(CcToolchain.java:901)\r\n\tat com.google.devtools.build.lib.rules.cpp.CcToolchain.create(CcToolchain.java:596)\r\n\tat com.google.devtools.build.lib.rules.cpp.CcToolchain.create(CcToolchain.java:79)\r\n\tat com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:380)\r\n\tat com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:250)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:534)\r\n\tat com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:747)\r\n\tat com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:311)\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:382)\r\nFAILED: Build did NOT complete successfully (18 packages loaded)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi, I added system information.\r\nThanks", "Thanks @ebubekirtabak . Did you make sure to follow the instructions for [preparing the Mac OS X environment](https://www.tensorflow.org/install/install_sources#PrepareMac)? Can you include the commands you used to install those dependencies?", "Yes, I installed environments and tested on python.\r\n\r\n````\r\n ebubekirtabak@Ebubekir-MacBook-Pro \ue0b0 ~ \ue0b0 python                                                                                                                                                                   \ue0b2 127 \u21b5 \ue0b2 1979 \ue0b2 20:56:43\r\nPython 2.7.15 (default, Jun 17 2018, 12:46:58)\r\n[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\n2018-07-17 23:53:16.670518: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n>>> print(sess.run(hello))\r\nHello, TensorFlow!\r\n>>>\r\n```", "Nagging Assignee @karmel: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@asimshankar -- are you the best person to advise on Go installation?", "From the error messages, in particular from:\r\n```\r\nCaused by: java.lang.IllegalArgumentException: No supported apple platform registered for target cpu ?os_x86_64\r\n```\r\nit appears to me that bazel is having trouble finding the compiler.\r\n\r\nCan you build other `bazel` based projects, for example https://docs.bazel.build/versions/master/tutorial/cpp.html ?\r\n\r\nOr for that matter, can you build any C++ program (e.g., is `xcode` installed and configured?)\r\n\r\n(BTW, note that we also have release binaries for OS X, so you don't need to build `libtensorflow.so` from source if you're interested in the Go API.  See https://www.tensorflow.org/install/install_go", "@asimshankar -- Hi,\r\nxcode installed and configurated.I'm at work now.I will try to create a C++ program.and I will share the result with you.\r\nThanks", "Hi,\r\nI solved problem.I could not build a C++ program.I Then changed System language to English.\r\nProblem is solved and builded Tensorflow Go.\r\n\r\n```\r\nINFO: Elapsed time: 1384.367s, Critical Path: 119.16s\r\nINFO: 3490 processes, local.\r\nINFO: Build completed successfully, 3550 total actions\r\n```\r\n\r\nThank you so much. :)\r\n\r\n", "I don't believe this is an issue with bazel! Changing system language solve my problem, too. Thanks @ebubekirtabak . Here is my environment:\r\n```\r\n== check python ===================================================\r\npython version: 2.7.12\r\npython branch:\r\npython build version: ('v2.7.12:d33e0cf91556', 'Jun 26 2016 12:10:39')\r\npython compiler version: GCC 4.2.1 (Apple Inc. build 5666) (dot 3)\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Darwin\r\nos kernel version: Darwin Kernel Version 19.4.0: Wed Mar  4 22:28:40 PST 2020; root:xnu-6153.101.6~15/RELEASE_X86_64\r\nos release version: 19.4.0\r\nos platform: Darwin-19.4.0-x86_64-i386-64bit\r\nlinux distribution: ('', '', '')\r\nlinux os distribution: ('', '', '')\r\nmac version: ('10.15.4', ('', '', ''), 'x86_64')\r\nuname: ('Darwin', 'Ibrahim-MacBook-Pro-2.local', '19.4.0', 'Darwin Kernel Version 19.4.0: Wed Mar  4 22:28:40 PST 2020; root:xnu-6153.101.6~15/RELEASE_X86_64', 'x86_64', 'i386')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple clang version 11.0.3 (clang-1103.0.32.29)\r\nTarget: x86_64-apple-darwin19.4.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== check pips ===================================================\r\nnumpy              1.16.6\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 147: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(2, 7, 12, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\nBuild label: 0.26.1\r\nBuild time: Thu Jun 6 11:08:11 2019 (1559819291)\r\nBuild timestamp: 1559819291\r\nBuild timestamp as int: 1559819291\r\n```"]}, {"number": 20814, "title": " No module named 'datasets'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary by pip\r\n- **TensorFlow version (use command below)**:1.2.0\r\n- **Python version**:3.6 \r\n- **Bazel version (if compiling from source)**:1.15.0\r\n- **GCC/Compiler version (if compiling from source)**:5.5.0\r\n- **CUDA/cuDNN version**:8.0\r\n- **GPU model and memory**:5.5.10\r\n- **Exact command to reproduce**:N/A\r\n\r\n### Describe the problem\r\nI want to run the command 'python mobilenet.py', but this error occurs to me. I followed some solutions, such as [link1](https://github.com/tensorflow/models/issues/3233) and [link2](https://stackoverflow.com/questions/39811840/how-can-jupyter-access-a-new-tensorflow-module-installed-in-the-right-path). However they don't work for me. I wonder if there is a way to solve it and how. Thank you!\r\n", "comments": ["Nagging Assignee @tatatodd: It has been 76 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry to know that those solutions didn't work for you. You need to configure the PYTHONPATH correctly.\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}]