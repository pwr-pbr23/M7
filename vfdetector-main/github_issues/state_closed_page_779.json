[{"number": 30181, "title": "Can't cross-compile TFLite \"Minimal\" for Raspberry Pi Zero (armv6 target)", "body": "**System information**\r\n- OS Platform and Distribution: Linux Debian 10\r\n- TensorFlow installed from: source\r\n- TensorFlow version: master branch @ a6989c95e336eb9f73ac7dd2920bee76d7e7e49e\r\n- GCC/Compiler version: 8.3.0 (Debian 8.3.0-2)\r\n- Python, Installed using ..., Bazel, CUDA, GPU: n/a\r\n\r\n**Background**\r\nHi,\r\nI've been trying to build a Tensorflow Lite C++ project for the Raspberry Pi Zero. For a simple base, I'm working from the \"minimal\" example provided. Building the Tensorflow Lite static library natively can take upwards of 5-6 hours, and for simplicity I've been trying to cross-compile. However, I haven't yet gotten minimal to compile. Although there's documentation on compiling for armv7 Pi's, I haven't been able to find anything on an armv6 platform.\r\n\r\n**Steps I've taken, following the docs [here](https://www.tensorflow.org/lite/guide/build_rpi).**\r\n- Git clone the tf repo\r\n- Run ``download_dependencies.sh``\r\n- Modify ``build_rpi_lib.sh`` to target armv6, i.e. with:\r\n```\r\nCC_PREFIX=arm-linux-gnueabihf- make -j 3 -f tensorflow/lite/tools/make/Makefile TARGET=rpi TARGET_ARCH=armv6\r\n```\r\n- First attempt! Run that build script. It quickly errors out in a few different header, saying `` sorry, unimplemented: Thumb-1 hard-float VFP ABI``. For example:\r\n```\r\nIn file included from /usr/arm-linux-gnueabihf/include/c++/8/bits/stl_algobase.h:62,\r\n                 from /usr/arm-linux-gnueabihf/include/c++/8/memory:62,\r\n                 from ./tensorflow/lite/arena_planner.h:18,\r\n                 from tensorflow/lite/arena_planner.cc:15:\r\n/usr/arm-linux-gnueabihf/include/c++/8/ext/type_traits.h: In function \u2018bool __gnu_cxx::__is_null_pointer(std::nullptr_t)\u2019:\r\n/usr/arm-linux-gnueabihf/include/c++/8/ext/type_traits.h:162:35: sorry, unimplemented: Thumb-1 hard-float VFP ABI\r\n   __is_null_pointer(std::nullptr_t)\r\n                                   ^\r\nmake: *** [tensorflow/lite/tools/make/Makefile:242: /home/cgeary1/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow/lite/allocation.o] Error 1\r\n```\r\n**- I wonder if I could get away using the traditional ARM instruction set instead of Thumb...** Add``-marm`` to CFLAGS and CXXFLAGS in ``rpi_makefile.inc`` under the armv6 ifeq, then try again. I.e.:\r\n```\r\n  ifeq ($(TARGET_ARCH), armv6)\r\n    CXXFLAGS += \\\r\n      -march=armv6 \\\r\n      -mfpu=vfp \\\r\n      -funsafe-math-optimizations \\\r\n      -ftree-vectorize \\\r\n      -fPIC \\\r\n      -marm\r\n\r\n    CFLAGS += \\\r\n      -march=armv6 \\\r\n      -mfpu=vfp \\\r\n      -funsafe-math-optimizations \\\r\n      -ftree-vectorize \\\r\n      -fPIC \\\r\n      -marm\r\n\r\n    LDFLAGS := \\\r\n      -Wl,--no-export-dynamic \\\r\n      -Wl,--exclude-libs,ALL \\\r\n      -Wl,--gc-sections \\\r\n      -Wl,--as-needed\r\n  endif\r\n```\r\n- Compiles for a little while, and finishes libtensorflow-lite.a !! But it errors out as soon as it moves on to minimal. Complaints look like ``undefined reference to `__atomic_load_8'`` and ``undefined reference to `flatbuffers::ClassicLocale::instance_'``. Relevant log is attached for all the details.\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/3331025/log.txt)\r\n\r\nI'm not sure whether I'm doing something wrong, whether there's a bug in the make system, or whether there may be a more intractable problem trying to cross-compile this codebase for the Pi Zero. I'm not really sure what else to try, so for now I'm just working on the Pi.\r\n\r\nEdit: I'm not able to try this right now, but I've realized that the flatbuffers error is likely to be solved as in #29806. The __atomic_load_8 error is the one that I can't get past.\r\n\r\n\r\n", "comments": ["> Complaints look like undefined reference to `__atomic_load_8'\r\n\r\n@covertg Add `-latomic` to your LDFLAGS", "@andrey-utkin That did it! Thank you!", "Closing this issue since the issue is resolved. Please reopen if the issue still persists. Thanks!\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30181\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30181\">No</a>\n", "@covertg I am trying to cross compile on Raspberry pi zero. Appreciate if you could share yr code in a github gist or repo? thanks", "@andrey-utkin\r\nadding -latomic to LDFLAGS  didn't work for me.\r\nI also tried adding  `set(CMAKE_CXX_LINK_FLAGS \"${CMAKE_CXX_LINK_FLAGS} -latomic\")` to the end of every `CMakeLists.txt` but it also failed."]}, {"number": 30180, "title": "How to specify shape of input for TFLite model after receiving SavedModel format?", "body": "I use TF 2.0. Let's suppose I have a model that I successfully converted to SavedModel format. I used following code to convert my model in Imperative API (subclassed tf.keras.Model) to SavedModel:\r\n\r\n```python\r\ntf.keras.experimental.export_saved_model(\r\n    model, file_path,\r\n    serving_only=True,\r\n    input_signature=[tf.TensorSpec(shape=[None, None, None, 3], dtype=tf.float32)])\r\n```\r\n\r\nNow I want to convert my model to TFLite, but I have a problem, because I need to specify all dims except for batch. It's okay, but I want to do it after saving my model in SavedModel format. For example, if I want to do several models with different shapes, this feature is required (e.g, [None, 1280, 720, 3], [None, 600, 600, 3] etc.). \r\n\r\nHow can I do it in step where I convert my model to TFLite? I mean in this step (or something like that after receiving SavedModel format):\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(file_path)\r\ntflite_model = converter.convert()\r\n```\r\n\r\nMaybe I can specify `input_shape` in `convert()` or another way?", "comments": ["There is no option to specify `input_shape` in 2.0. You can reload your SavedModel yourself, set the input shape, and then call `from_concrete_functions`. Something similar to the following code (which hasn't been tested):\r\n\r\n```\r\nmodel = tf.saved_model.load(export_dir)\r\nconcrete_func = model.signatures[\r\n  tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\nconcrete_func.inputs[0].set_shape([None, 1280, 720, 3])\r\nconverter = TFLiteConverter.from_concrete_functions([concrete_func])\r\n...\r\n```", "@gargn oh, thanks, great workaround :)", "We can leave this issue to be opened if you plan to add this ability to converter directly.", "Adding this functionality would require the addition of `input_arrays` to the API. However, with the functions based focus in 2.0, users aren't expected to know their model's input arrays. We determined adding `input_shapes` to the API would introduce additional usability issues. Since the workaround is relatively simple, we decided that was the preferred approach for saved models with dynamic shapes.\r\n\r\nWe will be investigating how to improve support for dynamic shapes in TFLite which should eliminate the need for this parameter: https://github.com/tensorflow/tensorflow/issues/24607.", "> There is no option to specify `input_shape` in 2.0. You can reload your SavedModel yourself, set the input shape, and then call `from_concrete_functions`. Something similar to the following code (which hasn't been tested):\r\n> \r\n> ```\r\n> model = tf.saved_model.load(export_dir)\r\n> concrete_func = model.signatures[\r\n>   tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n> concrete_func.inputs[0].set_shape([None, 1280, 720, 3])\r\n> converter = TFLiteConverter.from_concrete_functions([concrete_func])\r\n> ...\r\n> ```\r\n\r\ni have train on  [here collab image classificatioin](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb) and have model.tflite, could you give some full tutorial to modified the shape until export the tflite ?\r\n```\r\ninception_v3_spec = model_spec.ImageModelSpec(\r\n    uri='https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1')\r\ninception_v3_spec.input_image_shape = [299, 299] // this same like above ? but i got error buffersize when implementation to android (force close)\r\n```\r\n\r\n\r\nbut in object detection guide\r\n* Convert the frozen graph to the TFLite model.\r\n```\r\ntflite_convert \\\r\n  --input_shape=1,300,300,3 \\\r\n  --input_arrays=normalized_input_image_tensor \\\r\n  --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 \\\r\n  --allow_custom_ops \\\r\n  --graph_def_file=exported_model/tflite_graph.pb \\\r\n  --output_file=<directory with the TensorFlow examples repository>/lite/examples/object_detection/android/app/src/main/assets/detect.tflite\r\n```\r\n`input_shape=1,300,300,3` because the pretrained model works only with that input shape.\r\n\r\n`allow_custom_ops` is necessary to allow TFLite_Detection_PostProcess operation.\r\n\r\n`input_arrays` and `output_arrays` can be drawn from the visualized graph of the example detection model.\r\n```"]}, {"number": 30179, "title": "How to add value range constraints for 1-norm of vector?", "body": "In my problem, I want to train some specific vectors which meet several unusual constraints. However, the usual norm constraints in tensorflow cannot be directly used in my problems. The 1-norm of vector I want to train must meet the minimum and maximum values constraints, i.e.\r\n<img src=\"http://chart.googleapis.com/chart?cht=tx&chl= 0 \\leq \\||\\mathbf{x}\\||^1_1 \\leq 2\" style=\"border:none;\">. \r\nAs for vector with two elements, the 1-norm constraint means the sum of the absolute values of the two elements must meet the minimum and maximum values constraints.\r\nI hope I have made my problem clear enough for you to understand. ", "comments": []}, {"number": 30178, "title": "conv2d_transpose param tensor shapes differ from conv2d shapes", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- Mobile device: Not tested\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.7.3\r\n- Bazel version:\r\n- GCC/Compiler version:\r\n- CUDA/cuDNN version: 10.0/7.3.1\r\n- GPU model and memory: Nvidia, driver 418.56, 11178MiB\r\n\r\n**Describe the current behavior**\r\nWhen training a model which contains conv2d_transpose layers (using the NCHW format), the last 2 dimensions of the layers weight tensor swap when reloading the model. \r\n\r\nExample: \r\n- after building the model: transpose_layer1 has the weight tensor (3, 3, 63, 61)\r\n- when reloading the model: transpose_layer1 has the weight tensor (3, 3, 61, 63)\r\n\r\nAfter swapping the two dimensions via numpys transpose method the shape (3, 3, 63, 61) is restored and the model successfully trains.\r\n\r\n**Describe the expected behavior**\r\nThe expected behaviour would be that the last 2 dimensions don't swap which is the case for all conv2d layers. This occurs only in the conv2d_transpose layers.\r\n\r\n**Setup to reproduce the issue**\r\nThe minimum is a small model which has a conv2d_transpose layer in it in which the last two dimensions differ from each other. After training it on GPU with NCHW data format the described behaviour occurs. \r\n", "comments": ["@kassemyu In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Hey @gadagashwini, sorry for the long wait. \r\n\r\nThis is a small code snippet for reproduction. Please set the breakpoint at the last `print(...)` and evaluate the shapes-dictionary. You will see that `conv2d` layer 'conv1' has the kernel shape \r\n`(3, 3, 1, 32)`, indicating that 1 is the depth/number of channels for the input. \r\n32 is the number of filters/outputs/feature maps. \r\n\r\nIf you look at the `conv2d_transposed` layer 'up1' the number of outputs is set to 16 but the kernel shape is `(3,3,16,32)` indicating that the input dimensions are 16 even though they are 32 from the `conv2d` layer 'conv3' preceding it. I believe that this shape should instead be `(3,3,32,16)` because the number of outputs for the `conv2d_transposed` layer is set to 16. This would then behave the same way conv2d layers are at the moment.\r\n\r\nCan you please take a look at this behaviour?\r\nI originally thought that this had to do with the `data_format` but the described behaviour was observed for both data_formats and had nothing to do with reloading the saved tensors.\r\n\r\nSo far I have experienced this in tf 1.12 and 1.13.\r\n\r\nWith kind regards,\r\nYussuf\r\n\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.layers as layers\r\n\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\r\ntrain_X = mnist.train.images\r\ntrain_Y = mnist.train.labels\r\ntest_X = mnist.test.images\r\n\r\n\r\ndef shapes_of_built_model(layer_names):\r\n    layer_names_and_shapes = {}\r\n    for name in layer_names:\r\n        layer_names_and_shapes[name] = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)[0].shape\r\n    return layer_names_and_shapes\r\n\r\n\r\ndef model(inputs_):\r\n    # Encoder\r\n    conv1 = layers.conv2d(inputs_, num_outputs=32, kernel_size=(3, 3), scope='conv1')\r\n    conv_str2 = layers.conv2d(conv1, num_outputs=32, kernel_size=(3, 3), stride=2, scope='conv_str2')\r\n    conv2 = layers.conv2d(conv_str2, num_outputs=32, kernel_size=(3, 3), scope='conv2')\r\n    encoded = layers.conv2d(conv2, num_outputs=32, kernel_size=(3, 3), stride=2, scope='encoding')\r\n\r\n    conv3 = layers.conv2d(encoded, num_outputs=32, kernel_size=(3, 3), scope='conv3')\r\n    upsample1 = layers.conv2d_transpose(conv3, num_outputs=16, kernel_size=3, stride=2, scope='up1')\r\n    upsample2 = layers.conv2d_transpose(upsample1, num_outputs=32, kernel_size=3,  stride=2, scope='up2')\r\n    logits = layers.conv2d(upsample2, num_outputs=1, kernel_size=(3, 3),  scope='logits', padding='SAME')\r\n    decoded = tf.sigmoid(logits, name='reconstruct')\r\n    return decoded\r\n\r\n\r\nwith tf.name_scope('Input'):\r\n    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='X')\r\n    y = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='Y')\r\n\r\noutput_logits = model(x)\r\n\r\nwith tf.variable_scope('Train'):\r\n    with tf.variable_scope('Loss'):\r\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\r\n    tf.summary.scalar('loss', loss)\r\n    with tf.variable_scope('Optimizer'):\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.05, name='Adam-op')\r\n        optimizer = optimizer.minimize(loss)\r\n    with tf.variable_scope('Accuracy'):\r\n        correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')\r\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\r\n    tf.summary.scalar('accuracy', accuracy)\r\n    with tf.variable_scope('Prediction'):\r\n        cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')\r\n\r\n\r\ninit = tf.global_variables_initializer()\r\nmerged = tf.summary.merge_all()\r\nsess = tf.InteractiveSession()\r\nsess.run(init)\r\n\r\nnames = ['conv1', 'conv_str2', 'conv2', 'encoding', 'conv3', 'up1', 'up2']\r\nshapes = shapes_of_built_model(names)\r\nprint('set breakpoint here')\r\n```", "This issue is continued at: https://github.com/tensorflow/tensorflow/issues/31192"]}, {"number": 30177, "title": "Add a missing header file to MICROLITE_CC_HDRS.", "body": "When running\r\n\r\n$ make -f tensorflow/lite/experimental/micro/tools/make/Makefile generate_projects\r\n\r\nthe kernel tests fails to compile for mbed since tensorflow/lite/kernels/internal/compatibility.h is missing from the header list in the Makefile.\r\n\r\nThis PR adds that file to the list of headers.", "comments": []}, {"number": 30176, "title": "Getting `bad_alloc` in allocateTensors ", "body": "**What I've done:**\r\nI've picked up [one of iOS Tensorflow examples]([https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/ios]) and tried to use my `.tflite` model\r\n\r\n**What I expected**:\r\nLoaded model just like in the example.\r\n\r\n**What I've got**:\r\n`uncaught exception of type std::bad_alloc` and here are the logs with it:\r\n```\r\n    2019-06-26 15:25:45.414765+0300 Celly[5485:1152189] Initialized TensorFlow Lite runtime.\r\n    Celly(5485,0x1006aebc0) malloc: can't allocate region\r\n    *** mach_vm_map(size=1800945664) failed (error code=3)\r\n    Celly(5485,0x1006aebc0) malloc: *** set a breakpoint in malloc_error_break to debug\r\n    libc++abi.dylib: terminating with uncaught exception of type std::bad_alloc: std::bad_alloc\r\n```\r\n\r\n\r\nIs there any known workaround or help from documentation or so? Is this problem in the model, so it needs to be optimised or configured appropriately? ", "comments": ["It seems that the issue is coming from the model, so it needs to be optimized. I managed to avoid out of memory crash by increasing the number of threads the tensorflow interpreter could use.", "please follow this [**link**](https://stackoverflow.com/questions/9456728/how-to-deal-with-bad-alloc-in-c) and https://github.com/tensorflow/tensorflow/issues/30176#issuecomment-507981166 for the clarification of your issue, I hope it helps. Feel free to open new issue if the issue still persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30176\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30176\">No</a>\n"]}, {"number": 30175, "title": "Add link libtensorflow_framework.so => libtensorflow_framework.so.1", "body": "**System information**\r\n- TensorFlow version (you are using): 1.14.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nStarting from TensorFlow 1.14.0, the pip packages come with a libtensorflow_framework.so.1 instead of a libtensorflow_framework.so. This breaks the build process of codes that extend tensorflow. Adding a symbolic link libtensorflow_framework.so => libtensorflow_framework.so.1 would greatly help.\r\n", "comments": ["Unfortunately a wheel file is just a fancy zip archive so a symbolic link gets translated into a new copy of the file. So the file gets included twice, bloating the size of the pip package.\r\n\r\nWe will slowly evolve into a state where we can have the symlink in place, but it will take some time, unfortunately.", "The recommended approach for extending TF is to use tf.sysconfig.get_link_flags() instead of relying on manually specifying shared object path.", "Thanks for the quick reply. In the mean time, do you have any solution for a distutils-based package that has the following:\r\n```\r\nExtension('_my_custome_operations', source_files,\r\n        libraries=['tensorflow_framework'],\r\n        library_dirs=...,\r\n        ...)\r\n```\r\nBecause of the .1 suffix, the linker is not finding libtensorflow_framework. Putting the full name (libtensorflow_framework.so.1) doesn't work either. I had to either revert to Tensorflow 1.13.2 or add the symlink manually.", "Inspired by @byronyi 's comment, I tried what `tf.sysconfig.get_link_flags()` returns, i.e. I put `libraries=[':libtensorflow_framework.so.1']` and it works. I should probably find a way to just pass what `tf.sysconfig.get_link_flags()` to distutils, though.", "@mdorier Can you please let us know if you are happy to close if no issue persists", "\r\n\r\n\r\n> Inspired by @byronyi 's comment, I tried what `tf.sysconfig.get_link_flags()` returns, i.e. I put `libraries=[':libtensorflow_framework.so.1']` and it works. I should probably find a way to just pass what `tf.sysconfig.get_link_flags()` to distutils, though.\r\n\r\nhello mdorier , can you show the code which help you solved this issue, i met the same problem", "@mathlf2015 Right now I still sort of hard-code `libraries=[':libtensorflow_framework.so.2']` when creating the `Extension` object. My code is a bit particular, it runs on a supercomputer and I cannot `import tensorflow` on the node where I'm building it (tensorflow is cross-compiler for compute nodes), so I ended up having my setup.py script load a small JSON file indicating the library name (`:libtensorflow_framework.so.2`), library dir, and include dir, as well as possible cxx_flags to add.", "> Right now I still sort of hard-code `libraries=[':libtensorflow_framework.so.2']` when creating the `Extension` object. My code is a bit particular, it runs on a supercomputer and I cannot `import tensorflow` on the node where I'm building it (tensorflow is cross-compiler for compute nodes), so I ended up having my setup.py script load a small JSON file indicating the library name (`:libtensorflow_framework.so.2`), library dir, and include dir, as well as possible cxx_flags to add.\r\n\r\n`File \"../../model/embedding/listing2vec.py\", line 247, in __init__\r\n    os.path.join(os.path.dirname(os.path.realpath(__file__)), 'utils/word2vec_ops.so'))\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/load_library.py\", line 61, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: libtensorflow_framework.so: cannot open shared object file: No such file or directory`\r\n\r\nmdorier ,thank you . when i try to load a xx.so  by load_library.py, i will always get the error ,i don't know how to us the \"tf.sysconfig.get_link_flags()\" to solve this . could you give me some advices.", "Not sure what you've done so far but on my laptop on Debian with tensorflow 2.1.0 installed via `pip` with the `--user` flag, `tf.sysconfig.get_link_flags()` returns `['-L/home/mdorier/.local/lib/python3.7/site-packages/tensorflow_core', '-l:libtensorflow_framework.so.2']`. From this, you should find out which one is a library directory (the one with `-L`) and which one is the library (`-l`), then remove these `-L` and `-l` and pass these as the `libraries` and `library_dirs` arguments to your `Extension` object in setup.py. That should build fine.\r\n\r\nThen if when trying to load the library you get a `No such file or directory`, try setting your `LD_LIBRARY_PATH` environment variable to point to the directory with the `libtensorflow_framework.so` library is located. I can't help much more; from experience this kind of problem can be very different from a platform to another.", "> Not sure what you've done so far but on my laptop on Debian with tensorflow 2.1.0 installed via `pip` with the `--user` flag, `tf.sysconfig.get_link_flags()` returns `['-L/home/mdorier/.local/lib/python3.7/site-packages/tensorflow_core', '-l:libtensorflow_framework.so.2']`. From this, you should find out which one is a library directory (the one with `-L`) and which one is the library (`-l`), then remove these `-L` and `-l` and pass these as the `libraries` and `library_dirs` arguments to your `Extension` object in setup.py. That should build fine.\r\n> \r\n> Then if when trying to load the library you get a `No such file or directory`, try setting your `LD_LIBRARY_PATH` environment variable to point to the directory with the `libtensorflow_framework.so` library is located. I can't help much more; from experience this kind of problem can be very different from a platform to another.\r\n\r\nthank you very much ,i solve my problem by [a](https://www.tensorflow.org/guide/create_op#compile_the_op_using_your_system_compiler_tensorflow_binary_installation) and [b](https://stackoverflow.com/questions/56888781/tensorflow-notfounderror-libtensorflow-framework-so-cannot-open-shared-file-or) and [c](https://github.com/openai/blocksparse/issues/41)"]}, {"number": 30174, "title": "using deeplab with TensorFlow Lite GPU delegate on ios", "body": "hi, did anyone know how I can use deeplab on ios application with tflite GPU delegate??, because I've searched everywhere but I didn't find anything.", "comments": ["This question is better suited for Stack Overflow as it's not a bug."]}, {"number": 30173, "title": "Keras model doesn't compile for Edge-tpu if Dense layer contains [16, 32, 64, ...] neurons", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow version (use command below): 1.15.0-dev20190626, 2.0-beta1\r\n- Python version: 3\r\n\r\n**Describe the current behavior**\r\nCouldn't convert keras model for Edge-tpu when Dense layer consists of [16, 32, 64, 128, 2^n and n>3]  neurons , but work for any other number.\r\nError:\r\n\r\n```\r\n ERROR: :119 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.\r\nERROR: Node number 33 (FULLY_CONNECTED) failed to prepare.\r\n```\r\n\r\n**Describe the expected behavior**\r\nConvert keras model for Edge-tpu successfully\r\n\r\n**Code to reproduce the issue**\r\nColab to reproduce issue:\r\nhttps://colab.research.google.com/drive/1H0L-UioDKHdl6QbT6OtJc5jplzWTi1qB", "comments": ["This looks like an Edge TPU issue that I'm not familiar with, so unassigning myself.", "Is this still an issue? Can you check with `TF2.0` and let us know whether the issue persists with latest TF version. Thanks!", "@YaroslavSchubert Is this still an issue? If not, please close this issue. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30173\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30173\">No</a>\n"]}, {"number": 30172, "title": "How to write the  .bmp images from tensors to disk after resizing ?", "body": "I am trying to read bmp images, resize them and write the image to disk using tensorflow. I have succeeded in reading it but unable to find a way write it to disk. Any idea how to do it ?\r\n\r\nHere is the code below:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nimg_path = \"D:/image01.bmp\"\r\n\r\nimg = tf.read_file(img_path)\r\n\r\nimg_decode = tf.image.decode_bmp(img, channels=1) # unit8 tensor\r\n\r\nIMG_WIDTH = 256\r\n\r\nIMG_HEIGHT = 256\r\n\r\nimg_cast = tf.cast(img_decode,dtype=tf.float32)\r\n\r\nimg_res = tf.image.resize_bilinear(img_4d, (IMG_HEIGHT, IMG_WIDTH), align_corners=True)\r\n```\r\nThe problem is I can't find a \"encode_bmp\" or any bmp related function that can be used to encode the image and save the resized image to disk.", "comments": ["@gadagashwini I tried reading, preprocessing images with tensorflow APIs. e.g. JPEG, PNG etc. I see a encode function for each formats but not the same for \"BMP\". Having that option to encode it back will help.", "@mohapatras Thanks for finding this. ", "@mohapatras I started and contributed the bmp decoder so that I can test TensorFlow in command line programs on Android (Android image decoding APIs are Java APIs. Not easy to use them in command line C++ programs). So, yes, if nobody contributed an encoder for bmp, maybe you can start one :-)", "@freedomtan I think \"decoder for bmp\" should be encoder.", "@mohapatras yes, updated :-)", "@ymodak Any solution yet on encoding bitmaps ? ", "To be honest, TensorFlow seems overkill for the flow described here. If you only want to programmatically resize images you can use ImageMagik or Python's PIL.\r\n\r\nI think adding an encoder for bitmaps to TensorFlow is very low priority, but PRs are always welcome.", "@mohapatras I added encode_bmp in tensorflow-io PR https://github.com/tensorflow/io/pull/539, you can take a look. If the API is stable enough later on we can push to TensorFlow core repo if needed.", "@yongtang Sorry for the late reply. I will look into it. Thank you.", "@mohapatras,\r\nCan you please let us know if [this PR](https://github.com/tensorflow/io/pull/539) has resolved this issue so that close it? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@rmothukuru Yes the issue is resolved. Sorry for the late reply."]}, {"number": 30171, "title": "undefined symbol: _ZN10tensorflow7strings6StrCatERKNS0_8AlphaNumES3_ error while importing tensorflow_text", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Kubuntu 18.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0b0\r\n- Python version: Anaconda python 3.7.3\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\nError on importing `tensorflow-text` making it impossible to be imported.\r\n\r\n**Describe the expected behavior**\r\nLibrary can be effortlessly imported and used.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nI created a new environment using\r\n```\r\nconda create --name tensorflow python=3.7 numpy matplotlib scikit-learn pandas scipy\r\nconda activate tensorflow\r\npip install tensorflow-text\r\n```\r\nthen, when trying to import `tensorflow_text` the following error appears\r\n```\r\n$ python\r\nPython 3.7.3 (default, Mar 27 2019, 22:11:17) \r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> import tensorflow_text as text\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/kuba/.anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_text/__init__.py\", line 20, in <module>\r\n    from tensorflow_text.python.ops import *\r\n  File \"/home/kuba/.anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_text/python/ops/__init__.py\", line 19, in <module>\r\n    from tensorflow_text.python.ops.greedy_constrained_sequence_op import greedy_constrained_sequence\r\n  File \"/home/kuba/.anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_text/python/ops/greedy_constrained_sequence_op.py\", line 34, in <module>\r\n    gen_constrained_sequence_op = load_library.load_op_library(resource_loader.get_path_to_datafile('_constrained_sequence_op.so'))\r\n  File \"/home/kuba/.anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/load_library.py\", line 61, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/kuba/.anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_text/python/ops/_constrained_sequence_op.so: undefined symbol: _ZN10tensorflow7strings6StrCatERKNS0_8AlphaNumES3_\r\n>>>\r\n```", "comments": ["Moved to https://github.com/tensorflow/text"]}, {"number": 30170, "title": "[XLA] Disable Dot->Multiply rewrite for outer products", "body": "Some backends (the graphcore one for instance)  might perform better with an outer product represented as a DOT operator, rather than a broadcast multiply.\r\n\r\nThis change adds an extra config flag to the set of algebraic simplifier options to allow the dot->multiply outer product rewrite to be disabled.\r\n", "comments": ["Could you provide some more context?\r\nIs the rewrite done for correctness or performance?\r\nDo all backends still work with the rewrite disabled?", "by default the rewrite optimization has been left 'on', so other backends should see no change in their behaviour.\r\n\r\nthe option is provided for performance.  the optimization which has been introduced into the algebraic simplifier isn't an optimization on our backend - it is a 'deoptimization'.\r\n\r\npersonally i don't like the algebraic simplifier, it seems to be a place where GPU (and maybe TPU?) optimizations are placed, without the option to disable them.  It would be better if it was a load of smaller stages which could be selected by different backends as appropriate.    however, since the introduction of the config options structure, at least there is a method for providing selection of the individual optimizations.  this change makes one of these.\r\n\r\n", "I understand your sentiment.\r\nCould you just double check that the GPU and CPU backends run as expected with this option off?\r\nI understand it's extra work, but I think it's best to avoid \"landmine\" \"break-everything\" options.", "sure thing, no prob.  i cannot test the GPU backend, but will test the CPU.\r\n\r\nby inspection though - line 99 in algebraic_simplifier.h (https://github.com/tensorflow/tensorflow/pull/30170/files#diff-7700c30a5844612ba5dcc1d2af9ad747R99) ensures that the option is 'true' unless explicitly set to false.\r\n\r\nLine 1743 in algebraic_simplifier.cc (https://github.com/tensorflow/tensorflow/pull/30170/files#diff-bd4d8ba07737ec3e43cba19e1badbe28R1743) is the only point in the main code where there is a change, and it is to add a test for the option which is true by default.\r\n\r\nThe unit test verifies that the default option does cause the rewrite.  It also tests that you can explicitly prevent it.  The unit test is run against the CPU backend and the GPU backend in the XLA GPU CI tests in the google test suite, so running the test suite against this diff will prove that it is ok.\r\n \r\n", "tested.  works ok with the CPU", "OK, this makes sense.", "thanks guys.\r\n\r\n@gbaned I do not think that either of the failures can be down to this change.  unless one is a merge issue - which i will check now.", "@DavidNorman Could you please resolve the conflicts? Thanks!", "done :)", "@DavidNorman I think I've approved before, I'm fine with leaving these tests as-is if the rest of the file follows the same convention.", "@cheshire thanks :)   i wasn't sure of the protocol there.", "@jlebar I think that the MacOS test is nothing to do with this change"]}, {"number": 30169, "title": "TFLite GPU works slower than CPU", "body": "\r\nI run tflite in Qualcomm 660, for a test\uff0cthere is just a con2d. But when i run it,\r\ncpu costs 30ms  while  cpu costs 130ms.\r\nI don't know why", "comments": ["\r\n![test](https://user-images.githubusercontent.com/8298991/60170483-18518980-983b-11e9-9a80-cce3d1aa3518.JPG)\r\n", "> I run tflite in Qualcomm 660, for a test\uff0cthere is just a con2d. But when i run it,\r\n> cpu costs 30ms while cpu costs 130ms.\r\n> I don't know why\r\n\r\nI believe there's a slight typo here. However can you please confirm the performance of gpu over cpu. Thanks!", "@ymodak  I am sorry ,i made a mistake in writing. gpu costs 130ms, much slower than cpu", "@zchfaq \r\n\r\nWow, that's a pretty weird tensor dims that I haven't seen in the past.  I'm pretty sure our shaders are not optimized for that use case.  How did you get to a tensor size of 1x1x32000x1 and then apply conv of 256x1x20x1?  Can you elaborate a bit?", "@impjdi \r\nI am  @zchfaq's colleague. Thank you for your prompt attention to this matter. Our task is audio processing\uff0cthe1x1x32000x1 tensor is original input \uff08sample rate 8000Hz\uff0c time 4s\uff09,apply conv of 256x1x20x1 for encoding.\r\n\r\n", "@maoxin7676 \r\n\r\nThanks for clarification.  I was able to check that it's even worse on my Pixel 2 XL than what you reported.\r\n\r\n[conv_2d.tflite.zip](https://github.com/tensorflow/tensorflow/files/3344500/conv_2d.tflite.zip)\r\n\r\nThis is an artificial TFLite file I have generated.  I'll check tomorrow why it's doing so bad.", "@maoxin7676 @zchfaq \r\n\r\nSorry that it took longer (I got busy earlier in the week, and then there was independence day holiday).\r\n\r\nAs I mentioned earlier, it's a pretty unusual configuration for which we haven't optimized.  I don't have your `CONV_2D`'s `Conv2DOptions`, so I cannot tell the exact settings.  However, now that everything is open sourced, you can play around yourself ;)\r\n\r\nThe parallelization of the `CONV_2D` op is specified right here:\r\n`//tensorflow/lite/delegates/gpu/gl/kernels/conv.cc` which may (or may not) lead to\r\n`//tensorflow/lite/delegates/gpu/gl/workgroups/default_calculator.cc`.  There is a limit of the total number of threads per GPU.  It can range from 128 to 512 depending on the model, but other than that, you can play around with the setting to optimize for your tensor shape.\r\n\r\nAlso note that the tensor shape of 1x1x32000x1 will be expanded internally to 1x1x32000x4.  So you will get a significant hit on the memory size.  OpenGL delegate is mainly memory bound, so if possible, I would try to change 1x1x32000x1 to 1x1x1x32000 and then change the `CONV_2D` somehow, but that may not be trivial.\r\n\r\nIf playing with workload and workgroup doesn't work, in worst case, you are left with writing your own shader program code for maximum parallelism.", "@Augnine Could you please let us know if this issue still persists ? If it is resolved then please feel free to move this issue to close status ? please check https://github.com/tensorflow/tensorflow/issues/30169#issuecomment-509416942 Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30169\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30169\">No</a>\n"]}, {"number": 30168, "title": "grpc+seastar: add grpc+seastar protocol", "body": "Re-submission of #27454 as the previous PR is messed up.\r\n\r\nThis PR serves as a placeholder for contribution from @liutongxuan and his colleagues in Alibaba.\r\n\r\nSince TF is going to have yet another (hopefully last) release before 2.0 (r1.14 to be cut on April 15), I am not sure if we have enough time (or incentive) to push this feature into the main repo.\r\n\r\ntensorflow/networking should serve as the final target after 2.0. As we are still transitioning and @annarev is working on the networking C API, I would still like to submit this PR against the main repo, mainly for the convenience of reviewing. When we feel like ready, we shall have another PR in https://github.com/tensorflow/networking for refactoring it as a standalone plugin.\r\n\r\nPing @jbedorf @poxvoculi @liutongxuan @lilbedwin @shanshanpt @YongCHN @swpnlptl.\r\n\r\nI believe there are a few review comments not fully addressed:\r\n\r\n- [ ] https://github.com/tensorflow/tensorflow/pull/27454#discussion_r296912350\r\n- [x] https://github.com/tensorflow/tensorflow/pull/27454#discussion_r296911797\r\n- [ ] https://github.com/tensorflow/tensorflow/pull/27454#issuecomment-504677772 (Optional)\r\n\r\nBoost is now not a direct dependency for this patchset, but still a transitive one for seastar.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30168) for more info**.\n\n<!-- need_author_consent -->", "CLA Agree", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30168) for more info**.\n\n<!-- need_author_cla -->", "Can one of the admins verify this patch?", "@liutongxuan Could you please sign cla ? Thank you.", "Agree", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 30167, "title": "Variable sized input doesn't work with dilated convolutional layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.10**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **v2.0.0-beta0-16-g1d91213fe7** -  **2.0.0-beta1**\r\n- Python version: **3.6**\r\n- CUDA/cuDNN version: **CPU**\r\n- GPU model and memory: **CPU**\r\n\r\n**Describe the current behavior**\r\nI have variable sized axis on the input for a dilated conv layer which fails when inputting the second batch(data is padded batch wise). I have tried additional padding both for % dilation_rate == 0 and % 2 == 0 without luck. This allows for some of the layers to work but not all, so it might be a simple calculation I need to figure out what additional padding to use but I cant figure out what this is.\r\n\r\n**Describe the expected behavior**\r\nWhen running without dilation im able to handle the variable sized input. Also the fact that the conv layers with dilation can handle any input as long as its the first I would expect that no additional padding would be needed.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nbs = 5\r\ntext_len_1 = 772\r\ntext_len_2 = 741\r\nembed_size = 300\r\nin_channels = 1\r\n\r\ntest_in_1 = tf.random.normal((bs, text_len_1, embed_size, in_channels))\r\ntest_in_2 = tf.random.normal((bs, text_len_2, embed_size, in_channels))\r\n\r\n\r\ndilated_convs = [tf.keras.layers.Conv2D(filters=10, kernel_size=(2, embed_size),\r\n                                                    dilation_rate=(dilation, 1),\r\n                                                    padding='valid')\r\n                             for dilation in range(2, 23)]\r\n\r\nfor conv in dilated_convs:\r\n    res = conv(test_in_1)\r\n\r\nfor conv in dilated_convs:\r\n    # Fails here, regardless of test_in_1 or 2 is called first\r\n    res = conv(test_in_2) \r\n```\r\n\r\n**Other info / logs**\r\n```\r\n2019-06-25 13:50:33.329503: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at spacetobatch_op.cc:219 : Invalid argument: padded_shape[0]=741 is not divisible by block_shape[0]=2\r\nTraceback (most recent call last):\r\n  File \"/home/name/anaconda3/envs/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-59b677f0b758>\", line 1, in <module>\r\n    runfile('/home/name/.PyCharm2019.1/config/scratches/dilated_conv_test.py', wdir='/home/name/.PyCharm2019.1/config/scratches')\r\n  File \"/snap/pycharm-professional/136/helpers/pydev/_pydev_bundle/pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"/snap/pycharm-professional/136/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/home/name/.PyCharm2019.1/config/scratches/dilated_conv_test.py\", line 22, in <module>\r\n    res = conv(test_in_2)\r\n  File \"/home/name/anaconda3/envs/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 712, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/name/anaconda3/envs/3.6/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 196, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/home/name/anaconda3/envs/3.6/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1078, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/home/name/anaconda3/envs/3.6/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 634, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/home/name/anaconda3/envs/3.6/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 617, in _with_space_to_batch_call\r\n    input=inp, block_shape=dilation_rate, paddings=paddings)\r\n  File \"/home/name/anaconda3/envs/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 9246, in space_to_batch_nd\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[0]=741 is not divisible by block_shape[0]=2 [Op:SpaceToBatchND]\r\n```", "comments": ["I have reproduced the issue in Colab using TF 2.0.0-beta1. Thanks!", "Awesome, hope you can help :) ", "Any updates on this?", "The issue here is your kernels is size of (2, embed_size), while the input size is (...,...,embed_size,...),\r\nso it is not feasible to dilate the input.\r\nSet your kernel to a smaller size would fix it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30167\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30167\">No</a>\n", "How come its working for the first run then? And isn't it the kernel thats dialated?\r\nAlso the dilation_rate=(dilation, 1) so it should only dialate in the text_len dimension?", "I'm having the same issue in Tensorflow 2.0.0-rc0, and it doesn't seem to have anything to do with the kernel size.\r\n\r\n```\r\nfrom tensorflow.keras import layers\r\n\r\nconvLayer = layers.Conv2D(\r\n    4, \r\n    (2,2),\r\n    data_format='channels_last',\r\n    padding='valid',\r\n    dilation_rate=3)\r\n```\r\n\r\nRunning:\r\n\r\n```\r\nconvLayer(np.random.random((3,6,6, 12)))\r\n```\r\nworks fine.\r\n\r\nBut if I run this right after:\r\n\r\n```\r\nconvLayer(np.random.random((3,7,7, 12)))\r\n```\r\n\r\nI receive the error message:\r\n\r\n```\r\nInvalidArgumentError: padded_shape[0]=7 is not divisible by block_shape[0]=3 [Op:SpaceToBatchND] \r\n```\r\n\r\n\r\nChanging the kernel size will not remove the error. Is there any possible way of allowing dilations with various input sizes without having to pad the inputs? ", "I'm still having this issue on tensorflow 2.2.0:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.layers.Conv2D(filters=8, kernel_size=3, dilation_rate=3)\r\n\r\nx1 = tf.ones((1, 12, 16, 1))\r\nx2 = tf.ones((1, 13, 16, 1))\r\n\r\nprint(model(x1).shape)\r\nprint(model(x2).shape)\r\nprint(model(x1).shape)\r\n```\r\n\r\nError logs:\r\n```\r\n(1, 6, 10, 8)\r\n(1, 7, 10, 8)\r\n2021-11-23 15:07:40.114251: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at spacetobatch_op.cc:219 : Invalid argument: padded_shape[0]=14 is not divisible by block_shape[0]=3\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 10, in <module>\r\n    print(model(x1).shape)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 968, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py\", line 207, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1106, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 638, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 621, in _with_space_to_batch_call\r\n    input=inp, block_shape=dilation_rate, paddings=paddings)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 9491, in space_to_batch_nd\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 6653, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[0]=14 is not divisible by block_shape[0]=3 [Op:SpaceToBatchND]\r\n```"]}, {"number": 30166, "title": "How does Tensorflow convert to TensorflowJS?", "body": "I am working on a conversion of a Java machine learning platform. I am curious on how did TensorFlow manage to transcompile. If so, which toolsets are used?", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 30165, "title": "TF 2.0 - Put Tensor into some Numpy functions continuously increases memory usage", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip package tensorflow==2.0.0-beta1\r\n- TensorFlow version (use command below): v2.0.0-beta0-17-g8e423e3 2.0.0-beta1\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.0.0/7.3.1\r\n- GPU model and memory: Titan Xp 11Gb\r\n\r\n**Describe the current behavior**\r\n Memory leak when we put Tensor into some Numpy functions (ex - np.array(), np.zeros_like()). Following attached code continuously increases memory usage.\r\n\r\n**Describe the expected behavior**\r\nNo memory usage explosion.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\nx = tf.random.normal((1024, 1024))\r\nfor i in range(int(1e7)):\r\n    y = np.array(x)\r\n    time.sleep(0.01)\r\n```", "comments": ["I have tried on Colab with TF GPU version 2.0beta1 and was able to reproduce the issue. The code snippet caused crashed in Colab after using all available RAM.  ", "@superbobry is this related to the recent __array_interface__ changes?", "I couldn't reproduce the issue using the nightly when running on CPU. Will be able to try on GPU on Monday.", "I've reproduced the leak on GPU, fix is on the way.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30165\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30165\">No</a>\n"]}, {"number": 30164, "title": "Tensorflow Webpage API tab Link error", "body": "## Description of issue (what needs changing):\r\n\r\nThe Tensorflow API for 1.13 leads to 1.12 page kindly fix the hyperlink .\r\n\r\n### Clear description\r\n\r\nThe change in the API list can cause confusion\r\n\r\n### Request visuals, if applicable\r\n![image](https://user-images.githubusercontent.com/11817160/60156197-98f79200-9809-11e9-846d-c7d2d1471a74.png)\r\n", "comments": ["Fixed in https://github.com/tensorflow/tensorflow/issues/30206"]}, {"number": 30163, "title": "Load image tutorials ", "body": "You didn't use keras_ds or feature_map_batch in model.fit", "comments": ["@fardindadashi Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the reproducible code snippet. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "I'm just saying the tutorials in https://www.tensorflow.org/tutorials/load_data/images the  keras_ds or feature_map_batch have not been used in model.fit function. Is that OK?", "@fardindadashi In the mentioned tutorial, the model.fit uses the dataset of image_paths and image_labels that is ds. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 30162, "title": " tf.keras model.fit calls slow with TPU distribute strategy", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below):1.14\r\n- Python version:3,6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTPU distribution strategy does not support model.fit_generator, and repeated model.fit calls result in a 50x slowdown presumably because it adds operations to graph. \r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n\r\nresolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)\r\ntf.contrib.distribute.initialize_tpu_system(resolver)\r\nstrategy =  tf.distribute.experimental.TPUStrategy(resolver)\r\nwith strategy.scope():\r\n    model = ..... ## Your tf.keras model\r\n    model.compile(loss = custom_loss,optimizer ='custom_optimizer)\r\n \r\nfor i in range(num_its):\r\n      data,labels = = next(generator_fn()) \r\n      model.fit(data,labels)   \r\n\r\n     \r\n\r\n\r\n\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "I have just added a few lines to illustrate the issue. I have a very big workflow, so I cannot add the whole code here. The issue is that when model.fit is called in a loop(as shown above), the training slows down considerably. \r\n", "@capilano It is difficult to reproduce the issue. Will it be possible to provide us the minimal code snippet to reproduce the issue. So that we can reproduce the issue on our environment for faster resolution. Thanks!", "I have not actually tested this code. I just made it here on the fly,so there may be minor errors.\r\nThe TPU distribute strategy does not support calls to the  model.fit_generator method because it throws an exception that  explicitly states that the model.fit_generator method is not supported with the current TPU distribute strategy. And so for a normal use case, in such a scenario an alternative would be to make a data generator function and make repeated calls to model.fit and this results in a 100x slowdown when compared to say, using tpu estimators instead.\r\n\r\n```\r\n#Importing Libraries\r\nimport numpy as np\r\nimport time\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.applications.resnet50 import ResNet50\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import GlobalAveragePooling2D,Dense,Input,Conv2D\r\n\r\n#Define network\r\n\r\ndef net():\r\n  inp = Input(shape=(224,224,3))\r\n  x = Conv2D(64,kernel_size =(3,3),padding='same',activation='relu',strides=2)(inp)\r\n  x = Conv2D(128,kernel_size =(3,3),padding='same',activation='relu',strides=2)(x)\r\n  x = Conv2D(256,kernel_size =(3,3),padding='same',activation='relu',strides=1)(x)\r\n  x = Conv2D(256,kernel_size =(3,3),padding='same',activation='relu',strides=2)(x)\r\n  x = Conv2D(256,kernel_size =(3,3),padding='same',activation='relu',strides=2)(x)\r\n  x = Conv2D(512,kernel_size =(3,3),padding='same',activation='relu',strides=2)(x)\r\n  x = Conv2D(512,kernel_size =(3,3),padding='same',activation='relu',strides=2)(x)\r\n  x = GlobalAveragePooling2D()(x)\r\n  out = Dense(10)(x)\r\n  model = Model(inputs = inp,outputs =out)\r\n  return model\r\n\r\n#TPU_init\r\nresolver = tf.contrib.cluster_resolver.TPUClusterResolver()\r\ntf.contrib.distribute.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\nwith strategy.scope():\r\n  model = net()\r\n  model.compile(loss = 'categorical_crossentropy',optimizer ='adam')\r\n\r\n#Data load\r\nx = np.ones((224,224,3),dtype=np.float32)\r\nn= np.random.randint(0,9)\r\ny= np.zeros((10,),dtype=np.float32)\r\ndata = []\r\nlabels = []\r\nfor i in range(1024):\r\n  data.append(x)\r\n  labels.append(y)\r\n\r\ndata = np.array(data)\r\nlabels = np.array(labels)\r\n\r\n#Fit function (Really slow. Should do this 100x faster)\r\nfor i in range(1000):\r\n  time1 = time.time()\r\n  model.fit(data,labels,batch_size = 1024,epochs=1)\r\n  time2 = time.time()\r\n  print(time2-time1)\r\n```\r\n\r\n## comments ##\r\n\r\n\r\nIdeally, the distribute strategy should support the fit_generator method because that makes it possible to use tf.records Dataset  and load data directly from GCS buckets because it is almost never going to be possible to preload data into memory esp when there is a data augmentation step in the input pipeline.", "@capilano I tried reproducing the issue with provided code but i received \r\n`name 'TPU_WORKER' is not defined`\r\nCan you help us to reproduce the issue. Thanks!", "TPU_WORKER is the TPU address. If you are using google colab (with TPU accelerator), I think you can leave it blank Just call the function without passing any argument. If that does not work,\r\n TPU_WORKER= 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\r\nI have also slightly updated the code, it works when eager execution is enabled. Otherwise, please replace the last two lines which make a one hot vector with np.zeros((10,))", "@capilano I have tried reproducing the issue by adding piece of code `tf.keras.layers.GlobalAveragePooling2D()(x)` Since GlobalAveragePolling2D(x) was not defined. But still i am unable to replicate your issue. Provide us the full minimal code snippet. It will indeed help us to move faster.", "Ok, I have changed the code. If you just copy/paste the code it should work. I have also changed the network, I am just using a few Conv layers to check, maybe the Renet50 Model has some unsupported layers. In the data loading path the last two lines are outside the for loop. I am not able to indent the code here for some reason", "@capilano Thanks for the complete code. I am able to reproduce the issue now with Tf 1.14.0. Thanks!", "I'm facing the same issue. As ```fit_generator``` does not work, I am using the same strategy because of the large dataset and involved data augmentation. But calling ```fit``` is extremely slow. \r\nThings that come to my mind:\r\n- re-loading the model to the TPU?\r\n- data transfer?\r\n\r\nEven when I load more data to decrease the ```fit``` calls (batch size > 2048), the sessions just fail. This makes me suspicious that it is maybe the data transfer... ", "You can use a tf records dataset and do this with fit and use your data augmentation pipeline using a map function as long as you can do your augmentations with tensorflow functions.", "I could reproduce the issue. But I am not sure whether it is 100X slower or not. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/50fceaf46f18b8a772a878c399f7676c/untitled570.ipynb) is the gist. Thanks!", "@jvishnuvardhan Just to give you guys a heads up, one  can directly pass a dataset to model.fit and so multiple calls to fit are not really necessary if you are using a pipeline with only tensorflow functions for data augmentation. ", "I am not surprised that the notebook is slow as the data processing is all happening on the Colab rather than the TPU system (which has much more processing power than the Colab VMs). \r\n\r\nWith the notebook, the `data` variable contains 224 x 224 x 3 x 4 (bytes/float) x 1024 = 588 MB of data, which has to be transferred per step. Transferring this amount of data over the network to the TPU + encoding and decoding overhead would be non-trivial.\r\n\r\nFor performance reasons especially on non-trivial image models, you need to use tf.data Datasets with TF supported ops, and load the raw data from GCS.", "Is `fit_generator` still not supported? "]}, {"number": 30161, "title": "tf.keras.BatchNormalization ignores Mask when calculating Mean/Variance", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: GTX 960M\r\n\r\n**Describe the current behavior**\r\ntf.keras.layers.BatchNormalization layers do not take the Mask into account when calculating mean/variance. This results in a Mean biased towards 0 and an inflated Variance when passing in padded samples. This is made worse by the fact that the BatchNormalization layer claims to support masking, yet all it does is pass it through to the next layer. This is an issue in TF 1.13, 1.14 and 2.0b where the implementation is imported straight from Keras.\r\n\r\nThe result from a toy example:\r\n```\r\npadded mean:\t\t1.5\r\nunpadded mean:\t\t3.0\r\n\r\npadded variance:\t3.25\r\nunpadded variance:\t2.0\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe Boolean Mask should be used when calculating the Mean and Variance, to ignore any 0's as a result of padded data.\r\n\r\n**Code to reproduce the issue**\r\nThis example compares the behavior of BatchNorm against a padded and un-padded 1D signal. The resulting Means and Variances come out very different.\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Mask value of -10\r\npadded_data = np.expand_dims(np.array([[1,2,3,4,5,-10,-10,-10,-10,-10] for _ in range(100)], dtype='float32'), 2)\r\ndata = np.expand_dims(np.array([[1,2,3,4,5] for _ in range(100)], dtype='float32'), 2)\r\n\r\ndef build_padding_model():\r\n    input = tf.keras.layers.Input((10, 1))\r\n    masked = tf.keras.layers.Masking(-10)(input)\r\n    normed = tf.keras.layers.BatchNormalization(momentum=0.01)(masked)\r\n    model = tf.keras.models.Model(input, normed)\r\n    model.compile(\"adam\", loss=\"mse\")\r\n    return model\r\n\r\ndef build_model():\r\n    input = tf.keras.layers.Input((5, 1))\r\n    normed = tf.keras.layers.BatchNormalization(momentum=0.01)(input)\r\n    model = tf.keras.models.Model(inputs=input, outputs=normed)\r\n    model.compile(\"adam\", loss=\"mse\")\r\n    return model\r\n\r\nif __name__ =='__main__':\r\n    batch_size = 100\r\n    signal_length = 20\r\n\r\n    pad = build_padding_model()\r\n    pad.fit(x=padded_data, y=padded_data, batch_size=10, epochs=5)\r\n\r\n    nopad = build_model()\r\n    nopad.fit(x=data, y=data, batch_size=10, epochs=5)\r\n\r\n    weights1 = pad.layers[2].get_weights()\r\n    weights2 = nopad.layers[1].get_weights()\r\n    print('\\npadded mean:\\t\\t' + str(weights1[2][0]) + '\\nunpadded mean:\\t\\t' + str(weights2[2][0]) + '\\n')\r\n    print('padded variance:\\t' + str(weights1[3][0]) + '\\nunpadded variance:\\t' + str(weights2[3][0]) + '\\n')\r\n```", "comments": ["For what it's worth, I have modified a version of the 1.13.1 BatchNormalization layer to take a mask in `call` which appears to be working. The change wasn't very big, though I haven't done anything to support renorm/fused/virtual/adjustment configurations. Would this be something worth creating a PR for?", "I have tried on colab with TF  1.13.1, 1.14 and 2.0b and was able to reproduce the issue.Thanks!", "The trick here is making sure we do this in a performant way, as the op being called underneath does not support masking. We could consider going a slower route, filtering out the masked items, in the cases where a mask exists. If you wanted to send a PR, we could take a look and see how bad the performance hit is/what the trade-offs are. ", "Hi @karmel \r\nSince there hasn't been any progress on the masked batch norm PR lately, I would like to ask about a cheaper workaround.\r\n\r\nInstead of zero padding the shorter sequences, one option is to sample from the existing sequences, e.g. by mirroring/replicating samples from the beginning/end. Could this be done conveniently in the `padded_batch` method of `tf.data.Dataset` with the `padding_values` parameter ?", "@georgesterpu -- a lot has changed since this issue was first posted. Is this still an issue in TF v2? Can you try with tf-nightly or the most recent release of tensorflow?", "@karmel thanks for the follow up.\r\nYes, this feature is not yet supported in tf-nightly, and I guess it is still an issue for anyone using the batch normalisation op on zero-padded sequences.", "@LukeBolly Thx for bring this up. I feel this an important feature to be added.\r\nBTW I have a question: I think this also a problem with LayerNormalization, where it does not use mask when calculating Mean/Variance. Please verify or correct me, if possible. @LukeBolly @georgesterpu Thx!", "OK after some research, I found LayerNormalization USES mask when computing these things. I modified the code of @LukeBolly as below to see this. Thx.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Mask value of -10\r\npadded_data = np.expand_dims(np.array([[1,2,3,4,5,-10,-10,-10,-10,-10] for _ in range(1000)], dtype='float32'), 2)\r\ndata = np.expand_dims(np.array([[1,2,3,4,5] for _ in range(1000)], dtype='float32'), 2)\r\n\r\ndef build_padding_model():\r\n    input = tf.keras.layers.Input((10, 1))\r\n    masked = tf.keras.layers.Masking(-10)(input)\r\n    normed = tf.keras.layers.LayerNormalization()(masked)\r\n    model = tf.keras.models.Model(input, normed)\r\n    model.compile(\"adam\", loss=\"mse\")\r\n    return model\r\n\r\ndef build_model():\r\n    input = tf.keras.layers.Input((5, 1))\r\n    normed = tf.keras.layers.LayerNormalization()(input)\r\n    model = tf.keras.models.Model(inputs=input, outputs=normed)\r\n    model.compile(\"adam\", loss=\"mse\")\r\n    return model\r\n\r\nif __name__ =='__main__':\r\n    batch_size = 100\r\n    signal_length = 20\r\n\r\n    pad = build_padding_model()\r\n    pad.fit(x=padded_data, y=padded_data, batch_size=10, epochs=5)\r\n\r\n    nopad = build_model()\r\n    nopad.fit(x=data, y=data, batch_size=10, epochs=5)\r\n\r\n    weights1 = pad.get_weights()\r\n    weights2 = nopad.get_weights()\r\n\r\n    print(\"\\npadded mean\" + str(weights1) + '\\n')\r\n    print(\"\\nopad mean\" + str(weights2) + '\\n')\r\n```", "> OK after some research, I found LayerNormalization USES mask when computing these things. I modified the code of @LukeBolly as below to see this. Thx.\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> import numpy as np\r\n> \r\n> # Mask value of -10\r\n> padded_data = np.expand_dims(np.array([[1,2,3,4,5,-10,-10,-10,-10,-10] for _ in range(1000)], dtype='float32'), 2)\r\n> data = np.expand_dims(np.array([[1,2,3,4,5] for _ in range(1000)], dtype='float32'), 2)\r\n> \r\n> def build_padding_model():\r\n>     input = tf.keras.layers.Input((10, 1))\r\n>     masked = tf.keras.layers.Masking(-10)(input)\r\n>     normed = tf.keras.layers.LayerNormalization()(masked)\r\n>     model = tf.keras.models.Model(input, normed)\r\n>     model.compile(\"adam\", loss=\"mse\")\r\n>     return model\r\n> \r\n> def build_model():\r\n>     input = tf.keras.layers.Input((5, 1))\r\n>     normed = tf.keras.layers.LayerNormalization()(input)\r\n>     model = tf.keras.models.Model(inputs=input, outputs=normed)\r\n>     model.compile(\"adam\", loss=\"mse\")\r\n>     return model\r\n> \r\n> if __name__ =='__main__':\r\n>     batch_size = 100\r\n>     signal_length = 20\r\n> \r\n>     pad = build_padding_model()\r\n>     pad.fit(x=padded_data, y=padded_data, batch_size=10, epochs=5)\r\n> \r\n>     nopad = build_model()\r\n>     nopad.fit(x=data, y=data, batch_size=10, epochs=5)\r\n> \r\n>     weights1 = pad.get_weights()\r\n>     weights2 = nopad.get_weights()\r\n> \r\n>     print(\"\\npadded mean\" + str(weights1) + '\\n')\r\n>     print(\"\\nopad mean\" + str(weights2) + '\\n')\r\n> ```\r\n\r\nI tried this code in colab with tf2.3.0 and the padded mean and nopad mean are exactly the same. However, if we change it to batch norm, the results are different.\r\n\r\nMoreover, for layer norm we could not pass a mask explicitly when calling it. This inconsistent behaviour is really confusing.", "@mathpluscode: Hi,\r\n\r\n\"However, if we change it to batch norm, the results are different.\" -> Batch norm in TF implementation does not take masking into account I think.\r\n\"Moreover, for layer norm we could not pass a mask explicitly when calling it\" -> We can do that actually - just created a custom layer and pass a mask you want thorough that layer. I did it before.", "> @mathpluscode: Hi,\r\n> \r\n> \"However, if we change it to batch norm, the results are different.\" -> Batch norm in TF implementation does not take masking into account I think.\r\n> \"Moreover, for layer norm we could not pass a mask explicitly when calling it\" -> We can do that actually - just created a custom layer and pass a mask you want thorough that layer. I did it before.\r\n\r\nHi @hoangcuong2011, thanks for your reply!\r\n\r\nAre you saying that we can write a custom layer to wrap the `tf.keras.layers.LayerNormalization` and use `tf.keras.layers.Masking` to use the given mask? Ideally, I'd like to call like `layer_norm(inputs=x, mask=mask)` to use layer norm.\r\n\r\nI'm not sure if I understand your meaning accurately. If it's possible, do you mind to share a piece of the code ;) Super thx!\r\n\r\n", "\"Are you saying that we can write a custom layer to wrap the tf.keras.layers.LayerNormalization and use tf.keras.layers.Masking to use the given mask?\" -> Yes this is what I meant. I previously wrote a custom layer that has tf.keras.layers.LayerNormalization inside. If you read about custom layer you might notice that we can pass whatever mask we want inside the custom layer.\r\n\r\nAbout the code: Sorry I did this for my previous company so I cannot share the code. But it is not that hard believe me. Plus you could learn something from doing that.\r\n", "@LukeBolly \r\nIs this still an issue,Could you check on later versions of 2.x  as 1.x is not supported actively", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I know this is a pretty old thread but just wanted to add some more info for this for those who may still be interested...\r\n\r\nI tested the PR from @LukeBolly that implements the masking in BatchNormalization (https://github.com/tensorflow/tensorflow/pull/30298) in TF 2.4 and it works when the input is 1D; however when it's a 2D input you have to pass in ```fused=False``` when defining the ```BatchNormalization``` layer. For example...\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Mask value of -10\r\npadded_data = np.expand_dims(np.array([[1,2,3,4,5,-10,-10,-10,-10,-10,6,7,8,9,10,-10,-10,-10,-10,-10] for _ in range(1000)], dtype='float32'), 2)\r\npadded_data = padded_data.reshape(1000, 2, 10, 1)\r\ndata = np.expand_dims(np.array([[1,2,3,4,5,6,7,8,9,10] for _ in range(1000)], dtype='float32'), 2)\r\ndata = data.reshape(1000, 2, 5, 1)\r\n\r\n\r\ndef build_padding_model():\r\n    input = tf.keras.layers.Input((2, 10, 1))\r\n    masked = tf.keras.layers.Masking(-10)(input)\r\n    normed = tf.keras.layers.BatchNormalization(fused=False)(masked)\r\n    model = tf.keras.models.Model(input, normed)\r\n    model.compile(\"adam\", loss=\"mse\")\r\n    return model\r\n\r\ndef build_model():\r\n    input = tf.keras.layers.Input((2, 5, 1))\r\n    normed = tf.keras.layers.BatchNormalization()(input)\r\n    model = tf.keras.models.Model(inputs=input, outputs=normed)\r\n    model.compile(\"adam\", loss=\"mse\")\r\n    return model\r\n\r\nif __name__ =='__main__':\r\n    batch_size = 100\r\n    signal_length = 20\r\n    \r\n    pad = build_padding_model()\r\n    pad.fit(x=padded_data, y=padded_data, batch_size=10, epochs=5)\r\n    \r\n    nopad = build_model()\r\n    nopad.fit(x=data, y=data, batch_size=10, epochs=5)\r\n    \r\n    weights1 = pad.get_weights()\r\n    weights2 = nopad.get_weights()\r\n    \r\n    print(\"\\npadded mean\" + str(weights1) + '\\n')\r\n    print(\"\\nopad mean\" + str(weights2) + '\\n')\r\n\r\npadded mean[array([1.4691201], dtype=float32), array([0.48987818], dtype=float32), array([5.4638605], dtype=float32), array([8.202366], dtype=float32)]\r\n\r\n\r\nopad mean[array([1.46912], dtype=float32), array([0.48987824], dtype=float32), array([5.4638658], dtype=float32), array([8.285133], dtype=float32)]\r\n```\r\nAs you can see it's \"mostly\" right (only minor difference in the last parameter).\r\n\r\nCompare this to without the ```fused=False``` flag:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Mask value of -10\r\npadded_data = np.expand_dims(np.array([[1,2,3,4,5,-10,-10,-10,-10,-10,6,7,8,9,10,-10,-10,-10,-10,-10] for _ in range(1000)], dtype='float32'), 2)\r\npadded_data = padded_data.reshape(1000, 2, 10, 1)\r\ndata = np.expand_dims(np.array([[1,2,3,4,5,6,7,8,9,10] for _ in range(1000)], dtype='float32'), 2)\r\ndata = data.reshape(1000, 2, 5, 1)\r\n\r\n\r\ndef build_padding_model():\r\n    input = tf.keras.layers.Input((2, 10, 1))\r\n    masked = tf.keras.layers.Masking(-10)(input)\r\n    normed = tf.keras.layers.BatchNormalization()(masked)\r\n    model = tf.keras.models.Model(input, normed)\r\n    model.compile(\"adam\", loss=\"mse\")\r\n    return model\r\n\r\ndef build_model():\r\n    input = tf.keras.layers.Input((2, 5, 1))\r\n    normed = tf.keras.layers.BatchNormalization()(input)\r\n    model = tf.keras.models.Model(inputs=input, outputs=normed)\r\n    model.compile(\"adam\", loss=\"mse\")\r\n    return model\r\n\r\nif __name__ =='__main__':\r\n    batch_size = 100\r\n    signal_length = 20\r\n    \r\n    pad = build_padding_model()\r\n    pad.fit(x=padded_data, y=padded_data, batch_size=10, epochs=5)\r\n    \r\n    nopad = build_model()\r\n    nopad.fit(x=data, y=data, batch_size=10, epochs=5)\r\n    \r\n    weights1 = pad.get_weights()\r\n    weights2 = nopad.get_weights()\r\n    \r\n    print(\"\\npadded mean\" + str(weights1) + '\\n')\r\n    print(\"\\nopad mean\" + str(weights2) + '\\n')\r\n\r\npadded mean[array([1.4775826], dtype=float32), array([0.47813976], dtype=float32), array([2.7319329], dtype=float32), array([11.675619], dtype=float32)]\r\n\r\n\r\nopad mean[array([1.46912], dtype=float32), array([0.48987824], dtype=float32), array([5.4638658], dtype=float32), array([8.285133], dtype=float32)]\r\n```\r\n\r\nNote that this comes with some drawbacks, namely significantly longer compute time (~30-40% longer per batch with ```fused=False``` in my own analyses.. YMMV). I also haven't tried making the same changes with ```SyncBatchNormalization``` so that you can do distributed GPU training with ```tf.distribute.MirroredStrategy``` for example. But this PR gets me the functionality I need for my projects.\r\n\r\nThanks again to @LukeBolly ", "@ymodak This is still an issue. Can someone reopen this?"]}, {"number": 30160, "title": "tensorflow 1.13 build using bazel on windows 10 Problem", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.13\r\n- Python version: 3.7.2\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.5.0\r\n- GPU model and memory: rtx 2070 moblie version (none maxq) 8gb\r\n\r\n\r\n\r\n**Describe the problem**\r\nI have solved all the previous issues while finishing the build those two last errors have appeared please help me with that it's starting to get morally painful here is the output after the second run:\r\n\r\n**Any other info / logs**\r\nC:\\tensorflow\\tensorflow>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nc:\\tensorflow\\tensorflow/.bazelrc\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nINFO: Invocation ID: 078842cd-7467-4a1e-8aa2-c216dbc9c091\r\nWARNING: C:/tensorflow/tensorflow/tensorflow/python/BUILD:2986:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: C:/tensorflow/tensorflow/tensorflow/python/BUILD:77:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: C:/tensorflow/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: C:/tensorflow/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: C:/tensorflow/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: C:/tensorflow/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: C:/tensorflow/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: C:/tensorflow/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:356:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: C:/tensorflow/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:233:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: C:/tensorflow/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:76:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: C:/tensorflow/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: C:/tensorflow/tensorflow/tensorflow/contrib/gan/BUILD:136:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: C:/tensorflow/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: C:/users/moura/_bazel_moura/wvk7snnt/external/protobuf_archive/BUILD:626:1: Linking of rule '@protobuf_archive//:python/google/protobuf/internal/_api_implementation.so' failed (Exit 1120): link.exe failed: error executing command\r\n  cd C:/users/moura/_bazel_moura/wvk7snnt/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\shared;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.10240.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\lib\\winv6.3\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\WINDOWS\\Microsoft.NET\\Framework64\\;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x86;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/moura/AppData/Local/Programs/Python/Python37-32/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/moura/AppData/Local/Programs/Python/Python37-32/lib/site-packages\r\n    SET TEMP=C:\\Users\\moura\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\n    SET TF_CUDA_VERSION=10.1\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\moura\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /DLL /SUBSYSTEM:CONSOLE /MACHINE:X64 @bazel-out/x64_windows-opt/bin/external/protobuf_archive/python/google/protobuf/internal/_api_implementation.so-2.params\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n   Creating library bazel-out/x64_windows-opt/bin/external/protobuf_archive/python/google/protobuf/internal/python/google/protobuf/internal/lib_api_implementation.so.ifso and object bazel-out/x64_windows-opt/bin/external/protobuf_archive/python/google/protobuf/internal/python/google/protobuf/internal/lib_api_implementation.so.exp\r\napi_implementation.o : error LNK2019: unresolved external symbol __imp_PyModule_AddIntConstant referenced in function PyInit__api_implementation\r\napi_implementation.o : error LNK2019: unresolved external symbol __imp_PyModule_Create2 referenced in function PyInit__api_implementation\r\nbazel-out/x64_windows-opt/genfiles/external/local_config_python/python37.lib : warning LNK4272: library machine type 'X86' conflicts with target machine type 'x64'\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/python/google/protobuf/internal/_api_implementation.so : fatal error LNK1120: 2 unresolved externals\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 3.046s, Critical Path: 0.15s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["@Mouradost Just to verify, did you follow the steps mentioned in the [Tensorflow build](https://www.tensorflow.org/install/source_windows) official website. If not please follow the steps. Let us know how it progresses. Thanks!", "> @Mouradost Just to verify, did you follow the steps mentioned in the [Tensorflow build](https://www.tensorflow.org/install/source_windows) official website. If not please follow the steps. Let us know how it progresses. Thanks!\r\n\r\nyes, I did.\r\n\r\n- I have installed Microsoft Visual C++ 2015 Redistributable, Build Tools 2015 update 3 and MSY2\r\n- Setup my environment variables\r\n\r\nI also did try with Tensorflow 1.14 and Bazel 0.24.1 but I always face the same message error that this link.exe  is causing problems", "@Mouradost Which protobuf version are you using? ", "> protobuf version\r\n\r\nsorry for my stupidity but how do I check this?", "Type `pip show protobuf` and it will display protobuf version. Thanks!", "> pip show protobuf\r\n\r\nName: protobuf\r\nVersion: 3.8.0\r\nSummary: Protocol Buffers\r\nHome-page: https://developers.google.com/protocol-buffers/\r\nAuthor: None\r\nAuthor-email: None\r\nLicense: 3-Clause BSD License\r\nLocation: c:\\users\\moura\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\r\nRequires: six, setuptools", "@Mouradost Downgrade the protobuf version to 3.7.1. And check the build. Let us know if that helps. Thanks!", "> @Mouradost Downgrade the protobuf version to 3.7.1. And check the build. Let us know if that helps. Thanks!\r\n\r\nSame issue", "duplicate #23401", "> duplicate #23401\r\n\r\nactually it's not the same issue he had an error with .rc `ERROR: Config value cuda is not defined in any .rc file` file while i have errors with setting up the paths\r\n", "When you run `python configure.py` as indicated by our guide at https://www.tensorflow.org/install/source_windows#configure_the_build\r\nThe script will generate the RC file that has the `cuda` config option. The error message points to either configure script not being run, or GPU not being configured during the script run.", "> When you run `python configure.py` as indicated by our guide at https://www.tensorflow.org/install/source_windows#configure_the_build\r\n> The script will generate the RC file that has the `cuda` config option. The error message points to either configure script not being run, or GPU not being configured during the script run.\r\n\r\nokay so how do I solve that?", "Run `python configure.py` before `bazel build`, answer prompts (will ask you about CUDA, Python, etc.)", "> Run `python configure.py` before `bazel build`, answer prompts (will ask you about CUDA, Python, etc.)\r\n\r\nI did and this is my config\r\n\r\nC:\\tensorflow\\tensorflow>python configure.py\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.24.1 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\moura\\AppData\\Local\\Programs\\Python\\Python37-32\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\moura\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\moura\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: N\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\nFound cuDNN 7 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 7.5\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.", "> > Run `python configure.py` before `bazel build`, answer prompts (will ask you about CUDA, Python, etc.)\r\n> \r\n> I did and this is my config\r\n> \r\n> C:\\tensorflow\\tensorflow>python configure.py\r\n> WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\n> You have bazel 0.24.1 installed.\r\n> Please specify the location of python. [Default is C:\\Users\\moura\\AppData\\Local\\Programs\\Python\\Python37-32\\python.exe]:\r\n> \r\n> Found possible Python library paths:\r\n> C:\\Users\\moura\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\r\n> Please input the desired Python library path to use. Default is [C:\\Users\\moura\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages]\r\n> \r\n> Do you wish to build TensorFlow with XLA JIT support? [y/N]: N\r\n> No XLA JIT support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with ROCm support? [y/N]: N\r\n> No ROCm support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with CUDA support? [y/N]: y\r\n> CUDA support will be enabled for TensorFlow.\r\n> \r\n> Found CUDA 10.1 in:\r\n> C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n> C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\n> Found cuDNN 7 in:\r\n> C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n> C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\n> \r\n> Please specify a list of comma-separated CUDA compute capabilities you want to build with.\r\n> You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\n> Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 7.5\r\n> \r\n> Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n> \r\n> Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y\r\n> Eigen strong inline overridden.\r\n> \r\n> Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n> --config=mkl # Build with MKL support.\r\n> --config=monolithic # Config for mostly static monolithic build.\r\n> --config=gdr # Build with GDR support.\r\n> --config=verbs # Build with libverbs support.\r\n> --config=ngraph # Build with Intel nGraph support.\r\n> --config=numa # Build with NUMA support.\r\n> --config=dynamic_kernels # (Experimental) Build kernels into separate shared objects.\r\n> Preconfigured Bazel build configs to DISABLE default on features:\r\n> --config=noaws # Disable AWS S3 filesystem support.\r\n> --config=nogcp # Disable GCP support.\r\n> --config=nohdfs # Disable HDFS support.\r\n> --config=noignite # Disable Apache Ignite support.\r\n> --config=nokafka # Disable Apache Kafka support.\r\n> --config=nonccl # Disable NVIDIA NCCL support.\r\n\r\ni was facing the same error on my system.", "> > > Run `python configure.py` before `bazel build`, answer prompts (will ask you about CUDA, Python, etc.)\r\n> > \r\n> > \r\n> > I did and this is my config\r\n> > C:\\tensorflow\\tensorflow>python configure.py\r\n> > WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\n> > You have bazel 0.24.1 installed.\r\n> > Please specify the location of python. [Default is C:\\Users\\moura\\AppData\\Local\\Programs\\Python\\Python37-32\\python.exe]:\r\n> > Found possible Python library paths:\r\n> > C:\\Users\\moura\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\r\n> > Please input the desired Python library path to use. Default is [C:\\Users\\moura\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages]\r\n> > Do you wish to build TensorFlow with XLA JIT support? [y/N]: N\r\n> > No XLA JIT support will be enabled for TensorFlow.\r\n> > Do you wish to build TensorFlow with ROCm support? [y/N]: N\r\n> > No ROCm support will be enabled for TensorFlow.\r\n> > Do you wish to build TensorFlow with CUDA support? [y/N]: y\r\n> > CUDA support will be enabled for TensorFlow.\r\n> > Found CUDA 10.1 in:\r\n> > C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n> > C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\n> > Found cuDNN 7 in:\r\n> > C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n> > C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\n> > Please specify a list of comma-separated CUDA compute capabilities you want to build with.\r\n> > You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\n> > Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 7.5\r\n> > Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n> > Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y\r\n> > Eigen strong inline overridden.\r\n> > Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n> > --config=mkl # Build with MKL support.\r\n> > --config=monolithic # Config for mostly static monolithic build.\r\n> > --config=gdr # Build with GDR support.\r\n> > --config=verbs # Build with libverbs support.\r\n> > --config=ngraph # Build with Intel nGraph support.\r\n> > --config=numa # Build with NUMA support.\r\n> > --config=dynamic_kernels # (Experimental) Build kernels into separate shared objects.\r\n> > Preconfigured Bazel build configs to DISABLE default on features:\r\n> > --config=noaws # Disable AWS S3 filesystem support.\r\n> > --config=nogcp # Disable GCP support.\r\n> > --config=nohdfs # Disable HDFS support.\r\n> > --config=noignite # Disable Apache Ignite support.\r\n> > --config=nokafka # Disable Apache Kafka support.\r\n> > --config=nonccl # Disable NVIDIA NCCL support.\r\n> \r\n> i was facing the same error on my system.\r\n\r\nhow did you solve it then?", "So I think I did solve the problem actually I had python 32bits and because everything else was on 64bits, it was causing conflict however now I have a new problem, it said that :\r\nImportError: Could not find 'cudart64_.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA  from this URL: https://developer.nvidia.com/cuda-90-download-archive\r\nwhere can I download cudart.dll and why it isn't include in the cuda download file?", "The link for installing CUDA is given in the error message you're quoting.\r\n\r\nI think the issue now is that the directory you installed CUDA to is not in the %PATH% directories.", "> The link for installing CUDA is given in the error message you're quoting.\r\n> \r\n> I think the issue now is that the directory you installed CUDA to is not in the %PATH% directories.\r\n\r\n\r\nthe link was for cuda toolkit 9.0 and I actually did include it in the %PATH%\r\n![Annotation 2019-07-19 190420](https://user-images.githubusercontent.com/17816194/61534678-d8a94680-aa62-11e9-9302-34384befc244.png)\r\n\r\n", "> The link for installing CUDA is given in the error message you're quoting.\r\n> \r\n> I think the issue now is that the directory you installed CUDA to is not in the %PATH% directories.\r\n\r\nI did actually find it in my files but under the name of cudart64_101.dll\r\n![Annotation 2019-07-19 191709](https://user-images.githubusercontent.com/17816194/61531630-faea9680-aa59-11e9-954e-40b40bd9e8a7.png)\r\n", "Well, that's CUDA 10.1 which is not supported in 1.13 afaik", "> Well, that's CUDA 10.1 which is not supported in 1.13 afaik\r\n\r\nI guess it is in 1.14 but I have the same error with it", "Not really supported there either. From what I remember the CUDA 10.1 support is only partial", "> Not really supported there either. From what I remember the CUDA 10.1 support is only partial\r\n\r\nOkay, then I will roll back to 10", "https://www.tensorflow.org/install/gpu#software_requirements\r\nPlease reopen this issue if still facing problem after rolling back to cuda 10.0\r\nThanks!", "> https://www.tensorflow.org/install/gpu#software_requirements\r\n> Please reopen this issue if still facing problem after rolling back to cuda 10.0\r\n> Thanks!\r\n\r\nsorry for the delay but I'm in vacation I will roll it back when I'm back home and keep you informed thanks for your support guys", "Hello guys,\r\nI roll back the cuda version to 10.0 but i still have some issues this is the problem that I'm facing\r\n`ERROR: C:/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:4252:1: C++ compilation of rule '//tensorflow/core/kernels:bincount_op_gpu' failed (Exit 5): python.exe failed: error executing command\r\n  cd C:/users/moura/_bazel_moura/wvk7snnt/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\shared;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.10240.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\lib\\winv6.3\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\WINDOWS\\Microsoft.NET\\Framework64\\;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x86;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/moura/AppData/Local/Programs/Python/Python37/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/moura/AppData/Local/Programs/Python/Python37/lib/site-packages\r\n    SET TEMP=C:\\Users\\moura\\AppData\\Local\\Temp\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7.4.2\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TF_NEED_TENSORRT=0\r\n    SET TMP=C:\\Users\\moura\\AppData\\Local\\Temp\r\n  C:/Users/moura/AppData/Local/Programs/Python/Python37/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/cub_archive /Ibazel-out/x64_windows-opt/genfiles/external/cub_archive /Ibazel-out/x64_windows-opt/bin/external/cub_archive /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/cub_archive/_virtual_includes/cub /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cublas/include /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX -nvcc_options=disable-warnings -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/bincount_op_gpu/bincount_op_gpu.cu.o /c tensorflow/core/kernels/bincount_op_gpu.cu.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nnvcc error   : 'cudafe++' died with status 0xC0000005 (ACCESS_VIOLATION)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 853.436s, Critical Path: 179.92s\r\nINFO: 2453 processes: 2453 local.\r\nFAILED: Build did NOT complete successfully`", "@ymodak can you please open again this issue"]}, {"number": 30159, "title": "Enable SupervisedInputReceiver", "body": "I want to serving a model with SupervisedInputReceiver, but I found this feature is not official supported by tensorflow. Is there replacement for this feature? if no, I suggest to enable this feature.\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nhttps://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/export/export.py#L251\r\nthis class is not in the related __init__.py, so can't use it officially\r\n\r\n**Will this change the current api? How?** I think no.\r\n\r\n**Who will benefit with this feature?** who want to serve model with both input/output.\r\n", "comments": ["Thanks for your contribution to the community. This feature looks like related to serving a model. Can you please raise the issue in [TensorFlow Serving](https://github.com/tensorflow/serving/issues) repository for better and faster help. Thanks! ", "sure, moved to TensorFlow Serving.", "kindly ping...", "@WangYongzhao Can you please provide how 'SupervisedInputReceiver' can help the wider community? ", "do you mean the difference of 'ServingInputReceiver' and 'SupervisedInputReceiver'?\r\nits function diff should be clear here. https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/export/export.py#L251\r\n\r\nfor my cases, I was doing some neural machine trasatlation experiment, I can use 'ServingInputReceiver' to generate some traslations, while i can use 'SupervisedInputReceiver' to get the relation of translation/source.\r\n\r\nThis should be a very generic usage, and that's why someone already implement this feature.\r\nJust don't know why not expose this feature.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 30158, "title": "No matching distribution found for every version of tensorflow", "body": "I'm trying to install tensorflow-gpu for Windows 10 with the last version of pip and python 3.5.1.\r\nthe problem is that whenever i try to install via pip it gives me the error \"No matching distribution found for tensorflow-gpu\", and not only for the gpu version. I get the same error for:\r\npip install tensorflow\r\npip install tensorflow-cpu\r\npip install tensorflow-gpu\r\npip3 install tensorflow\r\npip3 install tensorflow-cpu\r\npip3 install tensorflow-gpu\r\npip install --upgrade <all of the above>\r\nWith the links on the tensorflow site i get \"ERROR: https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.13.1-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.\" I also tried to download the .whl first and then installing from there but got the same Error.\r\n\r\nI tried many solutions on stack overflow and github but the result is always the same: no matching distribution or not supported wheel. I'm going crazy...", "comments": ["Turns out that tensorflow is just for x64 versions of python, so i had to uninstall the x32 version that i had previously installed (I don't know what i was thinking) and to reinstall the 64 version. Now it works (I hope)"]}, {"number": 30157, "title": "[TF 2.0 Docs] Doc update for math functions", "body": "Following functions are updated:\r\n\r\n- greater\r\n- greater_equal\r\n- is_finite\r\n- isinf\r\n- isnan\r\n- less\r\n- less_equal\r\n- log\r\n- log1p\r\n- lgamma\r\n\r\nReference issue: https://github.com/tensorflow/tensorflow/issues/25802", "comments": ["@martinwicke Could you please review this PR ?", "@SSaishruthi Could you please address the reviewer comments. Thanks!", "Can one of the admins verify this patch?", "Will update. Sorry for the delay", "@SSaishruthi Did you get a chance to look on reviewer comments? Please let us know on the update. Thanks!", "> @SSaishruthi Did you get a chance to look on reviewer comments? Please let us know on the update. Thanks!\r\n\r\nAddressed comments and questions that need to be clarified from my side\r\n\r\n@gbaned "]}, {"number": 30156, "title": "Memory Leak with tf.Tensor.getitem in tf.data.Dataset", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX and Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): `pipenv install --pre tensorflow==2.0.0-beta1`\r\n- TensorFlow version (use command below): `2.0.0-beta1`\r\n- Python version: `3.6.8`\r\n\r\n**Describe the current behavior**\r\nAs described in [tf.slice](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/slice), I tried to use `tf.Tensor.getitem` to perform a slice in a more pythonic way, but ran into memory leaks. \r\n\r\n```python\r\ndef map_to_xy_dataset(csv_dataset, params):\r\n    window_size = params[WINDOW_SIZE_KEY]\r\n    window_shift = params[WINDOW_SHIFT_KEY]\r\n    n_workers = tf.data.experimental.AUTOTUNE\r\n\r\n    # MEMORY LEAK IN THIS FUNCTION\r\n    def split_xy(window):\r\n        # For X, select all but the last value and flatten\r\n        range_x = tf.shape(window)[1] - 1\r\n        \r\n        # CAUSES MEMORY LEAK \r\n        x = tf.reshape(window[:, 0:range_x], [-1])\r\n\r\n\r\n        # Select a single Y value\r\n        # CAUSES MEMORY LEAK\r\n        y = window[0, -1]\r\n\r\n        return x, y\r\n\r\n    # Turn the csv dataset row x col tensor\r\n    row_dataset = csv_dataset.map(lambda *x: tf.reshape(x, [len(x)]))\r\n\r\n    windowed_dataset = row_dataset.window(\r\n        size=window_size, shift=window_shift,\r\n        drop_remainder=True).flat_map(lambda x: x.batch(window_size))\r\n\r\n    xy_dataset = windowed_dataset.map(split_xy, num_parallel_calls=n_workers)\r\n\r\n    return xy_dataset\r\n```\r\n\r\nI have **not** tried to reproduce this behavior with  [tf.slice](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/slice). I will update this issue if I do. \r\n\r\n**Describe the expected behavior**\r\nNo memory leaks.\r\n\r\n**Code to reproduce the issue**\r\nSee above.\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nInitial related comment: https://github.com/tensorflow/tensorflow/issues/19049#issuecomment-505585437", "comments": ["Will it be possible to provide a full code snippet that can replicate the issue.It will indeed help us to move faster. Thanks!", "@achandraa  Thanks for looking into this! Does this code snippet suffice? If not, let me know what other details you need.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef map_to_xy_dataset(csv_dataset, params):\r\n    window_size = 3\r\n    window_shift = 10\r\n    n_workers = tf.data.experimental.AUTOTUNE\r\n\r\n    # MEMORY LEAK IN THIS FUNCTION\r\n    def split_xy(window):\r\n        # For X, select all but the last value and flatten\r\n        range_x = tf.shape(window)[1] - 1\r\n        \r\n        # CAUSES MEMORY LEAK \r\n        x = tf.reshape(window[:, 0:range_x], [-1])\r\n\r\n\r\n        # Select a single Y value\r\n        # CAUSES MEMORY LEAK\r\n        y = window[0, -1]\r\n\r\n        return x, y\r\n\r\n    # Turn the csv dataset row x col tensor\r\n    row_dataset = csv_dataset.map(lambda *x: tf.reshape(x, [len(x)]))\r\n\r\n    windowed_dataset = row_dataset.window(\r\n        size=window_size, shift=window_shift,\r\n        drop_remainder=True).flat_map(lambda x: x.batch(window_size))\r\n\r\n    xy_dataset = windowed_dataset.map(split_xy, num_parallel_calls=n_workers)\r\n\r\n    return xy_dataset\r\n\r\ndef generate_dataset():\r\n    filenames_dataset = tf.data.Dataset.list_files(data_glob)\r\n\r\n    dataset = filenames_dataset.flat_map(map_file_to_xy_dataset)\r\n\r\n    return dataset\r\n\r\n\r\ndataset = generate_dataset()\r\n\r\nmodel = ...\r\n\r\nmodel.fit(dataset, ...)\r\n```\r\n", "@devstein I have tried to reproduce the issue with the above code but Looks like some of the entities like data_glob, map_file_to_xy_dataset are not defined. Can you help us to reproduce the issue. Thanks! ", "@gadagashwini Here is a more detailed example: \r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef map_file_to_xy_dataset(filename):\r\n    window_size = 3\r\n    window_shift = 10\r\n    n_workers = tf.data.experimental.AUTOTUNE\r\n\r\n    # Generate a dataset from the filename\r\n    csv_dataset = tf.data.experimental.CsvDataset(\"Iris.csv\",[\"float32\",\"float32\",\"float32\",\"float32\",\"float32\"])\r\n\r\n    # Cache at unique file location (i.e filename)\r\n    cached_dataset = csv_dataset.cache(\"Iris.csv\")\r\n\r\n    # MEMORY LEAK IN THIS FUNCTION\r\n    def split_xy(window):\r\n        # For X, select all but the last value and flatten\r\n        range_x = tf.shape(window)[1] - 1\r\n        \r\n        # CAUSES MEMORY LEAK \r\n        x = tf.reshape(window[:, 0:range_x], [-1])\r\n\r\n\r\n        # Select a single Y value\r\n        # CAUSES MEMORY LEAK\r\n        y = window[0, -1]\r\n\r\n        return x, y\r\n\r\n    # Turn the csv dataset row x col tensor\r\n    row_dataset = cached_dataset.map(lambda *x: tf.reshape(x, [len(x)]))\r\n\r\n    windowed_dataset = row_dataset.window(\r\n        size=window_size, shift=window_shift,\r\n        drop_remainder=True).flat_map(lambda x: x.batch(window_size))\r\n\r\n    xy_dataset = windowed_dataset.map(split_xy, num_parallel_calls=n_workers)\r\n\r\n    return xy_dataset\r\n\r\n\r\ndef generate_dataset():\r\n    filenames_dataset = tf.data.Dataset.list_files(\"Iris.csv\")\r\n\r\n    dataset = filenames_dataset.interleave(map_file_to_xy_dataset, cycle_length=10, block_length=10, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n\r\n    return dataset\r\n\r\n\r\n```", "@gadagashwini After more experimenting, I don't think `tf.Tensor.getitem` is the root of the issue. This [issue](https://github.com/tensorflow/tensorflow/issues/29697) describes the same behavior but is only caching. \r\n\r\nAlso, this issue is only persistent during the first epoch. Does `tf.data.Dataset.cache` cache to memory before it writes to disk?\r\n\r\n**Edit**\r\nI've narrowed it down further. This issue comes from using `interleave` with a large `cycle_length` (i.e total number of files) and a small `block_length` (i.e `block_length = 1`). \r\n\r\n@gadagashwini How does this relate to caching? Is there an internal memory buffer for each cached dataset while the lock file exists?", "@devstein I tried reproducing the issue with the above provided code but it didn't show any error. Please provide us complete code which replicates the reported issue. Thanks!\r\n ", "@gadagashwini It causes memory usage climbs, see my last comment. There is no error produced.", "@devstein the effect of `cycle_length` with `interleave` is that a multiple of `cycle_length` instances of the input pipeline returned by `map_file_to_xy_dataset` will be evaluated in parallel. If each instances maintains internal buffers, the memory usage will be multiplied by the `cycle_length` factor.\r\n\r\nIn your case, it looks like your input pipeline is using `window` and `map` with `num_parallel_calls`. Both of these transformations will maintain an internal buffer.\r\n\r\nIn other words, the behavior you are experiencing is not a memory leak (at least not based on the evidence you have provided), but high memory usage.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30156\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30156\">No</a>\n", "@jsimsa That makes sense, but why is this only prevalent when using `.cache(...)`? Memory usage is significantly lower when using the exact same parameters but not caching each `csv_dataset` to disk? ", "IIRC, the implementation of cache would uses an internal buffer for batching writes of a tensor. So that would explain why is the problem exacerbated by using `cache` in your input pipeline."]}, {"number": 30155, "title": "Fix bug in cudnn_fused_conv_rewriter.", "body": "The scale for the convolution result and for the side input must be scalar constants.\r\nBefore this change, any constant would be accepted.", "comments": ["Can we add a test for these? It should be trivial to write one: if it's a correctness fix, there would be a divergence vs. the interpreter."]}, {"number": 30154, "title": "Fix exchange of TF_Input and TF_Output in c_api.h", "body": "I'm not sure if this was kept intentionally, but it definitely caught my eye. The structs `TF_Input` and `TF_Output` are equivalent, so this change is probably only cosmetic.", "comments": ["@darthdeus Could you please address the build failures? Thanks!", "@darthdeus  gentle ping to address the build failures. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 30153, "title": "Update version numbers for TensorFlow 1.13.2", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 1 -> 1\nMinor: 13 -> 13\nPatch: 1 -> 2\n\nNo lingering old version strings \"1.13.1\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"1.13.1\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 30152, "title": "Implemented Gradient for RGBToHSV", "body": "Hello, \r\n\r\nThis PR implements the gradient for RGB to HSV operation which was marked as a [TODO](https://github.com/tensorflow/tensorflow/blob/08082cdab025ff095001e42839d3efbf9cd45a8e/tensorflow/python/ops/image_ops_impl.py#L46). The function is piecewise continuous and has a lot of sub-cases (with MAX() and MIN()) so the derivative calculation tends to get a bit complicated. I have left as much comments as possible in the code to explain the derivative calculations but I'd be glad to add something else if needed (i have a lot of whiteboard/pen-and-paper notes on this derivation!). \r\n\r\nI tried to add a test case to `tensorflow/python/ops/image_grad_test.py`, but I wasn't sure if that file is actually a real test file as I didn't find any bazel linakges. Most of the other functions in that file also seem to be using deprecated gradient functions which aren't part of TF2.0 so I wanted to first check here before I add any test(s) to the wrong file. \r\n", "comments": ["Looks better! Just missing the gradient checker tests I mentioned earlier.", "Hey @alextp \r\nI added a couple of tests however the gradient test doesn't pass. I am not sure if we can use the Jacobian method to verify gradients for such a complex function? \r\nI will re-check my derivation but I wanted to double check if the numerical method used to verify (ie Jacobian) can handle piecewise continuous functions with many conditions. \r\nhttps://en.wikipedia.org/wiki/HSL_and_HSV#From_RGB\r\nThis is the formula on which I calculated all the partial derivatives and as you can see there are many many sub-cases (MAX(r,g,b) and MIN(r,g,b) produce 6 ordered pairs for (max,min) and the derivative changes in each case) which need to be handled separately.  ", "The method we use to verify the gradients is just finite differences, so if we choose an area somewhat separate from the discontinuities it should be fine.\r\n\r\nAnother option to validate the gradient is to implement rgb_to_hsv in terms of basic TF ops (which is doable) and compare the result of tf.gradients there with the gradient implementation you provide.", "Okay, thank you! For the second option did you mean implement rgb_to_hsv() in terms of basic TF ops or the gradient of rgb_to_hsv in basic tf ops? \r\nI'll try out both the suggestions and get back to you. \r\n", "I meant implementing rgb_to_hsv in terms of basic ops so we can get its\ngradient also in terms of basic ops.\n\nOn Fri, Jun 28, 2019 at 1:10 PM Karthik Muthuraman <notifications@github.com>\nwrote:\n\n> Okay, thank you! For the second option did you mean implement rgb_to_hsv()\n> in terms of basic TF ops or the gradient of rgb_to_hsv in basic tf ops?\n> I'll try out both the suggestions and get back to you.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/30152?email_source=notifications&email_token=AAABHRKHLXFDBXV6UZWY3U3P4ZV3FA5CNFSM4H3MUANKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY3BG6Q#issuecomment-506860410>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNWYOKH5LZZX5QUJWTP4ZV3FANCNFSM4H3MUANA>\n> .\n>\n\n\n-- \n - Alex\n", "@kmh4321 Did you get a chance to look on reviewer comments? Please let us know on the update. Thanks! ", "@gbaned Sorry I was out on vacation last week. Will take a look at this now. ", "@alextp  I have found the issue. I implemented it as per the definition I found on the wikipedia page, however the TF implementation has one final step where the \"hues\" are converted from degrees to [0, 1]. I have fixed that [here](https://github.com/tensorflow/tensorflow/pull/30152/commits/07fc1591f2e7e0394cf4a3f132043a05065b9595#diff-614af081276f42dbb98e5d731d7a1a86R283). \r\n\r\nAs discussed above, I've also added a few test cases to avoid running into discontinuities and also added a [test](https://github.com/tensorflow/tensorflow/pull/30152/files#diff-bb1fdb8f5be7a85b4505589762579bf7R515) where I implement `rgb_to_hsv()` with native TF functions in a controlled input case (R>G>B). \r\n\r\nNow these tests are passing. Hope this is sufficient, will be glad to make any changes as required!\r\n"]}]