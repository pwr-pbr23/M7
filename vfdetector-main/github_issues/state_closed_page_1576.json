[{"number": 5634, "title": "[Windows/CMake] Add TensorBoard.", "body": "Enables some TensorBoard tests, and fixes `tf.contrib` so that these pass. Also removes the CMake-specific setup.py, and replaces it with the same one that Bazel uses.", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @andrewharp, @caisq and @gunan to be potential reviewers.\n", "looks like a flake, grrrr.\nJenkins, test this please.\n", "Interesting... the offending error seems to be:\n\n```\n22:06:28          C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/cmake_build/tensorflow/core/protobuf/meta_graph.pb.h: Permission denied\n22:06:28     18>C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1. [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_protos_cc.vcxproj]\n22:06:28     18>Done Building Project \"C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_protos_cc.vcxproj\" (default targets) -- FAILED.\n```\n\nAny idea what could be racing with the build? We're only running one build per machine at any one time, right?\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "This is building and running fine locally, so I suspect a flake....\n\n@tensorflow-jenkins test this please.\n", "I can confirm this PR successfully includes tf.contrib in the PIP package on Windows. @mrry Thank you, sir!\n", "@laudney Thanks for confirming this!\n\n@gunan This is building for me fine locally. I don't know why it's failing on Jenkins. Shall we just merge it?\n", "`Cannot remove entries from nonexistent file c:\\program\nfiles\\anaconda3\\lib\\site-packages\\easy-install.pth`\n\nThis is the error message again...\n\nI think we can merge.\n\nOn Thu, Nov 17, 2016 at 10:31 AM, Derek Murray notifications@github.com\nwrote:\n\n> @laudney https://github.com/laudney Thanks for confirming this!\n> \n> @gunan https://github.com/gunan This is building for me fine locally. I\n> don't know why it's failing on Jenkins. Shall we just merge it?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5634#issuecomment-261328636,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AHlCOfvptpGuQMZFUwVqU5hxpD58WGhnks5q_J1_gaJpZM4KzZuD\n> .\n", "@gunan Thanks - I went ahead and merged it. That error message turns up in successful builds too... see e.g. http://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/164/console\n\n(A possible avenue to explore is that we can try using regular Python 3.5 instead of Anaconda, in case we're tripping a weird behavior in Anaconda's package management.)\n"]}, {"number": 5633, "title": "module 'tensorflow.python.ops.gen_array_ops' has no attribute '_expand_dims'", "body": "After following CMake build for windows, python cannot be loaded with below error message.\r\n>>> import tensorflow\r\nAttributeError: module 'tensorflow.python.ops.gen_array_ops' has no attribute '_expand_dims'\r\nAfter checking git history, it's related with checkin from Andrew Selle.\r\nSHA-1: a41f0eae0cca59ff211495bd3e9da0daf4ed01e8\r\n\r\n### Environment info\r\nOperating System:\r\nWindows Server 2012 R2\r\n", "comments": ["It sounds like you have an old copy of `tensorflow/python/ops/gen_array_ops.py` that wasn't generated after updating past a41f0ea (which updates `hidden_ops.txt`). Could you try deleting the `tf_python/` directory and rerunning the build?\n", "To install tensorflow on windows, whl file is created at\ntensorflow\\contrib\\cmake\\build\\tf_python\\dist.\nI followed below instruction.\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md\nTo build and out whl, below command is necessary.\nMSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj\nStill, same error with recent git.\n\n2016-11-16 13:40 GMT+09:00 Derek Murray notifications@github.com:\n\n> It sounds like you have an old copy of tensorflow/python/ops/gen_\n> array_ops.py that wasn't generated after updating past a41f0ea\n> https://github.com/tensorflow/tensorflow/commit/a41f0eae0cca59ff211495bd3e9da0daf4ed01e8\n> (which updates hidden_ops.txt). Could you try deleting the tf_python/\n> directory and rerunning the build?\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5633#issuecomment-260852414,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AWZuauTDG0WfIMeearKlyaxdCXxhb-qWks5q-okvgaJpZM4KzQvm\n> .\n", "Closing this due to lack of recent activity. Please try the pre-built binaries."]}, {"number": 5632, "title": "Feature Request: Perspective Transforms", "body": "Currently image processing has some image manipulation functions like brightness adjustment or contrast adjustment.  It would be quite useful to have perspective transforms or other affine transformations to help distort training data.", "comments": ["Hmm, I wonder if you could use the spatial transformer code to achieve this.\nE.g. generate a sampling grid [0, height] x [0, width], apply your transformation (via batch_matmul) and then collect them exactly as in the spatial_transformer code. \n", "This is a reasonable feature request, but we may not get to it in the near term.  Contributions welcome!\n", "Can this be done with https://www.tensorflow.org/api_docs/python/tf/contrib/image/transform?", "@astromme, yes, this already is supported there. Closing as this feature already exists.\r\n", "I'm sorry about commenting so long after this was closed, but although this feature is implemented with tf.contrib.image.transform, that function does not allow gradients to backpropagate through it. So I'd call it a partial implementation."]}, {"number": 5631, "title": "tf.Print is not an identity op w/ tuples or lists of Tensors; clarify in docs or add warning msg?", "body": "**Documentation issue with bidirectional_dynamic_rnn:**\r\n\r\nFor \"outputs\":\r\n\r\n\"...It returns a tuple instead of a single concatenated Tensor, unlike in the bidirectional_rnn. If the concatenated one is preferred, the forward and backward outputs can be concatenated as tf.concat(2, outputs).\"\r\n\r\nThis doesn't appear to work.  As a fix:\r\n\r\noutput, _ = tf.nn.bidirectional_dynamic_rnn(...)\r\noutput=tf.concat(2, tf.unpack(output))\r\n\r\n...this appears to be necessary because tf.concat does not view the returned tuple as equivalent to a list of Tensors, but rather appears to treat the tuple (effectively) as a Tensor.\r\n\r\n**Possible bug in tf.concat:**\r\n\r\nUnclear whether tf.concat(2, output) actually should, in fact, return as the initial documentation for bidirectional_dynamic_rnn suggested.\r\n", "comments": ["@cbockman When you say it doesn't work, what error message do you see?  Also, please use code formatting for code (4 space indent) to make it easier to read.\n", "Apologies...\n\nOK, so actually got bitten by an unrelated issue.  This can probably be closed, although maybe worth some light tf.Print documentation clarification.\n\n`output, _ = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, inputs,lengths,dtype=tf.float32)`\n`output=tf.Print(output, [output], \"MSG\")`\n`output = tf.concat(2, output)`\n\nThe above created issues for me because tf.Print apparently transforms the tuple of tensors (output) into a single tensor.  This results in tf.concat receiving a single tensor as 'output', rather than a tuple, as was expected.\n\nThis may be 100% intended--and, in fact, the documentation for tf.Print is very understandably silent on what happens when a non-Tensor gets passed into tf.Print.  From a quick read of tf.Print as an \"identity\" op, however, I didn't anticipate that tf.Print could actually transform the input to something else.  (And yes, I now appreciate the internal consistency as to why this is true, given how TF generally tries to cast non-Tensors into Tensors, for convenience).\n\nMaybe worth clarifying, or maybe I'm just dumb...\n\nCertainly somewhat ironic, as I blindedly layered in a tf.Print for debug purposes, and managed to create another problem.\n", "Alternately/additionally, maybe add a light type check on def Print that\nflags if type is likely to get transformed?  A little crude, but given this\nis essentially just a debug operation, maybe a useful trade off.\n\nOn Nov 16, 2016 9:05 AM, \"Geoffrey Irving\" notifications@github.com wrote:\n\n> @cbockman https://github.com/cbockman When you say it doesn't work,\n> what error message do you see? Also, please use code formatting for code (4\n> space indent) to make it easier to read.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5631#issuecomment-261006162,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AEc6EtwYFladOLbFwaonQWjepjlWt6bwks5q-zfigaJpZM4KzHme\n> .\n", "@cbockman I think documentation for `tf.Print` doesn't make sense, since the same machinery applies to any function that takes tensors.  However, better error messages would certainly be good.  I'm not quite sure _why_ `tf.Print` would have silently absorbed a list of tensors, unless that's new functionality someone added.  Can you clarify: does it automatically concatenate (same rank) or automatically stack (one higher rank)?\n", "@girving tf.Print will stack.  e.g., tensor tuple of ( <1, ?, 2>, <1, ?, 2>) ==> <2, 1, ?, 2>\n\nre:documentation, I'd push back a little, in that referring to something as an \"identity op\"--which isn't really an identity op, under certain scenarios--is very confusing to anyone new to the project.  That said, Tensorflow is obviously not my project.  ;)\n\nOne potentially simple error msg would be to do a type check within tf.Print itself and echo out a warning msgif instance(list, tuple)...or perhaps if not instance(tensor) (away from command line, can't check if this isinstance(tensor) is supported...I assume it is?).\n\nYeah, this will only trigger when tf.Print is evaluated symbolically, but I think that will be sufficient for catching vast majority of failure modes related to this...and it is hard for me to foresee too many (any?) scenarios where this causes problems in any production environment (if tf.Print is truly intended as a debug as a debug tool, I don't know who would actually _want_ to pass in a list and get a stacked tensor back...if you want that functionality, use another op to do that).\n", "@cbockman TensorFlow makes an effort where possible to mirror numpy, and merging multiple tensors this way is standard numpy behavior.  `tf.Print` already says it takes a tensor.  Unfortunately, it's impractical to document all the dynamic type conversion behavior of Python code in every tensorflow function.\n", "@girving fair enough.  thanks for looking at this. \n"]}, {"number": 5630, "title": "`_bag_features` doesn't work properly in Random Forest (TensorForest)", "body": "TensorFlow r0.10 , but code appears to be the same in master.\r\n\r\n### To Reproduce\r\nCreate Random Forest (`tf.contrib.learn.TensorForestEstimator`) with default parameters (`ForestHParams`) with the exception of setting `feature_bagging_fraction` to anything below 1.0 e.g. 0.8. I believe the issue lies in the `_bag_features` implementation in `tensor_forest.py`.\r\n\r\n### Traceback\r\n```\r\nTraceback (most recent call last):\r\n  File \"/.../train.py\", line 55, in train\r\n    monitors=monitors)\r\n  File \"/usr/local/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 240, in fit\r\n    max_steps=max_steps)\r\n  File \"/usr/local/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 550, in _train_model\r\n    train_op, loss_op = self._get_train_ops(features, targets)\r\n  File \"/usr/local/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/random_forest.py\", line 163, in _get_train_ops\r\n    **self.training_args),\r\n  File \"/usr/local/python2.7/dist-packages/tensorflow/contrib/tensor_forest/python/tensor_forest.py\", line 369, in training_graph\r\n    tree_data = self._bag_features(i, tree_data)\r\n  File \"/usr/local/python2.7/dist-packages/tensorflow/contrib/tensor_forest/python/tensor_forest.py\", line 325, in _bag_features\r\n    split_data = array_ops.split(1, self.params.num_features, input_data)\r\n  File \"/usr/local/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 980, in split\r\n    name=name)\r\n  File \"/usr/local/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2231, in _split\r\n    num_split=num_split, name=name)\r\n  File \"/usr/local/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 627, in apply_op\r\n    (key, op_type_name, attr_value.i, attr_def.minimum))\r\nValueError: Attr 'num_split' of 'Split' Op passed 0 less than minimum 1.\r\n```\r\n", "comments": ["I'll link in a tensorforest person shortly, but it seems like the best solution for this would be to make `tf.split` accept `num_split == 0`.  The correct behavior would be to bail if the input tensor isn't empty along that dimension.\n", "@thomascolthurst: Do you agree that this is an issue in `tf.split`, or is it a different bug in tensorforest?\n", "Hi, Geoffrey.  Can you give me more information about what isn't working?\n (For example, a code snippet of how you are calling TensorForest, and any\nrelevant log output).  I assume you are setting the bagged_features\nparameter to true, and that you are seeing a crash or something inside the\n_bag_features function?\n\nThanks,\n-Thomas C\n\nOn Wed, Nov 16, 2016 at 12:51 PM, Geoffrey Irving notifications@github.com\nwrote:\n\n> @thomascolthurst https://github.com/thomascolthurst: Do you agree that\n> this is an issue in tf.split, or is it a different bug in tensorforest?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5630#issuecomment-261019177,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AWasccnv1t8TJnsiwWmMtUjlsXNPzHiPks5q-0KngaJpZM4KzHLi\n> .\n", "Hi, I just realized that this is indeed a silly bug on my side and a non-issue. The above log can be reproduced by setting the `num_features` hyperparameter to 0, which isn't even the log I meant to include, my apologies. The error I was really experiencing involved having an embedding input to the Random Forest where I forgot to incorporate the embedding dimension for each embedded feature in the total number of features; the data could therefore not be bagged properly. I'll go ahead and close this non-issue.\n", "Just a heads up, `_bagged_features` will fail if the `feature_bagging_fraction` is less than the fraction of a single feature; the parameter `bagged_num_features` is calculated by `self.bagged_num_features = int(self.feature_bagging_fraction * self.num_features)`, which will result in 0 bagged features, which results in the error `ValueError: List argument 'values' to 'Concat' Op with length 0 shorter than minimum length 2.` Additionally, this error is not entirely correct, as the `values` input to the `Concat` op can have a length of 1 and still work:\n\n```\n>>> sess.run(tf.concat(1, [1]))\n1\n>>> sess.run(tf.concat(1, 1))\n1\n```\n"]}, {"number": 5629, "title": "Merge release version back to master.", "body": "Changes are mostly for updating version strings.\r\nAlso change TF_PATCH_VERSION in master to `head`.", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @gunan and @tensorflower-gardener to be potential reviewers.\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "Merge from branch, CLA ok to ignore.\n\nAny idea what the failing tests are?\n", "Jenkins, test this please\n"]}, {"number": 5628, "title": "Checkpoint SNAPPY compression, incompatible for inference?", "body": "As referenced in [https://github.com/tensorflow/tensorflow/issues/1669](https://github.com/tensorflow/tensorflow/issues/1669), I got the same error when trying to restore from self-finetuned checkpoint\r\n\r\n> \"Data loss: Unable to open table file /tmp/model.ckpt-4971: Data loss: corrupted compressed block contents: perhaps your file is in a different file format and you need to use a different restore operator? It's likely that your checkpoint file has been compressed with SNAPPY.\"\r\n\r\nI fine-tuned the checkpoint `inception_resnet_v2_2016_08_30.ckpt` for my own images TF-Slim (modified [finetune_inception_v3_on_flowers.sh](https://github.com/tensorflow/models/blob/master/slim/scripts/finetune_inception_v3_on_flowers.sh)). The fine-tuning seems to go well, and I get good performance for the test error. But when I try to do inference using the newly created checkpoint I got the above-mentioned same error, the same error happening with with `inspect_checkpoint` -tool.\r\n\r\nThe demo inference from there works fine as well: [https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb](https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb)\r\n\r\nIn other words, how to save the checkpoints with compatible compression? And how is it possible to save checkpoints on my computer (Ubuntu 14.04, TF 0.11.0rc2, CUDA 8.0) which are not possible to open a moment later?", "comments": ["I added that line to to slim.learning.train in [train_image_classifier.py](https://github.com/tensorflow/models/blob/master/slim/train_image_classifier.py)\n\n```\nsaver=tf.train.Saver(write_version=1)\n```\n\nAnd got the `inspect_checkpoint` working, however now I got the following error with `freeze_graph`:\n\n``` Python\nTraceback (most recent call last):\n  File \"/home/petteri/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 135, in <module>\n    tf.app.run()\n  File \"/home/petteri/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 43, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"/home/petteri/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 132, in main\n    FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)\n  File \"/home/petteri/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 104, in freeze_graph\n    _ = tf.import_graph_def(input_graph_def, name=\"\")\n  File \"/home/petteri/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/importer.py\", line 258, in import_graph_def\n    op_def = op_dict[node.op]\nKeyError: u'DenseToDenseSetOperation'\n```\n\nThe inference now works okay with the demo inference code luckily, but could be eventually nice to combine the `.pbtxt` and the `.ckpt` into one protobuf graph file.\n", "@alexalemi Can you take a look?\n", "I used a few layers of tf-slim pretrained model(resnet_v1_50.ckpt) as my CNN's base layers, then I added several more my own layers and trained the network, when I use `saver.save`, it worked well, but when I try to restore, it end up with the Data loss issue just as above said. I tried to use tf.train.Saver(write_version=1), and it at least worked.", "That freeze_graph error seems to indicate that you're using an older version of TensorFlow to load the graph, where 'DenseToDenseSetOperation' was still in contrib?\r\n\r\nWe should catch this error case (loading a GraphDef file that was created by a newer version of the framework) earlier. I filed #6120 to track that work.", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow."]}, {"number": 5627, "title": "Documentation change for 'Adding a new op ShapeFn'", "body": "In [Adding a new Op](https://github.com/tensorflow/tensorflow/blob/7f3d8e1f8736f4c812e4a0a96154af3bd3750180/tensorflow/g3doc/how_tos/adding_an_op/index.md) there is the following line of code that describes how to set the shape function:\r\n\r\n```python\r\n@tf.RegisterShape(\"ZeroOut\")(common_shapes.call_cpp_shape_fn)\r\n```\r\nI think this is wrong (you do not want the `@` sign since it's not being used as a decorator). Also, having an explicit import would be helpful.\r\n\r\nSo it would be nice to change this to:\r\n```python\r\nfrom tensorflow.python.framework import common_shapes\r\ntf.RegisterShape(\"ZeroOut\")(common_shapes.call_cpp_shape_fn)\r\n```", "comments": ["@gibiansky Would you be up for sending a PR?\n", "We can just remove the line now -- by default all ops call their CPP shape function.  (Or at least, they will with the next push).\n"]}, {"number": 5626, "title": "glibc error with tensorflow 0.10.0", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nGlibc import error when using the v0.10.0 distribution of tensorflow\r\n\r\n`ImportError: /local/dist/x86_64-unknown-linux-gnu/lib/libc.so.6: version GLIBC_2.14 not found (required by /home/fjanoos/conda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so)\r\n`\r\n\r\nHowever, the version of glibc visible to python is > 2.15:\r\n```\r\n[19:34 - 1.44][firdaus@gsrs3 27] ~/local/cuda-7.5 >python\r\nPython 3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:53:06)\r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import platform\r\n>>> platform.libc_ver()\r\n('glibc', '2.2.5')\r\n>>>\r\n```\r\n\r\n### Environment info\r\nOperating System:\r\nDebian 7.8\r\n\r\nInstalled version of CUDA and cuDNN: \r\nCuda 7.5\r\nlibcudnn : 5.0\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\nCannot import tensorflow.\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Looking at what you have provided, the glibc version is \"2.2.5\", which according to https://ftp.gnu.org/gnu/glibc/ was released in 2002.\n\nI've had similar issues on an old RHEL6 box, there is a thread about it with a work around; you basically have to setup an environment by exporting LD_\\* variables to point to a newer version of GLIBC. \n", "could you point me to that thread.\n", "Sure,\n\nI followed these steps:\nhttp://stackoverflow.com/questions/33655731/error-while-importing-tensorflow-in-python2-7-in-ubuntu-12-04-glibc-2-17-not-f/34897674#34897674\n", "Is this issue resolved?\r\nAny objections to closing it?", "no. but please close it anyway. thanks\n\nOn Dec 8, 2016 08:47, \"gunan\" <notifications@github.com> wrote:\n\n> Is this issue resolved?\n> Any objections to closing it?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5626#issuecomment-265674709>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA4XLqZksjhc_Sdxb5TN07IUCcg9KNhuks5rF7YbgaJpZM4Ky79v>\n> .\n>\n"]}, {"number": 5625, "title": "ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory ", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN:   7.5\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\npip install --upgrade $TF_BINARY_URL\r\n\r\nthen \r\n$python\r\n>>import tensorflow\r\n......\r\nImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory \r\n\r\nI see from https://github.com/tensorflow/tensorflow/issues/5343, it says:\r\n\"Correct, starting 0.11.0rc1, all our prebuilt packages are now built for cuda8.\r\nWith cuda7.5, you either need to install the 0.11.0rc0 wheel file, or build from sources.\r\n\"\r\nso, may I know where I can find 0.11.0rc1 url for pip installation? like:\r\npip install [0.11.0rc1 url]\r\n\r\nno resource from https://pypi.python.org/pypi/tensorflow/0.11.0rc0\r\n\r\nI cannot build tensorflow from source code because I work in permission limited environment  and I cannot get the info where cuda is installed but only can use nvcc --version to get cuda related info.\r\n\r\n\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["I solved the issue.\nThe solution is replace url value of TF_BINARY_URL in tensorflow install page to be all rc0 version.\n\n# Ubuntu/Linux 64-bit, CPU only, Python 2.7\n\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl\n\n# Ubuntu/Linux 64-bit, GPU enabled, Python 2.7\n\n# Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see \"Install from sources\" below.\n\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl\n\n# Mac OS X, CPU only, Python 2.7:\n\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc0-py2-none-any.whl\n\n# Mac OS X, GPU enabled, Python 2.7:\n\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.11.0rc0-py2-none-any.whl\n\n# Ubuntu/Linux 64-bit, CPU only, Python 3.4\n\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc0-cp34-cp34m-linux_x86_64.whl\n\n# Ubuntu/Linux 64-bit, GPU enabled, Python 3.4\n\n# Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see \"Install from sources\" below.\n\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp34-cp34m-linux_x86_64.whl\n\n# Ubuntu/Linux 64-bit, CPU only, Python 3.5\n\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc0-cp35-cp35m-linux_x86_64.whl\n\n# Ubuntu/Linux 64-bit, GPU enabled, Python 3.5\n\n# Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see \"Install from sources\" below.\n\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp35-cp35m-linux_x86_64.whl\n\n# Mac OS X, CPU only, Python 3.4 or 3.5:\n\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc0-py3-none-any.whl\n\n# Mac OS X, GPU enabled, Python 3.4 or 3.5:\n\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.11.0rc0-py3-none-any.whl\n", "Any matching links for Python 3.6 on Ubuntu?", "Thanks", "Python 3.6 on ubuntu is export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.1.0-cp36-cp36m-linux_x86_64.whl. \r\n\r\nSee complete list here: http://tflearn.org/installation/", "**Problem in conda environment**\r\n\r\nfrom pycuda._driver import *  # noqa\r\nImportError: libcurand.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n**Solved with**\r\nconda install cudatoolkit=8.0\r\n"]}, {"number": 5624, "title": "[Windows/CMake] Enable SVD op and test.", "body": "", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @zheng-xq, @guschmue and @tensorflower-gardener to be potential reviewers.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please\n", "I am using windows 7 with keras and python 3.4. I would like to help with the tensorflow port.  I have models that currently train and test in theano, would be happy to help get tensorflow working and can validate it against theano for correctness.\n\nHow can I get started?\n", "Woohoo! Looks like only failure was a flaky test on mac. Merging.\n", "@mrry, @isaacgerg  Derek, do you have any specific instructions for Isaac about contributing on Windows, other than the generic instructions here: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md\n", "@isaacgerg instructions how to get a windows build are here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md\nKeras should work ok, I ran the Keras example directory with a tensorflow backend under windows+gpu some time ago and it was happy.\n", "@isaacgerg +1 to what @guschmue said. Currently we're working to get the Windows build at parity with what the Linux version supports. Our number-one priority is to get all of the kernels building, and you can see a list of the ones we're currently unable to build [here](https://github.com/tensorflow/tensorflow/blob/35f7c5c3fd4fe96b588ba578bbdbc95f47e9ac36/tensorflow/contrib/cmake/tf_core_kernels.cmake#L88). You can also see a list of tests that don't currently pass [here](https://github.com/tensorflow/tensorflow/blob/35f7c5c3fd4fe96b588ba578bbdbc95f47e9ac36/tensorflow/contrib/cmake/tf_tests.cmake#L129). Any contributions to make these work would be most valuable!\n\nPlease feel free to open an issue if you run into any problems.\n"]}, {"number": 5623, "title": "Branch 139215742", "body": "Internal push.", "comments": ["@rmlarsen, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @ebrevdo and @keveman to be potential reviewers.\n", "@tensorflow-jenkins test this please\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n"]}, {"number": 5622, "title": "Problem in the documentation", "body": "It seems that the documentation has a problem. For example, the most useful functions such as `fit` or `evaluate` in [the documentation of tf.contrib.learn](https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.learn.html#Estimator) is as follows:\r\n\r\n> tf.contrib.learn.Estimator.fit(x=None, y=None, input_fn=None, steps=None, batch_size=None, monitors=None, max_steps=None)\r\n> \r\n> See Trainable.\r\n> \r\n> Raises:\r\n> \r\n> ValueError: If x or y are not None while input_fn is not None.\r\n> ValueError: If both steps and max_steps are not None.\r\n> \r\n\r\n[Trainable](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/trainable.py) does not linked or described in the generated documentation. It's really annoying issue for beginners.", "comments": ["Assigning to @martinwicke for further triage.  There are quite a few of these missing links, so some sort of automated or semi-scripted method may be in order.\n", "I'm publishing the Trainable/Evaluable docs. Sorry about that.\n"]}, {"number": 5621, "title": "R0.11", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@felithub, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @tensorflower-gardener and @gunan to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 5620, "title": "Enable a mechanism for filtering out log messages using", "body": "an environment variable.  Reads the environment variable\r\non the first call to LOG, uses it to filter out based on the\r\nseverity of the log message.\r\n\r\nBy default, min log level is 0, mapping to INFO.\r\nIf you set the env var to 1, INFO is filtered out b t\r\nWARNING still gets printed.\r\n\r\nParsing code is rudimentary, to avoid bringing in\r\ndependencies into platform (e.g., strings/lib/numbers.h,\r\nenv_var.h, etc).  For this use-case it is probably fine.\r\n\r\nFixes #1258.\r\n", "comments": ["@vrv, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @andrewharp and @tensorflower-gardener to be potential reviewers.\n"]}, {"number": 5619, "title": "Issue installing from sources with GPU support (l2loss_op_gpu.cu.pic.o' was not created.)", "body": "### Problem:\r\nWhen trying to install from sources using GPU support following the instructions from Tensorflow [documentation](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#optional-install-cuda-gpus-on-linux) using bazel command,\r\n\r\n`bazel build -c opt --config=CUDA --verbose_failures tensorflow/tools/pip_package:build_pip_package`\r\n\r\nI encounter the following error.\r\n\r\n./tensorflow/core/kernels/l2loss_op.h(32): here\r\n            instantiation of \"void tensorflow::functor::L2Loss<Device, T>::operator()(const Device &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstTensor, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Scalar) [with Device=tensorflow::GPUDevice, T=Eigen::half]\" \r\ntensorflow/core/kernels/l2loss_op_gpu.cu.cc(29): here\r\n\r\n10 errors detected in the compilation of \"/tmp/tmpxft_0000551e_00000000-7_l2loss_op_gpu.cu.cpp1.ii\".\r\nERROR: /home/kenjakt/temp/tensorflow/tensorflow/core/kernels/BUILD:1695:1: output 'tensorflow/core/kernels/_objs/l2loss_op_gpu/tensorflow/core/kernels/l2loss_op_gpu.cu.pic.o' was not created.\r\nERROR: /home/kenjakt/temp/tensorflow/tensorflow/core/kernels/BUILD:1695:1: not all outputs were created or valid.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n\r\n### Related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nThe problem seems to be somewhat related to these issues\r\nhttps://github.com/tensorflow/tensorflow/issues/4103\r\nhttps://github.com/tensorflow/tensorflow/issues/2143\r\n\r\n### Environment info\r\nOperating System: Centos - 6.7\r\nCUDA: 7.5\r\ncuDNN: 5.1.3\r\nGCC: 4.8.2\r\nbazel: 0.3.2\r\npython: 2.7.11 : : Anaconda 4.0.0 (64-bit)\r\n\r\n### Other attempted solutions tried\r\nBuilding with -c opt option excluded as suggested in https://github.com/tensorflow/tensorflow/issues/4103\r\nAdding cxx_flags as mentioned by @chrisburr in https://github.com/tensorflow/tensorflow/issues/1066#issuecomment-200580370", "comments": ["@kenjakt It doesn't seem like you included the full error message.  Also, I'm not optimistic that we'll be able to fix this, since the related bugs you point to you appear to be nvidia bugs.\n", "Closing due to lack of activity."]}, {"number": 5618, "title": "0.11.0rc2 bug weird placeholder feeding problem when using exponential moving average", "body": "using 0.11.0rc2\r\n\r\nthis simple (nonesense) script breaks:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n\r\nBATCH_SIZE = 128\r\nlearning_rate = 0.01\r\n\r\ndef model(x):\r\n  \"\"\"Defines the CNN architecture and returns its output tensor.\"\"\"\r\n  ema = tf.train.ExponentialMovingAverage(decay=0.5)\r\n\r\n  def mean_var_with_update():\r\n    ema_apply_op = ema.apply([tf.constant(1.0), tf.constant(2.0)])\r\n    return tf.constant(1.0), tf.constant(2.0)\r\n\r\n  mean, var = tf.cond(is_training,\r\n                      mean_var_with_update,\r\n                      lambda: (tf.constant(2.0), tf.constant(3.0)))\r\n  return x\r\n\r\nbatch_size = tf.placeholder(tf.float32, name=\"batch_size\")\r\nlr = tf.placeholder(tf.float32, name=\"learning_rate\")\r\nis_training = tf.placeholder(tf.bool, name='is_training')\r\ninput_image_batch = tf.placeholder(tf.float32, shape=[BATCH_SIZE, 28, 28, 1],\r\n                                   name=\"input_image_batch\")\r\ninput_label_batch = tf.placeholder(tf.float32, shape=[None, 10], name=\"input_label_batch\")\r\nlogits = model(input_image_batch)\r\ninit_op = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init_op)\r\n```\r\n\r\nproduces:\r\n```\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:05:00.0)\r\nTraceback (most recent call last):\r\n  File \"/home/schlag/MyStuff/LearnSpecific/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1021, in _do_call\r\n    return fn(*args)\r\n  File \"/home/schlag/MyStuff/LearnSpecific/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1003, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/schlag/MyStuff/LearnSpecific/env/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'is_training' with dtype bool\r\n\t [[Node: is_training = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"slim.py\", line 48, in <module>\r\n    sess.run(init_op)\r\n  File \"/home/schlag/MyStuff/LearnSpecific/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/home/schlag/MyStuff/LearnSpecific/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/schlag/MyStuff/LearnSpecific/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/schlag/MyStuff/LearnSpecific/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'is_training' with dtype bool\r\n\t [[Node: is_training = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nCaused by op 'is_training', defined at:\r\n  File \"slim.py\", line 36, in <module>\r\n    is_training = tf.placeholder(tf.bool, name='is_training')\r\n  File \"/home/schlag/MyStuff/LearnSpecific/env/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1519, in placeholder\r\n    name=name)\r\n  File \"/home/schlag/MyStuff/LearnSpecific/env/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2039, in _placeholder\r\n    name=name)\r\n  File \"/home/schlag/MyStuff/LearnSpecific/env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/schlag/MyStuff/LearnSpecific/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2259, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/schlag/MyStuff/LearnSpecific/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1130, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'is_training' with dtype bool\r\n\t [[Node: is_training = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n```\r\n\r\nI don't understand the problem. If I remove the ema (which I'm not using in the example) it works. I can also fix this by adding a feed dict to the init operation like this: \r\n```\r\nsess.run(init_op, feed_dict={is_training.name: False})\r\n```\r\nI don't think I have this problem with v0.11. I'm using ema in my batchnorm function. I removed all the unnecessary code from my full implementation.\r\n", "comments": ["Is the problem that you haven't initialized _local_ variables? https://www.tensorflow.org/versions/r0.11/api_docs/python/state_ops.html#initialize_local_variables.\n", "In that case it tells me \n\n```\ninitialize_local_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n```\n\nAnd then breaks the same way it broke before.\n\n```\nsess.run(tf.initialize_local_variables())\nsess.run(tf.global_variables_initializer())\n#sess.run(tf.global_variables_initializer(), feed_dict={is_training: False}) # this works instead of above\n```\n", "@martinwicke What's the new variable initialization story?\n", "This has nothing to do with variable initialization per se. You are making a placeholder is_training, and you use it (in the cond). You must provide a value for it, even when calling the init_op. \n\nIt may be confusing to you that you need to feed is_training, but that's because is_training is used to control whether to run the EMA, and thus every part of the EMA depends on it, include the initializer for its variables. \n\nThis works as expected (even though I'm the first to admit the conditionals are easy to be confused about). I'll close the issue, but If you have specific suggestions about how to improve errors or documentation, I'd welcome PRs or comments/issues.\n"]}, {"number": 5617, "title": "Compile build error tensorflow/tools/pip_package", "body": "Hi\r\nWhen I tried \r\n```bazel build -c opt --config=cuda --local_resources 2048,.5,1.0 //tensorflow/tools/pip_package:build_pip_package```\r\n\r\n, I see the following error:\r\n```INFO: Found 1 target...\r\nERROR: /XXXXX/tensorflow/python/BUILD:1926:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o ... (remaining 396 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nclang: warning: argument unused during compilation: '-pthread'\r\nduplicate symbol __Z14tf_git_versionv in:\r\n    bazel-out/local_darwin-py3-opt/bin/tensorflow/core/libframework_internal.pic.lo(version_info.pic.o)\r\n    bazel-out/local_darwin-py3-opt/bin/tensorflow/core/libversion_lib.pic.a(version_info.pic.o)\r\nduplicate symbol __Z19tf_compiler_versionv in:\r\n    bazel-out/local_darwin-py3-opt/bin/tensorflow/core/libframework_internal.pic.lo(version_info.pic.o)\r\n    bazel-out/local_darwin-py3-opt/bin/tensorflow/core/libversion_lib.pic.a(version_info.pic.o)\r\nld: 2 duplicate symbols for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /XXXX/tensorflow/tensorflow/tools/pip_package/BUILD:81:1 Linking of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o ... (remaining 396 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nINFO: Elapsed time: 2.094s, Critical Path: 1.50s\r\n```\r\n\r\nfollowing bazel version:\r\n\r\n```\r\nBuild label: 0.4.0-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 2 19:15:37 2016 (1478114137)\r\nBuild timestamp: 1478114137\r\nBuild timestamp as int: 1478114137```", "comments": ["@aselle Looks like you added `tf_git_version`.  Can you take a look?\n", "@TatsujiNakayama Is this fixed by https://github.com/tensorflow/tensorflow/pull/5616?\n", "Sorry mistake...\nI will try\n", "@girving \nThanks, I succeed build from new source code.\n"]}, {"number": 5616, "title": "Remove version_info.cc from framework_internal", "body": "version_info.cc was being compiled into both version_lib and framework_internal, resulting in duplicated symbols. The header version.h should also be associated with version_lib.", "comments": ["Can one of the admins verify this patch?\n", "@javidcf, thanks for your PR! By analyzing the history of the files in this pull request, we identified @girving, @josh11b and @tensorflower-gardener to be potential reviewers.\n", "Jenkins, test this please.\n"]}, {"number": 5615, "title": "Fails to configure with \"Object of type 'path' has no field \"realpath\"\" (BuildFileContainsErrorsException)", "body": "```bash\r\n\u00bb ./configure \r\n~/tmp/tensorflow ~/tmp/tensorflow\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.5/dist-packages\r\n  .\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n/home/wojciech/.local/lib/python3.5/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with GPU support? [y/N] y\r\nGPU support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \r\nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \r\nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 6.1\r\n.\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n.\r\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/wojciech/tmp/tensorflow/third_party/gpus/cuda_configure.bzl\", line 517\r\n\t\t_create_cuda_repository(repository_ctx)\r\n\tFile \"/home/wojciech/tmp/tensorflow/third_party/gpus/cuda_configure.bzl\", line 432, in _create_cuda_repository\r\n\t\t_cuda_toolkit_path(repository_ctx, cuda_version)\r\n\tFile \"/home/wojciech/tmp/tensorflow/third_party/gpus/cuda_configure.bzl\", line 148, in _cuda_toolkit_path\r\n\t\tstr(repository_ctx.path(cuda_toolkit...)\r\n\tFile \"/home/wojciech/tmp/tensorflow/third_party/gpus/cuda_configure.bzl\", line 148, in str\r\n\t\trepository_ctx.path(cuda_toolkit_path).realpath\r\nObject of type 'path' has no field \"realpath\".\r\n```\r\n\r\ngit hash: 7f3d8e1f8736f4c812e4a0a96154af3bd3750180\r\n```bash\r\n \u00bb bazel version                                                                                   \r\nBuild label: 0.3.1\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\r\nBuild timestamp: 1469783392\r\nBuild timestamp as int: 1469783392\r\n```\r\npython 3.5.2\r\n\r\nWorks fine on the r0.11 branch", "comments": ["Duplicate of https://github.com/tensorflow/tensorflow/issues/5319.\n"]}, {"number": 5614, "title": "[Windows/CMake] Use a more recent version of SWIG", "body": "This fixes `reader_ops_test.py`, which was suffering from a bug in older versions of SWIG when using Python 3.5 and a SWIG-wrapped generator object.", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @gunan to be a potential reviewer.\n", "@gunan, just trying this out directly, now that the Jenkins pool is upgraded.\n", "@gunan @rmlarsen This PR appears to fix the Windows build. I propose merging this rather than my internal CL, since this will be more useful sooner....\n"]}, {"number": 5613, "title": "Renamed variable bias to pre_activation", "body": "pre_activation is more meaningful.", "comments": ["@jithinoc, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @benoitsteiner to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n"]}, {"number": 5612, "title": "Issue running Tensorflow", "body": "Please forgive me if I use incorrect language - I am very inexperienced utilizing command prompt commands and all similar activities and am trying my best.\r\n\r\nI followed the instructions for setting up tensorflow and had it working this afternoon in the command prompt on my Windows machine. This evening I go to start practicing tensorflow and there are issues. When I follow the following directions:\r\n\r\n1. Run the docker vm: docker-machine create vdocker -d virtualbox\r\n2. In the command prompt window:\r\n2.A. FOR /f \"tokens=*\" %i IN ('docker-machine env --shell cmd vdocker') DO %i\r\n2.B.  docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow\r\n\r\neverything works until 2.B. Then it talks about Jupyter notebook running at all IP addresses.\r\n\r\nIf i try using: docker run -it b.gcr.io/tensorflow/tensorflow:latest-devel\r\n\r\nit says that it can't read the CA certificate for ...vdocker\\\\ ca.pem and it can't find the path specified.\r\n\r\nFurther google searching mentioned something about a bash_profile:\r\n\r\nexport DOCKER_HOST=tcp://192.168.99.100:2376\r\nexport DOCKER_MACHINE_NAME=default\r\nexport DOCKER_TLS_VERIFY=1\r\nexport DOCKER_CERT_PATH=/Users/idanadar/.docker/machine/machines/default\r\n\r\nbut I have ZERO idea what they are referencing, what the above means, or what to do about it. I can code in python, but the ideas behind the commands to set up tensorflow are above my head. I am currently just googling things to put in the command prompt hoping it will work and I can start practicing tensorflow. Could anyone please provide me very specific instructions for what to do in the command prompt or the Docker QuickStart Terminal so I can resolve this issue? I appreciate your time very much. Sorry for asking such a basic question.", "comments": ["Basic questions are good, but are better suited to StackOverflow, since then future users will more easily find the answers.  Can you re-ask your question there?\n"]}, {"number": 5611, "title": "Is there any way to let TensorFlow support Bitcode on iOS?", "body": "I'm trying to use TensorFlow on a iOS App, but my team required Bitcode enabled in the project, so I seek for help here.\r\n", "comments": ["@petewarden \n", "@petewarden Is this an easy flag?\n", "It's possible this just requires updating some compiler flags in tensorflow/contrib/makefile/Makefile for the protobuf and tensorflow library builds, but it would require some investigation.\n", "@413029811 Can you try adding the `-fembed-bitcode` flag to the `Makefile` and see if it solves it for you?\n\nhttps://stackoverflow.com/questions/31486232/how-do-i-xcodebuild-a-static-library-with-bitcode-enabled\n", "I added -fembed-bitcode in Makefile like this:\n       ifeq ($(IOS_ARCH),ARMV7)\n        -fembed-bitcode\\\n        CXXFLAGS += -miphoneos-version-min=$(MIN_SDK_VERSION) \\\n        -arch armv7 \\\n        -D__thread= \\\n        -DUSE_GEMM_FOR_CONV \\\n        -Wno-c++11-narrowing \\\n        -mno-thumb \\\n        -DTF_LEAN_BINARY \\\n        -D__ANDROID_TYPES_SLIM__ \\\n        -fno-exceptions \\\n        -isysroot \\\n        ${IPHONEOS_SYSROOT}\n        LDFLAGS := -arch armv7 \\\n        -miphoneos-version-min=${MIN_SDK_VERSION} \\\n        -framework Accelerate \\\n        -Xlinker -S \\\n        -Xlinker -x \\\n        -Xlinker -dead_strip \\\n        -all_load \\\n        -L$(GENDIR)protobuf_ios/lib \\\n        -lz\n    endif\n    ifeq ($(IOS_ARCH),ARMV7S)\n        -fembed-bitcode\\\n        CXXFLAGS += -miphoneos-version-min=$(MIN_SDK_VERSION) \\\n        -arch armv7s \\\n        -D__thread= \\\n        -DUSE_GEMM_FOR_CONV \\\n        -Wno-c++11-narrowing \\\n        -mno-thumb \\\n        -DTF_LEAN_BINARY \\\n        -D__ANDROID_TYPES_SLIM__ \\\n        -fno-exceptions \\\n        -isysroot \\\n        ${IPHONEOS_SYSROOT}\n        LDFLAGS := -arch armv7s \\\n        -miphoneos-version-min=${MIN_SDK_VERSION} \\\n        -framework Accelerate \\\n        -Xlinker -S \\\n        -Xlinker -x \\\n        -Xlinker -dead_strip \\\n        -all_load \\\n        -L$(GENDIR)protobuf_ios/lib \\\n        -lz\n    endif\n    ifeq ($(IOS_ARCH),ARM64)\n        -fembed-bitcode\\\n        CXXFLAGS += -miphoneos-version-min=$(MIN_SDK_VERSION) \\\n        -arch arm64 \\\n        -D__thread= \\\n        -DUSE_GEMM_FOR_CONV \\\n        -Wno-c++11-narrowing \\\n        -DTF_LEAN_BINARY \\\n        -D__ANDROID_TYPES_SLIM__ \\\n        -fno-exceptions \\\n        -isysroot \\\n        ${IPHONEOS_SYSROOT}\n        LDFLAGS := -arch arm64 \\\n        -miphoneos-version-min=${MIN_SDK_VERSION} \\\n        -framework Accelerate \\\n        -Xlinker -S \\\n        -Xlinker -x \\\n        -Xlinker -dead_strip \\\n        -all_load \\\n        -L$(GENDIR)protobuf_ios/lib \\\n        -lz\n    endif\n    ifeq ($(IOS_ARCH),I386)\n        -fembed-bitcode\\\n        CXXFLAGS += -mios-simulator-version-min=$(MIN_SDK_VERSION) \\\n        -arch i386 \\\n        -D__thread= \\\n        -DUSE_GEMM_FOR_CONV \\\n        -Wno-c++11-narrowing \\\n        -DTF_LEAN_BINARY \\\n        -D__ANDROID_TYPES_SLIM__ \\\n        -fno-exceptions \\\n        -isysroot \\\n        ${IPHONESIMULATOR_SYSROOT}\n        LDFLAGS := -arch i386 \\\n        -mios-simulator-version-min=${MIN_SDK_VERSION} \\\n        -framework Accelerate \\\n        -Xlinker -S \\\n        -Xlinker -x \\\n        -Xlinker -dead_strip \\\n        -all_load \\\n        -L$(GENDIR)protobuf_ios/lib \\\n        -lz\n    endif\n    ifeq ($(IOS_ARCH),X86_64)\n        -fembed-bitcode\\\n        CXXFLAGS += -mios-simulator-version-min=$(MIN_SDK_VERSION) \\\n        -arch x86_64 \\\n        -D__thread= \\\n        -DUSE_GEMM_FOR_CONV \\\n        -Wno-c++11-narrowing \\\n        -DTF_LEAN_BINARY \\\n        -D__ANDROID_TYPES_SLIM__ \\\n        -fno-exceptions \\\n        -isysroot \\\n        ${IPHONESIMULATOR_SYSROOT}\n        LDFLAGS := -arch x86_64 \\\n        -mios-simulator-version-min=${MIN_SDK_VERSION} \\\n        -framework Accelerate \\\n        -Xlinker -S \\\n        -Xlinker -x \\\n        -Xlinker -dead_strip \\\n        -all_load \\\n        -L$(GENDIR)protobuf_ios/lib \\\n        -lz\n    endif\n\nWhen I tried to run compile_ios_tensorflow.sh, command line printed:\n/tensorflow/tensorflow/contrib/makefile/gen/lib/ios_ARMV7/libtensorflow-core-armv7.a(version_info.o), archive member 'version_info.o' with length 936 is not mach-o or llvm bitcode for architecture armv7\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nmake: **\\* [tensorflow/tensorflow/contrib/makefile/gen/bin/ios_ARMV7/benchmark] Error 1\n- '[' 2 -ne 0 ']'\n- echo 'armv7 compilation failed.'\n  armv7 compilation failed.\n- exit 1\n", "@petewarden Unless you want to tackle this, should I mark it contributions welcome?\n", "I'm not likely to get to this in the short term, so that would be great, thanks.\n", "I adapted the build scripts and got\r\n\r\n    ld: -bind_at_load and -bitcode_bundle (Xcode setting ENABLE_BITCODE=YES) cannot be used together", "It actually works. Here are the steps:\r\n\r\n- Add `export MACOSX_DEPLOYMENT_TARGET=\"10.12\"` to `build_all_ios.sh`, otherwise you'll get a linker error (see above).\r\n- Insert `-fembed-bitcode \\` in `compile_ios_protobuf.sh` and `Makefile` below the `-arch` lines\r\n- Run `build_all_ios.sh`\r\n\r\nYou now have a working TF bitcode enabled iOS build.", "`export MACOSX_DEPLOYMENT_TARGET=\"10.12\"` should be added to `compile_ios_protobuf.sh` if used independent of `build_all_ios.sh`."]}, {"number": 5610, "title": "Add missing backticks to documentation.", "body": "The formatting for [extract_image_patches](https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#extract_image_patches) is missing backticks, which messes up the code vs non-code formatting.", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "I signed the CLA earlier and I'm the commit author. Is there something I need to do to pass the check or can you override it?\n", "Your git commits should use the email you signed the CLA with (right now it's using a 'local' git address).\n\nYou can git commit --amend after setting your git email correctly, and then you can force push to the same branch.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 5609, "title": "Add documentation on how to use bucketing functions", "body": "Can someone please add documentation on how to use _tensorflow.contrib.training.bucket_ and _tensorflow.contrib.training.bucket_by_sequence_length_ functions?\r\n\r\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System: Mac OS X Sierra\r\n\r\nInstalled version of CUDA and cuDNN: None\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: Mac OS X Python 2.7 CPU\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.: 0.11.0rc1\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Have you read the documentation?  https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.training.html#bucket_by_sequence_length\n", "Yes, I read the documentation. I am confused about how to use the outputs of the function. If I do output.eval() it hangs the program. \n", "You probably need to start queue runners: https://www.tensorflow.org/versions/r0.11/how_tos/threading_and_queues/index.html.\n\n@ebrevdo It seems tedious, but should we mention queues everywhere we create one (or at least at the top of the bucketing doc)?  It's pretty easy to not know one has to start queue runners after calling an obscure function.\n", "Also, the following code doesn't function as I would expect. I'm sure I am missing something, which is not clear from the documentation. Update: seems like the function is meant to bucket one input at a time? the documentation says `The list or dictionary of tensors, representing a single element, to bucket` .. I don't understand, when would I want to bucket/batch a single input? Also, for bucketing/batching multiple inputs (e.g. multiple sentences), do I call the function multiple times with different inputs?\n\nFor the following code:\n\n```\nseq_lengths = np.array([6, 3, 2])\ninputs = []\ninputs.append(tf.convert_to_tensor(np.array([2,3,3,3,3,3])))\ninputs.append(tf.convert_to_tensor(np.array([2, 3, 4])))\ninputs.append(tf.convert_to_tensor(np.array([3, 4])))\n\nsequences, output = bucket_by_sequence_length(input_length=seq_lengths, tensors= inputs, batch_size=2, bucket_boundaries =[1, 2], allow_smaller_final_batch=True,\n                                              dynamic_pad=True, capacity=2)\n\ninit_op = tf.initialize_all_variables()\n\nsess = tf.Session()\n\nsess.run(init_op)\n\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\ntry:\n    while not coord.should_stop():\n        s, o= sess.run([sequences, output])\n\nexcept tf.errors.OutOfRangeError:\n    print('Done training -- epoch limit reached')\nfinally:\n    coord.request_stop()\n\ncoord.join(threads)\nsess.close()\n```\n\n`o` is `[array([[2, 3, 3, 3, 3, 3],\n       [2, 3, 3, 3, 3, 3]]), array([[2, 3, 4],\n       [2, 3, 4]]), array([[3, 4],\n       [3, 4]])]` however, I was expecting the tensors of lengths 3 and 6 to be in the same bucket.\n", "Looking at the docs for the bucket_by_sequence_length function:\n\nbucket_boundaries: int list, _increasing_ non-negative numbers. The edges\nof the buckets to use when bucketing tensors. Two extra buckets are\ncreated, one for input_length < bucket_boundaries[0] and one for\ninput_length >= bucket_boundaries[-1].\n\nIn your case if you had sequence_lengths [2, 3, 6] then you get 5 buckets:\n\n[len < 2, 2 <= len < 3, 3 <= len < 6, len >= 6]\n\neither way, you would need to set the last of the lengths to 7 in order to\nget 3 and 6 in the same bucket.\n\nOn Tue, Nov 15, 2016 at 9:59 AM, Sonal Gupta notifications@github.com\nwrote:\n\n> Also, the following code doesn't function as I would expect. I'm sure I am\n> missing something, which is not clear from the documentation.\n> \n> For the following code:\n> \n> `seq_lengths = np.array([6, 3, 2])\n> inputs = []\n> inputs.append(tf.convert_to_tensor(np.array([2,3,3,3,3,3])))\n> inputs.append(tf.convert_to_tensor(np.array([2, 3, 4])))\n> inputs.append(tf.convert_to_tensor(np.array([3, 4])))\n> \n> sequences, output = bucket_by_sequence_length(input_length=seq_lengths,\n> tensors= inputs, batch_size=2, bucket_boundaries =[1, 2],\n> allow_smaller_final_batch=True,\n> dynamic_pad=True, capacity=2)\n> Create the graph, etc.\n> \n> init_op = tf.initialize_all_variables()\n> Create a session for running operations in the Graph.\n> \n> sess = tf.Session()\n> Initialize the variables (like the epoch counter).\n> \n> sess.run(init_op)\n> Start input enqueue threads.\n> \n> coord = tf.train.Coordinator()\n> threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n> \n> try:\n> while not coord.should_stop():\n> \n> # Run training steps or whatever\n> \n> s, o= sess.run([sequences, output])\n> \n> except tf.errors.OutOfRangeError:\n> print('Done training -- epoch limit reached')\n> finally:\n> \n> # When done, ask the threads to stop.\n> \n> coord.request_stop()\n> Wait for threads to finish.\n> \n> coord.join(threads)\n> sess.close()\n> `\n> \n> o is [array([[2, 3, 3, 3, 3, 3],\n> [2, 3, 3, 3, 3, 3]]), array([[2, 3, 4],\n> [2, 3, 4]]), array([[3, 4],\n> [3, 4]])] however, I was expecting the tensors of lengths 3 and 6 to be\n> in the same bucket.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5609#issuecomment-260716613,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim8hi_DovDpG7cHR4h5ba_mikNts_ks5q-fL5gaJpZM4KyDNA\n> .\n", "Thanks for the reply. If I give `bucket_boundaries=[2,3, 7]` then I get the exception `ValueError: Dimensions must be equal, but are 4 and 3`. Can you please explain what `tensors` exactly mean? Documentation says `the list or dictionary of tensors, representing a single element, to bucket..`. How do I bucket/batch multiple sentences?\n", "I don't understand these two bucketing methods neither. Could someone write a simple instruction?", "Perhaps I should add an example in the docstring.\n\nOn Jan 22, 2017 6:12 PM, \"Kuang R\" <notifications@github.com> wrote:\n\n> I don't understand these two bucketing methods neither. Could someone\n> write a simple instruction?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5609#issuecomment-274381386>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5YdvZU6xc_9lHcCI8AqaCMXQ9Lpks5rVAx5gaJpZM4KyDNA>\n> .\n>\n", "I have the same issue as  sonalgupta  . I pass in a list of roughly 605742 sequence lengths, and I have 24 buckets, and it errors out with:\r\n\r\n\r\n`  File \"./step_train_rnnlm.py\", line 66, in <module>\r\n    main()\r\n  File \"./step_train_rnnlm.py\", line 61, in main\r\n    n_epochs=10)\r\n  File \"/home/malinin/dnn_lib/rnnlm.py\", line 138, in fit\r\n    data = self._construct_bucket_queue(trn_data_list, batch_size, capacity=20000, num_threads=8)\r\n  File \"/home/malinin/dnn_lib/base_model.py\", line 226, in _construct_bucket_queue\r\n    name=None)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/training/python/training/bucket_ops.py\", line 347, in bucket_by_sequence_length\r\n    math_ops.less_equal(buckets_min, input_length),\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1189, in less_equal\r\n    result = _op_def_lib.apply_op(\"LessEqual\", x=x, y=y, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2382, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1783, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 596, in call_cpp_shape_fn\r\n    raise ValueError(err.message)\r\nValueError: Dimensions must be equal, but are 25 and 605742`\r\n\r\n\r\nI have a feeling something is not being broadcast correctly, but can't put my finger on it.\r\n", "It does seem that way.  When you say you pass in a list, are you saying you\nare literally passing in a python list of length 605742, each entry\ncontaining a tensor?\n\nIf so, then what you want to be doing is passing in a list with a single\ntensor whose leftmost dimension is 605742.\n\nOn Mon, Jan 30, 2017 at 6:40 AM, KaosEngineer <notifications@github.com>\nwrote:\n\n> I have the same issue as sonalgupta . I pass in a list of roughly 605742\n> sequence lengths, and I have 24 buckets, and it errors out with:\n>\n> File \"./step_train_rnnlm.py\", line 66, in <module> main() File\n> \"./step_train_rnnlm.py\", line 61, in main n_epochs=10) File\n> \"/home/malinin/dnn_lib/rnnlm.py\", line 138, in fit data =\n> self._construct_bucket_queue(trn_data_list, batch_size, capacity=20000,\n> num_threads=8) File \"/home/malinin/dnn_lib/base_model.py\", line 226, in\n> _construct_bucket_queue name=None) File \"/usr/local/lib/python2.7/\n> dist-packages/tensorflow/contrib/training/python/training/bucket_ops.py\",\n> line 347, in bucket_by_sequence_length math_ops.less_equal(buckets_min,\n> input_length), File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\",\n> line 1189, in less_equal result = _op_def_lib.apply_op(\"LessEqual\", x=x,\n> y=y, name=name) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/\n> python/framework/op_def_library.py\", line 749, in apply_op op_def=op_def)\n> File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\",\n> line 2382, in create_op set_shapes_for_outputs(ret) File\n> \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\",\n> line 1783, in set_shapes_for_outputs shapes = shape_func(op) File\n> \"/usr/local/lib/python2.7/dist-packages/tensorflow/\n> python/framework/common_shapes.py\", line 596, in call_cpp_shape_fn raise\n> ValueError(err.message) ValueError: Dimensions must be equal, but are 25\n> and 605742\n>\n> I have a feeling something is not being broadcast correctly, but can't put\n> my finger on it.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5609#issuecomment-276078661>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-De_Ce5VESTPZPfofZpx2B3Nifsks5rXfZngaJpZM4KyDNA>\n> .\n>\n", "@ebrevdo Hi Eugene,\r\nI have just watched your session in #tfdevsummit it was really good and informative. I am trying to use bucket_by_sequence_length. But I am not able to understand the functionality as others, it will be very helpful if you give an example on some classic data sets like WMT dataset used in seq2seq.\r\nIn seq2seq tutorial the bucketing is done manually so I am expecting that will be done automatically with these bucketing functions. But I am not clear about giving the correct parameters to the function.\r\n\r\nIt would be a great help if you can give us a tutorial in tensor flow website.\r\n\r\n\r\nThanks,\r\nkushwanth naga goutham", "We are working on a new seq2seq tutorial. I'll try to ensure this new\nbucketing operation is in there as well.\n\nOn Mar 1, 2017 5:57 AM, \"Kushwanth Naga Goutham\" <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Hi Eugene,\n> I have just watched your session in #tfdevsummit it was really good and\n> informative. I am trying to use bucket_by_sequence_length. But I am not\n> able to understand the functionality as others, it will be very helpful if\n> you have give a example on some classic data sets like WMT dataset used in\n> seq2seq.\n> In seq2seq tutorial the bucketing is done manually so I am expecting that\n> will be done automatically with these bucketing functions. But I am not\n> clear about giving the correct parameters to the function.\n>\n> It would be a great help if you can give us a tutorial in tensor flow\n> website.\n>\n> Thanks,\n> kushwanth naga goutham\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5609#issuecomment-283346076>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxNcyAqu0b6QILzm733XNJe_e0xRks5rhXk8gaJpZM4KyDNA>\n> .\n>\n", "I finally got `bucket_by_sequence_length` to work, here is what I think was hard:\r\n\r\n* It is not clear that `bucket_by_sequence_length` needs `input_length` and `tensors` to be elements from a queue.\r\n* It is not clear that `input_length` just controls the bucketing, the padding works separately.\r\n* Creating a queue that contains tensors of different sequence lengths is not trivial.\r\n\r\nHere is the example I got to work:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass SequenceTable:\r\n    def __init__(self, data):\r\n        # A TensorArray is required as the sequences don't have the same\r\n        # length. Alternatively a FIFOQueue can be used.\r\n        # Because the data is read more than once by the queue,\r\n        # clear_after_read is set to False (but I can't confirm an effect).\r\n        # Because the items has diffrent sequence lengths the infer_shape\r\n        # is set to False. The shape is then restored in the .read method.\r\n        self.table = tf.TensorArray(size=len(data),\r\n                                    dtype=data[0].dtype,\r\n                                    dynamic_size=False,\r\n                                    clear_after_read=False,\r\n                                    infer_shape=False)\r\n\r\n        # initialize table\r\n        for i, datum in enumerate(data):\r\n            self.table = self.table.write(i, datum)\r\n\r\n        # setup infered element shape\r\n        self.element_shape = tf.TensorShape((None, ) + data[0].shape[1:])\r\n\r\n    def read(self, index):\r\n        # read index from table and set infered shape\r\n        read = self.table.read(index)\r\n        read.set_shape(self.element_shape)\r\n        return read\r\n\r\n\r\ndef shuffle_bucket_batch(input_length, tensors, shuffle=True, **kwargs):\r\n    # bucket_by_sequence_length requires the input_length and tensors\r\n    # arguments to be queues. Use a range_input_producer queue to shuffle\r\n    # an index for sliceing the input_length and tensors laters.\r\n    # This strategy is idendical to the one used in slice_input_producer.\r\n    table_index = tf.train.range_input_producer(\r\n        int(input_length.get_shape()[0]), shuffle=shuffle\r\n    ).dequeue()\r\n\r\n    # the first argument is the sequence length specifed in the input_length\r\n    # I did not find a ue for it.\r\n    _, batch_tensors = tf.contrib.training.bucket_by_sequence_length(\r\n        input_length=tf.gather(input_length, table_index),\r\n        tensors=[tensor.read(table_index) for tensor in tensors],\r\n        **kwargs\r\n    )\r\n\r\n    return tuple(batch_tensors)\r\n\r\n\r\n# these values specify the length of the sequence and this controls how\r\n# the data is bucketed. The value is not required to be the acutal length,\r\n# which is also problematic when using pairs of sequences that have diffrent\r\n# length. In that case just specify a value that gives the best performance,\r\n# for example \"the max length\".\r\nlength_table = tf.constant([2, 4, 3, 4, 3, 7], dtype=tf.int32)\r\n\r\nsource_table = SequenceTable([\r\n    np.asarray([3, 4], dtype=np.int32),\r\n    np.asarray([2, 3, 4], dtype=np.int32),\r\n    np.asarray([1, 3, 4], dtype=np.int32),\r\n    np.asarray([5, 3, 4], dtype=np.int32),\r\n    np.asarray([6, 3, 4], dtype=np.int32),\r\n    np.asarray([3, 3, 3, 3, 3, 3], dtype=np.int32)\r\n])\r\n\r\ntarget_table = SequenceTable([\r\n    np.asarray([9], dtype=np.int32),\r\n    np.asarray([9, 3, 4, 5], dtype=np.int32),\r\n    np.asarray([9, 3, 4], dtype=np.int32),\r\n    np.asarray([9, 3, 4, 6], dtype=np.int32),\r\n    np.asarray([9, 3], dtype=np.int32),\r\n    np.asarray([9, 3, 3, 3, 3, 3, 2], dtype=np.int32)\r\n])\r\n\r\nsource_batch, target_batch = shuffle_bucket_batch(\r\n    length_table, [source_table, target_table],\r\n    batch_size=2,\r\n    # devices buckets into [len < 3, 3 <= len < 5, 5 <= len]\r\n    bucket_boundaries=[3, 5],\r\n    # this will bad the source_batch and target_batch independently\r\n    dynamic_pad=True,\r\n    capacity=2\r\n)\r\n\r\nwith tf.Session() as sess:\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess, coord)\r\n\r\n    for i in range(6):\r\n        source, target = sess.run((source_batch, target_batch))\r\n        print(f'source_output[{i}]')\r\n        print(source)\r\n        print(f'target_output[{i}]')\r\n        print(target)\r\n        print('')\r\n\r\n    coord.request_stop()\r\n    coord.join(threads)\r\n```\r\n\r\nThis outputs something like:\r\n\r\n```\r\nsource_output[0]\r\n[[6 3 4]\r\n [5 3 4]]\r\ntarget_output[0]\r\n[[9 3 0 0]\r\n [9 3 4 6]]\r\n\r\nsource_output[1]\r\n[[1 3 4]\r\n [2 3 4]]\r\ntarget_output[1]\r\n[[9 3 4 0]\r\n [9 3 4 5]]\r\n\r\nsource_output[2]\r\n[[6 3 4]\r\n [2 3 4]]\r\ntarget_output[2]\r\n[[9 3 0 0]\r\n [9 3 4 5]]\r\n\r\nsource_output[3]\r\n[[3 3 3 3 3 3]\r\n [3 3 3 3 3 3]]\r\ntarget_output[3]\r\n[[9 3 3 3 3 3 2]\r\n [9 3 3 3 3 3 2]]\r\n```", "A simpler solution (not requiring TensorArray) is probably to create a\nqueue that you feed sequences into one at a time in a separate python\nthread - the \"reader\"; and then in the main thread you read from that queue\nand pass that to bucket_by_sequence_length.  I'll try to include that in\nthe new tutorial.\n\nOn Sat, Mar 4, 2017 at 3:38 AM, Andreas Madsen <notifications@github.com>\nwrote:\n\n> I finally got bucket_by_sequence_length it to work, here is what I think\n> was had:\n>\n>    - It is not clear that bucket_by_sequence_length needs input_length\n>    and tensors to be elements from a queue.\n>    - It is not clear that input_length just controls the bucketing, the\n>    padding works separately.\n>    - Creating a queue that contains tensors of different sequence lengths\n>    is not trivial.\n>\n> Here is the example I got to work:\n>\n> import numpy as npimport tensorflow as tf\n>\n> class SequenceTable:\n>     def __init__(self, data):\n>         # A TensorArray is required as the sequences don't have the same\n>         # length. Alternatively a FIFO query can be used.\n>         # Because the data is read more than once by the queue,\n>         # clear_after_read is set to False (but I can't confirm an effect).\n>         # Because the items has diffrent sequence lengths the infer_shape\n>         # is set to False. The shape is then restored in the .read method.\n>         self.table = tf.TensorArray(size=len(data),\n>                                     dtype=data[0].dtype,\n>                                     dynamic_size=False,\n>                                     clear_after_read=False,\n>                                     infer_shape=False)\n>\n>         # initialize table\n>         for i, datum in enumerate(data):\n>             self.table = self.table.write(i, datum)\n>\n>         # setup infered element shape\n>         self.element_shape = tf.TensorShape((None, ) + data[0].shape[1:])\n>\n>     def read(self, index):\n>         # read index from table and set infered shape\n>         read = self.table.read(index)\n>         read.set_shape(self.element_shape)\n>         return read\n>\n> def shuffle_bucket_batch(input_length, tensors, shuffle=True, **kwargs):\n>     # bucket_by_sequence_length requires the input_length and tensors\n>     # arguments to be queues. Use a range_input_producer queue to shuffle\n>     # an index for sliceing the input_length and tensors laters.\n>     # This strategy is idendical to the one used in slice_input_producer.\n>     table_index = tf.train.range_input_producer(\n>         int(input_length.get_shape()[0]), shuffle=shuffle\n>     ).dequeue()\n>\n>     # the first argument is the sequence length specifed in the input_length\n>     # I did not find a ue for it.\n>     _, batch_tensors = tf.contrib.training.bucket_by_sequence_length(\n>         input_length=tf.gather(input_length, table_index),\n>         tensors=[tensor.read(table_index) for tensor in tensors],\n>         **kwargs\n>     )\n>\n>     return tuple(batch_tensors)\n>\n> # these values specify the length of the sequence and this controls how# the data is bucketed. The value is not required to be the acutal length,# which is also problematic when using pairs of sequences that have diffrent# length. In that case just specify a value that gives the best performance,# for example \"the max length\".\n> length_table = tf.constant([2, 4, 3, 4, 3, 7], dtype=tf.int32)\n>\n> source_table = SequenceTable([\n>     np.asarray([3, 4], dtype=np.int32),\n>     np.asarray([2, 3, 4], dtype=np.int32),\n>     np.asarray([1, 3, 4], dtype=np.int32),\n>     np.asarray([5, 3, 4], dtype=np.int32),\n>     np.asarray([6, 3, 4], dtype=np.int32),\n>     np.asarray([3, 3, 3, 3, 3, 3], dtype=np.int32)\n> ])\n>\n> target_table = SequenceTable([\n>     np.asarray([9], dtype=np.int32),\n>     np.asarray([9, 3, 4, 5], dtype=np.int32),\n>     np.asarray([9, 3, 4], dtype=np.int32),\n>     np.asarray([9, 3, 4, 6], dtype=np.int32),\n>     np.asarray([9, 3], dtype=np.int32),\n>     np.asarray([9, 3, 3, 3, 3, 3, 2], dtype=np.int32)\n> ])\n>\n> source_batch, target_batch = shuffle_bucket_batch(\n>     length_table, [source_table, target_table],\n>     batch_size=2,\n>     # devices buckets into [len < 3, 3 <= len < 5, 5 <= len]\n>     bucket_boundaries=[3, 5],\n>     # this will bad the source_batch and target_batch independently\n>     dynamic_pad=True,\n>     capacity=2\n> )\n> with tf.Session() as sess:\n>     coord = tf.train.Coordinator()\n>     threads = tf.train.start_queue_runners(sess, coord)\n>\n>     for i in range(6):\n>         source, target = sess.run((source_batch, target_batch))\n>         print(f'source_output[{i}]')\n>         print(source)\n>         print(f'target_output[{i}]')\n>         print(target)\n>         print('')\n>\n>     coord.request_stop()\n>     coord.join(threads)\n>\n> This outputs something like:\n>\n> source_output[0]\n> [[6 3 4]\n>  [5 3 4]]\n> target_output[0]\n> [[9 3 0 0]\n>  [9 3 4 6]]\n>\n> source_output[1]\n> [[1 3 4]\n>  [2 3 4]]\n> target_output[1]\n> [[9 3 4 0]\n>  [9 3 4 5]]\n>\n> source_output[2]\n> [[6 3 4]\n>  [2 3 4]]\n> target_output[2]\n> [[9 3 0 0]\n>  [9 3 4 5]]\n>\n> source_output[3]\n> [[3 3 3 3 3 3]\n>  [3 3 3 3 3 3]]\n> target_output[3]\n> [[9 3 3 3 3 3 2]\n>  [9 3 3 3 3 3 2]]\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5609#issuecomment-284146137>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0qiVXzQWLANQo7XSsHobK_v_6hXks5riU09gaJpZM4KyDNA>\n> .\n>\n", "@ebrevdo Thanks. I got something like that working for just one sequence, but I couldn't see how when using a pair of sequences. This is why I used the strategy from `slice_input_producer` (slice a `Tensor` using a `range_input_producer`.\r\n\r\nPS: Another thing I'm confused about is how to get a shuffling behavior similar to `shuffle_batch*`.", "Can you say more about how you want to combine pairs of sequences in a\nbatching mechanism?\n\nTo get shuffling behavior similar to shuffle_batch you can use a\nRandomShuffleQueue object.  Keep in mind, because you're dealing with\nvariable-length tensors (before minibatching) and we don't have a Padding\nversion of this queue, you should use the .dequeue() method to pull out one\n(shuffled) entry at a time, instead of dequeue_many().  Construct the\nobject with the argument shapes=None.  You can use this queue between your\ninput reader and batch_sequences_by_length in order to shuffle the\nsequences before batching them.\n\nOn Sat, Mar 4, 2017 at 1:53 PM, Andreas Madsen <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Thanks. I got something like that\n> working for just one sequence, but I couldn't see how when using a pair of\n> sequences. This is why I used the strategy from slice_input_producer\n> (slice a Tensor using a range_input_producer.\n>\n> Another thing I'm confused about is how to get a shuffling behavior\n> similar to shuffle_batch*.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5609#issuecomment-284186093>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9keLzftloqs6ExChMFCGDqXsmAgks5rid1TgaJpZM4KyDNA>\n> .\n>\n", "> Can you say more about how you want to combine pairs of sequences in a\r\nbatching mechanism?\r\n\r\nA pair of sequences like in `train.batch([source, target])`, which is useful in a machine translation model.\r\n\r\n> To get shuffling behavior similar to shuffle_batch you can use a\r\nRandomShuffleQueue object. ...\r\n\r\nThanks for the tip, I will give that a go.", "@ebrevdo Thanks for working on a tutorial. Any idea when would it be posted? Is there a way to subscribe to notification for new tutorials?", "Probably it'll be posted to the google research blog and probably announced\nby the @TensorFlow twitter handle.\n\nOn Tue, Apr 4, 2017 at 10:56 AM, Sonal Gupta <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Thanks for working on a tutorial.\n> Any idea when would it be posted? Is there a way to subscribe to\n> notification for new tutorials?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5609#issuecomment-291580963>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyzwhUwaqh9ZgL0UrUCnI-7cxEwuks5rsoQ9gaJpZM4KyDNA>\n> .\n>\n", "@ebrevdo @AndreasMadsen \r\nI gave it a shot to do this with a queuerunner, however I got stuck at some point, this is my current code\r\n```\r\n    BUCKET_BOUNDARIES = [2,4]\r\n    MAX_LEN = 6\r\n\r\n    def _to_sparse_tensor(self, sequences):\r\n \r\n          sp_indices = []\r\n          sp_vals = []\r\n          sp_shape = [len(sequences), self.MAX_LEN]\r\n \r\n          for i in range(0, len(sequences)):\r\n              for j in range(0, len(sequences[i])):\r\n                  sp_indices.append([i, j])\r\n                  sp_vals.append(sequences[i][j])\r\n \r\n          return tf.SparseTensor(indices=sp_indices, values=sp_vals, shape=sp_shape)\r\n\r\n      def __init__(self, batch_size=32, name='x-train'):\r\n \r\n          # load train corpus\r\n          sources, targets, lengths, validation_sources, validation_targets, validation_lengths = self._load_corpus()\r\n \r\n          print(\"Finished corpus loading\")\r\n          # to a constant tensor\r\n          source = self._to_sparse_tensor(sources)\r\n          target = self._to_sparse_tensor(targets)\r\n          length = tf.convert_to_tensor(np.asarray(lengths, dtype=np.int32))\r\n \r\n          input_queue = tf.train.shuffle_batch([length, source, target], batch_size,\r\n                                              batch_size*64, # capacity\r\n                                              batch_size*32, # min_after_dequeue\r\n                                              num_threads=32,\r\n                                              allow_smaller_final_batch=False, name=name)\r\n \r\n          lengths_t, sources_t, targets_t = input_queue\r\n\r\n          source_batch, target_batch = self.shuffle_bucket_batch(\r\n              lengths_t, [sources_t, targets_t],\r\n              batch_size=batch_size,\r\n              bucket_boundaries=self.BUCKET_BOUNDARIES,\r\n              # this will pad the source_batch and target_batch independently\r\n              dynamic_pad=True,\r\n              capacity=batch_size*64,\r\n              num_threads=32,\r\n              allow_smaller_final_batch=False, name=name\r\n          )\r\n```\r\n\r\nwhere `shuffle_bucket_batch` looks like this:\r\n```\r\ndef shuffle_bucket_batch(input_length, tensors, shuffle=True, **kwargs):\r\n    # the first argument is the sequence length specifed in the input_length\r\n    # I did not find a ue for it.\r\n    _, batch_tensors = tf.contrib.training.bucket_by_sequence_length(\r\n        input_length=input_length,\r\n        tensors=tensors,\r\n        **kwargs\r\n    )\r\n    return tuple(batch_tensors)\r\n```\r\nI've put the sources and targets into SparseArray (doing it manually, since I couldn't find a tf method similar to `convert_to_tensor` but for sparse data) so I can feed them into `shuffle_batch` queue (it can operate on SparseArrays), then I pass the outputs to the shuffle_bucket_batch.\r\n\r\nThe problem that I've hit is that I get some sort of dimension mismatch between the input_length and the amounts of buckets:\r\n`ValueError: Dimensions must be equal, but are 3 and 31978 for 'x-train_1/LessEqual' (op:'LessEqual') with input shapes: [3], [32,31978].`\r\n\r\nthe error happens at line 346 in this file: http://git.hiddenunit.com/hakan/tersorflow/blob/42d26aa7001dda6928771db8b4244067c7924a01/tensorflow/contrib/training/python/training/bucket_ops.py\r\n\r\nThe main difference with @AndreasMadsen is that I don't use `range_input_produce` since the input_lengths, sources and targets are batched together. \r\n\r\nIs it reasonable to use `SparseArray`, `shuffle_batch` and `bucket_by_sequence_length`? Any ideas why the dimension don't fit together?", "@ebrevdo @itsmeolivia has the tutorial been posted somewhere? I can't find it... ", "@sonalgupta I've decided to open source an Atrous CNN architecture for text classification, there I have implemented the sequence with variable length reading from a file, you can see the project [here](https://github.com/randomrandom/deep-atrous-cnn-sentiment), the code that you need is located in the [BaseDataLoader class](https://github.com/randomrandom/deep-atrous-cnn-sentiment/blob/master/data/base_data_loader.py). This is the actual snippet that you need:\r\n\r\n```\r\ndef __load_batch(self, file_names, record_defaults, data_column, bucket_boundaries, field_delim=_CSV_DELIM,\r\n                     skip_header_lines=0,\r\n                     num_epochs=None, shuffle=True):\r\n\r\n        original_file_names = file_names[:]\r\n        file_names = self.__generate_preprocessed_files(file_names, data_column, bucket_boundaries,\r\n                                                        field_delim=field_delim)\r\n\r\n        filename_queue = tf.train.string_input_producer(\r\n            file_names, num_epochs=num_epochs, shuffle=shuffle\r\n        )\r\n\r\n        self.shuffle_queue = tf.RandomShuffleQueue(capacity=self._capacity, min_after_dequeue=self._min_after_dequeue,\r\n                                                   dtypes=[tf.int64, tf.int32], shapes=None)\r\n\r\n\r\n        example, label = self._read_file(filename_queue, record_defaults, field_delim, skip_header_lines)\r\n\r\n        voca_path, voca_name = BaseDataLoader._split_file_to_path_and_name(\r\n            original_file_names[0])  # TODO: will be break with multiple filenames\r\n        voca_name = KagglePreprocessor.VOCABULARY_PREFIX + voca_name\r\n        self.__vocabulary_file = voca_path + voca_name\r\n\r\n        # load look up table that maps words to ids\r\n        self.table = tf.contrib.lookup.index_table_from_file(vocabulary_file=voca_path + voca_name,\r\n                                                             default_value=KagglePreprocessor.UNK_TOKEN_ID,\r\n                                                             num_oov_buckets=0)\r\n\r\n        # convert to tensor of strings\r\n        split_example = tf.string_split([example], \" \")\r\n\r\n        # determine lengths of sequences\r\n        line_number = split_example.indices[:, 0]\r\n        line_position = split_example.indices[:, 1]\r\n        lengths = (tf.segment_max(data=line_position,\r\n                                  segment_ids=line_number) + 1).sg_cast(dtype=tf.int32)\r\n\r\n        # convert sparse to dense\r\n        dense_example = tf.sparse_tensor_to_dense(split_example, default_value=\"\")\r\n        dense_example = self.table.lookup(dense_example)\r\n\r\n        # get the enqueue op to pass to a coordintor to be run\r\n        self.enqueue_op = self.shuffle_queue.enqueue([dense_example, label])\r\n        dense_example, label = self.shuffle_queue.dequeue()\r\n\r\n        # add queue to queue runner\r\n        self.qr = tf.train.QueueRunner(self.shuffle_queue, [self.enqueue_op] * self.num_threads)\r\n        tf.train.queue_runner.add_queue_runner(self.qr)\r\n\r\n        # reshape from <unknown> shape into proper form after dequeue from random shuffle queue\r\n        # this is needed so next queue can automatically infer the shape properly\r\n        dense_example = dense_example.sg_reshape(shape=[1, -1])\r\n        label = label.sg_reshape(shape=[1])\r\n\r\n        _, (padded_examples, label_examples) = tf.contrib.training.bucket_by_sequence_length(lengths,\r\n                                                                                             [dense_example, label],\r\n                                                                                             batch_size=self._batch_size,\r\n                                                                                             bucket_boundaries=bucket_boundaries,\r\n                                                                                             dynamic_pad=True,\r\n                                                                                             capacity=self._capacity,\r\n                                                                                             num_threads=self._num_threads)\r\n\r\n        # reshape shape into proper form after dequeue from bucket queue\r\n        padded_examples = padded_examples.sg_reshape(shape=[self._batch_size, -1])\r\n        label_examples = label_examples.sg_reshape(shape=[self._batch_size])\r\n\r\n        return padded_examples, label_examples\r\n```\r\nThe above piece of code:\r\n1. picks a file name from a queue with file name\r\n1. reads single examples from the file via file reader\r\n1. loads vocabulary of words from a preprocessed file\r\n1. uses the vocabulary of words to turn the string tensor into tensor of ids\r\n1. puts the single examples into a RandomShuffleQueue which allows all the examples to be shuffled\r\n1. reads a single example from the RandomShuffleQueue and puts it to a bucket_by_sequence queue with dynamic padding\r\n1. and finally reads batches of examples from the bucket_by_sequence queue\r\n\r\nHope this is helpful to you", "@AndreasMadsen Thanks for the code above using TensorArray. Did you ever manage to replace it with a FIFOQueue. I have tried multiple times to no avail.  Many thanks.", "@francotheengineer you can replace it with something like this:\r\n\r\n```py\r\n\r\nclass SequenceQueueExternal(SequenceQueue):\r\n    data_file: str\r\n    writer = tf.python_io.TFRecordWriter\r\n\r\n    def __init__(self, external_encoding: str, *args, **kwargs):\r\n        self.data_file = path.realpath(external_encoding)\r\n\r\n        # detect if data file exists\r\n        has_data = (path.exists(self.data_file) and\r\n                    os.stat(self.data_file).st_size > 0)\r\n        super().__init__(not has_data, *args, **kwargs)\r\n\r\n        if self.need_data:\r\n            os.makedirs(path.dirname(self.data_file), exist_ok=True)\r\n            with open(self.data_file, 'w'):\r\n                self.writer = tf.python_io.TFRecordWriter(\r\n                    self.data_file,\r\n                    options=tf.python_io.TFRecordOptions(\r\n                        tf.python_io.TFRecordCompressionType.ZLIB\r\n                    )\r\n                )\r\n\r\n    def write(self,\r\n              length: int, source: np.ndarray, target: np.ndarray) -> None:\r\n        if not self.need_data:\r\n            raise RuntimeError(\r\n                'queue.write should not be called when need_data is false'\r\n            )\r\n\r\n        example = make_sequence_example(length, source, target)\r\n        self.writer.write(example.SerializeToString())\r\n\r\n    def read(self) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\r\n        if self.need_data:\r\n            self.writer.close()\r\n\r\n        # create filename queue of one filename. TFRecordReader demands this.\r\n        filename_queue = tf.train.string_input_producer(\r\n            [self.data_file],\r\n            num_epochs=None if self.repeat else 1,\r\n            name=self.name\r\n        )\r\n\r\n        # read serialized data\r\n        reader = tf.TFRecordReader(\r\n            options=tf.python_io.TFRecordOptions(\r\n                tf.python_io.TFRecordCompressionType.ZLIB\r\n            )\r\n        )\r\n        reader_dequeue = reader.read(filename_queue)\r\n\r\n        # parse data\r\n        length, source, target = parse_sequence_example(reader_dequeue.value)\r\n\r\n        # cast to original type\r\n        length = tf.cast(length, dtype=tf.int32)\r\n        source = tf.cast(source, dtype=self.dtype)\r\n        target = tf.cast(target, dtype=self.dtype)\r\n\r\n        # To get a continues shuffling behaviour similar to suffle_batch\r\n        # put in a RandomShuffleQueue\r\n        if self.shuffle:\r\n            length, source, target = shuffle_tensor_list(\r\n                (length, source, target),\r\n                capacity=self.batch_size * 128,\r\n                min_after_dequeue=self.batch_size * 64,\r\n                seed=self.seed,\r\n                name=self.name\r\n            )\r\n\r\n        return (length, source, target)\r\n```", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (26/25 (104%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\n------ The body of the message is 14836 characters long; only the first\n------ 5000 or so are included here.\nReceived: from o4.sgmail.github.com ([192.254.112.99]:55419)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES128-GCM-SHA256:128)\n\t(Exim 4.89)\n\t(envelope-from <bounces+848413-e6d9-mazecreator=mazecreator.com@sgmail.github.com>)\n\tid 1ddKNW-00048A-Fv\n\tfor mazecreator@mazecreator.com; Thu, 03 Aug 2017 12:58:17 -0500\nDKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=github.com; \n\th=from:reply-to:to:cc:in-reply-to:references:subject:mime-version:content-type:content-transfer-encoding:list-id:list-archive:list-post:list-unsubscribe; \n\ts=s20150108; bh=B2DquR6IstTcRsdhjQeqo1ZdRQY=; b=TBqhTIoVIOwougWf\n\tp5TL8lkpdAChnGKeRPBQOoQQmp0fvwpWmR44i4zk3eWSJdbV1D4PXejPVdfRWFuj\n\txKSDJzRL2sESrE8IHpXt9Nv+QbuHd2u/XJbHIHYoXXsKOXvCFO6S4TRXEP9CXRER\n\tijCQs89ZtOqE/6Dn1fIfdOoJJAY=\nReceived: by filter0439p1mdw1.sendgrid.net with SMTP id filter0439p1mdw1-8131-59836697-54\n        2017-08-03 18:08:23.727792632 +0000 UTC\nReceived: from github-smtp2a-ext-cp1-prd.iad.github.net (github-smtp2a-ext-cp1-prd.iad.github.net [192.30.253.16])\n\tby ismtpd0030p1mdw1.sendgrid.net (SG) with ESMTP id LAxiP_e0Rlupo7wGghbOCg\n\tfor <mazecreator@mazecreator.com>; Thu, 03 Aug 2017 18:08:23.641 +0000 (UTC)\nDate: Thu, 03 Aug 2017 18:08:23 +0000 (UTC)\nFrom: Andreas Madsen <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/issues/5609/320046002@github.com>\nIn-Reply-To: <tensorflow/tensorflow/issues/5609@github.com>\nReferences: <tensorflow/tensorflow/issues/5609@github.com>\nSubject: Re: [tensorflow/tensorflow] Add documentation on how to use bucketing\n functions (#5609)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_598366974122_66ae3ff2e60a5c303173a8\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: AndreasMadsen\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a0233cd248cebf628debe904ce04cc17dde108f66692cf00000001159b289792a169ce0b4836a1@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoGUNhusBD4cFgBlx8CGVIkfpotc4ks5sUgyXgaJpZM4KyDNA>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-SG-EID: BJ4qYjf5a3yL0lCrdDNghY4YYR+k1a+cluU6wEX1JqzbEs87w+mYm/unNG0159r/UEAApwEJBsj9up\n 5/RFJG+hFL0CfCPELQXc3qRzY5x+PGzPzqlgb+gHP3WIKr5vDdVS/MY1IiP2yzfqcCTkgYjkpG38mG\n dqi0wpbITd75JwnNOdhAEnRMZ3nsOIBiGHi3DabG8lcZ32lfkkONuaTIrPgFjyJfyD45sIqHImBChB\n A=\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n----==_mimepart_598366974122_66ae3ff2e60a5c303173a8\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n@francotheengineer you can replace it with something like this:\n\n```py\n\nclass SequenceQueueExternal(SequenceQueue):\n    data_file: str\n    writer = tf.python_io.TFRecordWriter\n\n    def __init__(self, external_encoding: str, *args, **kwargs):\n        self.data_file = path.realpath(external_encoding)\n\n        # detect if data file exists\n        has_data = (path.exists(self.data_file) and\n                    os.stat(self.data_file).st_size > 0)\n        super().__init__(not has_data, *args, **kwargs)\n\n        if self.need_data:\n            os.makedirs(path.dirname(self.data_file), exist_ok=True)\n            with open(self.data_file, 'w'):\n                self.writer = tf.python_io.TFRecordWriter(\n                    self.data_file,\n                    options=tf.python_io.TFRecordOptions(\n                        tf.python_io.TFRecordCompressionType.ZLIB\n                    )\n                )\n\n    def write(self,\n              length: int, source: np.ndarray, target: np.ndarray) -> None:\n        if not self.need_data:\n            raise RuntimeError(\n                'queue.write should not be called when need_data is false'\n            )\n\n        example = make_sequence_example(length, source, target)\n        self.writer.write(example.SerializeToString())\n\n    def read(self) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n        if self.need_data:\n            self.writer.close()\n\n        # create filename queue of one filename. TFRecordReader demands this.\n        filename_queue = tf.train.string_input_producer(\n            [self.data_file],\n            num_epochs=None if self.repeat else 1,\n            name=self.name\n        )\n\n        # read serialized data\n        reader = tf.TFRecordReader(\n            options=tf.python_io.TFRecordOptions(\n                tf.python_io.TFRecordCompressionType.ZLIB\n            )\n        )\n        reader_dequeue = reader.read(filename_queue)\n\n        # parse data\n        length, source, target = parse_sequence_example(reader_dequeue.value)\n\n        # cast to original type\n        length = tf.cast(length, dtype=tf.int32)\n        source = tf.cast(source, dtype=self.dtype)\n        target = tf.cast(target, dtype=self.dtype)\n\n        # To get a continues shuffling behaviour similar to suffle_batch\n        # put in a RandomShuffleQueue\n        if self.shuffle:\n            length, source, target = shuffle_tensor_list(\n                (length, source, target),\n                capacity=self.batch_size * 128,\n                min_after_dequeue=self.batch_size * 64,\n                seed=self.seed,\n                name=self.name\n            )\n\n        return (length, source, target)\n```\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/issues/5609#issuecomment-320046002\n----==_mimepart_598366974122_66ae3ff2e60a5c303173a8\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: quoted-printable\n\n<p><a href=3D\"https://github.com/francotheengineer\" class=3D\"user-mention\">=\n@francotheengineer</a> you can replace it with something like this:</p>\n<div class=3D\"highlight highlight-source-python\"><pre><span class=3D\"pl-k\">=\nclass</span> <span class=3D\"pl-en\">SequenceQueueExternal</span>(<span class=\n=3D\"pl-e\">SequenceQueue</span>):\n    data_file: <span class=3D\"pl-c1\">str</span>\n    writer <span class=3D\"pl-k\">=3D</span> tf.python_io.TFRecordWriter\n\n    <span class=3D\"pl-k\">def</span> <span class=3D\"pl-c1\">__init__</span>(<=\nspan class=3D\"pl-smi\"><span class=3D\"pl-smi\">self</span></span>, <span clas=\ns=3D\"pl-smi\">external_encoding</span>: <span class=3D\"pl-c1\">str</span>, <s=\npan class=3D\"pl-k\">*</span><span class=3D\"pl-smi\">args</span>, <span class=\n=3D\"pl-k\">**</span><span class=3D\"pl-smi\">kwargs</span>):\n        <span class=3D\"pl-c1\">self</span>.data_file <span class=3D\"pl-k\">=\n=3D</span> path.realpath(external_encoding)\n\n        <span class=3D\"pl-c\"><span class=3D\"pl-c\">#</span> detect if data f=\nile exists</span>\n        has_data <span class=3D\"pl-k\">=3D</span> (path.exists(<span class=\n=3D\"pl-c1\">self</span>.data_file) <span class=3D\"pl-k\">and</span>\n                    os.stat(<span class=3D\"pl-c1\">self</span>.data_file).st=\n_size <span class=3D\"pl-k\">&gt;</span> <span class=3D\"pl-c1\">0</span>)\n        <span class=3D\"pl-c1\">super</span>().<span class=3D\"pl-c1\">__init__=\n</span>(<span class=3D\"pl-k\">not</span> has_data, <span class=3D\"pl-k\">*</s=\npan>args, <span class=3D\"pl-k\">**</span>kwargs)\n\n        <span class=3D\"pl-k\">if</span> <span class=3D\"pl-c1\">self</span>.ne=\ned_data:\n            os.makedirs(path.dirname(<span class=3D\"pl-c1\">self</span>.data=\n_file), <span class=3D\"pl-v\">exist_ok</span><span class=3D\"pl-k\">=3D</span>=\n<span class=3D\"pl-c1\">True</span>)\n            <span class=3D\"pl-k\">with</span> <span class=3D\"pl-c1\">open</sp=\nan>(<span c\n", "For those searching for solutions in the future: https://github.com/francotheengineer/Bucket_by_sequence_length    Thanks. ", "`tf.contrib.data` (will be `tf.data` in 1.4) is moving to replace the queue based input pipelines.\r\n\r\nIt has a method that can allow this bucket-by-sequence-length functionality as well: [`group_by_window`](https://www.tensorflow.org/api_docs/python/tf/contrib/data/Dataset#group_by_window).\r\n\r\nIt's not a one-liner, but this [\"colorbot\"](https://github.com/mari-linhares/tensorflow-workshop/blob/master/code_samples/RNN/colorbot/colorbot.ipynb) demonstrates how you can use datasets and `group_by_window` to do your own `batch_by_sequence_length`.", "There's an additional example here\n<https://github.com/tensorflow/nmt/blob/master/nmt/utils/iterator_utils.py#L81>\nin\nthe tensorflow NMT tutorial.\n\nOn Tue, Oct 3, 2017 at 10:05 AM, Mark Daoust <notifications@github.com>\nwrote:\n\n> tf.contrib.data (will be tf.data in 1.4) is moving to replace the queue\n> based input pipelines.\n>\n> It has a method that can allow this bucket-by-sequence-length\n> functionality as well: group_by_window\n> <https://www.tensorflow.org/api_docs/python/tf/contrib/data/Dataset#group_by_window>\n> .\n>\n> It's not a one-liner, but this \"colorbot\"\n> <https://github.com/mari-linhares/tensorflow-workshop/blob/master/code_samples/RNN/colorbot/colorbot.ipynb>\n> demonstrates how you can use datasets and group_by_window to do your own\n> batch_by_sequence_length.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5609#issuecomment-333911658>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim4Kdvzk-9E_HY38B3c11VpzZPxvoks5somlkgaJpZM4KyDNA>\n> .\n>\n"]}, {"number": 5608, "title": "How to dynamically select scope of untied variables during tf.while_loop?", "body": "When using python for loop, one simply creates/selects a new untied variable by using the incrementor to change the scope:\r\n```\r\nfor i range(num_untied_layers):  \r\n     with tf.variable_scope(\"untied_attention_hop\" + \"/\" + str(i)):\r\n          Wt = tf.get_variable(\"W_t\")\r\n          bt = tf.get_variable(\"bias_t\")\r\n```\r\n\r\nHow would one do this with tf.while_loop?\r\n\r\n@DeNeutoy", "comments": ["@ethancaballero As far as I am aware it is not possible to do this, as the `tf.while_loop` is not actually creating a dynamic graph - it is merely an instruction to repeat part of the graph until a condition is satisfied.\n\nHowever, presumably there is some upper bound on the number of layers your want your model to be able to adaptively select (if not I would strongly advise this - you would be surprised how many steps these kind of models can take). What you should then do, is create these variables beforehand in a python for loop:\n\n```\nWt = []\nfor i in range(num_untied_layers):\n    with tf.variable_scope(\"W_{}\".format(i))\n        Wt[i] = tf.get_variable(\"Wt\")\n```\n\nThen, you can evaluate these within your loop by using tf.case - this is a bit ugly and there is probably a better way to do it:\n\n```\ncounter = tf.constant(0)\ndef generate_predicate_function_pair(counter, i):\n    return (tf.less(counter, i), function_using_ith_weight(inputs, weights, ...))\n\npredicate_list = [generate_predicate_function_pair(i) for i in range(max_attention_hops)] \n\n# Now start a while loop which contains a tensor constant called counter which just\n# increments the loop ...\n\nresult = tf.case(predicate_list, exclusive=True)\n```\n\nNote that you will also need to factor this max number of steps into your halting function, as otherwise it will crash if you go over it.\n", "Another idea would be to use TensorArrays:\n\nAs before:\n\n```\n\nwith tf.variable_scope(\"one_weight_to_rule_them_all\"):\n    large_weight = tf.get_variable(\"Wt\", [max_steps*input_size, output_size])\n\n# The clear_after_read variable must be False, otherwise the TA will \n# only allow you to read from that index once.\nweight_container = tf.TensorArray(tf.float32, max_steps, \n                             clear_after_read=False, dynamic_size=None)\n\n# This initialises the TensorArray with the weights broken up in to pieces.\n# The reason this has to be a TensorArray is so that we can index it with a tensor(!)\nweight_container = weight_container.unpack(large_weight)\n\n# Now inside the while_loop with a counter variable AND the \n# weight_container as a loop_var:\n\nweight_to_use = weight_container.read(counter)\n\n```\n\nCounter needs to be a integer scalar tensor which is incremented by one within the while_loop.\n\nHope either of these ideas helps!\n", "@DeNeutoy Thanks for the quick reply.\n\n^Does one of those ideas run faster than the other?\n\nAlso, do you have any advice for dealing with asynchronous halting times within a batch. For example, if one member of batch decides to take 20 hops (which finishes slower) and the rest of the batch decides to take \u2264 3 hops (which finish quickly), the next batch can't be processed until the longer 20 hop member finishes.\n\nWas this your workaround, and is there a reason that the cost isn't fetched in the asynchronous version? :\nhttps://github.com/DeNeutoy/act-rte-inference/blob/master/epoch.py#L108\n", "This type of question is better suited to StackOverflow.  Github issues are for bug reports and feature requests.\n"]}, {"number": 5607, "title": "Fix: Create a new script for gpu-mac tests.", "body": "Also fix the gpu-mac test build issues with the script.", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ebrevdo, @caisq and @rohan100jain to be potential reviewers.\n", "Jenkins, test this please.\n", "Running this script here:\nhttp://ci.tensorflow.org/view/Experimental/job/experimental-gpu-mac-pip/18/console\n\nwill see if the script correctly executes and exports the wheel files.\n", "On Wed, Nov 16, 2016 at 12:39 PM, Shanqing Cai notifications@github.com\nwrote:\n\n> ## _@caisq_ commented on this pull request.\n> \n> In tensorflow/tools/ci_build/mac/gpu/pip/run.sh\n> https://github.com/tensorflow/tensorflow/pull/5607:\n> \n> > +#\n> > +# Licensed under the Apache License, Version 2.0 (the \"License\");\n> > +# you may not use this file except in compliance with the License.\n> > +# You may obtain a copy of the License at\n> > +#\n> > +#     http://www.apache.org/licenses/LICENSE-2.0\n> > +#\n> > +# Unless required by applicable law or agreed to in writing, software\n> > +# distributed under the License is distributed on an \"AS IS\" BASIS,\n> > +# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n> > +# See the License for the specific language governing permissions and\n> > +# limitations under the License.\n> > +#\n> > +# ==============================================================================\n> > +#\n> > +# Usage:\n> \n> OK. I still have some remaining concerns. So it seems that this file is\n> meant to replace pip.sh on mac. But this script doesn't do certain things\n> that are done by pip.sh. For example, it doesn't do renaming of the pip\n> files as here:\n> https://github.com/tensorflow/tensorflow/blob/master/\n> tensorflow/tools/ci_build/builds/pip.sh#L167\n> \n> If you add that to this file, it'll further increase the amount of\n> duplication. So maybe instead of creating a new file, it will be better to\n> refactor pip.sh.\n> \n> Wait I missed that. (going back to my point that it is not obvious what\n> these scripts do.)\n> Why do we rename them?\n> Can we edit our pip build scripts to create pip packages with the correct\n> name in the first place?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5607, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AHlCOXr5iVsUPvjr6oJddgyHVh1JoZgPks5q-2nmgaJpZM4KyARJ\n> .\n", "Is this PR still being actively worked on? If so, I have the following remaining requests:\r\n1) Refactor the common code, such as the code for renaming the wheels to a common location, such as build_common.sh, to avoid duplication once we reach the goal of having different build configs in different folders, like mac/gpu, linux/cpu, etc. The wheel renaming turned out to be necessary to avoid install issues seen by many users. But Yifei should be the leading expert on this issue now, since she's worked on PyPi distribution.\r\n2) The line for getting the current script directory should be made a robust, if it's not too much trouble to make it work for Windows, IMHO.", "I will work on this, but it took a back seat due to the release.\nOnce the release is in line, I will get back to it.\n\nOn Mon, Nov 21, 2016 at 6:27 AM, Shanqing Cai notifications@github.com\nwrote:\n\n> Is this PR still being actively worked on? If so, I have the following\n> remaining requests:\n> 1. Refactor the common code, such as the code for renaming the wheels\n>    to a common location, such as build_common.sh, to avoid duplication once we\n>    reach the goal of having different build configs in different folders, like\n>    mac/gpu, linux/cpu, etc. The wheel renaming turned out to be necessary to\n>    avoid install issues seen by many users. But Yifei should be the leading\n>    expert on this issue now, since she's worked on PyPi distribution.\n> 2. The line for getting the current script directory should be made a\n>    robust, if it's not too much trouble to make it work for Windows, IMHO.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5607#issuecomment-261951892,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AHlCOTFXNiAzwnFvrAH4OJd1UKAIoavEks5rAapIgaJpZM4KyARJ\n> .\n", "I'm going to close this if it's not being actively worked on, although feel free to re-open if you want it that way. "]}, {"number": 5606, "title": "Support for tf.transpose for tensors of rank 12 or higher", "body": "Can support for `tf.transpose` please be added for tensors above rank 11? Can it be extended to tensors of rank size 40 or 50 on the gpu?\r\n\r\nI know this seems large, but I'm trying to implement this tensorization paper where this is demanded. \r\n\r\nhttps://arxiv.org/pdf/1611.03214v1.pdf", "comments": ["Unfortunately, the current code for transpose does template instantiation for each dimension.  We're already running into code size issues, so we won't be able to accept 40 or 50 (a rather large chunk of the tensorflow binary would become transposes).\n\nHowever, we'd be happy to accept PRs that implement arbitrary rank transposes in a manner that doesn't require unbounded template instantiation.  There are a couple options for this:\n1. Find a clever new algorithm that I've searched for but failed to find that efficiently does arbitrary rank transpose.  I would be thrilled.\n2. Split high rank transposes into a sequence of reshapes and smaller rank transposes.  This requires a fair amount of index machinery, but is \"straightforward\".  It could be done either in C++ and Python, but is probably easier to do in C++ since then the index manipulation doesn't have to be TensorFlow ops.\n\nIn either case, it would be good to add the optimization that flattens dimensions that transpose together so as to use lower rank code if possible.\n\nNote that [XLA](https://www.tensorflow.org/versions/master/resources/xla_prerelease.html) will likely have no trouble with high rank transposes, but that may take a while to land and wouldn't necessary be very efficient (for the same reasons I haven't found a fast high rank transpose algorithm).  Thus, I don't think the above work would be wasted.\n", "@girving Thanks for such a comprehensive response. \n\nWould it at all be possible to even raise the acceptable rank number to 15 or 20? This alone would open up alot of possibilities. Hopefully it wouldn't raise the binary size too much.\n\nIdea 1 would be miraculous but I'm just as lost as you are on how to specifically implement this. \n", "@LeavesBreathe Actually, I'm confused.  It looks like someone already wrote the slow but arbitrary rank version: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/transpose_functor_gpu.cu.cc#L27.  What error message you seeing?\n", "I asked about it on scicomp: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/transpose_functor_gpu.cu.cc#L27.  Curious if someone knows a faster version.\n", "I apologize but I don't have the error message in front of me right now. I'm going to close this thread as i'm not providing adequate documentation. From what I remember the message said \"tf.transpose does not support tensor of rank 12\"\n"]}, {"number": 5605, "title": "Remove _logfile_map, _logfile_map_mutex. They are not used.", "body": "", "comments": ["@yilei, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @vrv and @martinwicke to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}]