[{"number": 17210, "title": "NotFoundError: Op type not registered 'KafkaDataset' in binary.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: - \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('unknown', '1.6.0-rc1')\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.8.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**:\r\n\r\n~~~python\r\nfrom tensorflow.contrib.kafka.python.ops import kafka_dataset_ops\r\nfrom tensorflow.python.data.ops import iterator_ops\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import array_ops\r\n\r\ntopics = array_ops.placeholder(dtypes.string, shape=[None])\r\nnum_epochs = array_ops.placeholder(dtypes.int64, shape=[])\r\nbatch_size = array_ops.placeholder(dtypes.int64, shape=[])\r\n\r\nrepeat_dataset = kafka_dataset_ops.KafkaDataset(\r\n    topics, group=\"test\", eof=True).repeat(num_epochs)\r\nbatch_dataset = repeat_dataset.batch(batch_size)\r\n\r\niterator = iterator_ops.Iterator.from_structure(batch_dataset.output_types)\r\ninit_op = iterator.make_initializer(repeat_dataset)\r\ninit_batch_op = iterator.make_initializer(batch_dataset)\r\nget_next = iterator.get_next()\r\n~~~\r\n\r\nENV: https://pastebin.com/89aihba7\r\n\r\n### Describe the problem\r\nGetting `NotFoundError` even with `TF_NEED_KAFKA=1` and `--define with_kafka_support=true`:\r\n~~~\r\nNotFoundError: Op type not registered 'KafkaDataset' in binary running on 68a9f992375e. Make sure the Op and Kernel are registered in the binary running in this process.\r\n~~~\r\n`KafkaDataset` was merged into master last month. Is there something missing that needs to be done in order to utilize the new op? \r\n\r\n\r\n### Source code / logs\r\nhttps://pastebin.com/5Vy6b6kf", "comments": ["IIUC, the PR you're referring to is https://github.com/tensorflow/tensorflow/pull/14098\r\n\r\nI might be missing something but it seems that the BUILD target for the kernels [`//tensorflow/contrib/kafka:kafka_kernels`](https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/contrib/kafka/BUILD#L15) isn't included as a dependency of any other target so the kernels are not being linked in. \r\n\r\n@yongtang @jhseu , could you take a look? Should there be a kernel library DSO that is created and loaded at build time? Or, if not, the kafka kernels library target needs to be linked in somewhere, right?", "@MtDersvan @asimshankar At the moment the Kafka Op could be optionally built with ./configure (specify `y`):\r\n```\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [y/N]: \r\n```\r\n\r\nIt is not part of the default configuration (only with customized build).\r\n\r\nThere were some issues with `.so` as the op requires linkage with core and bazel could not resolve the linkage.\r\n\r\nThat was the case a couple of months ago. I will take a look and see if it is possible to build a `.so` and load dynamically now.", "@yongtang If I understand correctly, `--define with_kafka_support=true` or `TF_NEED_KAFKA=1` is equivalent to specifying `y` in `Do you wish to build TensorFlow with Apache Kafka Platform support? [y/N]`, so it should be working.\r\nI don't think tensorflow picks up the cpp kernel properly. I tried adding the [kernel](https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/contrib/kafka/BUILD#L15) itself  to the contrib [BUILD](https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/contrib/BUILD) but to no success.", "@yongtang same issue.  bazel 0.11.0, python3, cuda 9.0, cudnn 7.0, TF_NEED_KAFKA=1 \r\n\r\nfresh build from a couple days ago:  4faee3942d9983e0c96091b32095cc0d9ff494e0\r\n\r\nNote:  i didn't set `--with_kafka_support=true`\r\n\r\n", "@MtDersvan @cfregly Thanks for the info. Was on the road and had to do some other stuff so wasn't able to reply in time. But let me assign the issue to myself. I will take a look in the next several days.", "Added a PR #17418 for the fix."]}, {"number": 17209, "title": "Expected float32, got range(0, 3) of type 'range' instead.", "body": "### System information\r\n- **What is the top-level directory of the model you are using**:\r\n      C:\\Users\\Administrator\\Documents\\Projects\\models\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n     Because my previous [issue,](https://github.com/tensorflow/tensorflow/issues/17208) I disable the argument.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\n     Windows 10\r\n- **TensorFlow installed from (source or binary)**:\r\n      Binary\r\n- **TensorFlow version (use command below)**:\r\n      1.5\r\n- **CUDA/cuDNN version**:\r\n     CUDA v9.0\r\n- **GPU model and memory**:\r\n      NVDIA GeForce GT 730\r\n\r\n### Describe the problem\r\nIf my description or log is not clear enough, please tell me. Thanks in advance!\r\n\r\n### Source code / logs\r\n  File \"C:/Users/Administrator/Documents/Projects/models/research/object_detection/my_train.py\", line 164, in main\r\n    worker_job_name, is_chief, FLAGS.train_dir)\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\object_detection-0.1-py3.5.egg\\object_detection\\trainer.py\", line 255, in train\r\n    train_config.optimizer)\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\object_detection-0.1-py3.5.egg\\object_detection\\builders\\optimizer_builder.py\", line 50, in build\r\n    learning_rate = _create_learning_rate(config.learning_rate)\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\object_detection-0.1-py3.5.egg\\object_detection\\builders\\optimizer_builder.py\", line 108, in _create_learning_rate\r\n    learning_rate_sequence)\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\object_detection-0.1-py3.5.egg\\object_detection\\utils\\learning_schedules.py\", line 155, in manual_stepping\r\n    tf.constant(range(num_boundaries), dtype=tf.int32),\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 214, in constant\r\n    value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 433, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 344, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected float32, got range(0, 3) of type 'range' instead.", "comments": ["That is a python issue since range is not a list.\r\nUse tf.constant([0,..., num_boundaries-1]) instead.", "For anyone else running into this: this is an issue with the Object Detection API in the [tensorflow/models](https://github.com/tensorflow/models) repository. I've created a [pull request with a simple fix](https://github.com/tensorflow/models/pull/3442)."]}, {"number": 17208, "title": "__init__() got an unexpected keyword argument 'dct_method'", "body": "### System information\r\n- **What is the top-level directory of the model you are using**:\r\n      C:\\Users\\Administrator\\Documents\\Projects\\models\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n     Just changed the filename of 'train.py' to 'my_train.py'.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\n     Windows 10\r\n- **TensorFlow installed from (source or binary)**:\r\n      Binary\r\n- **TensorFlow version (use command below)**:\r\n      1.5\r\n- **CUDA/cuDNN version**:\r\n     CUDA v9.0\r\n- **GPU model and memory**:\r\n      NVDIA GeForce GT 730\r\n\r\n### Describe the problem\r\nIs the new version have some bugs or my installation is incorrect? because I saw the source code of the 'Image' class of tf.example_decoder, it is no an argument named 'dec_method' in the 'init' method. If my description or log is not clear enough, please tell me. Thanks in advance!\r\n\r\n### Source code / logs\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\object_detection-0.1-py3.5.egg\\object_detection\\trainer.py\", line 59, in create_input_queue\r\n    tensor_dict = create_tensor_dict_fn()\r\n  File \"C:/Users/Administrator/Documents/Projects/models/research/object_detection/my_train.py\", line 120, in get_next\r\n    dataset_builder.build(config)).get_next()\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\object_detection-0.1-py3.5.egg\\object_detection\\builders\\dataset_builder.py\", line 138, in build\r\n    label_map_proto_file=label_map_proto_file)\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\object_detection-0.1-py3.5.egg\\object_detection\\data_decoders\\tf_example_decoder.py\", line 110, in __init__\r\n    dct_method=dct_method),\r\nTypeError: __init__() got an unexpected keyword argument 'dct_method'", "comments": ["How did you fix this issue? I'm getting the exact same one @zeshaoaaa ?\r\nI \"patched it\" by commenting as you did:\r\n1) tf_example_decoder.py\", line 110, commented the \"dct_method=dct_method),\"\r\n\r\nAnd afterwards I didi what @JeroenDelcour did in issue #17209, that is :\r\nresearch/object_detection/utils/learning_schedules.py --> Changed range to list\r\nhttps://github.com/tensorflow/models/pull/3442/files.\r\n\r\nIs there a complete fix on the way? Does someone know what this error is due to?\r\nDo we have to pull request this patch?", "@RDaneelOlivav \r\nI just clone the 'r1.5' branch of the 'models' repository and uninstall 'tensorflow-gpu' and install 'tensorflow 1.5' instead, after that, the problem disappeared. \r\nSeems it comes from the source code of 'tensorflow-gpu 1.6'. I don't know, if my response can't solve your problem, I suggest you open a new issue for that. ", "Hey there,\r\n\r\nRemove `dct_method=dct_method` in `object_detection/data_decoders/tf_example_decoder.py` around line 109. This fixed it for me. \r\n\r\nUsing TensorFlow 1.5 and `b634d42d` of `tensorflow/models` without a problem ", "Thanks a lot @jqcorreia "]}, {"number": 17207, "title": "TF Keras inference is way slower than Numpy", "body": "I'm working on a reinforcement learning model implemented with Keras and Tensorflow. I have to do frequent calls to model.predict() on single inputs.\r\n\r\nWhile testing inference on a simple pretrained model, I noticed that using Keras' model.predict is WAY slower than just using Numpy on stored weights. Why is it that slow and how can I accelerate it? Using pure Numpy is not viable for complex models.\r\n\r\n    import timeit\r\n    import numpy as np\r\n    from tensorflow.python.keras.models import Sequential\r\n    from tensorflow.python.keras.layers import Dense\r\n    \r\n    w = np.array([[-1., 1., 0., 0.], [0., 0., -1., 1.]]).T\r\n    b = np.array([ 15., -15., -21., 21.])\r\n    \r\n    model = Sequential()\r\n    model.add(Dense(4, input_dim=2, activation='linear'))\r\n    model.layers[0].set_weights([w.T, b])\r\n    model.compile(loss='mse', optimizer='adam')\r\n    \r\n    state = np.array([-23.5, 17.8])\r\n    \r\n    def predict_very_slow():\r\n        return model.predict(state[np.newaxis])[0]\r\n    \r\n    def predict_slow():\r\n        ws = model.layers[0].get_weights()\r\n        return np.matmul(ws[0].T, state) + ws[1]\r\n    \r\n    def predict_fast():\r\n        return np.matmul(w, state) + b\r\n    \r\n    print(\r\n        timeit.timeit(predict_very_slow, number=10000),\r\n        timeit.timeit(predict_slow, number=10000),\r\n        timeit.timeit(predict_fast, number=10000)\r\n    )\r\n    # 5.168972805004538 1.6963867129435828 0.021918574168087623\r\n    # 5.461319456664639 1.5491559107269515 0.021502970783442876\r\n\r\nI'm using Tensorflow for CPU, version 1.5.0 installed from pypi for python 3.5 on Windows 10.", "comments": ["If it's just matmul, why not use tf.matmul?\r\n\r\n\r\nimport timeit\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nw = np.array([[-1., 1., 0., 0.], [0., 0., -1., 1.]]).T\r\nb = np.array([[ 15., -15., -21., 21.]])\r\ns = np.array([[-23.5, 17.8]])\r\nu = tf.matmul(w, s, transpose_b=True) + b.T\r\nsess = tf.Session()\r\n\r\nrand_array = np.random.rand(size, size)\r\n\r\nstart_time = time.time()\r\nfor _ in xrange(10):\r\n  np.dot(np.dot(rand_array,rand_array), rand_array)\r\n\r\nprint(\"--- %s seconds numpy multiply ---\" % (time.time() - start_time))\r\n\r\nstart_time = time.time()\r\nfor _ in xrange(10):\r\n  sess.run(z, feed_dict={x: rand_array})\r\n\r\nprint(\"--- %s seconds tensorflow---\" % (time.time() - start_time))\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "It's just one matmul in this demonstration, in reality more complex models are used.\r\nYou can only feed one input/state at a time, because the next state depends on the result.\r\nI already asked this question on [StackOverflow](https://stackoverflow.com/questions/48796619/why-is-tf-keras-inference-way-slower-than-numpy-operations)."]}, {"number": 17206, "title": "CPU restrictions do not reduce thread count", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux c72 4.13.0-17-generic #20~16.04.1-Ubuntu SMP Mon Nov 6 14:18:00 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: b'v1.4.1-5-gabf3c6d' 1.4.1\r\n\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: please see below\r\n\r\n### Describe the problem\r\n\r\nIt seems that the tensorflow configuration proto lets you specify the number of CPUs TF should use, but this does not limit the default number of threads TF creates in its thread pools. This seems like unexpected behavior to me. The default number of threads created by TF when no options are specified at all is equal to the number of cores TF detects on the machine it's running on. However, even when limiting the number of CPUs the thread count goes up. On a machine with many CPUs, I expected limiting to work, but it did not, until I limited the thread pool size explicitly.\r\n\r\nThis isn't a big deal but seemed like it could be a bug, or at least makes the CPU device count option a bit unintuitive.\r\n\r\n```\r\n(cpuc-3.5) 18:21:27 vladf@c72:~$ python -c \"import tensorflow as tf\r\n> tf.Session(config=tf.ConfigProto())\r\n> import os\r\n> pid = os.getpid()\r\n> os.system('ps -o nlwp {}'.format(pid))\"\r\nNLWP\r\n 144\r\n(cpuc-3.5) 18:21:30 vladf@c72:~$ python -c \"import tensorflow as tf\r\n> tf.Session(config=tf.ConfigProto(device_count={'CPU': 4}))\r\n> import os\r\n> pid = os.getpid()\r\n> os.system('ps -o nlwp {}'.format(pid))\"\r\nNLWP\r\n 144\r\n(cpuc-3.5) 18:22:35 vladf@c72:~$ python -c \"import tensorflow as tf\r\n> tf.Session(config=tf.ConfigProto(device_count={'CPU': 4},inter_op_parallelism_threads=4,intra_op_parallelism_threads=4))\r\n> import os\r\n> pid = os.getpid()\r\n> os.system('ps -o nlwp {}'.format(pid))\"\r\nNLWP\r\n  56\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "You should leave device_count set to 1 for CPU, but if you want to limit the thread count you need to set intra_op_parallelism_threads as well as inter_op_parallelism_threads. You should probably move the discussion to stackoverflow if you need further advice about this, since github issues are for bug reports and feature requests."]}, {"number": 17205, "title": "Update TrainingSpec and EvalSpec pydoc", "body": "Bring TrainingSpec and EvalSpec pydoc in line with pydoc of estimator.train() and evaluate()", "comments": []}, {"number": 17204, "title": "HtoD takes 2.5x longer than D2H", "body": "I'm noticing that HtoD copies are taking significantly longer than DtoH.\r\n\r\nIE, doing `sess.run(gpu_var)` vs `sess.run(assign_gpu_var, feed_dict={gpu_var.initial_value: arr})`\r\n100MB tensor takes 8ms on V100 to fetch (12.5GB/sec), but 21.67ms to feed (4.5 GB/sec)\r\n \r\nBenchmark\r\nhttps://github.com/diux-dev/cluster/blob/db10c890530e7ded9e4a933596803e3ae0de1db0/yuxin_numpy/square_minimize_cpu_pipeline.py\r\n\r\nIs there a way to make it faster? (something to do with memory pinning?)\r\n\r\n<img width=\"1152\" alt=\"screenshot 2018-02-22 17 47 07\" src=\"https://user-images.githubusercontent.com/23068/36574018-6e853e74-17f8-11e8-80dd-7509e7c643a2.png\">\r\n\r\nTensorFlow:\r\nversion: 1.5.0\r\n__git_version__: v1.5.0-0-g37aa430d84\r\nhttps://github.com/tensorflow/tensorflow/commit/37aa430d84\r\n", "comments": ["cc @zheng-xq ", "https://wolfr.am/sEh9TuuR\r\n\r\n![screenshot 2018-02-23 10 44 24](https://user-images.githubusercontent.com/23068/36610893-9555b374-1886-11e8-86bb-15d0cac793d6.png)\r\n", "H2D seems to take expected time when source data lives in TensorFlow CPU variable rather than numpy array (8ms instead of 20ms), so I suspect the difference is due to memory pinning.\r\n\r\nI tried feeding page-locked numpy array created using PyCUDA ([code](https://github.com/diux-dev/cluster/blob/master/yuxin_numpy/d2h_benchmark_pycuda.py)) but it made no difference in speed. I wonder if TensorFlow is smart enough to check if incoming numpy arrays are page-locked already, or if memory->page-locked memory is happening regardless\r\n\r\n```\r\n  import pycuda.driver as drv\r\n  drv.init()\r\n  print(\"%d device(s) found.\" % drv.Device.count())\r\n  current_dev = drv.Device(0) #device we are working on\r\n  ctx = current_dev.make_context() #make a working context\r\n  ctx.push()\r\n  np_array = drv.pagelocked_zeros((args.dim,), dtype=np.float32)\r\n  ...\r\n  sess.run(params.initializer, feed_dict={params.initial_value:np_array})\r\n\r\n```", "TensorFlow has its heuristics to determine whether a host memory should be page locked or not. Variables or any op that will be sent to device automatically gets page locked memory.\r\n\r\nHowever, I don't think the heuristics treats memory feeds as page locked. For high-performance models, it is recommended to go through a reader-op, instead of feeding a large amount of data. ", "@zheng-xq the application here is setting network parameters rather than reading data. Parameters come from external application (Ray)  and the goal is to get those values into corresponding TensorFlow parameter variables as fast as possible (cc @mrry)", "OK, I suspect it actually recognizes numpy arrays to be page-locked. I tried fetching GPU tensor as numpy, writing my own data into that numpy array, and feeding it back, and it's about 1.8x faster than a regular 64-byte aligned numpy array\r\n\r\n[tf_numpy_benchmark.py](https://github.com/diux-dev/cluster/blob/41b318a5e5248b8e236c31c323be6d7777fa6133/yuxin_numpy/tf_numpy_benchmark.py)\r\n```\r\npython tf_numpy_benchmark.py --allocator=numpy --benchmark=feed_gpu_variable\r\nfeed_gpu_variable             :   1.5 GB/sec, min: 65.37, median: 70.27, mean: 69.33\r\n\r\npython tf_numpy_benchmark.py --allocator=tf --benchmark=feed_gpu_variable\r\nfeed_gpu_variable             :   5.4 GB/sec, min: 18.52, median: 20.10, mean: 20.23\r\n\r\npython tf_numpy_benchmark.py --allocator=tfgpu --benchmark=feed_gpu_variable\r\nfeed_gpu_variable             :  10.1 GB/sec, min:  9.89, median:  9.94, mean:  9.97\r\n\r\n\r\n```", "BTW, PyTorch doesn't seem to have this limitation, I can take a regular numpy array, and load it onto GPU as fast as numpy array in page-locked memory\r\n\r\n[benchmark v2](https://github.com/diux-dev/cluster/blob/c396df42b7d13387b3be767f18f59e012bfd0277/yuxin_numpy/tf_numpy_benchmark.py)\r\n\r\n```\r\npython tf_numpy_benchmark.py --allocator=tfgpu --benchmark=pytorchgpu_from_numpy\r\npytorch_from_numpy            :  11.1 GB/sec, min:  9.05, median:  9.09, mean:  9.09\r\n\r\npython tf_numpy_benchmark.py --allocator=numpy --benchmark=pytorchgpu_from_numpy\r\npytorch_from_numpy            :   7.8 GB/sec, min: 12.78, median: 12.85, mean: 12.87\r\n\r\n```", "To summarize, getting numpy array onto GPU memory is about 4x faster in PyTorch than in TensorFlow\r\n```\r\npython tf_numpy_benchmark.py --benchmark=pytorchgpu_from_numpy --allocator=numpy --num-iters=101\r\npytorch_from_numpy            :   8.1 GB/sec, min: 12.34, median: 12.52, mean: 12.53\r\n\r\npython tf_numpy_benchmark.py --benchmark=feed_gpu_tensor --allocator=numpy --num-iters=101\r\nfeed_gpu_tensor               :   1.8 GB/sec, min: 56.98, median: 68.01, mean: 67.06\r\n```", "TensorFlow H2D becomes much faster if I enable tcmalloc.\r\n```\r\nsudo apt-get install -y google-perftools\r\nexport LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\"\r\npython tf_numpy_benchmark.py --benchmark=feed_gpu_tensor --allocator=numpy --num-iters=51\r\nfeed_gpu_tensor               :   7.9 GB/sec, min: 12.73, median: 13.07, mean: 13.00\r\n```\r\n\r\nTheory -- something in TensorFlow is sensitive to memory properties (posix_memalign flags?), and triggers an additional CPU memory copy before calling cudaMemCopyAsync. On other hand, PyTorch is not sensitive to this, and calls `cudaMemCopyAsync` right away. Hard to check because enabling cpu profiler also makes the slowness disappear, classical heisenbug\r\n\r\n", "@mrry WDYT?", "Closing because this issue seems to be resolved by 64-byte aligning input data."]}, {"number": 17202, "title": "Branch 186662441", "body": "", "comments": ["@yifeif this seems to have failed twice in a row for MacOS. I am trying again.", "This is an infra issue, macos contrib passes, so we can go ahead and merge."]}, {"number": 17201, "title": "Tensorflow build incorrectly complained about Bazel version", "body": "When I built Tensorflow, it complained my bazel was 0.4.5, asked me to upgrade to bazel 0.5.4 or above.\r\nSo I upgraded to the newest bazel 0.10.1. Then when I built Tensorflow again, it still complained:\r\n\r\nCurrent Bazel version is 0.10.1, expected at least 0.5.4\r\n\r\nSo Tensorflow thinks 0.1 is less than 0.5, it did not treat that as version 10 v.s. 5.\r\nPlease fix.\r\n\r\nThank you!\r\nJan\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L35\r\nhas fixed the bug. Please sync to the latest version of tensorflow."]}, {"number": 17200, "title": "TimeDistributed wrapper works with \"standalone\" Keras, but not with \"built-in\" Keras", "body": "The following lines worked with Keras (2.0.8) and Tensorflow (pre 1.4).\r\n\r\n`input_tensor = Input((input_epochs_per_output_epoch, samples_per_epoch, features_per_epoch))`\r\n\r\n`inner_intermediate = TimeDistributed(BatchNormalization(axis = -1), name=\"bn_input\")(input_tensor)`\r\n\r\n`many lines...`\r\n\r\n`inner_out = TimeDistributed(Bidirectional(GRU(intermediate_features, return_sequences=False, recurrent_dropout=recurrent_dropout, dropout=normal_dropout, kernel_initializer=kernel_initializer, kernel_regularizer=l2(l2_lambda), implementation=lstm_implementation), merge_mode='concat'), name=\"inner_out\")(inner_intermediate)`\r\n\r\n\r\nHowever, with Tensorflow 1.5.0, using the Keras within Tensorflow, the final line fails with:\r\n> ValueError: as_list() is not defined on an unknown TensorShape.\r\n\r\nBecause there are a lot of lines between 2 and 3, which have various layers wrapped by TimeDistributed, I've figured out that it is only GRU and LSTM layers that cause this error. I've tried specifying the input_shape for both the the wrappers and/or the layers themselves, to no avail.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Missing questions and answers below:\r\nHave I written custom code: No, using standard keras Functional API\r\nOS Platform and Distribution: macOS 10.13 & Ubuntu 16.04\r\nTensorFlow installed from: macOS: pip & Ubuntu: conda\r\nTensorFlow version: 1.5.0\r\nBazel version: N/A (was not compiled)\r\nCUDA/cuDNN version: Ubuntu: CUDA 9\r\nGPU model and memory: Ubuntu: GeForce 1080 Ti\r\nExact command to reproduce: written in initial post", "**new finding**\r\nThe above lines work perfectly if I use the keras package (2.1.4) directly, instead of keras included in Tensorflow.", "@adamjones1 It looks like this has been fixed in the latest tensorflow nightly.", "Nagging Assignee @fchollet: It has been 104 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing as this is resolved"]}, {"number": 17199, "title": "Error raised incorrectly in CheckInputFromValidContext", "body": "Happens in TF 1.5.\r\n\r\n**Error message and logs:**\r\nINFO:tensorflow:Cannot use 'transducer_training/while/rnn/strided_slice' as input to 'gradients/transducer_training/while/rnn/while/Select_1_grad/Select/f_acc' because 'transducer_training/while/rnn/strided_slice' is in a while loop.\r\n\r\ngradients/transducer_training/while/rnn/while/Select_1_grad/Select/f_acc while context: None\r\ntransducer_training/while/rnn/strided_slice while context: transducer_training/while/while_context\r\n\r\nTraceback for gradients/transducer_training/while/rnn/while/Select_1_grad/Select/f_acc:\r\n  File \"./loop_error.py\", line 219, in <module>\r\n    model = Model(cons_manager=constants_manager)\r\n  File \"./loop_error.py\", line 44, in __init__\r\n    self.targets, self.train_op, self.loss = self.build_training_step()\r\n  File \"./loop_error.py\", line 211, in build_training_step\r\n    train_op = tf.train.AdamOptimizer().minimize(loss)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 355, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 456, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 375, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py\", line 949, in _SelectGrad\r\n    return (None, array_ops.where(c, grad, zeros),\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 2540, in where\r\n    return gen_math_ops._select(condition=condition, x=x, y=y, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 4043, in _select\r\n    \"Select\", condition=condition, t=x, e=y, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1674, in __init__\r\n    self._control_flow_context.AddOp(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2251, in AddOp\r\n    self._AddOpInternal(op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2274, in _AddOpInternal\r\n    real_x = self.AddValue(x)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2207, in AddValue\r\n    real_val = grad_ctxt.grad_state.GetRealValue(val)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1050, in GetRealValue\r\n    history_value = cur_grad_state.AddForwardAccumulator(cur_value)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 908, in AddForwardAccumulator\r\n    name=\"f_acc\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 3578, in _stack_v2\r\n    stack_name=stack_name, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nTraceback for transducer_training/while/rnn/strided_slice:\r\n  File \"./loop_error.py\", line 219, in <module>\r\n    model = Model(cons_manager=constants_manager)\r\n  File \"./loop_error.py\", line 42, in __init__\r\n    self.transducer_hidden_state_new, self.train_saver = self.build_full_transducer()\r\n  File \"./loop_error.py\", line 183, in build_full_transducer\r\n    tf.while_loop(cond, body, init_state, parallel_iterations=1)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2934, in while_loop\r\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2720, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2662, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"./loop_error.py\", line 119, in body\r\n    dtype=tf.float32, initial_state=encoder_hidden_state_t)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 629, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 688, in _dynamic_rnn_loop\r\n    time_steps = input_shape[0]\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 573, in _slice_helper\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 737, in strided_slice\r\n    shrink_axis_mask=shrink_axis_mask)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 5501, in strided_slice\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"./loop_error.py\", line 219, in <module>\r\n    model = Model(cons_manager=constants_manager)\r\n  File \"./loop_error.py\", line 44, in __init__\r\n    self.targets, self.train_op, self.loss = self.build_training_step()\r\n  File \"./loop_error.py\", line 211, in build_training_step\r\n    train_op = tf.train.AdamOptimizer().minimize(loss)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 355, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 456, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 375, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py\", line 949, in _SelectGrad\r\n    return (None, array_ops.where(c, grad, zeros),\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 2540, in where\r\n    return gen_math_ops._select(condition=condition, x=x, y=y, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 4043, in _select\r\n    \"Select\", condition=condition, t=x, e=y, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1674, in __init__\r\n    self._control_flow_context.AddOp(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2251, in AddOp\r\n    self._AddOpInternal(op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2274, in _AddOpInternal\r\n    real_x = self.AddValue(x)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2207, in AddValue\r\n    real_val = grad_ctxt.grad_state.GetRealValue(val)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1050, in GetRealValue\r\n    history_value = cur_grad_state.AddForwardAccumulator(cur_value)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 908, in AddForwardAccumulator\r\n    name=\"f_acc\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 3578, in _stack_v2\r\n    stack_name=stack_name, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1672, in __init__\r\n    control_flow_util.CheckInputFromValidContext(self, input_tensor.op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_util.py\", line 200, in CheckInputFromValidContext\r\n    raise ValueError(error_msg + \" See info log for more details.\")\r\nValueError: Cannot use 'transducer_training/while/rnn/strided_slice' as input to 'gradients/transducer_training/while/rnn/while/Select_1_grad/Select/f_acc' because 'transducer_training/while/rnn/strided_slice' is in a while loop. See info log for more details.\r\n\r\n\r\n\r\n\r\n**Code to reproduce:**\r\n\r\n\r\n    import logging\r\n    import tensorflow as tf\r\n    from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple\r\n    from tensorflow.python.layers import core as layers_core\r\n    \r\n    logging.getLogger().setLevel(logging.DEBUG)\r\n    # NOTE: Time major\r\n    \r\n    # ---------------- Constants Manager ----------------------------\r\n    class ConstantsManager(object):\r\n        def __init__(self, input_dimensions, input_embedding_size, inputs_embedded, encoder_hidden_units,\r\n                     transducer_hidden_units, vocab_ids, input_block_size, beam_width):\r\n            assert transducer_hidden_units == encoder_hidden_units, 'Encoder and transducer have to have the same amount' \\\r\n                                                                    'of hidden units'\r\n            self.input_dimensions = input_dimensions\r\n            self.vocab_ids = vocab_ids\r\n            self.E_SYMBOL = len(self.vocab_ids)\r\n            self.vocab_ids.append('E_SYMBOL')\r\n            self.GO_SYMBOL = len(self.vocab_ids)\r\n            self.vocab_ids.append('GO_SYMBOL')\r\n            self.vocab_size = len(self.vocab_ids)\r\n            self.input_embedding_size = input_embedding_size\r\n            self.inputs_embedded = inputs_embedded\r\n            self.encoder_hidden_units = encoder_hidden_units\r\n            self.transducer_hidden_units = transducer_hidden_units\r\n            self.input_block_size = input_block_size\r\n            self.beam_width = beam_width\r\n            self.batch_size = 1  # Cannot be increased, see paper\r\n            self.log_prob_init_value = 0\r\n    \r\n    # ----------------- Model ---------------------------------------\r\n    \r\n    \r\n    class Model(object):\r\n        def __init__(self, cons_manager):\r\n            self.var_list = []\r\n            self.cons_manager = cons_manager\r\n            self.max_blocks, self.inputs_full_raw, self.transducer_list_outputs, self.start_block, self.encoder_hidden_init,\\\r\n                self.trans_hidden_init, self.logits, self.encoder_hidden_state_new, \\\r\n                self.transducer_hidden_state_new, self.train_saver = self.build_full_transducer()\r\n    \r\n            self.targets, self.train_op, self.loss = self.build_training_step()\r\n    \r\n        def build_full_transducer(self):\r\n            with tf.variable_scope('transducer_training'):\r\n    \r\n                embeddings = tf.Variable(tf.random_uniform([self.cons_manager.vocab_size,\r\n                                                            self.cons_manager.input_embedding_size], -1.0, 1.0),\r\n                                         dtype=tf.float32,\r\n                                         name='embedding')\r\n                # Inputs\r\n                max_blocks = tf.placeholder(dtype=tf.int32, name='max_blocks')  # total amount of blocks to go through\r\n                if self.cons_manager.inputs_embedded is True:\r\n                    input_type = tf.float32\r\n                else:\r\n                    input_type = tf.int32\r\n                inputs_full_raw = tf.placeholder(shape=(None, self.cons_manager.batch_size,\r\n                                                        self.cons_manager.input_dimensions), dtype=input_type,\r\n                                                 name='inputs_full_raw')  # shape [max_time, 1, input_dims]\r\n                transducer_list_outputs = tf.placeholder(shape=(None,), dtype=tf.int32,\r\n                                                         name='transducer_list_outputs')  # amount to output per block\r\n                start_block = tf.placeholder(dtype=tf.int32, name='transducer_start_block')  # where to start the input\r\n    \r\n                encoder_hidden_init = tf.placeholder(shape=(2, 1, self.cons_manager.encoder_hidden_units), dtype=tf.float32,\r\n                                                     name='encoder_hidden_init')\r\n                trans_hidden_init = tf.placeholder(shape=(2, 1, self.cons_manager.transducer_hidden_units), dtype=tf.float32,\r\n                                                   name='trans_hidden_init')\r\n    \r\n                # Temporary constants, maybe changed during inference\r\n                end_symbol = tf.get_variable(name='end_symbol',\r\n                                             initializer=tf.constant_initializer(self.cons_manager.vocab_size),\r\n                                             shape=(), dtype=tf.int32)\r\n    \r\n                # Turn inputs into tensor which is easily readable#\r\n    \r\n                inputs_full = tf.reshape(inputs_full_raw, shape=[-1, self.cons_manager.input_block_size,\r\n                                                                 self.cons_manager.batch_size,\r\n                                                                 self.cons_manager.input_dimensions])\r\n    \r\n                # Outputs\r\n                outputs_ta = tf.TensorArray(dtype=tf.float32, size=max_blocks)\r\n    \r\n                init_state = (start_block, outputs_ta, encoder_hidden_init, trans_hidden_init)\r\n    \r\n                # Initiate cells, NOTE: if there is a future error, put these back inside the body function\r\n                encoder_cell = tf.contrib.rnn.LSTMCell(num_units=self.cons_manager.encoder_hidden_units)\r\n                transducer_cell = tf.contrib.rnn.LSTMCell(self.cons_manager.transducer_hidden_units)\r\n    \r\n                def cond(current_block, outputs_int, encoder_hidden, trans_hidden):\r\n                    return current_block < start_block + max_blocks\r\n    \r\n                def body(current_block, outputs_int, encoder_hidden, trans_hidden):\r\n    \r\n                    # --------------------- ENCODER ----------------------------------------------------------------------\r\n                    encoder_inputs = inputs_full[current_block]\r\n                    encoder_inputs_length = [tf.shape(encoder_inputs)[0]]\r\n                    encoder_hidden_state = encoder_hidden\r\n    \r\n                    if self.cons_manager.inputs_embedded is True:\r\n                        encoder_inputs_embedded = encoder_inputs\r\n                    else:\r\n                        encoder_inputs = tf.reshape(encoder_inputs, shape=[-1, self.cons_manager.batch_size])\r\n                        encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\r\n    \r\n                    # Build model\r\n    \r\n                    # Build previous state\r\n                    encoder_hidden_c, encoder_hidden_h = tf.split(encoder_hidden_state, num_or_size_splits=2, axis=0)\r\n                    encoder_hidden_c = tf.reshape(encoder_hidden_c, shape=[-1, self.cons_manager.encoder_hidden_units])\r\n                    encoder_hidden_h = tf.reshape(encoder_hidden_h, shape=[-1, self.cons_manager.encoder_hidden_units])\r\n                    encoder_hidden_state_t = LSTMStateTuple(encoder_hidden_c, encoder_hidden_h)\r\n    \r\n                    #   encoder_outputs: [max_time, batch_size, num_units]\r\n                    encoder_outputs, encoder_hidden_state_new = tf.nn.dynamic_rnn(\r\n                        encoder_cell, encoder_inputs_embedded,\r\n                        sequence_length=encoder_inputs_length, time_major=True,\r\n                        dtype=tf.float32, initial_state=encoder_hidden_state_t)\r\n    \r\n                    # Modify output of encoder_hidden_state_new so that it can be fed back in again without problems.\r\n                    encoder_hidden_state_new = tf.concat([encoder_hidden_state_new.c, encoder_hidden_state_new.h], axis=0)\r\n                    encoder_hidden_state_new = tf.reshape(encoder_hidden_state_new,\r\n                                                          shape=[2, -1, self.cons_manager.encoder_hidden_units])\r\n    \r\n                    # --------------------- TRANSDUCER --------------------------------------------------------------------\r\n                    encoder_raw_outputs = encoder_outputs\r\n                    # Save/load the state as one tensor, use encoder state as init if this is the first block\r\n                    trans_hidden_state = tf.cond(current_block > 0, lambda: trans_hidden, lambda: encoder_hidden_state_new)\r\n                    transducer_amount_outputs = transducer_list_outputs[current_block - start_block]\r\n    \r\n                    # Model building\r\n                    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\r\n                        embedding=embeddings,\r\n                        start_tokens=tf.tile([self.cons_manager.GO_SYMBOL],\r\n                                             [self.cons_manager.batch_size]),  # TODO: check if this looks good\r\n                        end_token=end_symbol)  # vocab size, so that it doesn't prematurely end the decoding\r\n    \r\n                    attention_states = tf.transpose(encoder_raw_outputs,\r\n                                                    [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]\r\n    \r\n                    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n                        self.cons_manager.encoder_hidden_units, attention_states)\r\n    \r\n                    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n                        transducer_cell,\r\n                        attention_mechanism,\r\n                        attention_layer_size=self.cons_manager.transducer_hidden_units)\r\n    \r\n                    projection_layer = layers_core.Dense(self.cons_manager.vocab_size, use_bias=False)\r\n    \r\n                    # Build previous state\r\n                    trans_hidden_c, trans_hidden_h = tf.split(trans_hidden_state, num_or_size_splits=2, axis=0)\r\n                    trans_hidden_c = tf.reshape(trans_hidden_c, shape=[-1, self.cons_manager.transducer_hidden_units])\r\n                    trans_hidden_h = tf.reshape(trans_hidden_h, shape=[-1, self.cons_manager.transducer_hidden_units])\r\n                    trans_hidden_state_t = LSTMStateTuple(trans_hidden_c, trans_hidden_h)\r\n    \r\n                    decoder = tf.contrib.seq2seq.BasicDecoder(\r\n                        decoder_cell, helper,\r\n                        decoder_cell.zero_state(1, tf.float32).clone(cell_state=trans_hidden_state_t),\r\n                        output_layer=projection_layer)\r\n    \r\n                    outputs, transducer_hidden_state_new, _ = tf.contrib.seq2seq.dynamic_decode(decoder,\r\n                                                                                                output_time_major=True,\r\n                                                                                                maximum_iterations=transducer_amount_outputs)\r\n                    logits = outputs.rnn_output  # logits of shape [max_time,batch_size,vocab_size]\r\n                    decoder_prediction = outputs.sample_id  # For debugging\r\n    \r\n                    # Modify output of transducer_hidden_state_new so that it can be fed back in again without problems.\r\n                    transducer_hidden_state_new = tf.concat(\r\n                        [transducer_hidden_state_new[0].c, transducer_hidden_state_new[0].h],\r\n                        axis=0)\r\n                    transducer_hidden_state_new = tf.reshape(transducer_hidden_state_new,\r\n                                                             shape=[2, -1, self.cons_manager.transducer_hidden_units])\r\n    \r\n    \r\n                    # Note the outputs\r\n                    outputs_int = outputs_int.write(current_block - start_block, logits)\r\n    \r\n                    return current_block + 1, outputs_int, encoder_hidden_state_new, transducer_hidden_state_new\r\n    \r\n                _, outputs_final, encoder_hidden_state_new, transducer_hidden_state_new = \\\r\n                    tf.while_loop(cond, body, init_state, parallel_iterations=1)\r\n    \r\n                # Process outputs\r\n                outputs = outputs_final.concat()\r\n                logits = tf.reshape(\r\n                    outputs,\r\n                    shape=(-1, 1, self.cons_manager.vocab_size))  # And now its [max_output_time, batch_size, vocab]\r\n    \r\n                # For loading the model later on\r\n                logits = tf.identity(logits, name='logits')\r\n                encoder_hidden_state_new = tf.identity(encoder_hidden_state_new, name='encoder_hidden_state_new')\r\n                transducer_hidden_state_new = tf.identity(transducer_hidden_state_new, name='transducer_hidden_state_new')\r\n    \r\n            train_saver = tf.train.Saver()  # For now save everything\r\n    \r\n            return max_blocks, inputs_full_raw, transducer_list_outputs, start_block, encoder_hidden_init,\\\r\n                trans_hidden_init, logits, encoder_hidden_state_new, transducer_hidden_state_new, train_saver\r\n    \r\n        def build_training_step(self):\r\n            targets = tf.placeholder(shape=(None,), dtype=tf.int32, name='targets')\r\n            targets_one_hot = tf.one_hot(targets, depth=self.cons_manager.vocab_size, dtype=tf.float32)\r\n    \r\n            targets_one_hot = tf.Print(targets_one_hot, [targets], message='Targets: ', summarize=10)\r\n            targets_one_hot = tf.Print(targets_one_hot, [tf.argmax(self.logits, axis=2)], message='Argmax: ', summarize=10)\r\n    \r\n            stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=targets_one_hot,\r\n                                                                             logits=self.logits)\r\n            loss = tf.reduce_mean(stepwise_cross_entropy)\r\n    \r\n            # ERROR happens in this line\r\n            train_op = tf.train.AdamOptimizer().minimize(loss)\r\n            train_op = None\r\n            return targets, train_op, loss\r\n    \r\n    \r\n    constants_manager = ConstantsManager(input_dimensions=1, input_embedding_size=11, inputs_embedded=False,\r\n                                         encoder_hidden_units=100, transducer_hidden_units=100, vocab_ids=[0, 1, 2],\r\n                                         input_block_size=1, beam_width=5)\r\n    model = Model(cons_manager=constants_manager)\r\n    \r\n    with tf.Session() as sess:\r\n        writer = tf.summary.FileWriter(\"/tmp/graph\", sess.graph_def)\r\n        sess.run(tf.global_variables_initializer())\r\n        writer.flush()\r\n        writer.close()\r\n\r\n\r\n\r\n**Picture of the graph and offending tensor (in red)**\r\n\r\nThe `rnn` box is in a while loop itself.\r\n\r\n![graph](https://user-images.githubusercontent.com/9123400/36559487-0d147680-17c3-11e8-9ae9-d2dca5950579.jpeg)\r\n", "comments": ["Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Any new information? It's quite cumbersome working only in TF 1.4.1", "Unfortunately no, there are a number of open control flow bugs and I haven't got to this one yet. You can try running at head, it's possible this has already been fixed.", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This error is gone with 1.7!", "Awesome, thanks for confirming!", "How did you remove this error. I am facing a similar error\r\n@skye", " File \"C:\\Users\\DELL\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1338, in __init__\r\n    self.build()\r\n  File \"C:\\Users\\DELL\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1347, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"C:\\Users\\DELL\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1384, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"C:\\Users\\DELL\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 832, in _build_internal\r\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\r\n  File \"C:\\Users\\DELL\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 350, in _AddSaveOps\r\n    save = self.save_op(filename_tensor, saveables)\r\n  File \"C:\\Users\\DELL\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 266, in save_op\r\n    tensors)\r\n  File \"C:\\Users\\DELL\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1800, in save_v2\r\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\r\n  File \"C:\\Users\\DELL\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\DELL\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\DELL\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nUnknownError (see above for traceback): Failed to rename: out/heart_disease.chkp.index.tempstate11972684499671613654 to: out/heart_disease.chkp.index : Access is denied.\r\n; Input/output error\r\n[[Node: save_4/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_4/Const_0_0, save_4/SaveV2/tensor_names, save_4/SaveV2/shape_and_slices, Adam/beta_1, Adam/beta_2, Adam/decay, Adam/iterations, Adam/lr, Adam_1/beta_1, Adam_1/beta_2, Adam_1/decay, Adam_1/iterations, Adam_1/lr, Adam_2/beta_1, Adam_2/beta_2, Adam_2/decay, Adam_2/iterations, Adam_2/lr, dense_1/bias, dense_1/kernel, dense_2/bias, dense_2/kernel, dense_3/bias, dense_3/kernel, dense_4/bias, dense_4/kernel, dense_5/bias, dense_5/kernel, dense_6/bias, dense_6/kernel, dense_7/bias, dense_7/kernel, dense_8/bias, dense_8/kernel, dense_9/bias, dense_9/kernel, training/Adam/Variable, training/Adam/Variable_1, training/Adam/Variable_10, training/Adam/Variable_11, training/Adam/Variable_12, training/Adam/Variable_13, training/Adam/Variable_14, training/Adam/Variable_15, training/Adam/Variable_16, training/Adam/Variable_17, training/Adam/Variable_2, training/Adam/Variable_3, training/Adam/Variable_4, training/Adam/Variable_5, training/Adam/Variable_6, training/Adam/Variable_7, training/Adam/Variable_8, training/Adam/Variable_9, training_1/Adam/Variable, training_1/Adam/Variable_1, training_1/Adam/Variable_10, training_1/Adam/Variable_11, training_1/Adam/Variable_12, training_1/Adam/Variable_13, training_1/Adam/Variable_14, training_1/Adam/Variable_15, training_1/Adam/Variable_16, training_1/Adam/Variable_17, training_1/Adam/Variable_2, training_1/Adam/Variable_3, training_1/Adam/Variable_4, training_1/Adam/Variable_5, training_1/Adam/Variable_6, training_1/Adam/Variable_7, training_1/Adam/Variable_8, training_1/Adam/Variable_9, training_2/Adam/Variable, training_2/Adam/Variable_1, training_2/Adam/Variable_10, training_2/Adam/Variable_11, training_2/Adam/Variable_12, training_2/Adam/Variable_13, training_2/Adam/Variable_14, training_2/Adam/Variable_15, training_2/Adam/Variable_16, training_2/Adam/Variable_17, training_2/Adam/Variable_2, training_2/Adam/Variable_3, training_2/Adam/Variable_4, training_2/Adam/Variable_5, training_2/Adam/Variable_6, training_2/Adam/Variable_7, training_2/Adam/Variable_8, training_2/Adam/Variable_9)]] ", "The error is gone when you update to tf version 1.7 or above for me", "Actually when I check tf.__version__ it gives me 1.8.0 as version", "I'm not sure that this is the same error. It looks like you have an \"Access is denied.\" error in there, I think the folder where you're saving the checkpoints doesn't have the correct access rights.", "how to change the folder\n\nOn 13 June 2018 at 13:50, nikita68 <notifications@github.com> wrote:\n\n> I'm not sure that this is the same error. It looks like you have an\n> \"Access is denied.\" error in there, I think the folder where you're saving\n> the checkpoints doesn't have the correct access rights.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17199#issuecomment-396854490>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AmXFiA8CpLHsk2JnuoeEzCew7oG7DwQWks5t8MtigaJpZM4SP2Zs>\n> .\n>\n"]}, {"number": 17198, "title": "Extract kernel, bias values of a layer", "body": "This function will extract the kernel, and bias values of given tensor-flow model graph layer.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "`I signed it!`", "CLAs look good, thanks!\n\n<!-- ok -->", "Re @kaushik1094 does the PR look good to you?", "@protoget Yes, It looks good for me.", "Nagging Assignee @protoget: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 86 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 101 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 116 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 17197, "title": "Merge pull request #1 from tensorflow/master", "body": "Jan31_pull", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->"]}, {"number": 17196, "title": "Fix doc format for `Sequential`", "body": "see: https://www.tensorflow.org/versions/master/api_docs/python/tf/keras/Sequential\r\n\r\nThe # Note is converted to an H1 in markdown.\r\nThe 4-space indent is interpreted as pre-formatted text.", "comments": []}, {"number": 17195, "title": "C++ gradients for MaxPool3D, AvgPool, AvgPool3D", "body": "Looks like these just need to connect the gradient to an existing core op. Anyone already working on it? Otherwise, I'll sign up for these three.\r\n\r\n/cc @bpiel @suharshs", "comments": ["@kbsriram I'm not. thanks", "Nor am I. Thanks!", "Can you please provide more details? AvgPoolingGradOp already exists.\r\n", "@bignamehyp This is referring to C++ gradients: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/cc/gradients\r\n"]}, {"number": 17194, "title": "Add dict(features) instead of features in beginners guide", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "cla agreement", "CLAs look good, thanks!\n\n<!-- ok -->", "Anjing kau\n\nPada tanggal 22 Feb 2018 5.21 PM, \"kamilogerto2\" <notifications@github.com>\nmenulis:\n\n> cla agreement\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/17194#issuecomment-367634577>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Ail_EhU_xqvSwCZRGpQ04WQzw-yTX8_Dks5tXT--gaJpZM4SPDUY>\n> .\n>\n", "Stop send Gmail for me anjeng\n\nPada tanggal 22 Feb 2018 5.22 PM, \"Iful Mirwanda\" <kursiwayang@gmail.com>\nmenulis:\n\n> Anjing kau\n>\n> Pada tanggal 22 Feb 2018 5.21 PM, \"kamilogerto2\" <notifications@github.com>\n> menulis:\n>\n>> cla agreement\n>>\n>> \u2014\n>> You are receiving this because you are subscribed to this thread.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/pull/17194#issuecomment-367634577>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/Ail_EhU_xqvSwCZRGpQ04WQzw-yTX8_Dks5tXT--gaJpZM4SPDUY>\n>> .\n>>\n>\n", "Nagging Assignee @protoget: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 17193, "title": "Fix typo", "body": "It should be \"host C compiler\", not \"hostC compiler\". Compare `set_host_cxx_compiler`.", "comments": []}, {"number": 17192, "title": "TrainingHelper & ScheduledEmbeddingTrainingHelper  GPU Error - works on CPU", "body": "### System information\r\n- **Have I written custom code **: Yes\r\n- **OS Platform and Distribution**:16.04.3 LTS (Xenial Xerus)\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.4.1\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**:NA\r\n- **GCC/Compiler version (if compiling from source)**:c++ (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609\r\n- **CUDA/cuDNN version**:CUDA 8.0 / 6.0\r\n- **GPU model and memory**:GeForce GTX 1080, 8G RAM, NVIDIA DRIVER: 390.25\r\n- **Exact command to reproduce**:NA\r\n\r\n###  problem\r\nHaving trouble running an RNN dynamic decoder on GPU when using the following  decoding helpers:\r\ntf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper \r\ntf.contrib.seq2seq.TrainingHelper \r\n\r\nIt seems that there is a problem with the sequence length stopping condition, as the problem\r\nis not present when using  tf.contrib.seq2seq.GreedyEmbeddingHelper as a decoder helper.\r\n\r\nOn CPU all functions are working correctly, the problem arise on GPUs only.\r\nTried on TF-1.4 compiled from source, TF-1.4 installed from pip, and TF-1.5 installed from source.\r\nThey all fail\r\n\r\n### Error log:\r\n```\r\nNotFoundError (see above for traceback): No registered 'Switch' OpKernel for GPU devices compatible with node Decoders/StackedDecoder_text_rnn_0/decoder/TrainingHelperInitialize/cond/TensorArrayReadV3/Switch = Switch[T=DT_RESOURCE, _class=[\"loc:@Decoders/StackedDecoder_text_rnn_0/ScheduledEmbeddingSamplingWrapper/TrainingHelper/TensorArray\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Decoders/StackedDecoder_text_rnn_0/ScheduledEmbeddingSamplingWrapper/TrainingHelper/TensorArray/_281, Decoders/StackedDecoder_text_rnn_0/decoder/TrainingHelperInitialize/All)\r\n\t (OpKernel was found, but attributes didn't match)\r\n\t.  Registered:  device='GPU'; T in [DT_STRING]\r\n  device='GPU'; T in [DT_BOOL]\r\n  device='GPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_COMPLEX128]\r\n  device='GPU'; T in [DT_COMPLEX64]\r\n  device='GPU'; T in [DT_INT8]\r\n  device='GPU'; T in [DT_UINT8]\r\n  device='GPU'; T in [DT_INT16]\r\n  device='GPU'; T in [DT_UINT16]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_QINT32]\r\n  device='CPU'; T in [DT_QUINT8]\r\n  device='CPU'; T in [DT_QINT8]\r\n  device='CPU'; T in [DT_RESOURCE]\r\n  device='CPU'; T in [DT_STRING]\r\n  device='CPU'; T in [DT_BOOL]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_INT8]\r\n  device='CPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_UINT16]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]\r\n\t [[Node: Decoders/StackedDecoder_text_rnn_0/decoder/TrainingHelperInitialize/cond/TensorArrayReadV3/Switch = Switch[T=DT_RESOURCE, _class=[\"loc:@Decoders/StackedDecoder_text_rnn_0/ScheduledEmbeddingSamplingWrapper/TrainingHelper/TensorArray\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Decoders/StackedDecoder_text_rnn_0/ScheduledEmbeddingSamplingWrapper/TrainingHelper/TensorArray/_281, Decoders/StackedDecoder_text_rnn_0/decoder/TrainingHelperInitialize/All)]]\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Any resolution to this issue?", "@keotic Can you provide some code to help replicate this?\r\n\r\n@ebrevdo Can you comment on this?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Sounds like we need a host-only switch for GPU.\r\n\r\nHave you tried running with soft placement on?", "Yes soft placement is on.", "@keotic are you able to provide a script to reproduce this error?", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 31 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 17191, "title": "ValueError: Unknown activation function:relu6  while converting MobileNet under Keras to estimator using model_to_estimator", "body": "Hi, I try to convert a mobilenet model under `tf.keras.application` to estimator using model_to_estimator.\r\nI get an error (`ValueError: Unknown activation function:relu6`) due to relu6 is a customized activation defined in mobilenet.\r\n\r\nThanks for help.\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.6r\r\n- **Python version**: \r\n3.5\r\n- **Bazel version (if compiling from source)**:\r\n0.10.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\nGCC\r\n- **CUDA/cuDNN version**:\r\n9.0\r\n- **GPU model and memory**:\r\nGTX 1070\r\n- **Have I written custom code**: \r\nNo\r\n- **Exact command to reproduce**:\r\n```python\r\nimport tensorflow as tf\r\nkeras_mobilenet= tf.keras.applications.mobilenet.MobileNet(weights=None)\r\nkeras_mobilenet.compile(optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),\r\n                          loss='categorical_crossentropy',\r\n                          metric='accuracy')\r\nmobilenet_estimator = tf.keras.estimator.model_to_estimator(keras_model=keras_mobilenet)\r\n```\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nExact command to reproduce", "Thanks for your reply. I have just updated the post based on the issue template.", "Please take a look at this related [issue](https://github.com/keras-team/keras/issues/7431) in Keras.", "@brainnoise Thanks for your help. The following code works for my case, although I do expect a more user-friendly solution.\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras import backend as K\r\nfrom tensorflow.python.keras.utils import CustomObjectScope\r\ndef relu6(x):\r\n  return K.relu(x, max_value=6)\r\nwith CustomObjectScope({'relu6': relu6}):\r\n    keras_mobilenet= tf.keras.applications.mobilenet.MobileNet(weights=None)\r\n    keras_mobilenet.compile(optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),\r\n                          loss='categorical_crossentropy',\r\n                          metric='accuracy')\r\n    mobilenet_estimator = tf.keras.estimator.model_to_estimator(keras_model=keras_mobilenet)\r\n```", "If none of them worked (like my case) use exactly Keras 2.1.6 version, import relu6 from \"keras.applicatiions.mobilenet\". then use CustomObjectScope to load the model.\r\n ", "On tensorflow `1.12.0`, using \r\n\r\n```\r\n    tflite_model = tf.contrib.lite.TFLiteConverter.from_keras_model_file('tflite-models/keras_model_converted.h5').convert()\r\n    with open('tflite-models/model.tflite', 'wb') as f:\r\n        f.write(tflite_model)\r\n```\r\n\r\nI get:\r\n\r\n`ValueError: Unknown activation function:relu6`\r\n\r\nUsing hack above, run without error, but I wonder why hack for `DepthwiseConv2D` not needed?\r\n\r\n```\r\n    from tensorflow.python.keras.utils import CustomObjectScope\r\n    def relu6(x):\r\n        return K.relu(x, max_value=6)\r\n    with CustomObjectScope({'relu6': relu6}):\r\n        tflite_model = tf.contrib.lite.TFLiteConverter.from_keras_model_file('tflite-models/keras_model_converted.h5').convert()\r\n        with open('tflite-models/model.tflite', 'wb') as f:\r\n            f.write(tflite_model)\r\n```\r\n\r\n\r\n", "I think in this case using tensorflow stuff here makes problem! (at least I tried and got error). ", "@SofaPotatos  Thx. Your code works for me.", "Hi everyone, \r\n\r\nI think this is fixed now, old versions of keras.applications has a custom [activation function there](https://github.com/keras-team/keras-applications/blob/1.0.1/keras_applications/mobilenet.py#L77).\r\n\r\nBut now [the code](https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py#L361) uses [layers.ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU) with a max-value argument. \r\n\r\n"]}, {"number": 17190, "title": "tf.contrib.quantize: layer not quantized in absence of activation", "body": "In absence of an activation function, e.g., `tf.layers.Conv2D(...,activation=None,...)`, the graph matcher using the `activation_pattern` at\r\nhttps://github.com/tensorflow/tensorflow/blob/671baf080238025da9698ea980cd9504005f727c/tensorflow/contrib/quantize/python/quantize.py#L185-L192\r\nwon't match the layer that has no activation function, and consequently, that layer won't be quantized. \r\n\r\nMaybe previously, instead of no activation function, `tf.identity` was used? The package `tf.contrib.quantize` searches for the identity op.\r\nhttps://github.com/tensorflow/tensorflow/blob/671baf080238025da9698ea980cd9504005f727c/tensorflow/contrib/quantize/python/quantize.py#L35\r\n\r\nNote that if `activation = None`, then no operation is inserted:\r\nhttps://github.com/tensorflow/tensorflow/blob/671baf080238025da9698ea980cd9504005f727c/tensorflow/python/layers/convolutional.py#L192-L193\r\n\r\nExample: Box predictors in SSD from the TF Object Detection API.\r\nhttps://github.com/tensorflow/models/blob/be9b80251af1bc798553c9e5135f8b0f19fa0a81/research/object_detection/core/box_predictor.py#L688-L689\r\n\r\n@suharshs ", "comments": ["I submitted a fix for this a couple days ago :) It should work after the next sync.\r\n\r\nAlso, these rewrites are very new and only have been tried on a few models, but we plan to expand this to more models, so any information on models you are trying to quantize would be super helpful!\r\n\r\nThanks!", "Thanks for your great work.\r\n\r\nI am trying to quantize MobileNet SSD for detection on a custom dataset, for later use in an FPGA implementation. As a workaround, I modified the code to look for the Reshape node after the box predictors as \"activation node\", and if found, quantize the BiasAdd.\r\n\r\nI took a look on your PR, but didn't test it yet. I believe that your changes will match all BiasAdd patterns, even those ending in an activation. Won't that lead to double quantization, before and after the e.g. ReLU6, since a layer with activation would yield two matches? Again, not tested yet.", "The detail is that the function that finds matches only `yield`s them. So the final layer match (the one checking for BiasAdd with no activation) only runs after the FakeQuant nodes are added to the other matches. The FakeQuant nodes being added make the second pattern match not double match. Does that make sense?", "Ah okay, I see how it works. You modify the graph inbetween the matching steps, thus the inserted FakeQuant ops prevent the double matching. \r\n\r\nBtw, did you try to quantize MobileNet SSD by any chance? ", "@DominikAuras  \r\nhi ,i also want to  quantize MobileNet SSD for detection on a custom dataset, for later use in an FPGA implementation , could you share how to do it?"]}, {"number": 17189, "title": "Implementation of Unpooling operator", "body": "As discussed in #2169 ", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Re @rmlarsen  friendly pinging about cide review.", "@protoget @nio1814 sorry about the delay. I'll review this in more detail on Monday.", "Updated the documentation.", "The PR had no recent activity partly probably because I haven't reviewed. Will try to review soon. Sorry about that. Removing the `stalled` label for now.", "Again, will try to review soon. So sorry for my delay! ", "It has been 32 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 61 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 17188, "title": "TF1.3 to 1.5 drastic changes (apparently) due to gRPC", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.1.1503 (Core)\r\n- **TensorFlow installed from (source or binary)**: pip install\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**: 2.7.15(with TF 1.5) and 2.7.5(with TF1.3)\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nI built a distributed training system with TF1.3 and it works smoothly except that it sporadically freeze. I was informed by @mrry that TF1.5 has an upgrade version of gRPC that fixes a deadlock bug that causes servers to hang.\r\n\r\nSo I tried to upgrade to 1.5 today but then a lot of unexpected problems came up and not sure if I am missing anything important for 1.5 version. \r\n\r\nTL;DR\r\nSoon after I launch a job, it died right away and all of the time it is the PS showing various errors like \"unavailable: OS Error\", \"Stream removed\" and \"Transport closed\" during sess.run(tf.global_variables_initializer). Also this problem is more likely to happen if I launched more PS.\r\n\r\nSo, is there any important dependencies I have to check when upgrading from 1.3 to 1.5?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17187, "title": "TensorFlow - cuInit: CUDA_ERROR_NO_DEVICE", "body": "Hi, I was using TensorFlow with GPU support these past few months and it worked without any issues. I have installed cuda v8.0 and also have the cudNN library.  Recently, I started using TensorFlow for a project and noticed that it isn't computing on the GPU, and is using the GPU instead. I'm running TensorFlow 1.2.1 on Windows 10, with CUDA v8.0.\r\n\r\nHere is the code that I ran on the Python Interpreter:\r\n`import tensorflow as tf`\r\n`sess=tf.Session()`\r\n\r\nHere's the message on the command prompt:\r\n\r\n`>>> import tensorflow as tf\r\n>>> sess=tf.Session()\r\n2018-02-22 13:24:51.069445: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-02-22 13:24:51.079084: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-02-22 13:24:51.085961: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-02-22 13:24:51.092933: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-02-22 13:24:51.102940: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-02-22 13:24:51.110151: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-02-22 13:24:51.116103: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-02-22 13:24:51.123166: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-02-22 13:24:51.705468: **E c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE**\r\n2018-02-22 13:24:51.717633: I c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\stream_executor\\cuda\\cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: Cipher\r\n2018-02-22 13:24:51.728408: I c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\stream_executor\\cuda\\cuda_diagnostics.cc:165] hostname: Cipher`\r\n\r\nI already tried a bunch of solutions such as changing `CUDA_VISIBLE_DEVICES=0`, and even adding cudnn to my `PATH` variables\r\n\r\n\r\nI have not used TensorFlow in this example to write any custom code. I am running Windows 10, running TensorFlow 1.2 with GPU support, which I installed from the TensorFlow website. The GPU version is supported by CUDA v8.0. I have an NVIDIA 920M with a memory of 4GB.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "The issue was resolved once I reintstalled CUDA v8.0. Thank you!"]}, {"number": 17186, "title": "Conv2D is not called if it has only control dependency", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\nr1.5\r\n- **Python version**: \r\n2\r\n- **Bazel version (if compiling from source)**:\r\n0.5.4\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n9.0/7.0\r\n- **GPU model and memory**:\r\nTitan XP, 12GB\r\n\r\n### Describe the problem\r\nIf conv2d operation has control dependency in the graph, the operation is not executed at all. \r\nI've tested with tf.tfprof and timeline. If I connect the conv2d with its value, it is finally executed.\r\nIs it intended behavior in TensorFlow?\r\n\r\n### Source code / logs\r\n    ```\r\n    opts = tf.contrib.tfprof.model_analyzer.PRINT_ALL_TIMING_MEMORY\r\n    opts['min_accelerator_micros'] = 1\r\n    run_metadata = tf.RunMetadata()\r\n\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    #config.allow_soft_placement = True\r\n    config.inter_op_parallelism_threads=0\r\n    config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\r\n    batch = 32\r\n    size = 224\r\n    in_channels = 3\r\n    out_channels = 1\r\n    filter_size = 7\r\n    inp = tf.ones(shape=[batch, size, size, in_channels], dtype=tf.float32)\r\n    v = tf.Variable(inp)\r\n    filters = tf.ones(shape=[filter_size, filter_size, in_channels, out_channels], dtype=tf.float32)\r\n    conv2d = tf.nn.conv2d(v, filters, [1,1,1,1], padding='VALID')\r\n    with tf.control_dependencies([conv2d]): \r\n      train = tf.no_op() \r\n    with tf.Session(config=config) as sess:\r\n      sess.run(tf.global_variables_initializer())\r\n      sess.run(train, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)\r\n      # conv2d is not executed\r\n\r\n      sess.run(conv_ops, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)\r\n      # conv2d is executed\r\n    root_node = tf.contrib.tfprof.model_analyzer.print_model_analysis(\r\n                  tf.get_default_graph(),\r\n                  run_meta=run_metadata,\r\n                  tfprof_options=opts)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 17185, "title": "Tensorflow 1.6 ALWAYS looking for libcublas.so.8.0 with Cuda 9.0, Cudnn 7.0 (and libcublas.so.9.0)", "body": "### System information\r\n- **OS Platform and Distribution** -  Linux Ubuntu 16.04\r\n- **TensorFlow installed from** - source \r\n- **TensorFlow version** - tensorflow-1.6.0rc0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**:  0.10.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: Cuda 9.0/ cuDNN 7.0\r\n- **GPU model and memory**: Nvidia Quadro M1200 w/4GB GDDR5\r\n- **Exact command to reproduce**: `import tensorflow as tf`\r\n- **Have I written custom code**: No. Just trying to import tensorflow in python. The only line of code I wrote is  `import tensorflow as tf`\r\n\r\n### Problem\r\nTensorflow is looking for **libcublas.so.8.0** although I linked the cuda path during configuration to /usr/local/cuda-9.0  (also tried linking /usr/local/cuda ). How do I run tensorflow with cuda 9.0 and cudnn 7.0? \r\n\r\nOutputs of: \r\n`ldconfig -v` :\r\n\r\n> libcublas.so.9.0 -> libcublas.so.9.0.176\r\n> libcudnn.so.7 -> libcudnn.so.7.0.5\r\n\r\n`import tensorflow as tf`\r\n\r\n> ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\n> Failed to load the native TensorFlow runtime.\r\n\r\n## Complete Traceback\r\n`import tensorflow as tf`\r\n\r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/home/user/.local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import *\r\n>   File \"/home/user/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"/home/user/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"/home/user/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"/home/user/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"/home/user/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n> ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code", "If you build from source(as indicated by the information you provided) , the CUDA version TF will look for is what you picked during the build. Therefore, this is a misconfiguration during your build, rather than a bug in TF.", "Nope. I built from **source** (also tried with **pip** [_uninstalled tf, cleared cache before each install attempt_] )on **another** machine that has **Cuda 8, Cudnn 7.05**. I specified the path to the cuda-8.0 (_used actual [to cuda-8.0 folder] and symbolic path [to cuda folder] on separate occasions, same error_) and performed TF 1.6 configuration. When I try to import tf, I got :\r\n\r\n```\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\nFailed to load the native TensorFlow runtime.\r\n```\r\n\r\n@gunan  This issue is **not closed.** ", "I am sorry that you are still running into issues.\r\nHowever, I just verified on my test machine by building TF master on a machine cuda 8.0, cudnn 6.\r\ntf.test.is_gpu_available() returns true, and I can run a very simple graph.\r\n\r\n```\r\n$ $ echo $LD_LIBRARY_PATH\r\n/usr/local/cuda/lib64:/usr/local/cuda-8.0/lib64\r\n$ ldd /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\tlinux-vdso.so.1 =>  (0x00007ffcc1ccd000)\r\n\tlibtensorflow_framework.so => /usr/local/lib/python2.7/dist-packages/tensorflow/python/../libtensorflow_framework.so (0x00007f612d0d5000)\r\n\tlibcublas.so.8.0 => /usr/local/cuda/lib64/libcublas.so.8.0 (0x00007f612a725000)\r\n\tlibcusolver.so.8.0 => /usr/local/cuda/lib64/libcusolver.so.8.0 (0x00007f61271b5000)\r\n\tlibcudart.so.8.0 => /usr/local/cuda/lib64/libcudart.so.8.0 (0x00007f6126f4f000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f6126d4b000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f6126b2e000)\r\n\tlibgomp.so.1 => /usr/lib/x86_64-linux-gnu/libgomp.so.1 (0x00007f612690c000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f6126704000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f61263fb000)\r\n\tlibstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f6126079000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f6125e63000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f6125a99000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f6137ab8000)\r\n\tlibcuda.so.1 => /usr/lib/x86_64-linux-gnu/libcuda.so.1 (0x00007f6124e0b000)\r\n\tlibcudnn.so.6 => /usr/local/cuda/lib64/libcudnn.so.6 (0x00007f611b8a9000)\r\n\tlibcufft.so.8.0 => /usr/local/cuda/lib64/libcufft.so.8.0 (0x00007f6112a5b000)\r\n\tlibcurand.so.8.0 => /usr/local/cuda/lib64/libcurand.so.8.0 (0x00007f610eaf2000)\r\n\tlibnvidia-fatbinaryloader.so.387.34 => /usr/lib/nvidia-387/libnvidia-fatbinaryloader.so.387.34 (0x00007f610e8a0000)\r\n$ python \r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.6.0-rc1'\r\n>>> tf.test.is_gpu_available()\r\n....\r\nTrue\r\n>>> tf.Session().run(tf.constant('Hello World'))\r\n...\r\n2018-02-24 07:35:59.621155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10747 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\r\n'Hello World'\r\n```\r\n\r\nPlease check your LD_LIBRARY_PATH actually points to your cuda and cudnn library directories, please make sure you follow CUDA installation instructions carefully to make sure you have correct version of cuda correctly installed.\r\n\r\nAs I have built the code from scratch and I was able to successfully run TF with cuda 8, I do not have a way to help you with this.", "I have find the reason is ldconf, ldconfig is a dynamic link library management command whose purpose is to allow the dynamic link library to be usedby the system.\r\n\r\nThe default ldconf only search /lib and /usr/lib, as well as the library file under the directory listed in the configuration file /etc/ld. so. conf.\r\n\r\nso all of this is caused by the dynamic library of CUDA in the installed CUDA path such as : /path/cuda-9.0/lib64 or /path/cuda-9.0/lib. (for eample my CUDA is installed in /usr/local/cuda-9.0)\r\n\r\n  1.if you install the CUDA manual, then after install, you should add the path of cuda/lib64 to /etc/ld.so.conf file\r\n ` sudo echo \"/usr/local/cuda-9.0/lib64/\" >> /etc/ld.so.conf`\r\n  then\r\n`  sudo ldconfig`\r\n\r\nof course , you can add the path manual, like:\r\n`vim /etc/ld.so.conf`\r\nthen add the path '/usr/local/cuda-9.0' at the end. \r\nthen update it \r\n`sudo ldconfig` \r\nafter the operation, reopen the ipython or pycharm ,\r\n`import tensorflow as tf\r\nwow, you will enjoy it!\r\n\r\nif you install the CUDA by command such as 'dpkg -i cuda-repo-ubuntu1604_9.0.176-1_amd64.deb' or others, it may add the cuda lib path to the /etc/ld.so.conf automatically . but to be on the safe side, check the /etc/ld.so.conf and see if the path add to it .", "> I have find the reason is ldconf, ldconfig is a dynamic link library management command whose purpose is to allow the dynamic link library to be usedby the system.\r\n> \r\n> The default ldconf only search /lib and /usr/lib, as well as the library file under the directory listed in the configuration file /etc/ld. so. conf.\r\n> \r\n> so all of this is caused by the dynamic library of CUDA in the installed CUDA path such as : /path/cuda-9.0/lib64 or /path/cuda-9.0/lib. (for eample my CUDA is installed in /usr/local/cuda-9.0)\r\n> \r\n> 1.if you install the CUDA manual, then after install, you should add the path of cuda/lib64 to /etc/ld.so.conf file\r\n> ` sudo echo \"/usr/local/cuda-9.0/lib64/\" >> /etc/ld.so.conf`\r\n> then\r\n> ` sudo ldconfig`\r\n> \r\n> of course , you can add the path manual, like:\r\n> `vim /etc/ld.so.conf`\r\n> then add the path '/usr/local/cuda-9.0' at the end.\r\n> then update it\r\n> `sudo ldconfig`\r\n> after the operation, reopen the ipython or pycharm ,\r\n> `import tensorflow as tf\r\n> wow, you will enjoy it!\r\n> \r\n> if you install the CUDA by command such as 'dpkg -i cuda-repo-ubuntu1604_9.0.176-1_amd64.deb' or others, it may add the cuda lib path to the /etc/ld.so.conf automatically . but to be on the safe side, check the /etc/ld.so.conf and see if the path add to it .\r\n\r\ni tried it,but still wrong"]}, {"number": 17184, "title": "Error Creating Predictor from Core Estimator", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**:  3.5.3\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: 9.0.176\r\n- **GPU model and memory**: Titan V, 12288 MB\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nCreating a Predictor object by calling `from_estimator` results in the following error:\r\n\r\n```python\r\n----> 5 est_pred= predictor.from_estimator(est, serving_input_receiver_fn)\r\n\r\nc:\\libraries\\python35\\lib\\site-packages\\tensorflow\\contrib\\predictor\\predictor_factories.py in from_estimator(estimator, serving_input_receiver_fn, output_key, graph)\r\n     86       `Estimator`.\r\n     87   \"\"\"\r\n---> 88   if isinstance(estimator, estimator.Estimator):\r\n     89     raise TypeError('Espected estimator to be of type '\r\n     90                     'tf.python.estimator.Estimator, but got type '\r\n\r\nAttributeError: 'Estimator' object has no attribute 'Estimator'\r\n```\r\n\r\n### Source code / logs\r\nTo reproduce this error:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import predictor\r\n\r\ndef model_fn(features, labels, mode):\r\n    # this will trigger other errors once the type check is passed\r\n    return None\r\nest = tf.estimator.Estimator(model_fn=model_fn)\r\n\r\ndef serving_input_receiver_fn():\r\n    inputs = {'X': tf.placeholder(shape=[None, 1], dtype=tf.float32)}\r\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\r\n\r\nest_pred = predictor.from_estimator(est , serving_input_receiver_fn)\r\n```\r\n\r\nIt appears the type check on predictor_factories.py's line 88 has a typo, since ```estimator``` is overridden by the function argument. \r\n", "comments": ["Thanks for the report.\r\n\r\nI believe this was fixed in https://github.com/tensorflow/tensorflow/pull/15648 which is included in TensorFlow 1.6 (which is in release candidate form right now, a final release should be out soon)."]}, {"number": 17183, "title": "Feature Request: Create CTC loss function that Has an Optional Sequence Length Argument", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 8.1\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.4\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nN/A\r\n\r\n### Describe the problem\r\nSince the `sequence_length` parameter of CTC functions are each dependent on the input (specifically the number of time steps), why not let them determine the sequence lengths themselves? Or have a separate function that determines the sequence lengths.\r\n\r\n### Source code / logs\r\nI was thinking of adding something like this to ctc_loss (assuming inputs are batch major and of shape [None, num_time_steps, num_classes]):\r\n\r\n```\r\nbatch_size = input.shape[0]\r\nsequence_length = input.shape[1]\r\nsequence_lengths = tf.fill([batch_size], sequence_length)  \r\n```\r\n\r\nUnfortunately, this throws an error.\r\n\r\n`TypeError: Expected int32 passed to parameter 'dims' of op 'Fill', got [Dimension(None)] of type 'list' instead.`", "comments": ["Update: I figured out a way to do this. However, this assumes that the sequence_lengths of each example are all the same. \r\n\r\nGiven a tensor of shape `(None, num_time_steps, num_classes)`:\r\n\r\n```\r\nsequence_length_dims = tf.stack([tf.shape(input)[0]])\r\nsequence_lengths = tf.fill(sequence_length_dims, input.shape[1]) # to be passed to CTC functions\r\n```\r\n\r\nWith this, I plan to make the `sequence_length` argument of each CTC function optional.", "@ebrevdo can you please take a look?", "Not sure I understand how to make it optional.  You can certainly identify, for each minibatch entry, the first location that contains some \"padding\" value, and make that index the sequence length.  However, this shouldn't be the job of e.g. ctc_loss: you can call this function, get the sequence lengths vector, and pass them to ctc_loss.  If ctc_loss did this for you, it'd now have an *additional* argument, the \"padding value\", and would have to check that you passed in *either* the sequence_lengths *or* the padding_value; and the API of ctc_loss would be more complicated.", "I was thinking of creating something like this that assumes all sequence lengths are the same if no sequence_length is passed:\r\n\r\n```\r\ndef ctc_loss(labels, inputs, sequence_length=None, preprocess_collapse_repeated=True,\r\n             ctc_merge_repeated=True, ignore_longer_outputs_than_inputs=False, time_major=True):\r\n    if sequence_length is None:\r\n        if time_major:\r\n            sequence_length = tf.fill(tf.shape(inputs)[0:1], tf.shape(inputs)[1])\r\n        else:\r\n            sequence_length = tf.fill(tf.shape(inputs)[1:2], tf.shape(inputs)[0])\r\n    return tf.nn.ctc_loss(sparse_labels, inputs, sequence_length,\r\n                          preprocess_collapse_repeated=preprocess_collapse_repeated,\r\n                          ctc_merge_repeated=ctc_merge_repeated,\r\n                          ignore_longer_outputs_than_inputs=ignore_longer_outputs_than_inputs,\r\n                          time_major=time_major)\r\n```"]}, {"number": 17182, "title": "Log \"Waiting for new checkpoint at\" in tf.contrib.training.evaluate_repeatedly only after the checkpoint is found", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.5.0-rc1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n\r\n### Problem description\r\nI use `tf.contrib.training.evaluate_repeatedly` function found here: https://github.com/tensorflow/tensorflow/blob/37aa430d84ced579342a4044c89c236664be7f68/tensorflow/contrib/training/python/training/evaluation.py#L345\r\n\r\nThe two parameters that are relevant here are:\r\n`timeout=1, timeout_fn=my_timeout_fn`, where `my_timeout_fn` is a custom function and returns True or False. Basically I want to stop evaluation loop if some condition is met, in which case the function will return 1. I have to set timeout to some small value, in this case 1 sec, so that timeout_fn is triggered often. The issue with this is that I get the next log message very often:\r\n```\r\nWaiting for new checkpoint at /path/to/dir\r\n```\r\nI set timeout=10, but still I don't want to see that useless log message every 10 seconds.\r\n\r\nThe message comes from here:\r\nhttps://github.com/tensorflow/tensorflow/blob/37aa430d84ced579342a4044c89c236664be7f68/tensorflow/contrib/training/python/training/evaluation.py#L192\r\n\r\nOne of the solutions would be to put this logging command inside the caller of `wait_for_new_checkpoint` function and execute it once before `while True` loop on  line 248, and then right after ` yield checkpoint_path` on line 264. This way we get this message only after a checkpoint is found.\r\n", "comments": ["If you'd like to make a change, please feel free to send a pull request :)\r\nAlternatively, you could use [`tf.logging.set_verbosity(tf.logging.WARN)`](https://www.tensorflow.org/api_docs/python/tf/logging/set_verbosity) to avoid any `logging.info` messages from showing up.\r\n\r\nHope that helps."]}, {"number": 17181, "title": "Disabling kmeans tests for release testing.", "body": "", "comments": []}, {"number": 17180, "title": "MKL: Update mkl docs", "body": "", "comments": ["Decided to make bigger changes.", "@tfboyd Please take a look at the proposed changes to the Performance Guide once PR is ready."]}]