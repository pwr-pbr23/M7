[{"number": 41037, "title": "Update array_ops.py", "body": "\r\n Updated ` where_v2 (condition, x=None, y=None , name= None)`.\r\n\r\n **Changed `y` to `x`, according to issue #41005 on line 4485**", "comments": []}, {"number": 41036, "title": " error: 'tfl.concatenation'", "body": "```\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-162-bce3c984534c> in <module>\r\n      6 )\r\n      7 converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n----> 8 tflite_model = converter.convert()\r\n      9 with tf.io.gfile.GFile('model.tflite', 'wb') as f:\r\n     10   f.write(tflite_model)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n   1082           input_tensors=self._input_tensors,\r\n   1083           output_tensors=self._output_tensors,\r\n-> 1084           **converter_kwargs)\r\n   1085     else:\r\n   1086       result = _toco_convert_graph_def(\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    494       input_data.SerializeToString(),\r\n    495       debug_info_str=debug_info_str,\r\n--> 496       enable_mlir_converter=enable_mlir_converter)\r\n    497   return data\r\n    498 \r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    225       stdout = _try_convert_to_unicode(stdout)\r\n    226       stderr = _try_convert_to_unicode(stderr)\r\n--> 227       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    228   finally:\r\n    229     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\nloc(\"Concat_165\"): error: 'tfl.concatenation' op dimension size of dimension #3 of operand #0 must be equal to dimension size of dimension #3 of output, expected 40, got 20\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/anaconda3/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: <unknown>:0: error: loc(\"Concat_165\"): 'tfl.concatenation' op dimension size of dimension #3 of operand #0 must be equal to dimension size of dimension #3 of output, expected 40, got 20\r\n```\r\nI am converting a fronzen graph (.pb) to tflite and encountered this error. I have no idea why this is happen. Please help me.\r\nUsing below code for conversion.\r\n\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\r\n    graph_def_file = 'weights/yolov5s_v1.pb', \r\n    input_arrays = ['images'],\r\n    output_arrays = ['output','463','482'] \r\n)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\nwith tf.io.gfile.GFile('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\n```", "comments": ["@karanjakhar can you provide the pb file or a minimal step to reproduce the problem?", "https://drive.google.com/file/d/1fuVQiQ53G1DZmDELm0WkIchCcvMHOg7a/view?usp=sharing", "Thanks @karanjakhar I can reproduce with your model. Team will take a look.", "I looked into this and found that the input model has invalid shapes.\r\n\r\nHere, two operands of the concat operation don't have compatible shapes. Operands should have same dimensions except the axis dimension. Here, the dimensions are [1, 256, 40, 20] and [1, 256, 40, 40].\r\n\r\nThe first operand is coming from this function.\r\nhttps://gist.github.com/smit-hinsu/95150cb40a523f0d892fe32d1cd03009\r\n\r\nLet us know if this doesn't help you fix the dimensions in the model and need further assistance. If you can generate some sample data, you can try running the model with a sample input before conversion to identify source of the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41036\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41036\">No</a>\n"]}, {"number": 41033, "title": "Abandon the use of abseil internal API absl::base_internal::NominalCPUFrequency", "body": "Currently, an Abseil internal API is used [here](https://github.com/tensorflow/tensorflow/blob/62b6c316d2a9a1fb06aefb086856e76241280c08/tensorflow/core/platform/default/port.cc#L348), but according to Abseil's [Compatibility Guidelines](https://abseil.io/about/compatibility), the internal API should not be used outside of Abseil library.\r\n\r\nThe reason I raise this issue is that this API has been broken on s390x machines for a while, as I was trying to fix this issue, the Abseil community told me that this method was not called by any Abseil code, and Tensorflow should not depend on this API, see [here](https://github.com/abseil/abseil-cpp/pull/728#issuecomment-652044095). So even though this API is working on most architectures, I suggest abandon the use of this API and try to find an alternative one to avoid depending on Abseil internal APIs.", "comments": ["Hi @gunan , would you mind taking a look at this issue? Thanks.", "We definitely do not want to depend on this. However I have been swamped with other things.\r\nI will try to do this the first chance I get. In case anyone else would like to contribute, I believe this can be a nice simple contribution to TF.", "Thanks for the reply, I just have one thought that is it possible to reuse the implementation of cpu frequency [here](https://github.com/tensorflow/tensorflow/blob/5491487f9a17d5012808c505cb27375cf7e12ccf/tensorflow/core/platform/profile_utils/cpu_utils.cc#L71) in this case?", "It is definitely possible!\r\nwould you like to make the change?", "Sure, I can give it a try, I think it should be a relatively simple patch."]}, {"number": 41032, "title": "ImportError: Traceback", "body": "\r\n\r\n**System information**\r\n- Windowns 10\r\n- \r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version: Python 3.7.7\r\n- Installed using virtualenv? pip? conda?: pip install tensorflow    was(2.1.1)  and      pip install --upgrade tensorflow    was (2.2)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: galaxy gt 1030 (2Gb) / 8Gb(ram)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\thiag\\miniconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\thiag\\miniconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\thiag\\miniconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\thiag\\miniconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\thiag\\miniconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Uma rotina de inicializa\u00e7\u00e3o da biblioteca de v\u00ednculo din\u00e2mico (DLL) falhou.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@thiagohlopes,\r\n\r\nPlease check [this comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n) thread from a similar issue and let us know if it helps. Thanks!", "Please close all sessions of Jupyter Notebook and relaunch. Try to import again as a first step and it should work fine.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41032\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41032\">No</a>\n"]}, {"number": 41031, "title": "Segmentation Fault tflite::ops::builtin::transpose_conv::ResizeCol2ImTensor", "body": "@tensorflow/micro\r\n@tensorflow/lite\r\n\r\n**System information**\r\nHardware : Freescale i.MX6 Quad/DualLite\r\nProcessor: ARMv7 Processor rev 10 (v71)\r\nOS Platform and Distribution: Yocto built Linux distribution (kernel 4.9.4+)\r\nThe tf-lite library was built with common options and using default makefile: https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/lite/tools/make/Makefile\r\nAPI : CPP\r\n\r\n**Describe the problem**\r\nI trained a custom UNet and wanted to deploy it on the hardware using TFLite. Other custom models I've trained and converted to TF Lite work just fine but for this particular model I get a segmentation fault during `AllocateTensors()`.\r\nWorking with the python API does not result in errors and works just fine.\r\n\r\nThe relevant code : \r\n```\r\n\t/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\r\n\tLicensed under the Apache License, Version 2.0 (the \"License\");\r\n\tyou may not use this file except in compliance with the License.\r\n\tYou may obtain a copy of the License at\r\n\t    http://www.apache.org/licenses/LICENSE-2.0\r\n\tUnless required by applicable law or agreed to in writing, software\r\n\tdistributed under the License is distributed on an \"AS IS\" BASIS,\r\n\tWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n\tSee the License for the specific language governing permissions and\r\n\tlimitations under the License.\r\n\t==============================================================================*/\r\n\r\n\r\n\t#include <cstdio>\r\n\t#include <iostream>\r\n\t#include <iomanip>\r\n\t#include \"tensorflow/lite/interpreter.h\"\r\n\t#include \"tensorflow/lite/kernels/register.h\"\r\n\t#include \"tensorflow/lite/model.h\"\r\n\t#include \"tensorflow/lite/optional_debug_tools.h\"\r\n\t#include <time.h>\r\n\t#include <sys/time.h>\r\n\t#include <string>\r\n\r\n\t// This is an example that is minimal to read a model\r\n\t// from disk and perform inference. There is no data being loaded\r\n\t// that is up to you to add as a user.\r\n\t//\r\n\t// NOTE: Do not add any dependencies to this that cannot be built with\r\n\t// the minimal makefile. This example must remain trivial to build with\r\n\t// the minimal build tool.\r\n\t//\r\n\t// Usage: minimal <tflite model>\r\n\r\n\tusing namespace tflite;\r\n\tusing namespace std;\r\n\r\n\t#define TFLITE_MINIMAL_CHECK(x)                              \\\r\n\t  if (!(x)) {                                                \\\r\n\t    fprintf(stderr, \"Error at %s:%d\\n\", __FILE__, __LINE__); \\\r\n\t    exit(1);                                                 \\\r\n\t  }\r\n\r\n\tdouble get_wall_time(){\r\n\t    struct timeval time;\r\n\t    if (gettimeofday(&time,NULL)){\r\n\t\t//  Handle error\r\n\t\treturn 0;\r\n\t    }\r\n\t    return (double)time.tv_sec + (double)time.tv_usec * .000001;\r\n\t}\r\n\tdouble get_cpu_time(){\r\n\t    return (double)clock() / CLOCKS_PER_SEC;\r\n\t}\r\n\r\n\tint main(int argc, char* argv[]) {\r\n\t  if (argc < 2) {\r\n\t    fprintf(stderr, \"minimal <tflite model>\\n\");\r\n\t    return 1;\r\n\t  }\r\n\r\n\t  const char* filename = argv[1];\r\n\t  int startt, endd;\r\n\t  startt = clock();\r\n\t  // Load model\r\n\t  std::unique_ptr<tflite::FlatBufferModel> model =\r\n\t      tflite::FlatBufferModel::BuildFromFile(filename);\r\n\t  TFLITE_MINIMAL_CHECK(model != nullptr);\r\n\r\n\t  // Build the interpreter\r\n\t  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n\t  InterpreterBuilder builder(*model, resolver);\r\n\t  std::unique_ptr<Interpreter> interpreter;\r\n\t  int numthreads=stoi(argv[2]);\r\n\t  std::cout << \"allocating \" << numthreads << \" threads\" << std::endl;\r\n\t  builder(&interpreter, numthreads);\r\n\t  //interpreter->SetNumThreads(numthreads);\r\n\t  std::cout << \"threads allocated\" << std::endl;\r\n\r\n\t  TFLITE_MINIMAL_CHECK(interpreter != nullptr);\r\n\t  printf(\"first minimal check done \\n\");\r\n\t  endd = clock();\r\n\r\n\t  // Allocate tensor buffers.\r\n\t  printf(\"allocating tensors..\\n\");\r\n\t  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);\r\n\t  printf(\"=== Pre-invoke Interpreter State ===\\n\");\r\n\t  //tflite::PrintInterpreterState(interpreter.get());\r\n\r\n\t  double time_taken = double(endd-startt)/double(CLOCKS_PER_SEC);\r\n\t  std::cout << \"a number of threads should be selected\" << std::endl;\r\n\t  std::cout << \"Time taken to load in the model in tflite using CPP API :\" << fixed << time_taken << std::setprecision(5);\r\n\t  std::cout << \" sec \" << std::endl;\r\n\r\n\r\n\t  // Fill input buffers\r\n\t  std::cout << \"inputs : \" << interpreter->inputs().size() << \"\\n\";\r\n\t  std::cout << \"inputs(0) name : \" << interpreter->GetInputName(0) << \"\\n\";\r\n\t  std::cout << \"tensors size: \" << interpreter->tensors_size() << \"\\n\";\r\n\t  std::cout << \"nodes size: \" << interpreter->nodes_size() << \"\\n\";\r\n\t  int startinput = clock();\r\n\t  int input = interpreter->inputs()[0];\r\n\t  std::cout << \"input.1 : \" << input <<\"\\n\";\r\n\t  const std::vector<int> inputs = interpreter->inputs();\r\n\t  const std::vector<int> outputs = interpreter->outputs();\r\n\t  std::cout << \"number of inputs: \" <<inputs.size() << \"\\n\";\r\n\t  std::cout << \"number of outputs: \" <<outputs.size() << \"\\n\";\r\n\r\n\t  TfLiteIntArray* dims = interpreter->tensor(input)->dims;\r\n\t  int test0 = dims->data[0];\r\n\t  int wanted_channels = dims->data[1];\r\n\t  int wanted_height = dims->data[2];\r\n\t  int wanted_width = dims->data[3];\r\n\t  int test4 = dims->data[4];\r\n\t  int test5 = dims->data[5];\r\n\t  int test6 = dims->data[6];\r\n\t  std::cout << \"type of input tensor: \" << interpreter->tensor(input)->type << std::endl;\r\n\t  std::cout << \"height, width, channels of input : \" << wanted_height << \" \" << wanted_width << \" \"<< wanted_channels <<  \" \" << test0 << \" \" << test4 << \" \" << test5 << \" \" << test6 << std::endl;\r\n\r\n\t  int outputint = interpreter->outputs()[0];\r\n\t  TfLiteIntArray* dimso = interpreter->tensor(outputint)->dims;\r\n\t  int wanted_heighto = dimso->data[1];\r\n\t  int wanted_widtho = dimso->data[2];\r\n\t  int wanted_channelso = dimso->data[3];\r\n\t  std::cout << \"type of output tensor: \" << interpreter->tensor(outputint)->type << std::endl;\r\n\t  std::cout << \"height, width, channels of output : \" << wanted_heighto << \" \" << wanted_widtho << \" \"<< wanted_channelso << std::endl;\r\n\r\n\t  float* input_data_ptr = interpreter->typed_tensor<float>(input);\r\n\r\n\r\n\t  float valuefloat = 0.5;\r\n\t  std::cout << \"here we go with 3x480x640\" << std::endl;\r\n\t  for (int k=0; k<640; k++){\r\n\t    for (int i=0; i<3; ++i){\r\n\t\tfor (int j=0; j<480; j++){\r\n\t\t    *(input_data_ptr)=valuefloat;\r\n\t\t    input_data_ptr++;\r\n\t\t    }\r\n\t\t}\r\n\t  }\r\n\r\n\r\n\t  int stopinput = clock();\r\n\t  double time_taken2 = double(stopinput-startinput)/double(CLOCKS_PER_SEC);\r\n\t  std::cout << \"Time taken to load data :\" << fixed << time_taken2 << std::setprecision(5);\r\n\t  std::cout << \" sec \" << std::endl;\r\n\r\n\t   // Run inference\r\n\t   std::cout << \"start inference with \" << numthreads << \" threads\"<< std::endl;\r\n\t  //  Start Timers\r\n\t  double wall0 = get_wall_time();\r\n\t  double cpu0  = get_cpu_time();\r\n\t  TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);\r\n\t  //  Stop Timers\r\n\t  double wall1 = get_wall_time();\r\n\t  double cpu1  = get_cpu_time();\r\n\t  std::cout << \"wall time : \" << wall1 - wall0 << std::endl;\r\n\t  std::cout << \"CPU time : \" << cpu1 - cpu0 << std::endl;\r\n\r\n\t  printf(\"\\n\\n=== Post-invoke Interpreter State ===\\n\");\r\n\t  //tflite::PrintInterpreterState(interpreter.get());\r\n\r\n\t  // Read output buffers\r\n\t  // TODO(user): Insert getting data out code.\r\n\t  int output_idx = interpreter->outputs()[0];\r\n\t  float* output = interpreter->typed_tensor<float>(output_idx);\r\n\t  std::cout << \"OUTPUT: \" << *output << std::endl;\r\n\r\n\r\n\t  return 0;\r\n\t}\r\n```\r\n\r\nThe model I've been using is a custom UNet I trained in TensorFlow and converted to TFLite using post training quantization (full integer quantization-except for the inputs).\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nRunning gdb and checking the stacktrace results in the following:\r\n![image](https://user-images.githubusercontent.com/29673343/86367843-bfea6e80-bc7c-11ea-9113-f4d8e40be423.png)\r\n\r\n\r\nFor the record, I have another model converted with TF_OPS, next to the TF_LITE_BUILTIN_OPS. The binary on the hardware only supports TF_LITE_BUILTIN_OPS, and then the error looks different from what I am encountering now, namely like so : \r\n\r\n![image](https://user-images.githubusercontent.com/29673343/86368076-0e980880-bc7d-11ea-93f5-6b801da25755.png)\r\n\r\n\r\nHow do I go about this problem? What is causing this? ", "comments": ["@jdduke : Tagging you, because you were able to solve my previous issue (#40722) quite effectively. Could you perhaps help me out?", "In addition : when I convert the model using regular post quantization (--post_training_quantization flag in toco command) the error looks like this\r\n![image](https://user-images.githubusercontent.com/29673343/86369956-6899cd80-bc7f-11ea-9b5b-ea69f374faa8.png)\r\n", "Since we add TF ops into your TFLite model, you need a flex delegate to run. However, the above makefile script does not include an option for  flex delegate inclusion yet. FYI, @terryheo", "@FlorentijnD can you provide model files or minimal steps to reproduce the crashing problem at ResizeCol2ImTensor?", "The error I'm getting is quite different from a model where a flex delegate is needed to run, as I mentioned in my OP. Also, I am able to convert the model with only TFLITE_BUILTIN_OPS (see section B).\r\n\r\n#### A. The minimal steps to reproduce the RelizeCol2ImTensor;\r\n1. download the pb model\r\n[models.zip](https://github.com/tensorflow/tensorflow/files/4868192/models.zip)\r\n\r\n2. run the following command to convert\r\n`python3 convert_ingeterquant.py UNet_480640.pb model_integerquant.tflite`\r\nafter saving the following code snippet as `convert_integerquant.py`. And make sure tha data/ folder is in the right place (the data itself is not relevant by the way).\r\n```\r\nimport tensorflow as tf\r\nimport sys\r\nimport glob \r\nfrom random import shuffle\r\nimport cv2\r\nimport numpy as np\r\n\r\nNUM_CALIBRATION_STEPS = 5\r\n\r\ndef load_image(im, target_shape= (640, 480)):\r\n    img = cv2.imread(im, cv2.IMREAD_COLOR)\r\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n    #target_shape=(256,256) #test\r\n    return cv2.resize(img,  target_shape)\r\n\r\ndef normaliza_image(x):\r\n    mean=[0.485, 0.456, 0.406]\r\n    std=[0.229, 0.224, 0.225]\r\n    \r\n    x = np.array(x)/255. \r\n    x[..., 0] -= mean[0]\r\n    x[..., 1] -= mean[1]\r\n    x[..., 2] -= mean[2]\r\n    x[..., 0] /= std[0]\r\n    x[..., 1] /= std[1]\r\n    x[..., 2] /= std[2]\r\n    return x\r\n\r\ndef convert(inputpath, outputpath):\r\n    im_paths = glob.glob('./data/*')\r\n    print(im_paths)\r\n    shuffle(im_paths)\r\n    # make a converter object from the saved tensorflow file\r\n    converter = tf.lite.TFLiteConverter.from_frozen_graph(inputpath, #TensorFlow freezegraph .pb model file\r\n                                                          input_arrays=[\"input\"], # name of input arrays as defined in torch.onnx.export function before.\r\n                                                          output_arrays=[\"output/BiasAdd\"] # name of output arrays defined in torch.onnx.export function before.\r\n                                                          ,)\r\n    # tell converter which type of optimization techniques to use\r\n    # converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    def representative_dataset_gen():\r\n      for i in range(NUM_CALIBRATION_STEPS):\r\n        print(\"we are at step\",i)\r\n        test_img = load_image(im_paths[i])\r\n        model_input = normaliza_image(test_img)\r\n        model_inp = np.expand_dims(model_input,axis=0)\r\n        # Get sample input data as a numpy array in a method of your choosing.\r\n        yield [model_inp.astype(np.float32)]\r\n    converter.representative_dataset = representative_dataset_gen\r\n    tflite_quant_model = converter.convert()\r\n    open(outputpath, 'wb').write(tflite_quant_model)\r\n\r\nif __name__==\"__main__\":\r\n    print(sys.argv[1])\r\n    convert(sys.argv[1],sys.argv[2])\r\n``` \r\n3. the resulting tf lite model can also be found in the zip file and has the same name as mentioned in the CLI command. \r\n4. Yesterday I edited the OP, so the relevant code I mentioned is the code you should save as `inference.cpp`, compile for your target system and run using `./inference model_integerquant.tflite 1` (the 1 represents one thread). You can also find the executable for my target system in the zip file.\r\n\r\n#### B. For the other error;\r\n1. start off with the same .pb model and run `python3 convert_tf_tflite.py UNet_480640.pb model_tflitebuiltinsonly.tflite\r\nafter saving the following code snippet as `convert_tf_tflite.py`\r\n``` \r\nimport tensorflow as tf\r\nimport sys\r\n\r\n\r\ndef convert(inputpath, outputpath):\r\n    # make a converter object from the saved tensorflow file\r\n    converter = tf.lite.TFLiteConverter.from_frozen_graph(inputpath, #TensorFlow freezegraph .pb model file\r\n                                                          input_arrays=[\"input\"], # name of input arrays as defined in torch.onnx.export function before.\r\n                                                          #output_arrays=[\"ENet/logits_to_softmax\"] # name of output arrays defined in torch.onnx.export function before.\r\n                                                          output_arrays=[\"output/BiasAdd\"],)\r\n    # tell converter which type of optimization techniques to use\r\n    # converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS] # to use tf lite builtins\r\n                                           #tf.lite.OpsSet.SELECT_TF_OPS] # because some ops are not part of the tf lite builtins\r\n    \r\n    # to view the best option for optimization read documentation of tflite about optimization\r\n    # go to this link https://www.tensorflow.org/lite/guide/get_started#4_optimize_your_model_optional\r\n\r\n    # convert the model\r\n    tf_lite_model = converter.convert()\r\n    # save the converted model\r\n    open(outputpath, 'wb').write(tf_lite_model)\r\n\r\nif __name__==\"__main__\":\r\n    print(sys.argv[1])\r\n    convert(sys.argv[1],sys.argv[2])\r\n```\r\n\r\nYou can find the TF Lite model with the same name in the zip file too.\r\n\r\n2. run the executable you've generated in the previous step like so\r\n`./inference model_tflitebuiltinsonly.tflite 1`\r\n", "For the second problem, you need to create an inference program with the bazel source tree since tf.lite.OpsSet.SELECT_TF_OPS requires Flex delegate, which is not available in the TFLite's Makefile. Please refer to the C++ section in the following document to build your inference program with the bazel: https://www.tensorflow.org/lite/guide/ops_select#c", "I ran both models in the zip files with the nightly version under linux machines. There are no issues on running inferences. How about trying the latest versions?", "Good to know thanks. So if I understand correctly, the problem is due to the tf lite binary being built with https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/lite/tools/make/Makefile instead of the nightly version? \r\nWould https://github.com/tensorflow/tensorflow/blob/r2.2/tensorflow/lite/tools/make/Makefile do the trick and installing with flex delegate?\r\n\r\nConcerning your first remark, I am not using `tf.lite.OpsSet.SELECT_TF_OPS` during conversion (the line is commented out) which means I am using only the TF Lite builtins. ", "And to get on the same line here, you managed to compile the program and run the `./inference model_integerquant.tflite 1` CLI command without issues?\r\n", "If you deliver the models without SELECT_TF_OPS, you don't need to switch to bazel. Great! Actually, team does not have a plan to support flex delegate through Makefile yet. So, I recommend using bazel source tree for flex use cases.\r\n\r\nFor the segfault problem, could you check the behavior with r2.2 and r2.3?", "Actually, I do not have the device you have and just confirmed the model's inferences with the regular linux machine by using the TFLite benchmark tool instead.", "Is the TF Lite benchmark tool in CPP? because python works fine for me", "Yes, it is written in C++. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark\r\n\r\nCould you refer the inference C++ guide and examples?\r\nhttps://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_c\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc", "The thing is that building/compiling TF Lite binaries and cpp programs on the hardware is not possible and cross compiling is not straightforward. \r\nWe are first building the TF Lite library, so we'll try to build the TF Lite r2.2 library next for our target system. Then we'll cross compile the program again and run it on the hardware. I'll let you know when we have results on this. \r\nThe code in the OP is based on the minimal.cc you refer to in your last link.\r\n\r\nThis error should have nothing to do with the hardware right?", "If Python version had worked within the hardware you have and the Python binary had been compiled through the same \r\nprocess with the inference, the kernel code should be working without problems. However, in most cases, Python version will be installed via PIP. I guess they are based on different configurations. However, you are using an old version right now. It would be better to verify this behavior happens at the latest version first. I also recommend using a bazel version for your case since TFLite does have a limited support on the Makefile path.", "@FlorentijnD \r\nPlease update as per above comment.", "I was not entirely clear, my apologies. I meant that running the model locally in the tf lite interpreter in python works. Due to VFP mismatch between various python wheels and the microprocessor of the hardware, I was not able to install the python API on-device.", "@FlorentijnD It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version  2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41031\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41031\">No</a>\n"]}, {"number": 41030, "title": "[TFLite] Documentation: fix typo in doc for the post-training quantization, 16x8", "body": "Fix for the typo in the documentation for post-training quantization with 16x8 scheme. \r\nIt is **supported_ops** like for int8, not **supported_types** like for float16.\r\nAlso, added lines with code for calibration data.", "comments": ["Hi @jdduke Could you please review this small fix for the typo in the doc ? Thanks!"]}, {"number": 41029, "title": "Error while reading resource variable _AnonymousVar285 from Container", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@caijianwei1996 \r\n\r\nRequest you to fill issue template.\r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41029\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41029\">No</a>\n"]}, {"number": 41028, "title": "LSTM behaving differently with recurrent activation \"sigmoid\" and tf.keras.activations.sigmoid", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 3.10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.6.1\r\n- GPU model and memory: Tesla V100-SXM2-32GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nUsing tf.keras.layers.LSTM(units, recurrent_activation=tf.keras.activations.sigmoid) - My training is not converging\r\nUsing tf.keras.layers.LSTM(units, recurrent_activation='sigmoid') - Same training is converging\r\nAlso the time taken in former is higher than the latter. This I have found that in recurrent_v2.py, it is checking if recurrent_activation == 'sigmoid' which fails in case of tf.keras.activations.sigmoid, so the former one is not using CuDNN while latter one is using. What is different in those two cases is that the latter one is converging using CuDNN while the former one is not converging at all not using CuDNN.\r\n\r\n**Describe the expected behavior**\r\nTraining should converge for both of the above however training time variation is understood as former is not using CuDNN but the latter one is using.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nNo error is coming just training is acting differently\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@tvatsal1996,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "Here is a sample code for my issue on the specified system details:\r\n```\r\nimport tensorflow as tf\r\n\r\nclass TranNetwork(tf.keras.Model):\r\n    \"\"\"Transcription Network\r\n    \"\"\"\r\n    def __init__(self, num_lstm_layers, lstm_cell_size, dropout=0.0):\r\n        super(TranNetwork, self).__init__()\r\n\r\n        self.num_lstm_layers = num_lstm_layers\r\n        self.lstm_cell_size = lstm_cell_size\r\n        self.dropout = dropout\r\n        self.trans_layers = []\r\n        self.pooling = tf.keras.layers.MaxPool1D(pool_size=3, padding=\"same\")\r\n\r\n        # HERE IS THE ISSUE, USING RECURRENT ACTIVATION AS tf.keras.activations.sigmoid instead of 'sigmoid'\r\n        # causes model to not converge at all\r\n        recurrent_activation = 'sigmoid'\r\n        #recurrent_activation = tf.keras.activations.sigmoid\r\n        for l in range(num_lstm_layers):\r\n          self.trans_layers.append(tf.keras.layers.LSTM(\r\n                                     self.lstm_cell_size,\r\n                                     return_sequences=True,\r\n                                     dropout=self.dropout,\r\n                                     recurrent_activation=recurrent_activation))\r\n\r\n    def call(self, x, x_len, training=True):\r\n        seq_len = x_len\r\n        output = tf.clip_by_value(x, -3.0, 3.0)\r\n        for l in range(self.num_lstm_layers):\r\n            mask = tf.sequence_mask(seq_len)\r\n            output = self.trans_layers[l](output, mask=mask, training=training)\r\n            if l == 0:\r\n              seq_len = tf.cast(tf.math.ceil(tf.divide(seq_len, 3)), dtype=tf.int32)\r\n              output = self.pooling(output, training=training)\r\n\r\n        mask = tf.sequence_mask(seq_len)\r\n        output = self.trans_layers[-1](output, mask=mask, training=training)\r\n\r\n        return output, seq_len\r\n\r\nclass RNNTModel(object):\r\n    \"\"\" RNNT Model class for training\r\n    \"\"\"\r\n    def __init__(self, num_lstm_layers, lstm_cell_size, vocab_size, dropout=0.0):\r\n        self.vocab_size = vocab_size\r\n        self.trans = TranNetwork(num_lstm_layers, lstm_cell_size, dropout=dropout)\r\n        self.ctc_layer = tf.keras.layers.Dense(1+self.vocab_size, name='ctc')\r\n\r\n    def forward(self, x, y, x_len, y_len, training=True):\r\n        @tf.function(input_signature=[\r\n            tf.TensorSpec(shape=[None, None, 40], dtype=tf.float32),\r\n            tf.TensorSpec(shape=[None, None], dtype=tf.int32),\r\n            tf.TensorSpec(shape=[None, ], dtype=tf.int32),\r\n            tf.TensorSpec(shape=[None, ], dtype=tf.int32),\r\n            tf.TensorSpec(shape=[], dtype=tf.bool)])\r\n        def _forward(x, y, x_len, y_len, training=True):\r\n            trans_output, output_len = self.trans(x, x_len, training=training)\r\n            ctc_out = self.ctc_layer(trans_output, training=training)\r\n            output_d = {'ctc_out': ctc_out, 'output_len': output_len}\r\n            return output_d\r\n        return _forward(x, y, x_len, y_len, training=training)\r\n\r\n    def loss(self, y, x_len, y_len, ctc_out):\r\n        @tf.function(input_signature=[\r\n            tf.TensorSpec(shape=[None, None], dtype=tf.int32),\r\n            tf.TensorSpec(shape=[None, ], dtype=tf.int32),\r\n            tf.TensorSpec(shape=[None, ], dtype=tf.int32),\r\n            tf.TensorSpec(shape=[None, None, 1+self.vocab_size], dtype=tf.float32)])\r\n        def _loss(y, x_len, y_len, ctc_out):\r\n            ctc_loss = tf.nn.ctc_loss(y, ctc_out, y_len, x_len,\r\n                                      logits_time_major=False,\r\n                                      blank_index=self.vocab_size)\r\n            # to ignore invalid ctc loss case\r\n            mask = tf.dtypes.cast(\r\n              tf.math.greater_equal(x_len, y_len), dtype=tf.float32)\r\n            ctc_loss = tf.multiply(ctc_loss, mask)\r\n            ctc_loss = tf.reduce_sum(ctc_loss)\r\n            return ctc_loss\r\n        return _loss(y, x_len, y_len, ctc_out)\r\n\r\n    @property\r\n    def trainable_variables(self):\r\n        return self.trans.trainable_variables \\\r\n               + self.ctc_layer.trainable_variables\r\n\r\nmodel = RNNTModel(2, 32, 29, dropout=0.0)\r\noptimizer = tf.keras.optimizers.Adam(0.00025)\r\n\r\n@tf.function(input_signature=[\r\n    tf.TensorSpec(shape=[None, None, 40], dtype=tf.float32),\r\n    tf.TensorSpec(shape=[None, None], dtype=tf.int32),\r\n    tf.TensorSpec(shape=[None, ], dtype=tf.int32),\r\n    tf.TensorSpec(shape=[None, ], dtype=tf.int32)])\r\ndef train_step(x, y, x_len, y_len):\r\n    with tf.GradientTape() as tape:\r\n        output_d = model.forward(x, y, x_len, y_len, training=True)\r\n        ctc_loss = model.loss(y, output_d['output_len'],\r\n                              y_len, output_d['ctc_out'])\r\n    variables = model.trainable_variables\r\n    gradients = tape.gradient(ctc_loss, variables)\r\n    gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\r\n    optimizer.apply_gradients(zip(gradients, variables))\r\n    loss_norm_factor = 1.0 / tf.cast(tf.reduce_sum(y_len), dtype=tf.float32)\r\n    return ctc_loss * loss_norm_factor\r\n```", "@tvatsal1996,\r\nI did not face any error while running the given code with TF v2.2. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/e8109f044f1593a27bd1189197a112c4/41028.ipynb). \r\n\r\nLooks like the code doesn't reach the `recurrent_activation = tf.keras.activations.sigmoid` line. Could you please provide a reproducible example. Thanks! ", "> @tvatsal1996,\r\n> I did not face any error while running the given code with TF v2.2. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/e8109f044f1593a27bd1189197a112c4/41028.ipynb).\r\n> \r\n> Looks like the code doesn't reach the `recurrent_activation = tf.keras.activations.sigmoid` line. Could you please provide a reproducible example. Thanks!\r\n\r\nHi, it will not throw out any error. I was asking that these two models which are very much the same except recurrent activation is `\"sigmoid\"` in the first one and `tf.keras.activations.sigmoid` in the second one, but with `tf.keras.activations.sigmoid`, my model does not converge and with `\"sigmoid\"` it converges.\r\n\r\nAlso I have found out that with `\"sigmoid\"` it is using my CUDA library while with `tf.keras.activations.sigmoid` it is not using CUDA library. This is the basic difference. More specifically the class `LSTM` in `tensorflow.python.keras.layers.recurrent_v2` uses these two functions namely `standard_lstm` and `cudnn_lstm` for different modes, i.e. with CUDA or without CUDA. While using `\"sigmoid\"` it is using `cudnn_lstm` and while using  `tf.keras.activations.sigmoid` it is using `standard_lstm`. This is the only difference which I have figured out with the two executions and is behaving differently.", "Hi, I am still stuck in this and have not got any workaround till now. Please help !!", "I was digging through the implementations of GRU and LSTM when I stumbled over your post. Maybe this is an explanation for the behavior you are observing:\r\nIn the implementation of recurrent_v2.py, you can choose both recurrent_activation and activation, but this seems to have no effect when using the standard_lstm and standard_gru. Note that for cuDNN you must not change the activations.\r\nHere is the implementation:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/8c6f2d55762f3fc94f98fdd8b3c5d59ee1276dba/tensorflow/python/keras/layers/recurrent_v2.py#L1298\r\n\r\nThis was different in the older implementations, but recent versions seem to ignore the given activations. Or am I wrong?", "Yes, recent version seems to ignore the given activations in [standard_lstm](https://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/keras/layers/recurrent_v2.py#L1238), but my issue is that my model is not converging when run without CuDNN kernel. When being run with CuDNN kernel, i.e, when using  [gpu_lstm](https://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/keras/layers/recurrent_v2.py#L1320), it was converging with recurrent activation as sigmoid but while not using CuDNN kernel, i.e when using [standard_lstm](https://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/keras/layers/recurrent_v2.py#L1238) function, it was not converging. Is there something I am missing ?", "The check for cudnn kernel compatibility was updated a while ago to fix this case. See https://github.com/tensorflow/tensorflow/blob/05632ed9bad5bf9eee3edd57ade3d8250d580019/tensorflow/python/keras/layers/recurrent_v2.py#L1090. \r\n\r\nCould u try with the latest TF release?", "> I was digging through the implementations of GRU and LSTM when I stumbled over your post. Maybe this is an explanation for the behavior you are observing:\r\n> In the implementation of recurrent_v2.py, you can choose both recurrent_activation and activation, but this seems to have no effect when using the standard_lstm and standard_gru. Note that for cuDNN you must not change the activations.\r\n> Here is the implementation:\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/8c6f2d55762f3fc94f98fdd8b3c5d59ee1276dba/tensorflow/python/keras/layers/recurrent_v2.py#L1298\r\n> \r\n> This was different in the older implementations, but recent versions seem to ignore the given activations. Or am I wrong?\r\n\r\nNote that for standard_lstm/gpu_lstm, its the code path that _could_use_gpu_kernel = True (which means the value of activation and recurrent_activation should be tanh and sigmoid). The code path for _could_use_gpu_kernel = False is falling back to LSTM v1 which is in recurrent.py.", "Since this is already fixed in the latest nightly, I am closing this bug now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41028\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41028\">No</a>\n"]}, {"number": 41027, "title": "AutoGraph could not transform with \"Cause: Inconsistent ASTs detected\" with pymc4", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n  * Yes, with [pymc4](https://github.com/pymc-devs/pymc4)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n  * Ubuntu 16.04 on WSL (WSL1)\r\n  * Windows version : Windows 10 1909 (18363.900) \r\n- TensorFlow installed from (source or binary):\r\n  * binary\r\n- TensorFlow version (use command below):\r\n  * v1.12.1-35610-gd8c49c2fde 2.4.0-dev20200701\r\n- Python version:\r\n  * Python 3.8.3, built from source\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n  * No GPU\r\n\r\n**Describe the current behavior**\r\nWhen I execute a script which uses pymc4, TF writes logs which says:\r\n```\r\nWARNING:tensorflow:AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f2d04ec3f70> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Inconsistent ASTs detected. This is a bug. Cause: \r\ninconsistent values for field args: [<gast.gast.Name object at 0x7f2d04d7b370>] and []Diff:\r\n```\r\n**Describe the expected behavior**\r\nThe script runs without the warning log from TF.\r\n\r\n**Standalone code to reproduce the issue**\r\n1. Install pymc4 with ```pip install pymc4```\r\n   * This will also install TF\r\n1. Run [this script](https://gist.github.com/Isa-rentacs/aef5629956b17493072f44478bf0d3dd)\r\n\r\n**Other info / logs** \r\nThe log is available on [this gist](https://gist.github.com/Isa-rentacs/72ca156134a172d56cbe4ae5da827003)", "comments": ["@Isa-rentacs \r\nPlease share a simple stand alone indented code for us to replicate the issue faced. Also please let us know the tf version on which this error is faced.\r\nPlease refer to these issues with similar error:\r\n#37144 #38947 [link](https://stackoverflow.com/questions/44956460/valueerror-dimensions-must-be-equal-but-are-784-and-500-for-matmul-1-op-m/44956689#44956689) [link2](https://github.com/tensorflow/tensorflow/issues/36731#issuecomment-587294667)  #32377  #38691 #32319 #37251", "Both the code for repro and TF version is in the original post.\r\nGist for the code to reproduce : https://gist.github.com/Isa-rentacs/aef5629956b17493072f44478bf0d3dd\r\nTF version : 2.4.0-dev20200701\r\nTF-P version : 0.11.0-dev20200702\r\n", "@Isa-rentacs \r\nI ran the code shared and face different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/8f1c71988114e6fd0cd4b9c9b7b2ba95/untitled261.ipynb).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41027\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41027\">No</a>\n"]}, {"number": 41026, "title": "ucf101 video dataset seems to be gone", "body": "The ucf101 activity recognition dataset seems to have been completely removed. There's a link here https://www.tensorflow.org/datasets/catalog/ucf101 and when you go further both the landing page and the .zip file results in 404 errors. \r\n\r\nTo be honest I am a little surprised that Google wasn't hosting these datasets. You could link it up in Colaboratory so you didn't even have to download anything when using them from there. Just an idea.\r\n", "comments": ["Thanks for the issue. The correct weblink should point to https://www.crcv.ucf.edu/data-sets/ucf101/", "Thank you so much! And I'm so sorry for putting it here rather than in /datasets - I was  a bit confused I think =)", "Thank you for reporting\r\n\r\n> To be honest I am a little surprised that Google wasn't hosting these datasets.\r\n\r\nGoogle does not own the original data, so we are not legally allowed to re-host the dataset ourself (even if we would love to as it would greatly benefit the open source community).\r\n\r\nClosing this as we've updated the link internally."]}, {"number": 41025, "title": "Installation broken - Tensorflow 2.2 Cuda 10.1 Ubuntu 18.04", "body": "This is essentially the same issue to  #36121 with Tensorflow 2.2. The issue doesn't list a proper solution, only led to an update of documentation, that is now apparently again out of date.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version: 2.2\r\n- Python version:  3.7.5\r\n- Installed using virtualenv? pip? conda?: virtualenv 20.0.25\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce RTX 2070 with Max-Q Design/PCIe/SSE2\r\n\r\n\r\n**Describe the problem**\r\nI'm following the installation instructions:\r\nhttps://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_101\r\n\r\nFinished these steps (no errors):\r\n\r\n    #Add NVIDIA package repositories\r\n    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb\r\n    sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\n    sudo dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb\r\n    sudo apt-get update\r\n    wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\n    sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\n    sudo apt-get update\r\n    \r\n    # Install NVIDIA driver\r\n    sudo apt-get install --no-install-recommends nvidia-driver-430\r\n    # Reboot. Check that GPUs are visible using the command: nvidia-smi\r\n\r\n`nvidia-smi` returns:\r\n    NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2\r\n\r\nTwo notable points:\r\n\r\n- 440 was fetched\r\n- CUDA is listed as 10.2 (although the next step of installation tries to install 10.1:\r\n\r\n    sudo apt-get install --no-install-recommends \\\r\n        cuda-10-1 \\\r\n        libcudnn7=7.6.4.38-1+cuda10.1  \\\r\n        libcudnn7-dev=7.6.4.38-1+cuda10.1\r\n\r\nThis gives same error as #36121:\r\n\r\n    The following packages have unmet dependencies:\r\n     cuda-10-1 : Depends: cuda-runtime-10-1 (>= 10.1.243) but it is not going to be installed\r\n             Depends: cuda-demo-suite-10-1 (>= 10.1.243) but it is not going to be installed\r\n\r\n\r\nThe discussion in #36121 is bit unclear on resolution and differs slightly in driver/library version numbers.\r\nI'm on a freshly installed system and I have *not* attempted to install the dependencies manually, before consulting you.\r\n\r\nThank you!\r\n \r\n", "comments": ["@rsuhada-ampx,\r\nPlease check [this thread](https://github.com/tensorflow/tensorflow/issues/39116) regarding a similar issue and let us know if it helps. Thanks!", "> @rsuhada-ampx,\r\n> Please check [this thread](https://github.com/tensorflow/tensorflow/issues/39116) regarding a similar issue and let us know if it helps. Thanks!\r\n\r\nNo, [this reply](https://github.com/tensorflow/tensorflow/issues/39116#issuecomment-623686956) has not resolved the issue. The author says that he doesn't know what worked, but assumes doing:\r\n    \r\n    sudo apt -y full-upgrade\r\n\r\nThis doesn't work for me: my system is a fresh install, completely upgraded. The issue persists exactly the same as before.\r\n\r\nThank you.", "```\r\n   add-apt-repository universe && \\\r\n    apt-get update -yqq && \\\r\n    apt-get install freeglut3-dev -y && \\\r\n    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb && \\\r\n    apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub && \\\r\n    dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb && \\\r\n    apt-get update -y && \\\r\n    wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb && \\\r\n    apt-get install -y ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb && \\\r\n    apt-get update -y && \\\r\n    mkdir -p /usr/lib/nvidia && \\\r\n    apt-get install -y --no-install-recommends nvidia-driver-430 && \\\r\n    apt-get -y --fix-broken -o Dpkg::Options::=\"--force-overwrite\" install \\\r\n        cuda \\\r\n        cuda-10-1 \\\r\n        cuda-toolkit-10-1 \\\r\n        cuda-samples-10-1 \r\n```\r\n\r\n^^ This worked for me, note the install of `freeglut3-dev` a first step as advised [here](https://forums.developer.nvidia.com/t/cuda-install-unmet-dependencies-cuda-depends-cuda-10-0-10-0-130-but-it-is-not-going-to-be-installed/66488/9) for an earlier version", "Note that nothing in this issue is related to TensorFlow code. This question is better asked on StackOverflow/elsewhere, as we are trying to triage these issues to be only those relevant to the code itself.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41025\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41025\">No</a>\n", "> Note that nothing in this issue is related to TensorFlow code. This question is better asked on StackOverflow/elsewhere, as we are trying to triage these issues to be only those relevant to the code itself.\r\n\r\nWould not TF want to update its doco though to reflect right instructions thats mentioned here https://www.tensorflow.org/install/gpu \r\nPerhaps this issue is more of documentation. \r\n", "> Note that nothing in this issue is related to TensorFlow code. This question is better asked on StackOverflow/elsewhere, as we are trying to triage these issues to be only those relevant to the code itself.\r\n\r\nIn that case you might want to remove the option for build/install issue label as it misleads the users that they can get help here.", "> ```\r\n>    add-apt-repository universe && \\\r\n>     apt-get update -yqq && \\\r\n>     apt-get install freeglut3-dev -y && \\\r\n>     wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb && \\\r\n>     apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub && \\\r\n>     dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb && \\\r\n>     apt-get update -y && \\\r\n>     wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb && \\\r\n>     apt-get install -y ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb && \\\r\n>     apt-get update -y && \\\r\n>     mkdir -p /usr/lib/nvidia && \\\r\n>     apt-get install -y --no-install-recommends nvidia-driver-430 && \\\r\n>     apt-get -y --fix-broken -o Dpkg::Options::=\"--force-overwrite\" install \\\r\n>         cuda \\\r\n>         cuda-10-1 \\\r\n>         cuda-toolkit-10-1 \\\r\n>         cuda-samples-10-1 \r\n> ```\r\n> \r\n> ^^ This worked for me, note the install of `freeglut3-dev` a first step as advised [here](https://forums.developer.nvidia.com/t/cuda-install-unmet-dependencies-cuda-depends-cuda-10-0-10-0-130-but-it-is-not-going-to-be-installed/66488/9) for an earlier version\r\n\r\nThank you Suneeta! \r\n`apt-get install freeglut3-dev -y` definitely made a difference, though I run into issues anyway.\r\n\r\nFor posterity, if somebody had a similar issue:\r\n\r\n- I followed Suneeta's instructions\r\n- the installation still failed at the last step. I unfortunately could not save the error message\r\n- after restart I could not boot into Linux, but fortunately booting into recovery mode and running package repair worked\r\n- oddly enough after finally managing to get into the system:\r\n- `nvidia-smi` worked, but says I'm on v450 with CUDA 11 (!)\r\n- re-trying the last step of installation says that everything is installed and up to date (despite failing previously). I think the recovery repair fixed it.\r\n\r\n- Doing the basic tensorflow test (v2.2) worked - GPU detected. I haven't tried anything fancier, so not sure if the installation is stable... Fingers crossed.\r\n\r\nSo, I suspect (but don't know for sure), that you should reboot before doing the last step in Suneeta's instructions.\r\n\r\nAs this was a clean slate system, I do not think the official install instructions are correct for 18.04.", "@rsuhada-ampx apologies, there are countless of configurations and operating systems where TF could be installed and we only do the release in one (Ubuntu 16). Of course, we accept updates to the documentation if our configuration is broken, but for all others we can only provide minimal guidance in these types of issues and then redirect to StackOverflow / SIG Build.\r\n\r\nIn this case, you are using Ubuntu 18, while we were building for Ubuntu 16. So, installation of dependent packages is not documented and we don't have enough people to also fully investigate.", "Just putting it out there in case anyone needs it or finds it helpful .. I have jumped through manny hoops but this works without any fixup `--fix-broken` or overwrites `-o Dpkg::Options::=\"--force-overwrite\"`. It also pins everything so no moving target overwrites anything because there are many in the mix ... eg (kernels-commons).\r\n\r\n```bash\r\napt-get update -yqq && \\\r\n    apt-get install freeglut3-dev -y && \\\r\n    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb && \\\r\n    apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub && \\\r\n    dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb && \\\r\n    apt-get update -y && \\\r\n    wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb && \\\r\n    apt-get install -y ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb && \\\r\n    apt-get update -y && \\\r\n    mkdir -p /usr/lib/nvidia && \\\r\n    apt-get install -y --no-install-recommends \\\r\n        nvidia-driver-450=450.36.06-0ubuntu1 \\\r\n        libnvidia-common-450=450.36.06-0ubuntu1 \\\r\n        nvidia-kernel-common-450=450.36.06-0ubuntu1 \\\r\n        libnvidia-gl-450=450.36.06-0ubuntu1 \\\r\n        nvidia-dkms-450=450.36.06-0ubuntu1 \\\r\n        nvidia-kernel-source-450=450.36.06-0ubuntu1 \\\r\n        libnvidia-compute-450=450.36.06-0ubuntu1 \\\r\n        nvidia-compute-utils-450=450.36.06-0ubuntu1 \\\r\n        libnvidia-decode-450=450.36.06-0ubuntu1 \\\r\n        libnvidia-encode-450=450.36.06-0ubuntu1 \\\r\n        nvidia-utils-450=450.36.06-0ubuntu1 \\\r\n        xserver-xorg-video-nvidia-450=450.36.06-0ubuntu1 \\\r\n        libnvidia-cfg1-450=450.36.06-0ubuntu1 \\\r\n        libnvidia-ifr1-450=450.36.06-0ubuntu1 \\\r\n        libnvidia-fbc1-450=450.36.06-0ubuntu1 \\\r\n        nvidia-modprobe=450.36.06-0ubuntu1 \\\r\n        libxnvctrl0=450.36.06-0ubuntu1 \\\r\n        nvidia-settings=450.36.06-0ubuntu1 && \\\r\n    apt-get -y --no-install-recommends install \\\r\n        cuda-10-1 \\\r\n        cuda-toolkit-10-1 \\\r\n        cuda-samples-10-1 \\\r\n        cuda-drivers-450=450.36.06-1 \\\r\n        cuda-drivers=450.36.06-1 \\\r\n        cuda-documentation-10-1 && \\\r\n    apt-get install -y --no-install-recommends \\\r\n        libcudnn7=7.6.4.38-1+cuda10.1 \\\r\n        libcudnn7-dev=7.6.4.38-1+cuda10.1 && \\\r\n    apt-get install -y --no-install-recommends \\\r\n        libnvinfer6=6.0.1-1+cuda10.1 \\\r\n        libnvinfer-dev=6.0.1-1+cuda10.1 \\\r\n        libnvinfer-plugin6=6.0.1-1+cuda10.1 \r\n```\r\n\r\nSwitching the `450` nvidia driver to 440/430/418 will also work provided minor pin version resolves correctly "]}, {"number": 41024, "title": "Overhead of calling \" ModifyGraphWithDelegate() \"", "body": "**System information**\r\n- I have written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):\r\n- Mobile device : Qualcomm 855\r\n- TensorFlow installed from source:\r\n- TensorFlow version r.14:\r\n- Python version 3.6:\r\n- Bazel version : 3.1:\r\n- GCC/Compiler version : ndk21_rb\r\n- GPU model and memory:\r\n\r\nWe are running inference on GPU using TFLite. We observed that invoking ModifyGraphWithDelegate() takes 22 second on Qualcomm 855 device.\r\nThis time is so huge, We are not able to find any other way to reduce the 22 second overhead.\r\n\r\nIs there any way to get ride of this overhead.\r\nCould you please help me .\r\n\r\n", "comments": ["@impjdi \r\n\r\nThis issue becomes critical in Android 11. We got a lot of ANR timeout warning messages when launching GPU delegate.", "Oh my apologies.  Didn't notice this bug assigned to me.  For optimal performance, it performs some auto tuning which can take a long time.  If you want to bypass this auto tuning phase, you can set `TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER`, but your performance may not be as optimal.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 41023, "title": "[RNN] Loading quantized LSTM model with state handling into TF Lite interpreter fails.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs / Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary / pip\r\n- TensorFlow version (or github SHA if from source): 2.3.0rc0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import  LSTM, Input\r\n\r\n\r\nin_dat = Input(batch_shape=(1,1,128))\r\nin_states = Input(batch_shape=(1,1,128,2))\r\n\r\n\r\nin_state = [in_states[:,0,:,0], in_states[:,0,:,1]]\r\nout_dat, state_h, state_c = LSTM(128, \r\n                                 return_sequences=True, \r\n                                 return_state=True)(in_dat, initial_state=in_state)\r\n\r\nout_states = tf.stack([state_h, state_c],axis=-1)\r\n\r\nmodel = Model(inputs=[in_dat, in_states], \r\n                           outputs=[out_dat, out_states])\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\nwith tf.io.gfile.GFile('test_model.tflite', 'wb') as f:\r\n      f.write(tflite_model)\r\n\r\ninterpreter = tf.lite.Interpreter(model_path='./test_model.tflite')\r\n```\r\n\r\n**The output from the converter invocation**\r\nConversion works fine, the interpreter can not load the model:\r\n```\r\n2020-07-02 10:58:39.439478: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd0c32e2e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-02 10:58:39.439818: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:From /Applications/anaconda3/envs/tf23env/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\n2020-07-02 10:58:41.751516: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:tensorflow:From /Applications/anaconda3/envs/tf23env/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\n2020-07-02 10:58:48.388047: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n2020-07-02 10:58:48.388077: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\n2020-07-02 10:58:48.395287: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /var/folders/p4/yptj61953073c48n66hnlr100000gp/T/tmpu_2uf65d\r\n2020-07-02 10:58:48.419914: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2020-07-02 10:58:48.419953: I tensorflow/cc/saved_model/loader.cc:234] Reading SavedModel debug info (if present) from: /var/folders/p4/yptj61953073c48n66hnlr100000gp/T/tmpu_2uf65d\r\n2020-07-02 10:58:48.509456: I tensorflow/cc/saved_model/loader.cc:199] Restoring SavedModel bundle.\r\n2020-07-02 10:58:48.669926: I tensorflow/cc/saved_model/loader.cc:183] Running initialization op on SavedModel bundle at path: /var/folders/p4/yptj61953073c48n66hnlr100000gp/T/tmpu_2uf65d\r\n2020-07-02 10:58:48.753087: I tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 358438 microseconds.\r\n2020-07-02 10:58:49.363263: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor arg5 because it has fewer than 1024 elements (128).\r\nTraceback (most recent call last):\r\n  File \"test_tf_lite.py\", line 41, in <module>\r\n    interpreter_1 = tf.lite.Interpreter(model_path='./test_model.tflite')\r\n  File \"/Applications/anaconda3/envs/tf23env/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py\", line 198, in __init__\r\n    model_path, self._custom_op_registerers))\r\nValueError: Did not get operators, tensors, or buffers in subgraph 1.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n\r\n[test_model.tflite.zip](https://github.com/tensorflow/tensorflow/files/4863193/test_model.tflite.zip)\r\n\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Model can not be loaded by the interpreter\r\n- Without quantization, conversion, loading and invoke works fine.\r\n- Worked before with a tf-nightly, but I don't know anymore which one.\r\n- Same behaviour on MacOs and Linux\r\n- Same behaviour when converting from SavedModel format\r\n- Same behaviour with tf-lite runtime\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I also tried this code for conversion, I found in another issue, but with the same result:\r\n```python\r\n@tf.function\r\ndef call(x, states):\r\n    x, states = model([x,states])\r\n    return x, states\r\n\r\nbatch_size = 1\r\nconcrete_func = call.get_concrete_function(\r\n    x = tf.TensorSpec(shape=[batch_size,1,128], dtype=tf.float32), \r\n    states = tf.TensorSpec(shape=[batch_size,1,128, 2], dtype=tf.float32))\r\nexport_dir = '/tmp/saved_model'\r\nmodel.save(export_dir, save_format=\"tf\", signatures=concrete_func)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n```", "With adding ```unroll=True``` for the LSTM the interpreter is returning. But what is the reason?", "It seems that an empty session initializer is being carried to TFLite converter. Will fix this problem soon.", "@breizhn could you try the conversion with tomorrow's tf-nightly?", "@breizhn i have met the same problem. do u solve it?", "@abattery I will try it tomorrow!\r\n\r\n@arcral, with ```unroll=True``` it works fine for me.", "@abattery: With `tf-nightly-cpu==2.4.0-dev20200703`it works without `unroll=True`.\r\n\r\nAnother thing, I noticed, that the current version of the tf-lite runtime does not support converted `Dense` layer from conversion with tf-nightly. But loading with the builtin Interpreter works. \r\nThe error message with the current runtime is following:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_tf_lite.py\", line 113, in <module>\r\n    interpreter = tflite.Interpreter(model_path='./test_model.tflite')\r\n  File \"/Applications/anaconda3/envs/tfnightlyenv/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 204, in __init__\r\n    model_path, self._custom_op_registerers))\r\nValueError: Didn't find op for builtin opcode 'FULLY_CONNECTED' version '9'\r\nRegistration failed.\r\n```\r\n\r\nFor me it looks like a version mismatch. Will the runtime be updated soon? Or is there a flag for downwards compatibility? Maybe this is another issue.", "If you use the conversion with tf-nightly, the runtime version should be matched with the converter version to align operation versions. Could you use the interpreter of the same version with the converter?", "Where can I download a more current version of the TF-lite runtime. At [https://www.tensorflow.org/lite/guide/python](https://www.tensorflow.org/lite/guide/python) there is only a 2.1 version. \r\n\r\nThe Interpreter from the tf-nightly package is working."]}, {"number": 41022, "title": "ModuleNotFoundError: No module named 'tensorflow.compiler.tf2tensorrt'", "body": "Hello.\r\n\r\nI have created a small python application using tensorflow 2.2.0 and can run it with no problem using python 3.7.4\r\nThen I use pyinstaller to make a package of my project. \r\nBut when I try to run pyinstaller compiled version, I get this error (line 14 is \"import tensorflow as tf\").\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_wx1.py\", line 14, in <module>\r\n  File \"/usr/local/lib/python3.7/dist-packages/PyInstaller/loader/pyimod03_importers.py\", line 623, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/__init__.py\", line 41, in <module>\r\n  File \"/usr/local/lib/python3.7/dist-packages/PyInstaller/loader/pyimod03_importers.py\", line 623, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/python/__init__.py\", line 74, in <module>\r\n  File \"/usr/local/lib/python3.7/dist-packages/PyInstaller/loader/pyimod03_importers.py\", line 623, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/python/ops/standard_ops.py\", line 117, in <module>\r\n  File \"/usr/local/lib/python3.7/dist-packages/PyInstaller/loader/pyimod03_importers.py\", line 623, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/python/compiler/tensorrt/__init__.py\", line 22, in <module>\r\n  File \"/usr/local/lib/python3.7/dist-packages/PyInstaller/loader/pyimod03_importers.py\", line 623, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/python/compiler/tensorrt/trt_convert.py\", line 79, in <module>\r\n  File \"tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n  File \"tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n  File \"importlib/__init__.py\", line 127, in import_module\r\nModuleNotFoundError: No module named 'tensorflow.compiler.tf2tensorrt'\r\n```\r\n\r\nI cannot find any information Googling, so maybe you could help? \r\nOr maybe this is solely pyinstaller's responsibility and I should ask them?", "comments": ["@LumaRay,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here.\r\n\r\nAlso, please check [this thread](https://github.com/pyinstaller/pyinstaller/issues/4400) and let us know if it helps. Thanks!", "@amahendrakar,\r\nWell, I tried narrowing down my code and came to a conclusion that in order to reproduce you need just to create any python file and put a single line in it:\r\n\r\n`import tensorflow as tf`\r\n\r\nSorry, I could not make any use of the link you gave me.\r\n\r\nI use PyInstaller 3.6 and setuptools 44.1.1", "I'm having the same issue using PyInstaller 3.6 and setuptools 49.2.0", "In my case, I am able to make it work by creating custom hook file for tensorflow as below:\r\n\r\ntest.py -> [test.zip](https://github.com/tensorflow/tensorflow/files/4987535/test.zip)\r\nhook-tensorflow.py -> [hook-tensorflow.zip](https://github.com/tensorflow/tensorflow/files/4987536/hook-tensorflow.zip)\r\n\r\n- Copy hook-tensorflow.py to ./hooks/\r\n- pyinstaller test.py --additional-hooks-dir ./hooks/", "> In my case, I am able to make it work by creating custom hook file for tensorflow as below:\r\n> \r\n> test.py -> [test.zip](https://github.com/tensorflow/tensorflow/files/4987535/test.zip)\r\n> hook-tensorflow.py -> [hook-tensorflow.zip](https://github.com/tensorflow/tensorflow/files/4987536/hook-tensorflow.zip)\r\n> \r\n> * Copy hook-tensorflow.py to ./hooks/\r\n> * pyinstaller test.py --additional-hooks-dir ./hooks/\r\n\r\nThank you so much for this. In my particular case, your hook fixed my issue!", "> In my case, I am able to make it work by creating custom hook file for tensorflow as below:\r\n> \r\n> test.py -> [test.zip](https://github.com/tensorflow/tensorflow/files/4987535/test.zip)\r\n> hook-tensorflow.py -> [hook-tensorflow.zip](https://github.com/tensorflow/tensorflow/files/4987536/hook-tensorflow.zip)\r\n> \r\n> * Copy hook-tensorflow.py to ./hooks/\r\n> * pyinstaller test.py --additional-hooks-dir ./hooks/\r\n\r\nI have the same problem on Mac OSX but when I run ``pip install pyinstaller`` my antivirus reported a malware called \"win32.exe\" and the installation was aborted. So I can't install pyinstaller and I can't fix this problem by this way.", "I had the same problem on CentOS 7 where line 12 in pfexplorer.py  is \"import tensorflow as tf\".\r\nTensorflow 2.3.0 (CPU & GPU)\r\nPython 3.6.9\r\nCUDA10.1\r\nTensorRT6.0.1.5\r\n\r\n\r\n```\r\n/usr/local/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py:493: MatplotlibDeprecationWarning: \r\nThe MATPLOTLIBDATA environment variable was deprecated in Matplotlib 3.1 and will be removed in 3.3.\r\n  exec(bytecode, module.__dict__)\r\n2020-10-21 15:10:35.535785: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 3, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"src/launcher.py\", line 23, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"src/pfexplorer.py\", line 12, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/__init__.py\", line 41, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/python/__init__.py\", line 47, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/python/keras/__init__.py\", line 27, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/python/keras/models.py\", line 27, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/python/keras/engine/sequential.py\", line 27, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/python/keras/layers/__init__.py\", line 122, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/python/keras/layers/core.py\", line 45, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/python/keras/layers/ops/core.py\", line 26, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/python/ops/standard_ops.py\", line 117, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/python/compiler/tensorrt/__init__.py\", line 22, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow/python/compiler/tensorrt/trt_convert.py\", line 79, in <module>\r\n  File \"tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n  File \"tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n  File \"importlib/__init__.py\", line 126, in import_module\r\nModuleNotFoundError: No module named 'tensorflow.compiler.tf2tensorrt'\r\n[393] Failed to execute script test\r\n\r\n\r\n```\r\n\r\n", "> In my case, I am able to make it work by creating custom hook file for tensorflow as below:\r\n> \r\n> test.py -> [test.zip](https://github.com/tensorflow/tensorflow/files/4987535/test.zip)\r\n> hook-tensorflow.py -> [hook-tensorflow.zip](https://github.com/tensorflow/tensorflow/files/4987536/hook-tensorflow.zip)\r\n> \r\n> * Copy hook-tensorflow.py to ./hooks/\r\n> * pyinstaller test.py --additional-hooks-dir ./hooks/\r\n\r\nCould you explain a little more on the hook file? I dont have much experience with that.  Thank you so much.", "@LumaRay \r\nIs this still an issue ? It looks like you are using an older Version of Tensorflow . Can you please try to  use Latest Version 2.7.0 and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41021, "title": "Error when saving weights in h5 format for layer with nested layers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nGiven a layer with nested layers, when trying to save weights in h5 format, fails with ```RuntimeError: Unable to create link (name already exists)```\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras import datasets, layers, models\r\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\r\n\r\nclass NestedLayers(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super(NestedLayers, self).__init__()\r\n        self.units = [layers.Conv2D(16, (3,3), name=\"conv_2d_0\"),\r\n                      layers.Conv2D(16, (3,3), name=\"conv_2d_1\"),\r\n                      layers.Conv2D(16, (3,3), name=\"conv_2d_2\")]\r\n\r\n    def build(self, input_shape):\r\n        for i in range(0,2):\r\n            unit_input_shape = list(input_shape)\r\n            unit_input_shape[-1] = 1\r\n            unit = self.units[i]\r\n            unit.build(unit_input_shape)\r\n\r\n    def call(self, inputs):\r\n        split_inputs = tf.split(value=inputs,\r\n                                 num_or_size_splits=3,\r\n                                 axis=-1,\r\n                                 name=\"conv_grp_split\")\r\n        outputs = []\r\n        for i in range(0,2):\r\n            out = self.units[i](split_inputs[i])\r\n            outputs.append(out)\r\n        out = tf.keras.layers.concatenate(outputs, axis=-1, name=\"conv_grp_concat\")\r\n        return  out\r\n\r\n# Normalize pixel values to be between 0 and 1\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\n\r\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\r\n               'dog', 'frog', 'horse', 'ship', 'truck']\r\n\r\nmodel = models.Sequential()\r\nmodel.add(layers.InputLayer(input_shape=(32, 32, 3)))\r\nmodel.add(NestedLayers())\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(10, activation='softmax'))\r\nmodel.summary()\r\n\r\nimport datetime\r\nlog_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n\r\ncheck_pt = tf.keras.callbacks.ModelCheckpoint(\r\n                os.path.join(log_dir, \"model.ckpt.{epoch:04d}-{val_loss:.06f}.hdf5\"),\r\n                monitor='val_loss',\r\n                verbose=1,\r\n                save_best_only=False,\r\n                save_weights_only=True,\r\n                mode='max',\r\n                period=1)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nhistory = model.fit(train_images, train_labels, epochs=2,\r\n                    validation_data=(test_images, test_labels),\r\n                    callbacks = [check_pt])\r\n\r\n```\r\n\r\nFails with the following:\r\nEpoch 00001: saving model to logs/fit/20200702-114230/model.ckpt.0001-1.949398.hdf5\r\nTraceback (most recent call last):\r\n  File \"/nfs/site/home/tkrimer/work/mbe/dlo/src/internal_utils/train_cifar10_tf_2/nested_layers_save.py\", line 78, in <module>\r\n    callbacks = [tensorboard_callback, check_pt])\r\n  File \"/localdrive/users/tkrimer/venv/tf_2.1/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/localdrive/users/tkrimer/venv/tf_2.1/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 876, in fit\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/localdrive/users/tkrimer/venv/tf_2.1/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\", line 365, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/localdrive/users/tkrimer/venv/tf_2.1/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\", line 1177, in on_epoch_end\r\n    self._save_model(epoch=epoch, logs=logs)\r\n  File \"/localdrive/users/tkrimer/venv/tf_2.1/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\", line 1223, in _save_model\r\n    self.model.save_weights(filepath, overwrite=True)\r\n  File \"/localdrive/users/tkrimer/venv/tf_2.1/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 1151, in save_weights\r\n    hdf5_format.save_weights_to_hdf5_group(f, self.layers)\r\n  File \"/localdrive/users/tkrimer/venv/tf_2.1/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 639, in save_weights_to_hdf5_group\r\n    param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)\r\n  File \"/localdrive/users/tkrimer/venv/tf_2.1/lib/python3.7/site-packages/h5py/_hl/group.py\", line 139, in create_dataset\r\n    self[name] = dset\r\n  File \"/localdrive/users/tkrimer/venv/tf_2.1/lib/python3.7/site-packages/h5py/_hl/group.py\", line 373, in __setitem__\r\n    h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)\r\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/h5o.pyx\", line 202, in h5py.h5o.link\r\nRuntimeError: Unable to create link (name already exists)\r\n", "comments": ["@louisgriffin \r\nI ran the code shared above on tf 2.2 and do not face any error, can you please refer to [this gist](https://colab.research.google.com/gist/Saduf2019/bebe8742e76c3ceab03fbc86f6dc167e/untitled253.ipynb).\r\n\r\nAs per the error faced, please refer to [this link](https://www.bountysource.com/issues/45759314-python-3-string-error)  [link1](https://github.com/tensorflow/tensorflow/issues/27688) [link2](https://github.com/uber/ludwig/issues/210#issuecomment-582292921) [link3](https://stackoverflow.com/questions/59187054/runtimeerror-unable-to-create-link-name-already-exists-when-i-append-hdf5-fil)", "> @louisgriffin\r\n> I ran the code shared above on tf 2.2 and do not face any error, can you please refer to [this gist](https://colab.research.google.com/gist/Saduf2019/bebe8742e76c3ceab03fbc86f6dc167e/untitled253.ipynb).\r\n> \r\n> As per the error faced, please refer to [this link](https://www.bountysource.com/issues/45759314-python-3-string-error) [link1](https://github.com/tensorflow/tensorflow/issues/27688) [link2](https://github.com/uber/ludwig/issues/210#issuecomment-582292921) [link3](https://stackoverflow.com/questions/59187054/runtimeerror-unable-to-create-link-name-already-exists-when-i-append-hdf5-fil)\r\n\r\n@Saduf2019 \r\nI tried the suggestions in the links, like switching to v2 optimizer - it still does not work. It does have to do with eager_execution, because once eager_execution is disabled saving at check point succeeds", "@louisgriffin \r\nCan you please share a colab gist with the error faced.", "@Saduf2019 I added `model.save_weights(\"nested_layers.h5\")` to the [gist](https://colab.research.google.com/gist/Saduf2019/bebe8742e76c3ceab03fbc86f6dc167e/untitled253.ipynb) and it fails with the same error `Unable to create link, name already exists\"", "@louisgriffin \r\nPlease refer to below issues with similar error:\r\n#27688 [link](https://github.com/uber/ludwig/issues/210#issuecomment-582292921) [link1](https://stackoverflow.com/questions/59187054/runtimeerror-unable-to-create-link-name-already-exists-when-i-append-hdf5-fil) ", "> @louisgriffin\r\n> I ran the code shared above on tf 2.2 and do not face any error, can you please refer to [this gist](https://colab.research.google.com/gist/Saduf2019/bebe8742e76c3ceab03fbc86f6dc167e/untitled253.ipynb).\r\n> \r\n> As per the error faced, please refer to [this link](https://www.bountysource.com/issues/45759314-python-3-string-error) [link1](https://github.com/tensorflow/tensorflow/issues/27688) [link2](https://github.com/uber/ludwig/issues/210#issuecomment-582292921) [link3](https://stackoverflow.com/questions/59187054/runtimeerror-unable-to-create-link-name-already-exists-when-i-append-hdf5-fil)\r\n\r\n@Saduf2019 I investigated some more, and even though you don't see the error, are the actual files of for the h5 checkpoint being created? For me they are not created", "@Saduf2019 here is the a scenario where it fails even in the notebook:\r\n```python\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras import datasets, layers, models\r\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\r\n\r\nclass NestedLayers(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super(NestedLayers, self).__init__()\r\n        self.units = [layers.Conv2D(16, (3,3), name=\"conv_2d_0\"),\r\n                      layers.Conv2D(16, (3,3), name=\"conv_2d_1\"),\r\n                      layers.Conv2D(16, (3,3), name=\"conv_2d_2\")]\r\n\r\n    def build(self, input_shape):\r\n        for i in range(0,2):\r\n            unit_input_shape = list(input_shape)\r\n            unit_input_shape[-1] = 1\r\n            unit = self.units[i]\r\n            unit.build(unit_input_shape)\r\n\r\n    def call(self, inputs):\r\n        split_inputs = tf.split(value=inputs,\r\n                                 num_or_size_splits=3,\r\n                                 axis=-1,\r\n                                 name=\"conv_grp_split\")\r\n        outputs = []\r\n        for i in range(0,2):\r\n            out = self.units[i](split_inputs[i])\r\n            outputs.append(out)\r\n        out = tf.keras.layers.concatenate(outputs, axis=-1, name=\"conv_grp_concat\")\r\n        return  out\r\n\r\n# Normalize pixel values to be between 0 and 1\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\n\r\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\r\n               'dog', 'frog', 'horse', 'ship', 'truck']\r\n\r\nmodel = models.Sequential()\r\nmodel.add(layers.InputLayer(input_shape=(32, 32, 3)))\r\nmodel.add(NestedLayers())\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(10, activation='softmax'))\r\nmodel.summary()\r\n\r\nimport datetime\r\n\r\ncheck_pt = tf.keras.callbacks.ModelCheckpoint(\r\n                os.path.join(\"model.ckpt.{epoch:04d}-{val_loss:.06f}.hdf5\"),\r\n                monitor='val_loss',\r\n                verbose=1,\r\n                save_best_only=False,\r\n                save_weights_only=True,\r\n                mode='max',\r\n                period=1)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nhistory = model.fit(train_images, train_labels, epochs=2,\r\n                    validation_data=(test_images, test_labels),\r\n                    callbacks = [check_pt])\r\n\r\n```\r\n", "@Saduf2019 digging deeper into the issue, the main culprit is the fact that once _eager_execution_ is enabled NestedLayer's weights, that come from the nested layers within, are not named uniquely which leads to conflicts when trying to save it in h5 format. ", "@louisgriffin Was able to reproduce this error in Tf 2.2 and tf-nightly. Please find the gist [here](https://colab.research.google.com/gist/gowthamkpr/32c46e935e88bed1a8fd5f88b36a2427/untitled297.ipynb)", "@k-w-w do you have any suggestions as per workaround? ", "@k-w-w I would really appreciate some guidance/workaround for the main culprit here, which is weights in the NestedLayer are not named uniquely. It is becoming a problem, not only with the h5 format, but in general.", "I was facing this issue when upgraded to tf 2.3. I tried reinstalling h5py etc. none of them worked. Then, I reverted to tf 2.2 and the issue was resolved. no idea how.", "I found a workaround:\r\nForcing the internal layers weights to be named uniquely with a name scope. Check this out:\r\n``` python\r\nclass NestedLayers(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super(NestedLayers, self).__init__()\r\n        self.units = [layers.Conv2D(16, (3,3), name=\"conv_2d_0\"),\r\n                      layers.Conv2D(16, (3,3), name=\"conv_2d_1\"),\r\n                      layers.Conv2D(16, (3,3), name=\"conv_2d_2\")]\r\n\r\n    def build(self, input_shape):\r\n        for i in range(0,2):\r\n            unit_input_shape = list(input_shape)\r\n            unit_input_shape[-1] = 1\r\n            unit = self.units[i]\r\n            with tf.name_scope(\"BUILD_{}\".format(i)):\r\n               unit.build(unit_input_shape)\r\n\r\n    def call(self, inputs):\r\n        split_inputs = tf.split(value=inputs,\r\n                                 num_or_size_splits=3,\r\n                                 axis=-1,\r\n                                 name=\"conv_grp_split\")\r\n        outputs = []\r\n        for i in range(0,2):\r\n            out = self.units[i](split_inputs[i])\r\n            outputs.append(out)\r\n        out = tf.keras.layers.concatenate(outputs, axis=-1, name=\"conv_grp_concat\")\r\n        return  out\r\n```\r\nAlso, here is a [gist](https://colab.research.google.com/drive/1DPmCmde9JAuFc0TNuBMG99CbW3Og4hJr?usp=sharing)", "I also had this same issue when creating a model from an existing one the following way \r\n```\r\ndenoise_model_name = \"./data_model_1_00000220.h5\"\r\ndenoise_model = tf.keras.models.load_model(denoise_model_name)\r\n\r\nkeras_input = tf.keras.Input(shape=(44, 100, 3))\r\nx = denoise_model(keras_input)\r\nx = layers.Conv2D(64, kernel_size=(3, 3),\r\n                      activation='relu',\r\n                      kernel_initializer='he_uniform', padding='same')(x)\r\n```\r\n\r\nAnd adding a name to the new Conv2D layer also solved it", "I also experienced this issue in tf 2.3.1 when saving models repeatedly during a run, also when a model was constructed and compiled anew during an evaluation loop. This comes also with the caveat that this happened using mlflow.keras.log_model(). Nevertheless this calls tf.keras.models.save_model().\r\n\r\nFor me it helped to name all layers and also add randomized integers to the layer names whenever a model was constructed anew.", "Similar bug still exists in TensorFlow 2.4.0.", "> Similar bug still exists in TensorFlow 2.4.0.\r\n\r\nDid your problem get solved. I am facing the same issue.\r\n", "> > Similar bug still exists in TensorFlow 2.4.0.\r\n> \r\n> Did your problem get solved. I am facing the same issue.\r\n\r\nNope. I am using TensorFlow 2.2.2 which works fine.", "> I also experienced this issue in tf 2.3.1 when saving models repeatedly during a run, also when a model was constructed and compiled anew during an evaluation loop. This comes also with the caveat that this happened using mlflow.keras.log_model(). Nevertheless this calls tf.keras.models.save_model().\r\n> \r\n> For me it helped to name all layers and also add randomized integers to the layer names whenever a model was constructed anew.\r\n\r\nCan you explain this with the help of code. I tried similar but still facing the issue.", "The same error can be obtained this way:\r\n1) save an Adam optimizer in a Python variable (such that the same instance can be used again later) and use it to compile a Keras model\r\n2) call `fit()` on the model\r\n3) instantiate a Keras model (or load it from a .h5 file), and compile it using the same instance of Adam optimizer as before, the instance saved in a variable at point 1)\r\n4) call `fit()` on the second model, the one instantiated at point 3)\r\n5) save the second model as a .h5 file, you get the error **RuntimeError: Unable to create link (name already exists)**\r\n\r\nWhat happens is that after step 4), the Adam optimizer instance has in its `weights` attribute a list of 9 TF variables, instead of 5. Four of the variables have been duplicated, keeping the same respective name. When trying to save the mode in .h5 format, the `tf.keras.models.save_model()` function finds two variables with duplicated name `Adam/dense/kernel/m:0`, and stops with an error.\r\n\r\nIn this case, the error can be resolved by ensuring the same instance of Adam optimizer is not used to compile more than one model.", "This is annoying !!! Bugs came back after bugs. TF2 is terribly managing changes.", "@louisgriffin \r\nIt looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version (2.7.0) and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41021\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41021\">No</a>\n"]}, {"number": 41020, "title": "Failed to build 1.15.3 release with GCC9.3", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):   https://github.com/tensorflow/tensorflow/archive/v1.15.3.zip\r\n- TensorFlow version: 1.15.3\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): GCC 9.3\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\nBuild failed with error message:\r\n```\r\nexternal/grpc/src/core/lib/gpr/log_linux.cc:43:13: error: ambiguating new declaration of 'long int gettid()'\r\n   43 | static long gettid(void) { return syscall(__NR_gettid); }\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nbazel --output_user_root=$build_dir build --config=mkl --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mavx512f --copt=-mavx512cd  //tensorflow/tools/pip_package:build_pip_package 2>&1 | tee /root/build.log\r\n```\r\n\r\nRefer to this issue: https://github.com/grpc/grpc/issues/20043\r\nMay need to upgrade the grpc version.\r\n", "comments": ["We need a PR to upgrade `grpc` version. Can you send it please?", "Oh, this is against `r1.15`. That branch is fixed, we only update it for security reasons.", "@mihaimaruseac Thanks for the feed-back. I see there is a release of r1.15.3(https://github.com/tensorflow/tensorflow/releases/tag/v1.15.3) last monthly. If we still plan some release based on r1.15, may be we can submit a PR to that branch then?", "If `grpc` version bump covers a security vuln yes, we can accept it. Otherwise, unfortunately, we cannot justify a version bump.", "@mihaimaruseac Thanks for the reply. I don't think there is a security issue. So I will close this issue ticket.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41020\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41020\">No</a>\n"]}, {"number": 41018, "title": "error: \u2018tensorflow::error\u2019 has not been declared----Status(tensorflow::error::Code code, tensorflow::StringPiece msg);", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution : Linux Ubuntu 16.04\r\n- TensorFlow installed from :    source\r\n- TensorFlow version :  1.14\r\n- Python version:   3.5\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version :  5.4.0 20160609\r\n- CUDA/cuDNN version: 10.0/7.6.5\r\n- GPU model and memory:  GF1080 TI\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am a newer to c++, I want to accomplish CNN model with c++ tensorflow API. I follow the manual on the Internet and finishe tensorflow compiling and  generate the libtensorflow_cc.so file.   Now I want to build a test using cmake 3.13.0, I create a \"CMakeLists.txt\" and execute\r\n'''\r\ncmake .\r\nmake\r\n'''\r\n\r\nthe errors:\r\n\r\n'''\r\n[make[2]: Warning: File '/home/john/tensorflow/tensorflow/contrib/makefile/downloads/eigen/Eigen/src/Core/arch/NEON/Complex.h' has modification time 11068 s in the future\r\n[ 50%] Building CXX object CMakeFiles/tensorflow_cc_test.dir/hello.cpp.o\r\nIn file included from /home/john/tensorflow/tensorflow/core/lib/core/errors.h:21:0,\r\n                 from /home/john/tensorflow/tensorflow/core/platform/env.h:24,\r\n                 from /home/john/\u6587\u6863/tensorflow_cc_test/hello.cpp:1:\r\n/home/john/tensorflow/tensorflow/core/lib/core/status.h:45:22: error: \u2018tensorflow::error\u2019 has not been declared\r\n   Status(tensorflow::error::Code code, tensorflow::StringPiece msg);\r\n                      ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/status.h:45:34: error: expected \u2018)\u2019 before \u2018code\u2019\r\n   Status(tensorflow::error::Code code, tensorflow::StringPiece msg);\r\n                                  ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/status.h:56:15: error: \u2018error\u2019 in namespace \u2018tensorflow\u2019 does not name a type\r\n   tensorflow::error::Code code() const {\r\n               ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/status.h:90:17: error: \u2018error\u2019 in namespace \u2018tensorflow\u2019 does not name a type\r\n     tensorflow::error::Code code;\r\n                 ^\r\nIn file included from /home/john/tensorflow/tensorflow/core/platform/env.h:24:0,\r\n                 from /home/john/\u6587\u6863/tensorflow_cc_test/hello.cpp:1:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:30:23: error: \u2018error\u2019 in namespace \u2018tensorflow\u2019 does not name a type\r\n typedef ::tensorflow::error::Code Code;\r\n                       ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018void tensorflow::errors::AppendToMessage(tensorflow::Status*, Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:65:15: error: \u2018class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n       status->code(),\r\n               ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018tensorflow::Status tensorflow::errors::Cancelled(Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:95:9: error: \u2018error\u2019 is not a member of \u2018tensorflow\u2019\r\n         ::tensorflow::error::CONST,                                      \\\r\n         ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:103:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Cancelled, CANCELLED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018bool tensorflow::errors::IsCancelled(const tensorflow::Status&)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:19: error: \u2018const class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                   ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:103:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Cancelled, CANCELLED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:43: error: \u2018tensorflow::error\u2019 has not been declared\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                                           ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:103:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Cancelled, CANCELLED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018tensorflow::Status tensorflow::errors::InvalidArgument(Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:95:9: error: \u2018error\u2019 is not a member of \u2018tensorflow\u2019\r\n         ::tensorflow::error::CONST,                                      \\\r\n         ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:104:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(InvalidArgument, INVALID_ARGUMENT)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018bool tensorflow::errors::IsInvalidArgument(const tensorflow::Status&)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:19: error: \u2018const class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                   ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:104:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(InvalidArgument, INVALID_ARGUMENT)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:43: error: \u2018tensorflow::error\u2019 has not been declared\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                                           ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:104:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(InvalidArgument, INVALID_ARGUMENT)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018tensorflow::Status tensorflow::errors::NotFound(Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:95:9: error: \u2018error\u2019 is not a member of \u2018tensorflow\u2019\r\n         ::tensorflow::error::CONST,                                      \\\r\n         ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:105:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(NotFound, NOT_FOUND)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018bool tensorflow::errors::IsNotFound(const tensorflow::Status&)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:19: error: \u2018const class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                   ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:105:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(NotFound, NOT_FOUND)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:43: error: \u2018tensorflow::error\u2019 has not been declared\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                                           ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:105:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(NotFound, NOT_FOUND)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018tensorflow::Status tensorflow::errors::AlreadyExists(Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:95:9: error: \u2018error\u2019 is not a member of \u2018tensorflow\u2019\r\n         ::tensorflow::error::CONST,                                      \\\r\n         ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:106:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(AlreadyExists, ALREADY_EXISTS)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018bool tensorflow::errors::IsAlreadyExists(const tensorflow::Status&)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:19: error: \u2018const class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                   ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:106:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(AlreadyExists, ALREADY_EXISTS)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:43: error: \u2018tensorflow::error\u2019 has not been declared\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                                           ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:106:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(AlreadyExists, ALREADY_EXISTS)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018tensorflow::Status tensorflow::errors::ResourceExhausted(Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:95:9: error: \u2018error\u2019 is not a member of \u2018tensorflow\u2019\r\n         ::tensorflow::error::CONST,                                      \\\r\n         ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:107:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(ResourceExhausted, RESOURCE_EXHAUSTED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018bool tensorflow::errors::IsResourceExhausted(const tensorflow::Status&)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:19: error: \u2018const class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                   ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:107:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(ResourceExhausted, RESOURCE_EXHAUSTED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:43: error: \u2018tensorflow::error\u2019 has not been declared\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                                           ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:107:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(ResourceExhausted, RESOURCE_EXHAUSTED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018tensorflow::Status tensorflow::errors::Unavailable(Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:95:9: error: \u2018error\u2019 is not a member of \u2018tensorflow\u2019\r\n         ::tensorflow::error::CONST,                                      \\\r\n         ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:108:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Unavailable, UNAVAILABLE)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018bool tensorflow::errors::IsUnavailable(const tensorflow::Status&)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:19: error: \u2018const class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                   ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:108:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Unavailable, UNAVAILABLE)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:43: error: \u2018tensorflow::error\u2019 has not been declared\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                                           ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:108:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Unavailable, UNAVAILABLE)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018tensorflow::Status tensorflow::errors::FailedPrecondition(Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:95:9: error: \u2018error\u2019 is not a member of \u2018tensorflow\u2019\r\n         ::tensorflow::error::CONST,                                      \\\r\n         ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:109:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(FailedPrecondition, FAILED_PRECONDITION)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018bool tensorflow::errors::IsFailedPrecondition(const tensorflow::Status&)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:19: error: \u2018const class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                   ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:109:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(FailedPrecondition, FAILED_PRECONDITION)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:43: error: \u2018tensorflow::error\u2019 has not been declared\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                                           ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:109:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(FailedPrecondition, FAILED_PRECONDITION)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018tensorflow::Status tensorflow::errors::OutOfRange(Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:95:9: error: \u2018error\u2019 is not a member of \u2018tensorflow\u2019\r\n         ::tensorflow::error::CONST,                                      \\\r\n         ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:110:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(OutOfRange, OUT_OF_RANGE)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018bool tensorflow::errors::IsOutOfRange(const tensorflow::Status&)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:19: error: \u2018const class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                   ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:110:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(OutOfRange, OUT_OF_RANGE)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:43: error: \u2018tensorflow::error\u2019 has not been declared\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                                           ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:110:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(OutOfRange, OUT_OF_RANGE)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018tensorflow::Status tensorflow::errors::Unimplemented(Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:95:9: error: \u2018error\u2019 is not a member of \u2018tensorflow\u2019\r\n         ::tensorflow::error::CONST,                                      \\\r\n         ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:111:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Unimplemented, UNIMPLEMENTED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018bool tensorflow::errors::IsUnimplemented(const tensorflow::Status&)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:19: error: \u2018const class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                   ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:111:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Unimplemented, UNIMPLEMENTED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:43: error: \u2018tensorflow::error\u2019 has not been declared\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                                           ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:111:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Unimplemented, UNIMPLEMENTED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018tensorflow::Status tensorflow::errors::Internal(Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:95:9: error: \u2018error\u2019 is not a member of \u2018tensorflow\u2019\r\n         ::tensorflow::error::CONST,                                      \\\r\n         ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:112:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Internal, INTERNAL)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018bool tensorflow::errors::IsInternal(const tensorflow::Status&)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:19: error: \u2018const class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                   ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:112:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Internal, INTERNAL)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:43: error: \u2018tensorflow::error\u2019 has not been declared\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                                           ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:112:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Internal, INTERNAL)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018tensorflow::Status tensorflow::errors::Aborted(Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:95:9: error: \u2018error\u2019 is not a member of \u2018tensorflow\u2019\r\n         ::tensorflow::error::CONST,                                      \\\r\n         ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:113:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Aborted, ABORTED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018bool tensorflow::errors::IsAborted(const tensorflow::Status&)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:19: error: \u2018const class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                   ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:113:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Aborted, ABORTED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:43: error: \u2018tensorflow::error\u2019 has not been declared\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                                           ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:113:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Aborted, ABORTED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018tensorflow::Status tensorflow::errors::DeadlineExceeded(Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:95:9: error: \u2018error\u2019 is not a member of \u2018tensorflow\u2019\r\n         ::tensorflow::error::CONST,                                      \\\r\n         ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:114:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(DeadlineExceeded, DEADLINE_EXCEEDED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018bool tensorflow::errors::IsDeadlineExceeded(const tensorflow::Status&)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:19: error: \u2018const class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                   ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:114:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(DeadlineExceeded, DEADLINE_EXCEEDED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:43: error: \u2018tensorflow::error\u2019 has not been declared\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                                           ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:114:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(DeadlineExceeded, DEADLINE_EXCEEDED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018tensorflow::Status tensorflow::errors::DataLoss(Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:95:9: error: \u2018error\u2019 is not a member of \u2018tensorflow\u2019\r\n         ::tensorflow::error::CONST,                                      \\\r\n         ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:115:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(DataLoss, DATA_LOSS)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018bool tensorflow::errors::IsDataLoss(const tensorflow::Status&)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:19: error: \u2018const class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                   ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:115:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(DataLoss, DATA_LOSS)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:43: error: \u2018tensorflow::error\u2019 has not been declared\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                                           ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:115:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(DataLoss, DATA_LOSS)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018tensorflow::Status tensorflow::errors::Unknown(Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:95:9: error: \u2018error\u2019 is not a member of \u2018tensorflow\u2019\r\n         ::tensorflow::error::CONST,                                      \\\r\n         ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:116:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Unknown, UNKNOWN)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018bool tensorflow::errors::IsUnknown(const tensorflow::Status&)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:19: error: \u2018const class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                   ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:116:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Unknown, UNKNOWN)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:43: error: \u2018tensorflow::error\u2019 has not been declared\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                                           ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:116:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Unknown, UNKNOWN)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018tensorflow::Status tensorflow::errors::PermissionDenied(Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:95:9: error: \u2018error\u2019 is not a member of \u2018tensorflow\u2019\r\n         ::tensorflow::error::CONST,                                      \\\r\n         ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:117:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(PermissionDenied, PERMISSION_DENIED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018bool tensorflow::errors::IsPermissionDenied(const tensorflow::Status&)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:19: error: \u2018const class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                   ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:117:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(PermissionDenied, PERMISSION_DENIED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:43: error: \u2018tensorflow::error\u2019 has not been declared\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                                           ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:117:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(PermissionDenied, PERMISSION_DENIED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018tensorflow::Status tensorflow::errors::Unauthenticated(Args ...)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:95:9: error: \u2018error\u2019 is not a member of \u2018tensorflow\u2019\r\n         ::tensorflow::error::CONST,                                      \\\r\n         ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:118:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Unauthenticated, UNAUTHENTICATED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: In function \u2018bool tensorflow::errors::IsUnauthenticated(const tensorflow::Status&)\u2019:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:19: error: \u2018const class tensorflow::Status\u2019 has no member named \u2018code\u2019\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                   ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:118:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Unauthenticated, UNAUTHENTICATED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:100:43: error: \u2018tensorflow::error\u2019 has not been declared\r\n     return status.code() == ::tensorflow::error::CONST;                  \\\r\n                                           ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:118:1: note: in expansion of macro \u2018DECLARE_ERROR\u2019\r\n DECLARE_ERROR(Unauthenticated, UNAUTHENTICATED)\r\n ^\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h: At global scope:\r\n/home/john/tensorflow/tensorflow/core/lib/core/errors.h:158:21: error: \u2018tensorflow::error\u2019 has not been declared\r\n using ::tensorflow::error::OK;\r\n                     ^\r\nCMakeFiles/tensorflow_cc_test.dir/build.make:62: recipe for target 'CMakeFiles/tensorflow_cc_test.dir/hello.cpp.o' failed\r\nmake[2]: *** [CMakeFiles/tensorflow_cc_test.dir/hello.cpp.o] Error 1\r\nCMakeFiles/Makefile2:72: recipe for target 'CMakeFiles/tensorflow_cc_test.dir/all' failed\r\nmake[1]: *** [CMakeFiles/tensorflow_cc_test.dir/all] Error 2\r\nMakefile:83: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n'''\r\n\r\nI can not search Internet for anythin about this, can somebody help?\r\n", "comments": ["I forgot to submit the CMakeLists Info:\r\n'''\r\ncmake_minimum_required(VERSION 3.10)\r\nproject(tensorflow_cc_test)\r\nset(CMAKE_CXX_STANDARD 11)\r\nset(TENSORFLOW_DIR /home/john/tensorflow)\r\ninclude_directories(${TENSORFLOW_DIR})\r\ninclude_directories(${TENSORFLOW_DIR}/tensorflow/contrib/makefile/downloads/absl)\r\ninclude_directories(${TENSORFLOW_DIR}/tensorflow/contrib/makefile/downloads/protobuf/src)\r\ninclude_directories(${TENSORFLOW_DIR}/tensorflow/contrib/makefile/gen/proto)\r\ninclude_directories(${TENSORFLOW_DIR}/tensorflow/contrib/makefile/gen/protobuf-host/include)\r\ninclude_directories(${TENSORFLOW_DIR}/tensorflow/contrib/makefile/downloads)\r\ninclude_directories(${TENSORFLOW_DIR}/tensorflow/contrib/makefile/downloads/eigen)\r\ninclude_directories(${TENSORFLOW_DIR}/tensorflow/contrib/makefile/downloads/nsync/public)\r\n\r\nlink_directories(${TENSORFLOW_DIR}/tensorflow/contrib/makefile/gen/lib)\r\nlink_directories(${TENSORFLOW_DIR}/tensorflow/contrib/makefile/gen/protobuf-host/lib)\r\nlink_directories(${TENSORFLOW_DIR}/tensorflow/contrib/makefile/downloads/nsync/builds/default.linux.c++11)\r\nlink_directories(${TENSORFLOW_DIR}/bazel-bin/tensorflow)\r\n\r\nadd_executable(tensorflow_cc_test hello.cpp)\r\ntarget_link_libraries(tensorflow_cc_test tensorflow_cc tensorflow_framework)\r\n'''\r\n", "I add a declare \"class tensorflow::error::Code;\" , the errors disappear.", "@Johnny19-91 \r\nPlease confirm if the issue is resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41018\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41018\">No</a>\n", "Hi @Johnny19-91 \r\nI just came across the same build issue you described here. I'm unable to compile my project (linking to built libs) due to several erros on \"errors.h\" and \"status.h\" files, mainly related to this first one:\r\n`error: C2039: 'error' is not a member of 'tensorflow'`\r\n\r\nDo you mind explaining better how did you manage to get rid of this issue? \r\nI saw your comment about declaring the class \"class tensorflow::error::Code;\" but couldn't really get what/where you did. That is exactly my point of failure, \"error\" and \"error::Code\".\r\nOr maybe @Saduf2019 has more info about how this was solved?\r\n\r\nFollow my environment:\r\n\r\n**System information**\r\n\r\nI'm also using CMake and trying to link the generated DLLs to my source code (C++).\r\n**OS Platform and Distribution:** Windows 10 \r\n**TensorFlow installed from:** source\r\n**TensorFlow version:** 2.1.0\r\n**Python version:** 3.8.3 (x64 - using venv)\r\n**Bazel version:** 0.29.0\r\n**GCC/Compiler version:** MSVC 19.27.29111.0\r\n**CUDA/cuDNN version:** not used\r\n**GPU model and memory:** not used\r\n\r\nObs.: On my CMakeLists.txt I'm using \"find_library\" to get tensorflow.dll and tensorflow_cc.dll (not sure which one should I use, I bulit both using Bazel) and am also including the headers directory on \"target_include_directories($<BUILD_INTERFACE:>)\". Finally, I'm including the libraries found to \"target_link_libraries\"\r\n\r\n\r\nThanks in advance folks!\r\nCheers.", "sorry, I sign in the github and do not find the topic.\r\n\r\n\r\n\r\n---Original---\r\nFrom: \"Thales Elero Cervi\"<notifications@github.com&gt;\r\nDate: Sat, Nov 7, 2020 01:16 AM\r\nTo: \"tensorflow/tensorflow\"<tensorflow@noreply.github.com&gt;;\r\nCc: \"Mention\"<mention@noreply.github.com&gt;;\"Johnny19-91\"<lijianning0000@foxmail.com&gt;;\r\nSubject: Re: [tensorflow/tensorflow] error: \u2018tensorflow::error\u2019 has not been declared----Status(tensorflow::error::Code code, tensorflow::StringPiece msg); (#41018)\r\n\r\n\r\n\r\n\r\n \r\nHi @Johnny19-91\r\n I just came across the same build issue you described here. I'm unable to compile my project (linking to built libs) due to several erros on \"errors.h\" and \"status.h\" files, mainly related to this first one:\r\n error: C2039: 'error' is not a member of 'tensorflow'\r\n \r\nDo you mind explaining better how did you manage to get rid of this issue?\r\n I saw your comment about declaring the class \"class tensorflow::error::Code;\" but couldn't really get what/where you did. That is exactly my point of failure, \"error\" and \"error::Code\".\r\n Or maybe @Saduf2019 has more info about how this was solved?\r\n \r\nFollow my environment:\r\n \r\nSystem information\r\n \r\nI'm also using CMake and trying to link the generated DLLs to my source code (C++).\r\n OS Platform and Distribution: Windows 10\r\n TensorFlow installed from: source\r\n TensorFlow version: 2.1.0\r\n Python version: 3.8.3 (x64 - using venv)\r\n Bazel version: 0.29.0\r\n GCC/Compiler version: MSVC 19.27.29111.0\r\n CUDA/cuDNN version: not used\r\n GPU model and memory: not used\r\n \r\nObs.: On my CMakeLists.txt I'm using \"find_library\" to get tensorflow.dll and tensorflow_cc.dll (not sure which one should I use, I bulit both using Bazel) and am also including the headers directory on \"target_include_directories($<BUILD_INTERFACE:&gt;)\". Finally, I'm including the libraries found to \"target_link_libraries\"\r\n \r\nThanks in advance folks!\r\n Cheers.\r\n \r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or unsubscribe."]}, {"number": 41017, "title": "Cherry-pick dlpack fix #40843 into r2.3", "body": "Cherry-pick dlpack fix #40843 into r2.3", "comments": ["@azaks3 this is a relatively critical bugfix (it fixes an easy-to-reproduce memory leak) on a feature which is not used by our benchmarks / regression tests, so if we do merge this in we should not need to rerun all the release tests or do another RC. The community that uses this feature though considers it very important as it efficiently allows GPU arrays created from TF to be reused by other libraries and vice versa.\r\n\r\nSo I think this is a very low risk high reward cherry-pick. Should we do it?", "Should there be a unit test to make sure a deleter is called?\r\nLGTM for cherry-pick", "I couldn't figure out how to unit test but we manually verified the leak is gone."]}, {"number": 41016, "title": "Add the steps related to Pickling the Tokenization in Text Classification Tutorial", "body": "## URL(s) with the issue: https://www.tensorflow.org/tutorials/text/text_classification_rnn\r\n\r\n## Description of issue (what needs changing): \r\nCan include the Code related to Pickling the Tokenization and then Loading the Pickle File while predicting on the Loaded Saved Model.\r\n\r\n### Clear description : \r\nRecently, I have worked on a `Text Classification project`. Everything was good and the Model is `95%` Accurate. But when performing `Predictions` after `Loading the Saved Model`, and while performing Inference using `Tensorflow Serving`, **Model's Predictions were very poor**. After investigating for more than 10 days, we figured out that we need to **Save the Tokenizations in a Pickle File and then Load that Pickle File while Performing Predictions on a Loaded Saved Model or using TF Serving or using TF Lite**.\r\n\r\n### Code that can be added in the Documentation: \r\n\r\nBelow code needs to be added after the command, `tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)`\r\n\r\n\r\n```\r\nimport pickle\r\n\r\n# Saving the Tokenization in Pickle File\r\n\r\nwith open('tokenizer.pickle', 'wb') as handle:\r\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n\r\n```\r\n\r\nAlso, below code needs to be added before the command, `train_sequences = tokenizer2.texts_to_sequences(Train_Resume_Data)`\r\n\r\n```\r\n# Loading the Tokeniztion Pickle File\r\nwith open('tokenizer.pickle', 'rb') as handle:\r\n    tokenizer2 = pickle.load(handle)\r\n```\r\n\r\nThis will help all the Developers working on Text Classification projects or any other NLP Task. Thanks.", "comments": ["Hi, can I work on this issue?", "@yil532 Please feel free to open a PR to update this doc. Thanks!", "Thanks! Working on it", "I didn't find line of code `tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)` in the [link you shared](https://www.tensorflow.org/tutorials/text/text_classification_rnn) Could you be more specific where the issue exists?\r\n@rakeshmothukuru1 ", "@yil532,\r\nI'm surprised to see that Tokenization is not applied in [Text Classification Tutorial](https://www.tensorflow.org/tutorials/text/text_classification_rnn). Can someone clarify why Tokenization is not required in Text Classification Tutorial?\r\n\r\nSorry for the confusion. It's not present in [Text Classification Tutorial](https://www.tensorflow.org/tutorials/text/text_classification_rnn) but is present in [Image Captioning Tutorial](https://www.tensorflow.org/tutorials/text/image_captioning#preprocess_and_tokenize_the_captions). So this change can be applied in [Image Captioning Tutorial](https://www.tensorflow.org/tutorials/text/image_captioning#preprocess_and_tokenize_the_captions).", "Hi, we are actively working on a fix for this, using the new [text vectorization layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) which will make this less of a headache for everyone."]}, {"number": 41015, "title": "Add file block cache interface", "body": "@mihaimaruseac \r\nThis PR adds an interface for cache mechanism.", "comments": ["About the dependencies from `tensorflow/core`, it seems that `absl` provides the same functionalities so we dont need to worry"]}, {"number": 41014, "title": "tf.estimator.predict cannot run consecutively on Colab TPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab TPU\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Colab Jupyter Notebook\r\n- TensorFlow version (use command below):1-15.2\r\n- Python version: 3.6\r\n- Bert-tensorflow version: 1.0.1\r\n\r\n**Describe the current behavior**\r\nI don't know if this is a good place to report the tf.estimator.predict API bug. I am trying to use bert-tensorflow to do fine tuning. After the estimator.train, I used tf.estimator.predict to predict the test dataset twice in sequence with 2 checkpoints (both different or same checkpoints). The two predict calls are in 2 separate jupyter cells. For the first predict, it runs ok, the prediction probabilities are returned as expected. For the second predict, it stuck in the logging info \"Shutting down InfeedController thread\" and not move further. I've tried several method to make the 2nd predict run as the 1st one, but it just not works out. To provide more information, I just paste the log where the 2nd predict stuck:\r\n\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = output_weights:0, shape = (2, 1024)\r\n> INFO:tensorflow:  name = output_bias:0, shape = (2,)\r\n> INFO:tensorflow:Done calling model_fn.\r\n> INFO:tensorflow:TPU job name worker\r\n> INFO:tensorflow:Graph was finalized.\r\n> INFO:tensorflow:Restoring parameters from gs://sr2pr/liuyi_test_early_stop/early_stop_ckpts/model.ckpt-853\r\n> INFO:tensorflow:Running local_init_op.\r\n> INFO:tensorflow:Done running local_init_op.\r\n> INFO:tensorflow:Init TPU system\r\n> INFO:tensorflow:Initialized TPU in 0 seconds\r\n> INFO:tensorflow:Starting infeed thread controller.\r\n> INFO:tensorflow:Starting outfeed thread controller.\r\n> INFO:tensorflow:Initialized dataset iterators in 0 seconds\r\n> INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\r\n> INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\r\n> INFO:tensorflow:Outfeed finished for iteration (0, 0)\r\n> INFO:tensorflow:Stop infeed thread controller\r\n> INFO:tensorflow:Shutting down InfeedController thread.\r\n> \r\n\r\nI do know the estimator.predict returns a iterator/generator, which prevents interleaving calls as indicated in the official tensorflow document. I think my case is not interleaving operation? I don't know if this is caused by colab or the predict API? and any idea to work around this bug? \r\n\r\nCurrently, I have no way to run predict twice unless I shutdown the Colab, close the browser tab, upload the jupyter notebook, and reinitialize everything and configure everything again. \r\n\r\nAny possible discussions that may help are appreciated.\r\n", "comments": ["@liuyibox,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. \r\n\r\nAlternatively you can also share the gist of the Colab notebook you are running. Thanks!", "@amahendrakar \r\nThanks for your reply. The main body of calling the predict is as follows. In the test code below, the multiple checkpoints are stored in a list which are traversed in sequence. The problem is that: the first checkpoint can be used to predict smoothly, the second will stuck in \"Shutting down InfeedController thread\".  \r\n```\r\n  if FLAGS.do_test:\r\n\r\n    test_examples = processor.get_test_examples(FLAGS.data_dir)\r\n    test_labels, test_text = processor.get_test_labels(FLAGS.data_dir)\r\n    num_actual_test_examples = len(test_examples)\r\n    if FLAGS.use_tpu:\r\n      # TPU requires a fixed batch size for all batches, therefore the number\r\n      # of examples must be a multiple of the batch size, or else examples\r\n      # will get dropped. So we pad with fake examples which are ignored\r\n      # later on.\r\n      while len(test_examples) % FLAGS.predict_batch_size != 0:\r\n        test_examples.append(PaddingInputExample())\r\n    test_file = os.path.join(FLAGS.output_dir, \"test.tf_record\")\r\n    file_based_convert_examples_to_features(test_examples, label_list,\r\n                                            FLAGS.max_seq_length, tokenizer,\r\n                                            test_file)\r\n\r\n    tf.logging.info(\"***** Running test prediction*****\")\r\n    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\r\n                    len(test_examples), num_actual_test_examples,\r\n                    len(test_examples) - num_actual_test_examples)\r\n    tf.logging.info(\"  Batch size = %d\", FLAGS.predict_batch_size)\r\n\r\n    test_drop_remainder = True if FLAGS.use_tpu else False\r\n    test_input_fn = file_based_input_fn_builder(\r\n        input_file=test_file,\r\n        seq_length=FLAGS.max_seq_length,\r\n        is_training=False,\r\n        drop_remainder=test_drop_remainder)\r\n\r\n    test_epoch_list = []\r\n    test_result_file_list = []  \r\n\r\n    print('checkpoints path is designated to be ',FLAGS.output_dir)\r\n    print('num_train_steps: ', FLAGS.total_train_steps)\r\n    print('FLAGS.iterations_per_loop: ', FLAGS.iterations_per_loop)\r\n    ckpts = np.arange(0, FLAGS.total_train_steps, FLAGS.iterations_per_loop).tolist()\r\n    if ckpts[-1] != FLAGS.total_train_steps:\r\n      ckpts.append(FLAGS.total_train_steps) \r\n    print(\"checkpoints are: \", ckpts)  \r\n    for ckpt_idx, ckpt in zip(range(len(ckpts)), ckpts):\r\n      \r\n      print('========== This is to test {} =========='.format(os.path.join(FLAGS.output_dir,\"model.ckpt-\"+str(ckpt))))\r\n      test_ckpt_start = datetime.now()\r\n      result = estimator.predict(input_fn=test_input_fn, checkpoint_path = os.path.join(FLAGS.output_dir,\"model.ckpt-\"+str(ckpt)))\r\n      test_ckpt_time = datetime.now() - test_ckpt_start\r\n      test_epoch_list.append(test_ckpt_time)\r\n\r\n      output_test_file = os.path.join(FLAGS.output_dir, \"model.ckpt-\"+str(ckpt)+\"-test_results.txt\")\r\n      test_result_file_list.append(output_test_file)\r\n      with tf.gfile.GFile(output_test_file, \"w\") as writer:\r\n        writer.write(\"\\t\".join(LABEL_NAMES) + \"\\tPredicted Label\\tCorrect Label\\tPred Correct\\tText\\n\")\r\n        tf.logging.info(\"***** Test results *****\")\r\n        num_tests = 0\r\n        num_correct = 0\r\n        true_positives = 0\r\n        true_negatives = 0\r\n        false_positives = 0\r\n        false_negatives = 0\r\n        result_counter = 0\r\n        for prediction in result:\r\n          result_counter += 1\r\n          if result_counter >= num_actual_test_examples:\r\n            break\r\n          num_tests += 1\r\n          probabilities = prediction[\"probabilities\"]\r\n\r\n          pred = np.argmax(probabilities)\r\n          pred_label = LABEL_NAMES[pred]\r\n          pred = str(pred)  # Note: This string is still numeric, not the label name!\r\n          pred_correct = \"\"\r\n          if pred == test_labels[result_counter]:\r\n            num_correct += 1\r\n            pred_correct = \"1\"\r\n            if pred_label == TEST_MEASURE_CLASS:\r\n              true_positives += 1\r\n            else:\r\n              true_negatives += 1\r\n          else:\r\n            pred_correct = \"0\"\r\n            if pred_label == TEST_MEASURE_CLASS:\r\n              false_positives += 1\r\n            else:\r\n              false_negatives += 1\r\n          \r\n          output_line = \"\\t\".join(\r\n              str(class_probability)\r\n              for class_probability in probabilities)\r\n          output_line += \"\\t\" + pred_label\r\n          output_line += \"\\t\" + LABEL_NAMES[int(test_labels[result_counter])]\r\n          output_line += \"\\t\" + pred_correct\r\n          output_line += \"\\t\" + test_text[result_counter] + \"\\n\"\r\n          writer.write(output_line)\r\n        \r\n        # Precision\r\n        if true_positives + false_positives > 0:\r\n          precision = true_positives / (true_positives + false_positives)\r\n        else:\r\n          precision = 0\r\n        # Recall\r\n        if true_positives + false_negatives > 0:\r\n          recall = true_positives / (true_positives + false_negatives)\r\n        else:\r\n          recall = 0\r\n        # F-score\r\n        if precision + recall > 0:\r\n          fscore = 2 * (precision * recall) / (precision + recall)\r\n        else:\r\n          fscore = 0\r\n        # Balanced Accuracy\r\n        if true_negatives + false_positives > 0:\r\n          true_negative_rate = true_negatives / (true_negatives + false_positives)\r\n          balanced_accuracy = (recall + true_negative_rate) / 2\r\n        else:\r\n          balanced_accuracy = 0\r\n\r\n        writer.write(\"%s = %s\\n\" % (\"Epoch\", str(ckpt_idx)))\r\n        writer.write(\"%s = %s\\n\" % (\"Test Time\", str(test_ckpt_time)))\r\n        writer.write(\"%s = %s\\n\" % (\"Test Count\", str(num_tests)))\r\n        writer.write(\"%s = %s\\n\" % (\"Correct\", str(num_correct)))\r\n        writer.write(\"%s = %s\\n\" % (\"Naive Accuracy\", str(num_correct / num_tests)))\r\n        writer.write(\"%s = %s\\n\" % (\"True Positives\", str(true_positives)))\r\n        writer.write(\"%s = %s\\n\" % (\"True Negatives\", str(true_negatives)))\r\n        writer.write(\"%s = %s\\n\" % (\"False Positives\", str(false_positives)))\r\n        writer.write(\"%s = %s\\n\" % (\"False Negatives\", str(false_negatives)))\r\n        writer.write(\"%s = %s\\n\" % (\"Precision\", str(precision)))\r\n        writer.write(\"%s = %s\\n\" % (\"Recall\", str(recall)))\r\n        writer.write(\"%s = %s\\n\" % (\"F-score\", str(fscore)))\r\n        writer.write(\"%s = %s\\n\" % (\"Balanced Accuracy\", str(balanced_accuracy)))\r\n\r\n        tf.logging.info(\"Scored %d tests\", num_tests)\r\n        tf.logging.info(\"Correct: %d\", num_correct)\r\n        tf.logging.info(\"Naive Accuracy: %f\", num_correct / num_tests)\r\n        tf.logging.info(\"True Positives: %d\", true_positives)\r\n        tf.logging.info(\"True Negatives: %d\", true_negatives)\r\n        tf.logging.info(\"False Positives: %d\", false_positives)\r\n        tf.logging.info(\"False Negatives: %d\", false_negatives)\r\n        tf.logging.info(\"Precision: %f\", precision)\r\n        tf.logging.info(\"Recall: %f\", recall)\r\n        tf.logging.info(\"F-score: %f\", fscore)\r\n        tf.logging.info(\"Balanced Accuracy: %f\", balanced_accuracy)\r\n```\r\nIn the estimator.predict, the test_input_fn is defined as the following 2 segments:\r\n\r\n```\r\n    test_input_fn = file_based_input_fn_builder(\r\n        input_file=test_file,\r\n        seq_length=FLAGS.max_seq_length,\r\n        is_training=False,\r\n        drop_remainder=test_drop_remainder)\r\n```\r\n```\r\ndef file_based_input_fn_builder(input_file, seq_length, is_training,\r\n                                drop_remainder):\r\n  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\r\n\r\n  name_to_features = {\r\n      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\r\n      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\r\n      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\r\n      \"label_ids\": tf.FixedLenFeature([], tf.int64),\r\n      \"is_real_example\": tf.FixedLenFeature([], tf.int64),\r\n  }\r\n\r\n  def _decode_record(record, name_to_features):\r\n    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\r\n    example = tf.parse_single_example(record, name_to_features)\r\n\r\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\r\n    # So cast all int64 to int32.\r\n    for name in list(example.keys()):\r\n      t = example[name]\r\n      if t.dtype == tf.int64:\r\n        t = tf.to_int32(t)\r\n      example[name] = t\r\n\r\n    return example\r\n\r\n  def input_fn(params):\r\n    \"\"\"The actual input function.\"\"\"\r\n    batch_size = params[\"batch_size\"]\r\n\r\n    # For training, we want a lot of parallel reading and shuffling.\r\n    # For eval, we want no shuffling and parallel reading doesn't matter.\r\n    d = tf.data.TFRecordDataset(input_file)\r\n    if is_training:\r\n      d = d.repeat()\r\n      d = d.shuffle(buffer_size=100)\r\n\r\n    d = d.apply(\r\n        tf.contrib.data.map_and_batch(\r\n            lambda record: _decode_record(record, name_to_features),\r\n            batch_size=batch_size,\r\n            drop_remainder=drop_remainder))\r\n\r\n    return d\r\n\r\n  return input_fn\r\n```\r\n\r\nOutside these definition, here is how I call the predict in another cell:\r\n```\r\nFLAGS.do_train = False\r\nFLAGS.do_eval = False\r\nFLAGS.do_test = True\r\nFLAGS.do_predict = False\r\nFLAGS.do_train_and_eval = False\r\n\r\nstart_time = datetime.now()\r\nmain(start_time)\r\nprint(\"Testing took:\", datetime.now() - start_time)\r\n```\r\nFor your reference, I just pasted the log information for the second predict run:\r\n\r\n\r\n> WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fee467ad158>) includes params argument, but params are not passed to Estimator.\r\n> INFO:tensorflow:Using config: {'_model_dir': 'gs://sr2pr/liuyi_test_early_stop/early_stop_ckpts', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\r\n> cluster_def {\r\n>   job {\r\n>     name: \"worker\"\r\n>     tasks {\r\n>       key: 0\r\n>       value: \"10.25.26.154:8470\"\r\n>     }\r\n>   }\r\n> }\r\n> isolate_session_state: true\r\n> , '_keep_checkpoint_max': 100, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fee43408da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.25.26.154:8470', '_evaluation_master': 'grpc://10.25.26.154:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=853, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fee434085c0>}\r\n> INFO:tensorflow:_TPUContext: eval_on_tpu True\r\n> WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fee467e4d08>) includes params argument, but params are not passed to Estimator.\r\n> INFO:tensorflow:Using config: {'_model_dir': 'gs://sr2pr/liuyi_test_early_stop/early_stop_ckpts', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\r\n> cluster_def {\r\n>   job {\r\n>     name: \"worker\"\r\n>     tasks {\r\n>       key: 0\r\n>       value: \"10.25.26.154:8470\"\r\n>     }\r\n>   }\r\n> }\r\n> isolate_session_state: true\r\n> , '_keep_checkpoint_max': 100, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fee422077f0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.25.26.154:8470', '_evaluation_master': 'grpc://10.25.26.154:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=853, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fee434085c0>}\r\n> INFO:tensorflow:_TPUContext: eval_on_tpu True\r\n> INFO:tensorflow:Writing example 0 of 50272\r\n> INFO:tensorflow:*** Example ***\r\n> INFO:tensorflow:guid: test-0\r\n> INFO:tensorflow:tokens: [CLS] unable to authentic ##ate using ad account in view [SEP]\r\n> INFO:tensorflow:input_ids: 101 4039 2000 14469 3686 2478 4748 4070 1999 3193 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n> INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n> INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n> INFO:tensorflow:label: 0 (id = 0)\r\n> INFO:tensorflow:*** Example ***\r\n> INFO:tensorflow:guid: test-1\r\n> INFO:tensorflow:tokens: [CLS] unable to upgrade hosts to es ##xi 5 . 0 u ##3 and is getting an error \" this operation is not supported on the selected inventory object \" [SEP]\r\n> INFO:tensorflow:input_ids: 101 4039 2000 12200 6184 2000 9686 9048 1019 1012 1014 1057 2509 1998 2003 2893 2019 7561 1000 2023 3169 2003 2025 3569 2006 1996 3479 12612 4874 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n> INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n> INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n> INFO:tensorflow:label: 0 (id = 0)\r\n> INFO:tensorflow:*** Example ***\r\n> INFO:tensorflow:guid: test-2\r\n> INFO:tensorflow:tokens: [CLS] we had 3 es ##xi 5 . 5 hosts disco ##nne ##ct from vc ##enter this morning . the v ##ms were still unaffected and still operational . i used this kb to troubles ##hoot : https : / / kb . v ##m ##ware . com / self ##ser ##vic ##e / micro ##sit ##es / search . do ? language = en _ us & cm ##d = display ##k ##c & external ##id = 100 ##34 ##0 ##9 . everything in the check ##list is good . what solved the issue was either restart ##ing the management agents , or specifically just restart ##ing vp ##xa rec ##onne ##cted the host just fine . i ' m attach ##ing the logs for all [SEP]\r\n> INFO:tensorflow:input_ids: 101 2057 2018 1017 9686 9048 1019 1012 1019 6184 12532 10087 6593 2013 18315 29110 2023 2851 1012 1996 1058 5244 2020 2145 24720 1998 2145 6515 1012 1045 2109 2023 21677 2000 13460 23416 1024 16770 1024 1013 1013 21677 1012 1058 2213 8059 1012 4012 1013 2969 8043 7903 2063 1013 12702 28032 2229 1013 3945 1012 2079 1029 2653 1027 4372 1035 2149 1004 4642 2094 1027 4653 2243 2278 1004 6327 3593 1027 2531 22022 2692 2683 1012 2673 1999 1996 4638 9863 2003 2204 1012 2054 13332 1996 3277 2001 2593 23818 2075 1996 2968 6074 1010 2030 4919 2074 23818 2075 21210 18684 28667 18256 10985 1996 3677 2074 2986 1012 1045 1005 1049 22476 2075 1996 15664 2005 2035 102\r\n> INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n> INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n> INFO:tensorflow:label: 0 (id = 0)\r\n> INFO:tensorflow:*** Example ***\r\n> INFO:tensorflow:guid: test-3\r\n> INFO:tensorflow:tokens: [CLS] v ##m lost connection again during backup window . last sr i opened was 1750 ##38 ##0 ##6 ##90 ##7 and was told the issue was the v ##m tools . [SEP]\r\n> INFO:tensorflow:input_ids: 101 1058 2213 2439 4434 2153 2076 10200 3332 1012 2197 5034 1045 2441 2001 18171 22025 2692 2575 21057 2581 1998 2001 2409 1996 3277 2001 1996 1058 2213 5906 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n> INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n> INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n> INFO:tensorflow:label: 0 (id = 0)\r\n> INFO:tensorflow:*** Example ***\r\n> INFO:tensorflow:guid: test-4\r\n> INFO:tensorflow:tokens: [CLS] https : / / its ##m . vo ##da ##fo ##ne . com / ars ##ys / / ser ##v ##let / view ##forms ##er ##v ##let ? form = nt ##e % 3a ##not ##ifier & server = its ##m - pro ##d & e ##id = nt ##s ##00 ##01 ##9 ##32 ##48 ##20 ##8 service type : user service request service : vt ##n - secure mobility manager - pro ##d priority : 3 - medium summary : creation of ta ##c - case regarding missing sync between apple de ##p and air ##watch [SEP]\r\n> INFO:tensorflow:input_ids: 101 16770 1024 1013 1013 2049 2213 1012 29536 2850 14876 2638 1012 4012 1013 29393 7274 1013 1013 14262 2615 7485 1013 3193 22694 2121 2615 7485 1029 2433 1027 23961 2063 1003 23842 17048 18095 1004 8241 1027 2049 2213 1011 4013 2094 1004 1041 3593 1027 23961 2015 8889 24096 2683 16703 18139 11387 2620 2326 2828 1024 5310 2326 5227 2326 1024 28879 2078 1011 5851 12969 3208 1011 4013 2094 9470 1024 1017 1011 5396 12654 1024 4325 1997 11937 2278 1011 2553 4953 4394 26351 2090 6207 2139 2361 1998 2250 18866 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n> INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n> INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n> INFO:tensorflow:label: 0 (id = 0)\r\n> INFO:tensorflow:Writing example 10000 of 50272\r\n> INFO:tensorflow:Writing example 20000 of 50272\r\n> INFO:tensorflow:Writing example 30000 of 50272\r\n> INFO:tensorflow:Writing example 40000 of 50272\r\n> INFO:tensorflow:Writing example 50000 of 50272\r\n> INFO:tensorflow:***** Running test prediction*****\r\n> INFO:tensorflow:  Num examples = 50272 (50249 actual, 23 padding)\r\n> INFO:tensorflow:  Batch size = 32\r\n> checkpoints path is designated to be  gs://sr2pr/liuyi_test_early_stop/early_stop_ckpts\r\n> num_train_steps:  2559\r\n> FLAGS.iterations_per_loop:  853\r\n> checkpoints are:  [0, 853, 1706, 2559]\r\n> ========== This is to test gs://sr2pr/liuyi_test_early_stop/early_stop_ckpts/model.ckpt-853 ==========\r\n> INFO:tensorflow:***** Test results *****\r\n> INFO:tensorflow:Querying Tensorflow master (grpc://10.25.26.154:8470) for TPU system metadata.\r\n> INFO:tensorflow:Found TPU system:\r\n> INFO:tensorflow:*** Num TPU Cores: 8\r\n> INFO:tensorflow:*** Num TPU Workers: 1\r\n> INFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 8276302537946597306)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9000101427539453289)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 8518970402231929580)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9502851374819066520)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 10839553371045225842)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 13839547643692205269)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 3498128454668765351)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15451249720587501361)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 17988533590690348132)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 7563947479981978740)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2712428284589480861)\r\n> INFO:tensorflow:Calling model_fn.\r\n> WARNING:tensorflow:Entity <function file_based_input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7fee44d598c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\r\n> WARNING: Entity <function file_based_input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7fee44d598c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\r\n> WARNING:tensorflow:Entity <function _InputsWithStoppingSignals.insert_stopping_signal.<locals>._map_fn at 0x7fee44d599d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> WARNING: Entity <function _InputsWithStoppingSignals.insert_stopping_signal.<locals>._map_fn at 0x7fee44d599d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> WARNING:tensorflow:Entity <function _InputsWithStoppingSignals.insert_stopping_signal.<locals>._map_fn at 0x7fee452502f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> WARNING: Entity <function _InputsWithStoppingSignals.insert_stopping_signal.<locals>._map_fn at 0x7fee452502f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> WARNING:tensorflow:Entity <function _InputsWithStoppingSignals.__init__.<locals>._set_mask at 0x7fee45395840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> WARNING: Entity <function _InputsWithStoppingSignals.__init__.<locals>._set_mask at 0x7fee45395840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> INFO:tensorflow:*** Features ***\r\n> INFO:tensorflow:  name = input_ids, shape = (4, 128)\r\n> INFO:tensorflow:  name = input_mask, shape = (4, 128)\r\n> INFO:tensorflow:  name = is_real_example, shape = (4,)\r\n> INFO:tensorflow:  name = label_ids, shape = (4,)\r\n> INFO:tensorflow:  name = segment_ids, shape = (4, 128)\r\n> INFO:tensorflow:**** Trainable Variables ****\r\n> INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30522, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_12/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_12/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_12/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_12/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_12/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_12/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_12/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_12/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_12/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_12/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_12/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_12/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_12/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_12/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_12/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_12/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_13/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_13/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_13/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_13/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_13/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_13/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_13/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_13/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_13/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_13/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_13/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_13/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_13/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_13/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_13/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_13/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_14/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_14/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_14/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_14/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_14/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_14/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_14/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_14/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_14/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_14/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_14/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_14/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_14/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_14/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_14/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_14/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_15/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_15/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_15/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_15/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_15/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_15/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_15/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_15/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_15/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_15/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_15/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_15/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_15/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_15/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_15/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_15/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_16/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_16/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_16/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_16/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_16/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_16/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_16/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_16/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_16/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_16/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_16/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_16/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_16/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_16/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_16/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_16/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_17/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_17/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_17/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_17/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_17/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_17/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_17/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_17/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_17/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_17/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_17/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_17/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_17/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_17/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_17/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_17/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_18/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_18/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_18/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_18/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_18/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_18/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_18/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_18/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_18/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_18/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_18/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_18/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_18/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_18/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_18/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_18/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_19/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_19/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_19/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_19/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_19/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_19/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_19/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_19/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_19/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_19/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_19/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_19/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_19/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_19/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_19/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_19/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_20/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_20/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_20/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_20/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_20/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_20/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_20/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_20/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_20/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_20/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_20/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_20/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_20/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_20/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_20/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_20/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_21/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_21/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_21/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_21/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_21/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_21/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_21/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_21/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_21/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_21/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_21/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_21/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_21/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_21/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_21/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_21/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_22/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_22/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_22/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_22/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_22/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_22/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_22/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_22/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_22/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_22/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_22/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_22/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_22/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_22/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_22/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_22/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/encoder/layer_23/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\r\n> INFO:tensorflow:  name = output_weights:0, shape = (2, 1024)\r\n> INFO:tensorflow:  name = output_bias:0, shape = (2,)\r\n> INFO:tensorflow:Done calling model_fn.\r\n> INFO:tensorflow:TPU job name worker\r\n> INFO:tensorflow:Graph was finalized.\r\n> INFO:tensorflow:Restoring parameters from gs://sr2pr/liuyi_test_early_stop/early_stop_ckpts/model.ckpt-853\r\n> INFO:tensorflow:Running local_init_op.\r\n> INFO:tensorflow:Done running local_init_op.\r\n> INFO:tensorflow:Init TPU system\r\n> INFO:tensorflow:Initialized TPU in 0 seconds\r\n> INFO:tensorflow:Starting infeed thread controller.\r\n> INFO:tensorflow:Starting outfeed thread controller.\r\n> INFO:tensorflow:Initialized dataset iterators in 0 seconds\r\n> INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\r\n> INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\r\n> INFO:tensorflow:Outfeed finished for iteration (0, 0)\r\n> INFO:tensorflow:Stop infeed thread controller\r\n> INFO:tensorflow:Shutting down InfeedController thread.\r\n> \r\n> \r\n\r\nI hope this helps in explanation my problem. \r\n", "Any assignee could take this issue? If necessary, I could paste the whole repo that produces the error.", "It's confirmed when we want to iterate over the result returned from `TPUEstimator.predict`, we cannot use the `enumerate`. `enumerate` does not raise the `StopIteration` exception when it iterates over the generator result. However, if I understood the estimator source code correctly, `TPUEstimator.predict` needs this exception to provide something called `Signal` to TPU worker and shut down the TPU worker. In this way, the TPUEstimator can reinitialize from a new checkpoint when it is called with another checkpoint directory path.\r\n\r\nBy applying the normal `next(result)` instead of `enumerate(result)`, this issue is resolved. So this issue can be closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41014\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41014\">No</a>\n"]}, {"number": 41013, "title": "Fix the function name in debugging.md", "body": "There no function named 'experimental_execute_functions_eagerly'\r\n\r\nOnly function named 'experimental_run_functions_eagerly' described at:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/config/experimental_run_functions_eagerly", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41013) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41013) for more info**.\n\n<!-- ok -->", "@kkimdev, @mdanatg Can you please review this PR ? Thanks!"]}, {"number": 41012, "title": "tflite RuntimeError: Encountered unresolved custom op: CombinedNonMaxSuppression.Node number 148 (CombinedNonMaxSuppression) failed to prepare.", "body": "**System information**\r\n- Linux \r\n- TF 2.5.0-dev20200629\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\nConverter:\r\n\r\n```\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(curr_dir + \"saved_model_2\")\r\n    converter.experimental_new_converter = True\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                           tf.lite.OpsSet.SELECT_TF_OPS]\r\n    converter.allow_custom_ops = True\r\n\r\n    tflite_model = converter.convert()\r\n```\r\n\r\nInference:\r\n\r\n```\r\n# Load the TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"./model.tflite\")\r\ninterpreter.allocate_tensors()\r\nprint(\"all ok\")\r\n\r\n# # Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# # Test the model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\n# # The function `get_tensor()` returns a copy of the tensor data.\r\n# # Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n```\r\n\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n  File \"inference.py\", line 19, in <module>\r\n    interpreter.invoke()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py\", line 524, in invoke\r\n    self._interpreter.Invoke()\r\nRuntimeError: Encountered unresolved custom op: CombinedNonMaxSuppression.Node number 148 (CombinedNonMaxSuppression) failed to prepare.\r\n```\r\n\r\nI'm wondering why the flex delegate couldn't prepare the CombinedNonMaxSuppression function, even though I have the `SELECT_TF_OPS` flag enabled?", "comments": ["Thai, could you help adding CombinedNonMaxSuppression op to flex?", "OK. Let me add it.", "Hi @thaink \r\n\r\nHave you had a chance to add the op to flex?\r\nAlso, for my understanding, is this a matter of adding CombinedNonMaxSuppression to \r\n//tensorflow/lite/delegates/flex/whitelisted_flex_ops.cc ?", "@aselva-eb Right. A PR is on the way to add it.", "Thank you @thaink! ", "> @aselva-eb Right. A PR is on the way to add it.\r\n\r\nHi @thaink, Is there an approx ETA? ", "I think it will be supported within this week. Thanks.", "CombinedNonMaxSuppression is added at master. Please check it.", "> CombinedNonMaxSuppression is added at master. Please check it.\r\n\r\nDid it make it into `2.4.0.dev20200714` release ?\r\n\r\nRunning that version of tf_nightly but have issues importing tensorflow. (This must've recently broke because in the previous days release, although it gave me warnings, it never brought me to a halt on the import). This is within a Docker container with the tensorflow/tensorflow docker image. Tried a fresh container too. This doesn't happen on MacOS but I can't test on it because FlexOps aren't supported on MacOS yet\r\n\r\n```\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "No, it should be in the nightly tomorrow.", "@thaink \r\nThanks for adding it to flex ops. Invoking the model passes now, however, my output is suspiciously all 0s on a random input. Will need to dig into this a bit more. ", "@aselva-eb\r\nplease let us know if this is still an issue else please move the issue to closed status.", "> @aselva-eb\r\n> please let us know if this is still an issue else please move the issue to closed status.\r\n\r\nWe can close this ticket. I believe it is resolved. Thanks!"]}, {"number": 41011, "title": "micro fix in limitations.md", "body": "", "comments": []}, {"number": 41010, "title": "[INTEL MKL] Pad+Conv fusion for bf16", "body": "This PR enables pad+conv fusion for bf16 and related test case.", "comments": []}, {"number": 41008, "title": "Correctly handle resource variables in control_dependencies", "body": "This PR includes the `FuncGraph` changes required to rollforward #40564 and fixes handling of resource variables as input to control dependencies.\r\n\r\n/cc @alextp @crccw", "comments": ["Looks like the `cla/google` bot is stuck (Just to confirm that I have signed the CLI and have contributed to TF in the past).", "> Can we please add some tests to this change? There seems to a be quite a few subtle changes here which are very hard to understand as to why they are needed.\r\n\r\n@jaingaurav That's a fair point. Those changes are mainly required for fixing the `AutoCastVariable` issues when assigning to a variable in a distributed strategy and have been discussed in #40564 which was reverted due to internal failures on TPU.\r\n\r\nUnfortunately the only way I was able to find a unittest for this was with the upcoming changes to `AutoCastVariable`. I resubmitted the changes in #41214 which includes [a unittest which would fail without these changes](https://github.com/tensorflow/tensorflow/pull/41214/files#diff-7c464547ccc6362ecbb81b73b4cfbad0R364-R380) and additional comments with links to the relevant discussion that try to answer the the comments you made."]}, {"number": 41007, "title": "[INTEL MKL] Adding oneDNN partials for Ubuntu versions 16.04. 18.04, 20.04", "body": "This PR adds [OneDNN](https://github.com/oneapi-src/oneDNN) partials for 3 most recent versions of `Ubuntu` that are still supported for a while.\r\n\r\nTo build these docker images, simply run:\r\n```\r\nalias asm_images=\"docker run --rm -v $(pwd):/tf -v /var/run/docker.sock:/var/run/docker.sock tf-tools python3 assembler.py \"\r\nasm_images --release versioned --repository intel/intel-optimized-tensorflow --arg TF_BRANCH=v2.1.0 --arg _TAG_PREFIX=2.1.0-ubuntu- --build_images --exclude_tags_matching '.*gpu.*'\r\n```\r\nOnce the builds are complete you should have the following images:\r\n\r\n```\r\nintel/intel-optimized-tensorflow:2.1.0-ubuntu-20.04-onednn-jupyter\r\nintel/intel-optimized-tensorflow:2.1.0-ubuntu-20.04-onednn\r\nintel/intel-optimized-tensorflow:2.1.0-ubuntu-18.04-devel-onednn-jupyter\r\nintel/intel-optimized-tensorflow:2.1.0-ubuntu-18.04-devel-onednn\r\nintel/intel-optimized-tensorflow:2.1.0-ubuntu-16.04-devel-onednn-jupyter\r\nintel/intel-optimized-tensorflow:2.1.0-ubuntu-16.04-devel-onednn\r\nintel/intel-optimized-tensorflow:2.1.0-ubuntu-18.04-onednn-jupyter\r\nintel/intel-optimized-tensorflow:2.1.0-ubuntu-18.04-onednn\r\nintel/intel-optimized-tensorflow:2.1.0-ubuntu-20.04-devel-onednn-jupyter\r\nintel/intel-optimized-tensorflow:2.1.0-ubuntu-20.04-devel-onednn\r\nintel/intel-optimized-tensorflow:2.1.0-ubuntu-16.04-onednn-jupyter\r\nintel/intel-optimized-tensorflow:2.1.0-ubuntu-16.04-onednn\r\n```\r\n", "comments": ["/assign @claynerobison @dzungductran", "/assign @angerson ", "@ashahba Thank you for your contribution. Can you please sign CLA? Thanks!", "@gbaned  My `CLA` should be good. Let me double check", "All new `OneDNN` images now can built like this for example:\r\n\r\n```alias asm_images=\"docker run --rm -v $(pwd):/tf -v /var/run/docker.sock:/var/run/docker.sock tf-tools python3 assembler.py \"\r\nasm_images --release onednn --repository intel/intel-optimized-tensorflow --arg TF_BRANCH=v2.1.0 --arg _TAG_PREFIX=2.1.0-ubuntu --build_images --only_tags_matching '.*onednn.*'\r\n```\r\n", "@angerson I addressed all the review requests.", "@angerson any other feedbacks on this? Can we go forward with `approve` and merge?\r\nI have a couple other subsequent PRs that are relying on this one to go in first.\r\n\r\nThanks."]}, {"number": 41006, "title": "Add SaveableObjects to SavedModel.", "body": "When objects are loaded from the SavedModel, they don't retain their `_gather_saveables_for_checkpoint` functions, which can result in values not being loaded from the checkpoint.\r\n\r\nThis CL adds a field in the SavedModel proto that stores a save and restore function for each SaveableObject in each node. When loading into Python, the SaveableObjects are restored using the functions.\r\n\r\nPiperOrigin-RevId: 318549786\r\nChange-Id: I688c72d7658e1bca98abf373a13a0e15a7fb83e2", "comments": []}, {"number": 41005, "title": "tf.where doc on Args \"x\" seems incorrect", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/where#args_1\r\n\r\n## Description of issue (what needs changing):\r\nFor Args \"x\".\r\nOriginal:\r\nIf provided, a Tensor which is of the same type as y, and has a shape broadcastable with condition and y.\r\nShould be:\r\nIf provided, a Tensor which is of the same type as **x**, and has a shape broadcastable with condition and y.\r\n", "comments": ["@amahendrakar @jvishnuvardhan  I would like to work on this issue", "Can you help, how to edit that as in api_def of where visibility is set to hidden.", "@jvishnuvardhan will you please help me with this?\r\n", "`x` and `y` are two tensors. On the description of `x` we are saying that `x` has the same type as the `y` tensor.", "Thanks for the update. In that case, it looks like the document problem is with \"y\".\r\n\r\nOriginal:\r\ny | If provided, a Tensor which is of the same type as\u00a0y, and has a shape broadcastable with\u00a0condition\u00a0and\u00a0x.\r\nShould be:\r\ny | If provided, a Tensor which is of the same type as\u00a0**x**, and has a shape broadcastable with\u00a0condition\u00a0and\u00a0x.", "#41044 landed and fixed this in nightly.", "It does not seem to be fixed in https://www.tensorflow.org/api_docs/python/tf/where.", "@chenni you can see the updated version in nightly mode!!", "Sorry, but would you mind sharing the link to the nightly mode? Not sure how to check that. thx.", "@Chenni \r\n### **You can see nightly version in the link given below**\r\n[`tf.where`](https://www.tensorflow.org/api_docs/python/tf/where?version=nightly)`", "Thanks! But the main website https://www.tensorflow.org/api_docs/python/tf/where that is visible to all the users are still not updated after more than a week. When will that be fixed?", "The main website is always in sync with the last TF release. So it will be updated when we release 2.3.0 (in a few weeks)"]}, {"number": 41003, "title": "OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nGetting 'OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution' error , while Eager execution is actually enabled\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras import datasets, layers, models\r\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\r\n\r\n# Normalize pixel values to be between 0 and 1\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\n\r\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\r\n               'dog', 'frog', 'horse', 'ship', 'truck']\r\n\r\nmodel = models.Sequential()\r\nmodel.add(layers.InputLayer(input_shape=(32, 32, 3)))\r\nmodel.add(layers.Conv2D(16, (3,3)))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(10, activation='softmax'))\r\nmodel.summary()\r\n\r\ndef to_list(x):\r\n  if isinstance(x, list):\r\n    return x\r\n  return [x]\r\n\r\nouts = []\r\nfor l in model.layers:\r\n    output_tensors = [to_list(inbound.output_tensors)[0] for inbound in l.inbound_nodes]\r\n    outs.append(output_tensors)\r\n\r\nins = []\r\nfor l in model.layers:\r\n    input_tensors = [to_list(inbound.input_tensors)[0] for inbound in l.inbound_nodes]\r\n    ins.append(input_tensors)\r\n\r\nassert tf.executing_eagerly()\r\nfor i_tensor in ins:\r\n    if i_tensor in outs:\r\n        print(\"BOOM\")\r\n\r\n\r\n```\r\nNote that it passes the assertion, making sure the eager execution is on, but still raises the error on the if statement and suggests to 'Use Eager execution'. What am I missing? The eager execution is on by default in TF 2.2\r\n\r\n", "comments": ["@louisgriffin \r\nCan you please refer to [this link](https://github.com/matterport/Mask_RCNN/issues/1911#issuecomment-595682013) and let us know if it helps.\r\n[Link1](https://stackoverflow.com/questions/59308263/using-a-tf-tensor-as-a-python-bool-is-not-allowed-in-graph-execution-use-ea) [link2](https://github.com/tensorflow/tensorflow/issues/37393#issuecomment-646815275) #36848", "> @louisgriffin\r\n> Can you please refer to [this link](https://github.com/matterport/Mask_RCNN/issues/1911#issuecomment-595682013) and let us know if it helps.\r\n> [Link1](https://stackoverflow.com/questions/59308263/using-a-tf-tensor-as-a-python-bool-is-not-allowed-in-graph-execution-use-ea) [link2](https://github.com/tensorflow/tensorflow/issues/37393#issuecomment-646815275) #36848\r\n\r\n@Saduf2019 In the link you provided you suggest using tf 1.15? I need tf 2.x", "@louisgriffin I have run into a different error. Please find the gist [here](https://colab.research.google.com/gist/gowthamkpr/567a1a1200e6901d397b5a4e8bb9e8dd/untitled301.ipynb)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41003\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41003\">No</a>\n"]}]