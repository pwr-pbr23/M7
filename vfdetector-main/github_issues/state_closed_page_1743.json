[{"number": 606, "title": "Fix syntax error in input.py", "body": "Mentioned in #480\n", "comments": ["LGTM\n\ntested\n"]}, {"number": 605, "title": "batch_matmul doesn't work on GPUs", "body": "It seems not possible to register the `batch_matmul` operation to a GPU device. For example this fails:\n\n```\nwith tf.device('/gpu:0'):\n    a = tf.constant(npr.rand(10, 5, 5).astype('float32'))\n    b = tf.constant(npr.rand(10, 5, 5).astype('float32'))\n    c = tf.batch_matmul(a, b)\n\nsess = tf.Session(config=tf.ConfigProto(allow_soft_placement=False))\nsess.run(c)\n```\n\nThis is a fundamental operation and so GPU support is rather important. I'm getting the error on Ubuntu 14.04 with CUDA 7.0.\n", "comments": ["I have the same question too.\n", "I would also greatly appreciate this to work on GPUs.\n", "You can uncomment Ln270 of batch_matmul.cc to try it out on your machine.\nWe had mixed experience of the implementation (some reported working great,\nsome reported it rans out of gpu ram from time to time).\n\nOn Fri, Jan 15, 2016 at 3:31 AM ponythewhite notifications@github.com\nwrote:\n\n> I would also greatly appreciate this to work on GPUs.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/605#issuecomment-171938542\n> .\n", "Any updates on this? It used to sort of work when uncommenting the relevant line in batch_matmul.cc, but now it often gives an error, specifically `Blas SGEMMBatched launch failed`. Also performance actually seems worse with it on the GPU than the CPU.\n", "This was fixed a while ago.  Woot.\n"]}, {"number": 604, "title": "Update os_setup.md to point to 0.6.0 release", "body": "Fixes #603\n", "comments": []}, {"number": 603, "title": "Install Documentation still links to 0.5", "body": "https://www.tensorflow.org/versions/master/get_started/os_setup.html#pip_install\n\nThe Virtualenv mentions the version 0.5 instead of 0.6 and this may cause some confusion since the rest of the documentation links 0.6.\n\nsee:\n\nUbuntu/Linux 64-bit, CPU only:\n(tensorflow)$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n\nUbuntu/Linux 64-bit, GPU enabled:\n(tensorflow)$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n\nMac OS X, CPU only:\n(tensorflow)$ pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\n\nI noticed that the \n", "comments": ["https://github.com/tensorflow/tensorflow/pull/604\n"]}, {"number": 602, "title": "Strange behavior and verbosity in Tensorboard for RNN", "body": "I only have one encoder, but it lists 50+ encoder variables in the side bar (see attached). This also happens for my decoder. I presume that it is duplicating the variables for each time step? Notice that it says 500+ inputs!\n\nAlso, the graph becomes unwieldy and barely loaded in Safari (crashed in Chrome, wouldn't load in Firefox), and is displayed with a <i>very</i> long list of variable names (see attached).\n\n![tf1](https://cloud.githubusercontent.com/assets/1364252/11982491/e9e57346-a968-11e5-9d05-1c6e6faded1e.jpg)\n![tf2](https://cloud.githubusercontent.com/assets/1364252/11982496/eceb63ca-a968-11e5-9c59-3960f635f653.jpg)\n", "comments": ["Does the graph tab ever render in Firefox? See issue #650.\n", "Looks like this fell through the cracks.  @danmane: Any thoughts? \n", "@dsmilkov and @jameswex are point people on graph visualizer issues - I know we've fixed a few issues of this general form, but automatic grouping for repeated nodes isn't perfect yet. Can one of you comment if we should close this issue? Also, would grouping all of these ops under a shared namespace help, or just kick the problem down into a new unmanagably massive namespace?\n", "What is the code that creates all of the encoder[num] ops? The graph visualizer will automatically group together nodes that are of the form [name]_[num], but these don't have an underscore between name and num.\n\nWhen using tensorflow to create an op a number of times with the same name for the op each time, tensorflow will automatically add _[num] to each of the created ops after the first one.\n", "If I remember correctly I generated these graphs using the initial 0.5 release and I added monitoring to variables in the Seq2Seq example. So I believe the encoder[num] ops were generated simply from the RNN cells (LSTM as part of a MultiRNN object, I think) that were part of that example.\n", "Do you happen to have this graph created with a more-recent release? If so, does the problem exist? Do you have the code you added to the example that I could take a look at?\n", "Unfortunately, I didn't keep the code or model that I used to generate the images. I'm working on a small script that can hopefully reproduce the problem, but just a simple RNN so far has the clean output you had suggested. Maybe this happens when generating a sequence using an RNN with an output projection layer (like in the seq2seq translate tutorial), or when when using an embedding (multi-dimensional output sequences). Sorry I haven't been able to provide much help on diagnosising the root cause. I'll do a few more tests to see if I can reproduce the problem and get back to you.\n", "I'm going to close this for now since we don't have a repro and we don't know if it is still reproducing. @jstaker7 please re-open if you find a repro.\n"]}, {"number": 601, "title": "failed call to cuInit: CUDA_ERROR_UNKNOWN after Docker build on Macbook Pro (Late 2013) with Linux", "body": "This issue is similar to to #394. I believe i'm seeing it because tensorflow can't find `libcuda.so`.\n\nI know that my `libcuda.so` can be found in `/usr/lib/x86_64-linux-gnu/`, but is this accessible to the compiler and to python without further configuration?  I tried adding it to my docker user's `$LD_LIBRARY_PATH`, but that did not fix the problem. \n\nI'm running on:\n- Macbook Pro (Late 2013) Hardware\n- Mint 17\n- 3.0 Compute Capability\n- CUDA 7.0 & cuDNN 2.0 \n- running docker service under a docker user & group.  no sudo access for docker user.\n\non host os, values for `$CUDA_SO` and `$DEVICES` that are passed into `./docker_run_gpu.sh` are:\n\n```\n$ export CUDA_SO=$(\\ls /usr/lib/x86_64-linux-gnu/libcuda* | xargs -I{} echo '-v {}:{}')\n$ export DEVICES=$(\\ls /dev/nvidia* | xargs -I{} echo '--device {}:{}')\n\n$ echo $CUDA_SO\n-v /usr/lib/x86_64-linux-gnu/libcuda.so:/usr/lib/x86_64-linux-gnu/libcuda.so -v /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1 -v /usr/lib/x86_64-linux-gnu/libcuda.so.352.68:/usr/lib/x86_64-linux-gnu/libcuda.so.352.68\n\n$ echo $DEVICES\n--device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl\n```\n\nI customized `Dockerfile.devel-gpu` as below.  I prepended `TF_CUDA_COMPUTE_CAPABILITIES=3.0` and  `TF_UNOFFICIAL_SETTING=1` for the `./configure` step.  \n\nShould I add the `libcuda.so` path to `ENV LD_LIBRARY_PATH`?\n\n``` Dockerfile\n# ........\n\nRUN git clone --recursive https://github.com/tensorflow/tensorflow.git && \\\n    cd tensorflow && \\\n    git checkout 0.6.0\nWORKDIR /tensorflow\n\n# Configure the build for our CUDA configuration.\nENV CUDA_TOOLKIT_PATH /usr/local/cuda\nENV CUDNN_INSTALL_PATH /usr/local/cuda\nENV TF_NEED_CUDA 1\n\nRUN TF_CUDA_COMPUTE_CAPABILITIES=3.0 TF_UNOFFICIAL_SETTING=1 ./configure && \\\n    bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package && \\\n    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip && \\\n    pip install --upgrade /tmp/pip/tensorflow-*.whl\n\nWORKDIR /root\n\n# Set up CUDA variables\nENV CUDA_PATH /usr/local/cuda\nENV LD_LIBRARY_PATH /usr/local/cuda/lib64\n\n# TensorBoard\nEXPOSE 6006\n# IPython\nEXPOSE 8888\n\nRUN [\"/bin/bash\"]\n```\n\nThen I built the image and ran it with `./docker_run_gpu.sh tf/tf`\n\nStarted tensorflow with: `python -m tensorflow.models.image.mnist.convolutional`\n\nAnd I'm getting `failed call to cuInit: CUDA_ERROR_UNKNOWN` when tensorflow starts up. But this is after reporting that it `successfully opened CUDA library libcuda.so locally`.\n\n```\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\n\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 8\nmodprobe: ERROR: ../libkmod/libkmod.c:556 kmod_search_moddep() could not open moddep file '/lib/modules/3.19.0-32-generic/modules.dep.bin'\nE tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUDA_ERROR_UNKNOWN\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:114] retrieving CUDA diagnostic information for host: 24b008aee65f\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: 24b008aee65f\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.68  Tue Dec  1 17:24:11 PST 2015\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04)\n```\n", "comments": ["i think i'm running into errors because modprobe fails in the container. \n\n```\nmodprobe: ERROR: ../libkmod/libkmod.c:556 kmod_search_moddep() could not open moddep file '/lib/modules/3.19.0-32-generic/modules.dep.bin'\n```\n\n I also needed to add my docker user to sudo group, so I did that.  but then `/lib/modules` wasn't mapped in the container, so i fixed that with the following addition to my run script\n\n```\nexport LIB_MODULES=$(\\uname -r | xargs -I{} echo '-v /lib/modules/{}:/lib/modules/{}')\n\n# ....\n\ndocker run -it $CUDA_SO $LIB_MODULES $DEVICES \"$@\"\n```\n\nbut now, when i run `sudo modprobe nvidia` i'm getting 'modprobe: FATAL: Module nvidia not found.'\n", "i was able to build on my local machine for now.  going to try again with docker later, but i think the issue was my host machine configuration.  for some reason the docker container is not permitted or cannot access the nvidia drivers.\n", "You should not need to run modprobe in the container, only on the host.\nOnce that runs, the /dev/nvidia\\* devices are accessible and you can launch\nthe container.\n\nOn Sun, Dec 27, 2015 at 3:52 PM, DC notifications@github.com wrote:\n\n> i was able to build on my local machine for now. going to try again with\n> docker later, but i think the issue was my host machine configuration. for\n> some reason the docker container is not permitted or cannot access the\n> nvidia drivers.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/601#issuecomment-167449139\n> .\n", "@ebrevdo k, i'm going to build my docker image once more and try again, but i believe i still get the `CUDA_ERROR_UNKNOWN` when i launch the Tensorflow MNIST example.  \n\nFrom what I recall, the `/dev/nvidia*` devices were missing in the container on my last build, even though `docker_run_gpu.sh` maps those files.  furthermore, i modified the docker gpu shell script to include the following bc these files seemed to be missing or inaccessible from the container.\n\n```\n# trying to make modprobe accessible\n# - making this available makes modprobe accessible\n# - but modprobe still can't find nvidia\nexport LIB_MODULES=$(\\uname -r | xargs -I{} echo '-v /lib/modules/{}:/lib/modules/{}')\n```\n\ni will update again after the container builds.\n", "also @ebrevdo, i've got a quick question which may save me a lot of time:\n\ni'm working with a 30GB image set, where the images range from 50MB-150MB.  i want to use the GPU version of OpenCV, which doesn't offer python bindings.  in order to perform OpenCV modifications in TensorFlow nodes, do i need to write my own TensorFlow ops?  And if so, can I do that without rebuilding the entire project? And a TensorFlow op be written in c++ outside of the main repository and just link back into it?  Or are there build artifacts used in the main build of TensorFlow that require all ops to be available? \n\nOr does using OpenCV in that way kind of defeat the purpose of TensorFlow.  I plan on using `SIFT`, possibly `SURF` (since i can GPU accel), colorspace + filter + segmentation + morphology and other algorithms.  There are only a few of these algorithms directly implemented in TensorFlow.  It would seem that I could compose some of the available API functions in TensorFlow to recreate most of the algorithms I could need, but doing so would take more time.  E.G., I don't know that I could simply/efficiently replicate SIFT in TensorFlow without spending quite a few hours.  \n\nThis is my first computer vision project and it is not classification, but quantifying attributes of types of objects in images to create a neural net prediction model for the same types of images.\n\nThese are the kinds of questions that would save me lots of time if I didn't need to search for the answer.  Is there a chatroom or something where I can pose questions like this to people working with OpenCV and TensorFlow?\n", "in addition to this, i logged my output when running `docker build . -t tf/tf`. I can upload that to a gist or something, if it would help.  i think there's something wrong with my host system.\n\nfrom inside the container, `ls -ll /dev/nvidia*` gives me:\n\n```\nroot@f206ea45c11e:~# ls -ll /dev/nvidia*\ncrw-rw-rw- 1 root root 195,   0 Dec 28 22:17 /dev/nvidia0\ncrw-rw-rw- 1 root root 195, 255 Dec 28 22:17 /dev/nvidiactl\n```\n\nmy docker container arguments from `./docker_run_gpu.sh` include: `/lib/modules/3.19.0-32-generic:/lib/modules/3.19.0-32-generic`, though i'm not sure this is required.  it was required for me to add this to the shell script in order to make the folder accessible within the container.  now it seems to be accessible (and the nvidia & nvidia-uvm driver files are included) and I'm no longer seeing the following error when i run tensorflow:\n\n```\nmodprobe: ERROR: ../libkmod/libkmod.c:556 kmod_search_moddep() could not open moddep file '/lib/modules/3.19.0-32-generic/modules.dep.bin'\n```\n\ninstead, i'm seeing:\n\n```\nmodprobe: FATAL: Module nvidia-uvm not found.\n```\n\ni'm still getting `CUDA_ERROR_UNKNOWN` when running MNIST Convolutional from inside the container:\n\n```\nmodprobe: FATAL: Module nvidia-uvm not found.\nE tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUDA_ERROR_UNKNOWN\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:114] retrieving CUDA diagnostic information for host: f206ea45c11e\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: f206ea45c11e\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.68  Tue Dec  1 17:24:11 PST 2015\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.68\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: \nInitialized!\nEpoch 0.00\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 84.6%\n```\n\n`nvidia-smi` tells me only ~200MB of GPU RAM are being used, but my CPU's are being hammered.  \n", "furthermore, when i run `python -m tensorflow.models.image.mnist.convolutional` after i loaded the container with my use (instead of the `docker` user i created to manage the docker service) i get slightly different output, shown below.  both are in my `docker` group.  i don't see the call to or error from modprobe.   as you've said modprobe access shouldn't be required, i don't know why i'm seeing it above or why there is a difference, since both users are in the docker group.  \n\n```\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nSuccesfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nSuccesfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nSuccesfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nSuccesfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nE tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUDA_ERROR_UNKNOWN\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:114] retrieving CUDA diagnostic information for host: ebc1748c6a6f\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: ebc1748c6a6f\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.68  Tue Dec  1 17:24:11 PST 2015\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.68\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: \nInitialized!\nEpoch 0.00\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 84.6%\n```\n\nlike i said, this isn't a huge deal, but i eventually need to run my software on an amazon `g2.2xlarge` instance, which also have compute capability 3.0, with `GRID K520` GPU's.  However, i don't anticipate running into these exact problems in the cloud.  i think the problem lies in my host system configuration, so i think amazon images should be cleaner. \n", "+1\n", "+1\n", "Hi All\nI have also been trying to have a healthy docker based developer environment. I have had some success. I have a Ubuntu 14 box with TitanX GPU. Here are some docker files and a runner script which might help.\n\nTensorflow Dockerfile: https://github.com/abhishekpatnia/Docker/blob/master/docker_images/Dockerfile.tensorflow\n\nAlso I wanted IDE support so I created a Docker image for Pycharm which can be triggered from inside a container. For example you can use the tensorflow image as base for this dockerfile.\nhttps://github.com/abhishekpatnia/Docker/blob/master/docker_images/Dockerfile.pycharmDE\n\nAlso have a helper script to run docker containers. This will allow you to have multiple sessions with the same container. It sets up proper ports and CUDA specific things.\nhttps://github.com/abhishekpatnia/Docker/blob/master/bin/dockersh\n", "+1\n", "+1\n", "@martinwicke, @aselle: Who's the best person for Docker issues?  This issue seems to have fallen through the cracks.   \n", "I had the same problem with running tensorflow on a Ubuntu machine after I upgraded my driver to 352.63 and 352.93. (I remember it works with 346.\\* but when I try to install 346._, it installs 352._ automatically for some reason). \n\nI finally figured out that it's caused by permission issue. (I can run it with root) So, I changed the permission of the libcuda.so.352-63 file to executable by anyone and it works well now. \n\nHope this will be helpful to those still struggling with this issue. \n\nI didn't try the docker one, but I guess it's also caused by permission setting.\n", "@caisq is this being worked on? Or should we reassign?\n", "No this is not fixed ... the solution of sudo apt-get install nvidia-modprobe does not work for some folks\n", "I'm not sure whether Linux on Mac falls in our scope of support. @gunan, @martinwicke , any thoughts?\n", "It is impossible for us to test for or even reproduce, so I don't see much we can do on our end. \n\nThat said, if someone figures it out and sends a patch with a fix, I'd be very excited to accept that. \n\nI've changed the labels appropriately.\n", "looks like there is no nvidia-docker support on OSX\r\nhttps://github.com/NVIDIA/nvidia-docker/issues/175\r\n\r\nTherefore, this seems to be blocked for us by nvidia docker.", "It was a permission issue for me. Thanks @PhoenixDai, following fixed it.\r\n\r\n`chmod 655 /var/lib/nvidia-docker/volumes/nvidia_driver/367.57/lib64/libcuda.so.367.57`", "I have chgmod 777 all libcuda.so.* files in /usr/lib, /usr/lib64 and /usr/local/cuda-8.0/, but still doesn't work. However I can successfully run under SU. ", "issue was solved after I add s authority to /usr/bin/nvidia-modprobe.", "I fix it by reboot the os but now it shows agains ,Did anyone knows the root reason ?"]}, {"number": 600, "title": "Translate (Seq2Seq) Tutorial Expectations", "body": "The output I am getting from the translate.py tutorial looks corrupted.\nhello -> G0\n\nI followed the translate tutorial here:\nhttps://www.tensorflow.org/versions/master/tutorials/seq2seq/index.html\n\nI wanted to create a small test model, limiting to 1M records\npython translate.py --data_dir data --train_dir train --en_vocab_size=40000 --fr_vocab_size=40000 --size=256 --num_layers=2 --steps_per_checkpoint=50 --max_train_data_size=1000000\n\nNo errors during the training.\n\nAfter 15 hours, I stopped the training when the perplexity was below 10 points.\nglobal step 19500 learning rate 0.2655 step-time 2.76 perplexity 8.63\n  eval: bucket 0 perplexity 13.15\n  eval: bucket 1 perplexity 10.71\n  eval: bucket 2 perplexity 12.78\n  eval: bucket 3 perplexity 14.38\n\nAfter stopping the training I deleted the last corrupted training file.\nrm train/translate.ckpt-19500\n\nIf I try to translate simple words I get junk.\npython translate.py --decode --data_dir data --train_dir train\nCreated model with fresh parameters.\n-> hello\nG0 Processing Processing Processing Processing Processing Processing Processing Processing Processing\n-> house\nG0 G0 p\u00e2turage d\u2019infrastructures d\u2019infrastructures Twin Twin Twin Twin Twin\n-> Who is the president of the United States?\nexp\u00e9di\u00e9es exp\u00e9di\u00e9es exp\u00e9di\u00e9es m0 m0 m0 m0 m0 m0 m0 m0 m0 m0 m0 m0\n\nOther translation technologies (es. Moses) will provide a decent translation with that training data.\n\nIf there a glitch somewhere or did I do something wrong?\n", "comments": ["I am also getting same error. Did you get the answer.\n", "Few more info to reproduce:\nI am using Mac OSX El Captain and TF 0.6\n\nI installed in this way:\n$ sudo easy_install pip\n$ sudo pip install --upgrade virtualenv\n$ virtualenv --system-site-packages /Users/marcotrombetti/Documents/tensorflow-python-env (io ho creato una cartella in Documents) \n$ source /Users/marcotrombetti/Documents/tensorflow-python-env/bin/activate  \n$ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py2-none-any.whl (Please note that the url is different compared to the documentation)\n\nFiles have been installed here:\n/Library/Python/2.7/site-packages/tensorflow/models/rnn/translate/\n\nAt one point,  I was missing the translate.py files (not sure if this was only related to a 0.5 problem), anyway I got it from here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/translate.py\n\n@vnkmr7620 did you test on a different configuration and/or release?\n", "Hi Marco. I think what you see is not a bug. For one, 2-layer network of size 256 is simply too small for translation. Second, I don't think you train it long enough. And neural network based methods require more data than hand-programmed systems like Moses. As specified in the tutorial - try running a 3-layer network with size 1024 for ~300-400K steps -- I think you'll see better results.\n", "would love to know if that fixed their issue\n", "Slightly related: How much memory do you guys have on your GPU? I have 3GB on my GPU, and 3 layers with size 1024 runs out of memory.\n", "Indeed, it runs out of memory on my 970 too. You can try to lower the batch size,  but it might become too slow. We're working on making TensorFlow more memory efficient but it will take a while. It does fit into 12GB, so you can run it on a Titan X / Black or Tesla K40 / K80, but these are quite expensive.\n", "I have a tesla k80 I'm building on. What does take a while look like? A week ? A month? \u00a0I have been training this model for 3 days now. I think for all the examples/tutorials we should include some basic runtime stats to give a ballpark.\n\nOn Fri, Feb 12, 2016 at 10:21 PM, Lukasz Kaiser notifications@github.com\nwrote:\n\n> ## Indeed, it runs out of memory on my 970 too. You can try to lower the batch size,  but it might become too slow. We're working on making TensorFlow more memory efficient but it will take a while. It does fit into 12GB, so you can run it on a Titan X / Black or Tesla K40 / K80, but these are quite expensive.\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/tensorflow/tensorflow/issues/600#issuecomment-183587119\n", "It might depend on your hyper-parameters. The tutorial model is not tuned and has a lot of data, so it would probably take months to fully converge. But you can tune it to converge faster, and it should get to reasonable perplexity in a few days anyway. Also, real-time is hard to jugde, can differ from machine to machine. What's your step number and perplexity -- that's the question to ask (and some step-number and perplexity ballpark are included in the tutorial).\n", "I have tried with different batch sizes but it still crashes. I am running it on an AWS x2-large GPU instance which has 15 GB RAM. it fails in the read_data part of translate.py script. when I do htop, I see that the memory usage keeps increasing. Can you please tell me how can I run the machine translation example on smaller data? currently its running on training-giga-fren.tar\n", "Hi! Can you try using this option: --max_train_data_size=500000 ? It should reduce the training data size, and currently the whole data-set is loaded into memory. You can also modify the data-loading function to load it batch-by-batch instead of loading it all into memory at once.\n", "@lukaszkaiser thanks for your inputs. I do have one more query. Iets say I have stopped training  at perplexity 15. Now I will run some tests and if I am not satisfied with the results ad want to see how the results will be for perplexity of \"8\", I will keep for retrain. The training starts from checkpoint 50, that is at a higher perplexity (1700), whereas I expect it to start at perplexity \"15\" the point where I had stopped initially. Is there a way to retrain from the checkpoint I had stopped at?\n", "This is strange: usually it'd start at the latest checkpoint. It's all saved in the --train_dir directory and the \"checkpoint\" file there specifies what's the latest checkpoint. Could it be that you used /tmp and your checkpoint got deleted, e.g., on restart?\n", "@lukaszkaiser  No, I see that the checkpoint data as well as other directories are intact. Caould there be any other reason?\n", "@lukaszkaiser you say that the training data should be huge. I tried with 10M sentences for English to French Translation. I see that the results are not good. I even tried to use AdagradOptimizer optimizer in place of GradientDescentOptimizer  in seq2seq_model.py. The perplexity of the first two buckets is below 10 for checkpoints after 15000.  Can you tell me if this is a good behavior?  Also at what point or at what value of perplexity will the training stop gracefully?\n", "I think the perplexity needs to go to around 4 for the results to be good. On a single GPU, this can take about a month of training with this simple tutorial script. There is a lot that can be sped up there, and better parameter settings can help, but I hope that gives you a general idea for the expectations.\n", "@lukaszkaiser \nIf I have multiple GPU's, will tensorflow use all of them for computations by default or I will have to configure it? \n", "It will, but the automatic placement currently might not be optimal. You can place manually with tf.device and that could make it faster (esp. if you place different LSTM layers on separate GPUs, as they can often work in parallel). I think this is a good question, but this bug is not the best place for it, as it's mostly unrelated and will be hard to find. If you want to ask about speeding up training, maybe open a stack overflow thread?\n", "Also, If I may ask, what's the minimum hardware requirement to be able to successfully train the model?", "You probably need 12GB or more. You can also try using fewer layers and/or smaller layers, and smaller batch sizes, if it's running out of memory.", "We're working on making the requirements much lower with dynamic seq2seq, see #4686 -- when that's in, it should be much easier to train such models.", "Does anyone have a model already pretrained that's available for download?  I just want to load up a pretrained model and see how good the translations are.  Information about how long it took to train and what kind of hardware was used would also be useful.", "@ekkus93 It seems that `tensorflow/models/rnn/translate.py` has disappeared from the master branch. In addition, I have also trained this model using GPU for one week but the outputs do not make any sense.", "We are planning to make a better model and tutorial as part of tensorflow-models. The model in core was intended as an illustration of more basic TF points, not a competitive translation model -- tensorflow-models repo seems to be a better place for that. Hope you can agree!", "Sure. Looking forward to the new intuitive illustration examples coming.", "Hi Guys,\r\nI was trying to use the command tf.device() in the translate example to be set on the GPU, it was working till the step of saving a check point and it crashed with a message pointing out that I need a CPU for this, so I removed the command, but I noticed that the step time is taking the same time when I was running the code without a GPU installed before, Do you know how can I make sure that the code is running on the GPU?", "If you are using NVIDIA GPUs, just type\r\n\r\n% nvidia-smi\r\n", "Thanks it seems that tensrflow is not utilizing the GPU, did you guys had to do anything specifically to run the example on the GPU ?\r\nI have CUDA and CuDNN installed, and I have tried the command tf.device(\"gpu\") before and it was working and the test went through! \r\nAny advice ?\r\n ", "translate.py uses GPU in not all the process. You should check the status of GPU by\r\n\r\n%nvidia-smi\r\n\r\nand\r\n\r\n%top\r\n\r\nwhen messages come out, or stopped. ( when running top, push \"1\" key to see every single CPU's performance).\r\nMaybe you can see your GPU is running.", "I think you need to use tf.device(\"/gpu:0\") or sth like that. You can also set soft placements in session (with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess) -- this will place on CPU all ops it cannot place on GPU, like the Saver.", "@lukaszkaiser , in general, for sequence to sequence models with extremely long sequences, how do you recommend fitting them into memory without endangering the attention mechanism?", "It depends how long. If it's into the thousands but not too high, say upto 4000 or so, our recent attention models from tensor2tensor (https://github.com/tensorflow/tensor2tensor) work quite well. If it's larger, then you need to think. Memory is only one problem, you also need to consider how will the network know where to propagate gradients, how will it find correlations if they are very-long-term? We managed to get sth working in many-thousands distance setting in the paper \"Learning to Remember Rare Events\", but at the cost of an extra loss that can be hard to train. I'd say it's an ongoing research problem at this point. But sometimes you can just work around it, e.g., put a bunch of strided convs or pooling first and down-size your input: if you don't need the specific tokens, just general summary, that often works best."]}, {"number": 599, "title": "improved array_ops.md", "body": "In my environment, the documentation of [reverse_sequence](https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#reverse_sequence) looks like this:\n\n---\n\n<img width=\"854\" alt=\"2015-12-23 23 47 57\" src=\"https://cloud.githubusercontent.com/assets/1457682/11978610/41dd6e20-a9d0-11e5-8f39-9aaba1c3aba2.png\">\n## \n\nIt seems like this is affected by no newline.\n", "comments": ["Like the other one, this needs fixing at the source: https://github.com/tensorflow/tensorflow/blob/091ed8cd4db82212fc6395ce4f1edf6b55ea2181/tensorflow/core/ops/array_ops.cc#L583\n", "OK, I've fixed it too.\n"]}, {"number": 598, "title": "SVG folder of tensorboard not found", "body": "if you run the mnist_fully_connected tutorial and inspect it with tensorboard afterwards, you find the following: \n\nthe graph-viz is calling this url for a image\nhttp://localhost:6006/lib/svg/summary-icon.svg\n\n<img width=\"1277\" alt=\"screen shot 2015-12-23 at 15 20 24\" src=\"https://cloud.githubusercontent.com/assets/9993908/11978047/bc880e52-a988-11e5-8478-c08ba0a65d43.png\">\n\nwhich is not there... \n", "comments": ["This file was removed in commit f7918e1dcd5b0c1f8114f488fc35a63a81e94535, so it's possible that you have a version conflict. (It sounds like you have an old version of some of the TensorBoard assets.) Can you try upgrading to TensorFlow 0.6.0 and see if this solves the problem?\n", "i built it from source and it happened... checked your @mrry suggestion and it fixed it.. \n\njust a quick reminder to keep everything in sync (at least try to) in the source as well :) \n"]}, {"number": 597, "title": "Can you add the paper references for each of the implementations?", "body": "Instead of labeling the constants and functions with description text, could you please add the paper references?\n\nThanks!\n", "comments": ["What are you referring to? \n", "Closing due to lack of activity.  Feel free to reopen if more information is available.\n", "@martinwicke: DId you read the question or did you just ignore it?\n@girving: Why did you close the question without a clear response?\n\nTensorflow is not written in a pedagogical manner but rather a opaque manner.  This should change with some paper references and a glossary section so people can drill-down on the vocab with sources cited.  I cannot believe that an open-source project would have two unrelated responses to my question in the last three months.  Where have the Tensorflow developers gone?\n", "@shyamalschandra: Martin asked you for specifics and you didn't reply, so I closed it due to lack of activity.  We would be happy to add specific references to papers to specific places in the code, but a blanket \"Add more documentation\" bug is hard to work with.\n", "@girving and @martinwicke: Okay, can you start by adding paper references to all the \"implementations\" inside the Tensorflow repository?  The version number is now around 2.0 preview.  Can you please tell the technical writers to add this information?", "There are many papers referenced in the docstrings. If you would like to add more, please send PRs. "]}, {"number": 596, "title": "Fixed typo in image.md", "body": "", "comments": ["You actually have to update it here: https://github.com/tensorflow/tensorflow/blob/8b5d9ed13f188662dde7cce75362cd2394bde40c/tensorflow/python/ops/image_ops.py#L115\n\nthen when we re-run our doc generation script, it will be fixed.\n", "Oops, I've fixed and pushed it now.\n"]}, {"number": 595, "title": "add default decay value for RMSPropOptimizer", "body": "Set RMSPropOptimizer decay parameter default value to 0.9, for simplicity.\n", "comments": ["Hm, looks like we're pretty inconsistent about default args for our optimizers.  In Adam we set default values for all parameters (including learning rate), for others, everything but learning rate has a default, and here everything but learning rate and decay has a default.  \n\nI'd be okay with this, since 0.9 seems to be a pretty common default from what I can tell, but @vincentvanhoucke for validation.\n\n(You should add documentation specifying the default in the docstring, at least).\n", "ok, I added default value to docstring\n", "Looks reasonable. @vrv what's the protocol for this? Looks like I have the right permissions to merge the pull request, is that something I can just go ahead with and all the merging takes care of itself?\n", "For simple changes like this, yeah.  But I would recommend that @aymericdamien sqaushes the commits into one first before merging.\n", "ok, I squashed the 2 commits together.\n"]}, {"number": 594, "title": "Will the tensorflow support nesterov momentum optimization?", "body": "Does anybody have experience about \"momentum\" and \"nesterov momentum\"?\n\ndoes the \"nesterov momentum\" perform much better than than \"momentum\"?\n", "comments": ["@qixianbiao working on this right now.\n", "@eduardo-elizondo, @qixianbiao are there any news on that? Probably a branch with a draft implementation?\n", "+1\n", "+1\n", "+1. Any update on this?\n", "`ApplyNesterovMomentumOp`'s computation of grad is differ from other optimization op, we should overide some functions in `optimizer.py`, I'll try it.\n", "@vrv Has this issue been resolved?\n", "Yup!\n"]}, {"number": 593, "title": "TensorArray failure with automatic gradient computation for nested scan/map_fn/fold", "body": "I drew up this simple example to show that nested While gradient computation fails in the python interface (as the gradContext for the forwardContext of the second input of the Add op is None). Toggling the comment demonstrates the issue is not with all control flow graphs.\n\n``` python\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\n\nOUTER_LOOP_MAX = 5\nINNER_LOOP_MAX = 3\n\ninner = tf.placeholder(\"float32\", [1, INNER_LOOP_MAX])\nouter = tf.placeholder(\"float32\", [1, OUTER_LOOP_MAX])\n\nX = tf.Variable(tf.zeros([1], dtype=\"float32\"))\n\nmax = tf.constant(1)\n\ndef outer_cond_func(c1, outer_acc, outer_array):\n    return tf.less(c1, OUTER_LOOP_MAX)\n\n\ndef outer_body_func(c1, outer_acc, outer_array):\n    concat = tf.concat(0, [[0], tf.expand_dims(c1, 0)])\n    slice = tf.slice(outer_array, concat, [1, 1])\n    outer_num = tf.reduce_sum(slice)\n\n    def inner_cond_func(c2, inner_acc, inner_array):\n        return tf.less(c2, INNER_LOOP_MAX)\n\n    def inner_body_func(c2, inner_acc, inner_array):\n        concat2 = tf.concat(0, [[0], tf.expand_dims(c2, 0)])\n        inner_num = tf.reduce_sum(tf.slice(inner_array, concat2, [1,1]))\n        inner_acc += inner_num * outer_num * X\n\n        c2 += 1\n        return c2, inner_acc, inner_array\n\n    _, inside_summed_products, _ = control_flow_ops.While(inner_cond_func, inner_body_func, [tf.constant(0), tf.constant(0.0), inner])\n\n    def true_func():\n        return 2*outer_num\n\n    def false_func():\n        return 3*outer_num\n\n    cond_num = control_flow_ops.cond(tf.less(c1,max),true_func, false_func)\n    outer_acc = tf.add(outer_acc, inside_summed_products)\n    # outer_acc = tf.add(outer_acc, cond_num)\n    c1 += 1\n    return c1, outer_acc, outer_array\n\n_, value, _ = control_flow_ops.While(outer_cond_func, outer_body_func, [tf.constant(0), tf.constant(0.0), outer])\ncontrol_flow_ops.switch()\n\nloss = value * X\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)\n\nsess = tf.Session()\ninit = tf.initialize_all_variables()\nsess.run(init)\n\nfor i in xrange(1, 10):\n    feed_dict = {inner: [[1.0, 2.0, 3.0]], outer: [[4.0, 5.0, 6.0, 7.0, 8.0]]}\n    print sess.run([train_op, loss], feed_dict=feed_dict)\n```\n", "comments": ["+1\n", "Thank you for reporting this issue.\n\nWhile is not a documented feature, partly because of issues like this.  We\nwill work on it but make no promises as to when it'll be ready.\n\nOn Wed, Dec 23, 2015 at 4:15 PM, Angel Darquea notifications@github.com\nwrote:\n\n> +1\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/593#issuecomment-167012425\n> .\n", "Cool. I'm happy to help work on it. Vijay suggested I post the bug in case you guys were changing things there, just so you knew I was doing it. I'm still having trouble getting my dev environment set up (being able to just test the python changes without bazel recompiling all the c++ every time I make a small change to the python code), but hopefully I'll get that resolved soon.\n", "You should be able to rerun your python test directly from bazel-bin\nbetween changes (the underlying code should be symlinked to the source\ncode) without rerunning bazel test ....\n\nOn Wed, Dec 23, 2015 at 7:35 PM, trubin notifications@github.com wrote:\n\n> Cool. I'm happy to help work on it. Vijay suggested I post the bug in case\n> you guys were changing things there, just so you knew I was doing it. I'm\n> still having trouble getting my dev environment set up (being able to just\n> test the python changes without bazel recompiling all the c++ every time I\n> make a small change to the python code), but hopefully I'll get that\n> resolved soon.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/593#issuecomment-167036212\n> .\n", "Thanks, I have got it working in my debugger, though I have to manually edit the auto generated bazel test file because even though I make changes to kernel_tests/control_flow_ops_test.py before i run \"bazel test //tensorflow/python:control_flow_ops_py_test\", the generated file in bazel-bin/tensorflow/python/control_flow_ops_py_test.runfiles/tensorflow/python/kernel_tests/control_flow_ops_test.py doesn't include my changes (this seems to be the only file that isn't a symlink). Maybe this is a different bug?\n", "I believe the bug is caused by the following logic:\n\nWhen computing a gradient (gradients.py::gradients() function), the has_control_flow bool is set to true when there is an \"Exit\" op anywhere in the subgraph under consideration (comes from _PendingCount()). Obviously this is true for a nested while. As we visit all the ops in the subgraph in reverse order [op being iterated in a queue of ops] , op is told that it is in a GradWhileContext through the function control_flow_ops.EnterGradWhileContext(op) (this is the only place in the code that function is called). EnterGradWhileContext creates a grad_ctxt for op's control_flow_context assuming that the control_flow_context is a WhileContext. \n\nSo far so good. The issue is that in a nested While, presumably the output of the inner while will be used as the input to some node used in returning the output of the outer While, so the op associated with the op of the inner while output tensor will not have made it into the queue of ops before the tensor is needed as an input, and so will never have EnterGradWhileContext called on it. \n\nI propose modifying EnterGradWhileContext to search down the tree of input ops/output tensors so that a grad_ctxt is created and entered recursively for all ops to be used, and then similarly the whole tree exited when ExitGradWhileContext is called on an op.\n\nNot sure who might be able to let me know if this is a good idea or not?\n", "I see that this is significantly more complicated than just entering a GradWhileContext for each input as the frame has to be the same when the Executor tries to run the kernels. Currently focusing on the forwardAccumulator of historical values for a given context to see if I can get the frames to match up.\n", "@trubin are you still working on this?\n", "This should have been fixed at head.\n", "I am still getting the error in 0.7.1. See the following test that demonstrates the error:\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\n\nOUTER_LOOP_MAX = 5\nINNER_LOOP_MAX = 3\n\ninner = tf.placeholder(\"float32\", [INNER_LOOP_MAX])\nouter = tf.placeholder(\"float32\", [OUTER_LOOP_MAX])\n\nX = tf.Variable(tf.zeros([1], dtype=\"float32\"))\n\ndef outer_cond_func(c1, outer_acc, outer_array):\n    return tf.less(c1, OUTER_LOOP_MAX)\n\ndef outer_body_func(c1, outer_acc, outer_array):\n    slice = tf.slice(outer_array, tf.expand_dims(c1, 0), [1])\n    outer_num = tf.reduce_sum(slice)\n\n```\ndef inner_cond_func(c2, inner_acc, inner_array):\n    return tf.less(c2, INNER_LOOP_MAX)\n\ndef inner_body_func(c2, inner_acc, inner_array):\n    inner_num = tf.reduce_sum(tf.slice(inner_array, tf.expand_dims(c2, 0), [1]))\n    inner_acc += inner_num * outer_num * X\n\n    c2 += 1\n    return c2, inner_acc, inner_array\n\n_, inside_summed_products, _ = control_flow_ops.While(inner_cond_func, inner_body_func, [tf.constant(0), tf.constant(0.0), inner])\n\nouter_acc = tf.add(outer_acc, inside_summed_products)\nc1 += 1\nreturn c1, outer_acc, outer_array\n```\n\n_, value, __ = control_flow_ops.While(outer_cond_func, outer_body_func, [tf.constant(0), tf.constant(0.0), outer])\n\nloss = value \\* X\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)\n\nsess = tf.Session()\ninit = tf.initialize_all_variables()\nsess.run(init)\n\nfor i in xrange(1, 10):\n    feed_dict = {inner: [1.0, 2.0, 3.0], outer: [4.0, 5.0, 6.0, 7.0, 8.0]}\n    print sess.run([train_op, loss], feed_dict=feed_dict)\n", "@ebrevdo To be a little clearer, the error happens at graph construction time (see below). I'd be really excited to help solve this bug, but the last time I went down this path it turned out major changes were being made to the entire ControlFlow infrastructure. Could you let me know if that's the case again? \n\nThanks,\nTyler\n\nTraceback (most recent call last):\n  File \"/home/tyler/code/Sphere/SideProjects/TKGVS/test.py\", line 40, in <module>\n    train_op = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 190, in minimize\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 241, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 481, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py\", line 365, in _MulGrad\n    return (array_ops.reshape(math_ops.reduce_sum(grad \\* y, rx), sx),\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 502, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 829, in mul\n    return _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2104, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1133, in __init__\n    self._control_flow_context.AddOp(self)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1286, in AddOp\n    self._AddOpInternal(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1307, in _AddOpInternal\n    self.AddValue(x)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1244, in AddValue\n    real_val = grad_ctxt.grad_state.GetRealValue(val)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 655, in GetRealValue\n    h_value = cur_grad_state.AddForwardAccumulator(cur_value)\nAttributeError: 'NoneType' object has no attribute 'AddForwardAccumulator'\n", "Yes, we made some major changes to support nested loops.  There is no pending major changes in the pipeline for now.  You are welcome to take a look at this problem.  \n", "(I do suggest trying to get a PR in in the next couple of weeks, though)\nOn Mar 21, 2016 4:18 PM, \"Yuan Yu\" notifications@github.com wrote:\n\n> Yes, we made some major changes to support nested loops. There is no\n> pending major changes in the pipeline for now. You are welcome to take a\n> look at this problem.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/593#issuecomment-199531386\n", "Hey all, sorry I haven't been able to get to this until now. @yuanbyu from the todos in the code it looks like you might have been involved in writing control_flow_ops.py. I'm looking at the function GetRealValue , and I'm a little confused by this code (line 635-636 of control_flow_ops.py in 7003b76)  :\n\n```\n      if self._outer_grad_state:\n        cur_grad_state = cur_grad_state.outer_grad_state\n```\n\nIt would appear that you are verifying that self._outer_grad_state is not None in the if block, but then attempting to access cur_grad_state.outer_grade_state, so maybe the check should be that cur_grad_state.outer_grad_state is not None?\n", "Tensorflow is still not able to handle backprop over nested scans.\n\n@yuanbyu @ebrevdo, any updates on this? \n", "This is a known limitation :(\n\nThis will require some revamping of the TensorArray and a bit of additional\nwork, to make it happen.  No ETA right now.\n\nOn Wed, May 18, 2016 at 3:59 PM, Sherjil Ozair notifications@github.com\nwrote:\n\n> Tensorflow is still not able to handle backprop over nested scans.\n> \n> @yuanbyu https://github.com/yuanbyu @ebrevdo\n> https://github.com/ebrevdo, any updates on this?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/593#issuecomment-220182751\n", "+1, this functionality would be super useful for some stuff I am working on. **Specific use case**: implementing Adaptive Computation Time from http://arxiv.org/abs/1603.08983, Graves, which requires a rnn cell which itself performs an undetermined number of internal iterations before progressing to the next timestep. As far as I can tell, this requires the use of two nested while loops, one via tf.scan and another for the internal iterations. Any other possible solutions/suggestions to get around this would be much appreciated!\n", "ACT only requires a while_loop inside the rnncell, not a scan, and does not require TensorArray except in the outer dynamic_rnn call. You should be able to implement it without running into this bug.\n", "Still no eta for fixing nested scan.\n", "@ebrevdo, thanks for your hints - I have created a SO for a question I have regarding this; your TF wisdom would be gratefully received. \nhttp://stackoverflow.com/questions/37822097/using-tensorarrays-in-the-context-of-a-while-loop-to-accumulate-values\n", "nested functional ops should now work (or at least the change will be pushed to github w/in the next week).  reopen if you're still having a problem after the next push.\n", "You guys rock, thanks.\n\nOn Thu, Jun 16, 2016, 8:38 PM ebrevdo notifications@github.com wrote:\n\n> Closed #593 https://github.com/tensorflow/tensorflow/issues/593.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/593#event-695482187, or mute\n> the thread\n> https://github.com/notifications/unsubscribe/AA2YOMapWqrfetM5Q-AhktmhR8Tti69Vks5qMhaagaJpZM4G6XIv\n> .\n", "Sweet. Cheers!\n", "This appears to still be an issue\n", "Fix should be available w/in a week or so.\n\nOn Thu, Aug 11, 2016 at 7:05 PM, Matt Sevrens notifications@github.com\nwrote:\n\n> This appears to still be an issue\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/593#issuecomment-239344650,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim0Oa4kahXMwQ5kGFfMZYvFrucROOks5qe9T4gaJpZM4G6XIv\n> .\n", "A possible fix should be available in the next push.\n", "\ud83d\udc4d \n", "This issue still persists, and it is already 2018. Shouldn't this be given high priority to work on ? Without nested higher order function, mini batch program won't be possible for a number of algorithms to be implemented in TensorFlow.", "@shengc please open a bug with details about what version of TF you're using and a minimal reproducible example, and we'll look into it.", "@ebrevdo opened issue here: https://github.com/tensorflow/tensorflow/issues/17555"]}, {"number": 592, "title": "How can I get Bazel to build iteratively?", "body": "In 0.5.0, I could change files (e.g. tensorflow/models/rnn/seq2seq.py) and recompile iteratively with no problem. In 0.6.0, making changes in files like tensorflow/python/ops/rnn_cell.py, etc, gives me pretty long recompile times. I'm not sure that it's doing a full rebuild, but it's def not the snappy few second rebuilds I was experiencing before. I didn't see an issue about this and so I'm wondering if it's something with my setup. Is this a common problem?\n", "comments": ["Non sequitur: AFAIK you don't need to recompile after making changes to python files; the built binary in bazel-bin just points the interpreter at the current version of the source file.\n", "Doesn't that only apply if you're using bazel to run as well? I am building into a pip pckg.\n", "I now can't seem to reproduce this even though it was happening consistently up till this morning. I'm not sure what's different now :/\n"]}, {"number": 591, "title": "tf.Print does not work on embedding gradients", "body": "Is this expected? because an embedding gradient is an IndexedSlices rather than a Tensor.\n", "comments": ["You can pass the indices and values to print separately.\nOn Dec 22, 2015 9:47 AM, \"William Chan\" notifications@github.com wrote:\n\n> Is this expected? because an embedding gradient is an IndexedSlices rather\n> than a Tensor\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/591.\n", "Seems like a reasonable feature to have.\n", "What would be a minimal code example, and what would be the desired outcome ? \n", "@Mistobaan: Not sure what you mean by a minimal code example.  The desired behavior would be that `tf.Print(input, data)` works even if `data` contains a mix of `IndexedSlices` objects and things that can be converted to tensors, and the desired outcome would be output that looks something like\n\n```\n# tf.Print(_, [xs, IndexedSlices(ys, zs)])\n... [x...] IndexedSlices([y...], [z...])\n```\n\nSince it would be nice if the new functionality allowed `Tensor` and `IndexedSlices` to be mixed in the same call to `Print`, the best way to implement it might be to add an optional list of extra strings to `Print`, perhaps as a \"extras: list(string) = []\" attr.  The above would then be implemented in Python as something like\n\n```\ngen_logging_ops._print(_, [xs, ys, zs], prefixes=[\"\", \"IndexedSlices(\", \", \", \")\"])\n```\n\ninside the Python wrapper version in `tf.Print`.\n", "Hi,\n\nI tried this today; I didn't have any problems printing out an IndexedSlices object, or even a tensor and an IndexedSlices together (the IndexedSlices is converted to a tensor before printing). So I would suppose this is fixed in the meantime; feel free to reopen with more information if it still doesn't work for you.\n"]}, {"number": 590, "title": "pip install does not install the examples folder", "body": "When doing on my Mac:\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py2-none-any.whl\nI got tensorflow to install, but without the `examples`directory. \nThere is no mention of that in the setup here:\nThe page https://www.tensorflow.org/versions/master/get_started/os_setup.html\n\nI needed the `examples` to do this tutorial:\nhttps://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html\n\n(for the time being I just downloaded the whole zip of the repository and copy-pasted the folder, and so far appears to work)\n\nDid I miss anything when installing? \n", "comments": ["I solved the same problem as follows:\n\nTry by using virtualenv with option --python=python2.7  After that activate the virtual environment and make the installation with pip2.7. I hope this works!\n", "I have the same problem, @sukruozan @elzurdo \ndo you run it like\uff1f   virtualenv --system-site-packages  --python=python2.7  ~/tensorflow \n", "Yes, and after that I used pip2.7 to install the corresponding wheel file. It worked just fine for me...\n", "I believe this has been fixed, closing. Comment if it's still a problem in the recent 0.8 release.\n", "I had this issue with version 6.0.8", "facing this issue using version 0.11 and python 3.5.2 within an anaconda environment", "still having this issue version 0.12, python 2.7.10", "while installing with pip, there has no image_training dir under examples dir", "I had this issue Python 2.7.10, tf .12, OSX 10.12.3 "]}, {"number": 589, "title": "add identity_initializer", "body": "Add identity initializer to initiate a variable with custom values.\n", "comments": ["A test for this might be nice.\n", "@vrv I added a test case for `identity_initializer` but I think `identity_initializer` should not have additional shape variable but we should by pass https://github.com/carpedm20/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L110. However, I think it is not possible to by pass this without adding an additional parameter because return value of `identity_initializer` is only a `_initializer` method. Is there any suggestion for this or do you think requesting `shape` is necessary for `identity_initializer`?\n", "@mrry and @lukaszkaiser might have better answers for you here, though most of us are on vacation for the next few days / weeks.\n", "How would this be used?  If you already know the whole value including shape, I thought you could just pass the value directly where the initializer would otherwise go.\n", "I'm planning to work on adding an option to just pass a value instead of an initializer in tf.get_variable. Maybe this initializer will not be needed then. Currently you can use \"lamda x: x\" as initializer, but we should have a better way soon.\n", "I've just made a CL that allows to use a Tensor as initializer in get_variable (should appear soon on git). I hope this closes this request, but feel free to re-open if needed.\n", "Can one of the admins verify this patch?\n", "@lukaszkaiser: let's only close issues once they are publicly available\n\n(i am pushing soon)\n"]}, {"number": 588, "title": "Added GPU implementation for resize nearest neighbor.", "body": "Also added appropriate tests/benchmarks. Implementation for backwards step is still missing and will follow in another PR. I tried to model it after the code used for maxpooling. \n", "comments": ["Rebased again on master due to changes in `tensorflow/python/ops/image_ops_test.py`\n", "@martinwicke could I please get some feedback on this?\n", "Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "Assuming the tests pass, LGTM.\n", "That is, after the line comment.\n", "Cool, can you squash your commits?  We'll use this change as a way to test our eventual GPU test slave, maybe.\n", "@vrv rebased on master and squashed, as requested.\n", "Thanks! I'll wait to test until our GPU test machine comes online to test this. Should be today, I hope.\n", "Sorry this is taking so long. Hardware is hard.\n", ":computer: :hammer: ?\n\n :smile: \n", "Jenkins, test this please.\n", "Rebased on master again.\n", "Jenkins, test this please.\n", "Can one of the admins verify this patch?\n", "doh, port.h was removed -- @panmari you'll probably have to update the code one more time :(\n", "On the plus side, we finally have working GPU tests now.\n\nOn Tue, Feb 16, 2016 at 1:42 PM Vijay Vasudevan notifications@github.com\nwrote:\n\n> doh, port.h was removed -- @panmari https://github.com/panmari you'll\n> probably have to update the code one more time :(\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/588#issuecomment-184885352\n> .\n", "Sure, will do! I'd have to adapt the code anyway to support the new flag\nintroduced recently...\n\nOn Tue, Feb 16, 2016, 23:55 Martin Wicke notifications@github.com wrote:\n\n> On the plus side, we finally have working GPU tests now.\n> \n> On Tue, Feb 16, 2016 at 1:42 PM Vijay Vasudevan notifications@github.com\n> wrote:\n> \n> > doh, port.h was removed -- @panmari https://github.com/panmari you'll\n> > probably have to update the code one more time :(\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub\n> > <\n> > https://github.com/tensorflow/tensorflow/pull/588#issuecomment-184885352>\n> > .\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/588#issuecomment-184908475\n> .\n", "I added another commit to support the newly added `align_cornes` flag and adapted some other things necessary. Unfortunately, on my local machine one of the tests in `//tensorflow/python:image_ops_test` is now failing. Could this be due to changes in the GPU allocator?\n", "What's the failure in particular?\n", "@vrv in the test where I compare the cpu resizing with the gpu resizing, I get completely different results there even though the other tests pass\n\n```\nFAIL: testCompareNearestNeighbor (__main__.ResizeImagesTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/panmari/.cache/bazel/_bazel_panmari/52e003c51984ee00b8d04ed6ebb99872/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/image_ops_test.runfiles/tensorflow/python/ops/image_ops_test.py\", line 990, in testCompareNearestNeighbor\n    self.assertAllClose(cpu_val, gpu_val, rtol=1e-5, atol=1e-5)\n  File \"/home/panmari/.cache/bazel/_bazel_panmari/52e003c51984ee00b8d04ed6ebb99872/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/image_ops_test.runfiles/tensorflow/python/framework/test_util.py\", line 435, in assertAllClose\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 1359, in assert_allclose\n    verbose=verbose, header=header)\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 713, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError: \nNot equal to tolerance rtol=1e-05, atol=1e-05\n\n(mismatch 100.0%)\n x: array([[[[  0.,   1.,   2.],\n         [  0.,   1.,   2.],\n         [  3.,   4.,   5.],...\n y: array([[[[  5.903004e+20,   1.406543e+01,   4.097450e-34],\n         [  5.903004e+20,   1.406543e+01,   4.097450e-34],\n         [  5.903004e+20,   1.406543e+01,   4.097450e-34],...\n\n----------------------------------------------------------------------\nRan 16 tests in 0.759s\n\nFAILED (failures=1)\n   1.50654290e-04   9.99999975e-06\n   5.90300359e+15   1.50654290e-04   9.99999975e-06   5.90300359e+15\n   1.50654290e-04   9.99999975e-06   5.90300359e+15   1.50654290e-04\n   9.99999975e-06   5.90300359e+15   1.50654290e-04   9.99999975e-06\n   5.90300359e+15   1.50654290e-04   9.99999975e-06   5.90300359e+15\n   1.50654290e-04   9.99999975e-06   5.90300359e+15   1.50654290e-04\n   9.99999975e-06   5.90300359e+15   1.50654290e-04   9.99999975e-06\n   5.90300359e+15   1.50654290e-04   9.99999975e-06   5.90300359e+15\n   1.50654290e-04   9.99999975e-06   5.90300359e+15   1.50654290e-04\n   9.99999975e-06   5.90300359e+15   1.50654290e-04   9.99999975e-06]\narea_ratio_hist  [ 70  73  90 100 133 154 162  95  80  43]\n```\n", "@panmari: isn't that specifically the kernel you added that is showing different behavior between CPU and GPU?  That probably needs to be fixed, unless I'm mis-reading the error.\n", "@vrv sorry, it took some time until I got around fixing this. Seemed like `np.float32` was interpreted differently due to some change since 0.6, leading to the failing test. I generalized my code a bit to make an extension to other types more easy.\n", "@tensorflow-jenkins: test this please\n\n(i swear, we'll get this in!)\n", "Merged, woohoo!\n", "Cool, glad it finally happened :smile: \n"]}, {"number": 587, "title": "Android example finally works", "body": "Finally was able to compile and install to Nexus 7 (2013) Marshmallow 6.0.1 with:\nbazel mobile-install //tensorflow/examples/android:tensorflow_demo -c opt --start_app --copt=-mfpu=neon \n\nCrashes because no camera permission the first run so drop the --start_app  \n\nSimple solution is to use ->settings -> apps -> Tensorflow Demo -> Permissions \nand enable both storage and camera.\n\nSo I'm wondering about the --copt=-mfpu=neon \n\nTensorflow demo runs amazingly fast (a few seconds) on the Nexus 7 compared to waiting 10 minutes or so for the example label_image to produce results on an older Dell  - Intel(R) Core(TM)2 CPU         T7200  @ 2.00GHz\n", "comments": ["Thanks for the update. Closing as it's not an issue.\n"]}, {"number": 586, "title": "[feature request] numpy style `shape` sugar", "body": "I'd like to suggest numpy-like `shape` read-only property for `tf.Tensor`, something like:\n\n```\n@property\ndef shape(self):\n  return tuple(self.get_shape().as_list())\n```\n", "comments": ["I'd prefer not to have two ways to access (subtly different forms of) the same information. However, I wouldn't be opposed to changing `Tensor.get_shape()` to be a `Tensor.shape` property. Ideally you could use a (fully defined) `TensorShape` anywhere a list of integers is accepted. Would that work for your purposes?\n", "It is OK for me even now as is, the question what is the optimal / general way ;)\n\nIn my opinion, I see two possible solution based on what you said and on criterion _whether the `TensorShape` object can be used to initialize other objects or can be assigned to some other variables and carries information more than just a tuple of integers_:\n\n(1) if `TensorShape`  has practical significance as is: implement both methods; maybe reimplement `get_shape` as a property (called something like `tshape` for tensor shape)\n\n(2) if `TensorShape` is used only for internal convenience, change to the above suggested property\n", "Are there any plans on changing this? @DSLituiev's comment sounds very reasonable to me.\n", "There are no plans to change the use of `TensorShape`, and we're happy to accept suggestions/PRs that would make `TensorShape` usable wherever a list of integers would be.\n", "What is the use of `TensorShape`? Is it ever used downstream of `get_shape()`? As I understand, it cannot even be fed into `shape` parameter upon tensor initialization. \n", "Do you imply that you'll oppose `.shape` returning simple tuple because there is a downstream component that makes use of `TensorShape` data structure that has something other than what is contained in the tuple?\n", "Tensors in TensorFlow can have dynamic shapes. `TensorShape` is used throughout the Python API to represent shapes that might have one or more unknown dimensions, or even an unknown rank; and to combine such shapes easily.\n\nSo, yes, I'd be opposed to having two slightly incompatible properties/methods on `Tensor` that convey the same information, but cannot be used the same way. If we add `Tensor.shape`, it should return `TensorShape`, and we should deprecate `Tensor.get_shape()` at the same time.\n\nWhich tensor initialization function doesn't support `TensorShape` objects as the `shape` argument? We should fix that.\n", "@mrry  I totally agree that we shouldn't have both `.shape` and `.get_shape()`. One problem I have with the current `TensorShape` is that it cannot be combined into new shapes easily; please see the example below. Also, I think it's worth switching to the shorter and Numpy-equivalent `.shape` now (rather than waiting and breaking more code later).\n\n``` python\n>>> import tensorflow as tf\n>>> a = tf.placeholder(tf.float32, [None, None, None])\n>>> b = tf.reshape(a, [a.get_shape()[0], 10, -1])\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1092, in reshape\n    name=name)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/op_def_library.py\", line 411, in apply_op\n    as_ref=input_arg.is_ref)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 566, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/constant_op.py\", line 179, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/constant_op.py\", line 162, in constant\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\", line 332, in make_tensor_proto\n    _AssertCompatible(values, dtype)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\", line 272, in _AssertCompatible\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\nTypeError: Expected int32, got Dimension(None) of type 'Dimension' instead.\n>>> \n```\n", "probably this was the function. On the other hand, `tf.placeholder` accepts both `TensorShape` and integer tuples:\n\n```\ny = tf.placeholder(np.dtype(\"float\"), shape = (None,1,3) )\ny.get_shape()\n\n\"equivalent to\"\ny = tf.placeholder(np.dtype(\"float\"), shape =\n                   tf.TensorShape([tf.Dimension(None), tf.Dimension(1), tf.Dimension(3)])\n                    )\n```\n\n@mrry, you must know better, but I never yet faced an expression where I had to feed `TensorShape` and tuple (well, sometimes list) of integers did do the job.\n", "I mean: I understand that `TensorShape` is a convention for Python API, but I have not seen a case where it is what the final user might need. Do you have documentation on how the unknown rank is represented?\n", "@mrry: Should this be contributions welcome, or do we still have plans to do our own refactoring of `.shape` vs. `.get_shape`?\n", "We can't&mdash;or, at least, _really_ shouldn't&mdash;accept a contribution on this until the internal refactoring is done. However, the internal refactoring is P3, so it might be some time before it rises to the top of the pile.\n", "Sounds good, let's leave it as is.\n", "@mrry Any news on the internal refactoring?\n", "@mrry friendly ping?\n", "This remains blocked on a low-priority internal cleanup. I'd be happy to hand it off to somebody who is looking for something to do, but&mdash;due to the nature of the conflict&mdash;they would need to be a Google employee, \n", "@mrry Maybe I can help? I'm currently interning at Google MTV, feel free to ping me.\n", "@danijar I'll be in touch!\n", "What's the status of this? @yifeif may be able to help.\n", "That would be perfect. I don't have the time for it, unfortunately. Can you contact @mrry?\n", "Fixed by commit above."]}, {"number": 585, "title": "Failed installation on Mac OS X El Capitan", "body": "When I run the suggested command: \n\npip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\n\nI get the following error:\n\ntensorflow-0.5.0-py2-none-any.whl is not a supported wheel on this platform.\n\nPlease advise.\n- Tom\n", "comments": ["Can you try again with the latest release?\n\n``` bash\n$ pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py2-none-any.whl\n```\n\nIf that doesn't work, can you also please share information about your Python version?\n", "Python 3.5.1.  I am having the same issue.\n", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#pip-installation-pip_install -- did you use pip3 ?  Full command line and error message would be helpful.\n", "No, I did not.  But, it seems the problem was that I was using pip and the older version of the wheel.  If I change the wheel to use py3, then it works.  I do not seem to have pip3 installed.  It may be that pip is referring to Python 2 that was installed with my machine and then I installed anaconda and python 3 on my own.  So, maybe my path does not have pip3.  Anyway, I did this with pip and it worked.\n\npip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py3-none-any.whl\n\nI do not know if that will cause other problems in the future though.\n", "Right, this worked for me. Sorry, I guess I got confused and used the python 2 link instead of python 3. Using the following command line worked (as outlined in Install documentation): \nsudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py3-none-any.whl\n"]}, {"number": 584, "title": "[doc] Add anchor links to headers in api_doc files", "body": "Currently, there is no good way to link to sections of the API\ndocumentation. While there are links to sections/methods in the sidebar\nand in 'table of contents'-like areas of the doc, it would be better if\nthere was a way to grab a link directly from the section header/method\nname itself.\n\nThis commit achieves this by simply adding a markdown link to headers\nthat include an #id in braces. For example:\n    `### This Awesome Header {#awesome-header}`\nBecomes\n    `### [This Awesome Header](#awesome-header) {#awesome-header}`\n\nThe hope is that by making it easier to reference specific sections of\nthe documentation, it will encourage third-parties to create tutorials\nand pages that point directly to the subject matter at hand.\nAdditionally, it will become easier for colleagues to share reference\nlinks to one another.\n\nThis was done using the script available here:\n[https://github.com/samjabrahams/tensorflow_util/blob/master/py/add_header_permalink.py](https://github.com/samjabrahams/tensorflow_util/blob/master/py/add_header_permalink.py)\n\nIt's possible that making the entire header a link is not the best way\nto set up anchor links, but changing the specific implementation is not\na big deal. For example, if it's decided that there should be a 'chain'\nicon to the side of headers instead of making the entire header a link,\nthat should be easy to patch in.\n\nOne thing that should be considered, if this patch is accepted as-is,\nis the CSS styling of headers vs links on the TensorFlow website. If\nthings go wonky (i.e. headers become orange), I believe the following\naddition to main.css will suffice:\n\n``` css\n.block-copy h1 a, .block-copy h2 a, .block-copy h3 a, .block-copy h4 a,\n.block-copy h5 a {\n    color: #000;\n}\n```\n\nI know there's a bunch of lines to examine, but let me know what tweaks you may have to this idea!\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks, this is nice. Sadly, we won't be able to use the PR, because all the HTML is already auto-generated. I will add something like this to our processing pipeline. I'll probably go for the hover chain icon though. I'll leave this open until I actually do this.\n", "Cool, thanks for getting back on this. I should have realized that the API doc was auto-generated, whoops!\n\nShould be a fairly easy tweak on your end (once the hover-chain CSS is settled!). Let me know if there's anything related to this you'd like to delegate.\n", "Eventually I want the whole doc generation pipeline to be open source, so\npeople can preview their edits properly (and can fix issues in the\nprocessing), but we're not quite there yet.\n\nI'll get to this eventually.\nOn Fri, Dec 25, 2015 at 13:21 Sam Abrahams notifications@github.com wrote:\n\n> Cool, thanks for getting back on this. I should have realized that the API\n> doc was auto-generated, whoops!\n> \n> Should be a fairly easy tweak on your end. Let me know if there's anything\n> related to this you'd like to delegate.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/584#issuecomment-167262917\n> .\n", "Can one of the admins verify this patch?\n"]}, {"number": 583, "title": "Multiple CPU usage ineffective", "body": "I'm running tensorflow on a machine with 64 CPUs and no GPUs. I noticed that although tf detects the number of CPU's correctly (\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 64\nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 64\n), the total CPU usage is less than 16 most of the time. It increases to 20-30 occasionally but for a very short period of time. And even that is much lower than 64. \n", "comments": ["Can you try with a plan that is naively paralelizable to make sure it's not communication bottleneck somewhere?\n\nFor instance, rename the attached file into \"parallel.py\" and then run it as \"python parallel.py <numparallelism>\". On my macbook pro I get following numbers which shows linear scaling up to 4 cores\n\npython ~/g/src/parallel.py 1\ndone in 0.99, 10.13 ops/sec\n\npython ~/g/src/parallel.py 2\ndone in 0.97, 20.58 ops/sec\n\npython ~/g/src/parallel.py 3\ndone in 1.02, 29.55 ops/sec\n\npython ~/g/src/parallel.py 4\ndone in 1.04, 38.29 ops/sec\n\npython ~/g/src/parallel.py 5\ndone in 1.33, 37.45 ops/sec\n\n[parallel.txt](https://github.com/tensorflow/tensorflow/files/70372/parallel.txt)\n", "Two things:\n1. Here's what it writes to stderr:\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 32\nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 32\nI tensorflow/core/kernels/logging_ops.cc:79] [heavy op #0]\nI tensorflow/core/kernels/logging_ops.cc:79] [heavy op #0]\nI tensorflow/core/kernels/logging_ops.cc:79] [heavy op #0]\nI tensorflow/core/kernels/logging_ops.cc:79] [heavy op #0]\nI tensorflow/core/kernels/logging_ops.cc:79] [heavy op #0]\nI tensorflow/core/kernels/logging_ops.cc:79] [heavy op #0]\nI tensorflow/core/kernels/logging_ops.cc:79] [heavy op #0]\nI tensorflow/core/kernels/logging_ops.cc:79] [heavy op #0]\nI tensorflow/core/kernels/logging_ops.cc:79] [heavy op #0]\nI tensorflow/core/kernels/logging_ops.cc:79] [heavy op #0]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x7f77dc0125c0 Compute status: Out of range: FIFOQueue '_0_fifo_queue' is closed and has insufficient elements (requested 1, current size 0)\n         [[Node: fifo_queue_Dequeue = QueueDequeue[component_types=[DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](fifo_queue)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x7f77dc012de0 Compute status: Aborted: Queue '_0_fifo_queue' is already closed.\n\n##          [[Node: fifo_queue_Close_1 = QueueClose[cancel_pending_enqueues=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](fifo_queue)]]\n\nIs that ok?\n2. I run it with the following line:\nfor i in `seq 1 64`; do echo $i `./parallel.py $i 2>/dev/null`; done\nand here's the output:\n1 done in 1.38, 7.26 ops/sec\n2 done in 1.45, 13.79 ops/sec\n3 done in 1.65, 18.21 ops/sec\n4 done in 1.70, 23.50 ops/sec\n5 done in 1.73, 28.98 ops/sec\n6 done in 1.82, 33.05 ops/sec\n7 done in 1.86, 37.70 ops/sec\n8 done in 1.88, 42.65 ops/sec\n9 done in 1.73, 52.05 ops/sec\n10 done in 1.92, 52.03 ops/sec\n11 done in 2.07, 53.13 ops/sec\n12 done in 2.17, 55.29 ops/sec\n13 done in 1.83, 70.92 ops/sec\n14 done in 2.05, 68.13 ops/sec\n15 done in 2.36, 63.45 ops/sec\n16 done in 2.48, 64.64 ops/sec\n17 done in 2.03, 83.89 ops/sec\n18 done in 1.94, 92.69 ops/sec\n19 done in 2.51, 75.74 ops/sec\n20 done in 2.31, 86.41 ops/sec\n21 done in 2.04, 102.80 ops/sec\n22 done in 2.29, 96.04 ops/sec\n23 done in 2.19, 105.18 ops/sec\n24 done in 2.08, 115.29 ops/sec\n25 done in 2.39, 104.58 ops/sec\n26 done in 2.18, 119.52 ops/sec\n27 done in 2.36, 114.33 ops/sec\n28 done in 2.43, 115.42 ops/sec\n29 done in 2.77, 104.66 ops/sec\n30 done in 2.27, 132.39 ops/sec\n31 done in 2.57, 120.63 ops/sec\n32 done in 2.54, 126.02 ops/sec\n33 done in 2.59, 127.47 ops/sec\n34 done in 2.57, 132.32 ops/sec\n35 done in 2.50, 139.96 ops/sec\n36 done in 2.34, 153.63 ops/sec\n37 done in 2.71, 136.65 ops/sec\n38 done in 2.70, 140.72 ops/sec\n39 done in 2.71, 144.05 ops/sec\n40 done in 3.07, 130.32 ops/sec\n41 done in 2.73, 150.23 ops/sec\n42 done in 3.22, 130.30 ops/sec\n43 done in 2.82, 152.70 ops/sec\n44 done in 3.09, 142.43 ops/sec\n45 done in 2.68, 167.86 ops/sec\n46 done in 2.69, 170.91 ops/sec\n47 done in 2.73, 172.44 ops/sec\n48 done in 2.77, 173.29 ops/sec\n49 done in 3.28, 149.27 ops/sec\n50 done in 2.92, 171.16 ops/sec\n51 done in 3.25, 157.13 ops/sec\n52 done in 3.11, 167.12 ops/sec\n53 done in 3.32, 159.57 ops/sec\n54 done in 2.93, 184.45 ops/sec\n55 done in 3.46, 158.80 ops/sec\n56 done in 3.44, 162.72 ops/sec\n57 done in 3.26, 174.85 ops/sec\n58 done in 3.35, 173.29 ops/sec\n59 done in 3.72, 158.64 ops/sec\n60 done in 3.38, 177.43 ops/sec\n61 done in 3.50, 174.10 ops/sec\n62 done in 3.47, 178.43 ops/sec\n63 done in 3.97, 158.81 ops/sec\n\n## 64 done in 3.27, 195.81 ops/sec\n\nSo it does scale up to 64 threads...\n", "I've tested on 40 threads (2x CPU) and got the attached results.\n![tensorflowscalability](https://cloud.githubusercontent.com/assets/5303155/12011366/22076a50-acd3-11e5-8c10-f821a10e0cfc.png)\n\nSlow increase in 10-20 and 30-40 threads range probably connected to hyper-threading limitations.\nBut `models/mnist/convolutional.py` rarely occupies more than 25% of total CPU.\n", "This could be related to https://github.com/tensorflow/tensorflow/issues/551: the code ends up spending a lot of time locking and unlocking a mutex in the threadpool code.\n", "This is very likely the case.  With 8 vCPUs, the mutex locking is nearly undetectable.  With 16, it is about 10% of the runtime.  With 32, it dominates.  I can only imagine that with 64 it's completely unusable.\n\nIf you replace the mutex in the thread pool with a spinlock (I copied the source from the one here: https://github.com/mldbai/mldb/blob/master/arch/spinlock.h), does it change the shape of the graph?\n\nI have started work on a lockless threadpool implementation; it's complicated by the fact that thread pools are created and destroyed dynamically, so it needs to be able to deal with threads coming and going.\n", "@tridemax PR #763 may address the issue.  If it is easy to re-run the tests on that branch, it would be good to know what the effect is.\n", "Confirmed: part of this is caused by the thread pool locking.  The PR in #763 nearly halves the elapsed wall time to train the mnist convolutional.py example:\n\nfrom:\n\n```\n2464.97user 767.13system 7:48.46elapsed 689%CPU (0avgtext+0avgdata 655848maxresident)k\n0inputs+0outputs (0major+84760819minor)pagefaults 0swap\n```\n\nto:\n\n```\n2827.11user 724.31system 4:19.43elapsed 1368%CPU (0avgtext+0avgdata 1441204maxresident)k\n0inputs+0outputs (0major+61934308minor)pagefaults 0swaps\n```\n\nWe still only manage to 1/2 utilize the CPU, however.  Further improvements would probably come from splitting the minibatch into multiple shards that execute the whole graph independently, so that the holes during the reduction operations can be filled with convolutions from another batch.\n", "@jeremybarnes I've tested this on our experimental Windows build (as this server is Windows-based), so there is no direct way for me to test your branch. =(\n", "How can I expose this contention on a realistic program? Preferably from C++?\n\nI've tried to modify `tensorflow/examples/label_image/main.cc` to execute session->Run in parallel:\n\n``` c\n--- a/tensorflow/examples/label_image/main.cc\n+++ b/tensorflow/examples/label_image/main.cc\n@@ -32,6 +32,9 @@ limitations under the License.\n // The googlenet_graph.pb file included by default is created from Inception.\n\n #include <fstream>\n+#include <thread>\n+#include <memory>\n+#include <memory>\n\n #include \"tensorflow/cc/ops/const_op.h\"\n #include \"tensorflow/cc/ops/image_ops.h\"\n@@ -290,15 +293,22 @@ int main(int argc, char* argv[]) {\n   }\n   const Tensor& resized_tensor = resized_tensors[0];\n\n-  // Actually run the image through the model.\n-  std::vector<Tensor> outputs;\n-  Status run_status = session->Run({{input_layer, resized_tensor}},\n+  const int N = 20;\n+  std::vector<std::unique_ptr<std::thread>> threads(N);\n+  for (int i = 0; i < N; i++)\n+    threads[i].reset(new std::thread([&]() {\n+      // Actually run the image through the model.\n+      std::vector<Tensor> outputs;\n+      Status run_status = session->Run({{input_layer, resized_tensor}},\n                                    {output_layer}, {}, &outputs);\n-  if (!run_status.ok()) {\n-    LOG(ERROR) << \"Running model failed: \" << run_status;\n-    return -1;\n-  }\n+      if (!run_status.ok()) {\n+        LOG(ERROR) << \"Running model failed: \" << run_status;\n+      }\n+    }));\n+  for (int i = 0; i < N; i++)\n+    threads[i]->join();\n\n+  /*\n   // This is for automated testing to make sure we get the expected result with\n   // the default settings. We know that label 866 (military uniform) should be\n   // the top label for the Admiral Hopper image.\n@@ -321,6 +331,6 @@ int main(int argc, char* argv[]) {\n     LOG(ERROR) << \"Running print failed: \" << print_status;\n     return -1;\n   }\n-\n+  */\n   return 0;\n }\n```\n\nand to create multiple sessions as well:\n\n``` c\n--- a/tensorflow/examples/label_image/main.cc\n+++ b/tensorflow/examples/label_image/main.cc\n@@ -32,6 +32,9 @@ limitations under the License.\n // The googlenet_graph.pb file included by default is created from Inception.\n\n #include <fstream>\n+#include <thread>\n+#include <memory>\n+#include <memory>\n\n #include \"tensorflow/cc/ops/const_op.h\"\n #include \"tensorflow/cc/ops/image_ops.h\"\n@@ -268,13 +271,18 @@ int main(int argc, char* argv[]) {\n     return -1;\n   }\n\n+  const int N = 20;\n+  std::vector<std::unique_ptr<std::thread>> threads(N);\n+  for (int i = 0; i < N; i++)\n+    threads[i].reset(new std::thread([&]() {\n+\n   // First we load and initialize the model.\n   std::unique_ptr<tensorflow::Session> session;\n   string graph_path = tensorflow::io::JoinPath(root_dir, graph);\n   Status load_graph_status = LoadGraph(graph_path, &session);\n   if (!load_graph_status.ok()) {\n     LOG(ERROR) << load_graph_status;\n-    return -1;\n+    //return -1;\n   }\n\n   // Get the image from disk as a float array of numbers, resized and normalized\n@@ -286,19 +294,22 @@ int main(int argc, char* argv[]) {\n                               input_std, &resized_tensors);\n   if (!read_tensor_status.ok()) {\n     LOG(ERROR) << read_tensor_status;\n-    return -1;\n+    //return -1;\n   }\n   const Tensor& resized_tensor = resized_tensors[0];\n\n-  // Actually run the image through the model.\n-  std::vector<Tensor> outputs;\n-  Status run_status = session->Run({{input_layer, resized_tensor}},\n+      // Actually run the image through the model.\n+      std::vector<Tensor> outputs;\n+      Status run_status = session->Run({{input_layer, resized_tensor}},\n                                    {output_layer}, {}, &outputs);\n-  if (!run_status.ok()) {\n-    LOG(ERROR) << \"Running model failed: \" << run_status;\n-    return -1;\n-  }\n+      if (!run_status.ok()) {\n+        LOG(ERROR) << \"Running model failed: \" << run_status;\n+      }\n+    }));\n+  for (int i = 0; i < N; i++)\n+    threads[i]->join();\n\n+  /*\n   // This is for automated testing to make sure we get the expected result with\n   // the default settings. We know that label 866 (military uniform) should be\n   // the top label for the Admiral Hopper image.\n@@ -321,6 +332,6 @@ int main(int argc, char* argv[]) {\n     LOG(ERROR) << \"Running print failed: \" << print_status;\n     return -1;\n   }\n-\n+  */\n   return 0;\n }\n```\n\nBut in both cases it nicely consumes all my cores, no significant time spent in system and top functions in profile are all doing useful work:\n\n```\n+  20.62%  label_image  label_image           [.] float __vector(4) Eigen::internal::pmul<float __vector(4)>(float __vector(4) const&, float __vector(4) const&)\n+  16.66%  label_image  label_image           [.] void Eigen::internal::gebp_traits<float, float, false, false>::madd<float __vector(4), float __vector(4), float __vector(4)>(float __vector(4) const&, floa\n+  15.80%  label_image  label_image           [.] float __vector(4) Eigen::internal::padd<float __vector(4)>(float __vector(4) const&, float __vector(4) const&)\n+   6.55%  label_image  label_image           [.] float __vector(4) Eigen::internal::pload<float __vector(4)>(Eigen::internal::unpacket_traits<float __vector(4)>::type const*)\n+   6.20%  label_image  label_image           [.] void Eigen::internal::pbroadcast4<float __vector(4)>(Eigen::internal::unpacket_traits<float __vector(4)>::type const*, float __vector(4)&, float __vector(4\n+   4.57%  label_image  label_image           [.] Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 8, 4, false, false>::operator()(Eigen::internal::bla\n+   4.25%  label_image  label_image           [.] Eigen::internal::TensorIntDivisor<long, false>::divide(long) const\n```\n\nCan somebody suggest a modification to existing C++ examples that would expose the contention? thanks.\n\nI am on commit d4422ff4b2f142de1d0c626f73c734655d340e0d (Jan 26).\n", "label_image seems to deadlock (#929)\nso any good parallel benchmarks?\n", "@dvyukov I ran the mnist training example distributed with Tensorflow (it's Python, but it exercises the C++ code).  Unfortunately Reviewable swallowed the comment with the exact command line.  Note that you need to have a machine with more than 16 vcores and no GPU enabled to see the contention.\n", "@jeremybarnes thanks for mnist pointer. FTR the command line is:\n\n```\n$ bazel build --dynamic_mode=off -c opt tensorflow/models/image/mnist:convolutional\n$ bazel-bin/tensorflow/models/image/mnist/convolutional\n```\n\nHa, that also deadlocks for me.\nI think the reason why you do not see deadlocks is that your pool implementation actually provides FIFO for _externally_ submitted tasks accidentally (or maybe not!) _and_ that tasks that require FIFO ordering are exactly externally submitted. And it is also important that these tasks are submitted from a single external thread. If anything of the above becomes false, then I think you will also see deadlocks.\n\nI can kind-a work around deadlocks by creating 8x more threads than requested. But that results in 800 threads created (there are 2 thread pools in process (why?)). And I guess it's not guaranteed to work anyway.\n", "Doh. I've added a global FIFO queue for externally submitted work and deadlocks go away. The pool is still faster than the default one. But what I see is that _all_ work is on the global queue. So what provides speedup in our cases is merely thread spinning before blocking. Ouch. If we remove the worker work queue, I bet it will become just faster...\n\nSuch parallel algorithms are not going to scale whatever pool is used for scheduling. The parallelization part needs to be rewritten.\n", "Could we just end up using kind of automatic multi-towering for large CPUs, like with multi-GPU case? TensoFlow is supposed to have clustering technique built-in, so this should be an easy pick?\n", "@tridemax Do you mean some kind of higher-level partitioning?\n\nEven when I run tensorflow/models/image/mnist/convolutional with just 4 threads, it is still unable to utilize 4 cores (only about 3.5). The partitioning will also have some overheads for communicate between partitions (I don't know how large it is, though).\nOverall it does not look like good strategy long-term. Partitioning is used to mitigate inherent communication overheads. In this case we try to mitigate artificial overheads. Real cost of shared memory synchronization is pretty low.\n", "@dvyukov When I looked at the low CPU occupancy originally, I saw two main reasons:\n1.  Massive contention on the lock for the global work queue.  In order to solve point 2 below, we had structured our work into multiple overlapping and independent jobs, with a reduction at the end, so there were definitely lots of threads simultaneously submitting work.  The lock contention was overwhelming.  Replacing the mutex with a spinlock helped quite a bit.\n2.  Once that was fixed, it appears that there is also quite simply a lack of exploitable parallelism during execution of typical operation graphs.  I didn't look exactly at which ops were running when, but it appears that there are choke points in the computation graph that have either no or little parallelism (eg, a tree based reduction will only be able to occupy ~ 1/2 of the available cores on average).  We attempted to work around by running more than one independent graph, eg by running multiple shards of a minibatch independently in parallel, so that the high occupancy operations like convolutions from one could fill in the holes in the low-occupancy operations in the other.  This gives up some stochasticity for better occupancy, which is not always a tradeoff worth making.\n\nIn any event, the workloads we have for Tensorflow do cause many threads to submit jobs, which is partially what drove the design for the PR that I have open; I'd be open to a simpler global queue but I'm pretty sure it wouldn't solve our problem.\n", "Ok, then I guess we need to start with a representative set of benchmarks that we want to optimize. The only one that I was pointed to (mnist) submits work from a single thread.\n\nDifferent requirements lead to different designs.\n", "Agreed.  I can submit a benchmark that is more representative of our use-case.  It could be simulated by running label_image in 8-16 threads in parallel within a single process.\n\nIt would help if someone working on Tensorflow from Google could provide a position on what kinds of external use-cases are interesting vs the internal ones to help us agree on a starting point.\n", "@benoitsteiner Can you please answer @jeremybarnes question? Good representative benchmarks are a good thing regardless of outcome of everything else here. I've struggled to find any ready benchmarks.\n", "I think maybe OpenMP will help a lot.\n", "Actually, ANY kind of network we ran (MNIST, AlexNet, few hybrid CNN+LSTM setups) on 20 core (40 threads) machine, yielded 20-30% load, regardless of the task. \n", "With [ab02c5](https://github.com/tensorflow/tensorflow/commit/ab02c5ab2f1f10bc9e51f02f5125abed449cae87), I do see a significant speedup on CPUs.\n", "@songgc https://github.com/tensorflow/tensorflow/commit/ab02c5ab2f1f10bc9e51f02f5125abed449cae87 combined with a few other changes to make sure all the TensorFlow operations are multithreaded resulted in a 1.5x to 3x improvement on most benchmarks. Is that what you're seeing?\n", "@benoitsteiner Yes. I trained relatively small networks on intel CPUs and did inference on an ARM CPU. On both systems (learning and inference), I did see 1.5x to 1.8x gains. Thanks a lot!\n", "Hi, my problem is similar to @alantus  post in the beginning:\nI installed Keras with TF as backend on a machine with multiple cpus (each cpu can run only 1 thread). At runtime Keras/TF only uses 1 cpu at all.\n\nHow can I check how many devices TF was able to recognize?\n", "@tridemax @jeremybarnes Can you please test your benchmarks after commit ab02c5?\nScaling on CPU should be significantly improved.\n", "As of change `ab02c5`, we consider the CPU scaling issue generally resolved, so I'm going to close this bug. Please file new issues with benchmarks and steps to reproduce if you still see any performance issues.\n", "@vincentvanhoucke I still have a problem with multicore CPU utilization for MKL build\r\nhttps://github.com/tensorflow/tensorflow/issues/15320", "I have 48 cpu cores and 4 gpu cores. I am using tfrecord reader. Even with a trivial network, my cpu utilization is capped at 1800%. I tried sharding my input data and using parallel_interleave but there is no usage improvement."]}, {"number": 582, "title": "error parsing inception v3 file: 64MB python protobuf parsing limit", "body": "I just upgraded TF to 469b3ddb4bb914a02f496748dd4ee5dbcbd7fce0 and I'm now getting an error importing the inception v3 model provided at https://storage.googleapis.com/download.tensorflow.org/models/inception_dec_2015.zip\n\nI didn't have this problem before the upgrade. Here is the error\n\n```\nTraceback (most recent call last):\n  File \"test.py\", line 7, in <module>\n    graph_def.ParseFromString(fileContent)\ngoogle.protobuf.message.DecodeError: Error parsing message\n```\n\nAnd this is the code I'm using\n\n``` python\nimport tensorflow as tf\n\nwith open(\"tensorflow_inception_graph.pb\", mode='rb') as f:\n  fileContent = f.read()\ngraph_def = tf.GraphDef()\ngraph_def.ParseFromString(fileContent)\n\ninput_layer = tf.placeholder(\"float\", [1, 299, 299, 3])\ntf.import_graph_def(graph_def, input_map={ \"Mul\": input_layer })\n```\n", "comments": ["Could you please try writing fileContent to a file to make sure that you have successfully read the file content? Thanks.\n\nSherry\n", "Same code was working before the update. Were you not able to repeat the bug with the above?\n", "I am unable to reproduce the problem.\n\nSherry\n\nOn Tue, Dec 22, 2015 at 6:11 PM, ry notifications@github.com wrote:\n\n> Same code was working before the update. Were you not able to repeat the\n> bug with the above?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/582#issuecomment-166784764\n> .\n", "Hi, I encountered the same problem with current master, and the whole log are:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\n[libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\nTraceback (most recent call last):\n  File \"classify_image.py\", line 214, in <module>\n    tf.app.run()\n  File \"/home/linchaoz/.local/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"classify_image.py\", line 210, in main\n    run_inference_on_image(image)\n  File \"classify_image.py\", line 160, in run_inference_on_image\n    create_graph()\n  File \"classify_image.py\", line 142, in create_graph\n    graph_def.ParseFromString(f.read())\ngoogle.protobuf.message.DecodeError: Error parsing message\n```\n", "However, when I compile `main.cc`, it can load the model. It might be something wrong with the python interface or my python environment. I will update soon.\n", "@ry , I got a workaround by changing https://github.com/google/protobuf/blob/master/src/google/protobuf/io/coded_stream.h#L611 to `256 << 20` and then recompile.\n", "I run into a similar problem when trying to start the tensorboard on a log generated by a (fairly large) custom network:\n\n```\npanmari@pc:~$ tensorboard --logdir PycharmProjects/tensor_stuff/log_render_buffers/\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/TAG' on path /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/TAG\nWARNING:tensorflow:Unable to read TensorBoard tag\nStarting TensorBoard  on port 6006\n(You can navigate to http://localhost:6006)\n[libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\nException in thread Thread-2:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/threading.py\", line 810, in __bootstrap_inner\n    self.run()\n  File \"/usr/lib/python2.7/threading.py\", line 1082, in run\n    self.function(*self.args, **self.kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/event_accumulator.py\", line 242, in Update\n    self.Reload()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/event_accumulator.py\", line 175, in Reload\n    for event in self._generator.Load():\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/impl/directory_watcher.py\", line 82, in Load\n    for event in self._loader.Load():\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/impl/event_file_loader.py\", line 53, in Load\n    event.ParseFromString(self._reader.record())\nDecodeError: Error parsing message\n```\n\nThe workaround proposed by @ffmpbgrnn fixed it.\n", "@ffmpbgrnn it does work. thanks.\n", "@sherrym, haven't dug into this but still experiencing this bug on 97522f0acd53652baa57e42d06557ebb94bf0c4d and also @ffmpbgrnn's suggestion to change `kDefaultTotalBytesLimit` did allow me to not crash\n", "@panmari Same problem here with the current master. As soon as the complexity of my graph goes beyond a certain threshold, tensorboard does not pick up any data. It shows the run under runs but there is no graph or summaries. Tensorboard does not show any signs of a complain in the logs. However, when I restart it, it gives a similar error message:\n\n```\n[libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\nException in thread Thread-2:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/threading.py\", line 810, in __bootstrap_inner\n    self.run()\n  File \"/usr/lib/python2.7/threading.py\", line 1082, in run\n    self.function(*self.args, **self.kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/event_accumulator.py\", line 242, in Update\n    self.Reload()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/event_accumulator.py\", line 175, in Reload\n    for event in self._generator.Load():\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/impl/directory_watcher.py\", line 82, in Load\n    for event in self._loader.Load():\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/impl/event_file_loader.py\", line 53, in Load\n    event.ParseFromString(self._reader.record())\nDecodeError: Error parsing message\n```\n\n@ffmpbgrnn update: I tried the fix but it did not help. Now tensorboard does not complain about the missing graph definition and says \"Reading graph.pbtxt\" but once the loading is finished, the page stays empty. \n\nPS: I use chrome\n\nIs there any way to visual the graph using an external tool?\n", "The fix suggested by @ffmpbgrnn resolved the issue for me. It was necessary to increase the size to run `classify_image.py` in `models/image/imagenet`. Thanks for helping out :)\n", "Set environment `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python` would solve this. slower though.\n", "I started receiving the error `A protocol message was rejected because it was too big (more than 67108864 bytes).` after upgrading tensorflow to 0.7.0\n", "I'm experiencing the same issue with the latest 0.7.0 devel docker image when trying to run the image net example\n\n```\ndocker run -it b.gcr.io/tensorflow/tensorflow:latest-devel\n```\n\n```\n/tensorflow/tensorflow/models/image/imagenet# python classify_image.py\n[libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\nTraceback (most recent call last):\n  File \"classify_image.py\", line 213, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"classify_image.py\", line 209, in main\n    run_inference_on_image(image)\n  File \"classify_image.py\", line 159, in run_inference_on_image\n    create_graph()\n  File \"classify_image.py\", line 141, in create_graph\n    graph_def.ParseFromString(f.read())\ngoogle.protobuf.message.DecodeError: Error parsing message\n```\n", "@raingo 's  solution is also effective !  \n", "My solution for now was to downgrade to 0.6.0 and wait for the patch :)\n", "@raingo suggestion was the easiest fix\n", "FYI, we recently added custom protobuf pip packages that should overcome the 64MiB limit and also provide fast processing in python: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#protobuf-library-related-issues\n", "@vrv \nI got  Segmentation fault (core dumped) after installing protobuf 3.0.0b2.post2. But it works fine with the default protobuf 3.0.0b2. \n\n```\n$ python -c \"import tensorflow\"\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nSegmentation fault (core dumped)\n```\n", "@ffmpbgrnn \nI'm getting google.protobuf.message.DecodeError: Truncated message. Is this the same problem as \"Error parsing message\"?\nI tried revise the coded_stream.h to 256<<20 and reinstall protobuf. But it still shows me the same error.\n", "I've tried solutions listed above: remove tensorflow, protobuf and reinstall with source, reinstall protobuf with 'pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0b2.post2-cp27-none-linux_x86_64.whl'. There is no problem on importing tensorflow. However, when following serving tutorial on https://tensorflow.github.io/serving/serving_basic with my own network, same error occured: [libprotobuf ERROR external/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n", "@ffmpbgrnn I changed '256<<20' in coded_stream.h,and recompile,but it didn't work. My tensorflow is installed by pip,and then I uninstalled protobuf,download a new one,changed 256<<20,install new protobuf,but,the same error occurred:\" A protocol message was rejected because it was too big (more than 67108864 bytes).\" Could someone help me? I am crazy...:(\n@VDalibard\n", "@kevineger Could you show me what you did after you changed 256 << 20 in coded_stream.h? I have done the same thing which did seem not work. Did you reinstall protobuf and tensorflow after you did the 256 changes by bagel?\n", "@vrv @raingo  I'm on the latest tensorflow_serving master. We are still having issue with TS serving throwing this error when loading large models. we have tried the various approaches mentioned above and they dont seem to work.\n- PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python still getting the same error\n- the custom wheel package for protobuf specified here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#protobuf-library-related-issues This doesnt work because bazel installs the submodules (tensorflow and protofbuf) when building tensorflow_serving from source\n- `use_fast_cpp_protos=true` and `allow_oversize_protos=true` when running bazel as mentioned here https://github.com/tensorflow/serving/issues/24\n\nIs there any other alternative approaches that doesn't involved manually changing the coded_stream.h file in the bazel cache?\n", "@waichee: if building from source and only running in C++, then the protobuf parsing limit should be 2GiB,not 64.  Unfortunately, protobuf doesn't support protos larger than 32-bit (and from my understanding, will not in the near future), so if you have a variable / constant larger than 2GiB, you probably have to manually shard it right now :(\n", "@vrv that's bizarre...we are building from source c++ on the following commit hash 59225e413e9680d67ae90196fa536f5271fcf615 from master\n\nthis is what we're doing now and we are unable to get it to take in larger than 64Mb model if we remove the sed command on a centos7 base image\n\n```\nRUN cd serving && git checkout 59225e413e9680d67ae90196fa536f5271fcf615\n\nRUN cd /app/serving/tensorflow && echo \"/usr/lib64/python2.7/site-packages\" | ./configure\nRUN cd /app/serving && bazel fetch //tensorflow_serving/model_servers:tensorflow_model_server\n\n# Patch Protobuf to allow larger models to be loaded\nRUN sed -i 's/64 << 20/512 << 20/g' `find /root/.cache/bazel/_bazel_root/*/external/protobuf/src/google/protobuf/io/coded_stream.h`\n\n# Build Tensor Serving with Bazel\nRUN cd /app/serving && bazel build -c opt //tensorflow_serving/model_servers:tensorflow_model_server\n```\n\nare we missing something obvious?\n", "Same problem @waichee pointed out here.  I was using Java bindings on the head (as of yesterday).  I patched protobuf as above in the .cache directory and it worked fine after rebuilding.", "Hey, I get this same error using Java bindings. Trying the .cache workaround, but would be nice to have an easier, supported way to fix this issue.", "@dpressel @hollinwilkins : FYI #7338 should take care of this since protobuf 3.2.0 raised the limit from 64MB to 2GB", "I am still having this problem with Java bindings on latest of r1.0 branch.  A quick look in the tensorflow cache is still showing the 64MB limit.", "@dpressel : Looking at the contents and history of the r1.0 branch (and the `tensorflow/workspace.bzl` file), that change was not included in r1.0, but will be at head and in the 1.1 release.\r\n\r\nHope that helps.", "Haha ok, was guessing that was it.  Thanks", "graph_def.ParseFromString(fileContent)\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n\r\nThis error shows up only when tensorflow is used with GPU.\r\nThe ParseFromString works fine with CPU with no errors.\r\n", "@zxzhijia I have the same question with you, How did you solve it?  Thank you!", "@buaa-luzhi , we had to remove TensorFlow and reinstalled it from the source. ", "Thank you for your reply!\nYou use the tensorflow==0.12.1 version\nand my  tensorflow==0.11,\nWill there be any impact?\nThank you very much!\n\n\u57282017-05-19 16:30:38\uff0c\u9e7f\u667aby1603117@buaa.edu.cn\u5199\u9053\uff1a\n\n@buaa-luzhi , we had to remove TensorFlow and reinstalled it from the source.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.", "Still encountered a similar issue on 1.4 while using [import_pb_to_tensorboard.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/import_pb_to_tensorboard.py) to analyse a graph. Was able to use it by replacing the graph load from file with a [saved_model load](https://www.tensorflow.org/programmers_guide/saved_model#loading_a_savedmodel_in_python).", "@raingo 's solution is really cool!", "sorry , \"https://github.com/google/protobuf/blob/master/src/google/protobuf/io/coded_stream.h#L611 to 256 << 20 and then recompile.\" - line 611 is one function , what , how must be change this line ?", "just use 'read().strip()'", "I'm still having a similar issue, I believe. I tried @raingo 's solution but get this error now: google.protobuf.message.DecodeError: Truncated message.\r\n", "> Set environment `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python` would solve this. slower though.\r\n\r\nIs this referring to environment variables or something else? I put that in my environment variables and it didnt help. ", "> I'm still having a similar issue, I believe. I tried @raingo 's solution but get this error now: google.protobuf.message.DecodeError: Truncated message.\r\n\r\nSame here, first got 'DecodeError: Error parsing message', after implementing the env variable, now getting 'DecodeError: Truncated message'", "Actually, i figured out why mine wasn't working. It was because my pb file\nwas corrupt. So i did the training from the beginning and the new pb file\nworked. Could be same issue with yours?\n\nOn Thu, Sep 27, 2018 at 10:38 AM soul-departed <notifications@github.com>\nwrote:\n\n> I'm still having a similar issue, I believe. I tried @raingo\n> <https://github.com/raingo> 's solution but get this error now:\n> google.protobuf.message.DecodeError: Truncated message.\n>\n> Same here, first got 'DecodeError: Error parsing message', after\n> implementing the env variable, now getting 'DecodeError: Truncated message'\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/582#issuecomment-425114536>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AmeXcK9KLxI8xtG4Ym9qcAg52Lc19n3Jks5ufOL7gaJpZM4G5cDr>\n> .\n>\n", "Actually, it was a bit silly. I re-downloaded the file and got the md5 different with problem gone. Not sure, whether it's updated on server side or had any download issue.", "I resolved this by deleting the model I had downloaded and re-downloading."]}, {"number": 581, "title": "ConvNet tutorial", "body": "https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/deep_cnn/index.md\n\nQuote:\n\n> EXERCISE: The model architecture in inference() differs slightly from the CIFAR-10 model specified in cuda-convnet. In particular, the top layers of Alex\u2019s original model are locally connected and not fully connected. Try editing the architecture to exactly reproduce the locally connected architecture in the top layer.\n\nWhen I look at the Figure2 of AlexNet paper, it seems that the only locally connected layers are conv1 and conv3 (which I would classify as \"bottom\" layers), both fully connected layers (which are \"top\" layers in my understanding) are connected across the two GPUs. Or am I not understanding the exercise correctly?\n", "comments": ["I closed it at first because I realized that it refers to the CIFAR-10 paper by Alex, not to AlexNet, but now that I reread the tutorial, it appears to me that it actually implements the AlexNet, not the CIFAR-10 (at least not this one: http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf), because the CIFAR-10 paper by Alex is only two-layers deep and has RBMs, which I don't see in the tutorial, so I guess my question from the original post here is still valid.\n", "The architecture used by Alex for CIFAR-10 in cuda-convnet was never published. The most up-to-date code for specifying this architecture is probably Alex's branch in cuda-convnet2 on github.\n\nhttps://github.com/akrizhevsky/cuda-convnet2/blob/master/layers/layer-params-cifar10-11pct.cfg\n\nNote that what differs in his architecture is the usage of local3 and local4 in lieu of fully connected layers.\n"]}, {"number": 580, "title": "Failed compilation with avx or avx2 on Ubuntu 14.04", "body": "I am unable to compile when passing avx or avx2 to bazel, even though I have an avx2-compatible CPU (Xeon E5-2643 v3). This is the command I'm using to compile:\n\n```\nbazel build -c opt --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package\n```\n\nAlso I can compile just fine using the default CPU or GPU configurations, i.e. both of these commands work:\n\n```\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n```\n\nHere is the error message when turning on `verbose_failures`:\n\n ERROR: /usr/local/tensorflow/tensorflow/core/BUILD:157:1: C++ compilation of rule '//tensorflow/core:framework' failed: gcc failed: error executing command \n  (cd /root/.cache/bazel/_bazel_root/e86f6fce5559de9e3e13fb6adb66b858/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -mavx2 '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-ce5a455b34c0 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-ce5a455b34c0 -pthread -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/framework/tensorflow/core/framework/kernel_def_builder.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/framework/tensorflow/core/framework/kernel_def_builder.pic.d -fPIC -c tensorflow/core/framework/kernel_def_builder.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/framework/tensorflow/core/framework/kernel_def_builder.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: gcc failed: error executing command \n  (cd /root/.cache/bazel/_bazel_root/e86f6fce5559de9e3e13fb6adb66b858/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -mavx2 '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-ce5a455b34c0 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-ce5a455b34c0 -pthread -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/framework/tensorflow/core/framework/kernel_def_builder.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/framework/tensorflow/core/framework/kernel_def_builder.pic.d -fPIC -c tensorflow/core/framework/kernel_def_builder.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/framework/tensorflow/core/framework/kernel_def_builder.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n", "comments": ["I have upstreamed a partial fix to Eigen (see https://bitbucket.org/eigen/eigen/commits/f6e3953e6b44a8a0bfde7186cc162a612a904e15 for details).\nI'll work on the second half of the problem after the Christmas break.\n", "Any updates on this?\n", "This will be fixed with pull request https://github.com/tensorflow/tensorflow/pull/847\n", "The fix was merged. Closing this issue.\n", "@benoitsteiner I'm still having problems at a27d844e05447e65aa279ae5269a2d75590f46f6:\n\n`bazel build -c opt --copt=-mavx2 //tensorflow:tensorflow_py --verbose_failures`\n\n```\nINFO: From Compiling tensorflow/core/framework/op.cc:\nIn file included from ./tensorflow/core/framework/types.h:27:0,\n                 from ./tensorflow/core/framework/type_traits.h:22,\n                 from ./tensorflow/core/framework/allocator.h:25,\n                 from ./tensorflow/core/framework/op_kernel.h:22,\n                 from tensorflow/core/framework/op.cc:21:\n./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:36:52: fatal error: src/Tensor/TensorContractionThreadPool.h: No such file or directory\n #include \"src/Tensor/TensorContractionThreadPool.h\"\n                                                    ^\ncompilation terminated.\nERROR: /home/kearnes/git/tensorflow/tensorflow/core/BUILD:718:1: C++ compilation of rule '//tensorflow/core:framework_internal' failed: gcc failed: error executing command \n  (cd /home/kearnes/.cache/bazel/_bazel_kearnes/f662703ad8d0299d3b1319f4ce155cb9/tensorflow && \\\n  exec env - \\\n    PATH=/home/kearnes/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/kearnes/bin:/usr/local/cuda/bin \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -mavx2 '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-c8e5d094f3a9 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-c8e5d094f3a9 -pthread -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/framework_internal/tensorflow/core/framework/op.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/framework_internal/tensorflow/core/framework/op.pic.d -fPIC -c tensorflow/core/framework/op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/framework_internal/tensorflow/core/framework/op.pic.o): gcc failed: error executing command \n  (cd /home/kearnes/.cache/bazel/_bazel_kearnes/f662703ad8d0299d3b1319f4ce155cb9/tensorflow && \\\n  exec env - \\\n    PATH=/home/kearnes/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/kearnes/bin:/usr/local/cuda/bin \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -mavx2 '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-c8e5d094f3a9 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-c8e5d094f3a9 -pthread -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/framework_internal/tensorflow/core/framework/op.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/framework_internal/tensorflow/core/framework/op.pic.d -fPIC -c tensorflow/core/framework/op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/framework_internal/tensorflow/core/framework/op.pic.o).\nTarget //tensorflow:tensorflow_py failed to build\n```\n", "Yes, I'm also getting the same error at b159e211. \n", "Reopening since we had to rollback the change.\n", "I have another set of fixes pending in pull request https://github.com/tensorflow/tensorflow/pull/991\n", "Pull request https://github.com/tensorflow/tensorflow/pull/991 is now merged in the codebase. I'm closing this issue.\n", "@benoitsteiner @wicke @vrv \n\nUpdate: The build agents have Xeon CPUs, which don't support AVX2. Perhaps that's the cause of the failures below. If this is confirmed, there is no action required on this issue.\n\nNot sure if a new issue should be created since it has been three months since this issue was closed. But today I noticed a large number of test errors while doing \n`bazel test -c opt --copt=-mavx2 //tensorflow/...`\non the master branch, as a part of the effort to add mavx/mavx2 build to Jenkins.\n\n> Executed 414 out of 416 tests: 356 tests pass and 60 fail locally.\n\nThe logs from the individual failed test failures are not especially informative. For example: \n\n> ==================== Test output for //tensorflow/python:matmul_op_test:\n> external/bazel_tools/tools/test/test-setup.sh: line 52:  7125 Illegal instruction     (core dumped) \"$@\"\n\nTesting with --copt=-mavx or no AVX flag didn't see any failures.\n", "Do Jenkins machines have avx2 support?\n", "@vrv see updates above. They have Xeon CPUs, which aren't supposed to support either AVX or AVX2?But mavx build passes. \n", "Which Xeon?  AVX is pretty old, avx2 is relatively new -- I'd expect avx support on many machines but not avx2.\n", "Just looked it up: The slaves we have in the particular GCE region have Ivy Bridge Xeon, which supports AVX but not AVX2. So this explains it.\n"]}, {"number": 579, "title": "Requesting issue reopening", "body": "#469\n\nhttps://github.com/tensorflow/tensorflow/issues/469#issuecomment-166292118\n", "comments": []}, {"number": 578, "title": "Fixed random_contrast link in the Deep CNN tutorial", "body": "The `random_contrast` link was pointing to an older location that seems to no longer exist. Fixed it to point to the right location.\n\nI believe the tech report should also really point to http://www.cs.toronto.edu/~kriz/conv-cifar10-aug2010.pdf, which is actually a short 9 page report about using convolutional neural networks on CIFAR-10. The previously linked document is Krizhevsky's master thesis at 60 pages and is mostly about RBMs instead of convolutional neural networks and doesn't really fit the tutorial.\n", "comments": ["Can one of the admins verify this patch?\n", "I've removed the other commit that changed the tech report link.\n", "By the way, are you guys open to slight changes to the tutorial? The CIFAR tutorial talks a bit about the TensorBoard and I think it'd be nice if the command to run it was mentioned explicitly in the tutorial. Otherwise you have to look the documentation for the TensorBoard and open `cifar10_train.py` to see where the log files are being saved.\n", "Simple changes / fixes like that seem fine to me -- I'll route it to the appropriate reviewer.\n\nNote that a lot of the team is out this week and next.\n"]}, {"number": 577, "title": "Changing the number of (CPU) threads for cifar10", "body": "Whenever I run the cifar10_eval.py, in creates 32 threads as following:\n\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 32\n\nI think this number of threads is the number of threads running on CPUs, but when I check the usage, only 400-500% of CPUs are used. I wonder if there is anyway to change this number of threads?\n\nThanks\n", "comments": ["Did you figure out how to solve this issue?\n", "yes, the answer is here\nhttp://stackoverflow.com/questions/34389945/changing-the-number-of-threads-in-tensorflow-on-cifar10\n", "Thank you!\n"]}]