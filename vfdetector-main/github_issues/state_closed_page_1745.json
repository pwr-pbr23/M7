[{"number": 546, "title": "Registering all real number types for resize operations.", "body": "Simplifies code and only adds int16 and int64 as additional compatible\ntypes, which might come in handy at some point.\n", "comments": ["Thanks @panmari!\n\nThere are a few changes that will be necessary to get this to work.\n\n1) https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops.cc#L25  All of the ops that you added are currently only defined for those set of types.  So this file needs to be updated to additionally include the ones you are adding kernel registrations for.  My suggestion would be to change the attribute of T to be \"realnumbertype\".\n\n2) After doing that, we should also write some tests to make sure these additional kernels are actually invoked properly.  Unfortunately, our current ops/image_ops_test.py does not really test many of the kernels.  Would you be willing to add the tests for these kernels?\n\nThe way to do this would be to change https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_test.py#L576 to iterate various numpy dtypes for each of the kernels that we support.  We convert numpy dtypes to the appropriate tensorflow datatype, and this should be sufficient to test each of the kernels.\n\nIf this doesn't make sense, or you'd rather us take a look at it, let us know, since this is a change we really should make.\n", "Can one of the admins verify this patch?\n", "Is it possible add at least add a test for one of these new types, to make sure it works?  Again, we can take this over if that's too much work for you.\n", "No problem, I'm right now writing some tests. But I'm running in the same issues as mentioned in #472. Most python tests just fail with the following message:\n\n```\n+ /home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/python/gen_docs_combined --out_dir /home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/_tmp/gen_docs_test_2\nRuntimeError: module compiled against API version a but this version of numpy is 9\nTraceback (most recent call last):\n  File \"/home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/python/framework/gen_docs_combined.py\", line 23, in <module>\n    import tensorflow.python.platform\n  File \"/home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/python/__init__.py\", line 50, in <module>\n    from tensorflow.python.framework.framework_lib import *\n  File \"/home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/python/framework/framework_lib.py\", line 62, in <module>\n    from tensorflow.python.framework.ops import Graph\n  File \"/home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/python/framework/ops.py\", line 40, in <module>\n    from tensorflow.python.framework import versions\n  File \"/home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/python/framework/versions.py\", line 24, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: numpy.core.multiarray failed to import\n```\n", "There's probably some other installation of numpy that is interfering -- try searching for other installations, and if they are not needed, try removing them.  IIRC, this is saying that tensorflow was compiled with 1.10.1 (likely) but the runtime is loading 1.9.  Or maybe change the paths so that 1.10 is preferentially chosen over 1.9?  I'm not an expert here :(\n", "Luckily, the tests relevant for this PR worked for me. I extended some of them to test through all supported types. This required to change some test cases since 128 can not be represented as int8. Is there a way to make a custom error message for `assertAllClose`? It would be nice to know in these test for which resize algorithm/type they failed.\n", "Hmm, not that I know of.  We want to add 'parameterized test' support to the python unit tests to make this easy.  Alternatively, you could make the base class a test harness that takes in one type, and then instantiate one class for each type being tested, so that the name of the class tells you which type failed.\n\nThat being said, it's probably fine for now to leave it as is, since we do this type of test in a bunch of other files.  Thanks for making these changes!\n\nI'll take a closer look tomorrow.\n", "Yeah, parametrized tests would be a perfect fit here. Thanks for all your guidance.\n", "test this please\n", "@jendap, tests didn't kick off :(\n\n(Running manually)\n", "@vrv this was directed at a bot, not me?\n", "Yes, it was directed at the bot.  We need to change our regex to somehow make that more clear. \n"]}, {"number": 545, "title": "Tensorflow features", "body": "If there any difference between Tensorflow installed on Linux machine and Windows machine in terms of features.\nIf yes, can anyone please list down the features available for both the machine versions.\n", "comments": ["Currently, TensorFlow does not support Windows (except in a Linux docker container), and this is dependent on our build system (Bazel) adding Windows support. You can track issue #17 to see what progress has been made on this front.\n"]}, {"number": 544, "title": "Force embedding_lookup onto CPU in basic word2vec example - #514", "body": "Should fix https://github.com/tensorflow/tensorflow/issues/514. Of course the ideal solution would be to implement `ScatterSub` on the GPU, but I think this is a reasonable short-term fix so that help don't run into errors when running the script on a machine with GPU.\n", "comments": ["Ah, you're right. Pinning the embedding matrix to the CPU still resulted in the same error, however. I also had to put the nce weights onto the CPU, but I don't see why this would be the case. Any ideas?\n\nThe updated code now works on a GPU machine, this is the device placement: https://gist.github.com/dennybritz/9df7dd2553b0aa8db808\n", "https://github.com/tensorflow/tensorflow/blob/20723e2b3d58cc48b2a302f7ea9806c8a75fd18f/tensorflow/python/ops/nn.py#L631 \n\nnce_weights / bias are used as input to the nce_loss higher-level function, which also, under the hood, calls embedding_lookup, so the same property is required.\n\nI thought our placement algorithm was supposed to ensure that the variable would not be placed on GPU if all consumers can only be placed on CPU, so hopefully we can dig in later to figure out how to solve this more generally.\n\nWe also have a bug internally to add a self-test to word2vec_basic.py so we could catch this earlier.\n\nI'll take a closer look at this on Monday -- thanks for fixing this.\n", "That makes sense then, thanks for the explanation. I guess another way to fix this is to allow soft placement in the session, but I'm not sure if that's cleaner since it's less explicit about what's happening?\n", "Can one of the admins verify this patch?\n", "I'm generally not a fan of soft placement since it masks bugs, but in some cases it is necessary.  Here your annotations can solve the problem so let's go with this. \n\nCan you squash your commits?  I'll merge soon after that.\n", "Squashed it.\n", "Fixed the spacing. It'd be a good idea to link to the official style guide from the contribution page.\n", "Thanks, we're pretty inconsistent about hanging indents, so it was really about consistency within that file only.\n"]}, {"number": 543, "title": "Zero volatile GPU-Util but high GPU Memory Usage", "body": "Hi I am running a model implemented by tensorflow with only one GPU, the GPU usage is 95% while the volatile GPU-Util is 0.\n\nSpecifically I have Tesla k40m with cuda 7.0 and cudnn 6.5v2 installed on Centos 7.0. There are three files: data_loader.py, model.py and train.py in my project. In the train.py I firstly declared \n\" with tf.device('/gpu:0'):\" and then sess.run([train_op]). When I run the code, errors raised:\n\n\"tensorflow/core/common_runtime/gpu/gpu_init.cc:45] cannot enable peer access from device ordinal 0 to device ordinal 2\"\n\nOn the other hand, I installed tensorflow with Pip.\n\nAny help are more than welcome.\n", "comments": ["> > cannot enable peer access from device ordinal 0 to device ordinal 2\"\n\n@lglhuada, This is not an error, just log. It just means there is no efficient way to transfer data from gpu:0 to gpu:2. You can exclude either one of them through CUDA_VISIBLE_DEVICES\n\n> > the GPU usage is 95% while the volatile GPU-Util is 0.\n\nThis is also expected. TensorFlow always reserves most the GPU memory when it initializes, even before the first GPU kernels are received. So you will see a high memory usage at the beginning. But the fact GPU is not actually utilized means none of the kernels are running on GPU.\n\nCould you try to run the tutorials and see if you can get any GPU utilized? \n\n> > bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n> > bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n\nIf you still have problems after that, please provide more information about your machine set up. \n", "@zheng-xq Thanks for your answers. I have been trying to run the tutorial to check whether GPU are utilized. However, there are a lot errors. I will add comments if errors were still available.\n", "@zheng-xq hi, I have checked that I can use GPU with bazel-example and the GPU-util is 21%. So now I need bazel to build my project, right? thanks\n", "In this case, it is okay to use bazel to build your project, although it\nshouldn't be necessary.\n\nPlease make sure you installed the GPU-enabled TensorFlow binaries. If you\nbuilt from source, please use:\n\nbazel build -c opt --config=cuda\n//tensorflow/tools/pip_package:build_pip_package\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\npip install /tmp/tensorflow_pkg/tensorflow-0.6.0-cp27-none-linux_x86_64.whl\n\nOn Mon, Dec 28, 2015 at 1:21 AM, Guangliang Liu notifications@github.com\nwrote:\n\n> @zheng-xq https://github.com/zheng-xq hi, I have checked that I can use\n> GPU with bazel-example and the GPU-util is 21%. So now I need bazel to\n> build my project, right? thanks\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/543#issuecomment-167519968\n> .\n", "hi @zheng-xq  thanks for your quick feedback. Actually I tried to run my code without bazel-build and GPU-util is 0 with  command \" with tf.device('/gpu:0') \" , and I am sure I have installed GPU-enabled tensorflow binaries, cudatoolkit 7.0 and cudnn 6.5. What might lead to this issue?\n", "Hi I have solved my problem without bazel-build, and I modified codes following the code example of cifar10_multi_gpu_train. Thanks.\n", "@lglhuada I encountered the same problem. How did you make the code run in GPU? What's the key code?\n", "@OswinGuai If your computation is not as much as possible( such as add or plus operation), the GPU util is not obvious, try implement large models. \uff1b\uff09", "@lglhuada  Hi, recently I have the same issue, I  tried  three different types of  neural network : \r\n1.the simple feedforward network ,2.the pixelcnn 3.GAN. \r\nAnd weird things happened, \r\n1 uses very low usage of the gpu, and that project has very large training data.\r\n2 uses almost 100% of gpu .\r\n3 the usage of gpu is preodic , sometimes up to 50% sometimes down to 0%. \r\nAfter exiperiment, I finally found the reason, it's the code is to blame, actually the code was written by someone who has no gpu on his laptop,so it's totally unfriendly with gpu,most operations do run on the gpu,but there still some operations are run on the cpu,and when the code running on the cpu,the gpu has to wait,that causes the gap. there are some solutions for it , change the type of variables and constants to tf.float32. but that would not change a lot , so if the code is not friendly with gpu ,you should use tensorflow-cpu version,that might be faster than gpu-version. So  In conclusion, if your gpu does running on some codes and not running on some codes,it probably the code's problem.", "When I train the faster-rcnn-resnet101 model with object-detection repo, no matter how many GPUs I used, the train.py will consume all of the GPUs. e.g. when I use 1, 2 or 4 GPUs, train.py occupy all the memory of them, but only one GPU 'Volatile GPU-Util ' is 100%, the others are 0%.", "> %.\r\n\r\nso what you did? ... actually i am also facing same issue can u please help", "I had this problem when the `.record` files were invalid.\r\n\r\ncc @Kirancgi ", "thanks buddy will try it out\r\n@turowicz \r\n"]}, {"number": 542, "title": "tf.transpose error when trying to transpose matrix of bools (workaround below), also recommend adding tf.repeat", "body": "I'm trying to implement some conditioning flows when training RNNs.  Basically, once an end-of-sequence event has been detected, I reset the RNN state to zeros, and have been able to build in this logic successfully I think with tf.greater() and tf.select()\n\nI implemented a numpy.repeat(a, repeats) sort of equivalent in tensorflow.  I recommend in the future, this can be also added as a tf.repeat() function repeating a bunch of boolean values across a tensor for control flows.\n\nHere's my implementation of a repeater (it only works for non-generalised case, for my problem, so I need to generalise it for higher dimensional tensors in the future):\n\n```\ndef tfrepeat(a, repeats):\n    num_row = a.get_shape()[0].value\n    num_col = a.get_shape()[1].value\n    assert(num_col == 1)\n    result = [a for i in range(repeats)]\n    result = tf.concat(0, result)\n    result = tf.reshape(result, [repeats, num_row])\n    result = tf.transpose(result)\n    return result\n```\n\nThe issue I have is I needed to transpose the result in the end to have the dimensions line up, and the results are all boolean values, and currently I noticed tf.transpose doesn't transpose a matrix of bool's\n\nThe workaround I have was to apply the functions to real numbers, and afterwards, make the end result into a large bool matrix, although this isn't ideal.\n\nWorkaround:\n\n```\neoc_detection = inp[:,eoc_column]\neoc_detection = tf.reshape(eoc_detection, [num_batches, 1])\neoc_detection_state = tfrepeat(eoc_detection, num_state)\neoc_detection_state = tf.greater(eoc_detection_state, tf.zeros_like(eoc_detection_state,dtype=tf.float32))\nnew_state = tf.select(eoc_detection_state, initial_state, new_state)\n```\n\n`new_state` is the lstm state to be fed in next time. If the end of content state is detected in training, we reset it.  This way, I can allow batches of the same length to be trained.\n", "comments": ["Suggestions:\n1. I think your tfrepeat is just\n   \n   ``` python\n   def tfrepeat(a, repeats):\n     return tf.tile(a, [1, repeats]))\n   ```\n2. you can try to add a line in transpose_op.cc Ln186-194 to support bool for transpose. Though, you may not need it anymore if 1 works out for you.\n", "Thanks, I didn't know about tf.tile.  Will try that, and to get bool working for transpose.\n\nI think makes sense for the transpose method to work for any type of matrix that is not a numerical value.\n", "Yes, transpose should work for any dtype. The preferred solution is to use a suitable macro from register_types.h to make that happen. \n", "Looks like `tf.transpose` already works for any type on CPU, but only works for number types on GPU.  I'll fix it to use `TF_CALL_POD_TYPES` instead of `TF_CALL_NUMBER_TYPES`.\n", "Fix in review.\n"]}, {"number": 541, "title": "Allows IndexedSlices to be fed and fetched.", "body": "Addresses issue #518.\n\nBoth `ops.IndexedSlicesValue` and `ops.IndexedSlicesWithoutDenseShapeValue` are needed because `None` can't fed as a subfeed value (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/session.py#L357)\n", "comments": ["Can one of the admins verify this patch?\n", "Hi Kenton,\n\nThanks for resubmitting this as a pull request, and for adding the tests. I'd like to push a bit harder against `IndexedSlicesWithoutDenseShapeValue`. I've added some comments to `ops.py` that suggest where the problem with returning `None` could be addressed, and I'd prefer to address those internally. (Ideally all `IndexedSlices` would have dense shapes, but for the meantime we could address this with a helpful error message in the case that the fed value doesn't match the object being fed.)\n\nDerek.\n", "Thanks for reviewing this. If I understand your proposed solution correctly, fetching would fail if we use\n\n```\nlambda fetched_vals:\n  ops.IndexedSlicesValue(fetched_vals[0], fetched_values[1], fetched_values[2] if len(fetched_vals) == 3 else None)\n```\n\nsince this call: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/session.py#L321 would result in a TypeError due to `subfetch` being None. Or am I misinterpreting something?\n", "Not quite: I was suggesting that, in the event `IndexedSlices` doesn't have a `dense_shape`, the fetch function would return a list of two \"subfetch\" tensors (the indices and values), instead of returning `None` for the `dense_shape`.\n", "The latest version implements your suggestions along with relevant changes to the tests.\n", "Thanks for indulging me! :)\n\nThis LGTM, so I'll go ahead and merge it.\n"]}, {"number": 540, "title": "Extract glimpse has no shape function", "body": "I'm receiving this error when calling tf.image.extract_glimpse:\n\n**No shape function registered for standard op: ExtractGlimpse**\n\nCould someone confirm that this is a problem with the API itself? If so, how can I go about implementing a solution?\n", "comments": ["I found this was a problem too, and hacked a solution together by registering a shape for it:\n\n```\nops.RegisterShape(\"ExtractGlimpse\")\ndef _ExtractGlimpseShape(op):\n  \"\"\"Shape function for ExtractGlimpse op.\"\"\"\n  input_shape = op.inputs[0].get_shape().with_rank(4)\n  unused_size_shape = op.inputs[1].get_shape().merge_with(\n      tensor_shape.vector(2))\n  offsets_shape = op.inputs[2].get_shape().merge_with(\n      input_shape[:1].concatenate([2]))\n   offsets_shape = offsets_shape\n   size_value = tf.ConstantValue(op.inputs[1])\n  if size_value is not None:\n    height = size_value[0]\n    width = size_value[1]\n  else:\n    height = None\n    width = None\n  return [tf.TensorShape(\n      [input_shape[0], height, width, input_shape[3]])]\n```\n", "Thanks a lot; can I just run this once in my own code, or do I have to paste it into some of the tensorflow files? \n", "I've found an almost identical piece of code in /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/attentions_ops.py\n\nIt must be that the code isn't being called correctly; how did you fix this? Or in which file should the code be pasted? \n", "Thanks for reporting this - there is a bug in the imports for this code. I'm working on a fix, and it should be available soon.\n"]}, {"number": 539, "title": "Bug report, easy 1 line fix.", "body": "filename: tensorflow/python/framework/ops.py\n\ndef set_shapes_for_outputs(op):\n  \"\"\"Uses the registered shape functions to set the shapes for op's outputs.\"\"\"\n  try: \n    shape_func = _shape_registry.lookup(op.type)\n  except LookupError:\n    try: \n      shape_func = _default_shape_function_registry.lookup(op.type)\n    except LookupError:\n      raise RuntimeError(\"No shape function registered for standard op: %s\"\n                         % op.type)\n  shapes = shape_func(op)\n  if len(op.outputs) != len(shapes):\n    raise RuntimeError(\n        \"Shape function for op %s returned %g shapes but expecting %g\" %\n        (op, len(op.outputs), len(shapes)))\n  for output, s in zip(op.outputs, shapes):\n    output.set_shape(s)\n\nThe call of \"Shape function for op %s returned %g shapes but expecting %g\" %\" is reversed, should be (op, len(op.shapes), len(outputs)); (i swapped the shapes and outputs) ... took me awhile to debug an unrelated bug because i was getting the wrong debug message ;)\n", "comments": ["Submit a pull request?\n"]}, {"number": 538, "title": "Update contribution section in README.md", "body": "We now accept pull requests.\n", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "LGTM, Approval\n", "Should we delete the `readme-pr-change` branch?\n", "oops, done.\n", "Thanks!\n"]}, {"number": 537, "title": "Is there a way to turn off variable reuse a number of scopes down?", "body": "I'm having trouble with an under-sharing error for scope reuse. A specific example that I can point to using TF repo code is if we were to build an architecture with parallel attention modules.\n\nSay we did something in the attention_decoder in rnn/seq2seq.py like this:\n\n```\n        # module can be any of ['a', 'b', 'c'...]\n        with tf.variable_scope(module, reuse=None):\n          k = tf.get_variable('AttnW', [1, 1, attn_size, attention_vec_size])\n          hidden_features.append(tf.nn.conv2d(hidden, k, [1, 1, 1, 1], \"SAME\"))\n          v.append(tf.get_variable('AttnV', [attention_vec_size]))\n```\n\nI then build the model by running:\n\n```\nfor module in modules:\n  _outputs, _losses = seq2seq.model_with_buckets(..., \n                  lambda x, y: seq2seq.embedding_attention_seq2seq(x, y, module, False), \n                  ...)\n  ...\n```\n\nThis works just fine for one module, i.e. the first loop goes off without a hitch. When I get to the second module though, I get the following error:\n\nValueError: Under-sharing: Variable embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/b/AttnW_0 does not exist, disallowed. Did you mean to set reuse=None in VarScope?\n\nI realize that I can build the variables up front and then push them through functions to where they're needed. However, that seems really bad because then there are floating variables built in the beginning that are way out of program scope. Is there a better way?\n", "comments": ["Thanks for your report cinjon, I think it's a bug in seq2seq.model_with_buckets.\nIn particular, I think the problem is in these lines.\n      if j > 0:\n        vs.get_variable_scope().reuse_variables()\nIt just set variable sharing for the current scope, which is not the right thing to do: it should create a separate scope (reusing or not, depending on j). For now, this setting leaks outside of this function and compromises your second module.\n\nWe'll work on a fix, I'll also test it with a bunch of other models. In the meantime, can you try replacing these two lines in model_with_buckets by something like this:\n  with tf.variable_scope(\"model_with_buckets\", reuse=True if j > 0 else None):\n ... and shift the body to be in this scope ...\n\nI think that should help and be better than doing your own variables.\n\nThanks for catching this problem!\n", "Awesome, thanks for looking into this Lukasz. I'll try to implement your fix in the meantime and will report back.\n", "Hey Lukasz, I just had some time to try this out and I'm still getting the Under-Sharing error. This makes sense because the scope change you're suggesting takes place in a superset scope of the attention module. So when we set the reuse=True and the module introduces a new variable\u00a0name (given in the `with tf.variable_scope(module, reuse=None)`), it throws the error.\n\nIf I instead turn off the reuse=True in model_with_buckets, then we get an Over-Sharing error in the embedding wrapper. This also makes sense because of the `vs.get_variable_scope().reuse_variables()` in ops/rnn.py.\n\nIt seems like there isn't a way to break the reuse contract in a sub scope. Is that right? If so, is there a way to satisfy this without passing the variables through to the right function / making a separate attention module?\n", "I tried a few things to solve this including changing the `reuse_variables()` in rnn.rnn to similarly be a with block. They didn't work, so I moved towards making the attention module modular. \n\nThat's complicated by the fact that the attention_states aren't calculated until embedding_attention_seq2seq. I moved it to its own function like this:\n\n```\ndef attention_module(attention_states, module=None, num_heads=1):\n    ...\n    # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.                                                                                                                            \n    ...                                                                                                                              \n    for a in xrange(num_heads):\n      k_scope = 'AttnW_%d' % a\n      v_scope = 'AttnV_%d' % a\n      if module:\n        k_scope += '_%s' % module\n        v_scope += '_%s' % module\n\n      k = vs.get_variable(k_scope, [1, 1, attn_size, attention_vec_size])\n      hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], \"SAME\"))\n      v.append(vs.get_variable(v_scope, [attention_vec_size]))\n\n    def attention(query):\n      \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"\n      attn_scope = 'Attention'\n      if module:\n        attn_scope += '_%s' % module\n\n      for a in xrange(num_heads):\n        with vs.variable_scope(\"%s_%d\" % (attn_scope, a)):\n          ...\n      return ds\n\n    return attention\n```\n\nAnd then instantiated it in embedding_attention_seq2seq:\n\n```\n...\ntop_states = [array_ops.reshape(e, [-1, 1, cell.output_size]) for e in encoder_outputs]\nattention_states = array_ops.concat(1, top_states)\nattention_func = attention_module(attention_states, module, num_heads) \n...\n```\n\nBut there is still this Under-Sharing problem. This time, it's on the AttnW_0_b. And it happens because to instantiate the modules with the overall model, I am doing this:\n\n```\nfor num, module in enumerate(modules):\n  with vs.variable_scope('modules', reuse=True if num > 0 else None):\n    _outputs, _losses = seq2seq.model_with_buckets(..., \n                    lambda x, y: seq2seq.embedding_attention_seq2seq(x, y, module, False), \n                    ...)\n  ...\n```\n\nIf I remove the reuse=True on that, then the error is again over-sharing, this time because `modules/embedding_attention_seq2seq/RNN/cell_output/EmbeddingWrapper/embedding` already exists, which I'm pretty sure is a side-effect of rnn.py.\n\nIs there something else I could do to instantiate the graph like I'm describing? Thanks.\n", "Hi cinjon. I think I corrected the reuse leaking from model_with_buckets in a recent commit, so now you're hitting a different problem, this one.\n\n> If I remove the reuse=True on that, then the error is again over-sharing, this time because\n> modules/embedding_attention_seq2seq/RNN/cell_output/EmbeddingWrapper/embedding already exists.\n\nI think removing reuse=True in your case is the right thing to do (you don't want to reuse across modules, right?). But then - do you want to share the encoder embedding across modules? If you do, then you should create a variable just for the embedding and pass it to EmbeddingWrapper, I think. If you don't, then maybe just put each module in it's own scope (e.g., with variable_scope(\"module\"  + str(num))) -- that will make all variables unique.\n\nDoes that help? I'm not fully sure how exactly you want to share variables -- if every module is separate, then I think the best way is to have separate scope for each of them.\n", "I haven't been able to code up a solution where the modules share the encoder embedding yet, but yes, that's the idea. The only thing that I want to be unique is the attention component - the encoder should be idempotent across the modules. I'll report back on doing the former.\n", "@lukaszkaiser , I got this to work by declaring all of the attention modules on every run so there is no under-sharing. The gradients are then the sum of the gradients of each module. At step time, I only feed the losses for the current module into the output. This is inefficient, but it seems to work. I'm going to dig into the graph that TensorFlow made just to make sure that it's right and then work on making it more efficient.\n\nIt does seem though that it would be a lot easier to build this graph if there was a way to turn off reuse in a sub-scope.\n", "Hi again. I needed to make this more efficient, but my attempts haven't been working. I also realized that the previous solution I had built failed when there is more than one bucket (I had made it uni-bucket for testing). TF would throw an error about the encoder and decoder placeholders needing values. If I used a bucket of (10,12) and had set it up to also have a bucket of (20,25), then the error would be for encoders 10 through 20 and decoders 12 through 25.\n\nThis was confusing because it seemed like I was building the graph similarly. What I have now is that I make the encoder_cell and the embedding in the Seq2Seq init:\n\n```\n    ...\n    # Create the internal multi-layer cell for our RNN.                                                                                                                                                     \n    single_cell = rnn_cell.GRUCell(layer_size)\n    if use_lstm:\n      single_cell = rnn_cell.BasicLSTMCell(layer_size)\n    cell = single_cell\n    if num_layers > 1:\n      cell = rnn_cell.MultiRNNCell([single_cell] * num_layers)\n\n    encoder_cell = rnn_cell.EmbeddingWrapper(cell, total_vocab_size)\n    with vs.variable_scope('embedding_decoder_top_level'):\n        with ops.device(\"/cpu:0\"):\n            embedding = vs.get_variable(\"embedding\",\n                                        [vocab_size, cell.input_size])\n\n    # The seq2seq function: we use embedding for the input and attention.                                                                                                                                   \n    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n      return seq2seq.embedding_attention_seq2seq(\n        encoder_inputs, decoder_inputs, cell, vocab_size_in,\n        vocab_size_out, output_projection=output_projection,\n        feed_previous=do_decode, encoder_cell=encoder_cell, \n        embedding=embedding)\n```\n\nI'm then passing them through into the embedding_attention_seq2seq:\n\n```\n    self.module_graph = {}\n\n    for num, module in enumerate(modules):\n        scope_name = 'module_%s' % language\n        with vs.variable_scope(scope_name):\n          if forward_only:\n            outputs, losses = seq2seq.model_with_buckets(...)\n          else:\n            outputs, losses = seq2seq.model_with_buckets(...)\n\n        self.module_graph[module] = [outputs, losses]\n        params = [param for param in tf.trainable_variables()\n                  if param.name.startswith(scope_name)]\n\n        if not forward_only:\n            gradient_norms = []\n            updates = []\n            opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n            for b in xrange(len(buckets)):\n                gradients = tf.gradients(losses[b], params)\n                clipped_gradients, norm = tf.clip_by_global_norm(\n                    gradients, max_gradient_norm)\n            gradient_norms.append(norm)\n            updates.append(opt.apply_gradients(\n                zip(clipped_gradients, params), global_step=self.global_step))\n\n            self.module_graph[module].extend([gradient_norms, updates])\n```\n\nThis throws a confusing error even before I can start training it: \n\n```\n  File \".../tensorflow/python/ops/nn.py\", line 835, in sampled_softmax_loss\n    name=name)\n  File \".../tensorflow/python/ops/nn.py\", line 654, in _compute_sampled_logits\n    true_logits += true_b\n  File \".../tensorflow/python/ops/math_ops.py\", line 425, in binary_op_wrapper\n    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=\"y\")\n  File \".../tensorflow/python/framework/ops.py\", line 528, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \".../tensorflow/python/framework/ops.py\", line 472, in _TensorTensorConversionFunction\n    % (dtype.name, t.dtype.name, str(t)))\n\nValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: 'Tensor(\"module_a/model_with_buckets/module_a/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/Reshape_5:0\", shape=(?, 1), dtype=int32, device=/cpu:0)'\n```\n\nI also printed out the params (see below) to see if there was anything peculiar and I noticed the first value is `'module_a/RNN/cell_output/EmbeddingWrapper/embedding:0'` (cell_output my own name for scope in rnn/rnn.py). This is the encoder_cell that's passed to the embedding_attention_seq2seq but it's under module_a's scope. Afaict, this means that it won't share training with module_b. Is that right?\n\n```\n[u'module_a/RNN/cell_output/EmbeddingWrapper/embedding:0', \nu'module_a/RNN/cell_output/GRUCell/Gates/Linear/Matrix:0', \nu'module_a/RNN/cell_output/GRUCell/Gates/Linear/Bias:0', \nu'module_a/RNN/cell_output/GRUCell/Candidate/Linear/Matrix:0', \nu'module_a/RNN/cell_output/GRUCell/Candidate/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnW_0:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnV_0:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Gates/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Gates/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Candidate/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Candidate/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/Linear/Bias:0']\n```\n\nI realize this might be out of scope of a Github issue now. Let me know if you'd rather take this offline. Thanks @lukaszkaiser \n", "Some more info on the above error:\n\n```\n  File \".../tensorflow/python/ops/nn.py\", line 835, in sampled_softmax_loss\n    name=name)\n  File \".../tensorflow/python/ops/nn.py\", line 654, in _compute_sampled_logits\n    true_logits += true_b\n  File \".../tensorflow/python/ops/math_ops.py\", line 425, in binary_op_wrapper\n    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=\"y\")\n  File \".../tensorflow/python/framework/ops.py\", line 528, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \".../tensorflow/python/framework/ops.py\", line 472, in _TensorTensorConversionFunction\n    % (dtype.name, t.dtype.name, str(t)))\n\nValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: 'Tensor(\"module_a/model_with_buckets/module_a/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/Reshape_5:0\", shape=(?, 1), dtype=int32, device=/cpu:0)'\n```\n\nThis happens on the second module. The loop makes the first one without a hitch and then goes on to make the second one where it finds that the softmax Reshapes are now int32s.\n\nPrintout of true_b after `true_b = array_ops.reshape(true_b, [-1, num_true])` (~ L671) in ops/nn.py:\n\n```\n...\nTensor(\"modules/model_with_buckets/modules/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/Reshape_5:0\", shape=(?, 1), dtype=float32, device=/cpu:0)\n...\n```\n\nAnd then on the second module, just before the error is thrown:\n\n```\nTensor(\"modules_1/model_with_buckets/modules/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/Reshape_5:0\", shape=(?, 1), dtype=int32, device=/cpu:0)\n```\n", "It looks like if I change _compute_sampled_logits in ops/nn.py to cast all_b into a float32, then I get past this error and can build the graph, but later run into another error at step time where the params are not 1-dimensional.\n\n```\n...\nall_b = embedding_ops.embedding_lookup(biases, all_ids)\nall_b = math_ops.cast(all_b, dtypes.float32)\n...\n```\n\n```\n[[Node: modules_1/model_with_buckets/modules/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_INT32, \n_device=\"/job:localhost/replica:0/task:0/cpu:0\"](modules_1/model_with_buckets/modules/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/embedding_lookup_1/params_0, \nmodules_1/model_with_buckets/modules/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/concat)]]\n\n  File \"../tensorflow/python/ops/nn.py\", line 840, in sampled_softmax_loss\n    name=name)\n  File \"../tensorflow/python/ops/nn.py\", line 633, in _compute_sampled_logits\n    all_b = embedding_ops.embedding_lookup(biases, all_ids)\n  File \".../tensorflow/python/ops/embedding_ops.py\", line 82, in embedding_lookup\n    return array_ops.gather(params[0], ids, name=name)\n  File \".../tensorflow/python/ops/gen_array_ops.py\", line 301, in gather\n    name=name)\n  File \".../tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \".../tensorflow/python/framework/ops.py\", line 1850, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \".../tensorflow/python/framework/ops.py\", line 1049, in __init__\n    self._traceback = _extract_stack()\n```\n\nEDIT: I'm going to dig more into this tomorrow, but in the meantime, I forgot to mention an important part and that's that the above error comes after we use control_flow_ops.cond to choose a different attention weight/bias (via the def attention_module above) based on which module we're using. This happens in the attention_decoder, where instead of `attns = attention(new_state)`, we have a recursive build up of `attns = control_flow_ops.cond(module == m, lambda: attention_func(new_state), lambda: attns)`. \n", "I couldn't leave it alone and tried one more thing, which was to use a control_flow_ops.cond on the models themselves. What this looks like is that I have a cond set up to branch to a different model_with_buckets output / loss based off of the module. This didn't work because I run into a problem with branching in control_flow_ops's BuildCondBranch (~ L568: `elif v.name not in self._values:`) where because the function's output is a list of lists (the buckets), it chokes on getting the name. I can't just flatten the list though because then the branching is really weird.\n\nAt this point, I'm unsure what else to try to get this to work. I feel like I am missing some key understanding that should make this easy. :(\n", "@lukaszkaiser Thoughts?\n", "I was offline for a while cinjon, and I'm not sure I understand now what you were trying to accomplish. If you still have this problem, let's take it offline and, if needed, open another issue (just to not prolong this one, it becomes unreadable). Thanks!\n", "Hey Lukasz, thanks for the reply. \nI've set this aside for a minute while working on other nets. I appreciate you coming back round to it and I'll let you know when I look into this again.\n", "As per my last comment -- it's not clear from this thread what the issue is any more. If you're having some issue, feel free to open a new one and please explain precisely. Also note that variable reuse is inherited on purpose -- the other design would break compositionality across modules. If you have a variable in the same module or function, why not reuse it by reference?"]}, {"number": 536, "title": "Add a way to see the shape of the Tensors (arrows) in Tensorboard.", "body": "I think it would be useful to add functionality to check the calculated shape (if available) of the Tensors in the graph visualization. Maybe we could click the arrows to open a window that showed info about the tensor shape (and maybe other info too). What are your thoughts about that?\n", "comments": ["That's a good idea, but it will require some thought because the graph visualizer doesn't current get information about the execution (other than the computed summaries). Assigning to @danmane, who's been tracking feature requests for new versions of TensorBoard.\n", "I also had an idea for what the ultimate version of tensroboard can look like, that is putting aside the add_image/histogram/..._summary and let user click on each node of the computation graph and see what is exactly flowing in the tensors, if it images, vectors, ... just show the content, info, shape and values, something like the debugger area of most of the ideas which let you know the variables info in the run time. \n", "Does cc36921 solve this issue?\n", "Yes, exactly. I will update this issue and close it once the documentation is updated (the commit is under review). The doc talks about this new feature and show users how to use it.\n\nHere is the short doc: To include tensor shapes in the `GraphDef` pass `sess.graph.as_graph_def(add_shapes=True)` to the `SummaryWriter` when\nserializing the graph.\n", "Forgot to mention that the shapes shown in the graph visualizer are the statically inferred shapes from the python client. Most of the shapes are known in practice. However, ops whose output tensor's shape depends on some variable at run-time, will have unknown shape in the graph.\n", "Thanks!\n", "[Here is](https://www.tensorflow.org/versions/master/how_tos/graph_viz/index.html#tensor-shape-information) part of the tutorial explaining how to add tensor shapes to the graph visualizer.\n", "I have tensorflow 1.5 and I tried passing this to the summary_writer on the nmt tutorial, \r\ngraph_def=sess.graph.as_graph_def(add_shapes=True)\r\nIt says - passing as_graph_def to summarywriter is deprecated, use sess.graph instead. However, sess.graph does  not have add_shapes=True.\r\n\r\nWhat is a good way to see what the tensors in the arrows represent. It is hard to dig through the code and figure the tensors and their shapes and map it to the corresponding graph\r\n\r\n"]}, {"number": 535, "title": "What need processing power?", "body": "Hi,\n\nI maybe be wrong but from what I read about machine learing the learning process is what take a lot of time and need a lot of processing power, is it true? If yes, can I teach the neural network and then use the results on a small pc like rpi?\n\nthank you\n", "comments": ["Typically we use a much larger amount of computation to train a model with TensorFlow (using one or more GPUs if it is a complex model), because the throughput of examples processed per second is what determines how long it takes to train the network.\n\nFor inference - depending on the model - it is often possible to scale horizontally by using lower power devices, and scaling out to many independent devices (unless the model is too big to fit on a single device). That's what makes it possible to run inference on a mobile device (like in the [Android tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android)), and potentially it could also run inference on a Raspberry Pi.\n", "@mrry How much time and energy does an inference take on Android platform?\n", "This depends a lot on the particular model that you are running. @petewarden would have better statistics than I do, but it's certainly plausible to run the Inception model for image recognition on an Android device.\n", "To give you a rough idea, a small Inception model like the one used in the Android example can run in under a second on a mid-range phone like a Nexus 5, and will take two or three Joules. \n", "@petewarden  Is it compulsory to include the graph in the apk itself / can we program to configure the graph after installing the apk and download the graph later on runtime.\n", "@petewarden I would be interested in knowing more about using trained AIs on embedded systems. I am having hard time finding literature on this topic. Would you be able to direct me? Thanks in advance."]}, {"number": 534, "title": "Android Example SurfaceTextureRenderer", "body": "i've build the app with **sdk** = 23 and **ndk** = 21 and installed the apk on a nexus 4.\n_sometimes it works_ but often the camera preview is visible but without results on the top.\n\nlooking into Android Monitor, i get the following error message in case when the app doesn't work:\n\n```\n12-17 19:22:08.802 30054-30283/org.tensorflow.demo E/BufferQueueProducer: [unnamed-30054-2] dequeueBuffer: BufferQueue has been abandoned\n12-17 19:22:08.802 30054-30283/org.tensorflow.demo E/Legacy-CameraDevice-JNI: LegacyCameraDevice_nativeProduceFrame: Error while producing frame No such device (-19).\n12-17 19:22:08.803 30054-30283/org.tensorflow.demo W/SurfaceTextureRenderer: Surface abandoned, dropping frame. \n                                                                             android.hardware.camera2.legacy.LegacyExceptionUtils$BufferQueueAbandonedException\n                                                                                 at android.hardware.camera2.legacy.LegacyExceptionUtils.throwOnError(LegacyExceptionUtils.java:64)\n                                                                                 at android.hardware.camera2.legacy.LegacyCameraDevice.produceFrame(LegacyCameraDevice.java:583)\n                                                                                 at android.hardware.camera2.legacy.SurfaceTextureRenderer.drawIntoSurfaces(SurfaceTextureRenderer.java:752)\n                                                                                 at android.hardware.camera2.legacy.GLThreadManager$1.handleMessage(GLThreadManager.java:105)\n                                                                                 at android.os.Handler.dispatchMessage(Handler.java:98)\n                                                                                 at android.os.Looper.loop(Looper.java:135)\n                                                                                 at android.os.HandlerThread.run(HandlerThread.java:61)\n```\n", "comments": ["Is this with a recent version of the repo?\n\nPrior to commit https://github.com/tensorflow/tensorflow/commit/cd53f3c3302c9312c1840389a9988a879b8b9dd5 this would be possible due to a GC'd Surface resource.\n", "@andrewharp ok, i've rebuild with the latest pull, seems to be resolved. thx\n"]}, {"number": 533, "title": "Provide GPU implementations for resize image methods", "body": "As far as I can tell, these operations are only provided on CPU so far. It would be very convenient to also have GPU implementations. I'll have a go at it if there's nobody else working on it.\n\nEdit: I mean methods such as resize_nearest_neighbor, not the reshape operator.\n", "comments": ["Tensors on the GPU still store their shapes on the CPU, and `reshape` already works in that case.  The shape will never be stored on the GPU, so I think this should be closed.  Presumably you're seeing an error caused by forcing the shape argument onto the GPU; if so there should possibly be a different bug about placer functionality.\n", "Wouldn't tf.shape return a tensor that could be shipped onto the GPU? In\nthat case, all information could be on the GPU, and the actual resizing\ncode should be able to run on the GPU.\n\nOn Fri, Dec 18, 2015 at 4:41 PM Geoffrey Irving notifications@github.com\nwrote:\n\n> Tensors on the GPU still store their shapes on the CPU, and reshape\n> already works in that case. The shape will never be stored on the GPU, so I\n> think this should be closed. Presumably you're seeing an error caused by\n> forcing the shape argument onto the GPU; if so there should possibly be a\n> different bug about placer functionality.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/533#issuecomment-165927888\n> .\n", "The `Tensor` C++ class unconditionally stores the shape on the CPU regardless of where the data is, and essentially everything assumes this (for error detection, branching, etc.).  I think there's very little chance this situation will change.\n", "Note that as long as all the shape data is on the CPU, a `reshape` does O(0) GPU work even if the data is on the GPU.\n", "I think I'm implicitly assuming we're talking about the resize_image ops.\nSorry, that may be misinterpreting panmari's original request.\n\nOn Fri, Dec 18, 2015 at 4:54 PM Geoffrey Irving notifications@github.com\nwrote:\n\n> Note that as long as all the shape data is on the CPU, a reshape does\n> O(0) GPU work even if the data is on the GPU.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/533#issuecomment-165929024\n> .\n", "Ah, all my comments are wrong if this is about image resizing.\n", "@panmari -- which ops are you thinking about?\n\nOn Fri, Dec 18, 2015 at 5:06 PM Geoffrey Irving notifications@github.com\nwrote:\n\n> Ah, all my comments are wrong if this is about image resizing.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/533#issuecomment-165929904\n> .\n", "Given @panmari's recent pull request to extend the types supported by resize_image ops, I think it's those.\n", "Yes, as the @vrv and @martinwicke assumed, I meant the resize_image ops. I'll change the title to reflect that more clearly. \n", "Sorry to be the necromancer, but any plans on this?", "@TimZaman I think it's still contributions welcome. :)", "@girving I will send you a wheel of cheese from the Netherlands if you get someone to implement this. All bribes aside, this PR matched with this issue: https://github.com/tensorflow/tensorflow/pull/588 but sadly it's the lowest quality algo.", "@TimZaman It'd be better to send the wheel of cheese to whoever implements the feature.  Not sure where best to advertise that, though.", "So what's this then https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/resize_bilinear_op_gpu.cu.cc", "Looks like someone did it, at least for bilinear.  @panmari Should this be closed, or are there missing ones?", "PS resize bilinear and resize bicubic GPU implementations are slow (about 2 seconds for resizing 375 x 667 to 1280 x 1280, same as on CPU), when we switched to nearest neighbor, it made our CNN run 10x faster, see discussion in https://github.com/tensorflow/tensorflow/commit/c02edfa8c97c2cb0133534badc23dda0521e54ff#commitcomment-20615000\r\n\r\n\r\n", "I did the implementation for nearest neighbor in #588 (funny enough, blame doesn't show me anymore). That algorithm is good enough for my purposes and I've been using it since. So from my side, we can close this bug @girving. But there might be others that want fast implementations for other algorithms such as @TimZaman. Maybe I'll come around and do some benchmarking of the current implementation of bilinear and see what can be done. Who's the author of it?", "Cc @gpapan. ", "I did the gpu implementation for bilinear interpolation. The Cuda kernel can get faster (e.g., have each thread compute multiple channels reusing the indexing arithmetic). However, I find the quoted \"(about 2 seconds for resizing 375 x 667 to 1280 x 1280, same as on CPU)\" too large. What is the benchmark used? Are we certain that the gpu path is activated? Which gpu? What is the depth of the tensors? Does this cover forward only or both forward and backward? In any case, I think writing a benchmark suite is the first step that we should accomplish. ", "cc @strin who gave me these numbers", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "When I run resize_image using float32, it takes 18us. If I change it to float16, it changes to 60ms. Does anyone know what's going on?", "Ask questions on StackOverflow, please. _Speculating_: the fp32 might run on GPU, while the fp16 runs might run on CPU as no GPU implementation might be available.", "Indeed, half number types are not supported for bilinear resize on GPU, see https://github.com/tensorflow/tensorflow/blob/cf047f7755f3400ee128db2571042091fe9f8314/tensorflow/core/kernels/resize_bilinear_op_gpu.cu.cc#L215\r\n\r\nYou could try to compile it for all number types using TF_CALL_GPU_NUMBER_TYPES, then compile tensorflow and test it.", "Hi @panmari I would like to run `resize_images/ResizeBicubic` on the GPU. https://github.com/tensorflow/tensorflow/pull/8303 suggests that this is possible but the `tensorflow-gpu` installation from `pip` doesn't have GPU kernels for this op.\r\nHow can I use them?\r\nThank you,\r\nPhilip", "I have a related question: does gradient go through the image_resize op normally if I use this op in intermediate layers for upsampling?  Thank you", "@haeusser I believe not implemented for BICUBIC (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/kernels) GPU.", "@haeusser i tried using ResizeBicubic as part of my custom upsampling layer. Seems that it's 50x slower than Bilinear in backward pass. And I did not see any things related to Bicubic in your ref link. Could you share your solution? Or the current tf version just not implemented the bicubic grad yet?", "> I have a related question: does gradient go through the image_resize op normally if I use this op in intermediate layers for upsampling? Thank you\r\n\r\n@SummitKwan , the Bilinear and NearestNeigbour works good as part of upsampling process on gpu. Bicubic works well but slow since it seems that there are no gpu support kernel for this op currently.", "> So what's this then https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/resize_bilinear_op_gpu.cu.cc\r\n\r\nHello, I'm trying to use fp16 resize_bilinear on my RTX20280ti. Any information on which TF version contains this kernel? I installed tf1.15 nightly as well as tf2.0 using pip, but all i keep getting is \"DLL load failed\" error.\r\n\r\nAny help is appreciated!\r\n\r\nEdit:\r\nI'm using CUDA 9.0\r\ncuDNN 7.1", "It cost me one day to figure out that if you want to use gpu resize_images on tf1.13 you should pass tf.float32 images to tf.image.resize_images. If you use uint8, it would on CPU due to lack of uin8 kernel."]}, {"number": 532, "title": "Disable logging", "body": "Keep getting noise on the terminal:\n\n```\n tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 8\n```\n\nAnyway to turn this off?\n", "comments": ["Seems relevant to #566 , although I didn't get the solution from there.\n", "I think we disabled this log a while ago.\n"]}, {"number": 531, "title": "word2vec large corpus overflow", "body": "I get the following error when I try to run the word2vec_optimized.py file with a corpus larger than 2^31 characters.\n\n```\nexternal/re2/re2/re2.cc:571: RE2: invalid startpos, endpos pair. [startpos: 0, endpos: -656988395, text size: -656988395]\nW tensorflow/models/embedding/word2vec_kernels.cc:38] Invalid argument: The text file corpus_stripped.txt contains too little data: 0 words\nE tensorflow/core/framework/op_segment.cc:52] Create kernel failed: Invalid argument: The text file corpus_stripped.txt contains too little data: 0 words\nE tensorflow/core/common_runtime/executor.cc:262] Executor failed to create kernel. Invalid argument: The text file corpus_stripped.txt contains too little data: 0 words\n     [[Node: Skipgram = Skipgram[batch_size=500, filename=\"corpus_stripped.txt\", min_count=5, subsample=0.001, window_size=5, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nTraceback (most recent call last):\n  File \"word2vec_optimized.py\", line 436, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"word2vec_optimized.py\", line 418, in main\n    model = Word2Vec(opts, session)\n  File \"word2vec_optimized.py\", line 147, in __init__\n    self.build_graph()\n  File \"word2vec_optimized.py\", line 189, in build_graph\n    opts.words_per_epoch) = self._session.run([words, counts, words_per_epoch])\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 444, in _do_run\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: The text file corpus_stripped.txt contains too little data: 0 words\n     [[Node: Skipgram = Skipgram[batch_size=500, filename=\"corpus_stripped.txt\", min_count=5, subsample=0.001, window_size=5, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Skipgram', defined at:\n  File \"word2vec_optimized.py\", line 436, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"word2vec_optimized.py\", line 418, in main\n    model = Word2Vec(opts, session)\n  File \"word2vec_optimized.py\", line 147, in __init__\n    self.build_graph()\n  File \"word2vec_optimized.py\", line 187, in build_graph\n    subsample=opts.subsample)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/models/embedding/gen_word2vec.py\", line 69, in skipgram\n    name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n```\n", "comments": ["kyosha@gmail.com has a change, https://tensorflow-review.googlesource.com/#/c/1252/, which I think will help. You are also welcome to fix the code to fit your need. Cheers,\n", "Thanks, I ended up using the original implementation of word2vec.\n"]}, {"number": 530, "title": "Tensorboard from pip installation broken ", "body": "Tensorboard tries to load a file here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/tensorboard.py#L145\n\nbut `tensorboard/TAG` is missing from the pip installation, so it produces the following message:\n\n```\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/TAG' on path /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/TAG\nWARNING:tensorflow:Unable to read TensorBoard tag\nStarting TensorBoard  on port 6006\n(You can navigate to http://localhost:6006)\n```\n\nand does not render any logged events.\n", "comments": ["+1 I am also facing the same issue \n", "the same problem here. tensorboard cannot list the events, possibly related to the warning...\n", "I believe missing the TAG is just a warning and shouldn't cause any problems.  If you look at the source code of tensorboard.py, you'll see TAG is just for debugging.\n\nNot rendering any logged events is probably an independent problem -- can you verify / validate / reproduce the problem in a way we can help debug?\n", "I built the pip package from source and installed it. Tensorboard can't render the events if I run:\n\n```\ntensorboard --logdir=/tmp/logs\n```\n\nbut it works when I build tensorboard and run:\n\n```\n./bazel-bin/tensorflow/tensorboard/tensorboard --logdir=/tmp/logs\n```\n\nThe only difference here seems to be the pip packaging.\n", "I can't seem to reproduce the problem anymore. Perhaps I was write summaries incorrectly before. Closing this for now...\n", "I had similar problems until I realized that the summary writer did not actually write the file unless I called the summary writer's flush() function.\n\nI'm not sure, but it's probably because I had the summary writer's code snippet in a code part that did not close (and the code finished before the standard 120 seconds between each automatic flush).\n", "same problem here\n", "Same here. Out of a sudden I needed to flush the summary writer after writing to see any summaries...\n", "Do you mind filing another issue re: the summary writer not flushing, with a reproduction? Since it is pretty different from the TensorBoard pip installation issue discussed here.\n", "I got a similar same problem, this is the code to demonstrate:\n\n```\ngraph = tf.Graph()\nwith graph.as_default(), tf.device('/cpu:0'):\n  a = tf.constant(5.0)\n  b = tf.constant(6.0)\n  c = a * b\n\n  # Enter data into summary.\n  c_summary = tf.scalar_summary(\"c\", c)\n  merged = tf.merge_all_summaries()\n\nwith tf.Session(graph=graph) as session:\n  writer = tf.train.SummaryWriter(\"log/test_logs\", session.graph_def)\n\n  result = session.run([merged])\n  tf.initialize_all_variables().run()\n  writer.add_summary(result[0], 0)\n```\n\nI then ran `tensorboard --logdir={absolute path to log/test_logs}` but no event was listed there. Is there anything I should have written differently in the code maybe?\n\nNote that `log/test_logs` does contain files like `events.out.tfevents.1459102927.0a8840dee548`.\n", "Update: Running `https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py` also does not make tensorboard list the events.\n", "@jaycode: Can you `ls -lah` your events file and tell me the size? Also, how did you install TensorBoard, and which version of pip are you using?\n", "@danmane \nResults of ls -lah:\n\n```\ntotal 8.5K\ndrwxrwxrwx 1 root root 4.0K Mar 27 20:23 .\ndrwxrwxrwx 1 root root    0 Mar 27 20:36 ..\n-rwxrwxrwx 1 root root  592 Mar 28 06:09 events.out.tfevents.1459102927.0a8840dee548\n-rwxrwxrwx 1 root root  592 Mar 28 06:09 events.out.tfevents.1459103131.0a8840dee548\n-rwxrwxrwx 1 root root  500 Mar 28 06:09 events.out.tfevents.1459103348.0a8840dee548\n-rwxrwxrwx 1 root root  500 Mar 28 06:09 events.out.tfevents.1459103526.0a8840dee548\n-rwxrwxrwx 1 root root  500 Mar 28 06:09 events.out.tfevents.1459103820.0a8840dee548\n-rwxrwxrwx 1 root root  537 Mar 28 06:09 events.out.tfevents.1459110199.0a8840dee548\n```\n\nfor `test_logs`, and\n\n```\ntotal 1.2M\ndrwxrwxrwx 1 root root 4.0K Mar 27 20:38 .\ndrwxrwxrwx 1 root root    0 Mar 27 20:36 ..\n-rwxrwxrwx 1 root root  21K Mar 27 20:38 events.out.tfevents.1459111009.0a8840dee548\n-rwxrwxrwx 1 root root  32K Mar 27 20:38 events.out.tfevents.1459111051.0a8840dee548\n-rwxrwxrwx 1 root root 1.1M Mar 28 06:09 events.out.tfevents.1459111110.0a8840dee548\n```\n\nfor `mnist_logs` (from script [mnist_with_summaries.py](https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py)). Both shows no event in tensorboard.\n\nTensorflow and Tensorboard were both installed by default from tensorflow/tensorflow docker. Pip version is `pip 8.0.2 from /usr/local/lib/python2.7/dist-packages (python 2.7)`.\n", "@jaycode I am having similar problems. I am not able to even see the computation graph.\nAre you able to see the graph?\n", "Yeah. In my case the path was incorrect. Try running tensorboard with\n`--debug` to see what the problem is.\nOn Apr 11, 2016 2:58 AM, \"ultrons\" notifications@github.com wrote:\n\n@jaycode https://github.com/jaycode I am having similar problems. I am\nnot able to even see the computation graph.\nAre you able to see the graph?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/tensorflow/tensorflow/issues/530#issuecomment-208052591\n", "Thanks for the --debug option tip. I see only one event file in my directory though. Doesn't seem right.\nI was looking for a method to just visualize the computation graph and nothing else (no histogram, learning curve etc). Have you tried to do it by any chance?\n", "@ultrons We actually have an internal debugging tool that is just the graph visualizer without the rest of TensorBoard. It wouldn't be too difficult to export this as a standalone tool. If you want it, can you please file another issue requesting this feature, and we can ask @dsmilkov to take a look.\n", "@danmane , good to know that this functionality exists. I will create another issue to request this as a feature in future releases.\n", "I do not if it is correct to ask this here, but I am wondering about the installation for developers using bazel. The problem is that when I tried to compile tensorflow (tf), bazel cleans the cache so when i tried to run tensorboard (tb) it show the message that tb is not in the list. and then when I tried to compile tb it cleans tf. Is possible to keep both of them using the source code? and avoiding run bazel every single time that i need tf or tb?  or Do i need to have two installations folders one for tf and another for tb?\n", "@Fhrozen \nYou should be able to build both tensorflow and tensorboard using bazel without having the two conflict. I'm not sure what's going wrong in your case - you could try building all your targets (without using bazel run) and then run them by invoking the built target in `bazel-bin`.\nIf you keep having issues with bazel, please open a separate issue with a clear reproduction of the exact commands you're using and the errors that you're seeing.\n", "Install docker, tensorflow and python \r\nhttps://youtu.be/OiPjSnKAmSI"]}, {"number": 529, "title": "Spelling mistake in tensorflow.org", "body": "https://www.tensorflow.org/versions/master/resources/index.html\n\nIn the white paper section:\n\"...can be found in **out** white paper\"\n", "comments": ["Thanks for reporting, fix is on the way.\n\nOn Wed, Dec 16, 2015 at 5:04 PM mac craig notifications@github.com wrote:\n\n> https://www.tensorflow.org/versions/master/resources/index.html\n> \n> In the white paper section:\n> \"...can be found in _out_ white paper\"\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/529.\n", "No problem! \n"]}, {"number": 528, "title": "tf.image.resize_images and resize_{method} should work with single image as well as batch", "body": "Or at least not transparently do the wrong thing with the resulting shape.\nAt present, it's necessary to \n      image = tf.expand_dims(image, [0])\nand then squeeze it before passing to resize.\n", "comments": ["I've submitted a fix that corrects the broken case of this.  There was a missing squeeze if the input image was the same size as the desired output.\n", "@aselle \r\n\r\nI am encountering this again\r\n\r\n```\r\n    image = tf.image.resize(image, resize)\r\n\r\n  File \"//anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/image_ops_impl.py\", line 1182, in resize_images\r\n    skip_resize_if_same=True)\r\n\r\n  File \"//anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/image_ops_impl.py\", line 1029, in _resize_images_common\r\n    raise ValueError('\\'images\\' contains no shape.')\r\n\r\nValueError: 'images' contains no shape.\r\n\r\n```\r\n\r\nwhere `image` is a single image\r\n\r\n\r\n```python\r\ndef preprocess_image(\r\n    filename:str,\r\n    channels:int=3,\r\n    resize:list=None,\r\n    random_rotate:bool=False,\r\n    rotate_angle:float=0.,\r\n    random_transform:bool=False,\r\n    transform_config:dict=None\r\n)->list:\r\n    '''\r\n    Arguments:\r\n        filename (str): the full path filename of an image\r\n        channels (int): how many channels the image is assumed to have. Defaults\r\n            to 3.\r\n        resize (list): the resize dimensions to apply to the image. By default\r\n            is set to None (no resize transformation is applied).\r\n\r\n        random_rotate (bool): whether or not a random `angle` should be used.\r\n\r\n        rotate_angle (float): the angle of which to rotate. Expected to be between `0`\r\n            and `360`.\r\n\r\n        random_transform (bool): whether or not `transform_config` should be\r\n            provided to set `random_transform` or to `apply_transform`\r\n        transform_config (dict): the parameters to be passed to `apply_transform`\r\n            or `get_random_transform`. By default `None`.\r\n    Returns:\r\n        image (tf.Tensor): the pixel values fo the image\r\n    '''\r\n    image = tf.io.read_file(filename)\r\n    image = tf.image.decode_jpeg(image, channels=channels)\r\n\r\n    print(image) # <--- for debug\r\n\r\n    if rotate_angle is not None:\r\n        image = tf_rotate_image(image, rotate_angle, random_rotate)\r\n        # image /= 255.0\r\n\r\n    if resize is not None:\r\n        image = tf.image.resize(image, resize)\r\n        image /= 255.0  # normalize to [0,1] range\r\n\r\n    if transform_config is not None:\r\n        if random_transform:\r\n            image = apply_random_transform(image, **transform_config)\r\n        else:\r\n            image = apply_transform(image, **transform_config)\r\n    return image\r\n\r\n```\r\n\r\nprints out \r\n\r\n`Tensor(\"DecodeJpeg:0\", shape=(?, ?, 3), dtype=uint8)`"]}, {"number": 527, "title": "GLIBC error for RHEL, Centos clusters", "body": "After I tried all possible way to get Tensorflow work on my cluster, I re-open the issue according to this: https://github.com/tensorflow/tensorflow/issues/110\n\nFirst, bazel require GLIBC 2.14, 3 issues came up, however, no solution or idea for the problem is proposed.\nhttps://github.com/bazelbuild/bazel/issues/583\nhttps://github.com/bazelbuild/bazel/issues/590\nhttps://github.com/bazelbuild/bazel/issues/585\n\nAnd some issues reported on Tensorflow:\nhttps://github.com/tensorflow/tensorflow/issues/462\nhttps://github.com/tensorflow/tensorflow/issues/53\nhttps://github.com/tensorflow/tensorflow/issues/177\n\nI want to emphasize that the issue is more hopeless for clusters' users, a large amount of services with old Centos remains, and it is impossible to update GLIBC on server.\n\nI asked admin for others way around, and this is his response:\n\n> I have made some progress in installing Tensorflow, but it turned out that the version of glibc installed on [Cluster](2.12) is not new enough.\n> The version 0.1.1 of bazel checks the version of glibc during installation process and exits with an error that glibc 2.14 is not available.\n> The version 0.1.0 of bazel builds successfully, but later it turns out that Tensorflow requires even newer version glibc: 2.16.\n> Moreover, Glibc is very difficult, practically impossible, to install to a non-standard location, so I'm afraid that we cannot help with that either.\n\nI can confirm that we should never try to build our own GLIBC, you cannot even type 'ls' or 'cd' on server. Hence, the only hope is from tensorflow team support.\n\nBest Regards.\n", "comments": ["Similar issues arise on my cluster where multiple versions of GCC/glibcxx co-exist. Hard-coded `/usr/bin/gcc` and `/usr/lib64/libstdc++.so.6` doesn't make sense. By far, there seems to be no way to fix the `GLIBCXX` issues on CentOS/RHEL 6.x. \n", "Have a look here and there:\nhttp://stackoverflow.com/a/34897674/1990516\nhttp://stackoverflow.com/a/34900471/1990516\n", "We made some porogress on making bazel to work on RHEL/CentOS 6.x .\n\nhttps://github.com/bazelbuild/bazel/issues/760#issuecomment-173930465\n", "@martinwicke: Any progress on this?  \n", "Is this still an issue? If it isn't we'll close it here. There may be a bazel issue which should be tracked elsewhere.\n", "Automatically closing due to lack of activity.\n", "I believe the latest is that this is blocked on bazel. Not sure there's\nmuch we can do from the TensorFlow side. Have you tried building with cmake?\n", "I had GLIBC Errors of this sort and wrote some notes on installing TensorFlow from source on [Scientific Linux Cluster without root privilege](http://harlambert.co.uk/tensorflow_notes/). ", "I had the same problem when I installed TesnsorFlow using pip in CentOS 6.7 cluster. But using Miniconda, I directly install using 'conda install tensorflow' and it works.", "Thank you so much! I am trying to install and run TensorFlow in an old Red Hat 4.4.7-1 cluster for over a week and can not success due to the `glibc` problem. Now `conda install tensorflow` solved my problem. @KrnTneja ", "I tried another way: \r\n(1) install a newer version of Glibc (like 2.18) on the system \r\n\r\n(2) run Python3 by specifying the LD_library and interpreter like this; \r\n/path/to/glibc/2.18/lib/ld-linux-x86-64.so.2 --library-path /path/to/glibc/2.18/lib:/path/to/other/lib `which python3`\r\n\r\n(3) import tensorflow in Python. \r\nSo far it sometimes has other issues like not supporting ssl very well (may due to library issue?), but in terms of tensorflow, it works! ", "Thank you for this thread. conda install tensorflow solved my problem for RHEL 6.x, after creating a new Conda environment.", "Why did 'conda install tensorflow' not make the front page of the New York Times???? I just spent four days wrestling with building bazel, getting a new version of gcc, figuring out I needed to upgrade binutils, and still running into an assembler problem with the code in boringssl.\r\nSo kudos to whoever made this work -- and please advertise this more broadly!", "I've tried installing tensorflow with conda install tensorflow but when I import it I get the same GLIBC_XX not found issue. For me its 2.16 and my system CENTOS6 is on glibc2.12"]}, {"number": 526, "title": "Context in use?", "body": "Running the following with GPU support : \n\n`python convolutional.py`\n\nthrows the error:\n`F tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)`\n\n`Aborted`\n\nIt seems like 216 when calling cuCtxSetCurrent (which I'm assuming assigns the context to the calling CPU thread) corresponds to CUDA_ERROR_CONTEXT_ALREADY_IN_USE. \n\nWhat may be causing this error? It seems like the script successfully transfers data to the GPU and fails when initialize_all_variables() is called. \n", "comments": ["The complete log is here : http://pastebin.com/as0fWvYv\n", "And the same issue with tutorials_example_trainer. Log here : http://pastebin.com/vPkFfete\n", "@noisychannel, could you provide a bit more information about your running\nenvironment. I see that you have two K20m on your machine. Is this a\ndedicated machine, or something shared?\n\nOn Fri, Dec 18, 2015 at 11:06 AM, Derek Murray notifications@github.com\nwrote:\n\n> Assigned #526 https://github.com/tensorflow/tensorflow/issues/526 to\n> @zheng-xq https://github.com/zheng-xq.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/526#event-496149991.\n", "Tried both scenarios : \n1. Dedicated with 2 K20s. \n2. Shared with 2 K20s and 1 available and selected for use. \n\nSame issue in both cases. \n", "@noisychannel  I have the same issue. Did you solve it?\n", "No, the issue remains. \n", "@zheng-xq Is the bug fixed in the recently released tensorflow 0.7?\n", "Any updates here? \n", "I've started an offline conversation with the stream-executor team, since the error originates from stream-executor. Still wait for their response. \n\n@leary-google, @eliben, anything from the stream-executor side?\n", "I am still seeing the same cuda error for tensorflow 0.7. The error is `Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)`\n\nversion:\n\n```\ncommit b88971051fbc49fa1e0b91ec1b0b60defa11697e\nMerge: 5a30c8f 00986d4\nAuthor: Derek Murray <mrry@google.com>\nDate:   Fri Feb 26 05:08:35 2016 -0800\n```\n\nerror:\n\n```\n++ python cifar10_train.py\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.4 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.5 locally\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K40m\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:08:00.0\nTotal memory: 11.25GiB\nFree memory: 11.15GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:718] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:08:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256B\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 512B\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:107] Allocating 10.60GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:118] GPU 0 memory begins at 0x13047a0000 extends to 0x15aaa4019a\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\n./run.sh: line 4: 33880 Aborted                 python cifar10_train.py\n```\n", "Indeed, I am seeing the same error sometimes on a shared K40. It seems to happen when someone else has completed a job, and somehow the context is not cleared? I am sure that no job is actually executing on the GPU at the time.\n", "I am having the same issue with GPU:1, I can run without problems in GPU:0, but when trying to force the graph to be in GPU:1, using Graph.device(), I get the following: http://pastebin.com/ekTgqJ0U\n", "I encountered the error `Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)` recently, which turned out to be caused by someone accidently set the compute mode of GPU to be EXCLUSIVE_THREAD. Revert it back to DEFAULT solved my error. \n", "@zheng-xq: Should we contact the Stream Executor folk offline?  It looks like they might not have Github notifications turned on. \n", "Issue still exists in 0.9. \n", "Out of curiosity, how is this not a bigger issue? Is there a specific condition where this failure occurs? It seems like the process crashes whether all or any GPUs are available on the machine. \n\nHow do other people get around this? \n", "Regarding the comment of @zxvix, saying: \"I encountered the error Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216) recently, which turned out to be caused by someone accidently set the compute mode of GPU to be EXCLUSIVE_THREAD. Revert it back to DEFAULT solved my error.\"\n\nSometimes on shared clusters there are valid reasons for setting exclusive mode for GPUs.  Does TensorFlow require particular modes?  Is EXCLUSIVE_PROCESS a possibility?\n", "Adding @henline, who is the owner of stream-executor. \n", "I am seeing a similar issue on 0.9 compiled from source, HEAD:\n\n```\ncommit 554ddd9ad2d4abad5a9a31f2d245f0b1012f0d10\nMerge: 89e1cc5 a0745a7\nAuthor: yifeif <fengyifei2026@gmail.com>\nDate:   Tue Jul 26 16:17:21 2016 -0700\n```\n\nI am on a shared cluster with a scheduler, so I should have exclusive access to the node during my time slice. It looks like exclusive mode is set, but there are no running processes at the time time I try to use it:\n\n```\n[2016-07-28T17:59:19Z]: +------------------------------------------------------+                       \n[2016-07-28T17:59:19Z]: | NVIDIA-SMI 352.39     Driver Version: 352.39         |                       \n[2016-07-28T17:59:19Z]: |-------------------------------+----------------------+----------------------+\n[2016-07-28T17:59:19Z]: | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n[2016-07-28T17:59:19Z]: | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n[2016-07-28T17:59:19Z]: |===============================+======================+======================|\n[2016-07-28T17:59:19Z]: |   0  Tesla K40m          Off  | 0000:08:00.0     Off |                    0 |\n[2016-07-28T17:59:19Z]: | N/A   25C    P8    19W / 235W |     23MiB / 11519MiB |      0%    E. Thread |\n[2016-07-28T17:59:19Z]: +-------------------------------+----------------------+----------------------+\n[2016-07-28T17:59:19Z]:                                                                                \n[2016-07-28T17:59:19Z]: +-----------------------------------------------------------------------------+\n[2016-07-28T17:59:19Z]: | Processes:                                                       GPU Memory |\n[2016-07-28T17:59:19Z]: |  GPU       PID  Type  Process name                               Usage      |\n[2016-07-28T17:59:19Z]: |=============================================================================|\n[2016-07-28T17:59:19Z]: |  No running processes found                                                 |\n[2016-07-28T17:59:19Z]: +-----------------------------------------------------------------------------+\n[2016-07-28T17:59:21Z]: Using TensorFlow backend.\n[2016-07-28T17:59:22Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\n[2016-07-28T17:59:22Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\n[2016-07-28T17:59:22Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\n[2016-07-28T17:59:23Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.4.0.7 locally\n[2016-07-28T17:59:23Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\n[2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \n[2016-07-28T17:59:34Z]: name: Tesla K40m\n[2016-07-28T17:59:34Z]: major: 3 minor: 5 memoryClockRate (GHz) 0.745\n[2016-07-28T17:59:34Z]: pciBusID 0000:08:00.0\n[2016-07-28T17:59:34Z]: Total memory: 11.25GiB\n[2016-07-28T17:59:34Z]: Free memory: 11.15GiB\n[2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \n[2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \n[2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:08:00.0)\n[2016-07-28T17:59:34Z]: F tensorflow/stream_executor/cuda/cuda_driver.cc:395] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(cuda_context->context()) (0 vs. 216)\n[2016-07-28T17:59:34Z]: X_train shape: (60000, 1, 28, 28)\n[2016-07-28T17:59:34Z]: 60000 train samples\n[2016-07-28T17:59:34Z]: 10000 test samples\n[2016-07-28T17:59:41Z]: /tmp/wrapper5271742824235601482.sh: line 12: 61655 Aborted                 (core dumped) python mnist_cnn.py\n[2016-07-28T17:59:41Z]: Exited with code 0\n```\n", "Tensorflow guys, if I were you I would change the assert statement that\nfails here:\n\ncuda/cuda_driver.cc:395] Check failed: CUDA_SUCCESS ==\ndynload::cuCtxSetCurrent(cuda_context->context()) (0 vs. 216)\n\nto some code that prints out the text form of the CUDA exit code, and maybe\nfor good measure tries to invoke nvidia-smi to get extra information.  We\nfound this necessary in Kaldi in order to ensure that when there are\nproblems, all the information needed is in the log.\nDan\n\nOn Thu, Jul 28, 2016 at 11:52 AM, Mark Whitney notifications@github.com\nwrote:\n\n> I am seeing a similar issue on 0.9 compiled from source, HEAD:\n> \n> commit 554ddd9ad2d4abad5a9a31f2d245f0b1012f0d10\n> Merge: 89e1cc5 a0745a7\n> Author: yifeif fengyifei2026@gmail.com\n> Date:   Tue Jul 26 16:17:21 2016 -0700\n> \n> I am on a shared cluster with a scheduler, so I should have exclusive\n> access to the node during my time slice. It looks like exclusive mode is\n> set, but there are no running processes at the time time I try to use it:\n> \n> [2016-07-28T17:59:19Z]: +------------------------------------------------------+\n> [2016-07-28T17:59:19Z]: | NVIDIA-SMI 352.39     Driver Version: 352.39         |\n> [2016-07-28T17:59:19Z]: |-------------------------------+----------------------+----------------------+\n> [2016-07-28T17:59:19Z]: | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n> [2016-07-28T17:59:19Z]: | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n> [2016-07-28T17:59:19Z]: |===============================+======================+======================|\n> [2016-07-28T17:59:19Z]: |   0  Tesla K40m          Off  | 0000:08:00.0     Off |                    0 |\n> [2016-07-28T17:59:19Z]: | N/A   25C    P8    19W / 235W |     23MiB / 11519MiB |      0%    E. Thread |\n> [2016-07-28T17:59:19Z]: +-------------------------------+----------------------+----------------------+\n> [2016-07-28T17:59:19Z]:\n> [2016-07-28T17:59:19Z]: +-----------------------------------------------------------------------------+\n> [2016-07-28T17:59:19Z]: | Processes:                                                       GPU Memory |\n> [2016-07-28T17:59:19Z]: |  GPU       PID  Type  Process name                               Usage      |\n> [2016-07-28T17:59:19Z]: |=============================================================================|\n> [2016-07-28T17:59:19Z]: |  No running processes found                                                 |\n> [2016-07-28T17:59:19Z]: +-----------------------------------------------------------------------------+\n> [2016-07-28T17:59:21Z]: Using TensorFlow backend.\n> [2016-07-28T17:59:22Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\n> [2016-07-28T17:59:22Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\n> [2016-07-28T17:59:22Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\n> [2016-07-28T17:59:23Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.4.0.7 locally\n> [2016-07-28T17:59:23Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\n> [2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\n> [2016-07-28T17:59:34Z]: name: Tesla K40m\n> [2016-07-28T17:59:34Z]: major: 3 minor: 5 memoryClockRate (GHz) 0.745\n> [2016-07-28T17:59:34Z]: pciBusID 0000:08:00.0\n> [2016-07-28T17:59:34Z]: Total memory: 11.25GiB\n> [2016-07-28T17:59:34Z]: Free memory: 11.15GiB\n> [2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\n> [2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\n> [2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:08:00.0)\n> [2016-07-28T17:59:34Z]: F tensorflow/stream_executor/cuda/cuda_driver.cc:395] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(cuda_context->context()) (0 vs. 216)\n> [2016-07-28T17:59:34Z]: X_train shape: (60000, 1, 28, 28)\n> [2016-07-28T17:59:34Z]: 60000 train samples\n> [2016-07-28T17:59:34Z]: 10000 test samples\n> [2016-07-28T17:59:41Z]: /tmp/wrapper5271742824235601482.sh: line 12: 61655 Aborted                 (core dumped) python mnist_cnn.py\n> [2016-07-28T17:59:41Z]: Exited with code 0\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/526#issuecomment-235989554,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ADJVu5zDQGnFRHmS-sFWKkA2dnlN3v1Cks5qaPqBgaJpZM4G2yCN\n> .\n", "Hi, I'm the owner of StreamExecutor. Sorry for arriving late to this discussion.\n\nI believe this problem is caused in all cases by GPUs with their compute mode set to EXCLUSIVE_THREAD (just as mentioned by @zxvix). The solution is to set the compute mode to DEFAULT or EXCLUSIVE_PROCESS, which can be done via one of the following commands:\n\n```\n$ nvidia-smi --compute-mode=0 # for DEFAULT\n$ nvidia-smi --compute-mode=3 # for EXCLUSIVE_PROCESS\n```\n\nThe `nvidia-smi -q` command can also be used to query the current compute mode of the device.\n\nIf anyone is seeing this error when the device compute mode is either DEFAULT or EXCLUSIVE_PROCESS, please let me know, because I don't think that should be possible.\n\nStreamExecutor will not work in either EXCLUSIVE_THREAD or PROHIBITED compute mode, but in response to @danpovey's question about shared clusters, EXCLUSIVE_PROCESS mode should be fine.\n\nThere are no plans in StreamExecutor to support EXCLUSIVE_THREAD mode because it is listed as deprecated in the nvidia-smi help message. There are also no plans to support PROHIBITED mode because I think that mode prevents the creation of contexts, and StreamExecutor cannot function with that restriction.\n\nIn response to @danpovey's suggestion about adding a better error message for this case, I think that's a good idea. I will work on getting a patch up to warn about the device compute mode if `cuCtxSetCurrent` fails and the compute mode is set to an unsupported setting.\n", "Thanks, good to know it should work in EXCLUSIVE_PROCESS.\n", "So it sounds like this is working as intended, since StreamExecutor doesn't plan on supporting modes other than EXCLUSIVE_PROCESS.\n", "Just to confirm, StreamExecutor works with EXCLUSIVE_PROCESS. Hopefully, @danpovey's suggestion about better error messages will be added in soon. It may be hard for people to search for this issue. \n", "For anyone using GPU based tensorflow on Compute Canada resources, submitting the job by specifying EXCLUSIVE_PROCESS worked for me.", "@MidoAssran Thanks for that! Will try it out.", "I meet this error  \r\n![image](https://user-images.githubusercontent.com/7299296/31105041-dfe35b10-a795-11e7-9d64-22c460ebaefd.png)\r\nchange compute_mode does not solve the issue, any guidance?", "I can't really see what it's saying (next time paste as text), but in any\ncase I don't  know what this might be.  Maybe all the GPUs were already in\nuse.\n\n\nOn Mon, Oct 2, 2017 at 8:22 PM, 201power <notifications@github.com> wrote:\n\n> I meet this error\n> [image: image]\n> <https://user-images.githubusercontent.com/7299296/31105041-dfe35b10-a795-11e7-9d64-22c460ebaefd.png>\n> change compute_mode does not solve the issue, any guidance?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/526#issuecomment-333702147>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADJVux59GbGOM5GlqUBTsst_i5N6RwA_ks5soX42gaJpZM4G2yCN>\n> .\n>\n", "The GPU might be in use. This happened in a TF session followed immediately by another TF session.   \r\nI am not sure how to tell that if GPU is in use, since TF won't release memory after the first session finished. ", "changing compute mode didn't solve the problem, i am getting the same error, my cards are two tesla k80 's\r\nand driver version 375        cuda version 8.0          tensorflow version 1.1", "I'm getting the same error (or similar one)\r\n`2019-01-26 01:22:06.728780: F tensorflow/stream_executor/cuda/cuda_driver.cc:206] Check failed: CUDA_SUCCESS == cuCtxSetCurrent(cuda_context->context()) (0 vs. 4)\r\nAborted (core dumped)\r\n`\r\nSample of my Nvidia-smi command output:\r\n`Sat Jan 26 01:22:29 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 2080    Off  | 00000000:17:00.0 Off |                  N/A |\r\n|  0%   42C    P8     1W / 225W |      0MiB /  7952MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce RTX 2080    Off  | 00000000:65:00.0  On |                  N/A |\r\n|  0%   47C    P8    13W / 225W |    166MiB /  7951MiB |      6%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    1      1101      G   /usr/lib/xorg/Xorg                           165MiB |\r\n+-----------------------------------------------------------------------------+\r\n`\r\n\r\nI'm running TF verison 1.12 on Ubuntu 18.04 Cuda 10.0, CUDNN 7.3.1", "> Hi, I'm the owner of StreamExecutor. Sorry for arriving late to this discussion.\r\n> \r\n> I believe this problem is caused in all cases by GPUs with their compute mode set to EXCLUSIVE_THREAD (just as mentioned by @zxvix). The solution is to set the compute mode to DEFAULT or EXCLUSIVE_PROCESS, which can be done via one of the following commands:\r\n> \r\n> ```\r\n> $ nvidia-smi --compute-mode=0 # for DEFAULT\r\n> $ nvidia-smi --compute-mode=3 # for EXCLUSIVE_PROCESS\r\n> ```\r\n> \r\n> The `nvidia-smi -q` command can also be used to query the current compute mode of the device.\r\n> \r\n> If anyone is seeing this error when the device compute mode is either DEFAULT or EXCLUSIVE_PROCESS, please let me know, because I don't think that should be possible.\r\n> \r\n> StreamExecutor will not work in either EXCLUSIVE_THREAD or PROHIBITED compute mode, but in response to @danpovey's question about shared clusters, EXCLUSIVE_PROCESS mode should be fine.\r\n> \r\n> There are no plans in StreamExecutor to support EXCLUSIVE_THREAD mode because it is listed as deprecated in the nvidia-smi help message. There are also no plans to support PROHIBITED mode because I think that mode prevents the creation of contexts, and StreamExecutor cannot function with that restriction.\r\n> \r\n> In response to @danpovey's suggestion about adding a better error message for this case, I think that's a good idea. I will work on getting a patch up to warn about the device compute mode if `cuCtxSetCurrent` fails and the compute mode is set to an unsupported setting.\r\n\r\n@henline  Sorry to trouble you. I have set the mode to DEFAULT (nvidia-smi --compute-mode=0), and it indeed takes effect by the info. from nvidia-smi -q. But I still got the error \"tensorflow/stream_executor/cuda/cuda_driver.cc:225] Check failed: CUDA_SUCCESS == cuCtxSetCurrent(cuda_context->context()) (0 vs. 3)\"."]}, {"number": 525, "title": "Consider adding tf.identity_like()", "body": "Hey TF, \n\nThis is not a big deal, but you may want to add an identity_like function that theano has here:\nhttp://nullege.com/codes/search/theano.tensor.identity_like\n\nIt would be useful for orthogonality with RNN's. \n\nIt would basically be something like:\n\n``` python\ndef identity_like(input_tensor, scope = None):\n  with tf.variable_scope(scope or \"identity_like\"): #in this linear scope, the library that you're retriving is Linear\n    shape_0 = tf.shape(input_tensor)[0]\n  return tf.diag(tf.ones(shape_0))\n```\n", "comments": ["Please consider a different API here.\n\nI could imagine \"ones_like()\", \"uniform_random_like()\", etc. quickly causing clutter. \n", "Maybe an op that copies the shape, but lets you pass an initializer (many\nof those functions already exist, and more should exist anyway)?\n\nOn Thu, Dec 17, 2015 at 9:10 AM Hello1024 notifications@github.com wrote:\n\n> Please consider a different API here.\n> \n> I could imagine \"ones_like()\", \"uniform_random_like()\", etc. quickly\n> causing clutter.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/525#issuecomment-165514703\n> .\n", "Dup of #434.\n"]}, {"number": 524, "title": "More descriptive error message when op is not defined for GPU or CPU", "body": "When trying to use MaxPoolWithArgmax I got the following error:\n\nInvalidArgumentError: No OpKernel was registered to support Op 'MaxPoolWithArgmax' with these attrs\n     [[Node: MaxPoolWithArgmax_5 = MaxPoolWithArgmax[Targmax=DT_INT64, ksize=[1, 3, 3, 1], padding=\"SAME\", strides=[1, 2, 2, 1]](random_normal_4)]]\n\nWhich doesn't tell me that the op is available in GPU but not in CPU.\n", "comments": ["I second this as well -- someting that would clearly define gpu compatibility would be great. If the documentation had it, that would be even better. \n\nOn the docs, we could just assume all ops are gpu compatible, but if they are marked as \"CPU only\", it would help!!!\n", "The op may be defined on GPU, but just not for those specific set of attributes (e.g., it could have been the case that it was defined for Targmax=DT_INT32).\n\nIt would be too cryptic of an error message to enumerate all possible registrations for that Op (which might be tens of kernels) in the error message. \n\nIn addition, it's totally legal for an op to be defined on GPU, but the binary to not include it, so even if we put something in the docs, it wouldn't be right.\n\nI agree that we could do a better job here, but it'll take some care to make sure we don't spam the error message with content that nobody can read.\n", "Maybe it is enough to change the error by adding the information which\nplacement was requested:\n\nInvalidArgumentError: No OpKernel was registered to support Op\n'MaxPoolWithArgmax' _for device '/gpu:0'_ with these attrs\n[[Node: MaxPoolWithArgmax_5 = MaxPoolWithArgmaxTargmax=DT_INT64, ksize=[1,\n3, 3, 1], padding=\"SAME\", strides=[1, 2, 2, 1] http://random_normal_4/]]\n\nOn Wed, Dec 16, 2015 at 9:15 AM Vijay Vasudevan notifications@github.com\nwrote:\n\n> The op may be defined on GPU, but just not for those specific set of\n> attributes (e.g., it could have been the case that it was defined for\n> Targmax=DT_INT32).\n> \n> It would be too cryptic of an error message to enumerate all possible\n> registrations for that Op (which might be tens of kernels) in the error\n> message.\n> \n> In addition, it's totally legal for an op to be defined on GPU, but the\n> binary to not include it, so even if we put something in the docs, it\n> wouldn't be right.\n> \n> I agree that we could do a better job here, but it'll take some care to\n> make sure we don't spam the error message with content that nobody can read.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/524#issuecomment-165179772\n> .\n", "Newbie question: how do I get a list of all registered opkernels?  If it's easy, the error message could advise the user on how to peruse the list.  It would then become obvious to the user that, e.g., the op they wanted wasn't registered for the gpu...\n", "Can anyone help me with this No opkernel registered error: #1269 . Following from the main.cc in label_image example and looking into the android demo tensorflow_jni.cc, I see that the android demo is directly reading the RGB values and putting it into an input tensor, whereas the label_image example builds a graphdef builder and runs several ops to derive to its input tensor. Could this difference be the reason for that error?\n", "@martinwicke, @vrv: Martin's suggested fix seems reasonable.  Should we assign this to someone or give some more details are and mark it contributions welcome? \n", "I believe we have fixed the error message to be much more helpful, but someone would have to verify ;)\n", "It doesn't look like we fixed it: https://github.com/tensorflow/tensorflow/blob/e9953ee70d45bfba8083eaa7ac902714f1a0b0a5/tensorflow/core/common_runtime/simple_placer.cc#L514\n", "That codepath only gets hit if there are no kernels at all that support that op.  There is no placement specified, and no kernels registered.  I think the situation @sguada has been fixed, you're pointing at a different error path.\n", "@vrv: Assigning to you since you seem to be the C++ runtime czar.  Feel free to reassign.\n", "Oops, sorry, missed your comment.  Never mind.\n", "I am using TF1.1. I found this op have no CPU kernel for either Targmax=tf.int32 or Targmax=tf.int64. For GPU (CUDA8 on a Titan X), it has kernel for Targmax=tf.int64, but no kernel for Targmax=tf.int32", "Did you find the error message to be bad? If so, can you paste the error message here, and how you would have improved it?\r\n\r\nIf your comment is about the absence of the kernel registration, please open a separate issue (or send a PR).", "@jacksonloper You can find a list of opcodes available in the makefile build from here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/tf_op_files.txt"]}, {"number": 523, "title": "Import Error: Ubuntu 14.04 Python 2.7", "body": "I followed the Pip Installation instructions and successfully downloaded tensorflow (as shown as \"Successfully installed tensorflow wheel six setuptools\").\nHowever, when I entered \"import tensorflow as tf\" in python, it gives: \n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named tensorflow\n\nI don't know what is the problem. Help please? Thank you so much in advance! :D \n", "comments": ["is your python possibly a different one from that into whose directories tensorflow was installed? Can you do the following and paste the output:\n\n```\ncat tools/python_bin_path.sh\npython --version\n```\n\n(the first one, in the tensorflow base directory)\n", "This is the output: \n\nFor cat tools/python_bin_path.sh: \ncat: tools/python_bin_path.sh: No such file or directory\n\nFor python --version: \nPython 2.7.11 :: Anaconda 2.4.1 (64-bit)\n\nFrom the output, it seems like I don't have the script for \"python_bin_path\". Is there another way to this? Thanks! :D \n", "If you installed via `pip`, please check that you used the `pip` command that matches the `python` command in your PATH. For instance do:\n\n```\npip show tensorflow\nwhich pip\nwhich python\n```\n", "Closing due to inactivity.\n", "```\nconda uninstall tensorflow\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl --ignore-installed\n```\n\nshould fix it\n"]}, {"number": 522, "title": "mmist failure when running with gpu on EC2", "body": "I compiled tensorflow to run with CUDA 3.0 as described here\nhttps://gist.github.com/erikbern/78ba519b97b440e10640\n\nWhen running mint example I am getting the error below. Is there a quick explanation for this?\n\nW tensorflow/core/common_runtime/executor.cc:1076] 0x6d39c90 Compute status: Invalid argument: Expected multiples[0] > 0, bu\nt got 0\n         [[Node: gradients/Mean_grad/Tile = Tile[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/Mean\n_grad/Reshape, gradients/Mean_grad/floordiv/_41)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x6d39190 Compute status: Invalid argument: Expected multiples[0] > 0, bu\nt got 0\n         [[Node: gradients/Mean_grad/Tile = Tile[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/Mean\n_grad/Reshape, gradients/Mean_grad/floordiv/_41)]]\n         [[Node: Momentum/value/_43 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", se\nnd_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_653_Momentum/value\", tensor_\ntype=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py\", line 290, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py\", line 272, in main\n    feed_dict=feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 444, in _do_run\n    e.code)\n", "comments": []}, {"number": 521, "title": "resize_image_with_crop_or_pad doesn't work with input pipelines", "body": "Because `tf.image.resize_image_with_crop_or_pad()` requires its input to have a fully defined shape, it's not useful as part of an input pipeline since the size of images loaded will not be known in advance.\n\n``` python\n  queue = tf.train.string_input_producer(filenames)\n  reader = tf.WholeFileReader()\n  _, contents = reader.read(queue)\n  image = tf.image.decode_jpeg(contents, channels=3)\n  cropped = tf.image.resize_image_with_crop_or_pad(image, 224, 224)\n```\n\nIdeally the above would work...\n", "comments": ["Yes, that's a bug.\n", "I needed to use the method `resize_image_with_crop_or_pad` as part of an input pipeline and ran into a similar issue.\n\nI'm not yet proficient enough with TensorFlow to tell if this is useful. I made a small workaround for personal use. If this workaround would be a route to use in fixing this issue I'd be happy to update the tests, docs and change out the logic in the existing implementation. If anyone can advise on its usefulness I'd appreciate the feedback.\n\n[Example Implementation of resize_image_with_crop_or_pad](https://gist.github.com/eerwitt/51aba4bffd9ddd5c581c#file-resize_image_with_crop_or_pad_pipeline-py-L6)\n", "@eerwitt It works flawlessly! thanks.\n\n```\n    img_bytes = tf.read_file(fp)\n    img_u8 = tf.image.decode_jpeg(img_bytes, channels=ch)\n    image = tf.image.convert_image_dtype(img_u8, dtype=tf.float32)\n    img_cropped = resize_image_with_crop_or_pad(image,img_size[1],img_size[0])\n```\n", "Send a PR?\n\nOn Wed, Feb 10, 2016 at 11:25 AM Pouya Samangouei notifications@github.com\nwrote:\n\n> @eerwitt https://github.com/eerwitt It works flawlessly! thanks.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/521#issuecomment-182539003\n> .\n", "So any news to solve the problem ?\n", "Other than the workaround posted above, no.\n", "Here is a monkey patching module based on PR https://github.com/tensorflow/tensorflow/pull/2737\n\n[resize_image_patch.py](https://gist.github.com/gaohuazuo/4093eee2aee4d5b8746bff698506ee57#file-resize_image_patch-py)\n\n**Note**\n- Remember to pass argument `dynamic_shape=True`\n- Input image must have static rank of 3. This can be done by `tf.Tensor.set_shape([None, None, None])`\n", "Looks like the issue has been fixed by PR #2737. Maybe this issue could be closed?", "Thanks!"]}, {"number": 520, "title": "Changing 1 kernel op requires recompile of the entire kernel package... slow : (", "body": "For example, I make some working changes into any one tensorflow/core/my_kernel.{cc,h} file will require the recompile of the entire:\n\n240 tf_cuda_library(\n241     name = \"kernels\",\n\nlibrary, which may be OK for Google assuming you guys have many cores and very fast machines, but for the average developer on say mac laptop, its insanely slow ... total compile time can range several minutes : (\n\nI am not sure what the correct solution is, either fix this in Bazel or seperate out each kernel to have its own build rule?\n", "comments": ["This will (probably) be addressed by the work @keveman is doing to improve registration for user ops, so I'll assign to him for now.\n", "I added [instructions](https://www.tensorflow.org/versions/master/how_tos/adding_an_op/index.html#adding-a-new-op) for building ops and kernels outside of TensorFlow source tree. Please have a look to see if it fits your need.\n"]}, {"number": 519, "title": "Can I use the GPU when installing via pip? If so where is the 'configure' script? If not where should I clone the repo?", "body": "Hi all, \n\nI am installing tensorflow and am really stuck with the GPU setup ( pip install worked fine for CPU ).\n\nI was forced to install via source because I could not find the 'configure' script anywhere at all when installing by pip. In fact the source directory was not created at all, only the 'tensorflow' folder in the level below.\n\nTo install by source I followed the tutorial here: https://www.tensorflow.org/versions/master/get_started/os_setup.html#installation-for-linux and successfully ran bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n\nHowever, importing tensorflow into a python file gives the familiar 'No module' import error. I cloned the repo while in my home directory, but I am not sure if this is correct.\n\nIs the pip installation supposed to allow GPU use? If so where can I locate the 'configure' script? If not where should I clone the repo, and is this the cause of the import error? \n\nAny help is greatly appreciated because I am very much lost! \n", "comments": ["You don't need to run configure when you use the provided pip binaries. There are two different wheels that we provide for Linux, one with GPU support, and one without. If you want to use GPU, install this wheel: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl\n"]}, {"number": 518, "title": "Unable to fetch IndexedSlices", "body": "It appears that IndexedSlices cannot be fetched, as demonstrated by the following program, which is trying to inspect the gradients of an embedding matrix:\n\n```\nimport tensorflow as tf\n\nx = tf.placeholder(tf.int32)\ny = tf.placeholder(tf.float32)\nembeddings = tf.get_variable(\"embeddings\", [1, 1])\nloss = tf.reduce_mean(tf.square(y - tf.nn.embedding_lookup(embeddings, x)))\ngrads = tf.gradients(loss, embeddings)\n\nwith tf.Session() as session:\n    tf.initialize_all_variables().run()\n    print(\"Gradient value: {}\".format(session.run(grads, { x : 0, y : 0.0 })))\n```\n\nRunning the code above gives this error:\n\n```\nTypeError: Fetch argument <tensorflow.python.framework.ops.IndexedSlices object at 0x1053d69d0> of <tensorflow.python.framework.ops.IndexedSlices object at 0x1053d69d0> has invalid type <class 'tensorflow.python.framework.ops.IndexedSlices'>, must be a string or Tensor. (Can not convert a IndexedSlices into a Tensor or Operation.)\n```\n\nIs this result intended? If so, would there be a better way to inspect the gradients?\n\nThanks!\n", "comments": ["I got similar problems\n", "One possibility is to map `tf.convert_to_tensor` over `grads` so that every element is converted to a (dense) `Tensor`. Does that work for your purposes?\n", "Yes, that would work, but it would great it can be done without losing sparsity.\n", "It would certainly be possible to handle this by handling `IndexedSlices` in a similar manner to `SparseTensor` in `Session.run()`, using the subfeed/subfetch mechanism. We'd be happy to take contributions on this!\n", "Fixed by #541.\n"]}, {"number": 517, "title": "Create convolutional.py", "body": "An example from expert MNIST tutorial\n", "comments": ["Hi @sdemyanov, thanks for this change.\n\nUnfortunately, we don't yet accept changes via github, we hope to improve this in https://github.com/tensorflow/tensorflow/issues/26\n\nYou can take a look at https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md to see how to contribute right now.\n"]}]