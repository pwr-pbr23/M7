[{"number": 28382, "title": "[INTEL MKL] Disable MKL Op Rewrite for Conv with padding==EXPLICIT", "body": "Tensorflow recently added a new feature of supporting a new \"EXPLICIT\" padding value with Conv2D op.\r\nHowever, MklConv does not support it yet.\r\n\r\nThis PR disables MKL node rewritting for such a special case (thus TF will pick up the Eigen version Conv2D). ", "comments": ["Thank you very much for the quick code review and approval!"]}, {"number": 28381, "title": "TPU Performance Regression 1.12 -> 1.13", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 & 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0 & 1.13.1\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: N/A TPU\r\n- GPU model and memory: N/A TPU\r\n\r\n**Describe the current behavior**\r\n\r\nUpgrading from Tensorflow 1.12.0 to 1.13.1 shows a large performance regression. Training goes from 2100 examples/s & 8.2 global steps/s to 860 examples/s & 3.4 global steps/s.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect no performance penalty or improved performance for upgrading a library.\r\n\r\n**Other info / logs**\r\nHere are the outputs of `capture_tpu_profile` for the base case & regression case respectively.\r\n![Base Profile](https://user-images.githubusercontent.com/2245080/57163873-35bc2700-6da7-11e9-856a-677a6b021731.png)\r\n![ Regression Profile](https://user-images.githubusercontent.com/2245080/57163877-3785ea80-6da7-11e9-87de-e05dd121be30.png)\r\n\r\nFor reference this profile was captured during training on MobileNet but we have also observed this on other convolutional architectures. We also use quantization.", "comments": ["@jschneier \r\nplease let us know if the issue still persist.", "@jschneier\r\nplease update on the above comment", "I no longer work on this at all so I won't be able to provide updates.", "@jschneier\r\n in that case could you please let us know if we could move this issue to closed status. "]}, {"number": 28380, "title": "Updated tf.mul that was removed", "body": "tf.mul was removed but it was there in this saver example. Along with it, tf.initialize_all_variables() ops is going to be deprecated so updated it with tf.initialize_all_variables().", "comments": []}, {"number": 28379, "title": "Support calibration table save and load to INT8 TRTEngineOp", "body": "Saving and loading INT8 calibration tables helps users:\r\n\r\n1. Skip the calibration step which can take a long time.\r\n2. Skip passing around the calibration dataset, which can be large.\r\n\r\nInstead, user can share their TF-TRT FP32 model and calibration tables (which are small, just a few floats per layer) and the customers can do INT8 model conversion and load the calibration tables into that model.\r\n\r\nThis PR also prevents building static graph to test validity of INT8 engine. That is not needed.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28379) for more info**.\n\n<!-- need_sender_cla -->", "@ashwin please sign CLA", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28379) for more info**.\n\n<!-- need_author_consent -->", "@ashwin can you please resolve conflicts.", "@rthadur I merged `tensorflow/master` to my branch to resolve these conflicts. The last commit on my branch resolves these conflicts: https://github.com/ashwin/tensorflow/tree/calibration_save_load\r\n\r\nBut this PR is not getting updated from the last commit from my branch. Do you know how to update this PR so that it will take latest commits from my branch?\r\n\r\n**Update**: No worries. The PR has updated itself now. It actually took 15 minutes to do that! :smile: \r\n", "@rthadur PR error says: `cla/google \u2014 CLAs are signed, but unable to verify author consent`\r\n\r\nHow to fix this? I have already joined the NVIDIA CLA group.", "Hi @ashwin (if you haven't already) can you make sure all the commits in this PR are associated with your corporate email address? I think that could be one reason of the CLA issue.", "@aaroey Yes, all my commits are now with my corporate email address. It wasn't so when I first opened this PR. Yesterday, I \"amended\" my Git commits to change email address and force pushed to this branch to make that happen.", "> @aaroey Yes, all my commits are now with my corporate email address. It wasn't so when I first opened this PR. Yesterday, I \"amended\" my Git commits to change email address and force pushed to this branch to make that happen.\r\n\r\n@ashwin , i see you have changes from other authors , please have \"freedomtan\" to sign CLA.\r\n\r\n![image](https://user-images.githubusercontent.com/43972606/57798917-be6d8800-7702-11e9-921a-3b27cba0d635.png)\r\n\r\n\r\n\r\n\r\n", "@rthadur I only rebased onto the tensorflow master branch. I do not even know who @freedomtan is. It looks like @tensorflower-gardener merged @freedomtan 's commits, not me. Should I try to resubmit with a new PR?", "sure", "@ashwin interesting. I think a brute force solution for this is something like `git rebase master` + `git push`", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28378, "title": "Cherrypick for doc updates and libtensorflow fix", "body": "", "comments": []}, {"number": 28377, "title": "Looping over tf.data.Dataset raises OutOfRangeError in TF 2.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): very little custom\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.4\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0.0-dev20190501\r\n- Python version: 3.6.7 (miniconda)\r\n\r\n**Describe the current behavior**\r\n\r\n```sh\r\nException has occurred: OutOfRangeError\r\nEnd of sequence [Op:IteratorGetNextSync]\r\n  File \"<string>\", line 3, in raise_from\r\n  File \"/usr/local/miniconda3/envs/te/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2120, in iterator_get_next_sync\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"/usr/local/miniconda3/envs/te/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 614, in _next_internal\r\n    output_shapes=self._flat_output_shapes)\r\n  File \"/usr/local/miniconda3/envs/te/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 622, in next\r\n    return self._next_internal()\r\n  File \"/usr/local/miniconda3/envs/te/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 585, in __next__\r\n    return self.next()\r\n```\r\n\r\n**Code to reproduce the issue**\r\n\r\n```py\r\ndef train_model(model, X_train, y_train, lr=0.01, epochs=1000, batch_size=32):\r\n\r\n    training_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\r\n    training_data = training_data.shuffle(buffer_size=2048).batch(batch_size)\r\n    optimizer = tf.keras.optimizers.Adam(lr=lr)\r\n\r\n    def robust_mse(y_true, model_output):\r\n        y_pred, var = tf.transpose(model_output)\r\n        loss = 0.5 * tf.square(y_true - y_pred) * tf.exp(-var) + 0.5 * var\r\n        return tf.reduce_mean(loss)\r\n\r\n    for epoch in range(epochs):\r\n        print(f\"Starting epoch {epoch}/{epochs}\")\r\n\r\n        for X_batch, y_batch in training_data:\r\n\r\n            with tf.GradientTape() as tape:\r\n                output = model(X_batch)  # predictive mean and variance for each sample\r\n                loss = robust_mse(y_batch, output)\r\n\r\n            grads = tape.gradient(loss, model.trainable_variables)\r\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n```", "comments": ["Will it be possible to provide us a minimal code snippet that can reproduce the issue ? That will surely help us in root causing faster. Thanks!", "[Here's my training data](https://github.com/tensorflow/tensorflow/files/3151877/data.zip). I load that with `numpy`'s `genfromtxt` function:\r\n\r\n```py\r\nwith open(\"features.csv\") as file:\r\n    features = np.genfromtxt(file, delimiter=\",\")\r\nwith open(\"labels.csv\") as file:\r\n    labels = np.genfromtxt(file, delimiter=\",\")\r\n```\r\nthen feed that in as `X_train` and `y_train`.\r\n\r\nThe model is generated by a `build_model` function:\r\n\r\n```py\r\ndef build_model(input_dim, layers=None):\r\n    \"\"\"\r\n    Build dropout FCNN with two outputs for aleatoric and epistemic uncertainty estimation.\r\n\r\n    input_dim (int): number of features (columns in the matrix X)\r\n    layers (tuple): overrides the default_layers below\r\n    \"\"\"\r\n    default_layers = (\r\n        (\"Dense\", {\"units\": 100, \"activation\": \"tanh\"}),\r\n        (\"Dropout\", {\"rate\": 0.5}),\r\n        (\"Dense\", {\"units\": 50, \"activation\": \"relu\"}),\r\n        (\"Dropout\", {\"rate\": 0.3}),\r\n        (\"Dense\", {\"units\": 25, \"activation\": \"relu\"}),\r\n        (\"Dropout\", {\"rate\": 0.3}),\r\n        (\"Dense\", {\"units\": 10, \"activation\": \"relu\"}),\r\n        (\"Dropout\", {\"rate\": 0.3}),\r\n    )\r\n    model = tf.keras.Sequential()\r\n    model.add(tf.keras.layers.Input(shape=(input_dim,), name=\"input\"))\r\n\r\n    for layer, kwargs in layers or default_layers:\r\n        model.add(getattr(tf.keras.layers, layer)(**kwargs))\r\n\r\n    # predictive mean and data-dependent uncertainty output\r\n    model.add(tf.keras.layers.Dense(units=2, activation=\"linear\", name=\"output\"))\r\n    return model\r\n```\r\nwhere `input_dim = X_train.shape[1]`.\r\n\r\nLet me know if you need any more info.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "@ymodak How do you know it's not a bug? I'd be curious to know what I'm doing wrong in that case, especially since I was first asked to provide a minimal example of the problem by @achandraa rather than be referred to SO straight away."]}, {"number": 28376, "title": "Add Int32 and Int64 value support to SplitV.", "body": "Support additional data types for the SplitV operator in TensorFlow Lite.", "comments": ["@rthadur @alanchiao could I get a review on this change? thanks!", "Looking into this PR now"]}, {"number": 28375, "title": "compact rendering of layer I/O shapes in plot_model", "body": "Render the input/output shapes of a layer more compactly by replacing `None` with `?`, e.g. render `(None,100,100)` to `(?,100,100)`.", "comments": ["@dynamicwebpaige @rthadur any update on this?"]}, {"number": 28374, "title": "tf.data.Dataset allow change of structure", "body": "**System information**\r\n- TensorFlow version (you are using): v2\r\n- Are you willing to contribute it (Yes/No): only python code\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nMy current use case is as follows:\r\nI get a dataset from `tensorflow_datasets.load()` where the structure is `(image, label)`, e.g. `MNIST`. I would like to only use the `image` information of that dataset. Therefore I tried to find a way to \"unpack\" that dataset from a dict into a single element (so no key required to work on it).\r\n\r\n```python\r\n# Currently\r\n\r\ndataset = tfds.load('MNIST', split='train')\r\nfor d in dataset:\r\n  # Now d has the \"shape\" {'image': tensor, 'label': tensor}\r\n\r\n# What I would like to do\r\ndataset = tfds.load('MNIST', split='train')\r\ndataset = tf.data.Dataset.unpack(dataset, 'image')\r\nfor d in dataset:\r\n  # Now d is a tensor\r\n```\r\n\r\nWhen the dataset is `unpacked` I can also do further transformations on it, e.g. `filter`, `batch`, ...\r\n\r\n**Will this change the current api? How?** No\r\n\r\n**Who will benefit with this feature?** Users of `tensorflow_dataset` I assume.\r\n\r\n**Any Other info.**\r\n", "comments": ["It could also be seen as a kind of subset, where when the subset size is 1 it unpacks the dict to a tensor.\r\nThis would allow selecting multiple keys as a subset from a `tf.data.Dataset`.", "Hi @sleighsoft you can use the `map` transformation to do what you want:\r\n\r\n```\r\ndataset = tfds.load('MNIST', split='train')\r\ndataset = dataset.map(lambda x: x['image'])\r\n# now dataset only contains sequence of images\r\n```"]}, {"number": 28373, "title": "TensorFlow Lite converter type mismatch during keras model conversion", "body": "**System information**\r\n- OS Platform and Distributio: Elementary OS 5\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version:\r\ncudatoolkit               10.0.130\r\ncudnn                     7.3.1\r\n- GPU model and memory: GTX 1060 6 Gb\r\n\r\n**Error**\r\nTypeError: Input 'y' of 'Equal' Op has type float32 that does not match type int32 of argument 'x'.\r\nin tensorflow/python/framework/op_def_library.py, line 547, in _apply_op_helper\r\n\r\n**Code to reproduce the issue**\r\n`tflite_convert\r\n  --output_file=./tflite_model.tflite\r\n  --keras_model_file=./images/train-set/train-set_model.h5`\r\nHow net was trained and saved shown in file attached.\r\nFull console output attached also.\r\n[net and console.zip](https://github.com/tensorflow/tensorflow/files/3142890/net.and.console.zip)\r\n\r\nThe same if called by python API.\r\n\r\nTrained model:\r\n[train-set_model.h5.zip](https://github.com/tensorflow/tensorflow/files/3154779/train-set_model.h5.zip)\r\n\r\n", "comments": ["@Knize Can you please use the tf nightly build and check if the issue still persists or not.", "@muddham With tf-nightly result is\r\nE tensorflow/core/grappler/grappler_item_builder.cc:637] Init node conv2d_1/kernel/Assign doesn't exist in graph\r\nFull console output attached.\r\n[console.txt](https://github.com/tensorflow/tensorflow/files/3150277/console.txt)\r\n\r\n", "@Knize , can you please attach the train-set_model.h5 file as well, will try to reproduce the error on my work station and give you a solution.\r\n\r\nRegards\r\nAmit", "@Knize Please refer the stackoverflow [link](https://stackoverflow.com/questions/54122044/tensorflow-lite-init-node-doesnt-exist). Stackoverflow user says he could resolve this issue by downgrading to tensorflow 1.12.Please let us know how it progresses . Thanks!", "@amitsrivastava78 Uploaded. Thanks in advance. I trained another net in order to make file small enough to attach. So it's more or less dummy net only to reproduce problem.", "@muddham OK, i'll try and post results here in a day or two.", "@muddham Looks like downgrading to 1.12 helped. Thank you.", "@Knize As this is resolved. We will close this issue. Thanks!"]}, {"number": 28372, "title": "tensorflow2.0 InvalidArgumentError for multiple GPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows server 2016\r\n- **TensorFlow installed from (source or binary)**: pip install\r\n- **TensorFlow version (use command below)**: 2.0-alpha0\r\n- **Python version**: 3.6\r\n- **CUDA/cuDNN version**: cuda10.0; cudnn7.5 for cuda10.0\r\n- **GPU model and memory**: 4*1080Ti  each 11GB\r\n\r\n### Describe the problem\r\nI try to test the multiple GPU tutorials of tf2.0, both [keras](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/distribute/keras.ipynb) and [training loop](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/distribute/training_loops.ipynb) get the same error:\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by {{node training/Adam/NcclAllReduce}}with these attrs: [reduction=\"sum\", T=DT_FLOAT, num_devices=4, shared_name=\"c0\"]\r\n> Registered devices: [CPU, GPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[training/Adam/NcclAllReduce]] [Op:__inference_keras_scratch_graph_4244]\r\n\r\nall the code copy from the two link, and do not change one letter. It is a bug or I do something wrong?\r\n", "comments": ["Hi there, have you solved this ?", "> Hi there, have you solved this ?\r\n\r\nno, not even TF1.13, I guess the problem from the OS plateform.", "According to the guide, https://www.tensorflow.org/guide/distribute_strategy, MirroredStrategy defaults to using NCCL for cross device communication and NCCL is not available on Windows.\r\n\r\n\"If you wish to override the cross device communication, you can do so using the cross_device_ops argument by supplying an instance of tf.distribute.CrossDeviceOps. Currently we provide tf.distribute.HierarchicalCopyAllReduce and tf.distribute.ReductionToOneDevice as 2 other options other than tf.distribute.NcclAllReduce which is the default.\"", "> According to the guide, https://www.tensorflow.org/guide/distribute_strategy, MirroredStrategy defaults to using NCCL for cross device communication and NCCL is not available on Windows.\r\n> \r\n> \"If you wish to override the cross device communication, you can do so using the cross_device_ops argument by supplying an instance of tf.distribute.CrossDeviceOps. Currently we provide tf.distribute.HierarchicalCopyAllReduce and tf.distribute.ReductionToOneDevice as 2 other options other than tf.distribute.NcclAllReduce which is the default.\"\r\n\r\noh, thank u. I will try it", "hi there, after a day-long searching, I finally come out the idea. NCCL is a native library for GPUs communication which is only official supported for Linux. Thus here are two solutions:\r\n1. find windows non-official version of [NCCL](https://github.com/NVIDIA/nccl) (not recommend)\r\n2. As spatter's suggestion, custom reduction OP:[example](https://github.com/tensorflow/tensorflow/issues/21470)"]}, {"number": 28371, "title": "Suggestion for Neural Machine Translation with Attention tutorial", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:\r\n2.0\r\n- Doc Link:\r\nhttps://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention\r\n\r\n**Describe the documentation issue**\r\n\r\n1. max_length function is not needed. The keras.preprocessing.sequence.pad_sequences already pad the sequence to max length. One can just get max length by input_seqs.shape[1]\r\n2. need to explain why we skip 0 for \"convert\" and \"loss_function\". I think it is because of padding\r\n3. Validation set is not used.  This could be an good example to explain how we evaluate the model.\r\n4.  GRU unit in encoder,  Dense in Attention weight calculation,  GRU unit in decoder all have the same units parameter. This is not necessary.  A note could be added to explain that same unit is used for convenience.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n\r\nYes\r\n\r\n", "comments": ["Assigning to Mark who in charge of doc and colab. I think all the suggestions here make sense and would improve the readability for the colab. ", "These updates sound great. Can you please send a PR here (https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/text/nmt_with_attention.ipynb)?", "@jasonzhang2022 I think [this tutorial](https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention) was updated based on your suggestions. I am closing this issue as it was resolved. Please feel free to reopen if I am mistaken. Thanks!"]}, {"number": 28370, "title": "[ROCm] Fix for the broken `--config=rocm` build.", "body": "The --config=rocm build was broken by the following commit.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/084e14e7032f08810ee73b4bdc9b590cdcc0ef92\r\n\r\nThe changes made by the above commit did not contain the corresponding changes for the ROCm platform, which was leading to the build failure. Making the corresponding update for ROCm, to make the --config=rocm build working again.\r\n\r\n---------------------\r\n\r\n@tatianashp , @whchung, @timshen91 just FYI\r\n\r\nPlease approve and merge. As with other such PRs this week, the changes here are trivial and only applicable for the --config=rocm build.\r\n\r\nthanks", "comments": []}, {"number": 28369, "title": "UnicodeDecodeError during configure, compiling from source", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:1.14\r\n- Python version:3.5\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):0.21.1\r\n- GCC/Compiler version (if compiling from source):5.4.0\r\n- CUDA/cuDNN version: 10.0 / 7\r\n- GPU model and memory: GeForce GTX 1060 6GB\r\n\r\n\r\n**Describe the problem**\r\nWhen I run configure in the tensorflow directory, I get an error related to UnicodeDecodeError\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n./configure \r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.24.1 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /opt/ros/kinetic/lib/python2.7/dist-packages\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\n  /home/syha/syha_ws/devel/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/opt/ros/kinetic/lib/python2.7/dist-packages]\r\n/usr/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nTraceback (most recent call last):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 467, in <module>\r\n    main()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 459, in main\r\n    for key, value in sorted(find_cuda_config().items()):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 422, in find_cuda_config\r\n    _get_default_cuda_paths(cuda_version))\r\n  File \"third_party/gpus/find_cuda_config.py\", line 163, in _get_default_cuda_paths\r\n    ] + _get_ld_config_paths()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 143, in _get_ld_config_paths\r\n    match = pattern.match(line.decode(\"ascii\"))\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 15: ordinal not in range(128)\r\nAsking for detailed CUDA configuration...\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 10\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7\r\n\r\n\r\nPlease specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]: \r\n\r\n\r\nPlease specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: \r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 467, in <module>\r\n    main()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 459, in main\r\n    for key, value in sorted(find_cuda_config().items()):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 422, in find_cuda_config\r\n    _get_default_cuda_paths(cuda_version))\r\n  File \"third_party/gpus/find_cuda_config.py\", line 163, in _get_default_cuda_paths\r\n    ] + _get_ld_config_paths()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 143, in _get_ld_config_paths\r\n    match = pattern.match(line.decode(\"ascii\"))\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 15: ordinal not in range(128)\r\nAsking for detailed CUDA configuration...\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]:\r\n\r\n", "comments": ["It looks like this pull request was posted to fix this problem: https://github.com/tensorflow/tensorflow/pull/28209\r\n\r\nIt is not merged yet", "#28209 is now merged. So this should be fixed in master.\r\n", "Closing this issue since the associated PR has been merged. Feel free to reopen if the problem still persists. Thanks!", "@wdirons I used bazel-0.24 provided at https://public.dhe.ibm.com/ibmdl/export/pub/software/server/ibm-ai/conda/linux-ppc64le/ but #28209 fix didn't work for me and I am still getting:\r\n\r\n```\r\nCould not find any cuda.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\nof:\r\n        '/lib'\r\n        '/opt/ibm/spectrum_mpi/jsm_pmix/lib'\r\n        '/opt/ibm/spectrum_mpi/lib'\r\n        '/opt/ibutils/lib64'\r\n        '/opt/mellanox/hcoll/lib'\r\n        '/opt/mellanox/mxm/lib'\r\n        '/opt/mellanox/sharp/lib'\r\n        '/usr'\r\n        '/usr/lib64'\r\n        '/usr/lib64/mysql'\r\nAsking for detailed CUDA configuration...\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]:\r\n```", "@ghltshubh , Please open a new issue for this problem with the build issue template so I know what version of TensorFlow your trying to build. I don't think the problem you are seeing is related to this issue. Please tag me on the issue you create. Thank you."]}, {"number": 28368, "title": "Use atomic<bool> to guarantee visibility", "body": "For certain platforms with relaxed memory model, the updates to the bool array in the thread pool might not be immediately visible by the main thread.", "comments": []}, {"number": 28367, "title": "tf keras model.fit does not work with strings as labels", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy]**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf 1.12, '2.0.0-dev20190503'\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: not applicable\r\n- GPU model and memory: not applicable\r\n\r\n**Describe the current behavior**\r\nWhen using model.fit it throws an error:\r\n\r\n> tensorflow.python.framework.errors_impl.UnimplementedError: Cast string to float is not supported [Op:Cast] name: Cast/\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nThis should work the same as when using a lower level implementation with a GradientTape (working fine, no error)\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n    import tensorflow as tf\r\n    import numpy as np\r\n\r\n    images = np.random.rand(5, 108, 56, 3)\r\n    y_pred = np.random.rand(5, 4)\r\n    y_true = np.array(['aa', 'bb', 'cc', 'dd', 'ee'])\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices((images, y_true))\r\n    dataset = dataset.batch(5)\r\n    dataset = dataset.repeat()\r\n\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(16, [3,3], activation='relu'),\r\n        tf.keras.layers.GlobalAveragePooling2D(),\r\n        tf.keras.layers.Dense(4)\r\n    ])\r\n\r\n    def triplet_loss(y_true, y_pred):\r\n        all_diffs = tf.expand_dims(y_pred, axis=1) - tf.expand_dims(y_pred, axis=0)\r\n        distances = tf.sqrt(tf.reduce_sum(tf.square(all_diffs), axis=-1) + 1e-12)\r\n\r\n        furthest_positive = tf.reduce_max(distances, axis=1)\r\n\r\n        closest_negative = tf.map_fn(lambda x: tf.reduce_min(x),\r\n                                     distances)\r\n\r\n        diff = furthest_positive - closest_negative\r\n        diff = tf.nn.softplus(diff)\r\n\r\n        return diff\r\n\r\n    optimizer = tf.optimizers.Adam(learning_rate=0.001)\r\n\r\n    model.compile(loss=triplet_loss,\r\n                  optimizer=optimizer)\r\n\r\n    model.fit(dataset, steps_per_epoch=5, epochs=10, verbose=1)\r\n```\r\n\r\n\r\nWhen using a lower level implementation it works correctly:\r\n\r\n```python\r\nfor images, labels in dataset:\r\n    with tf.GradientTape() as tape:\r\n        y_pred = model(images, training=True)\r\n        loss_value = triplet_loss(labels, y_pred)\r\n\r\n        grads = tape.gradient(loss_value, model.trainable_variables)\r\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n        print('iteration done')\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@dmus This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "@muddham maybe this is a feature request instead of a bug. But because it works witu the more low level implementation I dont see a reason why it should not work with model.fit()", "I could reproduce the issue in TF2.0.0-alpha0. [Gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/49459479302f1c7d4aca06d3b9e4d3de/untitled158.ipynb) is here. Thanks!", "This is fixed TF 2.0 nightly '2.0.0-dev20190718'\r\nThanks!", "I am still getting this error with the same code snippet \r\n```\r\nIn [6]: tf.__version__                                                                                                                                                        \r\nOut[6]: '2.2.0'\r\n```\r\nError: \r\n```\r\nUnimplementedError:  Cast string to float is not supported\r\n\t [[node Cast (defined at <ipython-input-5-b46e9e31129b>:1) ]] [Op:__inference_train_function_978]\r\n```", "I think am getting the same error while using labels as strings running tf 2.2.0.  Int labels seem to work just fine:\r\n\r\nEpoch 1/5\r\n---------------------------------------------------------------------------\r\nUnimplementedError                        Traceback (most recent call last)\r\n<ipython-input-71-5aa7afde795f> in <module>()\r\n      1 model.fit(train_ds,\r\n      2           epochs=5,\r\n----> 3           steps_per_epoch = 3600//320)\r\n\r\n8 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    846                 batch_size=batch_size):\r\n    847               callbacks.on_train_batch_begin(step)\r\n--> 848               tmp_logs = train_function(iterator)\r\n    849               # Catch OutOfRangeError for Datasets of unknown size.\r\n    850               # This blocks until the batch has finished executing.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    578         xla_context.Exit()\r\n    579     else:\r\n--> 580       result = self._call(*args, **kwds)\r\n    581 \r\n    582     if tracing_count == self._get_tracing_count():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    609       # In this case we have created variables on the first call, so we run the\r\n    610       # defunned version which is guaranteed to never create variables.\r\n--> 611       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    612     elif self._stateful_fn is not None:\r\n    613       # Release the lock early so that multiple threads can perform the call\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2418     with self._lock:\r\n   2419       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2421 \r\n   2422   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n   1663          if isinstance(t, (ops.Tensor,\r\n   1664                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1665         self.captured_inputs)\r\n   1666 \r\n   1667   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1744       # No tape is watching; skip to running the function.\r\n   1745       return self._build_call_outputs(self._inference_function.call(\r\n-> 1746           ctx, args, cancellation_manager=cancellation_manager))\r\n   1747     forward_backward = self._select_forward_and_backward_functions(\r\n   1748         args,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    596               inputs=args,\r\n    597               attrs=attrs,\r\n--> 598               ctx=ctx)\r\n    599         else:\r\n    600           outputs = execute.execute_with_cancellation(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nUnimplementedError:  Cast string to float is not supported\r\n\t [[node Cast (defined at <ipython-input-44-42d40e8d46ca>:3) ]] [Op:__inference_train_function_1519]\r\n\r\nFunction call stack:\r\ntrain_function"]}, {"number": 28366, "title": "Error with custom ResourceBase", "body": "**My system information**\r\n- MacOS 10.12\r\n- Python 3.6.6\r\n- TensorFlow version: 1.13.1\r\n\r\n**Description:**\r\nWhen implementing a custom `ResourceBase` with the help of `tensorflow/core/framework/resource_op_kernel.h`, I get the following error\r\n\r\n        [F tensorflow/core/lib/core/refcount.h:90] Check failed: ref_.load() == 0 (1 vs. 0)\r\n        1]    29701 abort      python test.py\r\n\r\nHere is the full code to reproduce the issue:\r\n\r\n```cpp\r\nusing namespace tensorflow;\r\n\r\n/** CUSTOM RESOURCE **/\r\nclass MyVector : public ResourceBase {\r\n public:\r\n  string DebugString() override { return \"MyVector\"; };\r\n private:\r\n  std::vector<int> vec_;\r\n};\r\n\r\n/** CREATE VECTOR **/\r\nREGISTER_OP(\"CreateMyVector\")\r\n    .Attr(\"container: string = ''\")\r\n    .Attr(\"shared_name: string = ''\")\r\n    .Output(\"resource: resource\")\r\n    .SetIsStateful();\r\n\r\nclass MyVectorOp : public ResourceOpKernel<MyVector> {\r\n public:\r\n  explicit MyVectorOp(OpKernelConstruction* ctx) : ResourceOpKernel(ctx) {}\r\n\r\n private:\r\n  Status CreateResource(MyVector** resource) override {\r\n    *resource = CHECK_NOTNULL(new MyVector);\r\n    if(*resource == nullptr) {\r\n      return errors::ResourceExhausted(\"Failed to allocate\");\r\n    }\r\n    return Status::OK();\r\n  }\r\n\r\n  Status VerifyResource(MyVector* vec) override {\r\n    return Status::OK();\r\n  }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"CreateMyVector\").Device(DEVICE_CPU), MyVectorOp);\r\n```\r\n\r\nand then, after compiling, the error can be reproduced with this Python snippet of code:\r\n\r\n```python\r\ntest_module = tf.load_op_library('./test.so')\r\nmy_vec = test_module.create_my_vector()\r\nwith tf.Session() as s:\r\n  s.run(my_vec)\r\n```\r\n\r\nThanks a lot.\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this issue as it is not Build/Installation or Bug/Performance issue. Thanks!"]}, {"number": 28365, "title": "Retrain image detection with MobileNet for use in TFjs, failure in tensorflowjs_converter", "body": "It is unclear whether this is a documentation or a programming issue. If misfiled, please say so and suggest whether a new issue should be raised, or what else can be done.\r\n\r\n**System information**\r\n- TensorFlow version:\r\n ('v1.13.1-0-g6612da8951', '1.13.1') (see #27538)\r\n('v1.12.0-9492-g2c319fb415', '2.0.0-alpha0') (see #27539) and\r\ntf-nightly-2.0-preview (see https://stackoverflow.com/questions/55682557/)\r\n\r\n- Doc Link:\r\nhttps://www.tensorflow.org/hub/tutorials/image_retraining and\r\nhttps://www.tensorflow.org/tutorials/images/hub_with_keras\r\n\r\n**Describe the documentation issue**\r\nBoth documents mention but do not describe conversion to mobile models. Yet, conversion using `tensorflowjs_converter` failed in every attempt. See https://stackoverflow.com/questions/55849309/retrain-image-detection-with-mobilenet, which mentions two attempts by me as well as questions by others with the same problem.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n\r\nAs soon as it is working, I would gladly fix and/or augment the docs.  I have not gotten **`tensorflowjs_converter`** to **work on a retrained MobileNet model**, despite the docs stating that it should work, [e.g.](https://www.tensorflow.org/tutorials/images/hub_with_keras#export_your_model):\r\n\r\n> This saved model can loaded for inference later, or converted to TFLite or TFjs.", "comments": ["This issue is more suitable on tensorflow js repo. Please post it on [tensorflow js](https://github.com/tensorflow/tfjs). Thanks!", "@ymodak: It's now at https://github.com/tensorflow/tfjs/issues/1576. Thanks!"]}, {"number": 28364, "title": "An error ocurred while starting the kernel(tensorflow-gpu)", "body": "While using the keras to proceed the code suddenly\uff0cthe warining came out which are as follow\uff1a\r\n\r\nAn error ocurred while starting the kernel\r\n2019\udae1\udea9\udae1\udea7 16:08:03.433226: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1405] Found device 0 with properties: \r\nname: GeForce RTX 2060 major: 7 minor: 5 memoryClockRate(GHz): 1.2\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 6.00GiB freeMemory: 4.89GiB\r\n2019\udae1\udea9\udae1\udea7 16:08:03.435266: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2019\udae1\udea9\udae1\udea7 16:08:05.728803: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019\udae1\udea9\udae1\udea7 16:08:05.730056: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 0 \r\n2019\udae1\udea9\udae1\udea7 16:08:05.730321: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:984] 0: N \r\n2019\udae1\udea9\udae1\udea7 16:08:05.730723: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4644 MB memory) \u2011> physical GPU (device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\r\n\r\n**my system information**\r\n- win10\r\n- cuda9.2\r\n- cdunn7.2\r\n- python3.65\r\n- TensorFlow version :1.10.0\r\n- GPU/CPU:RTX2060,i7 9750h\r\n- memory:16g\r\n\r\nbut when i used tensorflow directly,without processing code by keras,there was no warning and competely successful.\r\nDespite all this,I still prefer keras rather than directly using tensorflow\uff0csomebody help me please XD.", "comments": ["The messages posted above all look like informational messages and not warnings. ", "> The messages posted above all look like informational messages and not warnings.\r\n\r\nThank you for your reply\uff0cAlthough it can not be considered as error or warning,it stuck in epoch1 and came out this message.", "@Oldhuntor This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "> @Oldhuntor This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n\r\nThank you for your reply,i'm going to ask question in that platform."]}, {"number": 28363, "title": "tf.image.resize_image_with_pad(image, 108, 56) gives 109 by 56 output for some images", "body": "[7953_4_3337_294_54_110.txt](https://github.com/tensorflow/tensorflow/files/3141289/7953_4_3337_294_54_110.txt)\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf 1.12, tf 1.13, tf-nightly\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: not applicable\r\n- GPU model and memory: not applicable\r\n\r\n**Describe the current behavior**\r\nFor some images, not for all tf.image.resize_image_with_pad(image, 108, 56) gives an output size of 109 by 56\r\n\r\n**Describe the expected behavior**\r\n tf.image.resize_image_with_pad(image, 108, 56) should always give an output of 108 by 56\r\n\r\n**Code to reproduce the issue**\r\n    `import tensorflow as tf \r\n    import pdb\r\n    import cv2\r\n    import base64\r\n\r\n    #tf.enable_eager_execution()\r\n\r\n    with tf.Session() as sess:\r\n        img_path = '7953_4_3337_294_54_110.txt'\r\n    \r\n        with open(img_path, 'r') as the_file:\r\n            image_string_base64 = the_file.read()\r\n\r\n        image_string = tf.decode_base64(image_string_base64)\r\n\r\n        image = tf.image.decode_png(image_string)\r\n        image = tf.image.resize_image_with_pad(image, 108, 56)\r\n\r\n        print(image)\r\n\r\n        img_array = sess.run(image)\r\n        print(img_array.shape)\r\n    \r\n        cv2.imwrite('test.png', cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))`\r\n\r\n**Other info / logs**\r\nWith eager execution it works correctly\r\n\r\n", "comments": ["It seems that tf.image.resize_image_with_pad gives slightly different results under eager execution than under graph mode", "@dmus Can you provide a code to reproduce the issue? Please use any public data if your data is private. Thanks!", "@jvishnuvardhan See here for a gist: https://colab.research.google.com/gist/dmus/edbed5154d07be197ced49e52ad3ea79/untitled0.ipynb", "@dmus This works for most of the other dimensions. I have changed `height` to 106,107, 109 etc and it worked as expected. When target height is 108, then this op produces an error as there are multiple calculations (for scaling image) when the code uses math_ops.floor function. Please take a look at the [source code](https://github.com/tensorflow/tensorflow/blob/a5c387b5ed78c9126424618d5c70f51cd3799857/tensorflow/python/ops/image_ops_impl.py#L1129) If you are interested. For temporary workaround is to use `image = tf.image.resize_image_with_crop_or_pad(image, 108, 56)`.\r\n\r\nI think this is a bug. Thanks for finding this.\r\n", "(Going through longstanding unresolved issues) \r\n\r\nThis should've been fixed starting tf2.0+. I've verified it and you can see the code snippet [here](https://colab.research.google.com/drive/1dkxgCqbwzd3RZgrYqRKWrLGD3zYHRGuh?usp=sharing).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28363\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28363\">No</a>\n"]}, {"number": 28362, "title": "AttributeError: 'tuple' object has no attribute 'ndims'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI am trying to run Virtual Batch Nomalization from [Here]( tensorflow/tensorflow/contrib/gan/python/features/python/virtual_batchnorm_impl.py ) with reference batch of size (10,50,50,1) of float32 type, which means that total 10 images of 50*50 type with channel=1. I write for this `S1=VBN(S)`. And getting error `AttributeError: 'tuple' object has no attribute 'ndims'`\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include minimal code snippet that can depict the issue. You can follow the [template](https://github.com/tensorflow/tensorflow/issues/new/choose) for the same.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@achandraa Hello. My os is Ubuntu(16.04, 64bit). Python version- 3.6.7, Tensorflow version- 1.13.1, keras-2.2.4, I install tensorflow from instruction `pip install tensorflow`. I use exactly same code as mention in my question for virtual batch normalization. ", "Tried checking your code snippet from the link provided but could not find it. Can you please help me to get the minimal code snippet that can depict the scenario. Thanks!", "https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/gan/python/features/python/virtual_batchnorm_impl.py", "@achandraa waiting for your valuable reply\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 28361, "title": "remove duplicate code", "body": "remove duplicate code", "comments": []}, {"number": 28360, "title": "update outdated doc", "body": "update outdated doc", "comments": []}, {"number": 28359, "title": "[LITE] INT16 type support for strided_slice op", "body": "", "comments": ["@siju-samuel thanks for your contribution , i see you have multiple PR , is it possible to merge all those here #28358,#28357,#28356,#28355,#28354\r\n", "Closing PR this as the changes have been moved here #28416"]}, {"number": 28358, "title": "[LITE] INT8 and INT16 types support for Tile op", "body": "", "comments": ["Closing PR this as the changes have been moved here #28416"]}, {"number": 28357, "title": "[LITE] INT16 support for Topk", "body": "", "comments": ["Closing PR this as the changes have been moved here #28416"]}, {"number": 28356, "title": "[LITE] INT16 support for transpose op", "body": "", "comments": ["Closing PR this as the changes have been moved here #28416"]}, {"number": 28355, "title": "[LITE]int8, uint8 & int16 type support for zeros_like", "body": "", "comments": ["Closing PR this as the changes have been moved here #28416"]}, {"number": 28354, "title": "[LITE]INT8 and INT32 support for split_v", "body": "", "comments": ["Closing PR this as the changes have been moved here #28416"]}, {"number": 28353, "title": "mAP increase but classification loss increase", "body": "mAP increase, but classification loss increase in object detection training. (by tensorflow object detection API)\r\nTrain data : 1500, Val data : 150\r\nClass : 1 (drone)\r\nI don't know what happen\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/3411873/57116734-11a21b00-6d92-11e9-867b-d10a7e16e52d.png)\r\n", "comments": ["It seems that you are using a two-stage detector such as [faster r-cnn](https://arxiv.org/pdf/1506.01497.pdf).It seems that the decline in RPN loss has a greater impact on detection performance.", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version and if possible provide us the code snippet to reproduce the issue at hand. If you are unclear what to include see the issue template displayed in the [Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}]