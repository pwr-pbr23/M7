[{"number": 15434, "title": "Revert \"Initial SRU Implementation (#13978)\"", "body": "This reverts commit e3e2ac9181c42eb82548726d8a250944b56180fd.", "comments": ["Should I submit the original PR again?", "Please do. I will add the API review tag to it.\r\nYou will also need to update the api_compatibility_test goldens."]}, {"number": 15433, "title": "[Feature Request] Automatic node placement", "body": "I read tensorflow white papaer and found node placement which allocates graph nodes to devices without manual configuration.\r\n\r\nhttps://www.reddit.com/r/MachineLearning/comments/4n6a0e/distributed_tensorflow_resource_allocation/\r\n\r\nHowever, this post says this feature was removed because it did not perform well.\r\n\r\nI think that automatic allocating graph nodes to devices for optimizing parallel execution is necessary to fully use distributed tensorflow. \r\n\r\nDo you have the plan to add this feature in the future tensorflow?\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Assignee @benoitsteiner: It has been 135 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "In progress."]}, {"number": 15432, "title": "[feature request] Switch to nvidia-docker v2?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.4.1\r\n- **Python version**: \r\n3.5.4\r\n- **Bazel version (if compiling from source)**:\r\nNone\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNone\r\n- **CUDA/cuDNN version**:\r\nNone\r\n- **GPU model and memory**:\r\nNone\r\n- **Exact command to reproduce**:\r\nNone\r\n\r\n### Describe the problem\r\nNow we are using nvidia-docker v1 for CI build. We should switch to v2 because v1 is now being deprecated. (https://github.com/NVIDIA/nvidia-docker/wiki/About-version-2.0) \r\n\r\n### Source code / logs\r\n\r\n", "comments": ["The tensorflow CI uses nvidia-docker v2 for a while now.\r\nIs there anything specific you saw that we missed?", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/ci_build.sh#L83\r\nIt still depends on nvidia-docker command? Instead of that, it should use:\r\n\r\n    docker run --runtime=nvidia ...\r\n\r\n\r\n", "`nvidia-docker` command is still available for backwards compatility with nvidia-docker2.\r\nThis script gets installed on your system once you install nvidia docker v2\r\nhttps://github.com/NVIDIA/nvidia-docker/blob/master/nvidia-docker"]}, {"number": 15431, "title": "Add shape function SingleImageRandomDotStereograms", "body": "This fix tries to address the issue raised in #15429 where there is no shape function for SingleImageRandomDotStereograms.\r\n\r\nThis fix adds the shape function for `SingleImageRandomDotStereograms`.\r\n\r\nNOTE: `SingleImageRandomDotStereograms` takes an attribute of `output_image_shape` which is in the format of `[X, Y, C]` (`[ImageX, ImageY, Channel]`. However, the actual\r\ndata output is in the format of `[ImageY, ImageX, Channel]` (`[h, w, c]`). So by default the output_image_shape has the value of [1024, 768, 1] but the output data will be [768, 1024, 1].\r\nAnd if `[1200, 800, 1]` is used explicitly then the output data shape will be `[800, 1200, 1]`.\r\n\r\nThis fix does not change the behavior for now.\r\n\r\nThis fix fixes #15429.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Jenkins, test this please."]}, {"number": 15430, "title": "Placeholders should have a feeding schedule in TensorFlow?", "body": "I have a parameter which should set **at the beginning** of each epoch and it is **constant** during the epoch's execution. Currently, we defined a placeholder for this param and during the training I have to pass the same value in each iteration:\r\n\r\n`sess.run(train_op, feed_dict={param: const_param})`\r\n\r\nIt's a bit inefficient since pass the same value at each step of the same epoch. Is it possible to define a placeholder and feed it once just at the beginning of each epoch?\r\n\r\nThis feature is available in _tf.data_ when set the tfrecords file at the beginning and fetch data iteratively.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Do not use placeholder in this case. You can just define it as a variable and use `var.load(value)` at the beginning of each epoch.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15429, "title": "RuntimeError: No C++ shape function registered for standard op: SingleImageRandomDotStereograms", "body": "single_image_random_dot_stereograms (windows bug)\r\nthe function works fine on ubuntu 16.04 / tf 1.4.0 (CPU)\r\n\r\nI know, that the support for unsupported libraries in tf.contrib on Windows is incomplete, and left up to the individual contributors. I found some sketchy solutions by editing gen_single_image_random_dot_stereograms_ops.py, but i could not figure it out correctly.\r\n\r\n\r\n**Source code**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.image import single_image_random_dot_stereograms\r\nimg=[[1,2,3,3,2,1],\r\n     [1,2,3,4,5,2],\r\n     [1,2,3,4,5,3],\r\n     [1,2,3,4,5,4],\r\n     [6,5,4,4,5,5]]\r\nsession = tf.InteractiveSession()\r\nsirds = single_image_random_dot_stereograms(\r\n    img,\r\n    convergence_dots_size=8,\r\n    number_colors=256,normalize=True)\r\nout = sirds.eval()\r\npng = tf.image.encode_png(out).eval()\r\nwith open('picture_out.png', 'wb') as f:\r\n  f.write(png)\r\n```\r\n\r\n**Error**\r\nWindows 10 Pro, 1709\r\nconda version : 4.3.30\r\npython version: 3.6.3\r\ntensorflow-gpu 1.4.0\r\n\r\n```\r\nFile \"C:\\Users\\###\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\", line 696, in _call_cpp_shape_fn_impl\r\n    \"No C++ shape function registered for standard op: %s\" % op.type)\r\nRuntimeError: No C++ shape function registered for standard op: SingleImageRandomDotStereograms\r\n```\r\n\r\nWindows 10 Pro, 1709\r\nconda version : 4.3.30\r\npython version: 3.5.4\r\ntensorflow 1.3.0/tensorflow 1.2.0\r\n\r\n`AttributeError: 'NoneType' object has no attribute 'single_image_random_dot_stereograms`\r\n", "comments": ["The `SingleImageRandomDotStereograms` does not have a shape function I think that is the issue.\r\n\r\nI added a PR #15431 with the shape function added.\r\n\r\nNote`SingleImageRandomDotStereograms` takes an attribute of `output_image_shape` which is in the format of `[X, Y, C]` (`[ImageX, ImageY, Channel]`).\r\n\r\nHowever, the actual data output is in the format of `[ImageY, ImageX, Channel]` (`[h, w, c]`). So by default the output_image_shape has the value of [1024, 768, 1] but the output data will be [768, 1024, 1].\r\n\r\nDon't know if we want to change the behavior or not, so I leave the behavior alone for now.\r\n"]}, {"number": 15428, "title": "[BUG] Cifar10 mutigpu loss does not decrease and is oscillating ", "body": "Hi\r\nI want to develope a resnet50 multigpu system in tensorflow,\r\nso I want to replace my model in [**cifar10_multigpu_train**](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py)\r\nbut when I run this code, loss does not decrease efficiently,until step 5000\r\nmy question is : can I trust this code? Is  the code correct?\r\n**I use:**\r\nubuntu 16.04\r\ninstalled tensorflow with pip2\r\ntensorflow 1.4\r\ncudnn       6 \r\ncuda     8.0.44\r\nGtx 1080 (2 gpus/8g memory)\r\n\r\ncommand to produce:\r\ngo to the above link,download **cifar10_multi_gpu_train.py** and run it \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 15427, "title": "Fix failing test //tensorflow/python:function_test", "body": "@benoitsteiner It seems `//tensorflow/python:function_test` is failing with my last commit from #13998 (sorry about that).\r\n\r\nIt looks like the `function_test` created a graph with python through a helper class and invoked tests through c api.\r\n\r\nI am not familiar with the `function_test`, though as `ClipByValue` is actually hidden, and is exposed through python only, with the gradient defined in python as well, I think replacing it will fix the issue?\r\n\r\nPlease take a look. Again, really sorry for the caused inconvenience.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["/cc @gunan @caisq as well.", "Looks like the `//tensorflow/tools/api/tests:api_compatibility_test` is also failing, caused by\r\n```\r\nERROR:tensorflow:2 differences found between API and golden.\r\nERROR:tensorflow:Issue 1        : New object tensorflow.nn.rnn_cell.SRUCell found (added).\r\nERROR:tensorflow:Issue 2        : Change detected in python object: tensorflow.nn.rnn_cell.\r\n```\r\n\r\nThe issue is from another PR #13978 which introduced the SRU implementation.\r\n\r\nIt seems that `tensorflow/core/api_def/update_api_def.sh` only adds the ops in c++ but does not update in golden.\r\n\r\nI run with\r\n```\r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n```\r\n\r\nand the issue seems to be fixed.\r\n\r\nPushed though to the PR, though not sure this is the right fix as I am not familiar with the API changes.\r\n", "@yongtang thanks, I was not aware of the script for API update. Sorry for your trouble.", "@martinwicke @ebrevdo, FYI: PR https://github.com/tensorflow/tensorflow/pull/13978 involved API changes, but was merged without an \"API review\" tag. This led to API compatibility test failures that @yongtang fixed in this PR. Please see whether it is okay to approve this PR or revert https://github.com/tensorflow/tensorflow/pull/13978 first and roll it forward later after the proper API change review process.", "A PR #15455 has been created to re-address `clip_by_value`. I will close this PR. Thanks all for the help!"]}, {"number": 15426, "title": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for          [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]", "body": "I am working on a TensorFlow Speech Recognition challenge and following https://www.tensorflow.org/tutorials/audio_recognition tutorial. The model training is completed, but I'm not able to Freeze the model.\r\nThis is what I get after typing the required command:\r\n\r\n```\r\n`C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\site-packages\\tensorflow\\examples\\speech_commands>python freeze.py \\\r\n2017-12-17 01:43:38.739183: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2017-12-17 01:43:39.455821: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 1.2415\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.36GiB\r\n2017-12-17 01:43:39.455957: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:03:00.0, compute capability: 5.0)\r\n2017-12-17 01:43:39.580220: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n2017-12-17 01:43:39.588122: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n2017-12-17 01:43:39.596502: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n2017-12-17 01:43:39.601314: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n2017-12-17 01:43:39.607839: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n2017-12-17 01:43:39.613370: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n         [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]\r\n         [[Node: save/RestoreV2_4/_3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_18_save/RestoreV2_4\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"freeze.py\", line 180, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"freeze.py\", line 117, in main\r\n    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\site-packages\\tensorflow\\examples\\speech_commands\\models.py\", line 123, in load_variables_from_checkpoint\r\n    saver.restore(sess, start_checkpoint)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1666, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n         [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]\r\n         [[Node: save/RestoreV2_4/_3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_18_save/RestoreV2_4\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nCaused by op 'save/RestoreV2_1', defined at:\r\n  File \"freeze.py\", line 180, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"freeze.py\", line 117, in main\r\n    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\site-packages\\tensorflow\\examples\\speech_commands\\models.py\", line 122, in load_variables_from_checkpoint\r\n    saver = tf.train.Saver(tf.global_variables())\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1218, in __init__\r\n    self.build()\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1227, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1263, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 751, in _build_internal\r\n    restore_sequentially, reshape)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 427, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 267, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1020, in restore_v2\r\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\VinithaNair\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n         [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]\r\n         [[Node: save/RestoreV2_4/_3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_18_save/RestoreV2_4\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]`\r\n```\r\nOS - Windows 10\r\nTensorFlow version - 1.4\r\nPython version - 3.6.3\r\nGPU - CUDA V 8 and cuDNN V 6.0\r\n\r\nI came across [https://github.com/tensorflow/tensorflow/issues/6082] and [https://github.com/tensorflow/tensorflow/issues/7547](url) where the suggested fix was to add \"./\" to the model name. But, in this case I'm not able find the list of codes where I'm supposed to make the change. How do I find the code that needs the fix? Or is there another issue that I'm unaware of?\r\n\r\nPlease help.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Hey, the problem has been sorted. Thank you!", "Hey there,\r\n\r\nI seem to the exact same problem. From what I have read, I can see that I need to add a ./ somehwhere in the code. Not sure where though.\r\n\r\nI am using a Windows 10 x64\r\nTensorflow was installed using the command pip3 install --upgrade tensorflow after installing python.\r\n\r\nThe training of the model has been done. And I have the following issue while trying to freeze the model:\r\nThe command I typed was: \r\n\r\n`_python  \"C:\\Program Files\\python35\\Lib\\site-packages\\tensorflow\\examples\\speech_commands/freeze.py\"_`\r\n\r\nThe result I got was:\r\n\r\n```\r\n2018-06-22 16:45:52.269435: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-06-22 16:45:54.710014: W T:\\src\\github\\tensorflow\\tensorflow\\core\\framework\\op_kernel.cc:1318] OP_REQUIRES failed at save_restore_tensor.cc:170 : Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\python35\\Lib\\site-packages\\tensorflow\\examples\\speech_commands/freeze.py\", line 180, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"C:\\Program Files\\python35\\Lib\\site-packages\\tensorflow\\examples\\speech_commands/freeze.py\", line 117, in main\r\n    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)\r\n  File \"C:\\Program Files\\python35\\Lib\\site-packages\\tensorflow\\examples\\speech_commands\\models.py\", line 123, in load_variables_from_checkpoint\r\n    saver.restore(sess, start_checkpoint)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1802, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\r\nCaused by op 'save/RestoreV2', defined at:\r\n  File \"C:\\Program Files\\python35\\Lib\\site-packages\\tensorflow\\examples\\speech_commands/freeze.py\", line 180, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"C:\\Program Files\\python35\\Lib\\site-packages\\tensorflow\\examples\\speech_commands/freeze.py\", line 117, in main\r\n    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)\r\n  File \"C:\\Program Files\\python35\\Lib\\site-packages\\tensorflow\\examples\\speech_commands\\models.py\", line 122, in load_variables_from_checkpoint\r\n    saver = tf.train.Saver(tf.global_variables())\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1338, in __init__\r\n    self.build()\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1347, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1384, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 835, in _build_internal\r\n    restore_sequentially, reshape)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 472, in _AddRestoreOps\r\n    restore_sequentially)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 886, in bulk_restore\r\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1546, in restore_v2\r\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n```\r\n\r\n\r\n\r\nWould be great if you could let us know how it worked out for you. \r\n\r\nThanks a lot,"]}, {"number": 15425, "title": "TypeError: broadcast() takes 1 positional argument but 2 were given", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**: b'v1.3.0-rc1-6044-g0b80606' 1.4.0    (2 days ago)\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Source code / logs\r\nI ran `tensorflow/benchmarks`, and got the following error.\r\n```\r\nTraceback (most recent call last):\r\n  File \"tf_cnn_benchmarks.py\", line 47, in <module>\r\n    tf.app.run()\r\n  File \"/MYHOME/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 124, in run\r\n    _sys.exit(main(argv))\r\n  File \"tf_cnn_benchmarks.py\", line 43, in main\r\n    bench.run()\r\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 956, in run\r\n    return self._benchmark_cnn()\r\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1046, in _benchmark_cnn\r\n    self._build_model_single_session())\r\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1563, in _build_model_single_session                                                                      all_top_5_ops, phase_train)\r\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1370, in _build_fetches\r\n    self.variable_mgr.preprocess_device_grads(device_grads))\r\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/variable_mgr.py\", line 386, in preprocess_device_grads\r\n    agg_small_grads_max_group=self._agg_small_grads_max_group)\r\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 332, in sum_gradients_all_reduce\r\n    if is_hierarchical else aux_device_groups[group_index], num_shards))\r\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 236, in sum_grad_and_var_all_reduce\r\n    tf.add)\r\n  File \"/MYHOME/.local/lib/python3.6/site-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 780, in build_nccl_then_ring\r\n    return _build_nccl_hybrid(input_tensors, red_op, upper_level_f)\r\n  File \"/MYHOME/.local/lib/python3.6/site-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 748, in _build_nccl_hybrid\r\n    send_op, dst_tensors = nccl.broadcast(level_2_output[w], dst_devices)\r\nTypeError: broadcast() takes 1 positional argument but 2 were given\r\n```\r\n\r\nThe reason is that the invocation of `nccl.broadcast` is different from its signature:\r\nhttps://github.com/tensorflow/tensorflow/blob/17e725c0558581cba19bd6c409698b2c3f88efe5/tensorflow/contrib/all_reduce/python/all_reduce.py#L748\r\nhttps://github.com/tensorflow/tensorflow/blob/17e725c0558581cba19bd6c409698b2c3f88efe5/tensorflow/contrib/nccl/python/ops/nccl_ops.py#L173-L182 \r\nProblems still exists in current HEAD.", "comments": ["@chsigg contrib/all_reduce needs to be updated due to the contrib/nccl API change made in https://github.com/tensorflow/tensorflow/commit/ca8af1d0dbb605087a4f8ae076188f2b9a26b1ba which broke some benchmarks. cc: @poxvoculi", "#15642 same problem", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "nccl.broadcast got changed without updating its uses.  The immediate problem has been fixed in the internal build and should roll out with the next update.", "@poxvoculi  thanks very much!\r\nhow do i get this fix now?  \r\none more question: **contrib/all_reduce** not support embedding lookup that has IndexedSlice gradients, can you give some advice?", "The fix is easy, you can patch your own client now if you're eager.\r\n\r\nIn all_reduce.py remove these lines\r\n\r\n      dst_devices = per_worker_devices[w][1:]\r\n      send_op, dst_tensors = nccl.broadcast(level_2_output[w], dst_devices)\r\n      # NOTE: need control dependency to ensure send_op executes\r\n      with ops.control_dependencies([send_op]):\r\n        with ops.device(per_worker_devices[w][0]):\r\n          dst_tensors.insert(0, array_ops.identity(level_2_output[w]))\r\n          down_values[w] = dst_tensors\r\n\r\nand replace with these lines:\r\n\r\n\r\n      dst_tensors = []\r\n      with ops.device(per_worker_devices[w][0]):\r\n        broadcast_src = nccl.broadcast(array_ops.identity(level_2_output[w]))\r\n      for d in per_worker_devices[w]:\r\n        with ops.device(d):\r\n          dst_tensors.append(array_ops.identity(broadcast_src))\r\n      down_values[w] = dst_tensors\r\n\r\nI'm not expert at NCCL, but I wouldn't be surprised if it only works on dense tensors of numeric types.  IndexedSlices are a sparse representation, right?  ", "@poxvoculi thanks, i had already tried same fix with you before this issue. \r\nyesterday i tried your code, but failed again at same error: (use tf_benchmark: \r\nvariable_update: distributed_all_reduce\r\nall_reduce_spec: nccl/xring) \r\n\r\nUnimplementedError (see above for traceback): This op should be replaced during graph optimization. \r\ncaused by \r\nFile python2.7/site-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 781, in build_nccl_then_ring \r\nreturn _build_nccl_hybrid(input_tensors, red_op, upper_level_f) \r\nFile \"python2.7/site-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 749, in _build_nccl_hybrid \r\nbroadcast_src = nccl.broadcast(array_ops.identity(level_2_output[w])) \r\nFile \"python2.7/site-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py\", line 187, in broadcast \r\nreturn gen_nccl_ops.nccl_broadcast(input=tensor, shape=tensor.shape) \r\nFile \"python2.7/site-packages/tensorflow/contrib/nccl/ops/gen_nccl_ops.py\", line 98, in nccl_broadcast \r\n\"NcclBroadcast\", input=input, shape=shape, name=name) \r\n\r\n**which i guess rewrite graph maybe happen in nccl send/recv op.**\r\n\r\n**IndexedSlices are a sparse representation, right?**\r\nYes,  excuse me for that i sayed that nccl in the issue above, actually it's contrib/all_reduce module, i fix the issue content error now, thanks.", "Hi All, I meet the same issue.\r\nNeed your help. Thanks.", "@Agoniii  can you post detail? thanks", "@anpark  I run tf_cnn_benchmarks with the commands `--variable_update=distributed_all_reduce --all_reduce_spec nccl/xring`. My problem is almost as same as yours.\r\n```\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1545, in _build_fetches\r\n    self.variable_mgr.preprocess_device_grads(device_grads))\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/variable_mgr.py\", line 386, in preprocess_device_grads\r\n    agg_small_grads_max_group=self._agg_small_grads_max_group)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 332, in sum_gradients_all_reduce\r\n    if is_hierarchical else aux_device_groups[group_index], num_shards))\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 236, in sum_grad_and_var_all_reduce\r\n    tf.add)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 780, in build_nccl_then_ring\r\n    return _build_nccl_hybrid(input_tensors, red_op, upper_level_f)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 749, in _build_nccl_hybrid\r\n    broadcast_src = nccl.broadcast(array_ops.identity(level_2_output[w]))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py\", line 187, in broadcast\r\n    return gen_nccl_ops.nccl_broadcast(input=tensor, shape=tensor.shape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/nccl/ops/gen_nccl_ops.py\", line 98, in nccl_broadcast\r\n    \"NcclBroadcast\", input=input, shape=shape, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3172, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1617, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nUnimplementedError (see above for traceback): This op should be replaced during graph optimization.\r\n\t [[Node: allreduce_160/NcclBroadcast = NcclBroadcast[T=DT_FLOAT, shape=[1001], _device=\"/job:worker/replica:0/task:0/device:GPU:0\"](allreduce_160/Slice)]]\r\n\t [[Node: group_deps_3_G19865 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device_incarnation=-2613134839557745882, tensor_name=\"edge_55804_group_deps_3\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/device:CPU:0\"]()]]\r\n```", "I've run into this UnimplementedError as well with 1.5. A small repro:\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.nccl as nccl\r\n\r\nif __name__ == '__main__':\r\n    devices = ['/gpu:0', '/gpu:0']\r\n    with tf.device(devices[0]):\r\n        data0 = tf.constant([1.1, 2.2, 3.3, 4.4])\r\n        received_tensor = nccl.broadcast(data0)\r\n    received_tensors = []\r\n    for device in devices[1:]:\r\n        with tf.device(device):\r\n            received_tensors.append(tf.identity(received_tensor))\r\n    sess = tf.Session()\r\n    sess.run(received_tensors)\r\n```\r\n\r\nIt looks like the registration of the optimization pass at this line never happens:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/bf1fad214febef6af5c101d8f953d0109c46dfbb/tensorflow/contrib/nccl/kernels/nccl_rewrite.cc#L270\r\n\r\nThis may be because _nccl_ops.so doesn't include kernels/nccl_rewrite.cc:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/bf1fad214febef6af5c101d8f953d0109c46dfbb/tensorflow/contrib/nccl/BUILD#L24\r\n\r\nUnfortunately Bazel throws a fit when I try to add the source file to this target.", "@chsigg can you clarify as to whether this test program is using contrib/nccl in the anticipated way?", "@benbarsdell Great!\r\n i add the same issue in tensorflow/benchmarks repo, but @tfboyd  closed it and ask me to  run with tf1.5.\r\nHope anyone can help!", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @chsigg, @poxvoculi: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @chsigg, @poxvoculi: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Reassigning to @zheng-xq ", "Seems to be fixed already.", "@ppwwyyxx  which version do you use?  Have you tested it ok?", "There might be other issues, but at least my original one seems fixed. As I can run tensorflow/benchmarks with TF1.6.", " failed again at same error: (use tf_benchmark:\r\nvariable_update: distributed_all_reduce\r\nall_reduce_spec: nccl/xring)\r\n\r\nTF1.6 \r\n\r\n@ppwwyyxx  what's your params?"]}, {"number": 15424, "title": "\"Not a valid TensorFlow Graph serialization\" at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph.loadGraph", "body": "Hi,\r\nI build libtensorflow_inference.so & libandroid_tensorflow_inference_java.jar from source, then use them in an android studio project. But when I run\r\nTensorFlowInferenceInterface tflite = new TensorFlowInferenceInterface(assetManager, MODEL_PATH);\r\nthe program always crashes and prints\r\n\"\r\nCaused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Invalid GraphDef\r\nat org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:551)\r\nat org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:105)\r\n......\r\n\"\r\nThe tflite file in MODEL_PATH is generated successfully by bazel toco from a model graph, and the tflite file is successfully read into byte array. Are there any special restrictions on the graph? What are the possible reasons why function TensorFlowInferenceInterface.loadGraph() throws an Exception?\r\n\r\nThanks a lot.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce"]}, {"number": 15423, "title": "Can't build with basel Python Configuration Error: --define PYTHON_BIN_PATH", "body": "Can't get what im doing wrong.\r\nWin7 python 3.6, tensorflow from master, cuda 9.0, cudnn 7.0.5 for cuda 9.0, basel and swig loaded today \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/12052\r\n\r\n> C:\\Users\\Andrey\\Desktop\\tensorflow>bazel clean\r\n> ...........\r\n> INFO: Starting clean (this may take a while). Consider using --async if the clea\r\n> n takes more than several minutes.\r\n> \r\n> C:\\Users\\Andrey\\Desktop\\tensorflow>python configure.py\r\n> WARNING: Running Bazel server needs to be killed, because the startup options ar\r\n> e different.\r\n> You have bazel 0.8.1 installed.\r\n> Please specify the location of python. [Default is C:\\Users\\Andrey\\Anaconda3\\pyt\r\n> hon.exe]:\r\n> \r\n> \r\n> Found possible Python library paths:\r\n>   C:\\Users\\Andrey\\Anaconda3\\lib\\site-packages\r\n> Please input the desired Python library path to use.  Default is [C:\\Users\\Andre\r\n> y\\Anaconda3\\lib\\site-packages]\r\n> \r\n> Do you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\n> XLA JIT support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with GDR support? [y/N]: y\r\n> GDR support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with VERBS support? [y/N]: y\r\n> VERBS support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with CUDA support? [y/N]: y\r\n> CUDA support will be enabled for TensorFlow.\r\n> \r\n> Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to d\r\n> efault to CUDA 9.0]:\r\n> \r\n> \r\n> Please specify the location where CUDA 9.0 toolkit is installed. Refer to README\r\n> .md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/\r\n> CUDA/v9.0]:\r\n> \r\n> \r\n> Please specify the cuDNN version you want to use. [Leave empty to default to cuD\r\n> NN 7.0]:\r\n> \r\n> \r\n> Please specify the location where cuDNN 7 library is installed. Refer to README.\r\n> md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/C\r\n> UDA/v9.0]:\r\n> \r\n> \r\n> Please specify a list of comma-separated Cuda compute capabilities you want to b\r\n> uild with.\r\n> You can find the compute capability of your device at: https://developer.nvidia.\r\n> com/cuda-gpus.\r\n> Please note that each additional compute capability significantly increases your\r\n>  build time and binary size. [Default is: 3.5,5.2]\r\n> \r\n> \r\n> Do you wish to build TensorFlow with MPI support? [y/N]: n\r\n> No MPI support will be enabled for TensorFlow.\r\n> \r\n> Please specify optimization flags to use during compilation when bazel option \"-\r\n> -config=opt\" is specified [Default is -march=native]:\r\n> \r\n> \r\n> Add \"--config=mkl\" to your bazel command to build with MKL support.\r\n> Please note that MKL on MacOS or windows is still not supported.\r\n> If you would like to use a local MKL instead of downloading, please set the envi\r\n> ronment variable \"TF_MKL_ROOT\" every time before build.\r\n> \r\n> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\n>  n\r\n> Not configuring the WORKSPACE for Android builds.\r\n> \r\n> \r\n> C:\\Users\\Andrey\\Desktop\\tensorflow>bazel build --config=opt --config=win-cuda //\r\n> tensorflow/tools/pip_package:build_pip_package\r\n> ...........\r\n> Loading:\r\n> Loading: 0 packages loaded\r\n> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (3 packages l\r\n> oaded)\r\n> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (24 packages\r\n> loaded)\r\n> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (34 packages\r\n> loaded)\r\n> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (72 packages\r\n> loaded)\r\n> ERROR: C:/users/andrey/desktop/tensorflow/third_party/py/numpy/BUILD:11:1: no su\r\n> ch package '@local_config_python//': Traceback (most recent call last):\r\n>         File \"C:/users/andrey/desktop/tensorflow/third_party/py/python_configure\r\n> .bzl\", line 291\r\n>                 _create_local_python_repository(repository_ctx)\r\n>         File \"C:/users/andrey/desktop/tensorflow/third_party/py/python_configure\r\n> .bzl\", line 251, in _create_local_python_repository\r\n>                 _check_python_bin(repository_ctx, python_bin)\r\n>         File \"C:/users/andrey/desktop/tensorflow/third_party/py/python_configure\r\n> .bzl\", line 204, in _check_python_bin\r\n>                 _fail((\"--define %s='%s' is not execut...)))\r\n>         File \"C:/users/andrey/desktop/tensorflow/third_party/py/python_configure\r\n> .bzl\", line 27, in _fail\r\n>                 fail((\"%sPython Configuration Error:%...)))\r\n> Python Configuration Error: --define PYTHON_BIN_PATH='C:/Users/Andrey/Anaconda3/\r\n> python.exe' is not executable. Is it the python binary?\r\n>  and referenced by '//third_party/py/numpy:headers'\r\n> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (72 packages\r\n> loaded)\r\n> ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' fai\r\n> led; build aborted: Loading failed\r\n> INFO: Elapsed time: 6,710s\r\n> FAILED: Build did NOT complete successfully (72 packages loaded)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I filled.", "If i remove check for python and python libs paths, i get this\r\nMay be problem with slash?\r\n\r\n> C:\\Users\\Andrey\\Desktop\\tensorflow\\tensorflow>bazel build --config=win-cuda //te\r\n> nsorflow/tools/pip_package:build_pip_package\r\n> Loading:\r\n> Loading: 0 packages loaded\r\n> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages l\r\n> oaded)\r\n> WARNING: C:/users/andrey/desktop/tensorflow/tensorflow/tensorflow/core/BUILD:180\r\n> 6:1: in includes attribute of cc_library rule //tensorflow/core:framework_header\r\n> s_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not bel\r\n> ow the relative path of its package 'tensorflow/core'. This will be an error in\r\n> the future. Since this rule was created by the macro 'cc_header_only_library', t\r\n> he error might have been caused by the macro implementation in C:/users/andrey/d\r\n> esktop/tensorflow/tensorflow/tensorflow/tensorflow.bzl:1131:30\r\n> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (65 packages\r\n> loaded)\r\n> INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (70 packa\r\n> ges loaded).\r\n> INFO: Found 1 target...\r\n> Building: no action running\r\n> [0 / 4] BazelWorkspaceStatusAction stable-status.txt\r\n> ERROR: C:/users/andrey/appdata/local/temp/_bazel_andrey/ghemteso/external/protob\r\n> uf_archive/BUILD:656:1: C++ compilation of rule '@protobuf_archive//:python/goog\r\n> le/protobuf/internal/_api_implementation.so' failed (Exit 1): cl.exe failed: err\r\n> or executing command\r\n>   cd C:/users/andrey/appdata/local/temp/_bazel_andrey/ghemteso/execroot/org_tens\r\n> orflow\r\n>   SET CUDA_COMPUTE_CAPABILITIE=None\r\n>     SET CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n>     SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.\r\n> 0\r\n>     SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9\r\n> .0\r\n>     SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C\r\n> :\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program\r\n> Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\ucrt;C:\\Program Files (x86)\\Win\r\n> dows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\inclu\r\n> de\\10.0.14393.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393\r\n> .0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\winrt;\r\n>     SET LD_LIBRARY_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\\r\n> lib64\\\r\n>     SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\\r\n> Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program\r\n> Files (x86)\\Windows Kits\\10\\lib\\10.0.14393.0\\ucrt\\x64;C:\\Program Files (x86)\\Win\r\n> dows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\1\r\n> 0.0.14393.0\\um\\x64;\r\n>     SET NO_WHOLE_ARCHIVE_OPTION=1\r\n>     SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\Com\r\n> monExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studi\r\n> o 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program F\r\n> iles (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Mic\r\n> rosoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual St\r\n> udio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team\r\n>  Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\r\n> \\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\\r\n> Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDK\r\n> s\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\Windows\\system32\r\n>     SET PWD=/proc/self/cwd\r\n>     SET PYTHON_BIN_PATH=C:/Users/Andrey/Anaconda3/python.exe\r\n>     SET PYTHON_LIB_PATH=C:/Users/Andrey/Anaconda3/lib/site-packages\r\n>     SET TEMP=C:\\Users\\Andrey\\AppData\\Local\\Temp\r\n>     SET TF_CUDA_CLANG=0\r\n>     SET TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2\r\n>     SET TF_CUDA_VERSION=9.0\r\n>     SET TF_CUDNN_VERSION=7\r\n>     SET TF_NEED_CUDA=1\r\n>     SET TF_NEED_OPENCL_SYCL=0\r\n>     SET TMP=C:\\Users\\Andrey\\AppData\\Local\\Temp\r\n>   C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /c ext\r\n> ernal/protobuf_archive/python/google/protobuf/internal/api_implementation.cc /Fo\r\n> bazel-out/x64_windows-py3-opt/bin/external/protobuf_archive/_objs/python/google/\r\n> protobuf/internal/_api_implementation.so/external/protobuf_archive/python/google\r\n> /protobuf/internal/api_implementation.o /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WI\r\n> N32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE\r\n> _STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd429\r\n> 1 /wd4250 /wd4996 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -w /Iexternal/protobuf_a\r\n> rchive /Ibazel-out/x64_windows-py3-opt/genfiles/external/protobuf_archive /Iexte\r\n> rnal/local_config_python /Ibazel-out/x64_windows-py3-opt/genfiles/external/local\r\n> _config_python /Iexternal/bazel_tools /Ibazel-out/x64_windows-py3-opt/genfiles/e\r\n> xternal/bazel_tools /Iexternal/local_config_python/python_include /Ibazel-out/x6\r\n> 4_windows-py3-opt/genfiles/external/local_config_python/python_include /Iexterna\r\n> l/bazel_tools/tools/cpp/gcc3 /showIncludes /MD /O2 /DHAVE_PTHREAD /wd4018 /wd451\r\n> 4 -DPYTHON_PROTO2_CPP_IMPL_V2\r\n> C:\\users\\andrey\\appdata\\local\\temp\\_bazel_andrey\\ghemteso\\execroot\\org_tensorflo\r\n> w\\external\\protobuf_archive\\python\\google\\protobuf\\internal\\api_implementation.c\r\n> c : fatal error C1083: Cannot open compiler generated file: '': Invalid argument\r\n> \r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> INFO: Elapsed time: 6,208s, Critical Path: 0,65s\r\n> FAILED: Build did NOT complete successfully", "@meteorcloudy could you take a look?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Was this resolved?\r\nWhat was the resolution? ", "@goydex The TF Bazel build with GPU support doesn't work currently on Windows. But the build with CPU support works well.\r\nYou can try to run this script:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/cpu/pip/build_tf_windows.sh", "Thanks. We need XLA enabled and AFAIK we need Bazel to build that. If theres no way to build Windows + GPU + XLA then we\u2019ll have to switch to developing on Linux.\n\nWhich IDE offers interactive debug and other good debug tools in Linux? \n\n\n> On Jun 6, 2018, at 1:26 AM, Yun Peng <notifications@github.com> wrote:\n> \n> @goydex The TF Bazel build with GPU support doesn't work currently on Windows. But the build with CPU support works well.\n> You can try to run this script:\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/cpu/pip/build_tf_windows.sh\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "the problem is with slashes! took me a while to figure that out. use msys2 to run bazel"]}, {"number": 15422, "title": "Add common error documentation", "body": "See https://github.com/tensorflow/tensorflow/issues/15258", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I've agreed to licence", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks @mikeknapp!"]}, {"number": 15421, "title": "Add complex64 and complex128 support for `tf.angle` on GPU", "body": "In PR #10643, complex64 and complex128 support have been added for `tf.angle` on CPU. However, because of the compiling errors, the complex support on GPU is not enabled yet.\r\n\r\nThe issue was that, std::arg is not available on nvidia device for GPU. This fix changes to used atan2 instead, which is available in CUDA.\r\n\r\nThe relevant test cases have bee enabled.\r\n\r\nThis fix is related to #10643.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["This would be super useful for any models relying on spectral transforms! Status?", "@carlthome The PR has been rebased to resolve merge conflict.", "Kuta dala. Harramta\nOn 10 Jun 2018 23:52, \"Alfred Sorten Wolf\" <notifications@github.com> wrote:\n\n> Nagging Reviewer @benoitsteiner <https://github.com/benoitsteiner>: It\n> has been 105 days with no activity and the awaiting review label was\n> assigned. Can you please take a look?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15421#issuecomment-396071921>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYAqYz_luBiJ9aLXaNPUZrgJ49IepqKjks5t7Wr6gaJpZM4REfuG>\n> .\n>\n", "I'm no longer at Google, and don't review pull requests most of the time.", "Nagging Reviewer @rmlarsen: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Ping @rmlarsen to a look at this PR.", "I think this is fine. We can consider upstreaming to Eigen later."]}, {"number": 15420, "title": "IteratorGetNext should have a None gradient defined", "body": "Currently `IteratorGetNext` has no gradient defined. This can cause failures like below in `tf.gradients`. The solution is to define `None` gradient, like the `tf.stop_gradient` op. A work-around when this failure occurs is to wrap dataset ops inside `tf.stop_gradient`\r\n\r\n```\r\n  File \"/Users/yaroslav/anaconda/envs/sep22/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 590, in gradients\r\n    (op.name, op.type))\r\nLookupError: No gradient defined for operation 'IteratorGetNext' (op type: IteratorGetNext)\r\n\r\n```", "comments": ["Closing because this only seems to happen with a custom optimized version of `_PendingCount`"]}, {"number": 15419, "title": "official MNIST example should avoid huge constants", "body": "Right now MNIST model loads dataset as a huge MNIST constant\r\nhttps://github.com/tensorflow/models/blob/master/official/mnist/mnist.py#L65\r\n\r\nThis makes graphdef > 1 GB in size. That causes slowness when trying to visualize graph/dump graphdef. It should instead avoid constant. IE, you can load dataset into tf.Variable", "comments": []}, {"number": 15418, "title": "Eager:  `gradients_function` can't compute the gradient for simple functions", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64-bit\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5.0-dev20171215\r\n- **Python version**: 3.6.3 |Anaconda, Inc.| (default, Nov  8 2017, 15:10:56) [MSC v.1900 64 bit (AMD64)]\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: See description\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nFirst, define a loss function:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\ntfe.enable_eager_execution()\r\n\r\ndef loss(w):\r\n    prediction = 2 * w + 1\r\n    true_value = 11\r\n    return tf.cast((true_value - prediction)**2, tf.float32)\r\n```\r\n\r\nThen, compute the gradient when w=0.1:\r\n\r\n`tfe.gradients_function(loss)(0.1)`\r\n\r\nThe output is as expected. Next, compute the gradient when w=50:\r\n\r\n`tfe.gradients_function(loss)(50)`\r\n\r\nThe output is:\r\n\r\n`[None]`\r\n\r\nI expected the output to be 360 because the gradient is -40 + 8 w.", "comments": ["It should work if you do\r\n```python\r\ntfe.gradients_function(loss)(50.0)\r\n```\r\ninstead of\r\n```python\r\ntfe.gradients_function(loss)(50)\r\n```\r\n\r\nThe root cause seems to be that the Log op doesn't support the dtype `DT_INT32`.", "Yes. Gradients are not defined for integer types, so this is working as intended.", "Wouldn't be useful to have a warning or error message?"]}, {"number": 15417, "title": "no protobuf package for macos Python 3.6", "body": "Following instructions on\r\nhttps://www.tensorflow.org/install/install_mac\r\n\r\n`pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp35-none-macosx_10_11_x86_64.whl\r\n`\r\n\r\nThis doesn't work for Python 3.6 with error\r\n`protobuf-3.1.0-cp35-none-macosx_10_11_x86_64.whl is not a supported wheel on this platform.\r\n`\r\nIf I just change URL cp36, there's no such file\r\n\r\n```pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl\r\n\r\n  Could not install requirement protobuf==3.1.0 from https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl because of error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl\r\nCould not install requirement protobuf==3.1.0 from https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl because of HTTP error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl for URL https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl\r\n\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Looks like this is something we missed.\r\n@jhseu FYI", "We don't build protobuf versions anymore, so we should probably just delete that from the instructions.", "So what's the recommended way of getting cpp version of protobuf? The default python version is slow (ie, timeline generation takes forever)", "The protobuf team has committed to building it for Linux upon every release to PyPI. For Mac, users will have to build it on their own.\r\n\r\nWe haven't been consistent about building it upon every release, so it's better not to rely on our builds.", "I deleted the mention of it in our documentation internally, and it should be reflected in the next push.", "I still see the mention of `Protobuf pip package 3.1` here:<br> https://www.tensorflow.org/install/install_mac\r\n\r\nRunning the command listed under Python 3.n results in an error:<br>`protobuf-3.1.0-cp35-none-macosx_10_11_x86_64.whl is not a supported wheel on this platform.`", "Just to clarify: this means that for linux the default `pip install protobuf` should just work fine? Asking because we're debugging some slow data loading here at the moment..."]}, {"number": 15416, "title": "tensorflow-aarch64 with keras", "body": "\r\n![screenshot_2017-12-17-15-05-30-239_ru iiec pydroid3](https://user-images.githubusercontent.com/31110466/34079284-0cd76cc4-e33c-11e7-8442-438e88e1a38d.png)\r\n![screenshot_2017-12-17-15-04-20-194_ru iiec pydroid3](https://user-images.githubusercontent.com/31110466/34079285-0d1d5edc-e33c-11e7-9109-fbfdce775814.png)\r\n![screenshot_2017-12-17-15-04-36-922_ru iiec pydroid3](https://user-images.githubusercontent.com/31110466/34079287-0d65de50-e33c-11e7-9b62-9d5b5027b131.png)\r\n\r\n\r\nHi! I want to use tensorflow library as backend of keras on my android device. I found and installed pydroid3, keras and tensorflow-aarch64. How can i use it? When i run code it throws exception \"no module named tensorflow\". Any ideas?\r\n\r\nOk, i will.\r\nI used Keras. When i import, it throws exception.\r\nMy OS is Android 7.1.2 MIUI 9, CPU arch - aarch64 (arm), GPU is Snapdragon 505. Using PyDroid3 app\r\nI installed tensorflow-aarch64 1.2 from pip.\r\nNo bazel.\r\nNo cuDNN.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Already updated!", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Unfortunately that's not a configuration that we support.\r\n\r\nAre you trying to run the tensorflow python environment or just the runtime?", "I have the same Issue.\r\nAre you support official in tensorflow-aarch64 package?\r\nhttps://pypi.org/project/tensorflow-aarch64/\r\n\r\nI try just \r\n'import tensorflow'   \r\nno module named tensorflow\".\r\n\r\nI am using oneplus 3t android device.\r\nandroid 8.1.0\r\ngpu not relevant. I want to use cpu version.\r\n", "We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.5  version or later and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "I currently not using it. If there will be an issue, I will open a new one.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/15416\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/15416\">No</a>\n"]}, {"number": 15415, "title": "sparse_multiclass_hinge_loss() error", "body": "Hello, \r\n\r\nI'm getting the error below using `sparse_multiclass_hinge_loss()`. Any hints would be highly appreciated.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = np.random.uniform(0, 1, size = (100, 5))\r\ny = np.random.choice(3, 100) \r\ny = y.reshape(100, 1)\r\n\r\nX = tf.placeholder(\"float32\", [None, 5])\r\nY = tf.placeholder(\"int32\", [None, 1])\r\n\r\nweights = {'w': tf.Variable(tf.random_uniform([5, 3]))}\r\nbiases = {'b': tf.Variable(tf.zeros([3]))}\r\n\r\nlogits = tf.add(tf.matmul(X, weights['w']), biases['b'])\r\n\r\nloss = tf.reduce_mean(tf.contrib.kernel_methods.sparse_multiclass_hinge_loss(logits=logits, labels=Y))\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    res = sess.run(loss, feed_dict={X: x, Y: y}) \r\n\r\n\r\nres\r\n```\r\n make_tensor_proto(values, dtype, shape, verify_shape)\r\n    369   else:\r\n    370     if values is None:\r\n--> 371       raise ValueError(\"None values not supported.\")\r\n    372     # if dtype is provided, forces numpy array to be the type\r\n    373     # provided if possible.\r\n\r\nValueError: None values not supported.\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\nTensorFlow installed from (source or binary): N\r\nTensorFlow version (use command below): 1.4\r\nPython version: 2.7.12\r\nBazel version (if compiling from source): NA\r\nGCC/Compiler version (if compiling from source): NA\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\nExact command to reproduce: See below."]}, {"number": 15414, "title": "Failed to convert a .pb file to a .lite file where there is a custom lite op sin", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04.5 LTS \r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.4.0\r\n- **Python version**: 2.7.6\r\n- **Bazel version (if compiling from source)**: 0.8.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nI followed \"[How to use custom operators](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/custom_operators.md)\" to create a .pb file and \"sin.cc\" in //tensorflow/contrib/lite/kernel/. Then I added \"TfLiteRegistration* Register_SIN();\" and \"AddCustom(\"Sin\", Register_SIN());\" in \"register.cc\"\r\nBut when I used the bazel command to covert the pb file, the sin op can not be converted\r\n\r\nHere is the command I used:\r\nbazel build //tensorflow/contrib/lite/toco:toco\r\nbazel-bin/tensorflow/contrib/lite/toco/toco --input_file=tftest/sin.pb --input_format=TENSORFLOW_GRAPHDEF --output_file=tftest/sin.tflite --output_format=TFLITE --inference_type=FLOAT --input_array=input --input_shape=1 --output_array=output\r\nHere is the corresponding ERROE information:\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: Sin.\r\n\r\nI tried to use ----allow_custom_ops, but it did not work. Here is the ERROR information:\r\nConverting unsupported operation: Sin\r\n\r\nI think I have to modify more files, but I do not know which files I should modify and how to modify. Could you please give a detailed demo?\r\nThx", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Have you solved this problem yet ? I met same error now,  I modified model.h, schema.fbs, BUILD, register.cc and operator.cc similar with Cast op, but I got the error after running ios example:\r\nDidn't find custom op for name 'xxx'\r\nRegistration failed.", "I think any working example for conversion behind 'using a pretrained mobilenet' would help (e.g. wasn't able to convert shufflenet from onnx to tflite, while conversion to tf works normally).", "any one have solution now ?\r\n"]}, {"number": 15413, "title": "Android Failed To Run Inference", "body": "Have I written custom code: No\r\nOS Platform and Distribution: Android\r\nTensorFlow installed from: binary\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\n\r\n### Describe the problem\r\nWith JNI libtensorflow_core.a etc. I can do inference with my model, which is frozen and stripped off unused op by transform_graph [tool](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/tools/graph_transforms/README.md).\r\nBut it took so long, about 50 secs to run on a 1x300x400x3 sized input image. I guess maybe the libtensorflow_core.a that I download from [TensorflowPrebuiltWebsite](http://ci.tensorflow.org/job/tensorflow-master-android/) is built in debug mode. The reason why I download libtensorflow_core.a is because I have trouble building it myself.\r\n\r\nSo I want to use Java API to do inference and see how much time it takes.\r\nThis is done by adding dependencies in build.gradle:\r\n`\r\ndependencies {\r\n    compile 'org.tensorflow:tensorflow-android:+'\r\n}\r\n`\r\n\r\nBut with Java API it throws exceptions for parsing the .pb file. I guess my .pb is incompatible. So I took a step back, I only froze the graph but not stripping off unused op. This time Java API is able to parse the .pb file. But after feed inputs, it throws exception when I call run method on org.tensorflow.contrib.android.TensorFlowInferenceInterface.run().\r\n\r\nI have also tried to download prebuilt libandroid_tensorflow_inference_java.jar and libtensorflow_inference.so from [TensorflowPrebuiltWebsite](http://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/), but not working.\r\n\r\nNotice that I can do inference with C++ code, that means my neural network model *.pb is CORRECT. I think this is a bug with Java API or its corresponding native implementation.\r\n\r\n### logs\r\n```\r\n12-16 11:33:59.954 23001-23001/cn.edu.zju.cad.rwang.rendering.testtensorflowjavaapiperformance I/System.out: debugger has settled (1330)\r\n12-16 11:33:59.998 23001-23001/cn.edu.zju.cad.rwang.rendering.testtensorflowjavaapiperformance I/HwCust: Constructor found for class android.app.HwCustHwWallpaperManagerImpl\r\n12-16 11:34:00.026 23001-23001/cn.edu.zju.cad.rwang.rendering.testtensorflowjavaapiperformance W/art: Before Android 4.1, method android.graphics.PorterDuffColorFilter android.support.graphics.drawable.VectorDrawableCompat.updateTintFilter(android.graphics.PorterDuffColorFilter, android.content.res.ColorStateList, android.graphics.PorterDuff$Mode) would have incorrectly overridden the package-private method in android.graphics.drawable.Drawable\r\n12-16 11:34:00.157 23001-23001/cn.edu.zju.cad.rwang.rendering.testtensorflowjavaapiperformance I/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded\r\n12-16 11:34:00.157 23001-23001/cn.edu.zju.cad.rwang.rendering.testtensorflowjavaapiperformance I/TensorFlowInferenceInterface: TensorFlow native methods already loaded\r\n12-16 11:34:00.231 23001-23012/cn.edu.zju.cad.rwang.rendering.testtensorflowjavaapiperformance I/art: humin current process: cn.edu.zju.cad.rwang.rendering.testtensorflowjavaapiperformance\r\n12-16 11:34:00.231 23001-23012/cn.edu.zju.cad.rwang.rendering.testtensorflowjavaapiperformance I/art: current process_level is : 0\r\n12-16 11:34:00.437 23001-23001/cn.edu.zju.cad.rwang.rendering.testtensorflowjavaapiperformance I/TensorFlowInferenceInterface: Model load took 169ms, TensorFlow version: 1.4.0\r\n12-16 11:34:00.437 23001-23001/cn.edu.zju.cad.rwang.rendering.testtensorflowjavaapiperformance I/TensorFlowInferenceInterface: Successfully loaded model from 'frozen_graph.pb'\r\n12-16 11:34:14.460 23001-23001/cn.edu.zju.cad.rwang.rendering.testtensorflowjavaapiperformance E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[is_training, feed_input], outputs:[hdr_conv_out/LeakyRelu/Maximum]\r\n12-16 11:34:14.789 23001-23001/cn.edu.zju.cad.rwang.rendering.testtensorflowjavaapiperformance I/Process: Sending signal. PID: 23001 SIG: 9\r\n```\r\n\r\n### Source code\r\nMy project is on git [here](https://gitee.com/ZhongBaby/TestTensorflowJavaAPIPerformance), please help me, thanks!\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Do you have more details on the exception message? That log line comes from [here](https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/android/java/org/tensorflow/contrib/android/TensorFlowInferenceInterface.java#L223). Having details about the exception message thrown there would help locate the problem.\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 15412, "title": "tf.contrib.memory_stats.MaxBytesInUse() got `Op type not registered 'MaxBytesInUse' in binary running` error", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nShould be Yes, as subtitle Source code below.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 7 x64\r\n- **TensorFlow installed from (source or binary)**:\r\n`pip install tensorflow-gpu` with Anaconda on windows prompt following the tensorflow official tutorial\r\n- **TensorFlow version (use command below)**:\r\ntensorflow-gpu 1.4.0\r\n- **Python version**: \r\n3.6, Anaconda\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\n8.0/6.0\r\n- **GPU model and memory**:\r\nGTX 1080TI\r\n- **Exact command to reproduce**:\r\na simply `python code.py` or following source in python\r\n\r\n### Describe the problem\r\nThe GPU training is all fine on my placement. \r\nBut when I wanna track the memory usage I got the error using MaxBytesInUse(),\r\nthe problem is not solved even I upgrade TF from 1.3 to 1.4.\r\nAnd of course the same error using BytesInUse().\r\n\r\nIs any solution or is the possibility that the method not support for Win7? Thanks.\r\n\r\n### Source code / logs\r\nThe Op created failed even the simple code execute as follow, when I run MaxBytesInUse() to get a tensor to `a` then it failed.\r\n\r\n```\r\nimport tensorflow as tf\r\na = tf.contrib.memory_stats.MaxBytesInUse()\r\n```\r\nAnd got the message\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\memory_stats\\python\\ops\\memory_stats_ops.py\", line 41, in MaxBytesInUse\r\n    return gen_memory_stats_ops.max_bytes_in_use()\r\n  File \"C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\memory_stats\\ops\\gen_memory_stats_ops.py\", line 88, in max_bytes_in_use\r\n    \"MaxBytesInUse\", name=name)\r\n  File \"C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2958, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2209, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2159, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\", line 627, in call_cpp_shape_fn\r\n    require_shape_fn)\r\n  File \"C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\", line 686, in _call_cpp_shape_fn_impl\r\n    input_tensors_as_shapes, status)\r\n  File \"C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'MaxBytesInUse' in binary running on MY-PC. Make sure the Op and Kernel are registered in the binary running in this process.\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nTensorFlow installed from\nBazel version\nExact command to reproduce", "Yup I have leave them some as N/A and now I have updated them more in detail. In addition, the package version should be `tensorflow-gpu` 1.4.0. Thanks.", "This op (and some other ops under `tf.contrib`) is not supported under windows cmake build yet. Unfortunately, there is no simple work around. But if you are able to build tensorflow from source, you can add command to `tf_python.cmake` for `tf.contrib.memory_stats` like this (as https://github.com/tensorflow/models/issues/2511#issuecomment-335279353 suggests):\r\nhttps://github.com/tensorflow/tensorflow/blob/e210cb140a60a74d5e9ce3bf9ebedb21b4910f1c/tensorflow/contrib/cmake/tf_python.cmake#L604-L640\r\nThen rebuild it may work (not tested).\r\n\r\nFor most users this is not an easy job and should be done by the developers who are familiar with windows cmake build.", "OK I see, thanks for your replying.", "Any plans to fix the build for this ?", "@Overdrivr As far as I know, the answer is no. @shivaniag Maybe we can mark it as contribution welcome?", "@drpngx After reading the thread, I guess the right fix is to add something similar to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/tf_python.cmake#L627-L663 for memory_stats. However, I don't have a Windows machine, so I won't be able to verify my fix. Any ideas? ", "Can this be written as a test? If so, it's great because we can check moving forward and also it will run on windows as part of our battery of tests.", "Closing due to staleness. Please check with the latest version of TensorFlow. Feel free to reopen if the issue still persists. Thanks!", "How is it possible to use `tf.contrib.memory_stats.MaxBytesInUse()` in TF 2.0+ ?", "I use custom dateset while using code of stylegan. when it building tensorflow graph the following error will come: ValueError: Op type not registered 'MaxBytesInUse' in binary running on DESKTOP. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) tf.contrib.resampler should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. while building NodeDef 'MaxBytesInUse'", "i also use differernt version of tensorflow but same result\r\nAnyone here who fix this issue", "in stylgan repo, itst recommended to use TensorFlow 1.10.0 which led to the \"MaxByteinUse\" error, \r\nchanging to'tensorflow-gpu==1.11.0' worked for me", "What do you think about `tf.constant(tf.config.experimental.set_memory_growth(device))` as an alternative to `tf.contrib.memory_stats.MaxBytesInUse()` ?"]}, {"number": 15411, "title": "Fix the CODEOWNERS file syntax", "body": "Note: even though this file is currently disabled, it still seems worth fixing the syntax in case it's enabled again in the future.\r\n\r\nSee https://help.github.com/articles/about-codeowners/ for details on CODEOWNERS syntax.\r\n\r\nIn particular we should change from this pattern:\r\n```\r\n# The `docs/*` pattern will match files like\r\n# `docs/getting-started.md` but not further nested files like\r\n# `docs/build-app/troubleshooting.md`.\r\ndocs/*  docs@example.com\r\n\r\n# In this example, @octocat owns any file in an apps directory\r\n# anywhere in your repository.\r\napps/ @octocat\r\n```\r\n\r\nto this pattern:\r\n```\r\n# In this example, @doctocat owns any files in the build/logs\r\n# directory at the root of the repository and any of its\r\n# subdirectories.\r\n/build/logs/ @doctocat\r\n```", "comments": []}, {"number": 15410, "title": "Calling tf.contrib.lite.toco_convert results in global name 'tempfile' is not defined error", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nRelated SO post - https://stackoverflow.com/questions/47645056/tensorflow-lite-toco-python-apl-nameerror-name-tempfile-is-not-defined\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nFollowed example on Tensorflow Lite documentation\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMacOS High Sierra\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n\r\n- **TensorFlow version (use command below)**:\r\n('v1.3.0-rc1-5910-ge2174cc943', '1.4.0')\r\n\r\n- **Python version**: \r\n2.7.10\r\n\r\n- **Bazel version (if compiling from source)**:\r\nbazel release 0.5.4-homebrew\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nApple LLVM version 9.0.0 (clang-900.0.34.1)\r\n\r\n- **CUDA/cuDNN version**:\r\nN/A (CPU only)\r\n\r\n- **GPU model and memory**:\r\nN/A\r\n\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport tensorflow as tf\r\nimg = tf.placeholder(name=\"img\", dtype=tf.float32, shape=(1, 64, 64, 3))\r\nval = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])\r\nout = tf.identity(val, name=\"out\")\r\nwith tf.Session() as sess:\r\n  tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [img], [out])\r\n  open(\"converteds_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n```\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nOpened python on terminal and ran\r\n\r\n```\r\nimport tensorflow as tf\r\nimg = tf.placeholder(name=\"img\", dtype=tf.float32, shape=(1, 64, 64, 3))\r\nval = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])\r\nout = tf.identity(val, name=\"out\")\r\nwith tf.Session() as sess:\r\n  tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [img], [out])\r\n  open(\"converteds_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n```\r\n\r\nResulting output is\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/lite.py\", line 198, in toco_convert\r\n    input_data.SerializeToString())\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/lite.py\", line 91, in toco_convert_protos\r\n    with tempfile.NamedTemporaryFile() as fp_toco, \\\r\nNameError: global name 'tempfile' is not defined\r\n```\r\n\r\nAs a sanity check I ran the following\r\n\r\n```\r\nimport tempfile\r\nwith tempfile.NamedTemporaryFile() as fp_toco:\r\n     print fp_toco.name\r\n```\r\nOutput was /var/folders/hd/7yc9wgwj5wvd43_d94b4m47m0000gn/T/tmpxxdYMY\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.", "comments": ["Same here. It also throws this error with Python 3.6.3 :: Anaconda custom (64-bit)", "I believe the bug has been fixed in master branch, could you test it on [tf-nightly](https://github.com/tensorflow/tensorflow#installation) ?", "Hello facaiy, I am using the master branch (nightly version). But the problem is still there =|, at least for high sierra", "Sorry about this. It seems it has to do with the interface sealing in tensorflow (where we try to prevent exposing symbols unrelated to the interfaces). You can work around this with the following code\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\n# manually put back imported modules\r\nimport tempfile\r\nimport subprocess\r\ntf.contrib.lite.tempfile = tempfile\r\ntf.contrib.lite.subprocess = subprocess\r\n\r\nimg = tf.placeholder(name=\"img\", dtype=tf.float32, shape=(1, 64, 64, 3))\r\nval = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])\r\nout = tf.identity(val, name=\"out\")\r\nwith tf.Session() as sess:\r\n  tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [img], [out])\r\n  open(\"converteds_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\nI'll make a more permanent fix soon.\r\n", "Thanks for the workaround", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I tried your way but its giving error. . . Kindly help. . . tf.__version__ = 1.6.0rc1 and Ubuntu\r\n\r\n#ERROR is as follows:\r\n\r\nConverted 2 variables to const ops.\r\nTraceback (most recent call last):\r\n \r\n File \"tf.py\", line 94, in <module>\r\n    tflite_model=tf.contrib.lite.toco_convert(sess.graph_def,[x],[mxc])\r\n\r\n  File \"/home/siteurl/anaconda3/envs/osrco/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py\", line 212, in toco_convert\r\n    input_data.SerializeToString())\r\n\r\n  File \"/home/siteurl/anaconda3/envs/osrco/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py\", line 134, in toco_convert_protos\r\n    (stdout, stderr))\r\n\r\n**RuntimeError: TOCO failed see console for info.\r\nb'2018-03-01 14:40:13.614956: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]** \r\n\r\nConverting unsupported operation: VariableV2\\n2018-03-01 14:40:13.615016: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] \r\n\r\nConverting unsupported operation: Assign\\n2018-03-01 14:40:13.615059: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] \r\n\r\nConverting unsupported operation: VariableV2\\n2018-03-01 14:40:13.615083: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] \r\n\r\nConverting unsupported operation: Assign\\n2018-03-01 14:40:13.615118: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] \r\n\r\nConverting unsupported operation: Log\\n2018-03-01 14:40:13.615212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] \r\n\r\nConverting unsupported operation: DynamicStitch\\n2018-03-01 14:40:13.615283: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] \r\n\r\nConverting unsupported operation: Reciprocal\\n2018-03-01 14:40:13.615341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]\r\n\r\n Converting unsupported operation: BroadcastGradientArgs\\n2018-03-01 14:40:13.615384: F tensorflow/contrib/lite/toco/import_tensorflow.cc:973] \r\n\r\nCheck failed: GetBoolAttr(node, \"transpose_b\") == false (1 vs. 0)\\nAborted (core dumped)\\n'\r\nNone\r\n\r\n#CODE is as follows:\r\n\r\nimport tensorflow  as tf\r\nimport pandas as pd\r\nimport numpy as np\r\nimport tempfile\r\nimport subprocess\r\ntf.contrib.lite.tempfile = tempfile\r\ntf.contrib.lite.subprocess = subprocess\r\n\r\nfrom tensorflow.python.tools import freeze_graph\r\nfrom tensorflow.python.tools import optimize_for_inference_lib\r\n\r\n\r\nprint(tf.__version__)\r\ndata=pd.read_csv('iris.data',names=['f1','f2','f3','f4','f5'])\r\n\r\ns=np.asarray([1,0,0])\r\nve=np.asarray([0,1,0])\r\nvi=np.asarray([0,0,1])\r\n\r\ndata['f5']=data['f5'].map({'Iris-setosa':s,'Iris-versicolor':ve,'Iris-virginica':vi})\r\n\r\n#print(data)\r\n\r\ndata=data.iloc[np.random.permutation(len(data))]\r\n\r\nprint(data)\r\n\r\ndata=data.reset_index(drop=True)\r\n\r\n#training data\r\ntrainFeats=data.ix[0:105,['f1','f2','f3','f4']]\r\ntemp=data['f5']\r\ntrainlabels=temp[0:106]\r\n\r\ny=tf.placeholder(tf.float32,shape=[106, 3])\r\n#weight and bias\r\nm=tf.Variable(tf.zeros([4,3]))\r\nx=tf.placeholder(tf.float32,shape=[106,4],name=\"Input\")\r\nc=tf.Variable(tf.zeros([3]))\r\nmxc = tf.nn.softmax((tf.matmul(x, m) + c) ,name=\"output\")\r\n\r\nloss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(mxc), reduction_indices=[1]))\r\n\r\ntrain_step = tf.train.AdamOptimizer(0.01).minimize(loss)\r\n\r\nsess = tf.InteractiveSession()\r\ninit = tf.initialize_all_variables()\r\nsess.run(init)\r\n\r\n\r\n#_=tf.contrib.tflite.Convert(sess.graph_def,[x],[mxc])\r\n\r\n\r\n#number of interations\r\nepoch=2000\r\nfor step in range(epoch):\r\n  print(sess.run([train_step,loss], feed_dict={x: trainFeats, y:[t for t in trainlabels.as_matrix()]}))\r\n\r\n#testData=data.ix[130,['f1','f2','f3','f4']]\r\n#testDataInFrormat=testData.reshape(1,4)\r\n#print(sess.run(tf.argmax(mxc),feed_dict={x:testDataInFrormat}))\r\n\r\ntf.train.write_graph(sess.graph_def,'pbtxtFiles/','savegraph.pbtxt',as_text=True)\r\n\r\ntf.train.Saver().save(sess,'pbtxtFiles/model.ckpt')\r\n\r\n\r\n\r\nMODEL_NAME = 'iris'\r\ninput_graph_path = 'pbtxtFiles/savegraph.pbtxt'\r\ncheckpoint_path = 'pbtxtFiles/model.ckpt'\r\ninput_saver_def_path = \"\"\r\ninput_binary = False\r\noutput_node_names = \"output\"\r\nrestore_op_name = \"save/restore_all\"\r\nfilename_tensor_name = \"save/Const:0\"\r\noutput_frozen_graph_name = 'pbtxtFiles/frozen_model_'+MODEL_NAME+'.pb'\r\noutput_optimized_graph_name = 'pbtxtFiles/optimized_inference_model_'+MODEL_NAME+'.pb'\r\nclear_devices = True\r\n\r\nfreeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\r\n                          input_binary, checkpoint_path, output_node_names,\r\n                          restore_op_name, filename_tensor_name,\r\n                          output_frozen_graph_name, clear_devices, \"\")\r\n\r\n\r\noutput_graph_def = optimize_for_inference_lib.optimize_for_inference(\r\n        sess.graph_def,\r\n        [\"Input\"], # an array of the input node(s)\r\n        [\"output\"], # an array of output nodes\r\n        tf.float32.as_datatype_enum)\r\n\r\ntflite_model=tf.contrib.lite.toco_convert(sess.graph_def,[x],[mxc])\r\nopen(\"wow.tflite\",\"w\").write(tflite_model)\r\n\r\n#IRIS.DATA is this:\r\n\r\n5.1,3.5,1.4,0.2,Iris-setosa\r\n4.9,3.0,1.4,0.2,Iris-setosa\r\n4.7,3.2,1.3,0.2,Iris-setosa\r\n4.6,3.1,1.5,0.2,Iris-setosa\r\n5.0,3.6,1.4,0.2,Iris-setosa\r\n5.4,3.9,1.7,0.4,Iris-setosa\r\n4.6,3.4,1.4,0.3,Iris-setosa\r\n5.0,3.4,1.5,0.2,Iris-setosa\r\n4.4,2.9,1.4,0.2,Iris-setosa\r\n4.9,3.1,1.5,0.1,Iris-setosa\r\n5.4,3.7,1.5,0.2,Iris-setosa\r\n4.8,3.4,1.6,0.2,Iris-setosa\r\n4.8,3.0,1.4,0.1,Iris-setosa\r\n4.3,3.0,1.1,0.1,Iris-setosa\r\n5.8,4.0,1.2,0.2,Iris-setosa\r\n5.7,4.4,1.5,0.4,Iris-setosa\r\n5.4,3.9,1.3,0.4,Iris-setosa\r\n5.1,3.5,1.4,0.3,Iris-setosa\r\n5.7,3.8,1.7,0.3,Iris-setosa\r\n5.1,3.8,1.5,0.3,Iris-setosa\r\n5.4,3.4,1.7,0.2,Iris-setosa\r\n5.1,3.7,1.5,0.4,Iris-setosa\r\n4.6,3.6,1.0,0.2,Iris-setosa\r\n5.1,3.3,1.7,0.5,Iris-setosa\r\n4.8,3.4,1.9,0.2,Iris-setosa\r\n5.0,3.0,1.6,0.2,Iris-setosa\r\n5.0,3.4,1.6,0.4,Iris-setosa\r\n5.2,3.5,1.5,0.2,Iris-setosa\r\n5.2,3.4,1.4,0.2,Iris-setosa\r\n4.7,3.2,1.6,0.2,Iris-setosa\r\n4.8,3.1,1.6,0.2,Iris-setosa\r\n5.4,3.4,1.5,0.4,Iris-setosa\r\n5.2,4.1,1.5,0.1,Iris-setosa\r\n5.5,4.2,1.4,0.2,Iris-setosa\r\n4.9,3.1,1.5,0.1,Iris-setosa\r\n5.0,3.2,1.2,0.2,Iris-setosa\r\n5.5,3.5,1.3,0.2,Iris-setosa\r\n4.9,3.1,1.5,0.1,Iris-setosa\r\n4.4,3.0,1.3,0.2,Iris-setosa\r\n5.1,3.4,1.5,0.2,Iris-setosa\r\n5.0,3.5,1.3,0.3,Iris-setosa\r\n4.5,2.3,1.3,0.3,Iris-setosa\r\n4.4,3.2,1.3,0.2,Iris-setosa\r\n5.0,3.5,1.6,0.6,Iris-setosa\r\n5.1,3.8,1.9,0.4,Iris-setosa\r\n4.8,3.0,1.4,0.3,Iris-setosa\r\n5.1,3.8,1.6,0.2,Iris-setosa\r\n4.6,3.2,1.4,0.2,Iris-setosa\r\n5.3,3.7,1.5,0.2,Iris-setosa\r\n5.0,3.3,1.4,0.2,Iris-setosa\r\n7.0,3.2,4.7,1.4,Iris-versicolor\r\n6.4,3.2,4.5,1.5,Iris-versicolor\r\n6.9,3.1,4.9,1.5,Iris-versicolor\r\n5.5,2.3,4.0,1.3,Iris-versicolor\r\n6.5,2.8,4.6,1.5,Iris-versicolor\r\n5.7,2.8,4.5,1.3,Iris-versicolor\r\n6.3,3.3,4.7,1.6,Iris-versicolor\r\n4.9,2.4,3.3,1.0,Iris-versicolor\r\n6.6,2.9,4.6,1.3,Iris-versicolor\r\n5.2,2.7,3.9,1.4,Iris-versicolor\r\n5.0,2.0,3.5,1.0,Iris-versicolor\r\n5.9,3.0,4.2,1.5,Iris-versicolor\r\n6.0,2.2,4.0,1.0,Iris-versicolor\r\n6.1,2.9,4.7,1.4,Iris-versicolor\r\n5.6,2.9,3.6,1.3,Iris-versicolor\r\n6.7,3.1,4.4,1.4,Iris-versicolor\r\n5.6,3.0,4.5,1.5,Iris-versicolor\r\n5.8,2.7,4.1,1.0,Iris-versicolor\r\n6.2,2.2,4.5,1.5,Iris-versicolor\r\n5.6,2.5,3.9,1.1,Iris-versicolor\r\n5.9,3.2,4.8,1.8,Iris-versicolor\r\n6.1,2.8,4.0,1.3,Iris-versicolor\r\n6.3,2.5,4.9,1.5,Iris-versicolor\r\n6.1,2.8,4.7,1.2,Iris-versicolor\r\n6.4,2.9,4.3,1.3,Iris-versicolor\r\n6.6,3.0,4.4,1.4,Iris-versicolor\r\n6.8,2.8,4.8,1.4,Iris-versicolor\r\n6.7,3.0,5.0,1.7,Iris-versicolor\r\n6.0,2.9,4.5,1.5,Iris-versicolor\r\n5.7,2.6,3.5,1.0,Iris-versicolor\r\n5.5,2.4,3.8,1.1,Iris-versicolor\r\n5.5,2.4,3.7,1.0,Iris-versicolor\r\n5.8,2.7,3.9,1.2,Iris-versicolor\r\n6.0,2.7,5.1,1.6,Iris-versicolor\r\n5.4,3.0,4.5,1.5,Iris-versicolor\r\n6.0,3.4,4.5,1.6,Iris-versicolor\r\n6.7,3.1,4.7,1.5,Iris-versicolor\r\n6.3,2.3,4.4,1.3,Iris-versicolor\r\n5.6,3.0,4.1,1.3,Iris-versicolor\r\n5.5,2.5,4.0,1.3,Iris-versicolor\r\n5.5,2.6,4.4,1.2,Iris-versicolor\r\n6.1,3.0,4.6,1.4,Iris-versicolor\r\n5.8,2.6,4.0,1.2,Iris-versicolor\r\n5.0,2.3,3.3,1.0,Iris-versicolor\r\n5.6,2.7,4.2,1.3,Iris-versicolor\r\n5.7,3.0,4.2,1.2,Iris-versicolor\r\n5.7,2.9,4.2,1.3,Iris-versicolor\r\n6.2,2.9,4.3,1.3,Iris-versicolor\r\n5.1,2.5,3.0,1.1,Iris-versicolor\r\n5.7,2.8,4.1,1.3,Iris-versicolor\r\n6.3,3.3,6.0,2.5,Iris-virginica\r\n5.8,2.7,5.1,1.9,Iris-virginica\r\n7.1,3.0,5.9,2.1,Iris-virginica\r\n6.3,2.9,5.6,1.8,Iris-virginica\r\n6.5,3.0,5.8,2.2,Iris-virginica\r\n7.6,3.0,6.6,2.1,Iris-virginica\r\n4.9,2.5,4.5,1.7,Iris-virginica\r\n7.3,2.9,6.3,1.8,Iris-virginica\r\n6.7,2.5,5.8,1.8,Iris-virginica\r\n7.2,3.6,6.1,2.5,Iris-virginica\r\n6.5,3.2,5.1,2.0,Iris-virginica\r\n6.4,2.7,5.3,1.9,Iris-virginica\r\n6.8,3.0,5.5,2.1,Iris-virginica\r\n5.7,2.5,5.0,2.0,Iris-virginica\r\n5.8,2.8,5.1,2.4,Iris-virginica\r\n6.4,3.2,5.3,2.3,Iris-virginica\r\n6.5,3.0,5.5,1.8,Iris-virginica\r\n7.7,3.8,6.7,2.2,Iris-virginica\r\n7.7,2.6,6.9,2.3,Iris-virginica\r\n6.0,2.2,5.0,1.5,Iris-virginica\r\n6.9,3.2,5.7,2.3,Iris-virginica\r\n5.6,2.8,4.9,2.0,Iris-virginica\r\n7.7,2.8,6.7,2.0,Iris-virginica\r\n6.3,2.7,4.9,1.8,Iris-virginica\r\n6.7,3.3,5.7,2.1,Iris-virginica\r\n7.2,3.2,6.0,1.8,Iris-virginica\r\n6.2,2.8,4.8,1.8,Iris-virginica\r\n6.1,3.0,4.9,1.8,Iris-virginica\r\n6.4,2.8,5.6,2.1,Iris-virginica\r\n7.2,3.0,5.8,1.6,Iris-virginica\r\n7.4,2.8,6.1,1.9,Iris-virginica\r\n7.9,3.8,6.4,2.0,Iris-virginica\r\n6.4,2.8,5.6,2.2,Iris-virginica\r\n6.3,2.8,5.1,1.5,Iris-virginica\r\n6.1,2.6,5.6,1.4,Iris-virginica\r\n7.7,3.0,6.1,2.3,Iris-virginica\r\n6.3,3.4,5.6,2.4,Iris-virginica\r\n6.4,3.1,5.5,1.8,Iris-virginica\r\n6.0,3.0,4.8,1.8,Iris-virginica\r\n6.9,3.1,5.4,2.1,Iris-virginica\r\n6.7,3.1,5.6,2.4,Iris-virginica\r\n6.9,3.1,5.1,2.3,Iris-virginica\r\n5.8,2.7,5.1,1.9,Iris-virginica\r\n6.8,3.2,5.9,2.3,Iris-virginica\r\n6.7,3.3,5.7,2.5,Iris-virginica\r\n6.7,3.0,5.2,2.3,Iris-virginica\r\n6.3,2.5,5.0,1.9,Iris-virginica\r\n6.5,3.0,5.2,2.0,Iris-virginica\r\n6.2,3.4,5.4,2.3,Iris-virginica\r\n5.9,3.0,5.1,1.8,Iris-virginica\r\n\r\nKindly help MR. @aselle \r\nKindly help MR. @facaiy \r\nKindly help MR. @leandroBorgesFerreira \r\nKindly help MR. @javierluraschi \r\nKindly help MR. @tensorflowbutler ", "above code is not giving any output. . . wow.tflite is not even created. . . Please help. . .", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "import tensorflow as tf\r\nimg = tf.placeholder(name=\"img\", dtype=tf.float32, shape=(1, 64, 64, 3))\r\nval = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])\r\nout = tf.identity(val, name=\"out\")\r\nwith tf.Session() as sess:\r\n  tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [img], [out])\r\n  open(\"converteds_model.tflite\", \"wb\").write(tflite_model)\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/lite.py\", line 198, in toco_convert\r\n    input_data.SerializeToString())\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/lite.py\", line 91, in toco_convert_protos\r\n    with tempfile.NamedTemporaryFile() as fp_toco, \\\r\nNameError: global name 'tempfile' is not defined\r\n\r\nAny Update with this issue. I am facing same problem.", "The tempfile issue is fixed in the newest code, but it wasn't released with TensorFlow 1.7.0 yet. \r\n\r\nYou can choose to:\r\n\r\n* Do the [workaround](https://github.com/tensorflow/tensorflow/issues/15410#issuecomment-352189481) mentioned above. \r\n* Use the TensorFlow nightly build (`pip uninstall tensorflow ; pip install tf-nightly`)\r\n* [Build TensorFlow from source code](https://www.tensorflow.org/install/install_sources)\r\n* Upgrade to the next TensorFlow release when it happens in the future. \r\n\r\nI'm closing this bug as the original issue was fixed. If you see other model conversion issue, please create a bug separately. Thanks! "]}, {"number": 15409, "title": "MKL: Fixes for concat and elementwise ops", "body": "Fixes concat for Svd test cases and disable 4 elementwise ops to get SSG/VGG-16 model to work with MKL DNN. Disabled tanh due to differences between Tensorflow and DNN primitives.", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Thanks @agramesh1. Could you fix your cla?", "@tensorflow-jenkins test this please", "@agramesh1 Could you sign the CLA?", "@drpngx I have CLA completed, but one of the other committer did not have one at the time of submission.  He obtained one since then, can you check again if @gzmkl has CLA?", "We don't have records for @gzmkl -- could you also sign the CLA?", "@drpngx thanks. I have already signed CLA (Corporate level). I just checked the website https://cla.developers.google.com/clas it shows CLA. ", "I signed it! ", "CLAs look good, thanks!\n\n<!-- ok -->", "@drpngx  It looks like the CLAs are fine now. Thanks.", "Thanks!"]}, {"number": 15408, "title": "New rolling window batch operation for tf.data.Dataset #15044", "body": "This is a new feature\r\n\r\nFor help pre-processing datasets, creates a transformation function that can be applied to a tf.data.Dataset Object to create a new Dataset by rolling a window across the initial Dataset to create the batches for the new Dataset.\r\n\r\n*Fails if Dataset is too large to fit in memory (possible future work?)\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "pylint issues. will fix then re-submit", "Thanks for your contribution and sorry for letting this PR stall. In the meantime, #16123 has been merged, which provides the same functionality with a more efficient implementation, so I'm going to close this one. "]}, {"number": 15407, "title": "Fix tflite models.md title", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->"]}, {"number": 15406, "title": "MKL: Adding missing reorders in ReLU and AddN", "body": "Commit to add missing reorder primitive in ReLU and AddN operators.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 15405, "title": "Bump the eigen dependency version.", "body": "Fixes #12052", "comments": []}]