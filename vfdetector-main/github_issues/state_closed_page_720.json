[{"number": 31976, "title": "[TF2.0] (rc0) Mocking tf.summary.image", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): `pip install tensorflow==2.0.0rc0` \r\n- TensorFlow version (use command below):\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CPU version\r\n- GPU model and memory: CPU version\r\n\r\n**Problem**\r\n\r\nI have been using successfully `unittest.mock.patch` on `tf.summary.image` with version `2.0.0-beta1`. When upgrading to `2.0.0rc0`, the mock doesn't appear anymore and the full function is used. I couldn't find the change that breaks this. Any idea how to perform the mock?\r\n\r\n**Sample code**\r\n\r\nThe following code is a sample of `tf.summary.image` call inside a callback, and running a simple `.fit`. It runs perfectly on `2.0.0-beta1`, but fails on `2.0.0rc0`.\r\n\r\nTo reproduce, have those 2 snippets (`tf_write_summary.py` and `test_tf_write_summary.py`) in a folder and run `python -m pytest test_tf_write_summary.py`.\r\n\r\n**`tf_write_summary.py`**\r\n```python\r\nfrom datetime import datetime\r\nfrom pathlib import Path\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.callbacks import Callback\r\n\r\nclass TestCallback(Callback):\r\n    def __init__(\r\n        self,\r\n        images,\r\n        output_dir=Path(\"./logs/activations_visualizations\"),\r\n    ):\r\n        super(TestCallback, self).__init__()\r\n        self.images = images\r\n        self.output_dir = Path(output_dir) / datetime.now().strftime(\"%Y%m%d-%H%M%S.%f\")\r\n        Path.mkdir(Path(self.output_dir), parents=True, exist_ok=True)\r\n\r\n        self.file_writer = tf.summary.create_file_writer(str(self.output_dir))\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        with self.file_writer.as_default():\r\n            # tf.summary.image should be a MagicMock object\r\n            tf.summary.image(\r\n                \"Test Callback\",\r\n                self.images,\r\n                step=epoch,\r\n            )\r\n```\r\n\r\n**`test_tf_write_summary.py`**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport tf_write_summary\r\n\r\n\r\ndef test_callback(mocker):\r\n    mock_image_summary = mocker.patch('tf_write_summary.tf.summary.image')\r\n    fake_image_values = [0]\r\n    callback = tf_write_summary.TestCallback(fake_image_values)\r\n\r\n    model = tf.keras.Sequential(\r\n        [\r\n            tf.keras.layers.Conv2D(\r\n                16,\r\n                (3, 3),\r\n                activation=None,\r\n                name=\"conv_1\",\r\n                input_shape=(28, 28, 3),\r\n            ),\r\n            tf.keras.layers.ReLU(name=\"activation_1\"),\r\n            tf.keras.layers.Flatten(),\r\n            tf.keras.layers.Dense(2, activation=\"softmax\"),\r\n        ]\r\n    )\r\n\r\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\r\n\r\n    y = np.array([1, 0] * 4).reshape((4, 2))\r\n    model.fit(np.random.random((4, 28, 28, 3)), y, epochs=1, batch_size=2, callbacks=[callback])\r\n\r\n    mock_image_summary.assert_called_once_with(\"Test Callback\", [0], step=0)\r\n```\r\n\r\n\r\nThanks for your time!", "comments": ["@annarev do you happen to know if there has been a change to lazy loading in the TF APIs that might have triggered this?  I'm not aware myself of anything that would have changed on the TB side for the `tf.summary.*` forwarding logic.", "The only lazy loading changes were made a part of adding a virtual pip package.\r\n@mihaimaruseac do you know if any of the virtual pip changes could cause this?", "I don't think so, but I can investigate.", "I just tried with nightly and it seems to be working with latest tf-nightly-2.0-preview.", "@RaphaelMeudec \r\ncould you please update on the above comment", "I think the 2.2.0-rc0 should also be ok, can you check, @RaphaelMeudec ?", "@Saduf2019 @mihaimaruseac Just tested against 2.1.0 and it's working. Thanks for the update! I'm closing the issue."]}, {"number": 31975, "title": "Prediction speed worse than community keras due to training related code running during inference", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04.2 (but we believe this to be irrelevant)\r\n- TensorFlow installed from (source or binary): pip (inside a docker image)\r\n- TensorFlow version (use command below): 1.14 (likely affects 1.12 - current 2.0 beta)\r\n- Python version: 3.6.8 (system python, we believe irrelevant)\r\n- CUDA/cuDNN version: 10.0.130 (but we believe irrelevant)\r\n- GPU model and memory: 1080 Ti\r\n\r\n**Describe the current behavior**\r\nCalling model.predict() includes calls to various non-prediction related things, in order of decreasing severity:\r\n* reset_metrics\r\n* get_progbar\r\n* standardize_user_data\r\n* validate_or_infer_batch_size\r\n\r\nIn our project, this results in a 2X prediction speed regression:\r\n35 ms per call for community keras\r\n70 ms per call for tensorflow keras\r\n\r\nA Snakeviz flamegraph for prediction of our network in Tensorflow Keras, showing unnecessary overhead:\r\n\r\n![Tensorflow Keras](https://user-images.githubusercontent.com/190617/63684638-ab884a80-c7fd-11e9-9dbe-5404eda2d70c.png)\r\n\r\n**Describe the expected behavior**\r\nOnly do things necessary for prediction while doing prediction.\r\n\r\nA Snakeviz flamegraph for prediction of our network in Community Keras, showing no unnecessary overhead:\r\n\r\n![Community Keras](https://user-images.githubusercontent.com/190617/63684637-ab884a80-c7fd-11e9-810d-6341eba99070.png)\r\n\r\n**Code to reproduce the issue**\r\nThis code is active for all calls to predict().\r\n\r\nThis is for a commercial project we are trying to migrate from community keras to tensorflow keras. It is a speed critical real-time robotics application where the 30ms is enough to miss our hard deadlines. If predict() is intended for production inference, we feel this should be resolved in tensorflow keras. If it is only intended for use during training, this should be noted in the mainline documentation, along with a suggestion for what ~is intended for production inference.\r\n\r\nAs a temporary workaround, we disabled some of these function calls in a local tensorflow fork, and the performance regression went away as expected. We took care to make sure this is not a profiling glitch. We excluded warmup time from the profile, and average over a number of samples.\r\n", "comments": ["Looking at the code in the 2.0 branch, this affects tensorflow 2.0 beta as well. It is not limited to tensorflow 1.14.\r\n\r\nWe haven't backported our code to 1.12 to test it, but looking at the code, it seems much of the overhead may have been introduced in the commit:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/4ca599630233afe5240deae44212f19412ab6423#diff-02db763382946f7695e505ce1c545dc4L250\r\n\r\nSee the addition at line:\r\ntensorflow/python/keras/engine/training_arrays.py:250\r\n\r\nIt is possible the issue goes even farther back; we haven't audited the old attribute based mechanism.", "Could you provide some code to reproduce? I did a quick sweep (various size DNNs, small and large input, small and large output) of predictions in the ~50 ms range, and found tf.keras and keras-team keras gave comparable performance.", "Thanks for looking into this! We can't share our model publicly as-is.\r\n\r\nI thought maybe the version of keras-team might have more overhead than the one we benchmarked. The keras-team we were using for benchmark was 2.2.4. We just looked at the current master commit: https://github.com/keras-team/keras/commit/a39f10acf3eac5e6ef69d084616982a4d4ebd8bb and the overall logic has not changed. \r\n\r\nIf you look at the predict_loop in keras-team:\r\n\r\nhttps://github.com/keras-team/keras/blob/master/keras/engine/training_arrays.py\r\n\r\nYou will see that the progress bar code is behind a \"verbose\" flag that is off by default, and since the prediction is factored in a separate loop from training, the other code called in the tf.keras version doesn't get called either. keras-team does a bunch of batching logic even when there is only a single sample, but apparently it's not as much overhead as the extra logic in the tf.keras version. \r\n\r\nMaybe the extra training logic that was left on for prediction scales badly in the number of tensors? Our network is a weird custom convnet+MLP that probably has more operations than most networks of the same run time, mostly in the MLP part. Maybe the performance issue is reproducible with a really deep MLP with really small layers, but I would not bother unless you're interested in it as an additional stress test. Instead of investing effort in studying why that code is slow in our use case, I would advocate disabling it all behind a flag that is off by default. \r\n\r\nTraining and profiling related code should not be active by default in a function that's only supposed to do prediction, right? ", "Ah, this is very useful. When I go to a deep stack of MLPs (several hundred) I do indeed see a very clear difference between tf.keras and keras-team keras. I'll take a look at what's causing the difference. Thanks!", "Thanks for looking into this!  We are still following and interested!", "Sure. I am currently trying to get a fix into the 2.0 release, but let me give an update in the mean time.\r\n\r\nIn Keras, there are a bunch of properties which recursively depend on all child layers. `metrics` is one, but there are others: `stateful`, `dynamic`, `layers`, etc. What I noticed during profiling is that every `model.predict` call we have to check whether these attributes have changed, and walking that graph was still causing significant python overhead even after I stopped collecting metrics in predict. (~12 ms for the 250 layer DNN that I'm testing on.)\r\n\r\nThe change that I'm working on adds a layer aware caching mechanism, so we can avoid walking the entire graph-of-layers if nothing has changed. (And generally people aren't going to repeatedly toggle `stateful` or add layers when doing inference...) However, because of the nature of this cache it's vital that we ensure correctness and check that it will invalidate on any potentially relevant mutation, so it's taking a bit longer than I initially anticipated.\r\n\r\nI'm really hoping to get this into 2.0. `model.predict` is definitely my recommended way to do inference directly from a keras model since it will make a `tf.function` for you, so I want that path to be a good experience. (There is pure c++ machinery for the really latency sensitive, but I know a lot of folks just want a reasonably convenient way in Python.)", "Can you try the latest `tf-nightly-gpu-2.0-preview`? I believe most if not all of the graph walking was cut out in https://github.com/tensorflow/tensorflow/commit/f43ba77daff7ff1bfeda52b976ae6e5d4a7dc265.", "Marking this issue as closed, since we believe this was fixed, please feel free to reopen if that is not the case.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31975\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31975\">No</a>\n"]}, {"number": 31974, "title": "LD_LIBRARY_PATH is set but library files can not be found", "body": "I have installed tensorflow via `pip install tensorflow-gpu` and I also have copied the label image on my local directory. When I run it, I see some shared library error related to libcu* but such files actually exist and LD_LIBRARY_PATH confirms that.\r\n```\r\n$ ls\r\nBUILD  data  hot.nvvp  label_image.py  main.cc  README.md\r\n$ python label_image.py\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0826 05:27:49.600733 140369765574464 deprecation_wrapper.py:119] From label_image.py:28: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\r\n\r\nW0826 05:27:50.118398 140369765574464 deprecation_wrapper.py:119] From label_image.py:45: The name tf.read_file is deprecated. Please use tf.io.read_file instead.\r\n\r\nW0826 05:27:50.122489 140369765574464 deprecation_wrapper.py:119] From label_image.py:59: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.\r\n\r\n2019-08-26 05:27:50.126417: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-08-26 05:27:50.130951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-26 05:27:50.131830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\npciBusID: 0000:03:00.0\r\n2019-08-26 05:27:50.131924: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:\r\n2019-08-26 05:27:50.131985: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:\r\n2019-08-26 05:27:50.132043: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:\r\n2019-08-26 05:27:50.132100: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:\r\n2019-08-26 05:27:50.132157: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:\r\n2019-08-26 05:27:50.132214: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:\r\n2019-08-26 05:27:50.135607: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-26 05:27:50.135639: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...\r\n2019-08-26 05:27:50.228821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-26 05:27:50.229782: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55625c9d47c0 executing computations on platform CUDA. Devices:\r\n2019-08-26 05:27:50.229803: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2019-08-26 05:27:50.249987: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3591645000 Hz\r\n2019-08-26 05:27:50.250225: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55625c8a63f0 executing computations on platform Host. Devices:\r\n2019-08-26 05:27:50.250242: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-08-26 05:27:50.250308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-26 05:27:50.250320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]\r\n2019-08-26 05:27:50.256827: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n2019-08-26 05:27:50.264294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-26 05:27:50.264335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]\r\nW0826 05:27:52.117340 140369765574464 deprecation_wrapper.py:119] From label_image.py:69: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\r\n\r\nmilitary uniform 0.8343058\r\nmortarboard 0.021869581\r\nacademic gown 0.010358071\r\npickelhaube 0.008008132\r\nbulletproof vest 0.005350866\r\n```\r\n\r\n\r\nPlease note messages like \r\n\r\n```\r\n2019-08-26 05:27:50.131924: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] \r\nCould not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot \r\nopen shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:\r\n```\r\n\r\nlibrary path is set correctly\r\n\r\n```\r\n$ ls -l /usr/local/cuda-10.1/lib64/libcurand*\r\nlrwxrwxrwx 1 root root       15 Aug 22 02:49 /usr/local/cuda-10.1/lib64/libcurand.so -> libcurand.so.10\r\nlrwxrwxrwx 1 root root       21 Aug 22 02:49 /usr/local/cuda-10.1/lib64/libcurand.so.10 -> libcurand.so.10.1.168\r\n-rwxr-xr-x 1 root root 59812280 Aug 22 02:49 /usr/local/cuda-10.1/lib64/libcurand.so.10.1.168\r\n-rw-r--r-- 1 root root 59842274 Aug 22 02:49 /usr/local/cuda-10.1/lib64/libcurand_static.a\r\n$ echo $LD_LIBRARY_PATH\r\n/usr/local/cuda-10.1/lib64:\r\n```\r\n\r\nAny more steps should I take?", "comments": ["@mahmoodn ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Thanks!", "It's looking for libcu*.so.10.0:\r\n```\r\n2019-08-26 05:27:50.132100: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:\r\n```\r\n\r\nYou appear to have CUDA 10.1.168 installed. You can symlink the libraries so that they appear to be CUDA 10.0 (i.e. ln -s ``/usr/local/cuda-10.1/lib64/libcurand.so.10.0 /usr/local/cuda-10.1/lib64/libcurand.so.10.1.168``). This has worked for me in the past, but shouldn't be expected to work in general.", "Does that mean the binary pip version has not been tested with cuda 10.1?", "Right, it has not been test with cuda 10.1. Refer to the [second table here](https://www.tensorflow.org/install/source) for which CUDA versions the various pip releases are built/tested against.", "OK. I will try to compile from source. I will close the issue. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31974\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31974\">No</a>\n"]}, {"number": 31973, "title": "[TF 2.0.0-rc0] Cannot find any 2.0.0 RC0 API references.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10 and macOS\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0-rc0 (both GPU and CPU versions)\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nNot a single reference to members of tensorflow can be found in IDE(PyCharm). \r\nHowever, programs run as expected.\r\n\r\n**Describe the expected behavior**\r\nReferences to members of tensorflow can be found.\r\n\r\n<img width=\"602\" alt=\"Screen Shot 2019-08-26 at 17 13 30\" src=\"https://user-images.githubusercontent.com/6904036/63680285-e1412980-c825-11e9-89c5-8f8abc376275.png\">\r\n\r\n", "comments": ["@makercob Hello, friend! I have the same problem. I found PyCharm cannot find the tensorflow_2.0.0-rc0 API, but jupyter notebook and jupyter lab can. I'm also  puzzled by this problem.", "Many others (including myself) reported the same problem in the following thread https://github.com/tensorflow/tensorflow/issues/26502. The previous issue is closed so I will leave this here hoping it has better coverage being an open issue. ", "@georgealexandruvlad uhmmm... PyCharm to blame? but 2.0.0b1 worked fine.", "@makercob well, you're right. I'm just looking for a solution for large projects which can't be implemented in jupyter/google colab. As I stated in my comment in the other thread:\r\n\r\n> If it is any workaround on this issue until a fix is released (from tensorflow team or PyCharm) I would be grateful to use it.\r\n\r\nI only ask for a solution to get it working because there are lots of good new features in the new release. This could mean a solution from the tensorflow team if there are some steps to be followed in order to make it work with PyCharm or another IDE in which they know it is working properly.", "Is there any workaround for that, while it is not solved?\r\n\r\nEDIT: I simply went back to beta for the moment, the references work without problems there.", "@georgealexandruvlad Could you please share a link to release notes of 2.0.0-rc0? I just can't find it anywhere. Thanks!", "@makercob Sure. I think this should be what you're searching for: https://github.com/tensorflow/tensorflow/releases", "@Efaq Any alternative to PyCharm? VS Code works not so well for me.", "We understand that this is a serious issue and we are actively trying to solve this. However, this is not something we can get in by the final release of 2.0 as the issue is an unfortunate side effect of several components: having to support both v1 and v2 code until the final release of 2.0, having to add deprecation wrapping to a lot of API endpoints in v1, adding the virtual pip package to help with modularization efforts and adding lazy loading to speed up `import tensorflow as tf` time.\r\n\r\nThat being said, once 2.0 is released we can clean up a large part of the code and can then solve this more efficient. Furthermore, once TensorFlow gets more modular we can clean up other parts of the code to make solving this issue more tractable. Unfortunately, we don't have a clear timeline for the second part, so we don't know exactly when we can say with 100% certainty that this issue gets solved.\r\n\r\nHowever, @annarev has submitted a change (1171258036e73c911d0487a3c2db8272fd9dc6be) that will enable autocomplete in a majority of use cases but we ran into some issues and it has been reverted (1d1f7dfcbdca4ce4acfdd769a3777a9877e90fe3). We will try to get this change submitted again in the coming days and then cherry-pick it on the next release candidate.\r\n\r\nIn any case, as we get new releases on the TF2.* series, this issue will get solved. Sorry for the delay and for the issues it causes", "@makercob I will try not to change, I have some preference for PyCharm. But in case it gets too annoying not to have the last version of TF, I will choose between VS and Spyder. **BUT** I don't even know if this would solve the issue :)\r\n\r\nWhat issues have you had with VS Code? Could you get autocompletion?", "@Efaq No autocompletion with VS Code either. :-/\r\nSpoiled by PyCharm, not everything seems right to me in VS Code when working with Python. say, instant variable evaluation and numpy array inspection.", "> @makercob Hello, friend! I have the same problem. I found PyCharm cannot find the tensorflow_2.0.0-rc0 API, but jupyter notebook and jupyter lab can. I'm also puzzled by this problem.\r\n\r\nJust a quick note about the general reason of why this happening. Jupyter dynamically figures out all the packages. PyCharm (for speed reasons) statically indexes everything. If things are only defined when dynamically run, PyCharm won't see them. Python packages are generally encouraged to have these things available statically. This explanation is slightly inaccurate, but close enough.\r\n\r\nUnfortunately, in my case, I value PyCharm's capabilities and workflow far more than any improvements and fixes in the new TensorFlow releases. So this is a blocker on me upgrading.", "Also, as noted by others, this will not be a PyCharm specific issue. Many IDEs expect to be able to statically resolve the packages.", "> Also, as noted by others, this will not be a PyCharm specific issue. Many IDEs expect to be able to statically resolve the packages.\r\n\r\nExactly, VScode has the same issue! :( why google use such goofy importing method in tensorflow?", "We needed a way to separate private internal APIs with no guarantees on stability with public stable(r) APIs so that code that works on Tensorflow x.y also works on Tensorflow x.{y+1} while evolving in private.\r\n\r\nWe also needed a way to suppot both a preview of 2.0 and 1.* at the same time.\r\n\r\nBut most of these needs will go away with the release of 2.0. Also, there is ongoing work to modularize TensorFlow and one of the components of that work is extracting the public API to a separate module. After that, autocomplete should work perfectly. See https://github.com/tensorflow/community/pull/77", "> We needed a way to separate private internal APIs with no guarantees on stability with public stable(r) APIs so that code that works on Tensorflow x.y also works on Tensorflow x.{y+1} while evolving in private.\r\n> \r\n> We also needed a way to suppot both a preview of 2.0 and 1.* at the same time.\r\n> \r\n> But most of these needs will go away with the release of 2.0. Also, there is ongoing work to modularize TensorFlow and one of the components of that work is extracting the public API to a separate module. After that, autocomplete should work perfectly. See [tensorflow/community#77](https://github.com/tensorflow/community/pull/77)\r\n\r\nThanks for your clarification. I hope we can have everything perfect soon.\r\nDo you have any idea as how long it may take to completely resolve this issue?", "Probably end of year", "The fix for a large part of autocomplete has been submitted here: https://github.com/tensorflow/tensorflow/commit/386da9758dde5d278d646701a0769b931e564485 and should be in latest nightly. However, there are still some parts that don't work and we are looking into them.", "> \r\n> \r\n> The fix for a large part of autocomplete has been submitted here: [386da97](https://github.com/tensorflow/tensorflow/commit/386da9758dde5d278d646701a0769b931e564485) and should be in latest nightly. However, there are still some parts that don't work and we are looking into them.\r\n\r\nThanks for your work! Does this mean that the autocomplete problem will be solved when the next tensorflow-2.0 version (may be rc1) is released ?", "@calmisential Nope. Spring is indeed pretty much flexible.", "A workaround which seems to work for now is importing as directly as possible. This means instead of \r\n`from tensorflow.keras import Model`\r\none has to do\r\n`from tensorflow_core.python.keras.models import Model`\r\n\r\nInstead of \r\n`import tensorflow as tf` and then\r\n`tf.keras.optimizers.Adam()`\r\n\r\nUse \r\n`from tensorflow_core.python.keras.optimizers import Adam`\r\nand then just\r\n`Adam()`\r\n\r\nIt makes it more cumbersome to import stuff, but at least autocomplete and other IDE features work", "Is this fixed for you in rc1? I still have a similar problem.\r\n`from tensorflow import keras` works\r\n`from tensorflow.keras import layers` does not work.", "@RunOrVeith as mentioned the problem remains in rc1 (and will probably until the release of 2.0)", "Ok I thought that might have changed because the release notes state \r\n`Fixes autocomplete for most TensorFlow API references by switching to use relative imports in API __init__.py files.`", "Chinese to such interesting problem: He He. \u5475\u5475 ", "> The fix for a large part of autocomplete has been submitted here: [386da97](https://github.com/tensorflow/tensorflow/commit/386da9758dde5d278d646701a0769b931e564485) and should be in latest nightly. However, there are still some parts that don't work and we are looking into them.\r\n\r\nJust to confirm, this is no longer working in the nightly release is it?", "Autocomplete has been fixed for `import tensorflow as tf` and `from tensorflow import keras` in rc1.\r\nHowever, `from tensorflow.keras import layers` won't be fixed even in 2.0 final. It requires major changes to TensorFlow (renaming lite/ directory) to get it working. These changes are too big for this release since it only accepts critical cherrypicks.\r\n\r\nAlso, recommended way to import TensorFlow is `import tensorflow as tf`.\r\n", "Just updated to rc1 and still have no success with `from tensorflow import keras` or `import tensorflow as tf`. This is gpu version if it makes a difference.\r\nI've rebuilt all indices and caches etc. to eliminate a PyCharm issue. Changing interpreter back to 1.14 goes straight back to the intended functionality.", "Hi, I've been coding in TF 2.0 for the past few days and used this cheap workaround for my (momentarily) limited use case: install `tensorflow-gpu==2.0.0-beta1` alongside with `tensorflow==2.0.0-rc1`. But it's quite strange, better wait for a proper fix.", "This has been fixed for rc1 in the newest PyCharm EAP: https://blog.jetbrains.com/pycharm/2019/09/2019-3-eap-2", "> This has been fixed for rc1 in the newest PyCharm EAP: https://blog.jetbrains.com/pycharm/2019/09/2019-3-eap-2\r\n\r\nIt works, thx a lot.", "Closing as it has been fixed. Thanks for verifying.", "Just to note, although PyCharm now correctly resolves TensorFlow, this solution does not solve the issue for other editors. There are others in this thread who have mentioned this issue also affects VS Code (and likely other editors).\r\n\r\n(... but I use PyCharm, so I'm satisfied)", "In that case, reopening.", "While the workaround of importing tensorflow_core to access API functionality normally under tf.config (e.g.\r\n```python\r\nfrom tensorflow_core.python.framework import config as tfconfig\r\n```\r\nallows autocompletion to function (tf v2.0.0-rc2, pycharm EAP: 193.3519.27),  it does not allow me to use tf.config functionality due to the import error below:\r\n```\r\nImportError: cannot import name 'module_util' from 'tensorflow.python.tools' (/opt/anaconda/envs/tf2rc/lib/python3.7/site-packages/tensorflow/python/tools/__init__.py)\r\n```\r\nAny suggestions on a temporary workaround for accessing tf.config functionality? Thanks very much in advance!!", "Importing directly from `tensorflow_core` is not supported and will result in your code being broken by future upgrades.", "Thanks Mihai, understood, I'm just looking for a temporary workaround to access tf.config, which isn't working with rc2. I think this is actually a separate issue and I'll create a separate ticket accordingly. Apologies for the confusion.", "This problem persist with Tensorflow 2.0 lastest release, using VS Code", "@ElPapi42 PyCharm 2019.3 coming to rescue, probably in Nov.", "I can reproduce this problem with tensorflow==1.15.0, using latest official PyCharm release (2019-2.3). Is the fix going to be applied to 1.X branch?", "> This problem persist with Tensorflow 2.0 lastest release, using VS Code\r\nYes, too mad with this issue\r\n", "Any timeline of a fix for this issue?", "The fix is to use `import tensorflow.compat.v2 as tf` instead of the regular import.", "> The fix is to use `import tensorflow.compat.v2 as tf` instead of the regular import.\r\n\r\nNot working for me :/", "We're having discussions about tensorflow2 completions in Jedi. I proposed two solutions in https://github.com/davidhalter/jedi/issues/1391#issuecomment-558857355:\r\n\r\n> One way to fix this would be to just replace keras = _sys.modules[\"tensorflow.keras\"] with from tensorflow import keras. This does essentially the same thing.\r\n>\r\n> It's also nice to use something like:\r\n>\r\n> ```\r\n> import typing\r\n> \r\n> if not typing.TYPE_CHECKING:\r\n>     keras = _sys.modules[\"tensorflow.keras\"]\r\n> ```\r\n>\r\n> This only works in Python 3 (typing needs to be installed).", "@davidhalter Thanks for helping debug this.\r\n\r\n@mihaimaruseac I think I found a solution that get's rid of the problematic workarounds in #34629", "Why was this closed?"]}, {"number": 31972, "title": "`tf.load_op_library` contains no custom ops", "body": "I face the same problem with that in https://github.com/tensorflow/tensorflow/issues/27307. However, the author did not give more details how he solved this problem. Is there anyone who has successfully solved this problem?", "comments": ["I added the op code in tf source code, complied it and installed tf complied binary. Then I extracted the tf op code and load the op built. So comes the duplex op code and tf cannot differentiate which op to load"]}, {"number": 31971, "title": "vs2015: c++ dependencies", "body": "I already build a tensorflow1.13r c++ lib with bazel0.21.0 .I found a problem when I try to create the VS project.\r\nSeverity\tCode\tDescription\tProject\tFile\tLine\tSuppression State\r\nError\tLNK2019\tunresolved external symbol \"char const * __cdecl tensorflow::core::GetVarint32PtrFallback(char const *,char const *,unsigned int *)\" (?GetVarint32PtrFallback@core@tensorflow@@YAPEBDPEBD0PEAI@Z) referenced in function \"char const * __cdecl tensorflow::core::GetVarint32Ptr(char const *,char const *,unsigned int *)\" (?GetVarint32Ptr@core@tensorflow@@YAPEBDPEBD0PEAI@Z)\ttensorflowc++test\tE:\\c++\\tensorflowc++test\\tensorflowc++test\\t1.obj\t1\t\r\n\r\nI know it meanings I haven't finishied the dependencies.But I don't know which .lib file should be includ. There are too many .lib files in bazel-out floder.\r\n![cut](https://user-images.githubusercontent.com/23164484/63672466-979c1300-c814-11e9-9237-6ac7246f6814.png)\r\n", "comments": ["@YuBicheng ,\r\nCan you please go through similar issue #[24885](https://github.com/tensorflow/tensorflow/issues/24885).Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31971\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31971\">No</a>\n"]}, {"number": 31970, "title": "ERROR: NotFoundError - File under path not being found.", "body": "I am trying to train a whole model based on the COCO dataset using this scripts provided but reducing the number of classes to only 6. \r\n\r\nI run the `download_and_preprocess_coco.sh` script which downloads the dataset and calls the `create_coco_tf_record.py` script which creates the TFRecords from the dataset previously downloaded. After that steps (successfully achieved) I try to run the `retrain_detection_model.sh` as it is described in the tutorial, but modifying the labels .pdtxt file in order to take into account only 6 clases and modifying the `pipeline.config` file in order to achieve the same (with a v2 net and training the whole model option).\r\n\r\nThe first error that came out was:\r\n\r\n`RuntimeError: Did not find any input files matching the glob pattern [u'/tensorflow/models/research/tmp/mscoco/coco_train.record-00001-of-00010']`\r\n\r\nWhen I do have a file under: `/tensorflow/models/research/tmp/mscoco/` which contains files of the following format:\r\n```\r\ncoco_testdev.record-00000-of-00100\r\ncoco_train.record-00024-of-00100\r\ncoco_val.record-00001-of-00010\r\n```\r\nBeing the first set of 5 numbers after the record part numbers that go from 00000 to 00099.\r\n\r\nSo I do have those files that the error reports I do not have, and I have the PATH specified in the pipeline.config file.\r\n\r\nI managed to move on a bit by skipping the use of the `glob` library in the `dataset_builder.py` script under the route `research/object_detection/builders/`. It is not working as it should, so by just removing the use of it the script runs a bit ahead, but it still throws and error:\r\n\r\n```\r\nNotFoundError (see above for traceback): /tensorflow/models/research/tmp/mscoco/coco_train.record-00001-of-00010; No such file or directory\r\n         [[node IteratorGetNext (defined at object_detection/model_main.py:105)  = IteratorGetNext[output_shapes=[[128], [128,300,300,3], [128,2], [128,3], [128,100], [128,100,4], [128,100,2], [128,100,2], [128,100], [128,100], [128,100], [128]],\r\n output_types=[DT_INT32, DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_BOOL, DT_FLOAT, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](IteratorV2)]]\r\n```\r\n\r\nI have not figured out how to move on from here.\r\n\r\n\r\nI paste my `pipeline.config` file:\r\n\r\n```\r\nmodel {\r\n  ssd {\r\n    num_classes: 2\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 300\r\n        width: 300\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: \"ssd_mobilenet_v2\"\r\n      depth_multiplier: 1.0\r\n      min_depth: 16\r\n      conv_hyperparams {\r\n        regularizer {\r\n          l2_regularizer {\r\n            weight: 3.99999989895e-05\r\n          }\r\n        }\r\n        initializer {\r\n          random_normal_initializer {\r\n            mean: 0.0\r\n            stddev: 0.00999999977648\r\n          }\r\n        }\r\n        activation: RELU_6\r\n        batch_norm {\r\n          decay: 0.97000002861\r\n          center: true\r\n          scale: true\r\n          epsilon: 0.0010000000475\r\n        }\r\n      }\r\n      override_base_feature_extractor_hyperparams: true\r\n    }\r\n    box_coder {\r\n      faster_rcnn_box_coder {\r\n        y_scale: 10.0\r\n        x_scale: 10.0\r\n        height_scale: 5.0\r\n        width_scale: 5.0\r\n      }\r\n    }\r\n    matcher {\r\n      argmax_matcher {\r\n        matched_threshold: 0.5\r\n        unmatched_threshold: 0.5\r\n        ignore_thresholds: false\r\n        negatives_lower_than_unmatched: true\r\n        force_match_for_each_row: true\r\n        use_matmul_gather: true\r\n      }\r\n    }\r\n    similarity_calculator {\r\n      iou_similarity {\r\n      }\r\n    }\r\n    box_predictor {\r\n      convolutional_box_predictor {\r\n        conv_hyperparams {\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 3.99999989895e-05\r\n            }\r\n          }\r\n          initializer {\r\n            random_normal_initializer {\r\n              mean: 0.0\r\n              stddev: 0.00999999977648\r\n            }\r\n          }\r\n          activation: RELU_6\r\n          batch_norm {\r\n            decay: 0.97000002861\r\n            center: true\r\n            scale: true\r\n            epsilon: 0.0010000000475\r\n          }\r\n        }\r\n        min_depth: 0\r\n        max_depth: 0\r\n        num_layers_before_predictor: 0\r\n        use_dropout: false\r\n        dropout_keep_probability: 0.800000011921\r\n        kernel_size: 1\r\n        box_code_size: 4\r\n        apply_sigmoid_to_scores: false\r\n        class_prediction_bias_init: -4.59999990463\r\n      }\r\n    }\r\n    anchor_generator {\r\n      ssd_anchor_generator {\r\n        num_layers: 6\r\n        min_scale: 0.20000000298\r\n        max_scale: 0.949999988079\r\n        aspect_ratios: 1.0\r\n        aspect_ratios: 2.0\r\n        aspect_ratios: 0.5\r\n        aspect_ratios: 3.0\r\n        aspect_ratios: 0.333299994469\r\n      }\r\n    }\r\n    post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 0.300000011921\r\n        iou_threshold: 0.600000023842\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n    normalize_loss_by_num_matches: true\r\n    loss {\r\n      localization_loss {\r\n        weighted_smooth_l1 {\r\n        }\r\n      }\r\n      classification_loss {\r\n        weighted_sigmoid_focal {\r\n          gamma: 2.0\r\n          alpha: 0.75\r\n        }\r\n      }\r\n      classification_weight: 1.0\r\n      localization_weight: 1.0\r\n    }\r\n    encode_background_as_zeros: true\r\n    normalize_loc_loss_by_codesize: true\r\n    inplace_batchnorm_update: true\r\n    freeze_batchnorm: false\r\n  }\r\n}\r\ntrain_config {\r\n  batch_size: 128\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n  data_augmentation_options {\r\n    ssd_random_crop {\r\n    }\r\n  }\r\n  sync_replicas: true\r\n  optimizer {\r\n    momentum_optimizer {\r\n      learning_rate {\r\n        cosine_decay_learning_rate {\r\n          learning_rate_base: 0.20000000298\r\n          total_steps: 50000\r\n          warmup_learning_rate: 0.0599999986589\r\n          warmup_steps: 2000\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.899999976158\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  fine_tune_checkpoint: \"/tensorflow/models/research/learn_human_car/ckpt/model.ckpt\"\r\n  from_detection_checkpoint: true\r\n  load_all_detection_checkpoint_vars: true\r\n  num_steps: 50000\r\n  startup_delay_steps: 0.0\r\n  replicas_to_aggregate: 8\r\n  max_number_of_boxes: 100\r\n  unpad_groundtruth_tensors: false\r\n}\r\ntrain_input_reader {\r\n  label_map_path: \"/tensorflow/models/research/object_detection/data/mscoco_label_map.pbtxt\"\r\n  tf_record_input_reader {\r\n    input_path: \"/tensorflow/models/research/tmp/mscoco/coco_train.record-00001-of-00010\"\r\n  }\r\n}\r\neval_config {\r\n  num_examples: 8000\r\n  metrics_set: \"coco_detection_metrics\"\r\n  use_moving_averages: false\r\n}\r\neval_input_reader {\r\n  label_map_path: \"/tensorflow/models/research/object_detection/data/mscoco_label_map.pbtxt\"\r\n  shuffle: false\r\n  num_readers: 1\r\n  tf_record_input_reader {\r\n    input_path: \"/tensorflow/models/research/tmp/mscoco/coco_val.record-?????-of-00010\"\r\n  }\r\n}\r\ngraph_rewriter {\r\n  quantization {\r\n    delay: 48000\r\n    weight_bits: 8\r\n    activation_bits: 8\r\n  }\r\n}\r\n```\r\n", "comments": ["@tomruarol \r\nThis issue is more suitable for TensorFlow Models repo. Please post it on models repo from [here](https://github.com/tensorflow/models/issues/new). Thanks!", "**@ravikyram** \r\nOkay, I posted it where you suggested.\r\nLink to the issue: https://github.com/tensorflow/models/issues/7504 ", "I am closing this issue here and we can track this issue in tensorflow/models#7504.  It will help us to follow easily.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31970\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31970\">No</a>\n"]}, {"number": 31969, "title": "Distributed Tensorflow", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n\r\nexample script provided by Tensorflow https://github.com/tensorflow/models/tree/master/official/vision/image_classification\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): tensorflow-gpu 1.14.0\r\n- Python version: python 3.6.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 10.0\r\n- GPU model and memory: Tesla M40 , each with 24GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nHi, i am using this script `resnet_cifar_main.py` in https://github.com/tensorflow/models/tree/master/official/vision/image_classification to test multi worker distributed strategy. I have added some codes to configure cluster in this file as follows\r\n```\r\nimport os\r\nimport json\r\nos.environ[\"TF_CONFIG\"] = json.dumps({\r\n      'cluster': {\r\n        'worker': # [\"username@100.102.33.44:8080\", \"username@100.102.32.179:8080\"]\r\n                   [\"username@100.102.32.179:8080\", \"username@100.102.33.40:8080\"]\r\n                  },\r\n          'task': {'type': 'worker', 'index': 0}\r\n          })\r\n```\r\nboth machines can login from each other without authentication using ssh. I use `python resnet_cifar_main.py --data_dir cifar-10-batches-bin/ --distribution_strategy multi_worker_mirrored --num_gpus 8` to run it.\r\nHowever,  the code stalls. Here is the information.\r\n```\r\n/data1/username/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/data1/username/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/data1/username/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/data1/username/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/data1/username/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/data1/username/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/data1/username/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/data1/username/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/data1/username/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/data1/username/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/data1/username/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/data1/username/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nW0826 11:51:16.547584 140208701863744 deprecation_wrapper.py:119] From /data1/username/models/official/utils/misc/keras_utils.py:154: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2019-08-26 11:51:16.563363: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-08-26 11:51:16.616212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:04:00.0\r\n2019-08-26 11:51:16.617656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:05:00.0\r\n2019-08-26 11:51:16.619078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:08:00.0\r\n2019-08-26 11:51:16.620503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:09:00.0\r\n2019-08-26 11:51:16.621998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:84:00.0\r\n2019-08-26 11:51:16.623482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:85:00.0\r\n2019-08-26 11:51:16.624936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:88:00.0\r\n2019-08-26 11:51:16.626392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:89:00.0\r\n2019-08-26 11:51:16.626574: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-26 11:51:16.627793: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-26 11:51:16.629061: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-26 11:51:16.629329: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-26 11:51:16.630910: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-26 11:51:16.632135: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-26 11:51:16.635787: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-26 11:51:16.658649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2019-08-26 11:51:16.659053: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-08-26 11:51:18.127150: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f84e4413620 executing computations on platform CUDA. Devices:\r\n2019-08-26 11:51:18.127186: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla M40 24GB, Compute Capability 5.2\r\n2019-08-26 11:51:18.127194: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla M40 24GB, Compute Capability 5.2\r\n2019-08-26 11:51:18.127200: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): Tesla M40 24GB, Compute Capability 5.2\r\n2019-08-26 11:51:18.127205: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): Tesla M40 24GB, Compute Capability 5.2\r\n2019-08-26 11:51:18.127210: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (4): Tesla M40 24GB, Compute Capability 5.2\r\n2019-08-26 11:51:18.127215: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (5): Tesla M40 24GB, Compute Capability 5.2\r\n2019-08-26 11:51:18.127221: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (6): Tesla M40 24GB, Compute Capability 5.2\r\n2019-08-26 11:51:18.127227: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (7): Tesla M40 24GB, Compute Capability 5.2\r\n2019-08-26 11:51:18.133237: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400090000 Hz\r\n2019-08-26 11:51:18.135546: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f84e6c0fe90 executing computations on platform Host. Devices:\r\n2019-08-26 11:51:18.135571: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-08-26 11:51:18.142054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:04:00.0\r\n2019-08-26 11:51:18.143568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:05:00.0\r\n2019-08-26 11:51:18.145001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:08:00.0\r\n2019-08-26 11:51:18.146484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:09:00.0\r\n2019-08-26 11:51:18.147938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:84:00.0\r\n2019-08-26 11:51:18.149383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:85:00.0\r\n2019-08-26 11:51:18.150933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:88:00.0\r\n2019-08-26 11:51:18.152413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:89:00.0\r\n2019-08-26 11:51:18.152461: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-26 11:51:18.152480: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-26 11:51:18.152498: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-26 11:51:18.152515: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-26 11:51:18.152532: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-26 11:51:18.152548: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-26 11:51:18.152566: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-26 11:51:18.175302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2019-08-26 11:51:18.175343: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-26 11:51:18.188053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-26 11:51:18.188075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 \r\n2019-08-26 11:51:18.188101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N N N N \r\n2019-08-26 11:51:18.188112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N \r\n2019-08-26 11:51:18.188119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N N \r\n2019-08-26 11:51:18.188126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N \r\n2019-08-26 11:51:18.188140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y \r\n2019-08-26 11:51:18.188147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N Y N Y Y \r\n2019-08-26 11:51:18.188154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y \r\n2019-08-26 11:51:18.188161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N Y Y Y N \r\n2019-08-26 11:51:18.203869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 20545 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.205583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 20545 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.207222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 20545 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.208873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 20545 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.210517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 20545 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.212232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 20545 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.213911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 20545 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.215580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 20545 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)\r\nW0826 11:51:18.218294 140208701863744 deprecation_wrapper.py:119] From /data1/username/models/official/utils/misc/keras_utils.py:155: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\r\n\r\n2019-08-26 11:51:18.221324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:04:00.0\r\n2019-08-26 11:51:18.222757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:05:00.0\r\n2019-08-26 11:51:18.224187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:08:00.0\r\n2019-08-26 11:51:18.225622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:09:00.0\r\n2019-08-26 11:51:18.227063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:84:00.0\r\n2019-08-26 11:51:18.228503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:85:00.0\r\n2019-08-26 11:51:18.229937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:88:00.0\r\n2019-08-26 11:51:18.231370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:89:00.0\r\n2019-08-26 11:51:18.231399: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-26 11:51:18.231418: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-26 11:51:18.231434: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-26 11:51:18.231450: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-26 11:51:18.231466: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-26 11:51:18.231483: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-26 11:51:18.231499: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-26 11:51:18.254277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2019-08-26 11:51:18.254618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-26 11:51:18.254634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 \r\n2019-08-26 11:51:18.254652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N N N N \r\n2019-08-26 11:51:18.254666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N \r\n2019-08-26 11:51:18.254673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N N \r\n2019-08-26 11:51:18.254680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N \r\n2019-08-26 11:51:18.254693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y \r\n2019-08-26 11:51:18.254700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N Y N Y Y \r\n2019-08-26 11:51:18.254707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y \r\n2019-08-26 11:51:18.254721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N Y Y Y N \r\n2019-08-26 11:51:18.270274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 20545 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.271763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 20545 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.273259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 20545 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.274711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 20545 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.276196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 20545 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.277656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 20545 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.279127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 20545 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.280830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 20545 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.292016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:04:00.0\r\n2019-08-26 11:51:18.293441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:05:00.0\r\n2019-08-26 11:51:18.294855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:08:00.0\r\n2019-08-26 11:51:18.296320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:09:00.0\r\n2019-08-26 11:51:18.297760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:84:00.0\r\n2019-08-26 11:51:18.299191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:85:00.0\r\n2019-08-26 11:51:18.300627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:88:00.0\r\n2019-08-26 11:51:18.302090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:89:00.0\r\n2019-08-26 11:51:18.302118: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-26 11:51:18.302137: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-26 11:51:18.302153: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-26 11:51:18.302169: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-26 11:51:18.302185: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-26 11:51:18.302201: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-26 11:51:18.302217: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-26 11:51:18.324811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2019-08-26 11:51:18.325169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-26 11:51:18.325185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 \r\n2019-08-26 11:51:18.325203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N N N N \r\n2019-08-26 11:51:18.325211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N \r\n2019-08-26 11:51:18.325224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N N \r\n2019-08-26 11:51:18.325232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N \r\n2019-08-26 11:51:18.325238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y \r\n2019-08-26 11:51:18.325252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N Y N Y Y \r\n2019-08-26 11:51:18.325259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y \r\n2019-08-26 11:51:18.325266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N Y Y Y N \r\n2019-08-26 11:51:18.341278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 20545 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.342725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 20545 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.344171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:2 with 20545 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.345625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:3 with 20545 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.347083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:4 with 20545 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.348544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:5 with 20545 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.349997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:6 with 20545 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:18.351556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:7 with 20545 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)\r\nI0826 11:51:18.352102 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0\r\nI0826 11:51:18.352772 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:0\r\nI0826 11:51:18.352954 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:1\r\nI0826 11:51:18.353148 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:2\r\nI0826 11:51:18.353321 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:3\r\nI0826 11:51:18.353487 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:4\r\nI0826 11:51:18.353651 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:5\r\nI0826 11:51:18.353815 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:6\r\nI0826 11:51:18.353976 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:7\r\nI0826 11:51:18.354135 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\nI0826 11:51:18.354294 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:0\r\nI0826 11:51:18.354458 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:1\r\nI0826 11:51:18.354618 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:2\r\nI0826 11:51:18.354776 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:3\r\nI0826 11:51:18.354935 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:4\r\nI0826 11:51:18.355093 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:5\r\nI0826 11:51:18.355252 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:6\r\nI0826 11:51:18.355409 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:7\r\nW0826 11:51:18.355474 140208701863744 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nI0826 11:51:18.356196 140208701863744 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['username@100.102.32.179:8080', 'username@100.102.33.40:8080']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:GPU:0', '/job:worker/task:0/device:GPU:1', '/job:worker/task:0/device:GPU:2', '/job:worker/task:0/device:GPU:3', '/job:worker/task:0/device:GPU:4', '/job:worker/task:0/device:GPU:5', '/job:worker/task:0/device:GPU:6', '/job:worker/task:0/device:GPU:7'), communication = CollectiveCommunication.AUTO\r\nW0826 11:51:19.012401 140208701863744 lazy_loader.py:50] \r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nW0826 11:51:19.093180 140208701863744 deprecation.py:323] From /data1/username/.local/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDeprecated in favor of operator or tf.math.divide.\r\nW0826 11:51:19.094039 140208701863744 deprecation.py:323] From /data1/username/models/official/vision/image_classification/cifar_preprocessing.py:80: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\nI0826 11:51:19.311534 140208701863744 cross_device_ops.py:1032] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nI0826 11:51:19.311678 140208701863744 cross_device_ops.py:1053] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nI0826 11:51:19.332920 140208701863744 cross_device_ops.py:1032] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nI0826 11:51:19.333035 140208701863744 cross_device_ops.py:1053] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nI0826 11:51:19.354012 140208701863744 cross_device_ops.py:1032] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nI0826 11:51:19.354128 140208701863744 cross_device_ops.py:1053] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nI0826 11:51:19.406786 140208701863744 cross_device_ops.py:1032] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nI0826 11:51:19.406906 140208701863744 cross_device_ops.py:1053] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nI0826 11:51:19.427956 140208701863744 cross_device_ops.py:1032] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nI0826 11:51:19.428072 140208701863744 cross_device_ops.py:1053] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nW0826 11:51:50.855791 140208701863744 deprecation.py:506] From /data1/username/.local/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nI0826 11:51:50.961928 140208701863744 distribute_coordinator.py:776] Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'worker': ['username@100.102.32.179:8080', 'username@100.102.33.40:8080']}, task_type = 'worker', task_id = 0, environment = None, rpc_layer = 'grpc'\r\nW0826 11:51:50.962039 140208701863744 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nW0826 11:51:50.962105 140208701863744 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n2019-08-26 11:51:50.967967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:04:00.0\r\n2019-08-26 11:51:50.969414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:05:00.0\r\n2019-08-26 11:51:50.970952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:08:00.0\r\n2019-08-26 11:51:50.972416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:09:00.0\r\n2019-08-26 11:51:50.973948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:84:00.0\r\n2019-08-26 11:51:50.975438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:85:00.0\r\n2019-08-26 11:51:50.976896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:88:00.0\r\n2019-08-26 11:51:50.978343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:89:00.0\r\n2019-08-26 11:51:50.978393: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-26 11:51:50.978413: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-26 11:51:50.978430: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-26 11:51:50.978447: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-26 11:51:50.978463: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-26 11:51:50.978479: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-26 11:51:50.978496: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-26 11:51:51.002211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2019-08-26 11:51:51.003014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-26 11:51:51.003031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 \r\n2019-08-26 11:51:51.003054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N N N N \r\n2019-08-26 11:51:51.003068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N \r\n2019-08-26 11:51:51.003075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N N \r\n2019-08-26 11:51:51.003089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N \r\n2019-08-26 11:51:51.003096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y \r\n2019-08-26 11:51:51.003103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N Y N Y Y \r\n2019-08-26 11:51:51.003116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y \r\n2019-08-26 11:51:51.003124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N Y Y Y N \r\n2019-08-26 11:51:51.021461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 20545 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.022911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 20545 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.024357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:2 with 20545 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.026395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:3 with 20545 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.028537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:4 with 20545 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.030097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:5 with 20545 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.031552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:6 with 20545 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.033061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:7 with 20545 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)\r\nI0826 11:51:51.033764 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0\r\nI0826 11:51:51.033987 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:0\r\nI0826 11:51:51.034153 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:1\r\nI0826 11:51:51.034312 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:2\r\nI0826 11:51:51.034467 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:3\r\nI0826 11:51:51.034620 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:4\r\nI0826 11:51:51.034772 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:5\r\nI0826 11:51:51.034923 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:6\r\nI0826 11:51:51.035072 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:7\r\nI0826 11:51:51.035221 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\nI0826 11:51:51.035371 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:0\r\nI0826 11:51:51.035520 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:1\r\nI0826 11:51:51.035670 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:2\r\nI0826 11:51:51.035823 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:3\r\nI0826 11:51:51.035972 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:4\r\nI0826 11:51:51.036118 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:5\r\nI0826 11:51:51.036265 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:6\r\nI0826 11:51:51.036412 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:7\r\nW0826 11:51:51.036476 140208701863744 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nI0826 11:51:51.037198 140208701863744 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['username@100.102.32.179:8080', 'username@100.102.33.40:8080']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:GPU:0', '/job:worker/task:0/device:GPU:1', '/job:worker/task:0/device:GPU:2', '/job:worker/task:0/device:GPU:3', '/job:worker/task:0/device:GPU:4', '/job:worker/task:0/device:GPU:5', '/job:worker/task:0/device:GPU:6', '/job:worker/task:0/device:GPU:7'), communication = CollectiveCommunication.AUTO\r\nI0826 11:51:51.037485 140208701863744 distribute_coordinator.py:438] Starting standard TensorFlow server, target = 'grpc://username@100.102.32.179:8080', session_config= allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    scoped_allocator_optimization: ON\r\n    scoped_allocator_opts {\r\n      enable_op: \"CollectiveReduce\"\r\n    }\r\n  }\r\n}\r\nexperimental {\r\n  collective_group_leader: \"/job:worker/replica:0/task:0\"\r\n}\r\n\r\n2019-08-26 11:51:51.039570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:04:00.0\r\n2019-08-26 11:51:51.041442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:05:00.0\r\n2019-08-26 11:51:51.043607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:08:00.0\r\n2019-08-26 11:51:51.045599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:09:00.0\r\n2019-08-26 11:51:51.047044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:84:00.0\r\n2019-08-26 11:51:51.048483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:85:00.0\r\n2019-08-26 11:51:51.049915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:88:00.0\r\n2019-08-26 11:51:51.052019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:89:00.0\r\n2019-08-26 11:51:51.052058: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-26 11:51:51.052077: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-26 11:51:51.052094: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-26 11:51:51.052110: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-26 11:51:51.052126: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-26 11:51:51.052142: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-26 11:51:51.052158: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-26 11:51:51.086456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2019-08-26 11:51:51.097618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-26 11:51:51.097635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 \r\n2019-08-26 11:51:51.097659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N N N N \r\n2019-08-26 11:51:51.097666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N \r\n2019-08-26 11:51:51.097680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N N \r\n2019-08-26 11:51:51.097687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N \r\n2019-08-26 11:51:51.097701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y \r\n2019-08-26 11:51:51.097708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N Y N Y Y \r\n2019-08-26 11:51:51.097722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y \r\n2019-08-26 11:51:51.097735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N Y Y Y N \r\n2019-08-26 11:51:51.135823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 20545 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.138364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:1 with 20545 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.141177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:2 with 20545 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.146457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:3 with 20545 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.151085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:4 with 20545 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.156594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:5 with 20545 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.162514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:6 with 20545 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.164078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:7 with 20545 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.165756: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:8080, 1 -> username@100.102.33.40:8080}\r\n2019-08-26 11:51:51.169005: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:8080\r\n2019-08-26 11:51:51.169031: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:369] Server already started (target: grpc://localhost:8080)\r\n2019-08-26 11:51:51.180244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:04:00.0\r\n2019-08-26 11:51:51.181868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:05:00.0\r\n2019-08-26 11:51:51.183521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:08:00.0\r\n2019-08-26 11:51:51.185127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:09:00.0\r\n2019-08-26 11:51:51.186747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:84:00.0\r\n2019-08-26 11:51:51.188347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:85:00.0\r\n2019-08-26 11:51:51.189975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:88:00.0\r\n2019-08-26 11:51:51.191620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:89:00.0\r\n2019-08-26 11:51:51.191660: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-26 11:51:51.191688: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-26 11:51:51.191708: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-26 11:51:51.191726: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-26 11:51:51.191749: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-26 11:51:51.191768: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-26 11:51:51.191801: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-26 11:51:51.217200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2019-08-26 11:51:51.217612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-26 11:51:51.217632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 \r\n2019-08-26 11:51:51.217643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N N N N \r\n2019-08-26 11:51:51.217668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N \r\n2019-08-26 11:51:51.217689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N N \r\n2019-08-26 11:51:51.217705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N \r\n2019-08-26 11:51:51.217720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y \r\n2019-08-26 11:51:51.217739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N Y N Y Y \r\n2019-08-26 11:51:51.217747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y \r\n2019-08-26 11:51:51.217758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N Y Y Y N \r\n2019-08-26 11:51:51.235327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 20545 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.236996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 20545 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.238641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:2 with 20545 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.240324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:3 with 20545 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.242048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:4 with 20545 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.243783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:5 with 20545 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.245461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:6 with 20545 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:51.247184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:7 with 20545 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)\r\nI0826 11:51:51.248429 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0\r\nI0826 11:51:51.248695 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:0\r\nI0826 11:51:51.248878 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:1\r\nI0826 11:51:51.249054 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:2\r\nI0826 11:51:51.249228 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:3\r\nI0826 11:51:51.249400 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:4\r\nI0826 11:51:51.249571 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:5\r\nI0826 11:51:51.249751 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:6\r\nI0826 11:51:51.249931 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:7\r\nI0826 11:51:51.250112 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\nI0826 11:51:51.250284 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:0\r\nI0826 11:51:51.250463 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:1\r\nI0826 11:51:51.250636 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:2\r\nI0826 11:51:51.250803 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:3\r\nI0826 11:51:51.250995 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:4\r\nI0826 11:51:51.251197 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:5\r\nI0826 11:51:51.251389 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:6\r\nI0826 11:51:51.251568 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:7\r\nW0826 11:51:51.251645 140208701863744 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nI0826 11:51:51.252460 140208701863744 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['username@100.102.32.179:8080', 'username@100.102.33.40:8080']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:GPU:0', '/job:worker/task:0/device:GPU:1', '/job:worker/task:0/device:GPU:2', '/job:worker/task:0/device:GPU:3', '/job:worker/task:0/device:GPU:4', '/job:worker/task:0/device:GPU:5', '/job:worker/task:0/device:GPU:6', '/job:worker/task:0/device:GPU:7'), communication = CollectiveCommunication.AUTO\r\nI0826 11:51:55.961434 140208701863744 distribute_coordinator.py:776] Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'worker': ['username@100.102.32.179:8080', 'username@100.102.33.40:8080']}, task_type = 'worker', task_id = 0, environment = None, rpc_layer = 'grpc'\r\nW0826 11:51:55.961624 140208701863744 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nW0826 11:51:55.961693 140208701863744 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n2019-08-26 11:51:55.967643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:04:00.0\r\n2019-08-26 11:51:55.969124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:05:00.0\r\n2019-08-26 11:51:55.970583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:08:00.0\r\n2019-08-26 11:51:55.972132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:09:00.0\r\n2019-08-26 11:51:55.973567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:84:00.0\r\n2019-08-26 11:51:55.974998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:85:00.0\r\n2019-08-26 11:51:55.976440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:88:00.0\r\n2019-08-26 11:51:55.977881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:89:00.0\r\n2019-08-26 11:51:55.977933: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-26 11:51:55.977954: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-26 11:51:55.977972: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-26 11:51:55.978010: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-26 11:51:55.978030: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-26 11:51:55.978050: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-26 11:51:55.978068: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-26 11:51:56.003848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2019-08-26 11:51:56.004245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-26 11:51:56.004263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 \r\n2019-08-26 11:51:56.004274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N N N N \r\n2019-08-26 11:51:56.004295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N \r\n2019-08-26 11:51:56.004313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N N \r\n2019-08-26 11:51:56.004322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N \r\n2019-08-26 11:51:56.004337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y \r\n2019-08-26 11:51:56.004346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N Y N Y Y \r\n2019-08-26 11:51:56.004361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y \r\n2019-08-26 11:51:56.004372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N Y Y Y N \r\n2019-08-26 11:51:56.022906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 20545 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:56.024706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 20545 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:56.027009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:2 with 20545 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:56.028847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:3 with 20545 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:56.030286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:4 with 20545 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:56.031725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:5 with 20545 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:56.033410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:6 with 20545 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:56.035645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:7 with 20545 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)\r\nI0826 11:51:56.036307 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0\r\nI0826 11:51:56.036504 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:0\r\nI0826 11:51:56.036674 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:1\r\nI0826 11:51:56.036839 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:2\r\nI0826 11:51:56.036999 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:3\r\nI0826 11:51:56.037156 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:4\r\nI0826 11:51:56.037314 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:5\r\nI0826 11:51:56.037470 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:6\r\nI0826 11:51:56.037625 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:7\r\nI0826 11:51:56.037780 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\nI0826 11:51:56.037934 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:0\r\nI0826 11:51:56.038089 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:1\r\nI0826 11:51:56.038243 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:2\r\nI0826 11:51:56.038396 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:3\r\nI0826 11:51:56.038550 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:4\r\nI0826 11:51:56.038703 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:5\r\nI0826 11:51:56.038856 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:6\r\nI0826 11:51:56.039009 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:7\r\nW0826 11:51:56.039075 140208701863744 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nI0826 11:51:56.039798 140208701863744 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['username@100.102.32.179:8080', 'username@100.102.33.40:8080']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:GPU:0', '/job:worker/task:0/device:GPU:1', '/job:worker/task:0/device:GPU:2', '/job:worker/task:0/device:GPU:3', '/job:worker/task:0/device:GPU:4', '/job:worker/task:0/device:GPU:5', '/job:worker/task:0/device:GPU:6', '/job:worker/task:0/device:GPU:7'), communication = CollectiveCommunication.AUTO\r\n2019-08-26 11:51:56.045441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:04:00.0\r\n2019-08-26 11:51:56.046901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:05:00.0\r\n2019-08-26 11:51:56.048481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:08:00.0\r\n2019-08-26 11:51:56.050782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:09:00.0\r\n2019-08-26 11:51:56.053113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:84:00.0\r\n2019-08-26 11:51:56.054669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:85:00.0\r\n2019-08-26 11:51:56.056100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:88:00.0\r\n2019-08-26 11:51:56.057525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:89:00.0\r\n2019-08-26 11:51:56.057556: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-26 11:51:56.057575: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-26 11:51:56.057599: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-26 11:51:56.057619: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-26 11:51:56.057639: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-26 11:51:56.057658: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-26 11:51:56.057678: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-26 11:51:56.114764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2019-08-26 11:51:56.125872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-26 11:51:56.125889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 \r\n2019-08-26 11:51:56.125916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N N N N \r\n2019-08-26 11:51:56.125924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N \r\n2019-08-26 11:51:56.125941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N N \r\n2019-08-26 11:51:56.125951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N \r\n2019-08-26 11:51:56.125967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y \r\n2019-08-26 11:51:56.125981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N Y N Y Y \r\n2019-08-26 11:51:56.125988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y \r\n2019-08-26 11:51:56.125996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N Y Y Y N \r\n2019-08-26 11:51:56.160059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 20545 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:56.161525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 20545 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:56.163024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:2 with 20545 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:56.164528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:3 with 20545 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:56.165979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:4 with 20545 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:56.167440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:5 with 20545 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:56.168887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:6 with 20545 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)\r\n2019-08-26 11:51:56.170380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:7 with 20545 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)\r\nI0826 11:51:56.170995 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0\r\nI0826 11:51:56.171188 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:0\r\nI0826 11:51:56.171359 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:1\r\nI0826 11:51:56.171522 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:2\r\nI0826 11:51:56.171682 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:3\r\nI0826 11:51:56.171840 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:4\r\nI0826 11:51:56.171997 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:5\r\nI0826 11:51:56.172154 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:6\r\nI0826 11:51:56.172314 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:7\r\nI0826 11:51:56.172470 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\nI0826 11:51:56.172623 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:0\r\nI0826 11:51:56.172777 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:1\r\nI0826 11:51:56.172931 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:2\r\nI0826 11:51:56.173083 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:3\r\nI0826 11:51:56.173235 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:4\r\nI0826 11:51:56.173387 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:5\r\nI0826 11:51:56.173537 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:6\r\nI0826 11:51:56.173689 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:7\r\nW0826 11:51:56.173753 140208701863744 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nI0826 11:51:56.174468 140208701863744 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['username@100.102.32.179:8080', 'username@100.102.33.40:8080']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:GPU:0', '/job:worker/task:0/device:GPU:1', '/job:worker/task:0/device:GPU:2', '/job:worker/task:0/device:GPU:3', '/job:worker/task:0/device:GPU:4', '/job:worker/task:0/device:GPU:5', '/job:worker/task:0/device:GPU:6', '/job:worker/task:0/device:GPU:7'), communication = CollectiveCommunication.AUTO\r\nW0826 11:51:56.174733 140208701863744 distributed_training_utils.py:1082] ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\n```\r\n`nvidia-smi` show that only a tiny part of gpu memory is used in node 0. and the other machine, i.e., the worker node 1, does not run any program. The worker node has the same python and tensorflow version.\r\nHere are my questions.\r\n1. should I set up some configuration in worker node 1? what and how?\r\n2. Is there anything wrong using the resnet_cifar_main.py for distributed training\r\n3. i found the port on node 0 was listening\r\n COMMAND     PID       USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME\r\nresnet_ci 51888 username 93u  IPv4 8730954      0t0  TCP *:webcache (LISTEN)\r\nhowever,  the port on node 1 was not opening.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I have found out why. The server set http proxy which leads to failure of connection.\r\ni just use the following command, then it works\r\n```\r\nunset http_proxy\r\nunset https_proxy\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31969\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31969\">No</a>\n"]}, {"number": 31968, "title": "Why HLO Importer doesn't support U32 type?", "body": "Hi:\r\nI am learning the internal data structure of Tensorflow and MLIR.  \r\nI first used ``tfcompile --graph=graph.pb  --config=graph.config.pbtxt   --xla_dump_to=\"/tmp/\" --cpp_class=\"mynamespace::MyComputation\" --xla_dump_hlo_as_text=true`` and then extracted ``module_0000.after_optimizations.txt`` which if I understood this correctly is the lowered HLO IR from input TF GraphDef.\r\n\r\nHowever when I pass this file into ``tf-mlir-translate --hlo-text-to-mlir-hlo /tmp/module_0000.after_optimizations.txt`` I got greeted with an internal error says 'Unsupported Type: U32'.\r\n\r\nIs there any reason U32 isn't supported in HLO or am I missing something obvious here?", "comments": ["@Naville ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version and provide more information on the issue.Thanks!\r\n"]}, {"number": 31967, "title": "Select compiler for building", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n1.14\r\n\r\n- Are you willing to contribute it (Yes/No):\r\n\r\nYes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nSelect the compiler to use for building in configure.py\r\n\r\n**Will this change the current api? How?**\r\n\r\nI don't think so.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone who needs it\r\n\r\n**Any Other info.**\r\n\r\n\r\nIs it possible in the python configure script to choose the compiler when building on a non-linux? On windows, I have several compilers, but the script always selects the wrong compiler. On linux, it asks for the compiler to use, but doesn't on other systems. I will try to see how the script works so it can detect the compilers and let the user select it so it won't choose the wrong compiler or version.", "comments": ["I had to remove the other MSVC compilers for now. If I understood how to enumerate all the MSVC versions in python, I would contribute the code, but it is a mystery to me how it finds the compiler on windows at all right now.", "[https://www.youtube.com/watch?v=jXZMvgFoqX0](url)", "First of all, I am sorry to miss this issue.\r\nI have a ton of github issues coming my way, and I am way behind on processing them.\r\n\r\nThese are all configured through bazel.\r\nhttps://docs.bazel.build/versions/master/windows.html#using-bazel-on-windows-1\r\n\r\nYou can use environment variables to direct bazel to use different compilers.\r\nAs bazel has these configurations available more easily than python, we do not have them in the configure script.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31967\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31967\">No</a>\n"]}, {"number": 31966, "title": "NotFoundError: _ZN10tensorflow31MaybeForwardRefInputToRefOutputEPNS_15OpKernelContextEii ", "body": "Python 3.6.5\r\ncompiled tensorflow-1.13.1-cp36-cp36m-linux_x86_64.whl\r\n\r\nThe compilation process of my newly created optimizer libgftrl_op.so can be compiled without error. However, when it is loaded by `resource_loader.get_path_to_datafile('libgftrl_op.so'))`, then comes the error message\r\n`\r\ntensorflow.python.framework.errors_impl.NotFoundError: /opt/ml/job/python/jarvis/tensorflow/libgftrl_op.so: undefined symbol: _ZN10tensorflow31MaybeForwardRefInputToRefOutputEPNS_15OpKernelContextEii\r\n`\r\n\r\nI've searched throughout google but with the key word of `_ZN10tensorflow31MaybeForwardRefInputToRefOutputEPNS_15OpKernelContextEii` I found nothing. \r\n\r\n\r\n`\r\nnm libgftrl_op.so | grep _ZN10tensorflow31MaybeForwardRefInputToRefOutputEPNS_15OpKernelContextEii`\r\ncommand is also executed, with the output of \r\n`                  U _ZN10tensorflow31MaybeForwardRefInputToRefOutputEPNS_15OpKernelContextEii`\r\n\r\nHowever, there's no same record of `                  U _ZN10tensorflow31MaybeForwardRefInputToRefOutputEPNS_15OpKernelContextEii` in the `libtensorflow_framework.so`\r\n\r\nSo what on earth the meaning of `_ZN10tensorflow31MaybeForwardRefInputToRefOutputEPNS_15OpKernelContextEii`  is actually confuses me a lot", "comments": []}, {"number": 31965, "title": "Crash-course issue", "body": "#31958  URL(s) with the issue:\r\nhttps://developers.google.cn/machine-learning/crash-course/reducing-loss/video-lecture\r\n\r\n## Description of issue (what needs changing):\r\nOn 1:50,it prompts me to do the gradient-descent practice\uff0cwhen i click the button,then redirect to the wrong page.\r\n\r\n### Correct links\r\n\r\nhttps://developers.google.cn/machine-learning/crash-course/reducing-loss/gradient-descent\r\n", "comments": ["I believe you are referring to this;\r\n![image](https://user-images.githubusercontent.com/42785357/63890025-f5f0ff00-c996-11e9-9b83-ffbd7cb5e49b.png)\r\nThe link points to https://developers.google.cn/machine-learning/crash-course/fitter/graph which I think is correct because you are given exercises and solutions to practice.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31965\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31965\">No</a>\n", "May be u are right.But it's weird for a native Chinese speaker\r\n\r\n________________________________\r\n\u53d1\u4ef6\u4eba: Yasir Modak <notifications@github.com>\r\n\u53d1\u9001\u65f6\u95f4: 2019\u5e748\u670829\u65e5 4:32\r\n\u6536\u4ef6\u4eba: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\n\u6284\u9001: Marcus <anymre@outlook.com>; Author <author@noreply.github.com>\r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] Crash-course issue (#31965)\r\n\r\n\r\nI believe you are referring to this;\r\n[image]<https://user-images.githubusercontent.com/42785357/63890025-f5f0ff00-c996-11e9-9b83-ffbd7cb5e49b.png>\r\nThe link points to https://developers.google.cn/machine-learning/crash-course/fitter/graph which I think is correct because you are given exercises and solutions to practice.\r\n\r\n\u2015\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/31965?email_source=notifications&email_token=AIUC3UWVBTRRV5ONGHMVVUTQG3OHFA5CNFSM4IPLFPY2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5MLRLQ#issuecomment-525908142>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AIUC3UVXQJ34BFT7C7AGBV3QG3OHFANCNFSM4IPLFPYQ>.\r\n"]}, {"number": 31964, "title": "ERROR: opt-einsum requires Python '>=3.5' but the running Python is 2.7.15", "body": "I want upgrade tensorflow-gpu==2.0.0-beta to  tensorflow-gpu==2.0.0-rc0. I upgrade with the following command: \r\n`pip install tensorflow-gpu==2.0.0-rc0 -U`\r\n\r\nI got an error like this:\r\n\r\n```\r\n  Downloading http://pypi.sys.srv/root/pypi/%2Bf/8ab/a07af4cf80e86/opt_einsum-3.0.1.tar.gz (66kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 13.6MB/s \r\nERROR: opt-einsum requires Python '>=3.5' but the running Python is 2.7.15\r\n...\r\n```\r\nBut it is ok for last version. How to solve the problem?", "comments": ["`pip install opt-einsum==2.3.2` works for me.", "@honeytidy \r\nCan you please confirm if @xmfbit 's workaround is working for you.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31964\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31964\">No</a>\n", "pip install opt-einsum==2.3.2 works for me too. Thanks @xmfbit.", "> `pip install opt-einsum==2.3.2` works for me.\r\n\r\nworks for me, too"]}, {"number": 31963, "title": "Broken link in XLA page", "body": "All the link on the following page is forwarding to a 404 page.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/operation_semantics.md", "comments": ["@muller0 ,\r\nWhen tried accessing the links in the page mentioned, I was able to access without any issues.Can you check again and let me know.Thanks!", "@oanush\r\nSorry. My firewall was blocking the pages.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31963\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31963\">No</a>\n"]}, {"number": 31962, "title": "Tensorflow 2.0 tf.function internal error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab, Ubuntu, Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0-beta, 2.0-rc, 2.0-nightly (v1.12.1-9694-g006e2933 2.0.0-dev20190825)\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10, also without CUDA\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nDecorating the training loop that consumes a `tf.Data.Dataset` with `@tf.function` causes an internal error in tensorflow (an object is returned to Python with an error set and I'm unable to understand the actual origin of the error). Not using `@tf.function`, the code works alright.\r\n\r\n**Describe the expected behavior**\r\nThe model gradients should be calculated well regardless of being within a `@tf.function` trace or not.\r\n\r\n**Code to reproduce the issue**\r\nCode is provided in the [colab notebook available here](https://colab.research.google.com/drive/1LCZqyGa8mPjBS6KXOnEH_T7VOWoFpnnB)\r\n\r\n**Other info / logs**\r\nWhile working with a rather complex module that operates on irregular data (Graphs), using `@tf.function` worsens performance in TF 2.0 due to bug #29075. While following the workaround described in that issue, I stumbled upon this issue.\r\n\r\nBecause of these issues, TF 2.0 is not a good fit for DL on irregularly shaped data.\r\n\r\nRelevant traceback:\r\n```\r\n    <ipython-input-8-e24395a4965a>:137 train_step  *\r\n        gradients = tape.gradient(loss, model.trainable_variables)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py:1015 gradient\r\n        unconnected_gradients=unconnected_gradients)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/imperative_grad.py:76 imperative_grad\r\n        compat.as_str(unconnected_gradients.value))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py:599 _aggregate_grads\r\n        if len(gradients) == 1:\r\n\r\n    SystemError: <built-in function len> returned a result with an error set\r\n```", "comments": ["I was able to replicate the issue in TF version 2.0 preview. Thanks!", "I was also able to replicate this error in the nightly 2.0 preview.", "This is a very strange error, it seems to originate in the gradient calculation. It happens inside a `Dataset.reduce`, but I don't know if that's the reason it crashes; MainModel is fairly complex, though it doesn't seem to do anything dangerous as far as I can tell.\r\n\r\n@saxenasaurabh, could you have a closer look at the error? Perhaps we can detect its root cause and raise a more specific error?\r\n", "Any updates on this? I can confirm this only is happening if I use tf.function.\r\n\r\nI see this happening first:\r\n\r\n2020-01-01 20:59:20.099731: W tensorflow/core/common_runtime/shape_refiner.cc:89] Function instantiation has undefined input shape at index: 1211 in the outer inference context.\r\n2020-01-01 20:59:20.567981: W tensorflow/core/common_runtime/shape_refiner.cc:89] Function instantiation has undefined input shape at index: 1211 in the outer inference context.\r\n2020-01-01 20:59:21.052197: W tensorflow/core/common_runtime/shape_refiner.cc:89] Function instantiation has undefined input shape at index: 1211 in the outer inference context.\r\n2020-01-01 20:59:21.441673: W tensorflow/core/common_runtime/shape_refiner.cc:89] Function instantiation has undefined input shape at index: 1211 in the outer inference context.\r\n2020-01-01 20:59:21.757365: W tensorflow/core/common_runtime/shape_refiner.cc:89] Function instantiation has undefined input shape at index: 1211 in the outer inference context.\r\n\r\nMy input data is a tf.data.Dataset with a padded_batch. The only other thing I could think of is the Dataset supplies nested dictionaries.", "I'm also having this error", "This looks like a bug related to `GradientTape` and `Dataset`. A few workarounds:\r\n\r\n1. Move the @tf.function to `train_step` (so that the dataset iteration is outside the tf.function). That also seems to speed things up, which might be another bug.\r\n\r\n2. Use `tf.gradients` instead of `GradientTape`. The downside is that tf.gradients doesn't work in eager mode:\r\n\r\n```\r\ndef train_step(xs, ys):\r\n    ys_hat = model(xs, training=True)\r\n    loss = loss_fn(ys, ys_hat)\r\n    gradients = tf.gradients(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\n    train_loss(loss)\r\n    train_metric(ys.edges, ys_hat.edges)\r\n```", "@mdanatg For now I applied https://github.com/tensorflow/tensorflow/pull/33497 locally which fixes the problem. Currently I'm already doing 1, I can try doing 2 as well.", "I checked the colab posted by @mmv against tf-nightly and it looks like #33497 resolved the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31962\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31962\">No</a>\n"]}, {"number": 31961, "title": "TPU: Fix logging statement", "body": "Simple fix to include `tpu_name` in the logging statement, as the original author likely intended.\r\n\r\nCurrent output looks like this:\r\n```\r\nWARNING:tensorflow:TPU system %s has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\r\n```", "comments": []}, {"number": 31960, "title": "No description for / option to select non default cuda location when building TF2.0 from source", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\ncommit hash:\r\n553a3b826a55acb78de18ca6dbca8c965c7cf78e\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0rc0\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: none\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 5.4\r\n- CUDA/cuDNN version: 9.1 + 10.1\r\n- GPU model and memory: various\r\n\r\nOption/Description how to select a non-default cuda path missing.\r\n\r\nI am working on a shared server with cuda 9.1 in /usr/local/cuda/\r\nI want to build tensorflow 2.0 rc0 from source using a different cuda version in a different location.\r\n\r\nExecuting ./configure it automatically selects the default one and i do not have an option to select a different one.\r\n\r\nDocumentation\r\nhttps://www.tensorflow.org/install/source#tensorflow_2\r\nonly says: ```If your system has multiple versions of CUDA or cuDNN installed, explicitly set the version instead of relying on the default```\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`./configure`\r\n\r\n", "comments": ["Can you provide a log of this?\r\n\r\nThe configure script normally pauses to ask for a CUDA path, and only skips if you hold down enter. It also continues automatically if you have configured the `TF_CUDA_PATHS` environment parameter. Try setting that variable to your own custom path.", "sure, what kind of log do you want me to provide?\r\nsetting `TF_CUDA_PATHS` i was able to get it to chose the right path, but it will still not ask for a custom location", "The entire shell log of executing this would be helpful:\r\n\r\n```\r\n$ env | grep TF_\r\n$ ./configure (or however you've been executing it)\r\n```", "`$ env | grep TF_` shows nothing\r\noutput of `$ ./confgure` is attached\r\n\r\n[logoutput_tf.txt](https://github.com/tensorflow/tensorflow/files/3562657/logoutput_tf.txt)\r\n", "its a slurm managed cluster and I'm loading cuda 10.0 as an additional module if that helps", "It looks like `third_party/gpus/find_cuda_config.py` eagerly assumes that having CUDA installed in the system default configuration means that you want to use that CUDA installation. Since `TF_CUDA_PATHS` can be overridden, I'm closing this as WAI. We'd welcome a PR to improve the experience, though.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31960\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31960\">No</a>\n", "I don't think this is a good solution. I'd at least mention overriding `TF_CUDA_PATHS` in the docs somewhere."]}, {"number": 31959, "title": "Issue #25802 Updating API docs of tf.math.acos", "body": "#25802", "comments": ["Just a nit: please use a proper title for the PR and link to the issue in the first comment, to make it easier to understand what it is being done.", "Okay, will keep that in mind next time. Thanks\n\nOn Mon 26 Aug, 2019, 9:36 PM Mihai Maruseac, <notifications@github.com>\nwrote:\n\n> Just a nit: please use a proper title for the PR and link to the issue in\n> the first comment, to make it easier to understand what it is being done.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/31959?email_source=notifications&email_token=ALIGAS5QVR6OEF7UKB326DDQGP5QZA5CNFSM4IPJ4OLKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5EZ5WY#issuecomment-524918491>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALIGASZE7HEZN36JCUVMBNLQGP5QZANCNFSM4IPJ4OLA>\n> .\n>\n", "ping @mihaimaruseac for review ? Thanks!", "@HarshSulakhe  Could you please check failed build errors? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 31958, "title": "Issue #25846 Updating the API docs of the tf.math.add function", "body": "#25846", "comments": ["Just a nit: please use a proper title for the PR and link to the issue in the first comment, to make it easier to understand what it is being done.", "@HarshSulakhe Could you please check reviewer comments and keep us posted. Thanks!", "Sorry for the delay and a large number of commits, I'm a little new to Git, messed up a bit.\r\nHave made the changes that were requested.", "@HarshSulakhe Could you please check reviewer comments and keep us posted. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Closing as stale."]}, {"number": 31957, "title": "Eager mode: Accessing contents of scalars in a tf.function", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Any\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to parallel download some images. I am using a `tf.data.Dataset` with the image urls as content. I want to store them in a GCS, so I am using functions from the `tf.io.gfile` package inside a `tf.function`. This function will be called through `tf.data.Dataset.map`.\r\n\r\nWhen the different `tf.io.gfile` functions are called inside the `tf.function`, like `makedirs`, it raises an Error indicating that it requires a binary or unicode string as input:\r\n\r\n```TypeError: Expected binary or unicode string, got <tf.Tensor 'StringJoin_1:0' shape=() dtype=string>```\r\n\r\nIf I try to use `.numpy()`, it is not available, as expected. The result is that I cannot download the images in the GCS.\r\n\r\n**Describe the expected behavior**\r\n\r\nAs a tensorflow package, I would expect the `tf.io.gfile` functions to allow the use of scalar string tensors, or I would expect tensorflow to provide a solution similar to the `.numpy()` function inside `tf.function` for these cases. If not, at least it should be a warning in the documentation that these functions cannot be used inside `tf.function`.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\n@tf.function\r\ndef test_string(value): \r\n    return tf.io.gfile.exists(value)\r\n\r\ntest_string(tf.constant('test'))\r\n```\r\n\r\nThe result in this case is:\r\n\r\n```\r\nTypeError: Expected binary or unicode string, got <tf.Tensor 'value:0' shape=() dtype=string>\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Was reproduced the issue with Tensorflow 1.14.0. Please see the gist [here](https://colab.research.google.com/drive/1EKMMLXpMreIz5_w3qTWKtodnbnBDydwZ). Thanks!", "gfile is a python implementation (through ` pywrap_tensorflow`), not a kernel ops. So it will not work in graph mode, that is why it is not working with tf.function and tf.data I believe.\r\n\r\n`tf.io.read_file` and `tf.io.write_file` are kernel ops so I they are compatible with tf.function and tf.data I think.", "@yongtang \r\n\r\nFollowing your advice, I have made another test using the `tf.io.write_file` function:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\n@tf.function\r\ndef test_function(name): \r\n    tf.io.write_file(name, tf.constant('2'))\r\n\r\ntest_function(tf.constant('hi.txt'))\r\n\r\nprint(open('hi.txt').read())\r\n```\r\n\r\nThe result is a:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-15-7e9fdc2b4e62> in <module>\r\n----> 1 print(open('hi.txt').read())\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: 'hi.txt'\r\n```\r\n\r\nIf I do exactly the same, but without the `@tf.function` decorator:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\ndef test_function(name): \r\n    tf.io.write_file(name, tf.constant('2'))\r\n\r\ntest_function(tf.constant('hi.txt'))\r\n\r\nprint(open('hi.txt').read())\r\n```\r\n\r\nThe result is the one expected:\r\n\r\n```2```\r\n\r\nI assume I am making something wrong, but sincerely I do not know what.\r\n\r\nIn any case, thanks for your comment.", "@jmgc I ran into the same erorr with TF 1.14.\r\n\r\nHowever, it looks like on TF 2.0.0RC0 it works fine (runs on colab) with the following:\r\n\r\n```python\r\n!pip install tensorflow==2.0.0rc0\r\n\r\nimport tensorflow as tf\r\n\r\nprint(tf.version.VERSION)\r\n\r\n#tf.enable_eager_execution()\r\n\r\n@tf.function\r\ndef test_function(name): \r\n    tf.io.write_file(name, tf.constant('2'))\r\n\r\ntest_function(tf.constant('hi.txt'))\r\n\r\nprint(open('hi.txt').read())\r\n```", "@jmgc I think this was resolved in `tf-nightly`. I ran your code without any issues with `!pip install tf-nightly`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/ca5f8eb7db941e4bc6b69ba04ced0dfb/tf_31957.ipynb). Thanks!", "@jvishnuvardhan it is correct that it works for the `tf.io.read_file` and `tf.io.write_file`. However, it does still not work with the `tf.io.gfile` functions.", "@jmgc Please check @yongtang reason [here](https://github.com/tensorflow/tensorflow/issues/31957#issuecomment-524909757). Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31957\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31957\">No</a>\n"]}, {"number": 31956, "title": "LossScaleOptimizer does not work", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GTX 1080Ti\r\n\r\n**Describe the current behavior**\r\nI am trying to run the sample code from [https://www.tensorflow.org/api_docs/python/tf/contrib/mixed_precision/LossScaleOptimizer](https://www.tensorflow.org/api_docs/python/tf/contrib/mixed_precision/LossScaleOptimizer) and get the following error when no gradient can be computed for some variables:\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\nC:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    526                 as_ref=input_arg.is_ref,\r\n--> 527                 preferred_dtype=default_dtype)\r\n    528           except TypeError as err:\r\n\r\nC:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)\r\n   1223     if ret is None:\r\n-> 1224       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1225 \r\n\r\nC:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    304   _ = as_ref\r\n--> 305   return constant(v, dtype=dtype, name=name)\r\n    306 \r\n\r\nC:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in constant(value, dtype, shape, name)\r\n    245   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 246                         allow_broadcast=True)\r\n    247 \r\n\r\nC:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    283           value, dtype=dtype, shape=shape, verify_shape=verify_shape,\r\n--> 284           allow_broadcast=allow_broadcast))\r\n    285   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n\r\nC:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n    453     if values is None:\r\n--> 454       raise ValueError(\"None values not supported.\")\r\n    455     # if dtype is provided, forces numpy array to be the type\r\n\r\nValueError: None values not supported.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\nC:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    540               observed = ops.internal_convert_to_tensor(\r\n--> 541                   values, as_ref=input_arg.is_ref).dtype.name\r\n    542             except ValueError as err:\r\n\r\nC:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)\r\n   1223     if ret is None:\r\n-> 1224       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1225 \r\n\r\nC:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    304   _ = as_ref\r\n--> 305   return constant(v, dtype=dtype, name=name)\r\n    306 \r\n\r\nC:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in constant(value, dtype, shape, name)\r\n    245   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 246                         allow_broadcast=True)\r\n    247 \r\n\r\nC:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    283           value, dtype=dtype, shape=shape, verify_shape=verify_shape,\r\n--> 284           allow_broadcast=allow_broadcast))\r\n    285   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n\r\nC:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n    453     if values is None:\r\n--> 454       raise ValueError(\"None values not supported.\")\r\n    455     # if dtype is provided, forces numpy array to be the type\r\n\r\nValueError: None values not supported.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-5d2950c170d2> in <module>\r\n     13 \r\n     14 # Call minimize() on the loss scale optimizer.\r\n---> 15 train_op = loss_scale_optimizer.minimize(loss)\r\n\r\nC:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\r\n    411 \r\n    412     return self.apply_gradients(grads_and_vars, global_step=global_step,\r\n--> 413                                 name=name)\r\n    414 \r\n    415   def compute_gradients(self, loss, var_list=None,\r\n\r\nC:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\mixed_precision\\python\\loss_scale_optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)\r\n    148     is_finite_grad = []\r\n    149     for g in grads:\r\n--> 150       is_finite_grad.append(math_ops.reduce_all(gen_math_ops.is_finite(g)))\r\n    151     is_overall_finite = math_ops.reduce_all(is_finite_grad)\r\n    152 \r\n\r\nC:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py in is_finite(x, name)\r\n   4919   try:\r\n   4920     _, _, _op = _op_def_lib._apply_op_helper(\r\n-> 4921         \"IsFinite\", x=x, name=name)\r\n   4922   except (TypeError, ValueError):\r\n   4923     result = _dispatch.dispatch(\r\n\r\nC:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    543               raise ValueError(\r\n    544                   \"Tried to convert '%s' to a tensor and failed. Error: %s\" %\r\n--> 545                   (input_name, err))\r\n    546             prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\r\n    547                       (input_name, op_type_name, observed))\r\n\r\nValueError: Tried to convert 'x' to a tensor and failed. Error: None values not supported.\r\n\r\n```\r\n**Describe the expected behavior**\r\nNo error would occur for some other optimizers such as AdamOptimizer and MovingAverageOptimizer, even if no gradient can be computed for some variables.\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\na1=tf.Variable(1., name='a1')\r\na2=tf.Variable(2., name='a2')\r\n\r\nmodel_params = [var for var in tf.global_variables() if 'a' in var.name]\r\nloss = a1**2\r\nopt = tf.train.AdamOptimizer(learning_rate=.1, beta1=0., beta2=0.9)\r\n\r\n# Choose a loss scale manager which decides how to pick the right loss scale\r\n# throughout the training process.\r\nloss_scale_manager = tf.contrib.mixed_precision.FixedLossScaleManager(5000)\r\n\r\n# Wraps the original optimizer in a LossScaleOptimizer.\r\nloss_scale_optimizer =tf.contrib.mixed_precision.LossScaleOptimizer(opt, loss_scale_manager)\r\n\r\n# Call minimize() on the loss scale optimizer.\r\ntrain_op = loss_scale_optimizer.minimize(loss, var_list=model_params)\r\n```\r\n", "comments": ["Sounds quite similar to #31953 ", "@tsc2017 ,\r\nCan you also refer the similar issues #[783](https://github.com/tensorflow/tensorflow/issues/783) and #[17](https://github.com/jacobgil/keras-grad-cam/issues/17).Thanks!", "@oanush Thanks for the information.\r\nNow I figure out that the problem can be fixed by removing pairs of grad and var in which grad is None:\r\n\r\n```\r\nimport tensorflow as tf\r\na1=tf.Variable(1., name='a1')\r\na2=tf.Variable(2., name='a2')\r\n\r\nmodel_params = [var for var in tf.global_variables() if 'a' in var.name]\r\nloss = a1**2\r\nopt = tf.train.AdamOptimizer(learning_rate=.1, beta1=0., beta2=0.9)\r\n\r\n# Choose a loss scale manager which decides how to pick the right loss scale\r\n# throughout the training process.\r\nloss_scale_manager = tf.contrib.mixed_precision.FixedLossScaleManager(5000)\r\n\r\n# Wraps the original optimizer in a LossScaleOptimizer.\r\nloss_scale_optimizer = tf.contrib.mixed_precision.LossScaleOptimizer(opt, loss_scale_manager)\r\n\r\n# Compute gradients\r\ngrads_and_vars = loss_scale_optimizer.compute_gradients(loss, var_list=model_params, colocate_gradients_with_ops=True)\r\n\r\n# Remove irrelevant (grad, var) pairs\r\ngrads_and_vars = [(grad, var) for grad, var in grads_and_vars if grad is not None]\r\n\r\n# Call apply_gradients() on the loss scale optimizer.\r\ntrain_op = loss_scale_optimizer.apply_gradients(grads_and_vars)\r\n```\r\n\r\nStill, I have doubt whether the source of LossScaleOptimizer should be updated so that its behavior is consistent with the other optimizers.", "I could reproduce the issue. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/814b3120591a9ca49aee1def82439599/tf_31956.ipynb) is the gist. Thanks!", "There was a technical decision made that the users should pass in valid trainable variables to avoid gradient to be None.", "This looks like a bug. However, `contrib` is being removed from TF 2.0, and so the contrib LossScaleOptimizer is deprecated and no longer maintained.\r\n\r\nYou can use a `tf.keras.mixed_precision.experimental.LossScaleOptimizer` with a tf.keras optimizer. Or in TF 1, a `tf.train.experimental.MixedPrecisionLossScaleOptimizer` with a non-keras optimizer.\r\n\r\nI'm closing this issue, as even if it is fixed, the fix will never make it to a stable version of TensorFlow.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31956\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31956\">No</a>\n"]}, {"number": 31955, "title": "[INTEL MKL] Fixes for regressions in unit tests.", "body": "This PR fixes a number of regressions caused by this commit https://github.com/tensorflow/tensorflow/commit/da3f7b14fff64c492a93305ccc98b70fafdd9dde  when MKL DNN is enabled. \r\n pywrap_tensorflow depends on a number of prebuilt shared libraries when MKL DNN is enabled, so importing of _pywrap_utils fails if pywrap_tensorflow has not already been imported.  Fixed this by switching the order of imports or adding imports of pywrap_tensorflow when needed.", "comments": ["Hi  @gbaned,  it looks like the PR is stuck at importing. Can you check the status?"]}, {"number": 31954, "title": "TPUStrategy with Keras in TF2.0.0rc0", "body": "Hi,\r\n\r\nAlthough the description of TF 2.0.0rc0 contains this\r\n _\"Distribution Strategy: TF 2.0 users will be able to use the tf.distribute.Strategy API to distribute training with minimal code changes, yielding great out-of-the-box performance. It supports distributed training with Keras model.fit, as well as with custom training loops. Multi-GPU support is available, along with experimental support for multi worker and Cloud TPUs. Check out the guide for more details.\"_\r\n\r\nWhen I try to implemented it with Keras and multiple TPU units, crashes with _\"NotImplementedError: Using multiple TPUs in a single session is not yet implemented\"_\r\n\r\nSo, is supported TPUStrategy with keras in this release or not?\r\n\r\nThank you\r\n", "comments": ["@nsantavas, will it be possible to provide sample code to investigate the issue. Thanks!", "@gadagashwini \r\n```\r\n################ TPU Initialization ##################\r\ntpu='tf2'\r\ntpu1='tf2-2'\r\n\r\ncluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\r\n    tpu=[tpu, tpu1])\r\ntf.config.experimental_connect_to_host(cluster_resolver.master())\r\ntf.tpu.experimental.initialize_tpu_system(cluster_resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\r\n\r\n######################################################\r\nwith strategy.scope():\r\n    model=create_model()\r\n\r\nmodel.fit(get_training_dataset, validation_data=get_validation_dataset, steps_per_epoch=steps_per_epoch ,validation_steps=val_steps, epochs=EPOCHS, verbose=1, initial_epoch=0, callbacks=clbk)\r\n\r\n```\r\n", "TPUStrategy supports a single TPU or a slice of TPU pod etc. It does not support 2 independent TPUs. If you want to use more TPU resources, you should use a bigger slice instead.\r\n@rxsang - can you confirm? Do we also need to clarify this in the docs somewhere?", "That's true. It is an error raised in TPUClusterResolver. We don't support connecting to independent TPUs.", "Closing this issue since its answered. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31954\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31954\">No</a>\n"]}, {"number": 31953, "title": "TF2.0 from source crashes with optimizer specified as instance", "body": "**System information**\r\n- Have I written custom code: **yes**\r\n- OS Platform and Distribution: **OpenSUSE Tumbleweed**\r\n- TensorFlow installed from: **source**\r\n- TensorFlow version: **v2.0.0-rc0-0-gc75bb66a99 2.0.0-rc0**\r\n- Python version: **3.7.3**\r\n- Bazel version: **0.26**\r\n- GCC/Compiler version: **gcc (SUSE Linux) 9.1.1 20190805 [gcc-9-branch revision 274114]**\r\n- CUDA/cuDNN version: **-**\r\n- GPU model and memory: **gfx803 - Ellesmere - AMD Radeon RX 580 - 8192MB**\r\n\r\n**Describe the current behavior**\r\nWhen the optimizer is specified in the form of a string (e.g. `adam`) in `model.compile(...)`, everything works as expected. When the optimizer is specified with non-default parameters, like:\r\n```python\r\nmodel.compile(loss='mse', optimizer=optimizers.Adam(lr=0.01337))\r\n```\r\nthe application crashes with the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 527, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 286, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 227, in constant\r\n    allow_broadcast=True)\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 265, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 437, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 541, in _apply_op_helper\r\n    values, as_ref=input_arg.is_ref).dtype.name\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 286, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 227, in constant\r\n    allow_broadcast=True)\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 265, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 437, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/seiji/.vscode/extensions/ms-python.python-2019.8.30787/pythonFiles/ptvsd_launcher.py\", line 43, in <module>\r\n    main(ptvsdArgs)\r\n  File \"/home/seiji/.vscode/extensions/ms-python.python-2019.8.30787/pythonFiles/lib/python/ptvsd/__main__.py\", line 432, in main\r\n    run()\r\n  File \"/home/seiji/.vscode/extensions/ms-python.python-2019.8.30787/pythonFiles/lib/python/ptvsd/__main__.py\", line 316, in run_file\r\n    runpy.run_path(target, run_name='__main__')\r\n  File \"/usr/lib64/python3.7/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"/usr/lib64/python3.7/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"/usr/lib64/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/mnt/data/Documents/seiji.li/FHWS/Master/SS2019/Masterarbeit/workspace/Autoencoder_Example/autoencoder_conv3.py\", line 28, in <module>\r\n    autoencoder.fit(train_features, train_features, verbose=1, shuffle=True)\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 734, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 674, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 189, in model_iteration\r\n    f = _make_execution_function(model, mode)\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 565, in _make_execution_function\r\n    return model._make_execution_function(mode)\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2149, in _make_execution_function\r\n    self._make_train_function()\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2081, in _make_train_function\r\n    params=self._collected_trainable_weights, loss=self.total_loss)\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizers.py\", line 476, in get_updates\r\n    grads = self.get_gradients(loss, params)\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizers.py\", line 92, in get_gradients\r\n    if None in grads:\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\", line 1283, in tensor_equals\r\n    return gen_math_ops.equal(self, other)\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\", line 3629, in equal\r\n    \"Equal\", x=x, y=y, name=name)\r\n  File \"/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 545, in _apply_op_helper\r\n    (input_name, err))\r\nValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.\r\nTerminated\r\n```\r\nThis **only** happens, when using the tensorflow compiled from source. The version `2.0.0b1` as distributed through pip works as expected.\r\nI have tested Tensorflow compiled from source **with** and **without** ROCm enabled. Both crash.\r\n\r\n**Describe the expected behavior**\r\nSpecifying an optimizer as variable instead of with a string and default parameters should work.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow\r\nfrom tensorflow.python.keras.layers import Input, Dense\r\nfrom tensorflow.python.keras.models import Sequential\r\nfrom tensorflow.python.keras import backend as K\r\nfrom tensorflow.python.keras import optimizers\r\n\r\noriginal_dim = 160\r\nintermediate_dim = 32\r\n# load data\r\ntrain_features = np.random.rand(100000, original_dim).astype(np.float32)\r\ntest_features = np.random.rand(10000, original_dim).astype(np.float32)\r\n\r\n############# BUILD MODEL #############\r\nautoencoder = Sequential()\r\nautoencoder.add( Input(shape=(original_dim,)) )\r\nautoencoder.add( Dense(intermediate_dim) )\r\nautoencoder.add( Dense(original_dim) )\r\nautoencoder.build()\r\nautoencoder.summary()\r\n\r\n# crashes:\r\nautoencoder.compile(loss='mse', optimizer=optimizers.Adam(lr=0.01337))\r\n#works:\r\n#autoencoder.compile(loss='mse', optimizer='adam')\r\n\r\n# Train model\r\nautoencoder.fit(train_features, train_features, verbose=1, shuffle=True)\r\n```", "comments": ["So the problem is that you access v1 style optimizer. If you use `tf.keras.optimizers.Adam` instead, you can get an `OptimizerV2` instance, which works fine in your example. My best practice is not to access private API, and try to run codes with public endpoints like `tf.*, tf.keras.*`. Workable colab is here: https://colab.research.google.com/drive/1ZlUU0fSmxtdBtJ3L4jdMOhI8WpbnZOnB\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizers.py\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras/optimizer_v2", "That was the problem. Should be more carefull, copying code from Stackoverflow next time. :sweat_smile: \r\nThank you!", "@seijikun Hi. Just out of curiosity, how do you run TF with AMD GPU acceleration on openSUSE TW ? Within a docker container or did you managed to compile and installed all ROCm stuff locally ? If the latter is the case, I definitely need to know how  :flushed:", "@saitam757 I am using it without docker. That's actually pretty easy. AMD provides RPM packages. Those are not meant for openSUSE (when installing normally, they will complain that stuff like the libc is missing). But if you install them like in this blog-post: https://www.clarenceho.net/2019/05/rocm-opencl-with-opensuse-tumbleweed.html, everything just works.", "@seijikun Thank you very much for this information. Just one question: For getting tensorflow to run you need all the libraries installed up to miopen. I guess, all rpms are installed in the same way ? Without dependencies ? Again, thanks a lot.\r\n", "@saitam757 I compiled tensorflow with ROCm myself. When doing that, the build fails one after another, stating what's missing. Packages that I remember installing are:\r\n- rocrand\r\n- rocfft\r\n- hsa-amd-aqlprofile\r\n- rocm-libs\r\n- rocm-device-libs\r\n- rocm-dev\r\n- hip_hcc\r\n- hip_base\r\n- miopen-hip", "@seijikun The configuration sounds exciting. :smiley: Compiled for Fedora, with all those dependencies. Thanks for these information. "]}, {"number": 31952, "title": "[TF 2.0] tf.gather doesn't work alongside @tf.function", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nDarwin Kernel Version 18.6.0\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n2.0.0-dev20190730\r\n- Python version:\r\nPython 3.6.8 :: Anaconda, Inc.\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n\r\n**Describe the current behavior**\r\nIt seems that when `tf.gather()` is called after a `tf.function`, the gradient cannot be calculated. The example code blow shows the bug. The code itself raises the following error message:\r\n\r\n> AssertionError: Expected all args to be Tensors or Variables; but got CompositeTensor\r\n\r\nThe code will work if we remove the `tf.function` decorator, or  put the `tf.gather` line inside the `tf.funtion` graph.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = tf.cast(np.random.randn(100, 100), tf.float32)\r\nz = tf.cast(np.random.randn(1, 100), tf.float32)\r\n\r\nlayer = tf.keras.layers.Dense(100)\r\n\r\n@tf.function  # <- removing this and the code works fine\r\ndef fun(x, layer):\r\n    y = layer(x)\r\n    return y\r\n\r\nwith tf.GradientTape() as tape:\r\n    y = fun(x, layer)\r\n    y = tf.gather(y, [0])  # if we put this line inside the function it works fine\r\n    loss = tf.norm(y - z)\r\n\r\ngrads = tape.gradient(loss, layer.trainable_variables)\r\n```\r\n\r\n", "comments": ["I have tried on Colab with TF version 2.0.0-dev20190730, recent nightly version 2.0.0-dev20190825 and was able to reproduce the issue.Please, find the [gist](https://colab.research.google.com/drive/1P4tejxmGIxKmXM4TFoesuaXTZUEsPpdx) here.Thanks!", "Hi,\r\nWe encountered the same bug, which currently prevents our migration from TF1 to TF2.\r\nAs David-mao said, it works perfectly well in Eager mode.\r\nThe problem arises only when calling tf.gather on tensors returned from a tf.function, and then calculating its gradients.", "@diNatale I found a very ugly workaround for this. You wrap `tf.gather` into a graph function:\r\n\r\n```\r\n\r\n@tf.function\r\ndef gather(x, ind):\r\n    return tf.gather(x + 0, ind)\r\n```\r\nand use this `gather` instead of `tf.gather` in your code.  I know it's absurd (the most absurd part is to have `+ 0` inside it, which is necessary for reasons unclear to me), but it works in my cases.\r\n\r\n\r\n", "Wow!\r\n\"+ 0\" - of course, how didn't we guess that :)\r\nIt does work for me if I use **both** the wrapper and the +0\r\nIs there any explanation for this behaviour?\r\n\r\nthanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31952\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31952\">No</a>\n"]}, {"number": 31951, "title": "No Brier Score Loss Function", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.14.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nhttps://en.wikipedia.org/wiki/Brier_score\r\n\r\n**Will this change the current api? How?**\r\nAdd support for calculating Brier Score\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to utilize Brier Score\r\n\r\n**Any Other info.**\r\n", "comments": ["Resolved in https://github.com/tensorflow/tensorflow/pull/31950"]}, {"number": 31950, "title": "Add brier score loss to tf.losses", "body": "https://en.wikipedia.org/wiki/Brier_score", "comments": []}, {"number": 31949, "title": "[TensorFlow Lite] GPU experimental does not work with converted SSD model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no (for the demo APPs);\r\nyes (for the object detection APP), but only added GPU delegates and used a self-converted SSD float model\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy): Google Pixel 2 XL (Android 8.0.0 and Android 9)\r\n- TensorFlow Lite version: GPU experimental 0.0.1 (I tested version 0.0.0 as well)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 64 bit\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0 / 7.6.0\r\n- GPU model and memory: GeForce GTX 970, AMD64\r\n\r\n**Describe the current behavior**\r\nI only got to run the demo APP on GPU with GPU experimental but not the object detection APP. For the object detection APP I had to add the GPU delegates, change some code, and use a self-converted SSD model. \r\n\r\nThe error occurs because of the converted SSD model. `GPU delegate does not support TFLite_Detection_PostProcess`. But this option is necessary to convert an SSD model successfully (with 4 outputs). This is the way I convert the ssdlite mobilenet model from model zoo: https://github.com/tensorflow/tensorflow/issues/31015#issuecomment-517165233. This is also the way which was recommended to me, because the conversion did not work with the frozen model provided by model zoo (or by getting the frozen model with the inference_graph script).\r\n\r\n**Describe the expected behavior**\r\nI want to run the object detection APP on GPU by using GPU experimental with a self-converted SSD model.\r\n\r\n**Code to reproduce the issue**\r\nI changed the current demo APP to GPU experimental and I had to do some minor changes in the code to get the APP running successfully on GPU. (To know which changes to make, I looked into the demo APP version r1.13.)\r\n\r\nFor the object detection APP I added the GPU delegates, changed isModelQuantized to false (and don't use isQuantized anymore), and use a self-converted SSD mobilenet model (because the APP is currently written for quantized models using CPU and doesn\u2019t have GPU delegates in the original code). I wrote the delegates as described in https://www.tensorflow.org/lite/performance/gpu_advanced#android_java and I converted the model this way: #31015 (comment). Then, I applied the same changes to the object detection APP as I did to the current demo APP before (by using the knowledge of demo APP version r1.13).\r\n\r\n- Current demo APP: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo\r\n- Demo APP version r1.13: https://github.com/tensorflow/tensorflow/tree/r1.13/tensorflow/lite/java/demo\r\n- Object Detection APP: https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android\r\n\r\n\r\n**Performance results tested on Google Pixel 2 XL (with Android 8.0.0 and Android 9):**\r\n\r\nDemo APP version r1.13\r\n- Quantized Model + CPU -> 80ms\r\n- Float Model + CPU -> 90ms\r\n- Float Model + GPU -> **38ms**\r\n\r\nCurrent demo APP\r\n- Quantized Model + CPU -> 80ms\r\n- Float Model + CPU -> 150ms\r\n- Float Model + GPU -> **28ms**\r\n\r\nObject Detection APP\r\n- (self-converted) Float Model + GPU -> **ERROR**\r\n\r\n\r\n**Full error**\r\nWhen running the object detection APP with GPU experimental 0.0.1 and a converted SSDlite mobilenet model.\r\n\r\n```\r\nE/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.detection, PID: 4159\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Next operations are not supported by GPU delegate:\r\n    CUSTOM TFLite_Detection_PostProcess: Operation is not supported.\r\n    First 114 operations will run on the GPU, and the remaining 1 on the CPU.TfLiteGlDelegate Invoke: Delegate should run on the same thread where it was initialized.Node number 115 (TfLiteGlDelegate) failed to invoke.\r\n\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:149)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:214)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:181)\r\n        at android.os.Handler.handleCallback(Handler.java:789)\r\n        at android.os.Handler.dispatchMessage(Handler.java:98)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n```\r\n\r\n(Note: With GPU experimental 0.0.0 it neither works but the camera keeps running, the model is not used, and therefore, nothing will be detected. No error occurs and only by debugging the APP, you can find out when the APP stops working properly. It\u2019s in the same line as before with 0.0.1 but the APP doesn't display any error. Therefore, 0.0.1 should be used in order to see what's going wrong.)\r\n\r\n\r\n**Questions**\r\n- Is there another way to convert an SSD model successfully (with 4 outputs) which then works with GPU delegate?\r\n- Will GPU delegate support TFLite_Detection_PostProcess in the near future? When would this be approximately?\r\n- Is there another way to solve this problem? - The goal is using the object detection APP on GPU with a self-converted SSD model (preferred with GPU experimental). (I couldn't get GPU nightly to work properly neither, the APP seems to run on CPU: https://github.com/tensorflow/tensorflow/issues/31948)\r\n\r\nThanks in advance!", "comments": ["The post-processing part, implemented as a custom op is not supported by GPU Delegate. That's why you saw\r\n```\r\n   CUSTOM TFLite_Detection_PostProcess: Operation is not supported.\r\n   First 114 operations will run on the GPU, and the remaining 1 on the CPU.\r\n```\r\nin messages you showed. If it goes well, that's not a problem. And, the custom op is supposed to be run on CPUs, that is falling back to TFLite CPU interpreter. However, it seems there was something wrong. Did you check that when calling Interpreter::ModifyGraphWithDelegate() or Interpreter::Invoke(), the caller has an EGLContext in the current thread and Interpreter::Invoke() is called from the same EGLContext. In your messages, there are\r\n```\r\nTfLiteGlDelegate Invoke: Delegate should run on the same thread where it was initialized.Node number 115 (TfLiteGlDelegate) failed to invoke.\r\n```\r\n\r\nThe constraint is said in [Advance GPU](https://www.tensorflow.org/lite/performance/gpu_advanced)\r\n```\r\nWhen calling Interpreter::ModifyGraphWithDelegate() or Interpreter::Invoke(), the caller must have an EGLContext in the current thread and Interpreter::Invoke() must be called from the same EGLContext. If an EGLContext does not exist, the delegate will internally create one, but then the developer must ensure that Interpreter::Invoke() is always called from the same thread in which Interpreter::ModifyGraphWithDelegate() was called.\r\n```", "Thank you for the explanation! Then the real issue is how to get interpreter.run() to run on the same thread where it was initialized. \r\n\r\nThe interpreter `tfLite` is initialized in `TFLiteObjectDetectionAPIModel.create()` but `tfLite.runForMultipleInputsOutputs(inputArray, outputMap)` is used in `TFLiteObjectDetectionAPIModel.recognizeImage()` which is called in a separate thread.\r\n\r\nFor testing purpose only, I initialized the interpreter with its GPU delegate new for every frame (in `recognizeImage()`) and this way the error disappears. But of course, this is not an option. The interpreter should only be initialized once at the beginning and interpreter.run() has to be called in the same thread.\r\n\r\nI read https://github.com/tensorflow/tensorflow/issues/25657 and I\u2019m trying to make it work. Is it easier with or without SSBO?", "@freedomtan Thanks for the quick response :D\r\n\r\n@val9299 It's probably easier with the SSBO.  Once you get the correctness right, i.e. when everything works as intended, I would switch to SSBO for better performance.  It's up to you though :)", "The object detection APP works now with GPU experimental, but I get only 75 ms per frame. Is this normal because it jumps back to CPU for TFLite_Detection_PostProcess or could it be better even without SSBO? I will try SSBO next :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31949\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31949\">No</a>\n", "@val9299 I was able to get ~ 45 ms per frame on Pixel 2 using benchmark_model or [my dirty benchmark](https://github.com/freedomtan/glDelegateBench). But that's on rooted device with most system components running at maximum frequencies and model with post processing. As far as I can remember, the TFLite post-processing custom op should take less than 5 ms. That is, it should be possible to get ~ 50 ms per frame.", "@freedomtan Thank you for this nice APP! I get 55 ms for my model with your APP, so I have to find out how to get the same performance with the object detection APP.\r\nWith ssdlite mobilenet I only get 60-65 ms.\r\n\r\nAnd I see this warining while running the APP: `E/libEGL: call to OpenGL ES API with no current context (logged once per thread)\r\n`", "> @freedomtan Thank you for this nice APP! I get 55 ms for my model with your APP, so I have to find out how to get the same performance with the object detection APP.\r\n> With ssdlite mobilenet I only get 60-65 ms.\r\n> \r\n> And I see this warining while running the APP: `E/libEGL: call to OpenGL ES API with no current context (logged once per thread) `\r\n\r\nhello, do you know what's the meaning of \" E/libEGL: call to OpenGL ES API with no current context\" ? and i use gpu delegate, seems the output is not corresponding to without delegate", "could you share youre code? @val9299 ", "I also have same error when run using virtual devices pixel2 in AS.\r\n```\r\nE/libEGL: call to OpenGL ES API with no current context (logged once per thread)\r\nCUSTOM TFLite_Detection_PostProcess: Operation is not supported.\r\n   First 114 operations will run on the GPU, and the remaining 1 on the CPU.\r\n```\r\n\r\nHave anyone have some solutions? @freedomtan @val9299", "@zychen2016: \r\nUnfortunately the code belongs to a company and I cannot share the research. Furthermore, I'm not working on it anymore and I didn't solve the \"E/libEGL\" problem. \r\n\r\nThe \"CUSTOM TFLite_Detection_PostProcess: [...]\" warning will always stand here when using the TFLite PostProcess (until TFLite supports this operation). If you didn't want this, you couldn't use PostProcess.", "> @zychen2016:\r\n> Unfortunately the code belongs to a company and I cannot share the research. Furthermore, I'm not working on it anymore and I didn't solve the \"E/libEGL\" problem.\r\n> \r\n> The \"CUSTOM TFLite_Detection_PostProcess: [...]\" warning will always stand here when using the TFLite PostProcess (until TFLite supports this operation). If you didn't want this, you couldn't use PostProcess.\r\n\r\nThank you! But could you tell me how to replace TFLite PostProcess with other Op?\r\n@val9299\r\n", "@freedomtan\r\n\r\nI have run your glDelegateBench in Android Studio using virtual devices Pixel2,But report this error\r\n\r\n```\r\nE/eglCodecCommon: glUtilsParamSize: unknow param 0x000082da\r\nE/eglCodecCommon: glUtilsParamSize: unknow param 0x000082da\r\nE/libEGL: call to OpenGL ES API with no current context (logged once per thread)\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n                  Process: com.mediatek.gldelegatebench, PID: 6701\r\n                  java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: OpenCL library not loaded - dlopen failed: library \"libOpenCL-pixel.so\" not found\r\n                  Falling back to OpenGL\r\n                  TfLiteGpuDelegate Init: [GL_INVALID_ENUM]: An unacceptable value is specified for an enumerated argument.: glGetBufferParameteri64v in tensorflow/lite/delegates/gpu/gl/gl_buffer.cc:46\r\n                  TfLiteGpuDelegate Prepare: delegate is not initialized\r\n                  Node number 31 (TfLiteGpuDelegateV2) failed to prepare.\r\n``` \r\n\r\nAnd if use Gpu to test , app will flash and be closed.", "@zychen2016 there are two problems:\r\n\r\n1. 'dlopen failed: library \"libOpenCL-pixel.so\"': this should be fine. Currently GPU Delegate tries first OpenCL and then OpenGL ES Compute Shader. Pixel 2 doesn't have OpenCL binaries. That's why you saw the message.\r\n2. I don't know if OpenGL ES Computer Shader is supported in Android SDK emulator. If no, this explains everything. If YES, congratulation, you get a change to improve either the GPU delegate or  the emulator."]}, {"number": 31948, "title": "[TensorFlow Lite] GPU delegates on Android with GPU nightly fail to run on GPU (it seems to run on CPU)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\nno (for the demo APP & classification APP); \r\nyes (for the object detection APP), but only added GPU delegate and used a self-converted SSD float model\r\n- Mobile device: Google Pixel 2 XL (Android 8.0.0 and Android 9)\r\n- TensorFlow Lite version: nightly 0.0.0 + GPU nightly 0.0.0\r\n- OS Platform and Distribution: Windows 10 64 bit\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0 / 7.6.0\r\n- GPU model and memory: GeForce GTX 970, AMD64\r\n\r\n**Describe the current behavior**\r\nI cannot get any APP to run on GPU using GPU nightly. All the APPs seem to run on CPU even though the GPU delegates are used by the interpreter and no error occurs.\r\n\r\n**Describe the expected behavior**\r\nThe APPs should run on GPU. The models running on GPU should be faster than the models running on CPU.\r\n\r\n**Code to reproduce the issue**\r\nI tried the demo APP and the classification APP without changes to the code. For the object detection APP I added the GPU delegate, changed isQuantized to false, and I use a self-converted SSD mobilenet model. I wrote the delegate as described in https://www.tensorflow.org/lite/performance/gpu_advanced#android_java and I converted the model this way: https://github.com/tensorflow/tensorflow/issues/31015#issuecomment-517165233.\r\n\r\n- Demo APP: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo\r\n- Classification APP: https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android\r\n- Object Detection APP: https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android\r\n\r\n**Other info / logs**\r\nPerformance results testing the APPs on Google Pixel 2 XL (with Android 8.0.0 and Android 9):\r\n\r\n_GPU nightly:_\r\nCurrent demo APP\r\n- Quantized Model + CPU -> 75ms\r\n- Float Model + CPU -> 135ms\r\n- Float Model + GPU -> **135ms**\r\n\r\nObject Detection APP \r\n- (self-converted) Float Model + GPU -> **100 ms**\r\n\r\n_CPU only_:\r\nObjekt Detection APP (I only changed targedSdkVersion to 28 to get the original code working):\r\n- Quantized Model + CPU -> 45-50ms\r\n\r\nThanks in advance!", "comments": ["```\r\nCPU only:\r\nObjekt Detection APP (I only changed targedSdkVersion to 28 to get the original code working):\r\n\r\nQuantized Model + CPU -> 45-50ms\r\n```\r\n\r\ncould you tell me that what is the  \"Quantized Model\"  ? ssd_mobilenet_v1 or v2?", "> ```\r\n> CPU only:\r\n> Objekt Detection APP (I only changed targedSdkVersion to 28 to get the original code working):\r\n> \r\n> Quantized Model + CPU -> 45-50ms\r\n> ```\r\n> \r\n> could you tell me that what is the \"Quantized Model\" ? ssd_mobilenet_v1 or v2?\r\n\r\nThe model which is used automatically by the Object Detection APP without changing anything. (I don't know anymore which model it was.)", "My apologies that I didn't respond to this issue.  Must have slipped and then dropped off my radar completely.\r\n\r\nAccording to your description of:\r\n\r\n```\r\nFloat Model + CPU -> 135ms\r\nFloat Model + GPU -> 135ms\r\n```\r\n\r\nIt looks like it's not benefitting from GPU at all, probably due to some incompatible op.  If you check the logs, you might see something like \"only xxx ops are covered by the GPU, and rest by the CPU\" or something in that line, when you call `modifyGraphWithDelegate`.   You may want to start from there.", "Does GPU-nightly support other ops than GPU-experimental? Because with GPU-experimental the demo APP works perfectly: https://github.com/tensorflow/tensorflow/issues/31949\r\n\r\nI'm not working on it anymore and the group decided on experimental, so nightly is not needed anymore. But thanks for the response!", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31948\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31948\">No</a>\n"]}, {"number": 31947, "title": "Fixes compilation errors", "body": "```\r\ng++ -std=c++14 -c -DTF_COMPILE_LIBRARY -DGL_GLEXT_PROTOTYPES -D__LITTLE_ENDIAN__ -DABSL_ATTRIBUTE_WEAK -DTFL_COMPILE_LIBRARY -I../../../.. -I../../tools/make/downloads/farmhash/src -I../../../../tensorflow/lite/tools/make/downloads/flatbuffers/include -I../../../../tensorflow/lite/tools/make/downloads/absl -I. cl/selectors/convolution_selector.cc -o cl/selectors/convolution_selector.o\r\ncl/selectors/convolution_selector.cc: In function 'tflite::gpu::Status tflite::gpu::cl::{anonymous}::SelectConvolutionTextureArray(const tflite::gpu::Convolution2DAttributes&, const BHWC&, const tflite::gpu::cl::CreationContext&, const tflite::gpu::cl::OperationDef&, tflite::gpu::cl::ModelHints, std::unique_ptr<tflite::gpu::cl::GPUOperation>*)':\r\ncl/selectors/convolution_selector.cc:39:5: error: 'IsConvPowerVRSupported' was not declared in this scope; did you mean 'IsConvBuffer1x1Supported'?\r\n   39 |     IsConvPowerVRSupported(op_def, attr)) {\r\n      |     ^~~~~~~~~~~~~~~~~~~~~~\r\n      |     IsConvBuffer1x1Supported\r\ncl/selectors/convolution_selector.cc:40:5: error: 'ConvPowerVR' was not declared in this scope\r\n   40 |     ConvPowerVR conv;\r\n      |     ^~~~~~~~~~~\r\nIn file included from ../../../../tensorflow/lite/delegates/gpu/cl/opencl_wrapper.h:22,\r\n                 from ../../../../tensorflow/lite/delegates/gpu/cl/cl_device.h:22,\r\n                 from ../../../../tensorflow/lite/delegates/gpu/cl/cl_context.h:19,\r\n                 from ../../../../tensorflow/lite/delegates/gpu/cl/kernels/gpu_operation.h:23,\r\n                 from ../../../../tensorflow/lite/delegates/gpu/cl/selectors/convolution_selector.h:21,\r\n                 from cl/selectors/convolution_selector.cc:16:\r\ncl/selectors/convolution_selector.cc:41:72: error: 'conv' was not declared in this scope; did you mean 'lconv'?\r\n   41 |     RETURN_IF_ERROR(CreateConvPowerVR(creation_context, op_def, attr, &conv));\r\n      |                                                                        ^~~~\r\n../../../../tensorflow/lite/delegates/gpu/common/status.h:67:27: note: in definition of macro 'RETURN_IF_ERROR'\r\n   67 |     const auto status2 = (status);     \\\r\n      |                           ^~~~~~\r\ncl/selectors/convolution_selector.cc:41:21: error: 'CreateConvPowerVR' was not declared in this scope\r\n   41 |     RETURN_IF_ERROR(CreateConvPowerVR(creation_context, op_def, attr, &conv));\r\n      |                     ^~~~~~~~~~~~~~~~~\r\n../../../../tensorflow/lite/delegates/gpu/common/status.h:67:27: note: in definition of macro 'RETURN_IF_ERROR'\r\n   67 |     const auto status2 = (status);     \\\r\n      |                           ^~~~~~\r\ncl/selectors/convolution_selector.cc:42:53: error: 'conv' was not declared in this scope; did you mean 'lconv'?\r\n   42 |     *ptr = absl::make_unique<ConvPowerVR>(std::move(conv));\r\n      |                                                     ^~~~\r\n      |                                                     lconv\r\ncl/selectors/convolution_selector.cc: In function 'tflite::gpu::Status tflite::gpu::cl::{anonymous}::SelectConvolutionTexture2D(const tflite::gpu::Convolution2DAttributes&, const tflite::gpu::cl::CreationContext&, const tflite::gpu::cl::OperationDef&, std::unique_ptr<tflite::gpu::cl::GPUOperation>*)':\r\ncl/selectors/convolution_selector.cc:63:5: error: 'IsConvPowerVRSupported' was not declared in this scope; did you mean 'IsConvBuffer1x1Supported'?\r\n   63 |     IsConvPowerVRSupported(op_def, attr)) {\r\n      |     ^~~~~~~~~~~~~~~~~~~~~~\r\n      |     IsConvBuffer1x1Supported\r\ncl/selectors/convolution_selector.cc:64:5: error: 'ConvPowerVR' was not declared in this scope\r\n   64 |     ConvPowerVR conv;\r\n      |     ^~~~~~~~~~~\r\nIn file included from ../../../../tensorflow/lite/delegates/gpu/cl/opencl_wrapper.h:22,\r\n                 from ../../../../tensorflow/lite/delegates/gpu/cl/cl_device.h:22,\r\n                 from ../../../../tensorflow/lite/delegates/gpu/cl/cl_context.h:19,\r\n                 from ../../../../tensorflow/lite/delegates/gpu/cl/kernels/gpu_operation.h:23,\r\n                 from ../../../../tensorflow/lite/delegates/gpu/cl/selectors/convolution_selector.h:21,\r\n                 from cl/selectors/convolution_selector.cc:16:\r\ncl/selectors/convolution_selector.cc:65:72: error: 'conv' was not declared in this scope; did you mean 'lconv'?\r\n   65 |     RETURN_IF_ERROR(CreateConvPowerVR(creation_context, op_def, attr, &conv));\r\n      |                                                                        ^~~~\r\n../../../../tensorflow/lite/delegates/gpu/common/status.h:67:27: note: in definition of macro 'RETURN_IF_ERROR'\r\n   67 |     const auto status2 = (status);     \\\r\n      |                           ^~~~~~\r\ncl/selectors/convolution_selector.cc:65:21: error: 'CreateConvPowerVR' was not declared in this scope\r\n   65 |     RETURN_IF_ERROR(CreateConvPowerVR(creation_context, op_def, attr, &conv));\r\n      |                     ^~~~~~~~~~~~~~~~~\r\n../../../../tensorflow/lite/delegates/gpu/common/status.h:67:27: note: in definition of macro 'RETURN_IF_ERROR'\r\n   67 |     const auto status2 = (status);     \\\r\n      |                           ^~~~~~\r\ncl/selectors/convolution_selector.cc:66:53: error: 'conv' was not declared in this scope; did you mean 'lconv'?\r\n   66 |     *ptr = absl::make_unique<ConvPowerVR>(std::move(conv));\r\n      |                                                     ^~~~\r\n      |                                                     lconv\r\ncl/selectors/convolution_selector.cc: In function 'tflite::gpu::Status tflite::gpu::cl::{anonymous}::SelectConvolutionBuffer(const tflite::gpu::Convolution2DAttributes&, const tflite::gpu::cl::CreationContext&, const tflite::gpu::cl::OperationDef&, std::unique_ptr<tflite::gpu::cl::GPUOperation>*)':\r\ncl/selectors/convolution_selector.cc:86:5: error: 'IsConvPowerVRSupported' was not declared in this scope; did you mean 'IsConvBuffer1x1Supported'?\r\n   86 |     IsConvPowerVRSupported(op_def, attr)) {\r\n      |     ^~~~~~~~~~~~~~~~~~~~~~\r\n      |     IsConvBuffer1x1Supported\r\ncl/selectors/convolution_selector.cc:87:5: error: 'ConvPowerVR' was not declared in this scope\r\n   87 |     ConvPowerVR conv;\r\n      |     ^~~~~~~~~~~\r\nIn file included from ../../../../tensorflow/lite/delegates/gpu/cl/opencl_wrapper.h:22,\r\n                 from ../../../../tensorflow/lite/delegates/gpu/cl/cl_device.h:22,\r\n                 from ../../../../tensorflow/lite/delegates/gpu/cl/cl_context.h:19,\r\n                 from ../../../../tensorflow/lite/delegates/gpu/cl/kernels/gpu_operation.h:23,\r\n                 from ../../../../tensorflow/lite/delegates/gpu/cl/selectors/convolution_selector.h:21,\r\n                 from cl/selectors/convolution_selector.cc:16:\r\ncl/selectors/convolution_selector.cc:88:72: error: 'conv' was not declared in this scope; did you mean 'lconv'?\r\n   88 |     RETURN_IF_ERROR(CreateConvPowerVR(creation_context, op_def, attr, &conv));\r\n      |                                                                        ^~~~\r\n../../../../tensorflow/lite/delegates/gpu/common/status.h:67:27: note: in definition of macro 'RETURN_IF_ERROR'\r\n   67 |     const auto status2 = (status);     \\\r\n      |                           ^~~~~~\r\ncl/selectors/convolution_selector.cc:88:21: error: 'CreateConvPowerVR' was not declared in this scope\r\n   88 |     RETURN_IF_ERROR(CreateConvPowerVR(creation_context, op_def, attr, &conv));\r\n      |                     ^~~~~~~~~~~~~~~~~\r\n../../../../tensorflow/lite/delegates/gpu/common/status.h:67:27: note: in definition of macro 'RETURN_IF_ERROR'\r\n   67 |     const auto status2 = (status);     \\\r\n      |                           ^~~~~~\r\ncl/selectors/convolution_selector.cc:89:53: error: 'conv' was not declared in this scope; did you mean 'lconv'?\r\n   89 |     *ptr = absl::make_unique<ConvPowerVR>(std::move(conv));\r\n      |                                                     ^~~~\r\n      |                                                     lconv\r\nmake: *** [Makefile:234: cl/selectors/convolution_selector.o] Error 1\r\n```\r\n", "comments": []}]