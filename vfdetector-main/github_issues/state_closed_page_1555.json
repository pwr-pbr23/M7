[{"number": 6268, "title": "Gradients in C and C++ APIs", "body": "I have started [porting Tensorflow to Node.js](https://github.com/kmalakoff/tensorflow-node) and was wondering about the status of gradients support in the C and C++ APIs:\r\n\r\n- https://www.tensorflow.org/versions/r0.12/how_tos/language_bindings/index.html\r\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/resources/roadmap.md \r\n\r\nCould I ask for some help and / or offer some assistance? A couple of ideas:\r\n\r\n1. I would be happy to test the work-in-progress API in Node.js using either of both [C](https://github.com/kmalakoff/tensorflow-node/tree/master/src/native/c) or [C++](https://github.com/kmalakoff/tensorflow-node/tree/master/src/native/cc). I have tried both APIs and would be happy to maintain both as a test case.  \r\n2. I would be happy to receive some guidance from someone on the tensorflow team on how to implement gradients / training without an official API. I see from this issue (https://github.com/tensorflow/tensorflow/issues/4473) that it might be possible, but it has been hard to study the Python and OCAML code to understand what to do. Some pseudocode or links to existing code that would need to replicated could be enough.\r\n3. Something else? For example, I could help move the gradient API forward by working with someone on the tensorflow team.\r\n\r\nLet me know if you have any suggestions for a path forward. Thank you!\r\n", "comments": ["We are developing C++ gradients support here (and it is a work in progress): https://github.com/tensorflow/tensorflow/tree/master/tensorflow/cc/gradients\r\nHowever, we have not yet exposed these through the C API.\r\nThank you for your offer to help. We will add \"contributions welcome\" for work items where we can use community support.", "Thank you for the information. I will try integrating the gradient functionality for some training cases in C++ over the weekend.\r\n\r\nQuick follow up on the C vs C++ APIs...I was wondering about their internal use at Google and recommendations in general. Specifically: 1) why there are both C variants given the extra work to support multiple APIs? 2) which one I should be focussing on?\r\n\r\nI'm assuming that the C++ API is the most commonly used one internally at Google (ignoring Python for a second) and maybe the C API was developed mainly for porting purposes (eg. more support with a variety of compilers and easier integration across more languages and platforms)? Thank you in advance for any insights into the reasoning and where to focus. \r\n\r\nI'll scan through the \"contributions welcome\" labels after giving the gradients code a try with an eye on porting to other languages.", "Thanks for your offer to assist @kmalakoff !\r\n\r\n@andydavis1 and @suharshs have been slowly chugging along gradients in C++ (using the [`REGISTER_OP_GRADIENT`](https://github.com/tensorflow/tensorflow/blob/4172ca2cd7ac34be67bda2600944d284e8907b95/tensorflow/core/framework/function.h#L423) macro). The broad plan is to (1) have gradients for every op registered in C++, (2) Add a C API call equivalent of `tf.gradients` that uses these.\r\n\r\n@andydavis1 or @suharshs might have suggestions on the most effective way for you to contribute towards this.", "Thanks @kmalakoff, @andydavis1, @asimshankar and @suharshs.\r\n\r\nI am also interested in this functionality for a TensorFlow creative coding wrapper I am writing for [Cinder](https://github.com/cinder/Cinder) and [OpenFrameworks](https://github.com/openframeworks/openFrameworks). I'd be willing to contribute as well.\r\n\r\nOn a related note, I wonder if anyone can comment on whether there is planned support in the C/C++ API for graph collections (TRAINABLE_VARIABLES, LOCAL_VARIABLES, etc) as per [this discussion](https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/V-pX3uDR2Xk).\r\n", "I've reviewed the gradient folder and the changes to it over time. I've seen this code before and found it a little difficult to reverse engineer the API to understand how it could be used to implement training with back propagation.\r\n\r\nOne idea to help point me in the right direction would be to do one pass through solving a real problem (eg. approach this using idea 2 above using a vertical slice through a real-world test case). For example, in [this comment](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/tutorials/example_trainer.cc#L51), you can see that someone was considering to implement MNIST probably because the provided example was missing the full flow needed for solving for weights in a neural network using back propagation. If we could write pseudocode or an actual test for this limited and classic MNIST example to outline the use of the proposed API, it could be a good way for me to get started.\r\n", "I know timeline forecasts are always tricky, but I was wondering if you could provide a rough ETA for when you think gradient calculation might be exposed in the C API - is it more a matter of weeks, months, years? ", "@kmalakoff Thank you for your offer to help out with our C++ gradients.\r\n\r\nHere are some instructions for anyone who wants to help out:\r\n\r\nGradients are currently being ported from\r\n[python](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/ops)\r\nto C++ (in this directory).\r\n\r\nContributions are welcome and much appreciated; please follow the instructions\r\nbelow.\r\n\r\n1.  Create the op gradient function in `foo_grad.cc` corresponding to the\r\n    `foo_grad.py` file where the op originated (i.e. `array_grad.py` op\r\n    gradients should be written in `array_grad.cc`).\r\n\r\n2.  Write the op gradient with the following naming scheme:\r\n\r\n        Status OpNameGrad(const Scope& scope, const Operation& op,\r\n                          const std::vector<Output>& grad_inputs,\r\n                          std::vector<Output>* grad_outputs) {\r\n          ...\r\n          return scope.status();\r\n        }\r\n        REGISTER_GRADIENT_OP(\"OpName\", OpNameGrad);\r\n\r\n3.  Ops gradients are implemented by using the [C++\r\n    API](https://www.tensorflow.org/api_docs/cc/).\r\n\r\n4.  Tests should be included in `foo_grad_test.cc`. Please see\r\n    [`array_grad_test.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/gradients/array_grad_test.cc)\r\n    for an many examples. Tests are as simple as, creating a placeholder input\r\n    for the op's inputs and calling `RunTest` (`RunTest` uses a [gradient\r\n    checker](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/framework/gradient_checker.cc)\r\n    to verify that the theoretical gradient matches the numeric gradient). For\r\n    example:\r\n\r\n        TEST_F(ArrayGradTest, IdentityGrad) {\r\n          TensorShape shape({5, 2});\r\n          auto x = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));\r\n          auto y = Identity(scope_, x);\r\n          RunTest(x, shape, y, shape);\r\n        }\r\n\r\nNOTE: There are some ops that require features from the C++ API that are not yet\r\nimplemented.\r\n\r\n*   Ops that require PartialTensorShape information cannot yet be implemented.\r\n\r\n*   Ops that require SparseTensor or IndexSlices (currently only in python)\r\n    cannot yet be implemented.\r\n\r\n*   Maybe more.", "@malmaud As you said C++ gradient calculation is not currently exposed via the C api; I will look into adding that in the next few weeks. That being said, many ops do not yet have C++  gradients implemented, so the exposed API may break for some unknown ops. In these cases contributions using the above instructions would be much appreciated :)", "@suharshs Is there any estimate on when the C++ gradient calculation will be exposed via the C API?", "Hi Anthony, I have a change to expose it in the works. It trickled down\ninto some other changes so hopefully in a week or two. I'll keep you\nposted!\n\nOn Mon, Mar 27, 2017, 19:51 Anthony Platanios <notifications@github.com>\nwrote:\n\n> @suharshs <https://github.com/suharshs> Is there any estimate on when the\n> C++ gradient calculation will be exposed via the C API?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6268#issuecomment-289647058>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABYidnCsZl9fhELZQoHQV5OdQ6MzUnI1ks5rqHWggaJpZM4LLAHb>\n> .\n>\n", "Sounds good! Thanks for the update Suharsh! :)", "Hi all, I have submitted a change that adds TF_AddGradients to the c api. Note the gradient functions are registered in tensorflow/cc/gradients, so they will need to be included to use the gradients. Let me know if you run into any issues. Thanks!", "I will close this issue now since it is addressed. For any bug/features/issues please create a new issue. And pull requests for new C++ gradients are much appreciated :)", "@suharshs Thanks a lot for this update!\r\n\r\nI am wondering why we need to include tensorflow/cc/gradients. I see that a lot more gradients are implemented in tensorflow/core/ops/*_grad.cc. Can we not use those directly?\r\n\r\nThank you!", "The gradients defined in tensorflow/core/ops/*_grad.cc are not the c++ gradients, they are gradients for FunctionDef's. Very confusing I know :(. We are hoping to simplify/maybe make the FunctionDef's use the c++ gradients defined in tensorflow/cc/gradients eventually.\r\n\r\nFor now, the gradients defined in tensorflow/core/ops should be ignored.", "@suharshs I see. That's pretty confusing indeed. And I guess this involves a lot of potential duplicate implementations. I'm generally unclear with respect to \"functions\" in the context of TensorFlow graphs. In any case, if I want to use gradients currently through JNI, should I import all the *_grad.cc files from tensorflow/cc/gradients in my JNI bindings file? What prevents you from importing those in the C API directly so that they're always available?", "@suharshs Actually, I'm not super familiar with C++ and I'm a bit unclear as to how to include those files. I currently only include the c_api.h file in my project (a Scala API for TensorFlow) and the linker takes care of linking that to the libtensorflow.so library functions. What should I do to use gradients? Should I specify some flags when compiling libtensorflow.so or should I add something to my JNI bindings header files? Thank you!", "You are right that we should include these by default. I will look into that and get back to you. Thanks!", "@suharshs Thanks! In the meantime, I just wanted ti ask if you plan to add support for gradients of control flow ops any time soon, as well as if there are any plans for indexed slices and sparse tensor gradients. Thanks a lot for the prompt responses!:)", "I don't have any new gradients prioritized at the moment. But for specific ones, new issues for each desired gradient function, or pull requests would be appreciated.", "@suharshs Thank you! I created a new issue for some gradients that are very useful for building machine learning models [here](https://github.com/tensorflow/tensorflow/issues/9645). \r\n\r\nControl flow is not absolutely necessary I guess, but it can be very useful for RNNs. However, the gradient implementation is not as simple as registering a gradient for an op. I think it has to be considered while performing back-propagation as a special case (similar to what the Python API does).", "@suharshs Hi, I am trying to find ways of getting gradients for the various variables in my network, so I can use these with ApplyGradientDescent or similar. This is for example how I define a single layer MLP:\r\n\r\nI am a bit lost with the various C or C++ APIs. I just relied on the r1.2 API guide for the ops etc. I presume that currently it's possible to define an optimization routine?\r\n\r\nThanks!\r\n\r\n```\r\ntensorflow::Scope *root;\r\n...\r\nstd::string scope = \"test\";\r\nauto curr = root->NewSubScope(scope);\r\n\r\n// input\r\nauto input_scope = curr.NewSubScope(\"input\");\r\nauto state_placeholder = tensorflow::ops::Placeholder(input_scope.WithOpName(\"state\"), tensorflow::DT_FLOAT);\r\nauto action_placeholder = tensorflow::ops::Placeholder(input_scope.WithOpName(\"action\"), tensorflow::DT_FLOAT);\r\n\r\n// operations\r\n// layer 1\r\nint layer_1_neurons = 400;\r\nauto layer_1_scope = curr.NewSubScope(\"dense_1\");\r\nauto W1 = tensorflow::ops::Variable(layer_1_scope.WithOpName(\"W\"), {state_dim, layer_1_neurons}, tensorflow::DT_FLOAT);\r\nvars_to_fetch.push_back(scope+\"/dense_1/W\");\r\nauto assignW1 = tensorflow::ops::Assign(layer_1_scope.WithOpName(\"W_init\"), W1, tensorflow::ops::TruncatedNormal(layer_1_scope, {state_dim, layer_1_neurons}, tensorflow::DT_FLOAT));\r\nvar_init_list.push_back(scope+\"/dense_1/W_init\");\r\nauto b1 = tensorflow::ops::Variable(layer_1_scope.WithOpName(\"b\"), {layer_1_neurons}, tensorflow::DT_FLOAT);\r\nvars_to_fetch.push_back(scope+\"/dense_1/b\");\r\nauto assignb1 = tensorflow::ops::Assign(layer_1_scope.WithOpName(\"b_init\"), b1, tensorflow::ops::ZerosLike(layer_1_scope, tensorflow::Tensor(tensorflow::DT_FLOAT, {layer_1_neurons})));\r\nvar_init_list.push_back(scope+\"/dense_1/b_init\");\r\nauto m1 = tensorflow::ops::MatMul(layer_1_scope, state_placeholder, W1);\r\nauto a1 = tensorflow::ops::BiasAdd(layer_1_scope, m1, b1);\r\n```", "@fferroni you may be interested in the status of https://github.com/tensorflow/tensorflow/pull/11377", "Gradient ported from python to C# now. So we can use gradient in .NET, and it's using r1.13 branch.\r\nCheck this [issue](https://github.com/SciSharp/TensorFlow.NET/issues/175).\r\n", "What is the current status of gradients in the C API now?  Is it possible to compute gradients using that API, or is it necessary to use the C++ API?"]}, {"number": 6267, "title": "Request for distribution of prebuilt headers and libraries", "body": "I have been working on a port of [tensorflow to Node.js](https://github.com/kmalakoff/tensorflow-node) and when the node module is installed, it currently requires a time consuming build step and the user to set up bazel. Here is how I trigger the install:  [script](https://github.com/kmalakoff/tensorflow-node/blob/master/scripts/build_tensorflow.sh) and [vendor folder structure](https://github.com/kmalakoff/tensorflow-node/tree/master/vendor).\r\n\r\nAs part of an investigation into this, I noticed that as part of [installing the python package](https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html), tensorflow is distributing the libraries...for example inside of https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0-py2-none-any.whl.\r\n\r\n<img width=\"691\" alt=\"screen shot 2016-12-12 at 12 19 34 pm\" src=\"https://cloud.githubusercontent.com/assets/756520/21115291/7c90ada0-c065-11e6-9322-c38d9a2277bf.png\">\r\n\r\nI was wondering if the tensorflow team would be open to building and hosting the headers and libraries for a variety of platforms and configurations (eg. cuda, no cuda) that can be downloaded and directly linked to in C++?\r\n\r\nFYI: I have also opened an issue with bazel (https://github.com/bazelbuild/bazel/issues/2206) about their plans for prebuilt asset distribution tooling.\r\n", "comments": ["CC @asimshankar ", "I am planning on distributing the library and header file for the TensorFlow [C API](www.tensorflow.org/code/tensorflow/c/c_api.h), which is intended for building other language bindings on ([Go](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/go), [Java](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java), [Rust](https://github.com/tensorflow/rust), [Haskell](https://github.com/tensorflow/haskell) etc.). Hopefully, that will be sufficient for the Node.js port as well? Our intention is for the C API to be sufficient for building language bindings (as in the [HOWTO](https://www.tensorflow.org/how_tos/language_bindings/).\r\n\r\nI wasn't planning on binary releases for C++, as it may be hard to guarantee ABI compatibility.\r\n\r\nI have to work on some guarantees around libc version compatibility, but I started on tentative scripts to build releases for [linux](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/linux) and [OS X](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/osx).\r\n\r\nWill update this issue when we start producing those binary artefacts along with the release.", "@asimshankar sounds good. If both could be distributed, I would use the C++ API.\r\n\r\nI have performed some basic steps of implementing a port to Node.js with both the [C](https://github.com/kmalakoff/tensorflow-node/tree/master/src/native/c) and [C++](https://github.com/kmalakoff/tensorflow-node/tree/master/src/native/cc) APIs. Definitely, C++ is a much better API when you can use it (I could when using Nan for Node.js, but probably more of an exception in porting approaches) and it looks like the team might be [starting with C++ and then porting back to C](https://github.com/tensorflow/tensorflow/issues/6268) so it might be more complete sooner, but since we are talking about official headers and libraries distributed for official releases, C++ and C feature sets should hopefully be in sync at each release. \r\n\r\nI'm not sure how header and library dependencies for eigen, protobuf, etc would be provided, but I would need a way to ensure I was using the same versions as tensorflow was built with.\r\n\r\nI did raise an issue to [bazel](https://github.com/bazelbuild/bazel/issues/2206) and am a bit surprised about coming back to C / C++ after 15+ years away that there isn't a common package manager for this ecosystem. However you decide to distribute these, it would be great to be as low friction as possible to use including setting up build environments and managing external library dependencies.\r\n\r\nI'm really looking forward to this!\r\n", "FYI, with TensorFlow 1.0 we've started releasing binary artifacts for the TensorFlow C library in URLs of the form: `https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-${TF_TYPE}-${OS}-${ARCH}-${VERSION}.tar.gz`\r\n\r\nFor example:\r\n- https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-darwin-x86_64-1.0.0.tar.gz\r\n- https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.0.0.tar.gz\r\n\r\nCurrently, we only build releases for linux and darwin for x86_64. These are built with the BUILD rules in [`tensorflow/tools/lib_package`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/lib_package)\r\nso can be built for other platforms as well.\r\n\r\nFor C++ we are trying to carve out an minimal API that doesn't depend on a lot of internal headers and are trying to package that. No ETA yet (CC @skyewm)\r\n\r\n\r\n", "I will close this issue. Please open a new one if there are aspects that are not solved by the binaries we currently distribute.", "@asimshankar  Hi, is c++ api library ready now? Or what's the current state? I am eager to use one TF c++ library just for inferences on server(I know `libtensorflow_inference.so` for Android). ", "@songmeixu : Nope we haven't had the chance to create prebuilt distributions of all the C++ headers, so those have to be manually built and packaged from source right now."]}, {"number": 6266, "title": " Could not import the Python Imaging Library (PIL) required to load image files", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n[http://stackoverflow.com/questions/35534220/importerror-could-not-import-the-python-imaging-library-pil-required-to-load](url)\r\n[http://stackoverflow.com/questions/26720968/pil-importerror-the-imaging-extension-was-built-for-another-version-of-pillow](url)\r\n### Environment info\r\nOperating System:\r\nusing docker and tensorflow is installed in a container with IPython notebook. \r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.11.0\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\nI tried the solution provided on stack overflow. I also uninstall pillow package and the install it but no result\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n![capture](https://cloud.githubusercontent.com/assets/3126387/21103146/593a86c6-c0d5-11e6-9beb-d685ed80588c.JPG)\r\n", "comments": ["Why do you think your problem is related to tensorflow?", "Because I used tensorflow docker image and It didnot run.", "@muneeb699 what version and tag of tensorflow docker image are you using? Did you get it from docker hub or gcr.io?", "I downloaded it from docker hub and i am using tensorflow 0.11", "I have the same problem with the tensorflow/tensorflow:0.12.1 image. In older images Pillow was included but it's missing in newer images.\r\n\r\nI checked the [Dockerfile](https://github.com/tensorflow/tensorflow/blob/55b01593515817992821423fec19733bca91c918/tensorflow/tools/docker/Dockerfile#L33) and it contains the instructions to install Pillow but the image from docker hub is build without.\r\n```\r\ndocker history --no-trunc tensorflow/tensorflow:0.12.1\r\nIMAGE                                                                     CREATED             CREATED BY                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 SIZE                COMMENT\r\nsha256:e3f7f02f1c664e17a5d1ce069175907d6bcea523af4776ef720749aeef81af4d   8 days ago          /bin/sh -c #(nop)  CMD [\"/run_jupyter.sh\"]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 0 B                 \r\n[...]\r\n<missing>                                                                 8 days ago          /bin/sh -c pip --no-cache-dir install         ipykernel         jupyter         matplotlib         numpy         scipy         sklearn         &&     python -m ipykernel.kernelspec                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       378.6 MB            \r\n[...]\r\n```\r\n\r\nWorkaround:\r\nOpen a Terminal in Jupyter and run `pip install Pillow`", "I have the same issue with tensor flow installed in virtualenv.\r\nI have installed Pillow via pip but it didn't solve the issue.\r\nI can use '' from PIM import Image '' in my Terminal interpreter but not in a Jupyter notebook.", "I used this as a workaround:\r\n\r\nAdd this:\r\n```python\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.image as mpimg\r\n```\r\n\r\nThen replace \r\n```python\r\n      image_data = (ndimage.imread(image_file).astype(float) - \r\n                    pixel_depth / 2) / pixel_depth\r\n```\r\nwith\r\n```python\r\n      image_data = (mpimg.imread(image_file).astype(float) - \r\n                    pixel_depth / 2) / pixel_depth\r\n```\r\n\r\nand also add\r\n```python\r\n    except ValueError as e:\r\n      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\r\n```\r\nin that same `try`-block", "Note that for me, the data loaded using @pieterjandesmedt 's answer resulted in incorrect data ranges.  \r\n\r\nIt seems matplotlib loads in the data with intensities in the range [0,1] rather than [0, 255], so you would also need to adjust `pixel_depth` to be 1 instead of 255.  Rather than doing that, I used PIL directly to load in the images and convert them to numpy arrays:\r\n\r\n```python\r\nfrom PIL import Image\r\n      ...\r\n      image_data = (np.asarray(Image.open(image_file)).astype(float) \r\n                              - pixel_depth / 2) / pixel_depth\r\n```", "Thanks @cantonios . Your solution worked for me", "Thanks\r\nimport image solved my problem too.\r\n", "Thanks  cantonios"]}, {"number": 6265, "title": "Feature request: tf.contrib.ffmpeg.decode_video()", "body": "Currently, there is support for audio decoding using FFMmpeg through [tf.contrib.ffmpeg.decode_audio()](https://www.tensorflow.org/versions/master/api_docs/python/contrib.ffmpeg.html). Would it be possible to add something similar for decoding video files?\r\n\r\nThanks!", "comments": ["This seems like it would be an interesting feature. @vrv, do you know if anyone internally is working on this? We would definitely welcome  a PR that implemented this", "@fredbertsch, any ideas?", "I am working on this.. Will post the updates here", "I implemented a workaround with a python op that wraps ffmpeg bash calls: [https://github.com/victorcampos7/tensorflow-ffmpeg](https://github.com/victorcampos7/tensorflow-ffmpeg)", "@ravindranathbr any update on this?", "+1 for having this op", "Caffe2 implemented this feature using the C API of FFmpeg.\r\nhttps://github.com/caffe2/caffe2/search?utf8=\u2713&q=video&type=", "@victorcampos7 Awesome! Can you possibly push your python op to tf.contrib?", "Added a PR #13242 for a very preliminary support of `decode_video`. It is done in a similar way as `decode_audio`. At the moment the output is `rgb24`. Please take a look."]}, {"number": 6264, "title": "Freeze weights during training", "body": " Is there any solution to freeze weights? For example, I want to keep some weights unchanged when them satisfy some condition(like weight<0.5) during training.", "comments": ["tf.Variable(weights, trainable=False) will maintain weights unchangeable during training.\r\nMaybe this [StackOverflow answer](http://stackoverflow.com/questions/35298326/freeze-some-variables-scopes-in-tensorflow-stop-gradient-vs-passing-variables) will help.\r\nFurther info about [tf.Variable class see here](https://www.tensorflow.org/versions/r0.12/api_docs/python/state_ops.html#Variable).\r\n", "> tf.Variable(weights, trainable=False) will maintain weights unchangeable during training.\r\nMaybe this StackOverflow answer will help.\r\nFurther info about tf.Variable class see here.\r\n\r\nThank you!  This operation is useful to keep the declared variable keep unchanged, but I want to \r\nkeep some parameters in this variable keep unchanged, others still can be trained. ", "`x = tf.select(x < 0.5, tf.stop_gradient(x), x)`\r\n", "This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag", "@andydavis1 I believe @BingzheWu is asking the same question I have over in StackOverflow. There is no resolution to this issue on StackOverflow so I have posted the following:\r\n\r\n[http://stackoverflow.com/questions/42517926/how-to-freeze-lock-weights-of-one-tensorflow-variable-e-g-one-cnn-kernel-of-o](url)\r\n\r\nPlease consider reopening or responding in StackOverflow. Several people have this same question.", "I have the same problem.", "With the tensorflow 1.0 onward versions there is tf.layers api where I can mention trainable=False\r\nE.g tf.layers.Dense(10, trainable=False). \r\nMy query is how to freeze the layers like basicLSTMcell or BidirectionalLSTM, i.e. for layer functions under tf.nn?", "So what's wrong with @ppwwyyxx 's answer (except changing `tf.select` to `tf.where`): `x = tf.where(x < 0.5, tf.stop_gradient(x), x)`? It seems correct..", "How to dynamically freeze and unfreeze weights in tensorflow training? @Carmezim @BingzheWu @arnaghizadeh ", "This is a possible answer I found on stackoverflow, with this mehod you can freeze a specific neural in your network. \r\n[https://stackoverflow.com/questions/44274288/freeze-parts-of-neural-net-in-tensorflow](url)", "@FrankLiOnLine Thank you, sir! ", "hi @BingzheWu \r\n\r\nDid u find any solution to your problem, even i am facing the same issue"]}, {"number": 6263, "title": "Document that tf.train.Supervisor is deprecated", "body": "I am using CUDA 8.0, cuDNN 5.1, ubuntu 16.04, GPU: TitanX, tensorflow r0.12.\r\nAnd I met some problems when using tf.train.Supervisor in distributed training. I have simplified my  code shown as belown:\r\n```python\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\n\r\nserver = tf.train.Server.create_local_server()\r\nlogs_path = \"mnist/logs\"\r\n\r\n\r\nglobal_step = tf.get_variable('global_step', [],\r\n                              initializer=tf.constant_initializer(0),\r\n                              trainable=False)\r\nwith tf.name_scope(\"weights\"):\r\n    W1 = tf.Variable(tf.random_normal([784, 100]))\r\n    W2 = tf.Variable(tf.random_normal([784, 100]))\r\n\r\ninit_op = tf.global_variables_initializer()\r\nprint(\"Variables initialized ...\")\r\nsv = tf.train.Supervisor(is_chief=True,\r\n                         logdir=logs_path,\r\n                         global_step=global_step,\r\n                         init_op=init_op,\r\n                         save_model_secs=600)\r\nwith sv.managed_session(server.target) as sess:\r\n    while not sv.should_stop():\r\n        print('==============')\r\nsv.stop()\r\n```\r\nThe problem is that if I set `logdir` explicitly in tf.train.Supervisor, then the code above will met error like this:`NotFoundError (see above for traceback): Key weights/Variable not found in checkpoint`. But if I comment the lines about defining W1 and W2, then the code could work. So I assume there might be come issues in saving and restoring the checkpoint files in `tf.train.Supervisor` or maybe I did not use `tf.train.Supervisor` correctly. \r\n", "comments": ["tf.Variable will add variable to GLOBAL_VARIABLES, and won't add it to MODEL_VARIABLES. It seem that Superivisor only save MODEL_VARIABLES. I fix this by call tf.contrib.framework.add_model_variable for all varibles defined by tf.Variable.", "@shiyemin, Do you mean add following lines:\r\n```python\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\n\r\nserver = tf.train.Server.create_local_server()\r\nlogs_path = \"mnist/logs\"\r\n\r\n\r\nglobal_step = tf.get_variable('global_step', [],\r\n                              initializer=tf.constant_initializer(0),\r\n                              trainable=False)\r\nwith tf.name_scope(\"weights\"):\r\n    W1 = tf.Variable(tf.random_normal([784, 100]))\r\n    W2 = tf.Variable(tf.random_normal([784, 100]))\r\n\r\nfor variable in tf.global_variables():\r\n    tf.contrib.framework.add_model_variable(variable)\r\ninit_op = tf.global_variables_initializer()\r\nprint(\"Variables initialized ...\")\r\nsv = tf.train.Supervisor(is_chief=True,\r\n                         logdir=logs_path,\r\n                         global_step=global_step,\r\n                         init_op=init_op,\r\n                         save_model_secs=600)\r\nwith sv.managed_session(server.target) as sess:\r\n    while not sv.should_stop():\r\n        print('==============')\r\nsv.stop()\r\n```\r\nI tried this, still no success.", "I have the same issue - using `tf.contrib.framework.add_model_variable(variable)` doesn't help.", "I encountered a similar error using Tensorflow r0.12 and r1.0 as well. For me, the code breaks when I start a new session using `with sv.managed_session(...) as sess`. I resolved it by clearing my logs/events in my log directory...", "I actually store the graph before I initialize a session. I tried with and\nwithout a session present and it doesn't make a difference.\n\n\n--Hannes\n\nOn Mon, Jan 30, 2017 at 5:49 PM, Kevin Chen <notifications@github.com>\nwrote:\n\n> I'm encountering a similar error using Tensorflow r0.12 and r1.0 as well.\n> For me, the code breaks when I start a new session using with\n> sv.managed_session(...) as sess.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6263#issuecomment-276217043>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AATuQ5QH1sYoetzYwryaZaKdKk_UzS10ks5rXmkFgaJpZM4LKRtn>\n> .\n>\n", "`tf.train.Supervisor` is deprecated, please use `tf.train.MonitoredSession` instead.  Assigning to @martinwicke since we should mark it deprecated in the docs.", "For anyone looking for more information about deprecation of \u00b4tf.trainSupervisor\u00b4 and upcoming update to guide, there are some newer comments here: #6604 ", "Can anyone update this example with tf.train.MonitoredSession? \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py", "Can anyone update document below with a phrase which clearly indicates that Supervisor is deprecated? Please make Tensorflow community united with clear vision to its API usage. \r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/train/Supervisor"]}, {"number": 6262, "title": "CMake Build Error for Computing Capability 2.1", "body": "Thanks a lot guys for giving support to windows \ud83d\udcaf \r\n\r\n\r\nBut since I'm having an old GPU(Nvidia 610M) with computing capabilty 2.1 I'm unable to use GPU. Is it possible to run Tensorflow on this GPU???\r\n\r\nFor trying my luck, I followed instructions from this [comment](https://github.com/tensorflow/tensorflow/issues/6001#issuecomment-264263354) and this cmake [instruction](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake) but I'm running into build errors.\r\n\r\nEnvironment : Windows 10 Pro 1607 64 Bit & Nvidia 610M with Python 3.5, Visual Studio 2015, CUDA 8\r\n\r\nI got build error on both MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj and MSBuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj  \r\n\r\nPlease help me in compiling for this GPU or it would be great if someone could share the whl.\r\nThank you\r\n![image](https://cloud.githubusercontent.com/assets/7101452/21090262/7b50fe1e-c064-11e6-86f0-0337ae526768.png)\r\n", "comments": ["Compute capability 2.1 isn't supported by our pre-built binaries: the 0.12.0rc0 supports 3.5 and 5.2, and future binaries (and nightly builds) support 3.0, \r\n\r\nCan you share the entire logs from `MSBuild` (e.g. as a gist)? It appears from the errors at the bottom that one of the code generators failed to build, but the error itself was lost.", "Please find the gist [here](https://gist.github.com/jithurjacob/630c99c0d6033fff1cdda886e2ddb044)", "Looking at the error, it seems like `gen_git_source.py` is failing on [this line](https://github.com/tensorflow/tensorflow/blob/6dc7f989f10ceb4cc6b19e627ab742e02a0787b7/tensorflow/tools/git/gen_git_source.py#L150), where it tries to run `git`. Is it possible that `git` isn't in a directory in your `%PATH%` environment variable?", "Thanks a lot for your help it was the reason. Here is an updated [gist](https://gist.github.com/jithurjacob/a9dc4b3b5001a21f3b1992f74cde577b)", "It looks like the error is caused by the use of `__shfl_down()`, support for which was [added in Compute Capability 3.x devices](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#warp-shuffle-functions). Since we don't officially support these older devices, I doubt that we will add a workaround for your GPU, although it may be possible to get TensorFlow to build by disabling kernels that use this feature.\r\n\r\n@benoitsteiner Do you happen to know which Eigen-based kernels make use of Compute Capability 3.x features? Or are there any `#define`s that could make the build work for @jithurjacob's older CC 2.1 GPU?", "We use `__shfl_down()` for contractions. It's not clear why this code is instantiated when compiling reductions on Windows: this doesn't happen on Linux for example.", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow."]}, {"number": 6261, "title": "text_classification_cnn dataset error", "body": "The dbpedia_csv.tar.gz file downloaded seems to be corrupt and the link for the dataset, i.e.\r\nhttps://googledrive.com/host/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M/dbpedia_csv.tar.gz\r\n\r\nis giving 404 error. The file dbpedia_csv.tar.gz seems to be downloaded but upon running tensorflow/tensorflow/examples/learn/text_classification_cnn.py, an error is produced giving\r\n\r\nSuccessfully downloaded dbpedia_csv.tar.gz 1657 bytes.\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 122, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"main.py\", line 88, in main\r\n    'dbpedia', test_with_fake_data=FLAGS.test_with_fake_data, size='large')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py\", line 64, in load_dataset\r\n    return DATASETS[name](size, test_with_fake_data)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py\", line 48, in load_dbpedia\r\n    maybe_download_dbpedia(data_dir)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py\", line 40, in maybe_download_dbpedia\r\n    tfile = tarfile.open(archive_path, 'r:*')\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 1678, in open\r\n    raise ReadError(\"file could not be opened successfully\")\r\ntarfile.ReadError: file could not be opened successfully\r\n", "comments": ["We're looking into finding a replacement link. In the meantime, you can probably download the file manually from https://drive.google.com/corp/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M", "duplicate of #6174."]}, {"number": 6260, "title": " Android error: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs", "body": "hi! there is a mistake when i try to use my model on Android to classify pictures. \r\n\r\ntensorflow_jni.cc:304 Error during inference: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs\r\n          \t [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]]\r\n\r\n### Environment info\r\nmy tensorflow version is 0.10 and i train pictures by tensorflow/examples/image_retraining/retain.py on ubuntu 16.04 with GPU\r\n\r\nhow can i solve it? thanks!\r\n\r\n", "comments": ["Sorry you're hitting problems! You can follow the instructions here to help prepare your model for mobile inference (since DecodeJpeg isn't supported there): https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/", "Thanks Pete!  Closing for issue for now..."]}, {"number": 6259, "title": "Embedding Projector Standalone isn't reading its demo data folder", "body": "Upon opening https://github.com/tensorflow/embedding-projector-standalone, there is nondescript popup error message and application loads with no data. \r\n\r\nindex.html has:\r\n`projector-config-json-path=\"oss_data/oss_demo_projector_config.json\"`\r\nbut the data isn't loading. \r\n\r\nProject also has no Readme so perhaps I'm not running it, correctly? ", "comments": ["Issue Resolved. It seems to only work in production environment, but not in development."]}, {"number": 6257, "title": "replace AsProtoField with AsProtoTensorContent for efficiency", "body": "This improves throughput of gRPC worker->client fetch about 25% (116 MB/s -> 151 MB/s) as measured by benchmark in https://github.com/tensorflow/tensorflow/issues/6256", "comments": ["Can one of the admins verify this patch?", "I'm not sure why this code exists... it seems safe always to do the `Swap()`, but perhaps there are concerns about aliasing....\r\n\r\nAnyway, this change looks safe, so\r\n\r\n@tensorflow-jenkins test this please.", "mrry@ did you want to review further? If not, can you add an approval?", "@martinwicke Just waiting to see that all the tests pass."]}, {"number": 6256, "title": "grpc worker->client fetch tops out at 120 MB/s", "body": "Fetching data as numpy arrays is slow for grpc sessions. Evaluating 128MB variable repeatedly from `Session(\"grpc://localhost..\")` only gets me 116 MB/s, compared to 5150MB/s for `Session()`\r\n\r\nhttps://gist.github.com/yaroslavvb/59bc8d3d635e1027306486cc418b26aa\r\n```\r\npython client_transfer_benchmark.py --profile\r\n5150.04 MB per second\r\n116.70 MB per second\r\n\r\n```\r\nAttached are CPU profiles for worker and for client. Worker uses 2x CPU and both of them are spending majority of the time in in gRPC code, specifically doing `strcmp` (reported as [__nss_hosts_lookup](https://github.com/gperftools/gperftools/issues/1)). \r\n\r\n[profile.client.pdf](https://github.com/tensorflow/tensorflow/files/644905/profile.client.pdf)\r\n[profile.worker.pdf](https://github.com/tensorflow/tensorflow/files/644906/profile.worker.pdf)\r\n\r\n\r\nThis is related but slightly different from https://github.com/tensorflow/tensorflow/issues/6116 -- that issue looks at worker->worker transfer which hits 520 MB/second for the same message size (degrading below 100 MB/second for larger messages)\r\n\r\nhttps://gist.github.com/yaroslavvb/e196107b5e0afc834652bd3153030c42\r\n```\r\npython benchmark_grpc_recv.py --data_mb=128\r\nLocal rate:       15214.63 MB/s\r\nDistributed rate: 569.27 MB/s\r\n\r\n\r\n```\r\n", "comments": ["@mrry do you want to comment on potential fixes in the future, or workarounds for now?", "We're aware of the inefficiencies in the client-to-and-from-master path when using the distributed runtime, and have plans to cut down on the serialization overhead, especially in the common case where the client and master (and often worker) are colocated in the same process.\r\n\r\n@yaroslavvb Can you share more about what your use case is for fetching large values from the distributed runtime? We haven't spent much time optimizing this until now because the common case in distributed execution involves running a training op that returns nothing (or perhaps some small progress indicator values). It would be good to understand your use case so we can prioritize the various steps we can take to improve performance here!", "@mrry Hi, I'm also using distributed training on one server because it's easier to implement and extend. \r\n\r\nWhen using a large network (e.g. Inception-resnet-2) or batchsize, the traffic between session will be very heavy.", "@shiyemin Just to clarify, are you fetching or feeding large values in your `sess.run()` calls? If not, it's unlikely that the client to/from master traffic will be heavy... just fetching loss values or summaries. If you are still finding yourself bottlenecked on this, feel free to share your code so we can track it down.\r\n\r\n(Note that worker-to-worker traffic is a separate issue, which merits attention.)", "@mrry Use-case is training AI agents on OpenAI Universe. You have a stochastic agent like [this one](https://github.com/openai/universe-starter-agent) interacting with an environment external to TensorFlow, so all of the training observations go through `feed_dict`. (technically the bulk of data transfer is feed rather than fetch, but they have similar speed)", "@yaroslavvb Great, thanks! So, just to be clear, is the overhead in *feeding* (client->worker) or does this model also *fetch* (worker->client) a large amount of data?", "Mainly feeding. For Universe you feed images of screen into your model -> run training/prediction step -> fetch keyboard commands, so feeding passes an order of magnitude more data.\r\n\r\nUpdated the benchmark above to test the feeding direction for direct and local grpc sessions:\r\n```\r\npython client_transfer_benchmark.py --direction=p2t --profile\r\n1577.86 MB per second\r\n 143.93 MB per second\r\n\r\n```", "@mrry great, I think fast LocalMaster should cover the main use-cases (since you could always feed to local-master and use worker->worker communication). How can I run the microbenchmark?", "There are still quite a few redundant copies to remove in the local case, but it's a start :). Here's a link to the benchmark: \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/7f65a77b86b15876bc019f85fc7031981e191306/tensorflow/python/client/session_benchmark.py\r\n\r\nYou should be able to run it directly if you have the PIP package installed. If you have other cases that you'd like to track, feel free to send a PR!", "With these changes I'm seeing about 2x improvement in transfer speed:\r\n\r\npython client_transfer_benchmark.py --direction=p2t --profile\r\n262.49 MB per second\r\n\r\npython client_transfer_benchmark.py --direction=t2p --profile\r\n308.64 MB per second\r\n", "Glad to hear it's moving in the right direction :). Hold on tight because there's another change coming (hopefully in the next push) that should improve feed throughput by a decent amount.", "Is bf00bcc the change you had in mind? I don't see any change with that commit on my benchmark, is this new in-memory wrapper activated by default?\r\n\r\n__git_version__=\"b'0.12.1-1591-g4433079'\"\r\n\r\npython client_transfer_benchmark.py --profile\r\n3203.52 MB per second\r\n280.34 MB per second\r\n\r\n\r\n", "Most of the improvement from that change will be for the common case where client, master, and worker are in the same address space (like in `session_benchmark.py`). If I've understood the code correctly (since you have a `tf.Session(server.target)`) it should also speed up feeds like this one in Universe's A3C implementation:\r\n\r\nhttps://github.com/openai/universe-starter-agent/blob/ea74a5104ffa19c81562f65000ac56be78e0e0ac/a3c.py#L276\r\n\r\nWe haven't tried hard (yet) to optimize the case where the client and master are in different processes, although it may be possible to slip in an implementation that avoids a copy when serializing fed tensors into a gRPC call.", "Ah, good catch, I was doing out-of-process transfer, updated [client_transfer_benchmark.py](https://gist.github.com/yaroslavvb/59bc8d3d635e1027306486cc418b26aa), now I'm seeing >700MB, nice!\r\n\r\n```\r\npython client_transfer_benchmark.py  --profile\r\n(local) 3177.19 MB per second\r\n(grpc) 718.74 MB per second\r\n\r\n```", "So I tried after compiling latest version (with XLA) and my benchmark got a bit slower\r\n\r\npython client_transfer_benchmark.py --direction=p2t --profile\r\n600 MB/s (down from 728)\r\n\r\npython client_transfer_benchmark.py --direction=t2p --profile \r\n1308 MB/s (down from 1879)\r\n", "@alextp 's\u00a0work on eliminating numpy copies (ie ndarray_tensor_bridge.cc in 5e2cef71 ) could potentially help speed up these local server situations (ie, currently we feed numpy arrays obtained from ALE/Mujoco at each train step, and networks are tiny)", "Ping -- is anyone still looking at this issue or should we close it?", "After many rounds of optimization, the headline number is certainly no longer true, so let's close this issue for now. I think we've got a gRPC version upgrade pending that should increase the throughput some more. Feel free to open a new issue if you're still running into throughput woes!"]}, {"number": 6255, "title": "TF0.12  NewCheckpointReader() Unsuccessful TensorSliceReader constructor", "body": "I am using Ubuntu 16.04.1 LTS.\r\n\r\nI recently switched from TF r0.10 to TF r0.12. With TF r0.12, NewCheckpointReader('name.ckpt') gives error 'Unsuccessful TensorSliceReader constructor'. The files: 'name.ckpt.meta', 'name.ckpt.index' and 'name.ckpt.data-00000-of-00001' exist (as produced by the saver object).\r\n\r\nIn TF 0.10, the saver was producing  the file 'name.ckpt' that I could read using NewCheckpointReader('name.ckpt'). This does not seem to work in TF r0.12. In TF r0.12, the python documentation says to use: tf.train.NewCheckpointReader(filepattern), but I cannot find any\r\ndetail about filepattern  (tried something obvious, but I could not make it work).\r\n\r\nIs this a bug ? If not, could somebody explain what is a filepattern  (corresponding to 'name.ckpt') in TF r0.12 ?\r\n\r\nIssue #751 seems similar to this one, but it is different as  that is related to the saver, rather than to NewCheckPointReader()\r\n\r\nThank you", "comments": ["Does #6142 help?", "Yes, thanks a lot !  If I use \"./name.ckpt\", it works."]}, {"number": 6254, "title": "Fail Installation through pip on Windows version 10.0.10586", "body": "For bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System: windows 10 (version 10.0.10586)\r\n\r\npip version \r\npip 9.0.1 from d:\\program files\\python\\lib\\site-packages (python 3.5)\r\n\r\nthe Fail issue information :\r\n\r\nC:\\Users\\Administrator>pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl\r\n\r\ntensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.\r\n![bug](https://cloud.githubusercontent.com/assets/8590489/21081141/91a281b0-bffb-11e6-898a-60fa48b243dc.png)\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Facing exactly same issue in Windows 8.1. Recently heard that TensorFLow can be used for Windows with some limitations.\r\n\r\n![err](https://cloud.githubusercontent.com/assets/16458319/21082354/f4a16020-bfff-11e6-8dfb-a72b2014a8cb.jpg)\r\n\r\nIs this correct or is there a new whl?\r\n\r\n..../windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl ", "@hsidlagh Are you using Python 3.5 64-bit? If not, that is the issue. Currently TensorFlow 0.12r only supports [64-bit Python distributions on Windows](https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html#pip-installation-on-windows).", "@mcjjin I am assuming you are using a 64-bit python distribution. If for now you want to have TensorFlow going while the issue is addressed seeing if the problem is with the wheel, please try:\r\n`pip install tensorflow`\r\n\r\nedit: @mcjjin my mistake about this suggestion. I don't know why for some reason I would assume PYPI whl is a different build from the provided on tensorflow.org. I suspected the distribution would be the problem for you also but thought about trying the above first. Glad you solved.", "Thanks  I replaced the Python 3.5 to 64-bit and it works \ud83d\udc4d ", "A few hours after me comment, I tried that too and it worked!! However I specified the complete URL. Thanks for the speedy response."]}, {"number": 6253, "title": "reduce_mean operation gives inconsistent results on GPU", "body": "After hyperparameter optimization, the program I wrote does some checks to see that the results are consistent. However even with the same seeds everywhere, I found that TF doesn't always give the same results. Furthermore: the results of numpy mean and tensorflow reduce_mean also differ.\r\n\r\nAfter quite some digging, I found it's because the reduce_mean operation on the GPU gives inconsistent results. I think it's because the last two bits of the mantissa of the float returned are sometimes 01 and sometimes 10. The differences are minimally, however after a lot of reduce_mean operations, the differences can become quite significant.\r\n\r\nBelow I've included a short code to reproduce the error. When using the GPU the results of the mean operation are sometimes different. When using the CPU the results are consistent However with both CPU & GPU the results of Numpy and TensorFlow are still sometimes different.\r\n\r\n\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n\r\n### Environment info\r\nOperating System:\r\nDebian 8.6\r\nKernel: Linux 3.16.0-4-amd64\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n/usr/local/cuda/lib64/libcudadevrt.a /usr/local/cuda/lib64/libcudart.so.8.0.44 /usr/local/cuda/lib64/libcudnn.so.5\r\n/usr/local/cuda/lib64/libcudart.so /usr/local/cuda/lib64/libcudart_static.a /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n/usr/local/cuda/lib64/libcudart.so.8.0 /usr/local/cuda/lib64/libcudnn.so /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\npip install tensorflow-gpu\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.12.rc1\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nimport os\r\n# comment line below to use CPU instead of GPU\r\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nSIZE = 1000\r\n\r\ntf_x = tf.placeholder(tf.float32, (None))\r\ntf_var2 = tf.reduce_mean(tf_x)\r\n\r\nx = np.random.rand(SIZE).astype(np.float32)\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\ntf_mean= np.empty(SIZE, dtype=np.float32)\r\nnp_mean=np.empty(SIZE, dtype=np.float32)\r\n\r\nfor j in range(SIZE):\r\n    x_evaled, mean_ = sess.run([tf_x, tf_var2], feed_dict={tf_x: x})\r\n    tf_mean[j] = mean_\r\n    np_mean[j] = x.mean()\r\n\r\nsame_ = (tf_mean == np_mean).astype(np.float32).mean()\r\nconsistency_ = (tf_mean == tf_mean[0]).astype(np.float32).mean()\r\n\r\n# print results\r\nprint('\\nMin, Max TF mean: {}, {} \\nMin, Max NP mean: {}, {}'.format(tf_mean.min(), tf_mean.max(), np_mean.min(), np_mean.max()))\r\nprint('TF & NP was the same: {:.2%}'.format(same_))\r\nprint('TF consistency: {:.2%}'.format(consistency_))\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\nChanging the float to 16, 32, or 64 bit made no difference\r\n\r\n### Logs or other output that would be helpful\r\nUsing CPU:\r\n'''\r\nMin, Max TF mean: 0.49667325615882874, 0.49667325615882874 \r\nMin, Max NP mean: 0.4966733157634735, 0.4966733157634735\r\nTF & NP were the same: 0.00%\r\nTF consistency: 100.00%\r\n'''\r\nUsing GPU:\r\n'''\r\nMin, Max TF mean: 0.49240124225616455, 0.4924013018608093 \r\nMin, Max NP mean: 0.49240124225616455, 0.49240124225616455\r\nTF & NP were the same: 29.80%\r\nTF consistency: 47.90%\r\n'''\r\n", "comments": ["You are correct, the GPU reduction is intentionally non-deterministic, because it allows the implementation to be substantially faster and it was felt that this is a better tradeoff for most users."]}, {"number": 6252, "title": "\"Notebook validation failed\" (docker, official image, gpu)", "body": "### The bug\r\n\r\nAn error box pops up whenever Jupyter tries to save [the 1_hello_tensorflow notebook](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/notebooks/1_hello_tensorflow.ipynb).\r\n\r\n### Environment info\r\nUbuntu 16.04.1\r\nNvidia driver 375.20\r\nGeForce GTX 1070\r\n\r\n### Minimal reproducible example\r\n\r\n`nvidia-docker run -it -p 8888:8888 -e PASSWORD=\"hi\" gcr.io/tensorflow/tensorflow:latest-gpu`\r\n\r\n### Logs\r\n\r\n```\r\n$ nvidia-docker run -it -p 8888:8888 -e PASSWORD=\"hi\" gcr.io/tensorflow/tensorflow:latest-gpu\r\n[...]\r\n[E 11:41:58.708 NotebookApp] Notebook JSON is invalid: Additional properties are not allowed (u'metadata' was unexpected)\r\n    \r\n    Failed validating u'additionalProperties' in stream:\r\n    \r\n    On instance[u'cells'][3][u'outputs'][0]:\r\n    {u'metadata': {},\r\n     u'name': u'stdout',\r\n     u'output_type': u'stream',\r\n     u'text': u'result:  [ 3.  3.  3.  3.]\\n'}\r\n```\r\n![screenshot from 2016-12-11 03-50-33](https://cloud.githubusercontent.com/assets/8968171/21079898/059c7ab4-bf55-11e6-8986-d8238f90cdcc.png)\r\n\r\n### Why I posted this here\r\nThe bug occurs on an official TensorFlow Docker image.\r\n\r\n### Dockerless version of the bug\r\nhttps://github.com/jupyter/notebook/issues/1964\r\n\r\n#### Minimal reproducible example\r\n\r\n```\r\nwget https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/docker/notebooks/1_hello_tensorflow.ipynb\r\njupyter notebook --port=8889 1_hello_tensorflow.ipynb\r\n```", "comments": ["The errors go away if you run all the cells. Closing.", "I'm seeing this too. Still seems worth fixing, doesn't it? Errors can be pretty discouraging even if (apparently) this one is harmless. I followed the commands in https://hub.docker.com/r/tensorflow/tensorflow/ in CPU mode btw, \"docker run -it -p 8888:8888 tensorflow/tensorflow\".", "New user here.  This is very confusing to see, especially when you are following the guide exactly.", "Another newbie here. How can I fix the import error?", "The \"works as designed\" closure is unhelpful. This is the first output given following the first user action taken in the welcome tutorial (literally \"Hello, TensorFlow\") following a successful TF Docker install. It typifies poor UX associated with some open source projects.\r\n\r\nI think the Hello TF user flow could be better than to begin with an error followed by a link to a dismissively closed defect and no comment as to whether the condition is something the user needs to work to resolve."]}, {"number": 6251, "title": "tensorflow crash when training in distribution", "body": "I'am train my model in distribution, running 12 workers and 2 ps server.\r\nBut the worker crash and dumping core files continuously.\r\n\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nnothing\r\n\r\n### Environment info\r\nOperating System:\r\ncentos 6.3 with cpu\r\ntensorflow version 0.12\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nif __name__ == \"__main__\":\r\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n  server = tf.train.Server(cluster,\r\n                           job_name=FLAGS.job_name,\r\n                           task_index=FLAGS.task_index)\r\n  if FLAGS.job_name == \"ps\":\r\n    server.join()\r\n  elif FLAGS.job_name == \"worker\":\r\n    with tf.device(tf.train.replica_device_setter(\r\n            worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n            cluster=cluster)):\r\n        # Read TFRecords files for training\r\n        filename_queue = tf.train.string_input_producer(\r\n            tf.train.match_filenames_once(FLAGS.train),\r\n            num_epochs=epoch_number)\r\n        serialized_example = read_and_decode(filename_queue)\r\n        batch_serialized_example = tf.train.shuffle_batch(\r\n            [serialized_example],\r\n            batch_size=batch_size,\r\n            num_threads=thread_number,\r\n            capacity=capacity,\r\n            min_after_dequeue=min_after_dequeue)\r\n        features = tf.parse_example(\r\n            batch_serialized_example,\r\n            features={\r\n                \"label\": tf.FixedLenFeature([], tf.float32),\r\n                \"ids\": tf.VarLenFeature(tf.int64),\r\n                \"values\": tf.VarLenFeature(tf.float32),\r\n            })\r\n        batch_labels = features[\"label\"]\r\n        batch_ids = features[\"ids\"]\r\n        batch_values = features[\"values\"]\r\n\r\n        # Read TFRecords file for validatioin\r\n        validate_filename_queue = tf.train.string_input_producer(\r\n            tf.train.match_filenames_once(FLAGS.eval),\r\n            num_epochs=epoch_number)\r\n        validate_serialized_example = read_and_decode(validate_filename_queue)\r\n        validate_batch_serialized_example = tf.train.shuffle_batch(\r\n            [validate_serialized_example],\r\n            batch_size=validate_batch_size,\r\n            num_threads=thread_number,\r\n            capacity=capacity,\r\n            min_after_dequeue=min_after_dequeue)\r\n        validate_features = tf.parse_example(\r\n            validate_batch_serialized_example,\r\n            features={\r\n                \"label\": tf.FixedLenFeature([], tf.float32),\r\n                \"ids\": tf.VarLenFeature(tf.int64),\r\n                \"values\": tf.VarLenFeature(tf.float32),\r\n            })\r\n        validate_batch_labels = features[\"label\"]\r\n        validate_batch_ids = features[\"ids\"]\r\n        validate_batch_values = features[\"values\"]\r\n        logits = inference(batch_ids, batch_values)\r\n        batch_labels = tf.to_int64(batch_labels)\r\n        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,\r\n                                                                       batch_labels)\r\n        loss = tf.reduce_mean(cross_entropy, name='loss')\r\n\r\n        print(\"Use the optimizer: {}\".format(FLAGS.optimizer))\r\n        if FLAGS.optimizer == \"sgd\":\r\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n        elif FLAGS.optimizer == \"momentum\":\r\n            # optimizer = tf.train.MomentumOptimizer(learning_rate)\r\n            print(\"Not support optimizer: {} yet, exit now\".format(FLAGS.optimizer))\r\n            exit(1)\r\n        elif FLAGS.optimizer == \"adadelta\":\r\n            optimizer = tf.train.AdadeltaOptimizer(learning_rate)\r\n        elif FLAGS.optimizer == \"adagrad\":\r\n            optimizer = tf.train.AdagradOptimizer(learning_rate)\r\n        elif FLAGS.optimizer == \"adam\":\r\n            optimizer = tf.train.AdamOptimizer(learning_rate)\r\n        elif FLAGS.optimizer == \"ftrl\":\r\n            optimizer = tf.train.FtrlOptimizer(learning_rate)\r\n        elif FLAGS.optimizer == \"rmsprop\":\r\n            optimizer = tf.train.RMSPropOptimizer(learning_rate)\r\n        else:\r\n            print(\"Unknow optimizer: {}, exit now\".format(FLAGS.optimizer))\r\n            exit(1)\r\n\r\n        #with tf.device(\"/cpu:0\"):\r\n        global_step = tf.Variable(0, name='global_step', trainable=False)\r\n        train_op = optimizer.minimize(loss, global_step=global_step)\r\n\r\n        # Compute accuracy\r\n        tf.get_variable_scope().reuse_variables()\r\n        accuracy_logits = inference(validate_batch_ids, validate_batch_values)\r\n        validate_softmax = tf.nn.softmax(accuracy_logits)\r\n        validate_batch_labels = tf.to_int64(validate_batch_labels)\r\n        correct_prediction = tf.equal(\r\n            tf.argmax(validate_softmax, 1), validate_batch_labels)\r\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\n        # Compute auc\r\n        validate_batch_labels = tf.cast(validate_batch_labels, tf.int32)\r\n        sparse_labels = tf.reshape(validate_batch_labels, [-1, 1])\r\n        derived_size = tf.shape(validate_batch_labels)[0]\r\n        indices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\r\n        concated = tf.concat(1, [indices, sparse_labels])\r\n        outshape = tf.pack([derived_size, LABEL_SIZE])\r\n        new_validate_batch_labels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\r\n        _, auc_op = tf.contrib.metrics.streaming_auc(validate_softmax,\r\n                                                     new_validate_batch_labels)\r\n\r\n        # Define inference op\r\n        sparse_index = tf.placeholder(tf.int64)\r\n        sparse_ids = tf.placeholder(tf.int64)\r\n        sparse_values = tf.placeholder(tf.float32)\r\n        sparse_shape = tf.placeholder(tf.int64)\r\n        inference_ids = tf.SparseTensor(sparse_index, sparse_ids, sparse_shape)\r\n        inference_values = tf.SparseTensor(sparse_index, sparse_values, sparse_shape)\r\n        inference_logits = inference(inference_ids, inference_values)\r\n        inference_softmax = tf.nn.softmax(inference_logits)\r\n        inference_op = tf.argmax(inference_softmax, 1)\r\n\r\n        # Initialize saver and summary\r\n        #checkpoint_file = checkpoint_dir + \"checkpoint.ckpt\"\r\n        steps_to_validate = FLAGS.steps_to_validate\r\n        init_op = tf.initialize_all_variables()\r\n\r\n        saver = tf.train.Saver(max_to_keep = 2)\r\n        keys_placeholder = tf.placeholder(\"float\")\r\n        keys = tf.identity(keys_placeholder)\r\n        tf.add_to_collection(\"inputs\", json.dumps({'key': keys_placeholder.name}))\r\n        tf.add_to_collection(\"outputs\", json.dumps({'key': keys.name,\r\n                                                    'softmax': inference_softmax.name,\r\n                                                    'prediction': inference_op.name}))\r\n        tf.scalar_summary('loss', loss)\r\n        tf.scalar_summary('accuracy', accuracy)\r\n        tf.scalar_summary('auc', auc_op)\r\n        summary_op = tf.merge_all_summaries()\r\n\r\n\r\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\r\n                             logdir=\"./train_process/\",\r\n                             init_op=init_op,\r\n                             summary_op=summary_op,\r\n                             saver=saver,\r\n                             global_step=global_step,\r\n                             save_model_secs=60)\r\n\r\n    # Create session to run graph\r\n    with sv.managed_session(server.target) as sess:\r\n        if mode == \"train\" or mode == \"train_from_scratch\":\r\n            while not sv.should_stop():\r\n                # Get coordinator and run queues to read data\r\n                coord = tf.train.Coordinator()\r\n                threads = tf.train.start_queue_runners(coord=coord, sess=sess)\r\n\r\n                start_time = datetime.datetime.now()\r\n\r\n                try:\r\n                    while not coord.should_stop():\r\n                        _, loss_value, step = sess.run([train_op, loss, global_step])\r\n                        if step % steps_to_validate == 0:\r\n                            accuracy_value, auc_value, summary_value = sess.run(\r\n                                [accuracy, auc_op, summary_op])\r\n                            end_time = datetime.datetime.now()\r\n                            print(\"[{}] Task: {}, Step: {}, loss: {}, accuracy: {}, auc: {}\".format(\r\n                                end_time - start_time,\r\n                                FLAGS.task_index,\r\n                                step, loss_value, accuracy_value,\r\n                                auc_value))\r\n\r\n                            start_time = end_time\r\n                except tf.errors.OutOfRangeError:\r\n                    print(\"Done training after reading all data\")\r\n                finally:\r\n                    coord.request_stop()\r\n                    print(\"coord stopped\")\r\n\r\n                # Wait for threads to exit\r\n                coord.join(threads)\r\n```\r\n\r\n### logs\r\n\r\ngdb python core.xxxxx , and then 'bt', shows that\r\n\r\n```\r\n\r\n#0  0x00007f5943d3f166 in pthread_detach () from /opt/glibc-2.17/lib/libpthread.so.0\r\n#1  0x00007f58fc8c8ab5 in std::thread::detach() ()\r\n    at /opt/install/gcc-build/x86_64-redhat-linux/libstdc++-v3/include/x86_64-redhat-linux/bits/gthr-default.h:674\r\n#2  0x00007f58fec7f0bf in tensorflow::(anonymous namespace)::PosixEnv::SchedClosure(std::function<void ()()>) ()\r\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#3  0x00007f58fea8e981 in tensorflow::SchedClosure(std::function<void ()()>) ()\r\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#4  0x00007f58fd5f72cd in tensorflow::Master::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequest const*, tensorflow::RunStepResponse*, std::function<void ()(tensorflow::Status const&)>) ()\r\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#5  0x00007f58fd5f10f9 in tensorflow::GrpcMasterService::RunStepHandler(tensorflow::Call<tensorflow::GrpcMasterService, tensorflow::grpc::MasterService::AsyncService, tensorflow::RunStepRequest, tensorflow::RunStepResponse>*) ()\r\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#6  0x00007f58fd5f2c4c in tensorflow::GrpcMasterService::HandleRPCsLoop() ()\r\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#7  0x00007f58fc8c8b80 in execute_native_thread_routine_compat ()\r\n    at ../../../../../gcc-6.2.0/libstdc++-v3/src/c++11/thread.cc:110\r\n#8  0x00007f5943d3df83 in start_thread () from /opt/glibc-2.17/lib/libpthread.so.0\r\n#9  0x00007f59433668bd in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113\r\n\r\n```", "comments": ["And when not in distribution mode, the training process works well. and the core dump not happend in every worker, I start 12 worker ,and cored 7 workers", "I restart the train and a few hours later ,it's cored again.  ", "@mrry any suggestions on what to try next to debug this?", "@mrry  @michaelisard   is there any progress here ? or do you need me to provide more infomation, such as core dump file? (maybe large...) ", "This appears to be a crash in the C++ runtime library, possibly triggered by the distributed runtime creating a larger number than the single-process version creates.\r\n\r\nIs there anything unusual about your configuration? e.g. Are you using the pre-built binaries? Did you build TensorFlow yourself with a custom libc, or libstdc++? Are there any resource limits on the number of threads that a process can create?).", "I'm install tensorflow 0.12 by pip. how to provide some useful infomation to you ? such as some system-command output ? can you tell me some . I'm wish to help solving this. @mrry ", "This core may caused by the difference of linux kernel, Maybe you need to compile the tf lib by yourself.", "An update on this issue: we have not been able to reproduce the crash. Since CentOS 6.3 is not a supported configuration, and it appears that the failure is happening inside standard library code, it is possible that there is some incompatibility between the pre-built binaries and either your libc or your kernel.\r\n\r\nSince we cannot easily reproduce the failure, I'm throwing open this issue to Community Support. In the mean time, you may want to try building from source on your platform, to see if that fixes the issue.", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow."]}, {"number": 6250, "title": "can't export data via event_accumulator.py", "body": "f you'd like to export data to visualize elsewhere (e.g. iPython Notebook), that's possible too. You can directly depend on the underlying classes that TensorBoard uses for loading data: python/summary/event_accumulator.py (for loading data from a single run) or python/summary/event_multiplexer.py (for loading data from multiple runs, and keeping it organized). These classes load groups of event files, discard data that was \"orphaned\" by TensorFlow crashes, and organize the data by tag.\r\n\r\nAnd I do it as what it sayid with the mnist example in tensorflow. But I can't get any event from the original data whereas it shows on the tensorboard normally.\r\n\r\nbelow is my code:\r\n\r\nx = EventAccumulator(path=\"/tmp/tensorflow/mnist/logs/mnist_with_summaries/\")\r\nx.Reload()\r\nprint(x.Tags())\r\nx.FirstEventTimestamp()\r\nprint(x.Tags())\r\nAnd the result showed like below:\r\n\r\n{'scalars': [], 'histograms': [], 'run_metadata': [], 'images': [], 'graph': False, 'audio': [], 'meta_graph': False, 'compressedHistograms': []}\r\n\r\nI can't get any tag or event from the original data. However, when I open the tensorboard. Everything just look fine.\r\n\r\n", "comments": ["This kind of usage question is better asked on [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow)."]}, {"number": 6249, "title": "InvalidArgumentError: Input to reshape is a tensor with 313600 values, but the requested shape requires a multiple of 4608", "body": "When I implemented this (https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py) by TensorFlow, I had an issue on reshaping. When I tried to flatten 12 \\* 12 \\* 32 tensor, I had an error message saying \r\n`\r\ntensorflow.python.framework.errors.InvalidArgumentError: Input to reshape is a tensor with 313600 values, but the requested shape requires a multiple of 4608\r\n     [[Node: Reshape_1 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](dropout/mul, Reshape_1/shape)]]`\r\n\r\nI tried to match the input shape, but the error message said \"a tensor with 313600 values\" which I had no idea where this came from. Therefore, I thought this might be an issue on TensorFlow.  \r\n\r\nAll the code and error message are uploaded here. http://stackoverflow.com/questions/40955223/tensorflow-python-framework-errors-invalidargumenterror-input-to-reshape-is-a-t \r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttp://stackoverflow.com/questions/38397258/error-tensorflow-cnn-dimension\r\nhttps://github.com/tensorflow/tensorflow/issues/2048\r\n\r\n### Environment info\r\nOperating System: macOS Sierra 10.12.1 \r\nPython 3.5.1\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nNo such file or directory\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: pip install tensorflow\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.11.0rc1\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\nI tried the following solutions and I confirmed that I defined the target image for reshaping/flattening.\r\nhttp://stackoverflow.com/questions/38397258/error-tensorflow-cnn-dimension\r\nhttps://github.com/tensorflow/tensorflow/issues/2048\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nI wrote all the code and errors here.\r\nhttp://stackoverflow.com/questions/40955223/tensorflow-python-framework-errors-invalidargumenterror-input-to-reshape-is-a-t\r\n\r\n", "comments": ["I'm sorry that nobody has answered on stackoverflow yet, but that is really the correct forum for this kind of usage question. Have you tried printing out the values of intermediate tensors e.g. h_conv1, h_conv2, h_pool, h_drop1? That should show you the shapes of all of them and help you understand where the tensor of size 313600 is coming from.", "Thanks. if I type `h_flat` after `h_flat = tf.reshape(h_drop1, [-1, 12*12*32])`, the output is `<tf.Tensor 'Reshape_4:0' shape=(?, 4608) dtype=float32>`.  Or, if I type `h_flat.get_shape()`, then the output is `TensorShape([Dimension(None), Dimension(4608)])`. ", "The error was from a shape mismatch. After I changed  conv2 to'VALID' from 'SAME', it worked.", "We answered this problem which is also related to architecture Understand this in following link\r\nhttps://stackoverflow.com/questions/46465925/input-to-reshape-is-a-tensor-with-37632-values-but-the-requested-shape-has-1505/52240509#52240509\r\nLet us know if you face any issue", "In my case, `an image size` and my `model's input shape` were different. Please check your image size again and again. Cuz, I've spent pretty long time on fixing this problem", "> The error was from a shape mismatch. After I changed conv2 to'VALID' from 'SAME', it worked.\r\n\r\nwhere can I find conv2 ?", "Epoch 1/25\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-91-894200829115> in <module>()\r\n----> 1 cnn.fit(x = training_set, validation_data = test_set, epochs = 25)\r\n\r\n8 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError:  Input to reshape is a tensor with 3936256 values, but the requested shape requires a multiple of 6272\r\n\t [[node sequential_5/flatten_4/Reshape (defined at <ipython-input-91-894200829115>:1) ]] [Op:__inference_train_function_3715]\r\n\r\nFunction call stack:\r\ntrain_function", "> ## Epoch 1/25\r\n> InvalidArgumentError Traceback (most recent call last)\r\n> in ()\r\n> ----> 1 cnn.fit(x = training_set, validation_data = test_set, epochs = 25)\r\n> \r\n> 8 frames\r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n> 58 ctx.ensure_initialized()\r\n> 59 tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n> ---> 60 inputs, attrs, num_outputs)\r\n> 61 except core._NotOkStatusException as e:\r\n> 62 if name is not None:\r\n> \r\n> InvalidArgumentError: Input to reshape is a tensor with 3936256 values, but the requested shape requires a multiple of 6272\r\n> [[node sequential_5/flatten_4/Reshape (defined at :1) ]] [Op:__inference_train_function_3715]\r\n> \r\n> Function call stack:\r\n> train_function\r\n@jeevan-tt\r\nFacing the same error, Still cannot figure out the problem. Did you resolve the error?", "Hi the reason for this error is because of the shape of input check all of your input it may seem that all of them have the same shape but you may find some of them in wired shapes like 28*28*3*3 as you see the third dimension is unusable.\r\n\r\n**Solves**\r\n\r\nwhen you use image flow define your shape as (28,28)\r\n\r\nsome times you may define input shape to all of the layers recall that you should define it only for the first layer"]}, {"number": 6248, "title": "Configure fails when run non-interactively", "body": "Running ./configure non-interactively fails for 0.12.0-rc0, and it worked on previous releases.  We're relying on this for the TensorFlow Rust bindings and expect it to configure with default options.  configure seems to be failing on a 'read -p', probably because of 'set -e' being added.\r\n\r\n### Environment info\r\nOperating System: Linux (Ubuntu 16.04.1)\r\nChecked out tag: 0.12.0-rc0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n./configure < /dev/null", "comments": ["Can you use `yes | ./configure` instead? That is what we use in our scripts. ", "`yes '' | ./configure` seems to work.", "It is also possible that there are some new environment variables you need to set, such as `TF_NEED_OPENCL`.\r\n\r\nClosing this issue, as it seems to be resolved now."]}, {"number": 6247, "title": "How can a tensor remain in gpu when the kernel is launched but does not finish yet?", "body": "It confuse me a lot. I'm reading the stream executor part of tensorflow's source code. I find that once a graph is executed on a gpu device, each kernel of the graph's nodes just allocates the output memory and launches the cuda kernel on gpu on a stream by passing the input and output device memory. So the op kernel is done but the real execution of the kernel (e.g. cuda) may not be done on the gpu yet. Here is the question: how can tensorflow assure the input memory allocated on the gpu devices still live when the execution really happens on the gpu? I checked the executor.cc file and found that once a kernel is launched, the input tensor is destroyed. Can anyone show me what I miss? Thanks!", "comments": ["In the gpu_device.cc file, the BaseGPUDevice::RequiresRecordingAccessedTensors() function return false since streams_.size() returns 1. So tensors allocated in a kernel's compute() method aren't recorded. No TensorReference will be created.", "When there is a single stream, all kernel launches on the GPU are serialized. So you can think of the GPU memory allocator in TensorFlow as \"simulating\" the memory usage of the actual kernels. It determines what memory is needed for kernel 1, then enqueues kernel 1 for launch, then \"frees\" the memory needed by kernel 1. When simulating what memory is needed for kernel 2, it can re-use any temporary memory used by kernel 1, before enqueueing kernel 2, because there is a single stream. This means that kernel 2 is guaranteed not to execute until after kernel 1 completes. As you observe, the memory allocator does not indicate what memory is actually in use on the stream, but it does indicate what memory is available to use for the next kernel to be enqueued, and that is the information that is needed when running the next Op.\r\n\r\nDoes that make sense?", "hi, @michaelisard . I know that all kernels launched on a stream are serialized. For a typical op's kernel's Compute method, the mode is as follows I think: first it gets the input tensor of the kernel, then allocates the out tensor memory on gpu and at last launches the kernel (e.g. cuda) and after all the above things done, the Compute() method returns. So the next Op's kernel's Compute method may be called at once. Tensor's TensorBuffer stores the pointer to the location of the real tensor's bytes and the bytes can be remained in gpu if there is an tensor object or an TensorReference object remains in the program. However once a op's kernel is done, it's input will be cleaned and I haven't see any Tensor or TensorReference object alive in the program since the device's RequiresRecordingAccessedTensors() method returns false. So how can this tensor's byte remains in gpu? In detail, is allocating and deallocating memory on gpu queued on the stream that executes the kernel?", "TF runtime does not allocate gpu memory per op. Instead, it has it's own\nallocator, which grabs a big chunk (the pool) from cuda and never returns\nto cuda. All tensor objects are allocated within the pool.\n\nCombined w/ single compute stream configuration, one can prove the memory\nsafety in a similar way as people prove multi-threaded program's memory\nsafety properties, albeit much simpler for a single gpu cuda w/ only one\ncopy-in stream and one copy-out stream.\n\nConsider the following 3 operations in sequence:\n   x = add(a, b)\n   y = add(x, c)\n   z = add(y, d)\nassuming, a, b, c, d, x, y, and z are of the same shape, and x and y are\nnot used before the 3rd operation.\n\nIf we, TF runtime, know that they are run on the same gpu device and hence\none single gpu stream, the following are relevant events on cpu and gpu\nrespectively:\ncpu:\n  allocate x\n  dispatch kernel for x = add(a, b)\n  allocate y\n  dispatch kernel for y = add(x, c)\n  deallocate x\n  allocate z\n  dispatch kernel for z = add(y, d)\n  deallocate y\n\ngpu:\n  execute kernel for x = add(a, b)\n  execute kernel for y = add(x, b)\n  execute kernel for z = add(y, b)\n\nThe TF data flow graph guarantees that the events on cpu above happens one\nafter another. The single cuda stream semantics guarantees events on gpu\nabove happens one after another. dispatch kernel and execute kernel has the\nstrict happen-after relation. If you draw the happen-after edges of these\nevents, you can see their correctness.\n\nMore importantly, this allows us the TF runtime to reuse the memory release\nby \"deallocate x\" immediately to \"allocate z\".\n\n\n\"\"\"\nI know that all kernels launched on a stream are serialized. For a typical\nop's kernel's Compute method, the mode is as follows I think: first it gets\nthe input tensor of the kernel, then allocates the out tensor memory on gpu\nand at last launches the kernel (e.g. cuda) and after all the above things\ndone, the Compute() method returns. So the next Op's kernel's Compute\nmethod may be called at once. Tensor's TensorBuffer stores the pointer to\nthe location of the real tensor bytes and\n\"\"\"\nUp to this point, your understanding matches what TF runtime does.\n\n\"\"\"it can be remained in gpu if there is an tensor object or an\nTensorReference object remains in the program.\nHowever once a op's kernel is done, it's input will be cleaned and I\nhaven't see any Tensor or TensorReference object alive in the program since\nthe device's RequiresRecordingAccessedTensors() method returns false. So\nhow can this tensor's byte remains in gpu? In detail, does allocating\nmemory on gpu is queued on the stream that executes the kernel?\"\"\"\n\nThis section of your description doesn't chime well with TF runtime\nimplementation. Maybe the point that TF runtime has its own memory pool can\nhelp you understanding, I hope. Without such a pool, yes, you need to queue\nthe cuda allocation and deallocation calls on the appropriate stream and/or\nadds proper waiting events, which we think is a rather complex scheme to\nengineer when differently cuda libraries are used concurrently.\n\n\n\nOn Sun, Dec 11, 2016 at 5:41 PM cuiguoxin <notifications@github.com> wrote:\n\n> hi, @michaelisard <https://github.com/michaelisard> . I know that all\n> kernels launched on a stream are serialized. For a typical op's kernel's\n> Compute method, the mode is as follows I think: first it gets the input\n> tensor of the kernel, then allocates the out tensor memory on gpu and at\n> last launches the kernel (e.g. cuda) and after all the above things done,\n> the Compute() method returns. So the next Op's kernel's Compute method may\n> be called at once. Tensor's TensorBuffer stores the pointer to the location\n> of the real tensor bytes and it can be remained in gpu if there is an\n> tensor object or an TensorReference object remains in the program. However\n> once a op's kernel is done, it's input will be cleaned and I haven't see\n> any Tensor or TensorReference object alive in the program since the\n> device's RequiresRecordingAccessedTensors() method returns false. So how\n> can this tensor's byte remains in gpu? In detail, does allocating memory on\n> gpu is queued on the stream that executes the kernel?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6247#issuecomment-266325602>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHk2biViME88SP5hGC-YxcD1vQ-xwj9fks5rHKZhgaJpZM4LJ0V4>\n> .\n>\n", "hi, @zffchen78 ,I checked your answer and the problem still was not solved for me. In gpu_device.cc file, I find that EigenCudaStreamDevice class has an deallocate() method which is  async and gets executed when the stream has done all the previous events. However I didn't find the use of the deallocated method. How does the Allocator like GPUBFCAllocator has this async action?", "I see. Thanks!!! @michaelisard @zffchen78 "]}, {"number": 6246, "title": "Added support for OpenCL to ApplyAdamOp", "body": "", "comments": []}, {"number": 6245, "title": "Update installation instructions for tensorflow-gpu. (#6231)", "body": "* Update installation instructions for tensorflow-gpu.\r\n\r\n* Update os_setup.md", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I think something went wrong in your rebase. Can you replay your changes on top of r0.12 to get a clean set of commits?", "@yifeif Did we take care of this?\r\nCan we close this PR?", "These commits look like the ones already in r0.12 branch?"]}, {"number": 6244, "title": "Fix 'partition_info' crash", "body": "Fixes this crash by not forcing partition_info if it is None:\r\nTraceback (most recent call last):\r\nFile \"main.py\", line 33, in \r\nglobal_network = UnrealModel(action_size, -1, device)\r\nFile \"/Users/babaktr/Desktop/unreal/model.py\", line 35, in init\r\nself._create_network(for_display)\r\nFile \"/Users/babaktr/Desktop/unreal/model.py\", line 45, in _create_network\r\nself._create_base_network()\r\nFile \"/Users/babaktr/Desktop/unreal/model.py\", line 71, in _create_base_network\r\nbase_conv_output = self._base_conv_layers(self.base_input)\r\nFile \"/Users/babaktr/Desktop/unreal/model.py\", line 90, in _base_conv_layers\r\nW_conv1, b_conv1 = self._conv_variable([8, 8, 3, 16], \"base_conv1\")\r\nFile \"/Users/babaktr/Desktop/unreal/model.py\", line 423, in _conv_variable\r\ninitializer=conv_initializer(w, h, input_channels))\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1024, in get_variable\r\ncustom_getter=custom_getter)\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 850, in get_variable\r\ncustom_getter=custom_getter)\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 346, in get_variable\r\nvalidate_shape=validate_shape)\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 331, in _true_getter\r\ncaching_device=caching_device, validate_shape=validate_shape)\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 677, in _get_single_variable\r\nexpected_shape=shape)\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 224, in init\r\nexpected_shape=expected_shape)\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 327, in _init_from_args\r\ninitial_value(), name=\"initial_value\", dtype=dtype)\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 665, in \r\nshape.as_list(), dtype=dtype, partition_info=partition_info)\r\nTypeError: _initializer() got an unexpected keyword argument 'partition_info'", "comments": ["Can one of the admins verify this patch?", "Are you trying to be backward compatible for custom initializers (outside of TF repo) that don't understand `partition_info`? I've faced similar issue, and fixed it to have all custom initializers on my side accept `partition_info` argument. ", "Possibly, it fixed my issue and I thought that maybe it would for others as well \ud83d\ude05 Feel free to close this  otherwise!", "This change looks benign enough to me, but please add a comment in code saying that we're doing it for compatibility. Otherwise it's unclear reading the code why we're doing it.", "@tensorflow-jenkins test this please\r\n", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Ah, there is a problem with this change. See the tests -- some initializers already take 3 arguments and don't want to work with 2. Do you think there is a work-around, or should we move everything to having 3 args? Take a look please, we cannot merge when tests are failing.", "Closing this PR based on your most recent comment. Feel free to reopen if you decide to tackle the changes later."]}, {"number": 6243, "title": "Remove rnn_cell.md from leftnav as it has been removed, so we can build website.", "body": "website.", "comments": []}, {"number": 6242, "title": "DNNLinearCombinedClassifier predict method generate : Predictions: <generator object _as_iterable at 0x7eff0a1b2910>", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Can you edit the template to provide more details of your system and what happened to generate the error?", "Please close this issue as I figured out the mistake I was doing. Apology for inconvenience. "]}, {"number": 6241, "title": "TensorFlow Models now missing ?", "body": "So currently doing some testing of the latest master branch on an AWS box. I built yesterday (UK time) and I went through installing tensorflow fine with no issues.\r\n\r\nToday I have done a fresh install again taking from the master branch and for some reason when I run my samples through I get the following:\r\n\r\npython -m tensorflow.models.image.mnist.convolutional\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so.8.0 locally\r\n/home/ubuntu/anaconda3/bin/python: Error while finding spec for 'tensorflow.models.image.mnist.convolutional' (<class 'ImportError'>: No module named 'tensorflow.models')\r\n", "comments": ["I believe they moved the models to a separate repository:\r\n[models](https://github.com/tensorflow/models)", "@kingtaurus is correct, [PR #731](https://github.com/tensorflow/models/pull/731) was merged moving the example models which includes mnist 12h ago. Tutorials and docs were updated although I don't know how much time it will take for them to be updated to tensorflow.org if they haven't. Cloning the repository and run from there should work for now.", "Closing for now: please reopen if there's another underlying issue thanks.", "Thanks Guys can you advise which is the lastest stable version to build from ?  I just did another build switching to tags/0.12.0-rc0 and when I try and run the same sample as above I get the below. Is it best to drop back to 0.11 ?\r\n\r\n\"Error while finding spec 'tensorflow.models.image.mnist.convolutional.py' (<class 'AttributeError'>: 'module' object has no attribute '__path__')\r\n\r\n", "0.12r is the latest stable version actually, I am not sure but what happens is it seems they are not going to include the models in the 0.12r wheel.", "I'm able to execute python convolutional.py --self_test with no issue but below command throwing error. How I can solve it? Can someone provide detailed resolution? ( Sorry I'm new to Python ) .\r\n\r\n[root@localhost ~]# virtualenv --system-site-packages -p python3 ~/tensorflow\r\nRunning virtualenv with interpreter /usr/bin/python3\r\nUsing base prefix '/usr'\r\nNew python executable in /root/tensorflow/bin/python3\r\nNot overwriting existing python script /root/tensorflow/bin/python (you must use /root/tensorflow/bin/python3)\r\nInstalling setuptools, pip, wheel...done.\r\n[root@localhost ~]#\r\n\r\n[root@localhost ~]# source ~/tensorflow/bin/activate\r\n\r\n(tensorflow) [root@localhost ~]# export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:/root/cuda/lib64\r\n\r\n(tensorflow) [root@localhost ~]# python -m tensorflow.models.image.mnist.convolutional\r\n/root/tensorflow/bin/python: Error while finding spec for 'tensorflow.models.image.mnist.convolutional' (<class 'ImportError'>: No module named 'tensorflow.models')"]}, {"number": 6240, "title": "what is op 'Switch' in RNN?", "body": "I build a two-layer RNN (GRU) network using tf.nn.dynamic_rnn with argument sequence_length. Then the graph contains an op 'Switch' which I haven't seen before. \r\n 919 node {\r\n 920   name: \"Test/Model/RNN/RNN/Assert/AssertGuard/Switch\"\r\n 921   op: \"Switch\"\r\n 922   input: \"Test/Model/RNN/RNN/All\"\r\n 923   input: \"Test/Model/RNN/RNN/All\"\r\n 924   attr {\r\n 925     key: \"T\"\r\n 926     value {\r\n 927       type: DT_BOOL\r\n 928     }\r\n 929   }\r\n\r\nI need to do inference in android and it complains this op is not registered in kernel files. I wonder what it is and there should be a python wrap or .cc file to implement this I guess.", "comments": ["@michaelisard Do you know what is it in .cc file? Thank you so much!", "I guess it's the condition argument in assert op?", "OK, it is in control_flow_ops and it's under comment.", "Do you have more questions about this or should I close the issue?", "No, thanks! I will close this."]}, {"number": 6239, "title": "Moving example models from github.com/tensorflow/tensorflow to github.com/tensorflow/models", "body": "Continuation of https://github.com/tensorflow/tensorflow/pull/6234", "comments": []}, {"number": 6238, "title": "Merge r0.12 back into master", "body": "There were some documentation conflicts, but I reran gen_docs.sh to fix those.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->", "@mrry I also reverted b5e6d667d28f8dd1ea21ed428fa560b0d4720afa in master, it looks like we now moved set_ops into core?", "We have, as part of the layers move.", "Jenkins, test this please.\r\nMacos tests timed out in this run:\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/3061/", "all tests pass!\r\n\r\nCLA ok since it is a merge PR."]}]