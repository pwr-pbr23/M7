[{"number": 1540, "title": "install sklearn to debian", "body": "and fix nightly-debian-cpu\n", "comments": ["I wanted to merge this as we have no tests for it anyway. But we require all the tests now...\n\n@tensorflow-jenkins test this please.\n", "I can't say how much I hate the new merge policy, nobody being able to merge if it's an actual merge. \n"]}, {"number": 1539, "title": "[clang+CUDA] No ZeroesLike[DT_BOOL] kernel", "body": "I'm working on getting tensorflow to build its GPU code with clang.\n\nI have a hacked up crosstool, a hacked up clang (for std::complex support), and some minor changes to eigen (which are awaiting review).  The branch is at jlebar/tensorflow@cuda-clang, but checking that out isn't sufficient, because you need to customize some paths to make crosstool happy.  Anyway you don't have my WIP compiler changes, so you won't get very far.  :)\n\nHaving said all that, I'm trying to get the tensorflow tests to pass, while I wait for tra@ to hopefully figure out the crosstool business.  I'm looking at this one\n\n```\n$ bazel test -c opt --config=cuda_clang //tensorflow/core:ops_array_grad_test \n```\n\n, which fails with\n\n```\nE tensorflow/core/common_runtime/executor.cc:332] Executor failed to create kernel. Not found: No registered 'ZerosLike' OpKernel for GPU devices compatible with node n4 = ZerosLike[T=DT_BOOL](n1)\n     [[Node: n4 = ZerosLike[T=DT_BOOL](n1)]]\nF tensorflow/core/ops/array_grad_test.cc:365] Check failed: ::tensorflow::Status::OK() == (sess->Run({{\"x:0\", x}, {\"dims:0\", dims}, {\"dy:0\", dy}}, {\"dx:0\", \"dx:1\"}, {}, &out)) (OK vs. Not found: No registered 'ZerosLike' OpKernel for GPU devices compatible with node n4 = ZerosLike[T=DT_BOOL](n1)\n     [[Node: n4 = ZerosLike[T=DT_BOOL](n1)]]\n     [[Node: dx = SymbolicGradient[Tin=[DT_FLOAT, DT_BOOL, DT_FLOAT], Tout=[DT_FLOAT, DT_BOOL], f=Reverse[T=DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_x_0/_2, _recv_dims_0/_4, _recv_dy_0/_6)]]\n     [[Node: dx/_8 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_14_dx\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]])\nexternal/bazel_tools/tools/test/test-setup.sh: line 52: 106730 Aborted                 (core dumped) \"$@\"\n```\n\nLooking through the code, it seems that there's no zeroing kernel for bools, so I'm not sure how this is supposed to work?  The obvious change to add a kernel, jlebar/tensorflow@286c1647cca9ebcdbce4497995c794d4b0c55633, doesn't work -- we seem to invoke the new kernel, but the whole program just silently dies.\n\nI'm pretty confused by what's going on here, what with the To32Bit functor being applied to an array of doubles (?) and so on.  Any pointers would be very much appreciated.\n", "comments": ["\\+ @keveman\n", "I believe that test should not be run on GPUs.  @zffchen78.\n\nI believe those tests are specifically not in the gpu-enabled test set: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/BUILD#L1143\n\nDo you know why it's trying to load the GPU for you?\n", "> Do you know why it's trying to load the GPU for you?\n\nHm, no idea -- I'm totally new to the codebase.  It's possible that our horrible crosstool hack doesn't quite work right; that's the only interesting change here...  Is the decision about whether the test is running on the GPU vs CPU made at compile time?  How is that decided?  Should -DGOOGLE_CUDA=1 not be passed when we compile this file?\n\n(This would be a lot easier if I could switch between different configs without doing a full rebuild.  There isn't an easy way to do that, is there?)\n\nThanks for your help.\n", "use --platform_suffix=clang_cuda for one build and --platform_suffix=somethingelse so bazel chooses different output directories for temp files, and then you can switch between them without problems :)\n\nBuilding unconditionally with -DGOOGLE_CUDA=1 on every file is likely to be a problem -- you should see some of the tensorflow.bzl rules like tf_cuda_library to see how we conditionally build some targets with -DGOOGLE_CUDA\n", "OK, that's probably the problem, thanks, I'll have a look tomorrow.\n", "> use --platform_suffix=clang_cuda so bazel chooses different output directories for temp files\n\nThanks, that's an improvement.  :)\n\nBut, sigh, bazel test seems to be stripping out LD_LIBRARY_PATH, even though the docs [1] say it doesn't.  So\n\n```\n$ bazel test --config=cuda --platform_suffix=cuda -c opt //tensorflow/tensorboard/backend:server_test\n```\n\nfails because it can't find libcudart.so.7.0, but `bazel run` works fine.  --spawn_strategy=standalone doesn't help.\n\nSo for running all the CUDA tests, I'm down to using xargs to bazel run them all.  Is that right?\n\nWhen I do so, many fail even with nvcc, including ops_array_grad_test -- I guess these just shouldn't be built with --config=cuda with or without clang.  (Which I should have figured out earlier, but eluded me because my base test run was missing --config=cuda -- sorry.)\n\nRunning all the tests with --config=cuda and --config=cuda_clang, the only differences in test success/failure are in six tests.\n\nThese fail with nvcc but not with clang:\n- models/embedding:word2vec_optimized_test\n- python:control_flow_ops_py_test\n- python:gradients_test\n- python:graph_util_test\n\nAnd these fail with clang but not nvcc:\n- python:decode_raw_op_test\n- python:slice_op_test\n\nBut since none of these tests match the glob from above, am I right in concluding that we're in the clear here?  If so we can close this bug.\n\n[1] http://bazel.io/docs/test-encyclopedia.html\n", "...actually, I take it back, there are a lot more differences between clang and nvcc.  It would definitely be helpful to understand which are meaningful.  I was operating on the assumption that TF wouldn't do things so crazy that we'd reasonably expect a difference in behavior with different compilers, but maybe that's wrong.  :)\n", "Interesting. I don't have to set LD_LIBRARY_PATH and bazel test runs for me. However @vrv had the same problem you are facing yesterday and we were unable to figure it out :( Nothing is obvious or easy with bazel.\n", "@damienmg is there something we're missing?\n", "@zheng-xq Any idea about this one? We are having a similar issue running on AWS instances (inside a docker container).\n", "`LD_LIBRARY_PATH` is redacted by Bazel. The documentation is wrong there.\n\nA hackish workaround is too edit tools/test/test-setup.sh to set LD_LIBRARY_PATH correctly in the Bazel source.\n\nFixing the relpath with C++ auto-configuration should do the long-term fix.\n", "@martinwicke, yes, \"bazel test\" by default strips out LD_LIBRARY_PATH. What I recall from before is that you have to specify --spawn_strategy=standalone or --spawn_strategy=local should fix that. If @jlebar can confirm \"--spawn_strategy=standalone\" does not fix the problem, this might be a different issue.  \n", "> --spawn_strategy=standalone doesn't help.\n\nI tried again and it does seem to work:\n\n```\nbazel test -c opt --config=cuda --platform_suffix=cuda --spawn_strategy=standalone //tensorflow/...\n```\n\nSorry, that may have been user error.\n\nWith the above command, many tests segfault or otherwise fail with this config, and it's still not clear to me which ones are supposed to work, nor is it clear whether there are some that happen to pass with nvcc but nonetheless shouldn't necessarily work with a different compiler.\n\nMore generally, I've decided to stop working on OSS tensorflow for the time being; I have been cut by too many sharp edges here.  We're working on getting the compiler to work internally within Google -- perhaps the internal build/test infrastructure for TF will work better and whatever changes we need to make there can be open-sourced.\n", "Unless you've had bad luck with the particular commit you're at, all the tests should pass (see https://github.com/tensorflow/tensorflow/blob/master/README.md, build status at head is in there, or look at ci.tensorflow.org for details). The tensorflow-master-\\* builds are the ones you want to look at, unless you're using a release. All tests should pass in a release.\n", "> all the tests should pass\n\nOK -- just so it's clear, I was getting that from https://github.com/tensorflow/tensorflow/issues/1539#issuecomment-198165898\n\nI just re-sync'ed to head and still get 38 test failures with --config=cuda, despite the readme saying that the tests are currently passing.  Maybe the point of confusion is between \"all tests pass\" and \"all tests pass with --config=cuda\"?\n\n```\n//tensorflow/contrib/ctc:ctc_decoder_ops_test                            FAILED in 15.5s\n//tensorflow/contrib/ctc:ctc_loss_op_test                                FAILED in 15.6s\n//tensorflow/contrib/distributions:gaussian_test                         FAILED in 15.5s\n//tensorflow/contrib/layers:initializers_test                            FAILED in 14.8s\n//tensorflow/contrib/layers:layers_test                                  FAILED in 16.0s\n//tensorflow/contrib/layers:optimizers_test                              FAILED in 2.5s\n//tensorflow/contrib/layers:regularizers_test                            FAILED in 14.8s\n//tensorflow/contrib/lookup:lookup_ops_test                              FAILED in 15.1s\n//tensorflow/contrib/skflow:test_base                                    FAILED in 1.5s\n//tensorflow/contrib/skflow:test_custom_decay                            FAILED in 2.9s\n//tensorflow/contrib/skflow:test_dropout_ops                             FAILED in 2.1s\n//tensorflow/contrib/skflow:test_estimators                              FAILED in 3.2s\n//tensorflow/contrib/skflow:test_multioutput                             FAILED in 2.6s\n//tensorflow/contrib/skflow:test_nonlinear                               FAILED in 3.6s\n//tensorflow/contrib/skflow:test_regression                              FAILED in 3.6s\n//tensorflow/core:ops_array_grad_test                                    FAILED in 14.1s\n//tensorflow/core:ops_math_grad_test                                     FAILED in 15.1s\n//tensorflow/examples/tutorials/mnist:mnist_with_summaries_test          FAILED in 12.1s\n//tensorflow/models/image/mnist:convolutional_test                       FAILED in 2.1s\n//tensorflow/python:bcast_ops_test                                       FAILED in 4.4s\n//tensorflow/python:conv_ops_test                                       TIMEOUT in 65.0s\n//tensorflow/python:cwise_ops_test                                       FAILED in 2 out of 5 in 32.0s\n//tensorflow/python:division_future_test                                 FAILED in 1.7s\n//tensorflow/python:ftrl_test                                            FAILED in 6.3s\n//tensorflow/python:gather_op_test                                       FAILED in 6.3s\n//tensorflow/python:gradient_checker_test                                FAILED in 5.9s\n//tensorflow/python:gradient_correctness_test                            FAILED in 5.3s\n//tensorflow/python:gradient_descent_test                                FAILED in 2.8s\n//tensorflow/python:gradients_test                                       FAILED in 3.6s\n//tensorflow/python:histogram_ops_test                                   FAILED in 3.0s\n//tensorflow/python:identity_op_py_test                                  FAILED in 3.2s\n//tensorflow/python:io_ops_test                                          FAILED in 14.8s\n//tensorflow/python:rnn_test                                             FAILED in 50.8s\n//tensorflow/python:slice_op_test                                        FAILED in 18.5s\n//tensorflow/python:sparse_concat_op_test                                FAILED in 11.0s\n//tensorflow/python:sparse_serialization_ops_test                        FAILED in 3.7s\n//tensorflow/python:sparse_split_op_test                                 FAILED in 11.8s\n//tensorflow/python:sparse_tensor_dense_matmul_op_test                   FAILED in 11.8s\n```\n\nI don't mean to point fingers or anything, but I hope between this and #1535, you can at least understand where I'm coming from when I talk about sharp edges.  :)\n", "1) Are you building with -c opt too?\n2) Pointers to logs would be helpful.\n", "1) Yes, I'm running with the same command as in https://github.com/tensorflow/tensorflow/issues/1539#issuecomment-201008522\n\n2) Now bazel test seems to have stopped passing LD_LIBRARY_PATH to the nvcc tests.  Same command, still passing --spawn_strategy=standalone, but now they all fail with\n\n```\nImportError: libcudart.so.7.0: cannot open shared object file: No such file or directory\n```\n\nMaybe the LD_LIBRARY_PATH is being cached at build time or something, I dunno.  I'm trying a clean build now.  I'll post logs if I can make it work again, but basically, most of the tests listed above segfault after outputting \"loaded foo.so\".\n", "Same thing after a clean build -- 332 failures, most with \"cannot open shared object file\".  I think bazel may be caching the LD_LIBRARY_PATH somehow or something.\n", "So indeed, I can reproduce this locally too:  bazel test -c opt --config=cuda tensorflow/... results in those errors.  However, running the test directly:\n\n```\nbazel-bin/tensorflow/core/kernels/cast_op_test\n```\n\nworks.  It used to be the case that standalone mode preserved LD_LIBRARY_PATH, but I guess no longer.  We'll look into it, but you do have direct-execution of the tests as a way to make progress.\n", "@jlebar: This issue has been quiet for a while. Did you eventually find a workaround, or is there still work ongoing to fix it?\n", "I am going to close this issue since we have //tensorflow/core/kernels:cast_op_test_gpu passing in latest builds.\r\nJustin, feel free to reopen if I am missing something here.", "@annarev This was a while ago, but I think this issue was specifically about building in OSS with clang; not sure if that's what you were saying is currently passing?  @ilya-biryukov has been looking at this lately; if he's happy that it's working, I'm happy.", "I should have read it more carefully, I was just looking at our cuda gcc builds. I will re-open this issue then.", "What is the status of this?", "@r4nt I know we made a lot of progress on clang builds, is this still a valid issue?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 1538, "title": "Why does one graph affects the cost of the other graph althongh the two graphs do not have any relationship?", "body": "I have got a strange problem, the first code is as below:\n\n``` python\nwith tf.device('/gpu:0'):\n    local3_value = np.load(\"local3.npy\")\n    weights4 = np.load(\"weights4.npy\")\n    biases4 = np.load(\"biases4.npy\")\n    weights3 = np.load(\"weights4.npy\")\n    biases3 = np.load(\"biases4.npy\")\n    with tf.variable_scope('local3') as scope:\n        local3 = tf.matmul(local3_value, weights3)\n    with tf.variable_scope('local4') as scope:\n        local4 = tf.nn.relu(tf.matmul(local3_value, weights4)+biases4, name = scope.name)\n\n    with tf.Session(config=tf.ConfigProto(log_device_placement=True))as sess:\n        sess.run(local3)\n        start_time  = time.time()\n        predictions = sess.run(local4)\n        duration = time.time() - start_time\n    print(duration)\n```\n\nThe duration is 0.244572162628\n\nThe second code is as below:\n\n``` python\nwith tf.device('/gpu:0'):\n    local3_value = np.load(\"local3.npy\")\n    weights4 = np.load(\"weights4.npy\")\n    biases4 = np.load(\"biases4.npy\")\n    weights3 = np.load(\"weights4.npy\")\n    biases3 = np.load(\"biases4.npy\")\n    with tf.variable_scope('local3') as scope:\n        local3 = tf.nn.softplus(weights3)\n    with tf.variable_scope('local4') as scope:\n        local4 = tf.nn.relu(tf.matmul(local3_value, weights4)+biases4, name = scope.name)\n\n\n    with tf.Session(config=tf.ConfigProto(log_device_placement=True))as sess:\n        sess.run(local3)\n        start_time  = time.time()\n        predictions = sess.run(local4)\n        duration = time.time() - start_time\n        print(duration)\n```\n\nThe duration is 0.0679910182953\n\nThe only difference between these two codes is the local3 layer. Why the graph of local3 affects the cost of graph of loca4? These two graphs do not have any relationship.\n", "comments": ["Is the difference in duration consistent when you perform multiple calls to `sess.run()` in a loop? As I mentioned in #1532, the first execution of a subgraph does not have a representative execution time, because it must perform various setup tasks.\n", "Closing this due to inactivity. Feel free to reopen if my previous answer didn't fix things for you!\n"]}, {"number": 1537, "title": "Fixing some python3 failures.  Fixes much of tensorflow python", "body": "tests and tensorboard server_test.\n", "comments": ["Okay, a bunch of fixes to fix all of our python3 breakages, and some non-python3 breakages too, from what I can tell.\n", "http://imgur.com/W1RqoBn\n"]}, {"number": 1536, "title": "test without long test_timeout", "body": "We should remove --test_timeout=3600.\n", "comments": ["@martinwicke @vrv do not merge yet, I will remove the bazel clean hack. It will make sure the tests are not cached. I should have used --cache_test_results=no probably. Anyway, we will remove that hack.\n", "It has build just fine without insanely long test timeouts. We can merge this. But expect to find some flaky tests in next few builds.\n", "@tensorflow-jenkins: test this please\n", "cwise_ops_test is just on the boundary it seems -- runs 295-305s -- should we make that test bigger?\n", "How about just increase the sharding - https://github.com/tensorflow/tensorflow/pull/1536/commits/ef3409d0eb76e9f3ec0c181616ab91330f4805b1?\n\nThat sharding is actually very cool feature! Bazel is pretty smart :)\n"]}, {"number": 1535, "title": "google.protobuf imports are resolving to /usr/local instead of TF's local copy", "body": "### Environment info\n\nOperating System: Current Goobuntu\n\nTF commit 945a4b4cd12df0fdf3f0cc24b128bb4ce66ebf88\ngoogle/protobuf@fb714b3606bd663b823f6960a73d052f97283b74\n### Steps to reproduce\n\n```\n$ bazel run -c opt //tensorflow/python:default_platform__resource_loader_test\n```\n\nResults in\n\n```\nTraceback (most recent call last):\n  File \"/usr/local/google/home/jlebar/.cache/bazel/_bazel_jlebar/b1c0b5b92791600151e68af1e7d712c0/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/default_platform__resource_loader_test.runfiles/tensorflow/python/platform/default/_resource_loader_test.py\", line 20, in <module>\nfrom tensorflow.python.platform import googletest\n  File \"/usr/local/google/home/jlebar/.cache/bazel/_bazel_jlebar/b1c0b5b92791600151e68af1e7d712c0/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/default_platform__resource_loader_test.runfiles/tensorflow/__init__.py\", line 23, in <module>\nfrom tensorflow.python import *\n  File \"/usr/local/google/home/jlebar/.cache/bazel/_bazel_jlebar/b1c0b5b92791600151e68af1e7d712c0/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/default_platform__resource_loader_test.runfiles/tensorflow/python/__init__.py\", line 49, in <module>\nfrom tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/local/google/home/jlebar/.cache/bazel/_bazel_jlebar/b1c0b5b92791600151e68af1e7d712c0/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/default_platform__resource_loader_test.runfiles/tensorflow/core/framework/graph_pb2.py\", line 9, in <module>\nfrom google.protobuf import symbol_database as _symbol_database\n  File \"/usr/local/google/home/jlebar/.cache/bazel/_bazel_jlebar/b1c0b5b92791600151e68af1e7d712c0/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/default_platform__resource_loader_test.runfiles/google/protobuf/symbol_database.py\", line 186, in <module>\n_DEFAULT = SymbolDatabase(pool=descriptor_pool.Default())\nAttributeError: 'module' object has no attribute 'Default'\n```\n\nWith @Yhg1s help, I've traced this down some, but now I'm stuck.\n\nThe issue seems to be that the descriptor_pool we import is not TF's local copy, but is instead the copy from /usr/lib/python2.7/dist-packages.\n\nTF's local copy of symbol_database.py imports google.protobuf.descriptor_pool, and when it does so, it has the correct sys.path.  Yet somehow we end up at the wrong descriptor_pool.\n\nThomas's idea was that maybe we're importing google from the wrong path before this point, and that's what's causing us to find the wrong path to descriptor_pool.  Indeed, judging from a -vv log [1], this seems to be the case.\n\nHere we import TF's local copy of google, and then _reimport_ it from /usr/lib.  Python doesn't normally do this, so Thomas's idea was that someone may be messing with sys.modules or performing other dark rituals.\n\nAt this point I'm kind of lost.\n\nI suspect things will work fine if I use a virtualenv, and Thomas gave me enough of an idea of how to do this.  But the build should work without that, right?  (It's not even documented how to build within a virtualenv.)\n\n[1] http://pastebin.com/raw/M3PTCTfY\n\n(Edit: Wrong twouters.)\n", "comments": ["\\+ @tra\n", "I can confirm that it works fine if I use a virtualenv.\n", "Yes, either use a virtualenv, or have the necessary version (currently 3.0.0b2) of protobuf installed system wide (via sudo pip install protobuf==3.0.0b2)\n", "FWIW, I think we need to find a better solution to our protobuf issues, in light of deeper issues like #1415\n", "Leaving this open until some solution emerges.\n", "@jlebar Can you please close this if you think that this has been addressed sufficiently in the latest TensorFlow?\n", "The target from comment 0 is gone, but plain bazel test -c opt //tensorflow/python/... works, with no virtualenvs.  So I think we're good here.  Thank you!\n", "Unfortunately, I just ran into this same thing on my desktop.\nubuntu 14.04\nbazel version 0.3.1\ngtx1080, nvidia driver version 367\ncuda8 + cudnn5\n__git__version for tf: 'v0.10.0-1703-g0ed1dc2'\n\nThen I set LD_LIBRARY_CONFIG\n\nThen:\n$ git clone ....tensorflow...\n$ bazel build tensorflow/tools/pip_package:build_pip_package -c opt --config=cuda\n$ mkdir pip_package \n$ bazel-bin/build tensorflow/tools/pip_package `pwd`/pip_package\n$ cd pip_package\n\nwhen I pip install, run python and then import tensorflow, I get the same error above.\nIf I create a virtualenv and pip install it, import tensorflow, I see that cuda loads successfully.\n\nShould I create a new issue, or should I reopen this one?\n"]}, {"number": 1534, "title": "Error running example on gpu", "body": "Running `bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu` with latest development code results in:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.4.0.7 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.0 locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX 780\nmajor: 3 minor: 5 memoryClockRate (GHz) 1.006\npciBusID 0000:03:00.0\nTotal memory: 3.00GiB\nFree memory: 2.93GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 780, pci bus id: 0000:03:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 780, pci bus id: 0000:03:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 780, pci bus id: 0000:03:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 780, pci bus id: 0000:03:00.0)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 780, pci bus id: 0000:03:00.0)\nAborted (core dumped)\n```\n\nWhat could be wrong with my gpu setting?\n", "comments": ["This still happens on latest code, it seems that it tries to create the same device multiple times. Under gdb I found that multiple threads are calling `BaseGPUDeviceFactory::CreateDevices` simultaneously. Is this the desired behaviour?\n", "Hmm, this is probably wrong (we at one point had GPU devices created statically once per process).  I'll try to get a fix for this at some point soon.\n", "The error turned out to be caused by my GPU setting (one user of the machine had set the compute_mode to be EXCLUSIVE_THREAD). This concurrent behaviour seems to cause no harm.\n", "Ahh, good to know!  I had seen a report of this before and thought we were doing something wrong.  Glad to hear it's supposed to work in this fashion too :)\n"]}, {"number": 1533, "title": "zlib.error: Error -3 while decompressing: invalid block type ", "body": "I used pip to install Tensorflow . I ran the commands provided in the link https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#common-problems\n\nThe commands i Ran were :\n\nsudo apt-get install python-pip python-dev\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n\nTensorflow got installed properly and a folder was created in the usr/local/lib/python2.7/dist-packages folder.\n\nI tried to create a model using the mnist  dataset and ran the following command in /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist folder\n\npython convolution.py \n\nThe following message was displayed in the console :\n\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting data/train-images-idx3-ubyte.gz\nTraceback (most recent call last):\n  File \"convolutional.py\", line 316, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"convolutional.py\", line 128, in main\n    train_data = extract_data(train_data_filename, 60000)\n  File \"convolutional.py\", line 75, in extract_data\n    buf = bytestream.read(IMAGE_SIZE \\* IMAGE_SIZE \\* num_images)\n  File \"/usr/lib/python2.7/gzip.py\", line 261, in read\n    self._read(readsize)\n  File \"/usr/lib/python2.7/gzip.py\", line 312, in _read\n    uncompress = self.decompress.decompress(buf)\nzlib.error: Error -3 while decompressing: invalid block type\n\nI removed the mnist data files present in the data folder and then again executed the python command  : python convolution.py , Now surprisingly it is giving a bit different error as :\nzlib.error: Error -3 while decompressing: invalid stored block lengths . \n\nI am using python2.7 running inside Ubuntu 14.04 Operating system . \n", "comments": ["Can you check whether the downloaded files are decompressible with `gunzip`?\n", "I'm assuming that [this question on Stack Overflow](http://stackoverflow.com/questions/36055090/zlib-error-error-3-while-decompressing-invalid-distance-code) is related. It does sound like something is corrupting your files as you download them (since `gunzip` isn't working). Can you try downloading the files using `wget` or `curl` and ensuring that they work this way?\n", "@mrry I am the one who has asked the question in Stackoverflow . There was a problem with the file . I downloaded it again and then it started working . Thnaks for your Answer . \n"]}, {"number": 1532, "title": "why the tensorflow run slower on gpu than on cpu", "body": "The tensorflow should run faster on gpu than on cpu , however, I got the opposite result. The time cost when running on gpu is longer than on cpu.The code is as below:\n\n``` python\n    with tf.device('/cpu:0'):\n        local3_value = np.load(\"local3.npy\")\n    with tf.variable_scope('local4') as scope:\n        weights4 = tf.get_variable('weights',[384,192])\n        biases4 = tf.get_variable('biases',[192])\n        local4 = tf.nn.relu(tf.matmul(local3_value, weights4)+biases4, name = scope.name)\n\n    saver = tf.train.Saver()\n\n    with tf.Session(config=tf.ConfigProto(log_device_placement=True))as sess:\n        saver.restore(sess, \"/tmp/cifar10_train/model.ckpt-27000\")\n        start_time  = time.time()\n        predictions = sess.run(local4)\n        duration = time.time() - start_time\n        print(duration)\n```\n\nThe variables such as weights4 and biases4 are restored from a trained model.\n", "comments": ["This looks like a very small layer, and you're only running it once. I suspect the cost is dominated by session setup costs and transferring the constant data to the GPU.\n\nTry running `predictions = sess.run(local4)` in a loop to understand the average step time in the limit. This will give you a better understanding of whether some code is more efficient on CPU or GPU.\n", "@mrry  I think It makes sense, thank you.\nI also got another problem, the first code is as below:\n`with tf.device('/gpu:0'):\n    local3_value = np.load(\"local3.npy\")\n    weights4 = np.load(\"weights4.npy\")\n    biases4 = np.load(\"biases4.npy\")\n    weights3 = np.load(\"weights4.npy\")\n    biases3 = np.load(\"biases4.npy\")\n    with tf.variable_scope('local3') as scope:\n        local3 = tf.matmul(local3_value, weights3)\n    with tf.variable_scope('local4') as scope:\n        local4 = tf.nn.relu(tf.matmul(local3_value, weights4)+biases4, name = scope.name)\n\n```\nwith tf.Session(config=tf.ConfigProto(log_device_placement=True))as sess:\n    sess.run(local3)\n    start_time  = time.time()\n    predictions = sess.run(local4)\n    duration = time.time() - start_time\n    print(duration)`\n```\n\n The duration is 0.244572162628\n\nThe second code is as below:\n`with tf.device('/gpu:0'):\n    local3_value = np.load(\"local3.npy\")\n    weights4 = np.load(\"weights4.npy\")\n    biases4 = np.load(\"biases4.npy\")\n    weights3 = np.load(\"weights4.npy\")\n    biases3 = np.load(\"biases4.npy\")\n    with tf.variable_scope('local3') as scope:\n        local3 = tf.nn.softplus(weights3)\n    with tf.variable_scope('local4') as scope:\n        local4 = tf.nn.relu(tf.matmul(local3_value, weights4)+biases4, name = scope.name)\n\n```\nwith tf.Session(config=tf.ConfigProto(log_device_placement=True))as sess:\n    sess.run(local3)\n    start_time  = time.time()\n    predictions = sess.run(local4)\n    duration = time.time() - start_time\n    print(duration)`\n```\n\nThe duration is 0.0679910182953\n\nThe only difference between these two codes is the local3 layer. Why the graph of local3 affects the cost of graph of loca4? These two graphs do not have any relationship.\n"]}, {"number": 1531, "title": "Replace build_opener with urlretrieve", "body": "Since used 'urlretrieve' without header to download the file from give url will cause fail it should use another way it is able to attach headers instead.\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "I signed the CLA. But the signed name isn't the name I committed. \n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "One of the datasets, notMNIST_large.tar.gz, is quite big. It will take time for downloading.\n", "Is this still an issue since we moved from Yaroslav's domain to:\nurl = 'http://commondatastorage.googleapis.com/books1000/'\n?\n", "@vincentvanhoucke\nSorry, I tested it on little bit older commit and pasted the fix to new repo that I just clone yesterday. I've tested the url = 'http://commondatastorage.googleapis.com/books1000/' , it's ok for me. \nAt the point, I'd would like to discuss, it's a best way just change url? Even though it is just an assignment we're able to adopt the code and use other url instead(data set). It's just my opinion.\n", "If that's all right, I marginally prefer the simpler original code. It's more readable to novice coders. I might be proven wrong in case we hit other DOS protection issues, in which case we should definitely revive this patch. Thanks for your help!\n"]}, {"number": 1530, "title": "Internal Compiler Error with retrain tutorial", "body": "Bryan Hynds reported this problem using one of our tutorials, in these YouTube comments:\nhttps://www.youtube.com/watch?v=h7xuEiZjqqo\n\n---\n\nI'm running into some problems with compiling.  I'm getting a compile error that's failing to build the target:\n\n```\nroot@d121482d25c8:/tensorflow# bazel build -c opt --copt=-mavx tensorflow/examples/image_retraining:retrain\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nINFO: From Compiling tensorflow/core/kernels/conv_ops.cc:\nIn file included from external/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/Core:35:0,\n                 from external/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/Tensor:14,\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\n                 from ./tensorflow/core/framework/types.h:23,\n                 from ./tensorflow/core/framework/type_traits.h:22,\n                 from ./tensorflow/core/framework/allocator.h:25,\n                 from ./tensorflow/core/framework/op_kernel.h:22,\n                 from ./tensorflow/core/framework/numeric_op.h:19,\n                 from tensorflow/core/kernels/conv_ops.cc:22:\nexternal/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, long int>, 16>, const Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, long int>, 16> > >; bool Vectorizable = true]':\nexternal/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[] (size_t index) { return values[index]; }\n                                                                   ^\nINFO: From Compiling tensorflow/core/kernels/argmax_op.cc:\ngcc: internal compiler error: Killed (program cc1plus)\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\nSee <file:///usr/share/doc/gcc-4.8/README.Bugs> for instructions.\nERROR: /tensorflow/tensorflow/core/BUILD:358:1: C++ compilation of rule '//tensorflow/core:kernel_lib' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 79 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\nTarget //tensorflow/examples/image_retraining:retrain failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 424.939s, Critical Path: 412.00s\ufeff\n```\n\nI've wiped the container and rebuilt it several times, and I can reproduce it every time I reach this point.  I'm compiling with --verbose_failures now to get more details.  I'll post the log once I have it.\ufeff\n", "comments": ["INFO: From Compiling tensorflow/core/kernels/argmax_op.cc:\ngcc: internal compiler error: Killed (program cc1plus)\n\ntypically means gcc ran out of memory I believe -- there are ways to give more 'resources' to bazel, but I don't know the incantation offhand.\n", "Thanks Vijay! Bryan, how much memory were you able to allocate to the VM in VirtualBox when you did that step?\n", "I have 4GB of RAM and 4 CPU cores allocated to my Boot2Docker VM.\n", "Yeah, it's definitely a memory issue.  dmesg output:\n\n```\nOut of memory: Kill process 26313 (cc1plus) score 213 or sacrifice child\nKilled process 26313 (cc1plus) total-vm:1318032kB, anon-rss:597484kB, file-rss:1480kB\n```\n", "I'm attempting with --local_resources 4096,4.0,1.0 to see if it'll complete.\n", "Failed again.  Bumping it down to --local_resources 2048,2.0,1.0 ...\n", "Failed.  Here's my free output if that helps:\n\n```\nroot@d121482d25c8:/tensorflow# free\n             total       used       free     shared    buffers     cached\nMem:       4045512     217568    3827944       2668       7224      37572\n-/+ buffers/cache:     172772    3872740\nSwap:      1952632     146408    1806224\n```\n", "I think you can pass -j 1 to prevent parallelism in the build that might help, or --ram_utilization_factor=50 or something lower than the default (67).  I don't know -- I'm grasping at straws at this point :(\n\n(From http://bazel.io/docs/bazel-user-manual.html).  \n", "Vijay, **-j 1** definitely slowed it down but it's still crunching through the compile.  I'll let it run overnight and check up on it in the morning.  Thanks for the tips and I'll keep you posted.\n", "Well, it took 25 minutes to compile, but using **--local_resources 2048,2.0,1.0 -j 1** completed successfully.  Thanks again. :)\n\n```\nroot@d121482d25c8:/tensorflow# bazel build -c opt --copt=-mavx --verbose_failures --local_resources 2048,2.0,1.0 -j 1 tensorflow/examples/image_retraining:retrain\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nINFO: From Compiling tensorflow/core/common_runtime/memory_types.cc:\ntensorflow/core/common_runtime/memory_types.cc: In function 'tensorflow::Status tensorflow::ValidateMemoryTypes(tensorflow::DeviceType, const tensorflow::Graph*)':\ntensorflow/core/common_runtime/memory_types.cc:57:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     for (int i = 0; i < inp_mvec.size(); ++i) {\n                                       ^\ntensorflow/core/common_runtime/memory_types.cc:60:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     for (int i = 0; i < out_mvec.size(); ++i) {\n                                       ^\nINFO: From Compiling tensorflow/core/kernels/matmul_op.cc:\nIn file included from external/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/Core:35:0,\n                 from external/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/Tensor:14,\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\n                 from ./tensorflow/core/kernels/matmul_op.h:19,\n                 from tensorflow/core/kernels/matmul_op.cc:20:\nexternal/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 2, 1, long int>, 16>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 2, 1, long int>, 16>, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 2, 1, long int>, 16> > >; bool Vectorizable = true]':\nexternal/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[] (size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long int>, 16>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorMap<Eigen::Tensor<const int, 2, 1, long int>, 16>, const Eigen::TensorMap<Eigen::Tensor<const int, 2, 1, long int>, 16> > >; bool Vectorizable = true]':\nexternal/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[] (size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<double, 2, 1, long int>, 16>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorMap<Eigen::Tensor<const double, 2, 1, long int>, 16>, const Eigen::TensorMap<Eigen::Tensor<const double, 2, 1, long int>, 16> > >; bool Vectorizable = true]':\nexternal/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[] (size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, long int>, 16>, const Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, long int>, 16> > >; bool Vectorizable = true]':\nexternal/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[] (size_t index) { return values[index]; }\n                                                                   ^\nINFO: From Compiling tensorflow/core/kernels/sparse_matmul_op.cc:\nIn file included from external/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/Core:35:0,\n                 from external/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/Tensor:14,\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\n                 from tensorflow/core/kernels/sparse_matmul_op.cc:22:\nexternal/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, long int>, 16>, const Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, long int>, 16> > >; bool Vectorizable = true]':\nexternal/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[] (size_t index) { return values[index]; }\n                                                                   ^\nINFO: From Compiling tensorflow/python/pywrap_tensorflow.cc:\nbazel-out/local_linux-opt/bin/tensorflow/python/pywrap_tensorflow.cc: In function 'PyObject* _wrap_PyRecordReader_New(PyObject*, PyObject*)':\nbazel-out/local_linux-opt/bin/tensorflow/python/pywrap_tensorflow.cc:3828:111: warning: 'arg2' may be used uninitialized in this function [-Wmaybe-uninitialized]\n     result = (tensorflow::io::PyRecordReader *)tensorflow::io::PyRecordReader::New((string const &)*arg1,arg2);\n                                                                                                               ^\nAt global scope:\ncc1plus: warning: unrecognized command line option \"-Wno-self-assign\" [enabled by default]\nTarget //tensorflow/examples/image_retraining:retrain up-to-date:\n  bazel-bin/tensorflow/examples/image_retraining/retrain\nINFO: Elapsed time: 1544.951s, Critical Path: 48.42s\n```\n", "Great to hear you're finally running, and sorry it took so long. I will update the YouTube comments, make a note in the next version of the tutorial, and close this for now.\n"]}, {"number": 1529, "title": "fix typos", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Hi @Mistobaan, unfortunately there are conflicts that need to be resolved.\n", "@vrv fixed \n"]}, {"number": 1528, "title": "Unique Error with Image Retraining Tutorial", "body": "", "comments": ["This sounds like a versioning issue. Does upgrading to a nightly version of TensorFlow solve it? Assigning to @shlens in case he knows of a change to the model that could have caused this.\n", "I am not aware of any changes in this code base. @petewarden but might be able to comment as well.\n", "I think this change might be more likely to have caused the error you're seeing: https://github.com/tensorflow/tensorflow/commit/01a6f5e504d9299395888a786e52c589c16af529\n\nIf the graph for the model that you're retraining was regenerated _after_ that change, and you're using TensorFlow version 0.7.1, then I'd expect to see that message. I'm not sure of the provenance of the trained model graphs though. @oweingrod: Can you confirm how you obtained the graph (i.e. which download link)? @shlens or @petewarden: Would you know if the graph had changed since the 0.7.1 release?\n\nHere are links to the Mac OS X nightly PIP packages: [Python 2](http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-py2-none-any.whl) / [Python 3](http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-py3-none-any.whl)\n", "@shlens: did you regenerate the graph at the same time that you regenerated the checkpoint?  If so, you'll probably need to replace the graph with the old version.\n", "> I'm assuming that it is the case that there is a discrepancy between the version of tensorflow used by bazel to build the inception graph for retraining, and my (likely slightly more recent) build that I am using to classify?\n\nI believe from @mrry and @shlens comments that you should only see the message if the version used for retraining was more recent than the version that you're calling in classify_image.py. Looking at the code, the data_format attribute was added to Conv2d back on February 19th:\nhttps://github.com/tensorflow/tensorflow/commit/898d26a1cef46c0848188e22ba4b435846649b6d\n\nI believe the other commit mentioned is a red herring, because it only affects pooling ops. So you'd have to be running code from before February 19th in classify_image.py to run into this error I believe. Can you try printing tf.version when you run your modified classify_image.py script?\n", "@martinwicke can you help on the version printing?\n", "I have a feeling the example was regenerated by accident using a newer version of TF that introduced new attributes.  If we used the old graph but a new checkpoint, I think things would work.\n", "I bet the new graph was built on a version _newer_ than 0.7.1.  My guess if you installed one of the pip nightlies off the main github page, you might have better luck, but we should really fix the graph.\n", "@shlens/@petewarden: deferring to you at this point, I've exhausted my knowledge of retrain.py :)\n", "@oweingrod What's the exact error that you see when you run classify_image.py?\n", "Great to hear, let us know what you end up building!\n", "@petewarden or @shlens Is there some way one of you could generate a graph that's compatible with the current release and upload it? I predict other people will run into the same issue, and telling people that they have to use the nightly isn't great....\n", "The problem was that the image_retraining example (which loads in an old graph and writes out a new one) was run with a nightly build, and then later classify_image.py was run on that new graph with an older build of TensorFlow. So I don't believe this requires any changes to the graph. I'll stop by your desk to make sure I'm making sense!\n", "It does make sense. Thanks Pete!\n", "@oweingrod\n\n**I run into the same problem on my computer. Do you mind to give any suggestion? I checked my tensorflow by pip show tensorflow:**\n\nMetadata-Version: 2.0\nName: tensorflow\nVersion: 0.8.0\nSummary: TensorFlow helps the tensors flow\nHome-page: http://tensorflow.org/\nAuthor: Google Inc.\nAuthor-email: opensource@google.com\nInstaller: pip\nLicense: Apache 2.0\nLocation: /home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages\nRequires: six, protobuf, wheel, numpy\nClassifiers:\n  Development Status :: 4 - Beta\n  Intended Audience :: Developers\n  Intended Audience :: Education\n  Intended Audience :: Science/Research\n  License :: OSI Approved :: Apache Software License\n  Programming Language :: Python :: 2.7\n  Topic :: Scientific/Engineering :: Mathematics\n  Topic :: Software Development :: Libraries :: Python Modules\n  Topic :: Software Development :: Libraries\nEntry-points:\n  [console_scripts]\n  tensorboard = tensorflow.tensorboard.tensorboard:main\n\n**I added print tf.**version** in my retrain_test.py code. It print out:**\n0.8.0\n\n**Below is the copy of my terminal:**\n\n(tensorflow)xu@xu-ThinkCentre-M72e:~/tensorflow/tensorflow/models/image/imagenet(master) $ python retrain_test.py /home/xu/Robotimages/Robot1/w/1.jpg\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.8.0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 750 Ti\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.2545\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.77GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:01:00.0)\nW tensorflow/core/kernels/batch_norm_op.cc:36] Op is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\nW tensorflow/core/kernels/batch_norm_op.cc:36] Op is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\nW tensorflow/core/kernels/batch_norm_op.cc:36] Op is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\nE tensorflow/core/common_runtime/executor.cc:332] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'T' not in Op<name=MaxPool; signature=input:float -> output:float; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](pool/control_dependency)\n     [[Node: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](pool/control_dependency)]]\nTraceback (most recent call last):\n  File \"retrain_test.py\", line 25, in <module>\n    {'DecodeJpeg/contents:0': image_data})\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: NodeDef mentions attr 'T' not in Op<name=MaxPool; signature=input:float -> output:float; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](pool/control_dependency)\n     [[Node: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](pool/control_dependency)]]\nCaused by op u'pool', defined at:\n  File \"retrain_test.py\", line 18, in <module>\n    _ = tf.import_graph_def(graph_def, name='')\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 240, in import_graph_def\n    op_def=op_def)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n", "I am having the same problem. I see that the proposed solution was to use nightly build. \nI created a model using `v0.9.0` (which is built by pulling master branch today) and I am trying to use it in  a script which uses `v0.9.0rc0` (installed by pip, must be super nightly for `v0.7.1`). \n\nThe python script is here https://github.com/eldor4do/Tensorflow-Examples/blob/master/retraining-example.py\n\nand full stack trace here :\n\n<pre>\nW tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\nE tensorflow/core/common_runtime/executor.cc:334] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'T' not in Op<name=MaxPool; signature=input:float -> output:float; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool/control_dependency)\n     [[Node: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool/control_dependency)]]\nTraceback (most recent call last):\n  File \"classify.py\", line 51, in <module>\n    run_inference_on_image()\n  File \"classify.py\", line 34, in run_inference_on_image\n    {'DecodeJpeg/contents:0': image_data})\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: NodeDef mentions attr 'T' not in Op<name=MaxPool; signature=input:float -> output:float; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool/control_dependency)\n     [[Node: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool/control_dependency)]]\nCaused by op u'pool', defined at:\n  File \"classify.py\", line 51, in <module>\n    run_inference_on_image()\n  File \"classify.py\", line 28, in run_inference_on_image\n    create_graph()\n  File \"classify.py\", line 15, in create_graph\n    _ = tf.import_graph_def(graph_def, name='')\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 274, in import_graph_def\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n</pre>\n\nCC @petewarden @mrry Please help!\n\n### EDIT / UPDATE:\n\nThis is strange, I used `0.9.0rc0` (installed by pip on OS X) for creating the model as well as utilizing the model, It worked! Previously I created a model with `0.9.0` built from sources on Ubuntu 14.04 and tried to use it on OS X (`0.9.0rc0` from pip), it raised the above exception.\n", "Hi @thammegowda , I installed 0.9.0rc0 but it didn't work. Strange.\n", "@Waffleboy Try using the same version for creating the model as well as for using the model (i.e. retrain.py and classify.py).\n\n(Here are the scripts am I using to retrain & classify - https://github.com/thammegowda/notes/tree/master/deeplearning/imagerec, these are working fine with `v0.9.0rc0` - installed using pip on OS X )\n", "@thammegowda I tried to use your classify.py as below: it gives me the same error:\n\npython classify.py --in=/home/xu/Desktop/Robot/Data/a/0.jpg --model=/home/xu/Desktop/Robot/retrained_graph.pb --labels=/home/xu/Desktop/Robot/retrained_labels.txt  \nW tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\nE tensorflow/core/common_runtime/executor.cc:334] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'T' not in Op<name=MaxPool; signature=input:float -> output:float; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool/control_dependency)\n     [[Node: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool/control_dependency)]]\nTraceback (most recent call last):\n  File \"classify.py\", line 81, in <module>\n    run_inference(images=images, out_file=args['out'], labels=labels, model_file=args['model'])\n  File \"classify.py\", line 35, in run_inference\n    {'DecodeJpeg/contents:0': image_data})\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: NodeDef mentions attr 'T' not in Op<name=MaxPool; signature=input:float -> output:float; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool/control_dependency)\n     [[Node: pool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool/control_dependency)]]\nCaused by op u'pool', defined at:\n  File \"classify.py\", line 81, in <module>\n    run_inference(images=images, out_file=args['out'], labels=labels, model_file=args['model'])\n  File \"classify.py\", line 20, in run_inference\n    create_graph(model_file)\n  File \"classify.py\", line 13, in create_graph\n    _ = tf.import_graph_def(graph_def, name='')\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 274, in import_graph_def\n    op_def=op_def)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n", "@thammegowda  I trained and tested them on the same machine with linux, anaconda, tensorflow 0.9.0. Any suggestion?\n", "@thammegowda  Ok, I reinstalled tensorflow 0.9.0rc0. It still shows the same error.\n\n## (tensorflow)xu@xu-Lenovo-Yoga-2-Pro:~$ pip show tensorflow\n\nMetadata-Version: 2.0\nName: tensorflow\nVersion: 0.9.0rc0\nSummary: TensorFlow helps the tensors flow\nHome-page: http://tensorflow.org/\nAuthor: Google Inc.\nAuthor-email: opensource@google.com\nInstaller: pip\nLicense: Apache 2.0\nLocation: /home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages\nRequires: numpy, six, protobuf, wheel\nClassifiers:\n  Development Status :: 4 - Beta\n  Intended Audience :: Developers\n  Intended Audience :: Education\n  Intended Audience :: Science/Research\n  License :: OSI Approved :: Apache Software License\n  Programming Language :: Python :: 2.7\n  Topic :: Scientific/Engineering :: Mathematics\n  Topic :: Software Development :: Libraries :: Python Modules\n  Topic :: Software Development :: Libraries\nEntry-points:\n  [console_scripts]\n  tensorboard = tensorflow.tensorboard.tensorboard:main\n", "@thammegowda Ok, it works now. I was using example files from tensorflow 0.9.0 from github. I updated it to 0.9.0rc0, now it works. Thank you.\n", "@zxzhijia I am glad you found a way to get around with this issue! Good luck\n"]}, {"number": 1527, "title": "Typo?", "body": "", "comments": ["Can one of the admins verify this patch?\n", "This change looks good. I'm passing it to @vrv to merge.\n"]}, {"number": 1526, "title": "changed predict to use self.batch_size by default in skflow", "body": "Also, this adds an example for skflow that uses sklearn's gridsearchcv.\n\nThis was a PR for skflow when it was a separate repo: https://github.com/tensorflow/skflow/pull/126.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "The [CLA page](https://cla.developers.google.com/clas) seems to always redirect to the individual CLA at the moment, though I clicked on the button to sign the corporate CLA.\n", "Jenkins, test this please.\n", "@mheilman : what did you sign as, so I can check the status of the corp CLA?\n", "@vrv, sorry, I haven't actually signed either because I need to sign the corporate CLA but the link for signing it redirects to the individual CLA (i.e., \"Your Employer\" on [this page](https://cla.developers.google.com/clas) links to [this page](https://cla.developers.google.com/clas/new?domain=DOMAIN_GOOGLE&kind=KIND_INDIVIDUAL)).\n", "Try https://cla.developers.google.com/clas/new?domain=DOMAIN_GOOGLE&kind=KIND_CORPORATE ?\n", "Thanks.  I signed the corporate CLA.  It should be under Civis Analytics.\n", "@mheilman Hey, just wanted to update - corp CLAs take few days to process and I still don't see Civis Analytics in the list. Will keep checking on Monday. And thanks for contribution, hope next time will be smoother!\n", "I just got an email saying that that the license agreement I signed was voided \"for the following reason:\", with no reason listed.  I just emailed cla-submissions@google.com to see what's up.\n\n**Update:** I got a response and need to re-sign the CLA.  I'll comment again when that's done.\n", "OK, we re-signed the CLA.  It should be under Civis Analytics.\n", "Jenkins, please test this!\n\nCLA looks good.\n", "@tensorflow-jenkins: test this please\n\nOur bot doesn't yet understand natural language ;)\n"]}, {"number": 1525, "title": "fix python3 supervisor and server_lib tests", "body": "as title.\n", "comments": ["Can one of the admins verify this patch?\n", "We've fixed those internally, they should come out shortly. Sorry we keep stepping on your feet. Everything will get better once we have the python3 lint running :)\n"]}, {"number": 1524, "title": "Can tensorflow support slicing tensor every N element", "body": "Slicing an array every N elements is a basic function of numpy or python list. However, I found that tensorflow doesn't support it. For example,\n\na = tf.constant(np.random.rand(3,100))\nb = a[:,::4]\nTraceback (most recent call last):\n  File \"/Applications/Eclipse.app/Contents/Eclipse/plugins/org.python.pydev_4.3.0.201508182223/pysrc/pydevd_exec.py\", line 3, in Exec\n    exec exp in global_vars, local_vars\n  File \"<console>\", line 1, in <module>\n  File \"/Users/yin.zheng/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 131, in _SliceHelper\n    \"Steps other than 1 are not currently supported\")\nNotImplementedError: Steps other than 1 are not currently supported\n\nI wonder that how can we use slicing on tensor to select every N elements (rows or lines)?\n\nBest wishes \nYin\n", "comments": ["At present this doesn't work, unless you write some complicated code that uses `tf.transpose()` and `tf.gather()` (euch). @ebrevdo is working on generalizing the slicing code as part of #206, so I'll merge this issue with that one.\n"]}, {"number": 1523, "title": "fixes #1522 close file object properly for read_data()", "body": "Fixes the issue raised here:  https://github.com/tensorflow/tensorflow/issues/1522\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "Jenkins, test this, PLEASE\n", "I don't understand the printouts when I click on \"Details\" for the checks that have failed. Is there something wrong with the code I contributed that caused those errors? Or does it fail because of some other reason? \n\nAlso I am now getting a warning saying `This branch is out-of-date with the base branch` , should I click on the `Update Branch` button? \n", "It looks like you branch might be taken from when the build was in a bad state. Can you please rebase to master and we'll rerun the tests? Thanks!\n", "FYI, you have to tell us when you rebase, github doesn't tell us :/\n\n@tensorflow-jenkins: test this please.\n", "Oh, sorry. Yes, I have clicked the `Update Branch` button on github now,  is that what you mean by rebasing? (apologies for my ignorance, it is my first time contributing code to someone elses project).\n", "I clicked it again after you sent the message. I realise now that i probably reset any tests you initiated with your comment. Sorry :( \n\nCan anyone call on jenkins to perform the tests? Or does it only respond to the admins?  \n", "It only responds to admins right now, sorry :(.  Let me kick it off one more time.  And no worries about any ignorance, we're honored to be your first code contribution!\n\n@tensorflow-jenkins: test this please.\n", "@tensorflow-jenkins: test this please (our tests are now passing so this will actually tell us something)\n", "Should I rebase again? it is saying the branch is out of date again. \n", "every time we merge any other PR, this happens.  It is weird github behavior they recently introduced, because it means everyone would have to rebase after any merge, which is insane for projects that have 30 outstanding PRs.  \n\nsince i have admin powers, I can merge though :)\n", "cool :) \n"]}, {"number": 1522, "title": "closing file object in read_data() function for word2vec tutorials", "body": "The `read_data()` function that appears in the following files:\n- tensorflow/tensorflow/examples/tutorials/word2vec/word2vec_basic.py\n- tensorflow/tensorflow/examples/udacity/5_word2vec.ipynb\n\nLook like this: \n\n```\ndef read_data(filename):\n  f = zipfile.ZipFile(filename)\n  for name in f.namelist():\n    return f.read(name).split()\n  f.close()\n```\n\n```\ndef read_data(filename):\n  f = zipfile.ZipFile(filename)\n  for name in f.namelist():\n    return tf.compat.as_str(f.read(name)).split()\n  f.close()\n```\n\nThere are two problems that these two versions of the function share: \n1. The for loop is redundant since the `return` statement exits the loop (and the function) with the first file in the loop. Also, there is only one file within the zip file anyway. \n2. Since the return statement exits the function early, it never executes the `f.close()` line. Therefore it doesn't get to close the file object properly. \n\nI propose something like this instead: \n\n```\ndef read_data(filename):\n  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n  with zipfile.ZipFile(filename) as f:\n    data = f.read(f.namelist()[0]).split()\n  return data\n```\n\n```\ndef read_data(filename):\n  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n  with zipfile.ZipFile(filename) as f:\n    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n  return data\n```\n\nI will submit a pull request with the proposed changes. \n", "comments": []}, {"number": 1521, "title": "[skflow] Corrected link to example and added link to new blog post", "body": "", "comments": ["Can one of the admins verify this patch?\n", "cc: @ilblackdragon When will these tests be running? Looks like it's stuck last night. \n", "@tensorflow-jenkins: test this please\n", "I think this can be merged. Errors are not related. \n"]}, {"number": 1520, "title": "Fixes tensorflow/skflow#148 - off by one error", "body": "Fixes tensorflow/skflow#148 - error in neural translation example for sampling validation examples.\n", "comments": ["Jenkins, test this please.\n", "@mrry, targets that fails are all not in skflow code:\n//tensorflow/contrib/linear_optimizer:sdca_ops_test                      FAILED in 2.7s\n  /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/workspace/bazel-out/local_linux-py3-opt/testlogs/tensorflow/contrib/linear_optimizer/sdca_ops_test/test.log\n//tensorflow/python:server_lib_test                                      FAILED in 1.5s\n  /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/workspace/bazel-out/local_linux-py3-opt/testlogs/tensorflow/python/server_lib_test/test.log\n//tensorflow/python:session_test                                         FAILED in 2.6s\n  /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/workspace/bazel-out/local_linux-py3-opt/testlogs/tensorflow/python/session_test/test.log\n//tensorflow/python:supervisor_test                                      FAILED in 12.4s\n  /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/workspace/bazel-out/local_linux-py3-opt/testlogs/tensorflow/python/supervisor_test/test.log\n//tensorflow/tensorboard/backend:server_test                             FAILED in 7.8s\n  /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/workspace/bazel-out/local_linux-py3-opt/testlogs/tensorflow/tensorboard/backend/server_test/test.log\n", "Ah, looks like the various fixes haven't made it out yet. Can you rebase against HEAD and we'll merge it then?\n", "Done.\n", "Jenkins, test this please.\n"]}, {"number": 1519, "title": "Number of broadcasting dimensions is limited", "body": "### Environment info\n\nOperating System: Ubuntu 14.04.4\n\nIf installed from sources, provide the commit hash: f92e338b178f75b0e7fc79aedc00680a49a982ac\n### Steps to reproduce\n\nRunning this script:\n\n``` python\nimport tensorflow as tf\n\nshape_1 = (1, 20, 30, 1, 50)\nshape_2 = (30, 1, 30, 20, 50)\n\nA = tf.Variable(tf.random_normal(shape_1))\nB = tf.Variable(tf.random_normal(shape_2))\n\nwith tf.Session() as S:\n    S.run(tf.initialize_all_variables())\n    C = S.run(A+B)\n```\n\nproduces\n\n``` python\ntensorflow.python.framework.errors.UnimplementedError: Broadcast between [1,20,30,1,50] and [30,1,30,20,50] is not supported yet.\n```\n### What did you try?\n\nReducing the tensor ranks results in success.\n\n``` python\nshape_1 = (1, 20, 30)\nshape_2 = (30, 1, 30)\n```\n### Logs or other output that would be helpful\n\n``` python\nTraceback (most recent call last):\n  File \"tf_broadcast_issue.py\", line 11, in <module>\n    C = S.run(A+B)\n  File \"/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/client/session.py\", line 332, in run\n    result = self._run(None, fetches, feed_dict, options_ptr, run_outputs_ptr)\n  File \"/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/client/session.py\", line 536, in _run\n    feed_dict_string, options, run_outputs)\n  File \"/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/client/session.py\", line 598, in _do_run\n    target_list, options, run_outputs)\n  File \"/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/client/session.py\", line 620, in _do_call\n    e.code)\ntensorflow.python.framework.errors.UnimplementedError: Broadcast between [1,20,30,1,50] and [30,1,30,20,50] is not supported yet.\n     [[Node: add = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable/read, Variable_1/read)]]\nCaused by op u'add', defined at:\n  File \"tf_broadcast_issue.py\", line 11, in <module>\n    C = S.run(A+B)\n  File \"/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/ops/variables.py\", line 544, in <lambda>\n    setattr(Variable, operator, lambda a, b: Variable._RunOp(operator, a, b))\n  File \"/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/ops/variables.py\", line 559, in _RunOp\n    return getattr(ops.Tensor, operator)(a._AsTensor(), b)\n  File \"/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/ops/math_ops.py\", line 502, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/ops/gen_math_ops.py\", line 44, in add\n    return _op_def_lib.apply_op(\"Add\", x=x, y=y, name=name)\n  File \"/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/framework/ops.py\", line 2102, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/framework/ops.py\", line 1129, in __init__\n    self._traceback = _extract_stack()\n```\n", "comments": ["#206 might resolve this, but I've logged it separately in case.\n", "Looks like member `BinaryOp.Compute` in [cwise_ops_common.h](https://github.com/tensorflow/tensorflow/blob/f92e338b178f75b0e7fc79aedc00680a49a982ac/tensorflow/core/kernels/cwise_ops_common.h#L85-L139) currently only supports up to two broadcasting dimensions.\n", "I think we've limited the number of dimensions to keep binary size down, rather than because it's any more complicated to broadcast on more dimensions. @benoitsteiner, is there some way we could handle higher dimensions in Eigen without another specialization, even if it leads to a slower codepath?\n", "@mrry If I'm not mistaken, the broadcasts we need in the examples above are [30, 1, 1, 20, 1] for the first tensor, and [1, 20, 1, 1, 1] for the second one. For the first tensor, we could collapse the 2nd and 3rd dimensions together, and apply a  [30, 1, 20, 1] broadcast. For the second one, we could similarly reshape the tensor into a 3d tensor by collapsing the last 3 dimensions, and apply a [1, 20, 1] broadcast.\n", "I want to caculate the Kronecker product of two matrix, and \ntf.reshape(matrix_A, [2,1,2,1]) \\* tf.reshape(matrix_B, [1,3,1,3]) raised the above UnimpementedError.\nI circumvented this broadcasting by using tf.tile to explictly broadcast this two matrix. \n\nIs there an easy way to do this broadcasting in tensorflow, like in Numpy?\n\nUpdate:\nI find tf.transpose can do things like dimshuffle in theano, so the problems above can be handled by combination of shuffle and broadcasting.\n", "@benoitsteiner any fix should be sure to cover the cases in #3534 as well.\n", "@benoitsteiner Friendly ping, please update this bug with the current status, thanks!\n", "Fixed by #4378."]}, {"number": 1518, "title": "Anyway to save the session as an object but not a file?", "body": "Hi,\n\nI know that the tensorflow can save all variables into a file as a checkpoint\nIs there anyway to save the tensor flow session into an object? like\na=Queue()\na.put( sess.status({%variables%}) )  #or maybe a.put( tf.train.status({%variables%}) )\n\nSo when I want to restore a session status. I just do a.get()\n\nIs that possible?\n\nThank you.\n", "comments": ["I'm not sure what semantics you're suggesting for `sess.status(...)` or `tf.train.status(...)`. If you're within the same Python process, you can treat a `tf.Session` like any other object.\n\nA `tf.Session` is not serializable, but you can use the `tf.MetaGraphDef` protocol buffer, along with a checkpoint file, to represent an entire model.\n", "I understand that tf.Session is as any other object. Maybe I did not explain my question clearly.\n\nsession_at_step1=tf.session().a_function\ntf.session().run(...some feeds....)\nsession_at_step2=tf.session().a_function\n\na_function may be \"copy()\", which means copy everything in the session to make a new session.\nor may be \"copy( (weight_1, bias_1) )\" to only copy the weight_1 and bias_1 and form a new session.\n\nIs there any a function to let session_at_step1 be different with session_at_step2?\n", "I think the answer is \"probably no,\" but I'm still unclear on what you want the behavior of `????` to be. If you describe a high-level use case, we would consider a feature request.\n", "I've updated the question and made it clear. I mean, instead of having a handle(pointer) to the session, if there is anyway to copy the session. \n", "Ah, ok. Right now there is no way to copy an entire session, and the best way to do what you're asking about is to use the `tf.train.Saver` to write out a `MetaGraphDef` proto, and import that into another session using `tf.train.import_meta_graph()`. This is still quite a new feature, but you can look at [the relevant tests](https://github.com/tensorflow/tensorflow/blob/df2ea2c8fb6fc2a653843ec521adb52305f1679f/tensorflow/python/training/saver_test.py#L762) to see an example of how it's used. It _does_ give you the ability to restore an entire graph and resume training etc.\n\nIf there is any improved tooling you'd like to see around this format, please let us know in a feature request!\n"]}, {"number": 1517, "title": "Extend existing ops to complex128", "body": "This is a follow-up to #1420.\nNow that `complex128` is supported in tensorflow, we should extend existing ops like `complex`, `real`, `imag`, \u2026 to support `complex128` as well.\nI've already done some work on this, but I'm currently stuck on expressing dependencies between input and output types.\nFor example, the spec for the `Real` op is currently\n\n``` c++\nREGISTER_OP(\"Real\")\n    .Input(\"in: complex64\")\n    .Output(\"out: float\");\n```\n\nI've extended this to\n\n``` c++\nREGISTER_OP(\"Real\")\n    .Input(\"in: T\")\n    .Output(\"out: Tout\")\n    .Attr(\"T: {complex64, complex128} = DT_COMPLEX64\")\n    .Attr(\"Tout: {float, double} = DT_FLOAT\");\n```\n\nand specified corresponding `Tout` constraints when registering the kernels.\n\nThe problem is that the auto-generated Python op will default to `float32` for the output.\n(It also turns `Tout` into an optional argument).\nI'd like to express \"If `T==complex128`, then `Tout==float64`\" somehow.\n\nThe only way of doing it that I could come up with is by writing a Python wrapper that essentially does\n\n``` python\ndef real(in):\n    if in.dtype == tf.complex64:\n        Tout = tf.float32\n    elif in.dtype == tf.complex128:\n        Tout = tf.float64\n    return generated_real(in, Tout=Tout)\n```\n\nWould that be an acceptable solution, or is there a better one?\n", "comments": ["This sounds like it might have consequences for backwards compatibility. @girving and @josh11b are our resident experts on that.\n", "Checking with Josh.  I think we may need an extra feature.\n", "@josh11b: In a perfect world the op spec would say you can't take the real part of a `complex128` and get a `float32`.  Reasonable to just ban that by not registering the kernels?\n\n@ibab: If so, your solution is fine.  I'd change it to `Tout=in.dtype.real_dtype`, but otherwise good.  There will be no backwards compatibility issues since both defaults are the old values.\n", "@ibab @girving I believe [numexpr](https://github.com/pydata/numexpr) has to consider similar issues when dealing with conversion between between float32, float64, complex64 and complex128. A contributor suggested the use of a casting table, https://github.com/pydata/numexpr/pull/181 may provide more insight.\n", "I recommend writing a Python wrapper that sets Tout based on the input type.  We don't have a better mechanism than that at the moment.\n", "+1 to @josh11b.  @sjperkins: Since this can easily be handled at the Python wrapper level as @ibab described, we'd like to avoid complicated the low level GraphDef structure. \n"]}, {"number": 1516, "title": "can't seem to find the urls to your provided binary packages", "body": "I was expecting to download a portable package (.whl or .egg) and use it as such:\n\n```\nimport sys; sys.path.append( '/extensions/TensorFlow...win32.egg' )\nimport tensorflow # if this line doesn't work, the egg/whl is not portable.\n# ...\n```\n\nwhen you say \"provided binary packages\" that just means I expect .pyd files within the egg or whl.\n\nfor an example of a portable egg, just look at pillow ;)\n", "comments": ["Are you asking about Windows support? (If I understand correctly, `.pyd` files are Windows DLLs.)\n\nWe don't currently support Windows. See issue #17 for a tracking bug for Windows support.\n", "ah alright, well just keep that method in mind then, I'll be watching ;)\n", "Closing this as a duplicate of #17. Hopefully it won't be too much effort to support that usage model as well!\n"]}, {"number": 1515, "title": "rnn.bidirectional_rnn  cause a problem", "body": "code:\n\n``` python\nimport tensorflow as tf\nfrom tensorflow.models.rnn import rnn,rnn_cell\n\nseq_length = 5\nrnn_size = 6\ninput_data = [[1,2,3,4,5],[6,1,8,9,10],[11,12,1,14,15]]\nwith tf.device(\"/cpu:0\"):\n embedding = tf.get_variable(\"embedding\", [20, rnn_size])\n result = tf.nn.embedding_lookup(embedding, input_data)\n inputs = tf.split(1,seq_length,result)\n inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n\ncell_bw = rnn_cell.BasicLSTMCell(rnn_size)\ncell_fw = rnn_cell.BasicLSTMCell(rnn_size)\nbi = rnn.bidirectional_rnn(cell_fw,cell_bw,inputs,cell_fw.zero_state(3, tf.float32),cell_bw.zero_state(3, tf.float32),sequence_length=5)\n\nwith tf.Session() as session:\n    tf.initialize_all_variables().run()\n    print(session.run(bi))\n```\n\nerror:\n\"Shapes %s and %s must have the same rank\" % (self, other))\nValueError: Shapes TensorShape([]) and TensorShape([Dimension(None)]) must have the same rank\n\nit seem that the inputs have a problem, Can you tell me why ?\n", "comments": ["Can you include a full stack trace? When I try to run your code I get a different error (also shape-related).\n", "(Assigning this to @ebrevdo, since he's most familiar with this code. I suspect he'll also need a stack trace.)\n", "Yes - stack trace is helpful.  But from the get-go looks like you're feeding a sequence_length scalar but should feed a [batch_size] length vector; perhaps [5, 5, 5] ?\n", "@ebrevdo, You are right. Problem has been solved\u3002\n\nThank you for your attention\n", "Great!  Closing this issue.\n"]}, {"number": 1514, "title": "Bugfix to test/run_and_gather_logs.", "body": "- Now soft exit if cpuinfo/psutils not found.\n- Uncomment test again.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Should I resubmit from my corporate email address?\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Jenkins, test this please?\n", "@ebrevdo can you please resolve the conflicts? Ideally rebase to new master?\n", "Bug fixed internally.  Closed.\n"]}, {"number": 1513, "title": "seems issues with softmax_cross_entropy_with_logits", "body": "matrix1 = tf.constant([[ -2.88181912e+11,-4.40300175e+11], [ -2.29688918e+11, -3.53027424e+11] ,[ -2.14650274e+11, -3.81851140e+11]])\nmatrix2 = tf.constant([[0, 1.0], [0, 1], [0, 1]])\nr = tf.nn.softmax_cross_entropy_with_logits(matrix1, matrix2)\n\nwith tf.Session() as sess:\n    result = sess.run(r)\n    print result\n\nwhen i run this, i get:\n\n[  1.52118264e+11   1.23338506e+11   1.67200866e+11]\n\nis this wrong ?\n", "comments": ["This seems like the intended behavior. You have a completely misclassified set of logits, and the cross entropy loss is very large.\n\nWhat were you expecting to see instead?\n", "@mrry, I find this because I am running an AlexNet, and after a few iterations, i get Floating Point Exception, I guess this maybe the number is too large, can you give some advice to fix this?\n", "its the problem of my network parameters, sorry for this.\n"]}, {"number": 1512, "title": "some learning decays from Stanford CS231n Karpathy lecture 6", "body": "exponential time decay\nstep decay\n1/t decay\nfrom Lecture 6-39 Stanford CS231n \n", "comments": ["Can one of the admins verify this patch?\n", "@vrv I've addressed your comments and made the step time decay function take the initial learning rate as a parameter instead of passing the learning rate back in every time.  Additional comments welcome.  Thanks.\n", "It's been over a month since the last activity, so I'm closing this -- feel free to re-open when you have something to update!\n", "@vrv Seems like I got testing working again.  I have updated this pull request with the changes you suggested.  Please let me know if there are any problems.\n"]}, {"number": 1511, "title": "No gradients  provided for any variable ?", "body": "Hi,\n\nWhen using tensorflow, I found_ '**ValueError: No gradients provided for any variable**' _\n\nI used AdamOptimizer and GradientDescentOptimizer, and I could see this same error.\n\nI didn't used tf.argmax but tf.nn.softmax, tf.tile, tf.nn.relu, tf.matmul, tf.tanh, tf.tile, tf.mul ...\n\nWhat is the reason for this error??\nand How can I solve this one??\n\nThanks !\n", "comments": ["This typically means that there is no path between any of the `tf.Variable` objects that you've defined and the loss tensor that you are trying to minimize. As a sanity check, does `tf.trainable_variables()` contain any variables? \n\nTo figure this out, we'll need to see the code that is failing. Can you reply with more details?\n", "I'm closing this for now&mdash;since there's no way to reproduce your error&mdash;but feel free to reopen if you can provide more details.\n", "I know this is closed, but I thought I might mention for the sake of Googler's: most non-linear operations (such as tf.cast) do not provide gradients. However, there might be a method to specify the gradient of a custom operation (e.g. piecewise linear trig function) . See here: #1095 .\n", "I was also having a similar problem, and found out it was because I had a `tf.round()` in my cost function, which is not really differentiable I think.\n", "I have the same problem, however, I could not find the reason. How to solve it? Thanks very much!\n\n`prediction_depth = sess.run(prediction_depth)\ndepth_loss = tf.sqrt(tf.reduce_mean(tf.square(prediction_depth - train_label_aug)))\ndepth_loss = tf.log(depth_loss)\nprint sess.run(depth_loss)\noptimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(depth_loss)\n_, l = sess.run([optimizer, depth_loss])`\n\n File \"main.py\", line 304, in main\n    optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(depth_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 198, in minimize\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 309, in apply_gradients\n    (converted_grads_and_vars,))\nValueError: No gradients provided for any variable: \n", "Your issue is at this line of code:\n\n`prediction_depth = sess.run(prediction_depth)`\n\nBasically, this makes `prediction_depth` become a number and it is no longer a tensor variable). It means you have just cut the connection between your `depth_loss` and every variables in your model.\n\nYou should remove that line and try running again.\n", "Hi Team,\n\nI am running into the same problem when I cast my tensor to float32. But without casting, I get the error that the expected type is float and not int. So, either way, I can't find a way to proceed...\n\nIn my setting, I am trying to minimize the squared error of the difference of two tensors.\n\n```\nsoftmax_w = tf.Variable(tf.zeros([SIZE_LSTM_UNITS, NUM_CLASSES], dtype=tf.float32))\nsoftmax_b = tf.Variable(tf.zeros([NUM_CLASSES], dtype=tf.float32))\nlogits = tf.matmul(out, softmax_w) + softmax_b\n```\n\nIf I compute the loss with casting as below: \n\n```\npredDiff = tf.cast(tf.sub(tf.arg_max(logits, 1), tf.arg_max(train_labels, 1)), tf.float32)\nl2loss = tf.nn.l2_loss(predDiff)\ntrainStep = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(l2loss)\n```\n\n(where, logits and train_labels are 1-hot vectors), then I get the following error: \n`trainStep = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(l2loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 198, in minimize\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 309, in apply_gradients\n    (converted_grads_and_vars,))\nValueError: No gradients provided for any variable: ((None, <tensorflow.python.ops.variables.Variable object at 0x7f2c7363bf90>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce284e9d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce284e510>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce26cf050>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce26cf450>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce2c9d510>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce287ae90>))`\n\nInstead, if I compute the loss without casting as below:\n\n```\npredDiff = tf.sub(tf.arg_max(logits, 1), tf.arg_max(train_labels, 1))\n```\n\nthen, I get the following error: \n`trainStep = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(l2loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\n    grad_loss=grad_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 238, in compute_gradients\n    self._assert_valid_dtypes([loss])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 379, in _assert_valid_dtypes\n    dtype, t.name, [v for v in valid_dtypes]))\nValueError: Invalid type tf.int64 for L2Loss:0, expected: [tf.float32, tf.float64, tf.float16].`\n\nNot sure if I am missing something obvious. Would it be possible for you to give any pointers? \n\nThanks a lot!\n", "@rosecatherinek For help with debugging your model, we'd recommend you post on Stack Overflow instead. (This doesn't seem to be a bug in TensorFlow, so much as a model architecture problem.)\n", "Thank you for your quick response. Will post on Stack Overflow.\n", "This can be hard to debug, and the reasons might be quite subtle, \r\n\r\nfor example if for some reason you switched arguments in softmax_cross_entropy_with_logits:\r\n\r\n`tf.nn.softmax_cross_entropy_with_logits(y,_y)` to\r\n`tf.nn.softmax_cross_entropy_with_logits(_y,y)`\r\n\r\nyou get the dreaded _No gradients provided for any variable_", "@xuanchien thanks very much! I have solved this problem according to your advice.", "Same problem, should be something like this:\r\n`losses = tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=self.input_y)`", "I have the same problem, however, I could not find the reason. How to solve it? Thanks very much!\r\nThe code contains two parts, first is read data:\r\n`import tensorflow as tf\r\nimport matplotlib.pyplot as plt \r\nfrom PIL import Image\r\n# import tensorflow.examples.tutorials.mnist.input_data as input_data\r\n\r\ndef read_and_decode(file_name):\r\n    filename_queue = tf.train.string_input_producer([file_name])\r\n\r\n    reader = tf.TFRecordReader() \r\n    _, serialized_example = reader.read(filename_queue)\r\n       \r\n    features = tf.parse_single_example(\r\n        serialized_example,\r\n        features = {\r\n            'height': tf.FixedLenFeature([], tf.int64),  \r\n            'width': tf.FixedLenFeature([], tf.int64),  \r\n            'depth': tf.FixedLenFeature([], tf.int64),   \r\n            'label': tf.FixedLenFeature([], tf.int64),   \r\n            'image_raw': tf.FixedLenFeature([], tf.string)\r\n        }\r\n    )\r\n\r\n    img = tf.decode_raw(features['image_raw'], tf.uint8)\r\n    img = tf.reshape(img, [32, 32, 3])\r\n    img = tf.cast(img, tf.float32) * (1./ 255) - 0.5\r\n    # img = tf.cast(img, tf.float32)\r\n    label = tf.cast(features['label'], tf.float32)\r\n    return img, label\r\n\r\ndef fetch_data(file_name, batch_size): \r\n    image, label = read_and_decode(file_name)\r\n    image_batch, label_batch = tf.train.shuffle_batch(\r\n                                            [image, label], \r\n                                            batch_size = batch_size, \r\n                                            capacity = 2000, \r\n                                            min_after_dequeue = 1000\r\n                                            )\r\n    init_op = tf.global_variables_initializer()\r\n\r\n    with tf.Session() as sess: \r\n        coord   = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(sess = sess, coord=coord)\r\n        sess.run(init_op)\r\n        #image = Image.fromarray(image, 'RGB')    \r\n        coord.request_stop()\r\n        coord.join(threads) \r\n    return image_batch, tf.convert_to_tensor([label_batch])\r\n`\r\n\r\nand second is classification:\r\n`# -*- coding: UTF-8 -*-\r\nimport tensorflow as tf\r\nimport read_data\r\nimport numpy as np\r\ndef conv_op(input_op, name, kh, kw, n_out, dh, dw, p):\r\n    n_in = input_op.get_shape()[-1].value # \u83b7\u53d6input_op\u7684\u901a\u9053\u6570\r\n\r\n    with tf.name_scope(name) as scope: # \u8bbe\u7f6escope\uff0c\u751f\u6210\u7684Variable\u4f7f\u7528\u9ed8\u8ba4\u7684\u547d\u540d\r\n        kernel = tf.get_variable(scope+\"w\",  # kernel\uff08\u5373\u5377\u79ef\u6838\u53c2\u6570\uff09\u4f7f\u7528tf.get_variable\u521b\u5efa\r\n                                 shape=[kh, kw, n_in, n_out], # \u3010\u5377\u79ef\u6838\u7684\u9ad8\uff0c\u5377\u79ef\u6838\u7684\u5bbd\u3001\u8f93\u5165\u901a\u9053\u6570\uff0c\u8f93\u51fa\u901a\u9053\u6570\u3011\r\n                                 dtype=tf.float32, \r\n                                 initializer=tf.contrib.layers.xavier_initializer_conv2d()) # \u53c2\u6570\u521d\u59cb\u5316\r\n        # \u4f7f\u7528tf.nn.conv2d\u5bf9input_op\u8fdb\u884c\u5377\u79ef\u5904\u7406\uff0c\u5377\u79ef\u6838kernel\uff0c\u6b65\u957fdh*dw\uff0cpadding\u6a21\u5f0f\u4e3aSAME\r\n        conv = tf.nn.conv2d(input_op, kernel, (1, dh, dw, 1), padding='SAME') \r\n        bias_init_val = tf.constant(0.0, shape=[n_out], dtype=tf.float32) # biases\u4f7f\u7528tf.constant\u8d4b\u503c\u4e3a0\r\n        biases = tf.Variable(bias_init_val, trainable=True, name='b') # \u5c06bias_init_val\u8f6c\u6210\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\r\n        z = tf.nn.bias_add(conv, biases) # \u5c06\u5377\u79ef\u7ed3\u679cconv\u548cbias\u76f8\u52a0\r\n        activation = tf.nn.relu(z, name=scope) # \u5bf9z\u8fdb\u884c\u975e\u7ebf\u6027\u5904\u7406\u5f97\u5230activation\r\n        p += [kernel, biases]  # \u521b\u5efa\u5377\u79ef\u5c42\u65f6\u7528\u5230\u7684\u53c2\u6570kernel\u548cbias\u6dfb\u52a0\u8fdb\u53c2\u6570\u5217\u8868\r\n        return activation # \u5c06\u5377\u79ef\u5c42\u7684\u8f93\u51faactivation\u4f5c\u4e3a\u51fd\u6570\u7ed3\u679c\u8fd4\u56de\r\n\r\ndef fc_op(input_op, name, n_out, p):  \r\n    n_in = input_op.get_shape()[-1].value # \u83b7\u53d6tensor\u7684\u901a\u9053\u6570 \r\n    with tf.name_scope(name) as scope:\r\n        kernel = tf.get_variable(scope+\"w\", # \u4f7f\u7528tf.get_variable\u521b\u5efa\u5168\u8fde\u63a5\u5c42\u7684\u53c2\u6570\r\n                                 shape=[n_in, n_out], # \u53c2\u6570\u7684\u7ef4\u5ea6\u6709\u4e24\u4e2a\uff0c\u8f93\u5165\u901a\u9053\u6570\u548c\u8f93\u51fa\u901a\u9053\u6570\r\n                                 dtype=tf.float32, \r\n                                 initializer=tf.contrib.layers.xavier_initializer())\r\n        # biases\u8d4b\u503c0.1\u4ee5\u907f\u514ddead neuron\r\n        biases = tf.Variable(tf.constant(0.1, shape=[n_out], dtype=tf.float32), name='b') \r\n        # \u5bf9\u8f93\u5165\u53d8\u91cfinput_op\u548ckernel\u505a\u77e9\u9635\u4e58\u6cd5\u5e76\u52a0\u4e0abiases\u3002\u518d\u505a\u975e\u7ebf\u6027\u53d8\u6362activation\r\n        activation = tf.nn.relu_layer(input_op, kernel, biases, name=scope) \r\n        p += [kernel, biases]\r\n        return activation\r\n\r\ndef mpool_op(input_op, name, kh, kw, dh, dw): \r\n    return tf.nn.max_pool(input_op,\r\n                          ksize=[1, kh, kw, 1], # \u6c60\u5316\u5c42\u5c3a\u5bf8kh*kw\r\n                          strides=[1, dh, dw, 1], # \u6b65\u957fdh*dw\r\n                          padding='SAME',\r\n                          name=name)\r\n\r\ndef inference_op(input_op, keep_prob):\r\n    # \u521d\u59cb\u5316\u53c2\u6570\u5217\u8868p\r\n    p = []\r\n    # assume input_op shape is 224x224x3\uff08\u7b2c\u4e00\u4e2a\u5377\u79ef\u5c42\u7684\u8f93\u5165input_op\uff09\r\n\r\n    # \u521b\u5efa\u7b2c\u4e00\u6bb5\u5377\u79ef\u7f51\u7edc -- outputs 112x112x64\r\n    # \u4e24\u4e2a\u5377\u79ef\u5c42\u7684\u5377\u79ef\u6838\u90fd\u662f3*3\uff0c\u5377\u79ef\u6838\u6570\u91cf\uff08\u8f93\u51fa\u901a\u9053\u6570\uff09\u5747\u4e3a64\uff0c\u6b65\u957f1*1\uff0c\u5168\u50cf\u7d20\u626b\u63cf\u3002\r\n    conv1_1 = conv_op(input_op, name=\"conv1_1\", kh=3, kw=3, n_out=64, dh=1, dw=1, p=p) # outputs 224x224x64\r\n    conv1_2 = conv_op(conv1_1,  name=\"conv1_2\", kh=3, kw=3, n_out=64, dh=1, dw=1, p=p) # outputs 224x224x64\r\n    pool1  = mpool_op(conv1_2,  name=\"pool1\",   kh=2, kw=2, dw=2, dh=2) # \u6807\u51c6\u76842*2\u7684\u6700\u5927\u6c60\u5316-outputs 112x112x64\r\n\r\n    # \u521b\u5efa\u7b2c\u4e8c\u6bb5\u5377\u79ef\u7f51\u7edc -- outputs 56x56x128\r\n    conv2_1 = conv_op(pool1,    name=\"conv2_1\", kh=3, kw=3, n_out=128, dh=1, dw=1, p=p)\r\n    conv2_2 = conv_op(conv2_1,  name=\"conv2_2\", kh=3, kw=3, n_out=128, dh=1, dw=1, p=p)\r\n    pool2 = mpool_op(conv2_2,   name=\"pool2\",   kh=2, kw=2, dh=2, dw=2)\r\n\r\n    # \u521b\u5efa\u7b2c\u4e09\u6bb5\u5377\u79ef\u7f51\u7edc -- outputs 28x28x256\r\n    # conv3_1 = conv_op(pool2,    name=\"conv3_1\", kh=3, kw=3, n_out=256, dh=1, dw=1, p=p)\r\n    # conv3_2 = conv_op(conv3_1,  name=\"conv3_2\", kh=3, kw=3, n_out=256, dh=1, dw=1, p=p)\r\n    # conv3_3 = conv_op(conv3_2,  name=\"conv3_3\", kh=3, kw=3, n_out=256, dh=1, dw=1, p=p)    \r\n    # pool3 = mpool_op(conv3_3,   name=\"pool3\",   kh=2, kw=2, dh=2, dw=2)\r\n\r\n    # # \u521b\u5efa\u7b2c\u56db\u6bb5\u5377\u79ef\u7f51\u7edc -- outputs 14x14x512\r\n    # conv4_1 = conv_op(pool3,    name=\"conv4_1\", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)\r\n    # conv4_2 = conv_op(conv4_1,  name=\"conv4_2\", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)\r\n    # conv4_3 = conv_op(conv4_2,  name=\"conv4_3\", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)\r\n    # pool4 = mpool_op(conv4_3,   name=\"pool4\",   kh=2, kw=2, dh=2, dw=2)\r\n\r\n    # # \u521b\u5efa\u7b2c\u4e94\u6bb5\u5377\u79ef\u7f51\u7edc -- outputs 7x7x512\r\n    # conv5_1 = conv_op(pool4,    name=\"conv5_1\", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)\r\n    # conv5_2 = conv_op(conv5_1,  name=\"conv5_2\", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)\r\n    # conv5_3 = conv_op(conv5_2,  name=\"conv5_3\", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)\r\n    # pool5 = mpool_op(conv5_3,   name=\"pool5\",   kh=2, kw=2, dw=2, dh=2)\r\n\r\n    # \u5907\u6ce8\uff1aVGGNet-16\u7684\u6bcf\u4e00\u6bb5\u5377\u79ef\u7f51\u7edc\u90fd\u4f1a\u5c06\u56fe\u50cf\u7684\u8fb9\u957f\u7f29\u5c0f\u4e00\u534a\uff0c\u4f46\u662f\u5c06\u5377\u79ef\u8f93\u51fa\u901a\u9053\u6570\u7ffb\u500d\u3002\r\n    # \u7b2c\u4e94\u6bb5\u5377\u79ef\u8f93\u51fa\u7684\u901a\u9053\u6570\u4e0d\u518d\u589e\u52a0\u3002\r\n\r\n    # flatten \u5c06\u7b2c\u4e94\u6bb5\u5377\u79ef\u7f51\u7edc\u7684\u8f93\u51fa\u7ed3\u679c\u8fdb\u884c\u6241\u5e73\u5316\r\n    shp = pool2.get_shape()\r\n    flattened_shape = shp[1].value * shp[2].value * shp[3].value\r\n\r\n    # tf.reshape\u51fd\u6570\u5c06\u6bcf\u4e2a\u6837\u672c\u5316\u4e3a\u957f\u5ea67*7*512 = 25088\u7684\u5411\u91cf\r\n    resh1 = tf.reshape(pool2, [-1, flattened_shape], name=\"resh1\") \r\n\r\n    # fully connected \u9690\u542b\u8282\u70b94096\u7684\u5168\u8fde\u63a5\u5c42\r\n    fc6 = fc_op(resh1, name=\"fc6\", n_out=4096, p=p)\r\n    fc6_drop = tf.nn.dropout(fc6, keep_prob, name=\"fc6_drop\")\r\n\r\n    fc7 = fc_op(fc6_drop, name=\"fc7\", n_out=4096, p=p)\r\n    fc7_drop = tf.nn.dropout(fc7, keep_prob, name=\"fc7_drop\")\r\n\r\n    fc8 = fc_op(fc7_drop, name=\"fc8\", n_out=10, p=p)\r\n    softmax = tf.nn.softmax(fc8) # \u5f97\u5230\u5206\u7c7b\u8f93\u51fa\u6982\u7387\r\n    predictions = tf.argmax(softmax, 1) # tf.argmax\u6c42\u8f93\u51fa\u6982\u7387\u6700\u5927\u7c7b\u522b\r\n    return tf.convert_to_tensor([predictions]), softmax, fc8, p\r\n\r\ndef main(file_name, batch_size, iter_times):\r\n    x = tf.placeholder('float', shape = [batch_size, 32, 32, 3])\r\n    y = tf.placeholder('float', shape = [1, batch_size])\r\n    predictions, _, _, _ = inference_op(x, keep_prob = 0.5)\r\n    predictions = tf.cast(predictions, tf.float32)\r\n\r\n    ##### tf.equal \u8fd4\u56de\u7684\u662fbool tensor  ######\r\n    ##### tf.reduce_mean() \u4e0d\u80fd\u662fbool\u503c ######\r\n    cross_entropy = -tf.reduce_sum(y * tf.log(predictions))\r\n    train = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\n    sess = tf.Session()\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n    \r\n    for i in xrange(iter_times):\r\n        train_x, train_y = read_data.fetch_data(file_name, batch_size) \r\n        # train_x, train_y = sess.run([train_x, train_y])\r\n        train.eval(feedict = {x: train_x, y: train_y})\r\n        \r\n        if i % 100 == 0 :\r\n            print \"%d step accuarcy is %f\" % (i, sess.run(cross_entropy))\r\n\r\nmain('train.tfrecords', 30, 2000)`", "for \r\n```\r\nwith tf.name_scope('loss'):\r\n    #cross_entropy = None\r\n    val = tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y_conv )\r\n    cross_entropy = tf.reduce_mean(val)\r\n\r\nwith tf.name_scope('adam_optimizer'):\r\n    #train_step = None\r\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\n```\r\n\r\nI get this error:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-10-65094b0104ba> in <module>()\r\n      6 with tf.name_scope('adam_optimizer'):\r\n      7     #train_step = None\r\n----> 8     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\n\r\n~/anaconda/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\r\n    320           \"No gradients provided for any variable, check your graph for ops\"\r\n    321           \" that do not support gradients, between variables %s and loss %s.\" %\r\n--> 322           ([str(v) for _, v in grads_and_vars], loss))\r\n    323 \r\n    324     return self.apply_gradients(grads_and_vars, global_step=global_step,\r\n\r\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(32,) dtype=float32_ref>\", \"<tf.Variable 'Variable_1:0' shape=(64,) dtype=float32_ref>\", \"<tf.Variable 'Variable_2:0' shape=(1024,) dtype=float32_ref>\", \"<tf.Variable 'Variable_3:0' shape=(10,) dtype=float32_ref>\", \"<tf.Variable 'Variable_4:0' shape=(5, 5, 1, 32) dtype=float32_ref>\", \"<tf.Variable 'Variable_5:0' shape=(5, 5, 32, 64) dtype=float32_ref>\", \"<tf.Variable 'Variable_6:0' shape=(3136, 1024) dtype=float32_ref>\", \"<tf.Variable 'Variable_7:0' shape=(1024, 10) dtype=float32_ref>\"] and loss Tensor(\"loss_4/Mean:0\", shape=(), dtype=float32).\r\n\r\n```\r\n\r\nhow should I fix?", "please help me to resolve this. i'm using python 3.6\r\n\r\nValueError                                Traceback (most recent call last)\r\n~\\NNV2\\train1.py in <module>()\r\n    106     case = args.case\r\n    107     logdir = '{}/{}/train1'.format(logdir_path, case)\r\n--> 108     train(logdir=logdir)\r\n    109     print(\"Done\")\r\n\r\n~\\NNV2\\train1.py in train(logdir, queue)\r\n     31     with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\r\n     32         var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'net/net1')\r\n---> 33         train_op = optimizer.minimize(loss_op, global_step=global_step, var_list=var_list)\r\n     34 \r\n     35     # Summary\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\r\n    405           \"No gradients provided for any variable, check your graph for ops\"\r\n    406           \" that do not support gradients, between variables %s and loss %s.\" %\r\n--> 407           ([str(v) for _, v in grads_and_vars], loss))\r\n    408 \r\n    409     return self.apply_gradients(grads_and_vars, global_step=global_step,\r\n\r\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables", "@rosecatherinek how do you solve the problem?", "@yangl326-Dylan \r\nLooks like I posted about 2 years back - so, I don't remember exactly. \r\nWhat I think I did at that time was to convert it to a regression problem instead of a classification - that was possible in my case. I don't think it would be an option for everyone. \r\nI couldn't get the cast and arg_max gradients to work - but I didn't dig deeper since I was chasing a deadline and was short on time. Thanks! ", "@rosecatherinek  thanks for reply. i also change into a regression problem by setting the logits' last dim to 1.  \r\nbut the result is not so good  compared with my base line, which is a classification model.", "I had the same Error \r\nBUT!! it got resolved!! when I checked my variable y(Desired value) and y_(expected value) which I didn't type properly..so i guess there was no gradient for program to check for precision..hence that error.. but now it got resolved.. btw I was referring some scratch code so please cross verify your code if you are also doing the same..", "> This can be hard to debug, and the reasons might be quite subtle,\r\n> \r\n> for example if for some reason you switched arguments in softmax_cross_entropy_with_logits:\r\n> \r\n> `tf.nn.softmax_cross_entropy_with_logits(y,_y)` to\r\n> `tf.nn.softmax_cross_entropy_with_logits(_y,y)`\r\n> \r\n> you get the dreaded _No gradients provided for any variable_\r\n\r\nThanks, that indeed where my problem is.", "Hi!\r\nI have met the same situation: \r\n`def generator(y, reuse=False,training=True):\r\n    #yb = tf.reshape(y, [BATCH_SIZE, 390, 390, 2], name=\"yb\")  # y:?*1*1*10\r\n    #z = np.array([z,yb])\r\n    #z = tf.convert_to_tensor(z)\r\n    # z:?*110\r\n    with tf.variable_scope(tf.get_variable_scope(),reuse=reuse):\r\n        if reuse:\r\n            tf.get_variable_scope().reuse_variables()\r\n    # \u7ecf\u8fc7\u4e00\u4e2a\u5168\u8fde\u63a5\u3001 batch_norm\u3001\u548crelu\r\n    h1 = first_fully_connect(y,300, name='g_h1_fully_connect')\r\n    h1 = tf.nn.sigmoid(h1)\r\n    #h1 = tf.concat([h1, y], 1)  # h1: ?*1034\r\n\r\n    h2 = fully_connect(h1, 200, name='g_h2_fully_connect')\r\n    h2 = tf.nn.sigmoid(h2)\r\n    #h2 = tf.reshape(h2, [BATCH_SIZE, 7, 7, 128])  # h2: ?*7*7*128\r\n    #h2 = conv_cond_concat(h2, y)  # h2: ?*7*7*138\r\n\r\n    #h3 = deconv2d(h2, output_shape=[BATCH_SIZE, 14, 14, 128], name='g_h3_deconv2d')\r\n    h3 = fully_connect(h2, 100,name='g_h3_fully_connect')\r\n    h3 = tf.nn.sigmoid(h3)  # h3: ?*14*14*128\r\n    #h3 = conv_cond_concat(h3, y)  # h3:?*14*14*138\r\n\r\n    #h4 = deconv2d(h3, output_shape=[BATCH_SIZE, 400, 400, 1], name='g_h4_deconv2d')\r\n    h4 = fully_connect(h3,400,name='g_h4_fully_connect')\r\n    #h5 = tf.transpose(h4)\r\n    #h5 = tf.nn.sigmoid(h4)  # h4: ?*28*28*1\r\n    h6 = tf.reshape(h4,[-1,400,400])\r\n\r\n    return h6`\r\n\r\n`def fully_connect(x,outshape, name):\r\n    x_T = tf.transpose(x)\r\n    shape = x_T.get_shape().as_list()\r\n    w_w = shape[1]\r\n    with tf.variable_scope(name):\r\n        weights = weight_variable([w_w, outshape], name='weights')\r\n        #weights_exp = tf.expand_dims(weights, axis=-1)\r\n        biases = bias_variable([1, outshape], name='biases')\r\n        #biases_exp = tf.expand_dims(biases, axis=-1\r\n        h = tf.matmul(x_T, weights) + biases\r\n        h_T = tf.transpose(h)\r\n        return h_T\r\n`\r\n`def first_fully_connect(x,outshape, name):\r\n    x_T = tf.transpose(x)\r\n    x_reshape = tf.reshape(x_T,[-1,400])\r\n    shape = x_reshape.get_shape().as_list()\r\n    w_w = shape[1]\r\n    with tf.variable_scope(name):\r\n        weights = weight_variable([w_w, outshape], name='weights')\r\n        #weights_exp = tf.expand_dims(weights, axis=-1)\r\n        biases = bias_variable([1, outshape], name='biases')\r\n        #biases_exp = tf.expand_dims(biases, axis=-1)\r\n        h = tf.matmul(x_reshape, weights) + biases\r\n        h_T = tf.transpose(h)\r\n        return h_T`\r\n", "> \r\n> \r\n> Thank you for your quick response. Will post on Stack Overflow.\r\n\r\nDid you get a good answer on Stackoverflow, and can you give me the link to the post?", "> Hi!\r\n> I have met the same situation:\r\n> `def generator(y, reuse=False,training=True):\r\n> #yb = tf.reshape(y, [BATCH_SIZE, 390, 390, 2], name=\"yb\") # y:?_1_1*10\r\n> #z = np.array([z,yb])\r\n> #z = tf.convert_to_tensor(z)\r\n> # z:?*110\r\n> with tf.variable_scope(tf.get_variable_scope(),reuse=reuse):\r\n> if reuse:\r\n> tf.get_variable_scope().reuse_variables()\r\n> # \u7ecf\u8fc7\u4e00\u4e2a\u5168\u8fde\u63a5\u3001 batch_norm\u3001\u548crelu\r\n> h1 = first_fully_connect(y,300, name='g_h1_fully_connect')\r\n> h1 = tf.nn.sigmoid(h1)\r\n> #h1 = tf.concat([h1, y], 1) # h1: ?*1034\r\n> \r\n> ```\r\n> h2 = fully_connect(h1, 200, name='g_h2_fully_connect')\r\n> h2 = tf.nn.sigmoid(h2)\r\n> #h2 = tf.reshape(h2, [BATCH_SIZE, 7, 7, 128])  # h2: ?*7*7*128\r\n> #h2 = conv_cond_concat(h2, y)  # h2: ?*7*7*138\r\n> \r\n> #h3 = deconv2d(h2, output_shape=[BATCH_SIZE, 14, 14, 128], name='g_h3_deconv2d')\r\n> h3 = fully_connect(h2, 100,name='g_h3_fully_connect')\r\n> h3 = tf.nn.sigmoid(h3)  # h3: ?*14*14*128\r\n> #h3 = conv_cond_concat(h3, y)  # h3:?*14*14*138\r\n> \r\n> #h4 = deconv2d(h3, output_shape=[BATCH_SIZE, 400, 400, 1], name='g_h4_deconv2d')\r\n> h4 = fully_connect(h3,400,name='g_h4_fully_connect')\r\n> #h5 = tf.transpose(h4)\r\n> #h5 = tf.nn.sigmoid(h4)  # h4: ?*28*28*1\r\n> h6 = tf.reshape(h4,[-1,400,400])\r\n> \r\n> return h6`\r\n> ```\r\n> \r\n> `def fully_connect(x,outshape, name): x_T = tf.transpose(x) shape = x_T.get_shape().as_list() w_w = shape[1] with tf.variable_scope(name): weights = weight_variable([w_w, outshape], name='weights') #weights_exp = tf.expand_dims(weights, axis=-1) biases = bias_variable([1, outshape], name='biases') #biases_exp = tf.expand_dims(biases, axis=-1 h = tf.matmul(x_T, weights) + biases h_T = tf.transpose(h) return h_T `\r\n> `def first_fully_connect(x,outshape, name): x_T = tf.transpose(x) x_reshape = tf.reshape(x_T,[-1,400]) shape = x_reshape.get_shape().as_list() w_w = shape[1] with tf.variable_scope(name): weights = weight_variable([w_w, outshape], name='weights') #weights_exp = tf.expand_dims(weights, axis=-1) biases = bias_variable([1, outshape], name='biases') #biases_exp = tf.expand_dims(biases, axis=-1) h = tf.matmul(x_reshape, weights) + biases h_T = tf.transpose(h) return h_T`\r\n\r\nhi,have you solved this problem now?I met the same problem and confused.", "I also met this problem with the code above\u2026\u2026 could anyone help with it\uff1f`ValueError: in converted code:\r\n\r\n    <ipython-input-1-81054f0385cb>:856 train_step  *\r\n        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:427 apply_gradients\r\n        grads_and_vars = _filter_grads(grads_and_vars)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:1025 _filter_grads\r\n        ([v.name for _, v in grads_and_vars],))\r\n\r\n    ValueError: No gradients provided for any variable: ['transformer_1/encoder_1/embedding_2/embeddings:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_98/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_98/bias:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_99/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_99/bias:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_100/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_100/bias:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_101/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_101/bias:0', 'transformer_1/encoder_1/encoder_layer_6/sequential_12/dense_102/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/sequential_12/dense_102/bias:0', 'transformer_1/encoder_1/encoder_layer_6/sequential_12/dense_103/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/sequential_12/dense_103/bias:0', 'transformer_1/encoder_1/encoder_layer_6/layer_normalization_30/gamma:0', 'transformer_1/encoder_1/encoder_layer_6/layer_normalization_30/beta:0', 'transformer_1/encoder_1/encoder_layer_6/layer_normalization_31/gamma:0', 'transformer_1/encoder_1/encoder_layer_6/layer_normalization_31/beta:0', 'transformer_1/encoder_1/encoder_layer_7/multi_head_attention_19/dense_104/kernel:0', 'transformer_1/encoder_1/encoder...`\r\n[\u65b0\u5efa\u6587\u672c\u6587\u6863.txt](https://github.com/tensorflow/tensorflow/files/3867497/default.txt)\r\n\r\n", "> This typically means that there is no path between any of the `tf.Variable` objects that you've defined and the loss tensor that you are trying to minimize. As a sanity check, does `tf.trainable_variables()` contain any variables?\r\n> \r\n> To figure this out, we'll need to see the code that is failing. Can you reply with more details?\r\n\r\ni had same kind of error and i also checked 'tf.trainable_variables()' so there are some trainable params but few on them are untrainable as i set them. My idea is to do partial gradient flow. Is there any way to fix this?\r\n\r\n\r\nThanks a bunch. :)", "Any update on how to fix it if np.float32() is part of the problem?", "Hi! For people who stumble here with the same problem, apart from all the reasons explained by the others, one of the reasons might be that you are using NumPy math instead of TF math. This StackOverflow link explains it better: https://stackoverflow.com/questions/59919691/why-is-gradienttape-returning-none-when-i-use-numpy-math. At least inside the gradient tape with loop try to avoid NumPy math. Hope this helps! Thanks!", "I think I have a similar issue. Anyone has an idea ? https://github.com/tensorflow/tensorflow/issues/41528\r\nThanks !\r\n", "I have been trying for days now for the same kind of value error: I hope if you one could explain what the issue is,\r\n\r\n\r\n''' \r\nimport os\r\nfrom collections import defaultdict\r\nimport numpy as np\r\nimport PIL\r\nimport glob\r\nfrom matplotlib import pyplot as plt\r\n%matplotlib inline\r\nfrom keras import Sequential, Model\r\nfrom keras.layers import Embedding, LSTM, Dense, Input, Bidirectional, RepeatVector, Concatenate, Activation\r\nfrom keras.activations import softmax\r\nfrom keras.utils import to_categorical\r\nfrom keras.preprocessing.sequence import pad_sequences\r\nfrom keras.applications.inception_v3 import InceptionV3\r\nfrom keras.optimizers import Adam\r\nfrom google.colab import drive\r\n\r\n\r\n\r\ndef load_image_list(filename):\r\n    with open(filename,'r') as image_list_f: \r\n        return [line.strip() for line in image_list_f]  \r\n\r\ntrain_list = load_image_list('/content/gdrive/My Drive/hw2_data/Flickr_8k.trainImages.txt')\r\ndev_list = load_image_list('/content/gdrive/My Drive/hw2_data/Flickr_8k.devImages.txt')\r\ntest_list = load_image_list('/content/gdrive/My Drive/hw2_data/Flickr_8k.testImages.txt')\r\n\r\ndef get_image(image_name):\r\n    image = PIL.Image.open(os.path.join(IMG_PATH, image_name))\r\n    return np.asarray(image.resize((299,299))) / 255.0                     \r\n\r\nimg_model = InceptionV3(weights='imagenet')\r\n\r\nnew_input = img_model.input\r\nnew_output = img_model.layers[-2].output        #featuers in that layer from second to the last layer\r\nimg_encoder = Model(new_input, new_output)      # This is the final Keras image encoder model we will use.\r\n\r\ndef img_generator(img_list):                #load your data for min batch size. image to incept3 (nirator for the text)\r\n    #...                                     iterat throug the data set and insert them into incpect3 \r\n  for im in img_list:\r\n    if (im[len(img_list):] in IMG_PATH):\r\n      img = get_image(im)\r\n      img_id = np.expand_dims(img, axis=0) #Add one more dimension: (1, 299, 299, 3) # Inception-V3 requires 4 dimensions\r\n      yield img_id  # shape: (1, 299, 299, 3)\r\n\r\n\r\nenc_train = img_encoder.predict_generator(img_generator(train_list), steps=len(train_list), verbose=1)\r\n\r\ntoken_list = load_image_list('/content/gdrive/My Drive/hw2_data/Flickr8k.token.txt')\r\n\r\n# get all imgs with their captions\r\ndef read_image_descriptions(filename):\r\n    captions = load_image_list(filename)\r\n    #captions = file.split('\\n')\r\n    descriptions ={}\r\n    for caption in captions[:-1]:\r\n        img, caption = caption.split('\\t')\r\n        caption = caption.lower().split()\r\n        desc = '<START> ' + ' '.join(caption) + ' <END>'\r\n        desc = desc.lower().split()\r\n        if img[:-2] not in descriptions:\r\n            descriptions[img[:-2]] = [ desc ]\r\n        else: \r\n            descriptions[img[:-2]].append(desc)\r\n    return descriptions \r\n\r\ndescriptions = read_image_descriptions(\"gdrive/My Drive/hw2_data/Flickr8k.token.txt\")\r\n\r\ntrain_desc= {}              #only train images descriptions\r\nfor im in train_list:\r\n  for key, values in descriptions.items():\r\n    if (im == key):\r\n      train_desc[key] = values\r\ndef create_vocabulary(preprocessed_caption):\r\n    vocabulary = set()\r\n    for img_captions in preprocessed_caption.values(): \r\n        for caption in img_captions:\r\n            for token in caption:\r\n                vocabulary.add(token)  \r\n    vocabulary = sorted(vocabulary)               \r\n    return vocabulary\r\n\r\n\r\nid_to_word = { i+1:word for i, word in enumerate(voca)}  #...\r\n\r\nword_to_id = {word: i+1 for i, word in enumerate(voca)}  #...\r\n\r\n\r\nmax(len(description) for image_id in train_list for description in train_desc[image_id])\r\n\r\nMAX_LEN = 40            # set the model \r\nEMBEDDING_DIM=300\r\nvocab_size = len(word_to_id)     #8920.  #7707 only training\r\n\r\n# Text input\r\ntext_input = Input(shape=(MAX_LEN,))\r\nembedding = Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_LEN)(text_input)\r\nx = Bidirectional(LSTM(512, return_sequences=False))(embedding)                    # 512 image //how many layer and instance will be using\r\npred = Dense(vocab_size, activation='softmax')(x)\r\nmodel = Model(inputs=[text_input],outputs=pred)\r\nmodel.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\r\n\r\nmodel.summary()\r\n\r\n\r\n# data generator, intended to be used in a call to model.fit_generator()\r\ndef text_training_generator(batch_size = 128):  \r\n    X, y = list(), list()\r\n    n=0\r\n    # loop for ever over images\r\n    while True:\r\n        for key, desc_list in train_desc.items():\r\n            #n+=1\r\n            # retrieve the photo feature\r\n            for desc in desc_list:\r\n                # encode the sequence\r\n                seq = [word_to_id[word] for word in desc if word in word_to_id]\r\n                #seq = tokenizer.texts_to_sequences([desc])[0]\r\n                # split one sequence into multiple X, y pairs\r\n                for i in range(1, len(seq),batch_size):\r\n                    # split into input and output pair\r\n                    in_seq, out_seq = seq[:i], seq[i]\r\n                    # pad input sequence\r\n                    in_seq = pad_sequences([in_seq], maxlen=MAX_LEN)[0]\r\n                    # encode output sequence\r\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\r\n                    # store\r\n                    X.append(in_seq)\r\n                    y.append(out_seq)\r\n            # yield the batch data\r\n                n+=1\r\n                if n==batch_size:\r\n                    #n = 0\r\n                  yield [np.array(X), np.array(y)]\r\n                  X, y = list(), list()\r\n                  n=0\r\n\r\nb,c = next(text_training_generator(batch_size=128))\r\nb.shape, c.shape                \r\n          \r\n\r\nbatch_size = 128\r\ngenerator = text_training_generator(batch_size)\r\nsteps = len(train_list) * MAX_LEN // batch_size \r\n\r\nmodel.fit_generator(generator, steps_per_epoch=steps, verbose=True, epochs=10)\r\n", "@xuanchien your comment allowed me to solve the same issue. In simple words, the argument logits of the method I used (tf.nn.sigmoid_cross_entropy_with_logits) has to be a tensor, not an ndarray that's returned with the .run() method. So thankyou! ", "In my case, I had just forgotten  to set the `loss` attribute of the model compiler"]}]