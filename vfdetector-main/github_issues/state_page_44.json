[{"number": 45109, "title": "Intermediate-level guide in Time Series Forecasting example", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/time_series.ipynb\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nI believe this guide is overly complex for most basic time-series forecasting measures. Due to the introduction of a class for windowing, while this makes it easy to re-use code between examples, it doesn't do a very good job of creating basic windows that users might benefit from for a basic use case.\r\n\r\nFor example, I only want some basic windowing, I found more help setting this up in the keras preprocessing docs here https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array than I did in this help page.\r\n\r\nWhile this page demonstrates many potential use cases quite simply, I believe another page is required which outlines the very basic steps just to get one of these working with an intro to windowing as well.\r\n", "comments": ["Perhaps this [lecture](https://www.coursera.org/lecture/tensorflow-sequences-time-series-and-prediction/preparing-features-and-labels-TYErD?utm_source=link&utm_medium=page_share&utm_content=vlp&utm_campaign=top_button) can help to get more clarity on setting up window sizes for time series data.", "Thanks, looks like it will be useful, I will review it in detail. However I feel the point still stands, it would be useful to have a simple-case code example in the docs. From my position, of having some experience with ML / neural nets but being new to tensorflow, it's more about understanding and adapting to the syntax and terminology rather than trying to learn the fundamentals, which I think this would help with.\r\n\r\nYour call though :)"]}, {"number": 45100, "title": "C and Java perfomance differences under Android", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.7\r\n- Mobile device: Android with MT8512 processor\r\n- TensorFlow installed from (source or binary): Source (C code) and binary (Java)\r\n- TensorFlow version (use command below): v1.12.1-45356-g73272ab087 2.5.0-dev20201107\r\n- Python version: 3.7\r\n- Bazel version: 3.1\r\n- Android NDK version: r21d with API level 22\r\n- Target architecture: armv7\r\n\r\n**Describe the current behavior**\r\n\r\nI have 2 different models running under TFLite and implementations in Java and C for benchmarking. First model runs about 10% slower using TFLite in C code than Java equivalent. Second model runs about 4-5% faster in C than in Java. Copying to and from input/output buffers is neglible in terms of execution time. Going multithreaded causes both models to run slower in C with bigger gaps.\r\n\r\n**Describe the expected behavior**\r\n\r\nInference in plain C should've been at least as fast as in Java in every case.\r\n\r\n\r\nTFLite in C is built with bazel command:\r\n`bazel build -c opt --config=android_arm --verbose_failures //tensorflow/lite/c:tensorflowlite_c`\r\nBinaries needed for Java are pulled from Tensorflow's provided repositories.\r\n\r\nAre there any compile options different for C and Java binaries that may cause performance differences? What else can cause it?\r\n\r\nThanks in advance\r\n", "comments": ["> **System information**\r\n> \r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.7\r\n> * Mobile device: Android with MT8512 processor\r\n\r\nThis is a dual-core processor with Amazon's AI Neural processor according to https://www.mediatek.com/blog/amazon-az1-neural-edge-processor-powered-by-mediatek. From this perspective, multithreading (i.e. running with >= 2 CPU threads) might create resource contention on CPU. Speaking of performance, have you tried TFLite NNAPI delegate that might be able to utilize the neural processor?\r\n\r\n> * TensorFlow installed from (source or binary): Source (C code) and binary (Java)\r\n> * TensorFlow version (use command below): v1.12.1-45356-g73272ab087 2.5.0-dev20201107\r\n\r\nI'm a bit of confused here. What's the exact version of this TF? v1.12.1 or the one pulled from GitHub on 2020/11/07?\r\n\r\n> * Python version: 3.7\r\n> * Bazel version: 3.1\r\n> * Android NDK version: r21d with API level 22\r\n> * Target architecture: armv7\r\n> \r\n> **Describe the current behavior**\r\n> \r\n> I have 2 different models running under TFLite and implementations in Java and C for benchmarking. First model runs about 10% slower using TFLite in C code than Java equivalent. Second model runs about 4-5% faster in C than in Java. Copying to and from input/output buffers is neglible in terms of execution time. Going multithreaded causes both models to run slower in C with bigger gaps.\r\n\r\nAre those performance differences repeatedly reproducible? What're the absolute latency difference? Just wondering whether it might be caused by OS jitters, automatic CPU throttling, OS scheduling etc.\r\n\r\nCould you try TFLite's benchmark model tool on your two models and report the performance here? Thx! There're two options here: \r\nFirst, we offer pre-built binaries as shown on https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary.\r\n\r\nSecondly, if you prefer to build your own benchmark model tool, you could refer to https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark to build the tool in the type of a C++ native binary, and https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark/android to build the tool in the type of a Java APK. The latter one offers perhaps a more faithful view of runtime performance.\r\n\r\nIn particular, when benchmarking NNAPI delegate performance, you could refer to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/README.md#nnapi-delegate (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/delegates/README.md#nnapi-delegate-provider gives more details) for various parameters we support.\r\n\r\n> \r\n> **Describe the expected behavior**\r\n> \r\n> Inference in plain C should've been at least as fast as in Java in every case.\r\n> \r\n> TFLite in C is built with bazel command:\r\n> `bazel build -c opt --config=android_arm --verbose_failures //tensorflow/lite/c:tensorflowlite_c`\r\n> Binaries needed for Java are pulled from Tensorflow's provided repositories.\r\n> \r\n> Are there any compile options different for C and Java binaries that may cause performance differences? What else can cause it?\r\n> \r\n> Thanks in advance\r\n\r\n", "> From this perspective, multithreading (i.e. running with >= 2 CPU threads) might create resource contention on CPU. \r\n\r\nI have also tried it on different CPU with 4 cores, and the results diverge a lot more when using 2 out of 4 cores in C vs Java.\r\n\r\n> Speaking of performance, have you tried TFLite NNAPI delegate that might be able to utilize the neural processor?\r\n\r\nCurrently I cannot do that.\r\n\r\n> I'm a bit of confused here. What's the exact version of this TF? v1.12.1 or the one pulled from GitHub on 2020/11/07?\r\n\r\n2.5.0-dev20201107, the command output turned to be weird\r\n\r\n> Are those performance differences repeatedly reproducible? What're the absolute latency difference? Just wondering whether it might be caused by OS jitters, automatic CPU throttling, OS scheduling etc.\r\n\r\nYeah, these are reproducible and similar every time. For one model (10% slower in C) the absolute difference would be about 0.5 seconds, 5.4 secs (C) vs 5.9 (Java).\r\n> Could you try TFLite's benchmark model tool on your two models and report the performance here?\r\n\r\nI am working on that, does the APK benchmark accept exactly same parameters as the C++ one? It doesn't seem to respect `input_layer_value_range` `input_layer` and `input_layer_shape` parameters, resulting in errors - model needs some constraints on the input.\r\n\r\n", "> > From this perspective, multithreading (i.e. running with >= 2 CPU threads) might create resource contention on CPU.\r\n> \r\n> I have also tried it on different CPU with 4 cores, and the results diverge a lot more when using 2 out of 4 cores in C vs Java.\r\n> \r\n> > Speaking of performance, have you tried TFLite NNAPI delegate that might be able to utilize the neural processor?\r\n> \r\n> Currently I cannot do that.\r\n> \r\n> > I'm a bit of confused here. What's the exact version of this TF? v1.12.1 or the one pulled from GitHub on 2020/11/07?\r\n> \r\n> 2.5.0-dev20201107, the command output turned to be weird\r\n> \r\n> > Are those performance differences repeatedly reproducible? What're the absolute latency difference? Just wondering whether it might be caused by OS jitters, automatic CPU throttling, OS scheduling etc.\r\n> \r\n> Yeah, these are reproducible and similar every time. For one model (10% slower in C) the absolute difference would be about 0.5 seconds, 5.4 secs (C) vs 5.9 (Java).\r\n\r\nTypo? Is C a bit faster here?\r\n> \r\n> > Could you try TFLite's benchmark model tool on your two models and report the performance here?\r\n> \r\n> I am working on that, does the APK benchmark accept exactly same parameters as the C++ one? It doesn't seem to respect `input_layer_value_range` `input_layer` and `input_layer_shape` parameters, resulting in errors - model needs some constraints on the input.\r\n\r\nYes, the APK one should accept the exact same set of parameters that the C++ one accepts. @hajuho Could you help to take a look at this?", "> Typo? Is C a bit faster here?\r\n\r\nYeah, sorry that was a typo. Java was meant to be faster."]}, {"number": 45093, "title": "AttributeError: 'TensorArray' object has no attribute 'mark_used' with tf.function", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- TensorFlow version (use command below):  v2.3.0-54-gfcc4b966f1 2.3.1\r\n\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n\r\n**Describe the current behavior**\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\ntfd = tfp.distributions \r\n\r\nclass Test(tf.Module):\r\n  def __init__(self):\r\n    self.log_likes_list = None\r\n    self.i = tf.constant(0)\r\n\r\n  @tf.function\r\n  def __call__(self, samples):\r\n\r\n    @tf.function\r\n    def rnd():\r\n        return tfd.Normal(0,1).sample()+ tfd.Normal(3,1).sample()\r\n      \r\n    if self.log_likes_list is None:\r\n        self.log_likes_list = tf.TensorArray(tf.float32, size=samples) \r\n\r\n    def cond(x,i):\r\n        return tf.less(i, samples) \r\n\r\n    def body(x,i):\r\n        #x=x.write(i,tfm.reduce_sum(tfd.Normal(rnd(), 1).log_prob(0.4)))\r\n        # AttributeError: 'TensorArray' object has no attribute 'mark_used'\r\n        x.write(i,tfm.reduce_sum(tfd.Normal(rnd(), 1).log_prob(0.4))).mark_used()\r\n        return x, i+1 \r\n\r\n    self.log_likes_list, i = tf.while_loop(cond, body, [self.log_likes_list, self.i])\r\n\r\n    self.log_likes = self.log_likes_list.stack()\r\n\r\n    self.log_like = tfm.reduce_mean(self.log_likes)\r\n\r\n    loss = self.log_like\r\n\r\n    return loss\r\n\r\n\r\nT= Test()\r\nt= T(5)\r\n\r\nT.log_likes, T.log_like, \r\n# the code in below is not running \r\nT.log_likes_list.stack()\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nHow can get values in t.log_likes, and t.log_like, and t.log_likes_list?\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://colab.research.google.com/drive/1ddlIXUBOHd5umTaCW-xghWk3bTOIKhVf#scrollTo=-yj19j0eC_cj\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@stokhos \r\nI ran the code shared and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/08c39b7061660280481784ce0790615a/untitled468.ipynb).\r\nALso grant access tot the drive shared.", "@Saduf2019 \r\nYou can check out my code in this link.\r\nhttps://colab.research.google.com/drive/1ddlIXUBOHd5umTaCW-xghWk3bTOIKhVf?usp=sharing\r\n\r\nIn you link, you need add following lines:\r\n```python\r\nimport tensorflow_probability as tfp\r\ntfd = tfp.distributions\r\ntfm = tf.math\r\n```", "I am able to replicate the issue please find the [gist here](https://colab.research.google.com/gist/Saduf2019/56d83db34bc7d3dda33dc00a31ee21b8/untitled474.ipynb).", "@cheshire FYI\r\n\r\nSorry for the delay. I think the warning about `mark_used` is quite misleading, we'll need to fix it. You almost never want to use it. The warning itself was meant to caution that you should always assign the result of `x.write` back to `x`. It's similar to Pandas, if you're familiar with it.\r\n\r\nThere is another limitation of tf.function here: when you want to pass values outside the `tf.function`, you must either return them, or use `tf.Variable`. Assigning to `self.property` won't work properly. This is something we intend to fix, and should have a warning message, but in the mean time, a workaround is to use Variables.\r\n\r\nI rewrote your code to get it to run without error, see below.\r\nA few more notes:\r\n * Since `tf.Variable` cannot change shape, I used padding for `self.log_likes`\r\n * I also took the liberty to write the `tf.while_loop` in a shorter form since the function is autographed\r\n * I recommend calling the function with a Tensor (that is, `tf.constant(5)` instead of `5`) to avoid unwanted retracing\r\n * I turned `jit_compile` to False since it seems to reveal a bug; I think that will be fixed soon, and I do recommend turning it on when possible because it helps performance\r\n * `rnd` doesn't need a separate tf.function - as long as it's called inside one it will be compiled\r\n\r\n```\r\nclass Test(tf.Module):\r\n  def __init__(self, max_samples=100):\r\n    # Variables to store things across tf.function boundaries (that are not returns/args)\r\n    self.i = tf.Variable(0)\r\n    self.max_samples = max_samples\r\n    self.log_likes = tf.Variable(tf.zeros([max_samples]))\r\n    self.log_like = tf.Variable(0.0)\r\n\r\n  @tf.function  # Should be able to use jit_compile=True soon, once a shape bug is fixed\r\n  def __call__(self, samples):\r\n\r\n    def rnd():\r\n        return tfd.Normal(0,1).sample()+ tfd.Normal(3,1).sample()\r\n\r\n    # Read things from outside tf.function\r\n    x = tf.TensorArray(tf.float32, size=samples)\r\n    i = self.i.read_value()\r\n\r\n    x = x.unstack(self.log_likes.read_value())\r\n    while i < samples:\r\n      # Always assign back to `x` after `x.write`\r\n      x = x.write(i, tfm.reduce_sum(tfd.Normal(rnd(), 1).log_prob(0.4)))\r\n      i += 1\r\n\r\n    log_likes = x.stack()\r\n    log_like = tfm.reduce_mean(log_likes)\r\n    loss = tfm.reduce_mean(log_like)\r\n\r\n    # Put things back\r\n    self.i.assign(i)\r\n    self.log_likes.assign(tf.pad(log_likes, [[0, self.max_samples - samples]]))\r\n    self.log_like.assign(log_like)\r\n\r\n    return loss\r\n\r\nT= Test()\r\n# Use a Tensor for samples, otherwise you might get many retraces.\r\nt= T(tf.constant(5))\r\n```\r\n", "Was able to reproduce the issue  using TF Nightly(2.6).Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/20de0ac979407371e4365456fe8ff1a1/untitled474.ipynb).Thanks!", "> I turned jit_compile to False since it seems to reveal a bug; I think that will be fixed soon, and I do recommend turning it on when possible because it helps performance\r\n\r\n@mdanatg could you clarify which bug, what error message, and how to repro?", "@cheshire I can't remember if I filed a bug, and if I did, it was internal. The repro is straightforward however: copy the snippet above, and add `jit_compile=True` in the line with the comment: `@tf.function  # Should be able to use jit_compile=True soon, once a shape bug is fixed`. At a quick test, it appears the bug is still there.", "@mdanatg There's no XLA bug, it correctly prints that `self.i` is on CPU, and it can't use it from GPU. The problem there is that using an int32 tensor force-places it on CPU, so I had to use int64 as a workaround and to have it correctly running on GPU.\r\n\r\nFurthermore, the conversion pass seems to have issues with `samples` being different from `max_samples`, but I haven't looked too closely, as the following works:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\n\r\ntfd = tfp.distributions\r\ntfm = tf.math\r\n\r\nclass Test(tf.Module):\r\n  def __init__(self, max_samples=100):\r\n    # Variables to store things across tf.function boundaries (that are not returns/args)\r\n    self.i = tf.Variable(0, dtype=tf.int64)\r\n    self.max_samples = max_samples\r\n    self.log_likes = tf.Variable(tf.zeros([max_samples]))\r\n    self.log_like = tf.Variable(0.0)\r\n\r\n  @tf.function(jit_compile=True) \r\n  def __call__(self, samples):\r\n\r\n    def rnd():\r\n        return tfd.Normal(0,1).sample()+ tfd.Normal(3,1).sample()\r\n\r\n    # Read things from outside tf.function\r\n    x = tf.TensorArray(tf.float32, size=tf.cast(samples, tf.int32))\r\n    i = self.i.read_value()\r\n\r\n    x = x.unstack(self.log_likes.read_value())\r\n    while i < samples:\r\n      # Always assign back to `x` after `x.write`\r\n      x = x.write(tf.cast(i, tf.int32),\r\n                  tfm.reduce_sum(tfd.Normal(rnd(), 1).log_prob(0.4)))\r\n      i += 1\r\n\r\n    log_likes = x.stack()\r\n    log_like = tfm.reduce_mean(log_likes)\r\n    loss = tfm.reduce_mean(log_like)\r\n\r\n    # Put things back\r\n    self.i.assign(i)\r\n    padded = tf.pad(log_likes, [[0, self.max_samples - samples]])\r\n    self.log_likes.assign(padded)\r\n    self.log_like.assign(log_like)\r\n\r\n    return loss\r\n\r\n\r\nT= Test()\r\nt = T(tf.constant(100, dtype=tf.int64))\r\n```", "@cheshire are you sure we're seeing the same error? This is what I'm getting (tried on tf-nightly too, same error):\r\n\r\n```\r\nUnimplementedError: Shape of resource _AnonymousVar7 cannot be changed after initialization: old shape was [100], new shape is [195]\r\n\t [[{{node AssignVariableOp_1}}]] [Op:__inference___call___292]\r\n```", "@mdanatg I'm looking into that one. The int32 error appears when running on a GPU.\r\n\r\nOn CPU, the error you mention is gone if you run with samples=max_samples.\r\n\r\nShape-wise, I'm trying to see what is exactly going on. If we run in pure eager, with no `tf.function` annotation, how is `x = x.unstack(self.log_likes.read_value())` supposed to work?\r\n\r\n`x` is of size `samples`, and `self.log_likes` is of size `self.max_samples`, so I don't see how that could work? Eager execution complaints about the same thing. I would guess that is what confusing the compilation.", "Yes, that sounds like the reason. I wouldn't test this one in eager mode - TensorArray has different implementations in eager and graph and they seem to have diverged (cleaning them up is a longstanding todo...).\r\n\r\nThe `size` passed to TensorArray is just its initial size - unstack is supposed to resize it if necessary, but the documentation doesn't seem to clarify that. Would that explain the error?", "> The size passed to TensorArray is just its initial size - unstack is supposed to resize it if necessary\r\n\r\nI don't think that's the case - at least that's not what eager runtime is doing.", "I would definitely use the graph implementation for reference - I don't think the eager implementation is even used much."]}, {"number": 45091, "title": "Adding a Lite flag to tf.keras.applications.EfficientNetBX", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.0-rc2\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, EfficientNetB0 to B7 are available via keras.applications.\r\nIn May, tensorflow released a [blog entry](https://blog.tensorflow.org/2020/03/higher-accuracy-on-vision-models-with-efficientnet-lite.html) that 'Lite' variants of EfficientNet are available on [tensorflow/tpu](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite). \r\nThe models can in addition be used from tensorflow [Hub](https://tfhub.dev/tensorflow/efficientnet/lite0/classification/2) and tfLite model maker. Yet, these models are created as TF1 graphs and cannot be readily used for fine-tuning etc. in tensorflow 2.X.\r\n\r\nIs it possible to introduce a flag in tf.keras.applications.EfficientNet that will load the 'lite' variant, similar to the 'minimalistic' flag of MobileNetV3?\r\n\r\n**Will this change the current api? How?**\r\nYes, tensorflow/python/keras/applications/efficientnet.py would get an additional flag that will load the corresponding weight file, and will remove squeeze&excite modules and substitute swish by relu6 \r\n\r\n**Who will benefit with this feature?**\r\nEveryone who wants to use EfficientNet Lite in tensorflow2, for fine-tuning or layer-wise custom actions.\r\n\r\n**Any Other info.**\r\n", "comments": ["Is this feature doable? if so, i would like to tried to make it.", "Over to @lintian06 for further comment.", "@lintian06 \r\nI managed to modify the existing code to add option for EfficientNet Lite variant architecture and to convert the `ckpt` weights to `.h5` via utilility script provided in [Tensorflow repo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/applications/efficientnet_weight_update_util.py).\r\n\r\nAre you open for PRs regarding this issue?"]}, {"number": 45074, "title": "Deploy micro_speech to ESP32 , is running ,but do not get any output,how to modify?where is wrong", "body": "I deplay micro_speech to esp32,when I say \"yes\" or \"no\" ,there is any output,\r\nhow to where is wrong?\r\n\r\nbelow is informtion..\r\nI (32) boot: ESP-IDF v4.1-dirty 2nd stage bootloader\r\nI (32) boot: compile time 17:06:37\r\nI (32) boot: chip revision: 3\r\nI (35) boot_comm: chip revision: 3, min. bootloader chip revision: 0\r\nI (42) qio_mode: Enabling default flash chip QIO\r\nI (48) boot.esp32: SPI Speed      : 80MHz\r\nI (52) boot.esp32: SPI Mode       : QIO\r\nI (57) boot.esp32: SPI Flash Size : 2MB\r\nI (61) boot: Enabling RNG early entropy source...\r\nI (67) boot: Partition Table:\r\nI (70) boot: ## Label            Usage          Type ST Offset   Length\r\nI (78) boot:  0 nvs              WiFi data        01 02 00009000 00006000\r\nI (85) boot:  1 phy_init         RF data          01 01 0000f000 00001000\r\nI (93) boot:  2 factory          factory app      00 00 00010000 00100000\r\nI (100) boot: End of partition table\r\nI (104) boot_comm: chip revision: 3, min. application chip revision: 0\r\nI (112) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x0d604 ( 54788) map\r\nI (137) esp_image: segment 1: paddr=0x0001d62c vaddr=0x3ffb0000 size=0x01970 (  6512) load\r\nI (139) esp_image: segment 2: paddr=0x0001efa4 vaddr=0x40080000 size=0x00404 (  1028) load\r\n0x40080000: _WindowOverflow4 at C:/Users/Administrator.1-PC/Desktop/esp-idf/components/freertos/xtensa_vectors.S:1778\r\n\r\nI (144) esp_image: segment 3: paddr=0x0001f3b0 vaddr=0x40080404 size=0x00c68 (  3176) load\r\nI (154) esp_image: segment 4: paddr=0x00020020 vaddr=0x400d0020 size=0x23be0 (146400) map\r\n0x400d0020: _stext at ??:?\r\n\r\nI (204) esp_image: segment 5: paddr=0x00043c08 vaddr=0x4008106c size=0x084b8 ( 33976) load\r\n0x4008106c: esp_crosscore_isr at C:/Users/Administrator.1-PC/Desktop/esp-idf/components/esp32/crosscore_int.c:69\r\n\r\nI (222) boot: Loaded app from partition at offset 0x10000\r\nI (222) boot: Disabling RNG early entropy source...\r\nI (222) cpu_start: Pro cpu up.\r\nI (226) cpu_start: Application information:\r\nI (231) cpu_start: Project name:     micro_speech\r\nI (236) cpu_start: App version:      v4.1-dirty\r\nI (241) cpu_start: Compile time:     Nov 22 2020 17:05:27\r\nI (247) cpu_start: ELF file SHA256:  00472dcd8c03272a...\r\nI (253) cpu_start: ESP-IDF:          v4.1-dirty\r\nI (258) cpu_start: Single core mode\r\nI (263) heap_init: Initializing. RAM available for dynamic allocation:\r\nI (270) heap_init: At 3FFAE6E0 len 00001920 (6 KiB): DRAM\r\nI (276) heap_init: At 3FFB5980 len 0002A680 (169 KiB): DRAM\r\nI (282) heap_init: At 3FFE0440 len 0001FBC0 (126 KiB): D/IRAM\r\nI (289) heap_init: At 40078000 len 00008000 (32 KiB): IRAM\r\nI (295) heap_init: At 40089524 len 00016ADC (90 KiB): IRAM\r\nI (301) cpu_start: Pro cpu start user code\r\nI (318) spi_flash: detected chip: generic\r\nI (318) spi_flash: flash io: qio\r\nW (318) spi_flash: Detected size(4096k) larger than the size in the binary image header(2048k). Using the size in the binary image header.\r\nI (330) cpu_start: Starting scheduler on PRO CPU.\r\nI (418) I2S: DMA Malloc info, datalen=blocksize=2400, dma_buf_count=3\r\nI (418) I2S: PLL_D2: Req RATE: 16000, real rate: 16025.000, BITS: 32, CLKM: 39, BCK: 4, MCLK: 4096000.000, SCLK: 1025600.000000, diva: 64, divb: 4\r\nI (468) TF_LITE_AUDIO_PROVIDER: Audio Recording started", "comments": ["#45185 Maybe it's similar?\r\nwhat platform are you working on?"]}, {"number": 45063, "title": "KeyError when taking derivative to input of Conv2D with tf.function only", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, see below\r\n- OS Platform and distribution: Fedora\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.3.1\r\n- Python version: 3.8.6\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GeForce GTX 1080 Ti\r\n\r\n**Describe the current behavior**\r\nCrashes with a stacktrace: `KeyError: 'strides'`.\r\n\r\n<details><summary>code</summary>\r\n<p>\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\nimport tensorflow_datasets as tfds\r\n\r\ndata, data_info = tfds.load(\"mnist\", split=\"train\", as_supervised=True, with_info=True)\r\ndata = data.map(lambda x, _: tf.cast(x, tf.float32) / 255.)\r\ndata_shape = data_info.features[\"image\"].shape\r\ndimension = tf.reduce_prod(data_shape).numpy()\r\n\r\nlatent_distribution = tfp.distributions.MultivariateNormalDiag(\r\n    loc=[0.] * dimension,\r\n    scale_diag=[1.] * dimension,\r\n)\r\n\r\ninput = tf.keras.Input(shape=data_shape)\r\nstate = input\r\nstate = tf.keras.layers.Conv2D(64, kernel_size=4, strides=2, use_bias=False)(state)\r\nstate = tf.keras.layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(state)\r\nstate = tf.keras.layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(state)\r\nstate = tf.keras.layers.Activation(tf.nn.softplus)(state)  # Essential to trigger bug\r\nstate = tf.keras.layers.Conv2D(1, kernel_size=4, strides=1, use_bias=False)(state)\r\nstate = tf.keras.layers.Flatten()(state)\r\nf = tf.keras.Model(inputs=input, outputs=state)\r\n\r\noptimizer = tf.keras.optimizers.Adam()\r\n\r\n@tf.function  # Essential to trigger bug\r\ndef train_step(data):\r\n    with tf.GradientTape() as tape:\r\n        tape.watch(f.trainable_variables)\r\n\r\n        with tf.GradientTape() as c_tape:\r\n            c_tape.watch(data)\r\n\r\n            with tf.GradientTape() as a_tape:\r\n                a_tape.watch(data)\r\n                b = f(data)\r\n            a = a_tape.gradient(b, data)\r\n            a_flat = tf.reshape(a, (-1, dimension))\r\n\r\n        c = c_tape.batch_jacobian(a, data)\r\n        c = tf.reshape(c, (-1, dimension, dimension))\r\n\r\n        d = latent_distribution.log_prob(a_flat)\r\n        _, e = tf.linalg.slogdet(c)\r\n        ff = tf.reduce_mean(d + e)\r\n\r\n        loss = -ff\r\n\r\n    gradients = tape.gradient(loss, f.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients, f.trainable_variables))\r\n\r\ntrain_data = data.batch(1)\r\nfor batch in train_data:\r\n    train_step(batch)\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n<details><summary>log</summary>\r\n<p>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-11-aa3781dddb11> in <module>\r\n      1 for batch in train_data:\r\n----> 2     train_step(batch)\r\n\r\n.../python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n.../python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    821       # This is the first call of __call__, so we have to initialize.\r\n    822       initializers = []\r\n--> 823       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    824     finally:\r\n    825       # At this point we know that the initialization is complete (or less\r\n\r\n.../python3.8/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    694     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)\r\n    695     self._concrete_stateful_fn = (\r\n--> 696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n    697             *args, **kwds))\r\n    698 \r\n\r\n.../python3.8/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2853       args, kwargs = None, None\r\n   2854     with self._lock:\r\n-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2856     return graph_function\r\n   2857 \r\n\r\n.../python3.8/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3211 \r\n   3212       self._function_cache.missed.add(call_context_key)\r\n-> 3213       graph_function = self._create_graph_function(args, kwargs)\r\n   3214       self._function_cache.primary[cache_key] = graph_function\r\n   3215       return graph_function, args, kwargs\r\n\r\n.../python3.8/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3063     arg_names = base_arg_names + missing_arg_names\r\n   3064     graph_function = ConcreteFunction(\r\n-> 3065         func_graph_module.func_graph_from_py_func(\r\n   3066             self._name,\r\n   3067             self._python_function,\r\n\r\n.../python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n.../python3.8/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    599         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    602 \r\n\r\n.../python3.8/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    971           except Exception as e:  # pylint:disable=broad-except\r\n    972             if hasattr(e, \"ag_error_metadata\"):\r\n--> 973               raise e.ag_error_metadata.to_exception(e)\r\n    974             else:\r\n    975               raise\r\n\r\nKeyError: in user code:\r\n\r\n    <ipython-input-9-5c54f4064666>:24 train_step  *\r\n        gradients = tape.gradient(loss, f.trainable_variables)\r\n    .../python3.8/site-packages/tensorflow/python/eager/backprop.py:1067 gradient  **\r\n        flat_grad = imperative_grad.imperative_grad(\r\n    .../python3.8/site-packages/tensorflow/python/eager/imperative_grad.py:71 imperative_grad\r\n        return pywrap_tfe.TFE_Py_TapeGradient(\r\n    .../python3.8/site-packages/tensorflow/python/eager/backprop.py:162 _gradient_function\r\n        return grad_fn(mock_op, *out_grads)\r\n    .../python3.8/site-packages/tensorflow/python/ops/nn_grad.py:50 _Conv2DBackpropInputGrad\r\n        strides=op.get_attr(\"strides\"),\r\n    .../python3.8/site-packages/tensorflow/python/eager/backprop.py:121 get_attr\r\n        raise KeyError(attr)\r\n\r\n    KeyError: 'strides'\r\n```\r\n</p>\r\n</details>\r\n\r\nAnd log on a different setup:\r\n- OS Platform and distribution: Arch\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 2.5.0, 2cad9d750cadd825910b61351a731eb0e8031608\r\n- Python version: 3.8.6-1\r\n- CUDA/cuDNN version: 11.1.1-1 / 8.0.5.39-1\r\n- GPU model and memory: GeForce GTX 960M\r\n\r\n<details><summary>log</summary>\r\n<p>\r\n\r\n```\r\n2020-12-01 22:58:30.236554: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-12-01 22:58:31.813385: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-01 22:58:31.814170: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2020-12-01 22:58:33.296576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-01 22:58:33.296901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0\r\ncoreClock: 1.0975GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-12-01 22:58:33.296923: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-12-01 22:58:33.321243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-12-01 22:58:33.321345: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-12-01 22:58:33.335247: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-12-01 22:58:33.339668: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-12-01 22:58:33.359017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2020-12-01 22:58:33.365850: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-12-01 22:58:33.367723: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-12-01 22:58:33.367855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-01 22:58:33.368285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-01 22:58:33.368606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0\r\n2020-12-01 22:58:33.369722: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-01 22:58:33.369860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-01 22:58:33.370218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0\r\ncoreClock: 1.0975GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-12-01 22:58:33.370243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-12-01 22:58:33.370266: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-12-01 22:58:33.370284: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-12-01 22:58:33.370301: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-12-01 22:58:33.370316: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-12-01 22:58:33.370330: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2020-12-01 22:58:33.370344: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-12-01 22:58:33.370362: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-12-01 22:58:33.370428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-01 22:58:33.370793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-01 22:58:33.371107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0\r\n2020-12-01 22:58:33.371882: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-12-01 22:58:34.128469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-01 22:58:34.128514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      0 \r\n2020-12-01 22:58:34.128520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 0:   N \r\n2020-12-01 22:58:34.129295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-01 22:58:34.129675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-01 22:58:34.130039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-01 22:58:34.130359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1635 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2020-12-01 22:58:34.278962: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:127] None of the MLIR optimization passes are enabled (registered 2)\r\n2020-12-01 22:58:34.293661: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2601325000 Hz\r\n2020-12-01 22:58:35.392106: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-12-01 22:58:36.093824: I tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Loaded cuDNN version 8005\r\n2020-12-01 22:58:40.021160: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-12-01 22:58:40.920336: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-12-01 22:58:42.314731: I tensorflow/core/util/cuda_solvers.cc:180] Creating CudaSolver handles for stream 0x55ae4f348220\r\n2020-12-01 22:58:42.315311: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2020-12-01 22:58:43.103484: F tensorflow/core/util/cuda_solvers.cc:115] Check failed: cusolverDnCreate(&cusolver_dn_handle) == CUSOLVER_STATUS_SUCCESS Failed to create cuSolverDN instance.\r\n[1]    2834 abort (core dumped)  python bug.py\r\n```\r\n</p>\r\n</details>", "comments": ["@vandenheuvel \r\n\r\nCan you try with latest TF stable version 2.3 , 2.4-rc2 and see if the issue still persists.I have tried in colab with TF 2.3 and see the below error message (`AttributeError: 'PrefetchDataset' object has no attribute 'output_shapes'`).Thanks!", "@ravikyram thanks for taking a look. I updated the initial post with an update. I changed the cuda, cudnn and tensorflow version but the problem persists.", "@vandenheuvel,\r\nWhen trying to reproduce your issue, I am facing:\r\n\r\n**`InvalidArgumentError:  Input is not invertible.`** , even after adding the line, \r\n`data_shape = tf.compat.v1.data.get_output_shapes(data)[0]`\r\n\r\n\r\nPlease find the [Gist](https://colab.research.google.com/gist/rmothukuru/7547be8e7aa1a1f16ea983894816ec9c/gh_45063.ipynb). \r\n\r\nPlease help us reproduce your issue. Thanks!", "@rmothukuru @ravikyram I carefully recreated the code on *yet* another system. I updated the initial comment.\r\n\r\nNote that I now know exactly which **combination** of factors triggers this issue: the tf.function annotation and the softplus activation layer, running code on the GPU. If either of those two codelines is removed, the code works as one expects. Specifically, the activation layer should follow a convolutional layer.", "Could reproduce the error with TF Version 2.3. Please find the [Gist](https://colab.research.google.com/gist/rmothukuru/75e240716a5c8cb5ddc9e719c676b4d3/gh_45063.ipynb). Thanks!", "@rmothukuru could you update the label? TF 2.3 and I'm not sure whether keras still applies.", "Adding the `contributions welcome` label to this issue for further investigation by the community. If you are interested in working on this issue, please leave a comment and I will assign it to you. Thanks!", "@nikitamaia this is unambiguously a reproducible bug, could you reapply the label?", "Hi @vandenheuvel, yes sorry about that. I got this issue confused with another one :) Added the bug label back.", "While trying to reproduce the error, I noticed adding a Dense layer before the softplus runs fine. The problem occurs only when softplus has an immediate Conv2D before. I also went through the files mentioned in the Traceback. Spent an hour trying to modify the model arguments and layers. The KeyErrors iterate over **\"strides\", \"explicit_padding\"** and in few cases **\"dilation\"**. @vandenheuvel is it posible that there be a bug in your code at the part that calculates gradients (function `train_step` to be precise), may be `tf.reshape` not producing expected result. I could not figure out this part because it was quite math heavy. Changing the dimensions in `tf.reshape` results in the error mentioned by @rmothukuru. \r\n> \r\n> **`InvalidArgumentError: Input is not invertible.`** , even after adding the line,\r\n> `data_shape = tf.compat.v1.data.get_output_shapes(data)[0]`\r\n\r\nIf @vandenheuvel could explain this step , I might continue to look into this issue\r\n", "@Suraj1199 thanks for taking a look.\r\n\r\nThe Hessian determinant of a scalar function is computed. Note that the code works when removing the `@tf.function` annotation. As such, I don't think there is a bug in the snippet.\r\n\r\nIs there anything else I can clarify?", "No further clarification needed, thanks @vandenheuvel , seems like this problem occurs only during graph execution. I looked into its [source code](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/eager) to find the bug.", "@Suraj1199 do you mean that it only occurs during **non**-eager execution?", "Yes, edited.\n\n@vandenheuvel You might find these helpful:\n\n* [<code> tf.function</code> limitations](https://www.tensorflow.org/guide/function#limitations)\n* [<code>tf.GradientTape</code> details](https://www.tensorflow.org/guide/autodiff)", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> No further clarification needed, thanks @vandenheuvel , seems like ...\r\n\r\n@sushreebarsa This bug has been successfully reproduced. Please remove the `awaiting response` label."]}, {"number": 45030, "title": "Request implementation of SSE regularization", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3 (also used 1.14 before)\r\n- Are you willing to contribute it (Yes/No): \r\nI would like to help but I am not too familiar with tensorflow operator level coding.\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nSSE (stochastic shared embedding) has been proven to be a very useful embedding level regularization techniques in deep learning when dropout and weight decay do not work well enough for embeddings. \r\n\r\nSee paper: \"Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers\": https://papers.nips.cc/paper/2019/file/37693cfc748049e45d87b8c7d8b9aacd-Paper.pdf; \"SSE-PT: Sequential Recommendation Via Personalized Transformer\"(https://dl.acm.org/doi/10.1145/3383313.3412258); \"Multimodal Categorization of Crisis Events in Social Media\" (https://openaccess.thecvf.com/content_CVPR_2020/papers/Abavisani_Multimodal_Categorization_of_Crisis_Events_in_Social_Media_CVPR_2020_paper.pdf). \r\n\r\nBut this feature is largely missing in tensorflow implementation and makes it hard for users to use SSE, unlike dropout. Ideally it should be as easy to use SSE as using dropout or l2.\r\n\r\n**Will this change the current api? How?**\r\nYes, this would require a new embedding_lookup operator that supports embeddings stochastic transitioning according to a predefined sparse probability transition matrix or a uniform transition mechanism at default. The uniform case is implemented at https://github.com/wuliwei9278/SSE-PT as a simple example but this is largely a hack. \r\n\r\n**Who will benefit with this feature?**\r\nI work at Linkedin and when training embeddings using this regularization technique in addition to dropout and l2 would lead to faster convergence of neural nets and better performance but I had to use it using a hack by duplicating training data with embedding ids masked into different ids, which is not very efficient and cannot be scaled when dataset size is large. Including this would make my life much easier, and likewise for my colleagues and many researchers, whose models also suffer from model embedding layer overfitting.\r\n\r\n**Any Other info.**\r\nPlease contact liwwu@linkedin.com for more information.", "comments": ["Just want to follow up to see if there are any questions or concerns that I can help to address. Happy Thanksgiving!", "any progress on this?"]}, {"number": 45026, "title": "Reduce on MirroredVariable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.0rc2\r\n- Python version: 3.8.3\r\n- CUDA/cuDNN version:  11.0\r\n- GPU model and memory: 4*1080Ti\r\n\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nRun the following code:\r\n```python\r\nimport tensorflow as tf\r\n\r\ns = tf.distribute.MirroredStrategy()\r\n\r\nwith s.scope():\r\n    v = tf.Variable(tf.ones((5,)))\r\nfor x in v.values:\r\n    print(x, x.device)\r\n\r\n# @tf.function\r\ndef run():\r\n    def fn(value):\r\n        ctx = tf.distribute.get_replica_context()\r\n        # value = 0.0 + value\r\n        return ctx.all_reduce(tf.distribute.ReduceOp.SUM, value)\r\n    return s.run(fn, args=(v,))\r\n\r\nprint(run())\r\n```\r\n\r\nOutputs:\r\n```\r\nPerReplica:{\r\n  0: tf.Tensor([1. 1. 1. 1. 1.], shape=(5,), dtype=float32),\r\n  1: tf.Tensor([1. 1. 1. 1. 1.], shape=(5,), dtype=float32),\r\n  2: tf.Tensor([1. 1. 1. 1. 1.], shape=(5,), dtype=float32),\r\n  3: tf.Tensor([1. 1. 1. 1. 1.], shape=(5,), dtype=float32)\r\n}\r\n```\r\n\r\nWhich is clearly wrong.\r\n\r\n**Describe the expected behavior**\r\n\r\nExpect output:\r\n```\r\nPerReplica:{\r\n  0: tf.Tensor([4. 4. 4. 4. 4.], shape=(5,), dtype=float32),\r\n  1: tf.Tensor([4. 4. 4. 4. 4.], shape=(5,), dtype=float32),\r\n  2: tf.Tensor([4. 4. 4. 4. 4.], shape=(5,), dtype=float32),\r\n  3: tf.Tensor([4. 4. 4. 4. 4.], shape=(5,), dtype=float32)\r\n}\r\n```\r\n\r\nUncomment the line `# value = 0.0 + value` will make the result correct.", "comments": ["Was able to reproduce your issue in Tensorflow 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/adf1514f5ae43210262ae4cac4467589/45026.ipynb), Thanks!"]}, {"number": 45021, "title": "execution_profile_test_with_xla_hlo_profile_cpu fails on s390x", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): Ubuntu 7.5.0-3ubuntu1~18.04\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen running test case  `//tensorflow/compiler/xla/tests:execution_profile_test_with_xla_hlo_profile_cpu` on s390x, the test case fails with following error:\r\n```\r\n[ RUN      ] ExecutionProfileTest.ExecuteWithExecutionProfile\r\n2020-10-29 23:11:59.065069: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 1555500000 Hz\r\n2020-10-29 23:11:59.065621: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2aa54027e20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-29 23:11:59.065633: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-29 23:11:59.077512: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2aa54028350 initialized for platform Interpreter (this does not guarantee that XLA will be used). Devices:\r\n2020-10-29 23:11:59.077526: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Interpreter, <undefined>\r\ntensorflow/compiler/xla/tests/execution_profile_test.cc:60: Failure\r\nExpected: (execution_profile.compute_cycle_count()) > (0), actual: 0 vs 0\r\n[  FAILED  ] ExecutionProfileTest.ExecuteWithExecutionProfile (36 ms)\r\n[----------] 1 test from ExecutionProfileTest (36 ms total)\r\n```\r\n\r\nOn debugging this test case a little, I found that the Test case is able to call `RecordCycleDelta` and registering values in `new_cycle_count` but on calling `compute_function_`, the `profile_counters` are not filled.\r\nThe compute function here is returning a valid value in `profile_counters` for x86 but returning 0 for s390x.\r\nhttps://github.com/tensorflow/tensorflow/blob/21f5c12e3d9c5b0c2f4c45c70a3da08b4edf212d/tensorflow/compiler/xla/service/cpu/cpu_executable.cc#L188\r\n\r\n**Describe the expected behavior**\r\nThe `profile_counters` should be populated with a valid value and test case should pass.\r\n\r\n**Standalone code to reproduce the issue**\r\n```bazel --host_jvm_args=\"-Xms1024m\" --host_jvm_args=\"-Xmx2048m\" test --host_javabase=\"@local_jdk//:jdk\" --test_tag_filters=-gpu,-benchmark-test,-v1only,-no_oss,-oss_serial -k --test_timeout 300,450,1200,3600 --build_tests_only --test_output=errors --verbose_failures -- //tensorflow/compiler/xla/tests:execution_profile_test_with_xla_hlo_profile_cpu```\r\n", "comments": ["Hi @r4nt , A very happy and safe new year to you! Could you find anything on this issue?", "@skribm9 It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "Hi @sushreebarsa - these problems still exist on s390x TF 2.5.0."]}, {"number": 45013, "title": "Test linking fails due to missing ldl", "body": "\r\n**System information**\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.0-rc2\r\n\r\n**Describe the problem**\r\n\r\nBuilding the tests `//tensorflow/python/...` fails with a link error on `//tensorflow/python/tools:aot_compiled_test` due to stacktrace.h using `dladdr` but nothing in the chain links against `-ldl`.\r\n\r\nChain: aot_compiled_test -> \"//tensorflow/core:test_main\" -> \"//tensorflow/core/platform:test_main\" -> \":stacktrace_handler\"\r\n\r\n**Any other info / logs**\r\nLooks like the issue can simply be fixed by adding the ldl flag to the ls test_main rule\r\n", "comments": ["@Flamefire, \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Just running `bazel test //tensorflow/python/...` should be enough, however I guess it depends on the system whether libdl.so is already linked transitively.\r\n\r\nSo I'd rather argue from a correctness/review way:\r\n- `dladdr` is used: https://github.com/tensorflow/tensorflow/blob/602bac2ab8c7f995ce0a028121061568109b5b16/tensorflow/core/platform/default/stacktrace.h#L55\r\n- That inline function calling `dladdr` is used: https://github.com/tensorflow/tensorflow/blob/dec8e0b11f4f87693b67e125e67dfbc68d26c205/tensorflow/core/platform/default/stacktrace_handler.cc#L88\r\n- The `cc_library` rule for that does not link libdl: https://github.com/tensorflow/tensorflow/blob/602bac2ab8c7f995ce0a028121061568109b5b16/tensorflow/core/platform/BUILD#L633-L638\r\n- `test_main` links against the stacktrace rule but but does not link libdl either: https://github.com/tensorflow/tensorflow/blob/602bac2ab8c7f995ce0a028121061568109b5b16/tensorflow/core/platform/BUILD#L959-L977\r\n- And finally the test links against `test_main` but not libdl: https://github.com/tensorflow/tensorflow/blob/3c4b0fc75f07991c85b56eb8d7515f3cfb9b7fc8/tensorflow/python/tools/BUILD#L501-L519\r\n\r\nConclusion: A function (`dladdr`) which requires a library (`libdl`) is used but that library is never linked (explicitly).\r\n\r\nIt is used for other rules though, e.g. https://github.com/tensorflow/tensorflow/blob/f2846d35f44ae7a5cd0070381a5cff94f5ddb6fc/tensorflow/core/lib/gif/BUILD#L32", "@Flamefire Could you please try using latest stable version of **TF 2.6.0** and let us know if the issue still persists? Thank you!", "Yes the issue still persists"]}, {"number": 45009, "title": "TF Lite issue when loading a saved TF Lite model on platforms with different endianness", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): UB18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2.1\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): bazel 2.0.0- (@non-git)\r\n- GCC/Compiler version (if compiling from source): gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nThis issue is created for tracking purposes in respect to the discussion done in PR request #44063 \r\n@jdduke Thank you for offering to look into this issue and letting me tag you here.\r\n\r\nThe problem is that the TfLite saved models only support Little Endian format by default. When loading these model, flatbuffer is not able to handle some non-native data types like Tensor. We need some extra efforts on both the saving side and loading side to manually do the byte-swap on Big-Endian machine. \r\n\r\n**Describe the expected behavior**\r\nWe should be able to address the endianness issue while keeping the saved model in Little-Endian format. \r\n\r\n", "comments": ["Thanks for the report! I've filed an internal tracking bug to figure out next steps, will keep you posted on progress.", "Hi @jdduke, I think I just got some useful information in addressing this issue.\r\nBased on my investigation, the issue here is not limited to serialize and deserialize tflite flatbuffer models, because I also identified this issue when using the experimental `mlir_quantizier`, in which case the flatbuffer model is converted into a mlir module then quantized and converted back.\r\nI believe the major issue is related to the `buffers` field as defined in the [schema](https://github.com/tensorflow/tensorflow/blob/47764eaf7473224cd3fe3cfb34706cfd219c532c/tensorflow/lite/schema/schema.fbs#L1154). The `Buffer` data type is 1 byte long but they actually represent data of longer bytes like `int` or `float`, which may cause the data in the `buffers` field not to be stored in little-endian, while flatbuffer objects should always be stored in little-endian.\r\nWhen the `buffers` field of the model is populated, it is using methods similar to `memcpy`, like in the old tflite toco converter [here](https://github.com/tensorflow/tensorflow/blob/47764eaf7473224cd3fe3cfb34706cfd219c532c/tensorflow/lite/toco/tflite/export.cc#L227) and in the new mlir converter [here](https://github.com/tensorflow/tensorflow/blob/47764eaf7473224cd3fe3cfb34706cfd219c532c/tensorflow/compiler/mlir/lite/flatbuffer_export.cc#L694). As a result, the underlying data in the `buffers` field is in host machine native endianness rather than little-endian.\r\nIn comparison, other fields in the schema like `quantization_parameters` or `shape` are all typed values, so when populating them in the model, flatbuffer compiler will handle the endianness automatically (i.e. always store them in little endian, byteswap when needed).\r\n\r\nThe issue with the `buffers` field is a fundamental thing that needs to be fixed for big-endian machines to use tflite properly, but currently, it is not entirely broken because of the following reasons:\r\n1. Both write and read to `buffers` field is using host endianness, so the issue will not emerge if the flatbuffer model is untouched - it only emerges when the flatbuffer model is exported or converted, like serialized into a `.tflite` file and being read on another machine, or being converted into a mlir module as I mentioned above.\r\n2. I am not sure about this one but the `buffers` field seems only being used for constant operator tensors. It looks like those tensors are the only tensors whose values are stored in the model `buffers`. So the tensors whose values are calculated after model evaluation will not have this issue.\r\n\r\nDue to the schema definition, we could not rely on any flatbuffer API changes to fix this issue. I think to fix this, we need to deal with the endianness manually whenever we try to populate the `buffers` field in the tflite model (like in the two converters I mentioned above) or read the `buffers` field of a finished model (like in `InterpreterBuilder` and maybe somewhere else). It looks to me that we need to do a case split for the data type every single time when `buffers` field is accessed, which is a bit annoying to be honest. I hope you guys could come up with something more element to deal with this issue, and please let me know if there is anything I could help, thanks.", "You're right, that does leave us in a tricky situation.\r\n\r\nFor my own understanding, are you most interested in supporting the (big endian) -> (big endian) path, where both the conversion and execution take place on a big-endian machine? Or all potential permutations of little/big endian conversion and deployment?", "For me, it will be best if we could settle this once and for all - i.e. make sure it is consistent across all platforms so that the entire flatbuffer models will work seamlessly across different machines and converters. So I think the second approach would be more preferred (while the first approach could be a temporary workaround for us).\r\nMy ideal solution on this would be implementing something similar to [byte_swap.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/tensor_bundle/byte_swap.cc) or other flatbuffer native method like EndianScalar at all the places where the `buffers` field is read or write (I hope there are not too many places besides the 2 converts and the `ParseTensors` call in `interpreter_builder.cc`). But I'm afraid it has to be more complicated as we will have to enumerate through the buffer vector to implement the byte-swap so we could not use those straightforward methods like `memcpy` anymore.", "> For me, it will be best if we could settle this once and for all\r\n\r\nAgreed, my question is more toward determining next steps and ordering. There could be a few simple tweaks we could make to get big -> big working, on the way toward full generality, if that is a blocker for any specific use-cases or project you're working on. If not, then the order of work doesn't necessarily matter and we can focus on the general solution.", "Sure, thanks for being so thoughtful! I think the big -> big route does not have a general blocker right now. As mentioned before, to save and load `.tflite` binary model on the same machine does not have any issues.\r\nWhat I did notice that may require a bit of tweaks is [here](https://github.com/tensorflow/tensorflow/blob/fcc4b966f1265f466e82617020af93670141b009/tensorflow/compiler/mlir/lite/flatbuffer_import.cc#L269). In this function and some lines below we are trying to access the buffer data in little-endian format. This method is taking endianness into consideration but unfortunately, they will retrieve the wrong data on big-endian machines right now. Do you think it will be necessary if we change this code into reading host endianness as a workaround and then change it back once we fix the `buffer` issue?", "According to this,\r\nhttps://google.github.io/flatbuffers/md__internals.html\r\n`A FlatBuffer is a binary file and in-memory format consisting mostly of scalars of various sizes, all aligned to their own size. Each scalar is also always represented in little-endian format, as this corresponds to all commonly used CPUs today. FlatBuffers will also work on big-endian machines, but will be slightly slower because of additional byte-swap intrinsics.`\r\n\r\nSo, it looks like we'd better use little-endian for `buffers` for consistency.", "> According to this,\r\n> https://google.github.io/flatbuffers/md__internals.html\r\n> `A FlatBuffer is a binary file and in-memory format consisting mostly of scalars of various sizes, all aligned to their own size. Each scalar is also always represented in little-endian format, as this corresponds to all commonly used CPUs today. FlatBuffers will also work on big-endian machines, but will be slightly slower because of additional byte-swap intrinsics.`\r\n> \r\n> So, it looks like we'd better use little-endian for `buffers` for consistency.\r\n\r\nYes, that is true, and I think this is basically what we intend to achieve at last. Currently, all the other fields defined in the schema (which have types) are working properly except `buffers`, so it would make sense to fix `buffers` to use little-endian.\r\n\r\n\r\nOn the other hand, for those fields that are correctly stored in little-endian format, there do exists some places in the codebase where the access to these flatbuffer data is a bit careless, like the issue you are mentioning. It will be better to access data only using flatbuffer methods rather than the raw data pointer because flatbuffer methods will handle the endianness automatically. I am working on a PR for this particular issue and I will also keep an eye on any other similar issue related to flatbuffer endianness.", "A short update:  I forgot to mention this before, that the `CustomQuantization` field and `custom_options` field may also require similar fixes as `buffers` field as per [schema definition](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs). I did not encounter these two fields frequently but theoretically they have the same issue as `buffers` field as they are also `ubyte` typed. ", "I think I also encountered similar issues when I ran the test case `//tensorflow/lite/tools/optimize:quantize_model_test` on BE machines.\r\n\r\nThe `LSTM` related tests in this test case would fail on BE platforms due to the following reasons:\r\n1. `LSTM` related binary model files are not loaded correctly on BE machines since the tensor data is stored in LE format.\r\n2. In certain quantization process, values should be copied into buffers from the tail on BE machines but actually they are not.\r\n\r\nI made the code changes below to make this test case pass on BE machines:\r\n1. Swap the buffer data once the binary model files are loaded.\r\n2. Fill the buffers from the tail in two quantization related functions: `SymmetricQuantizeFloatsToInt16()` (in `tensorflow/lite/tools/optimize/quantization_utils.cc`) and `PortableSymmetricQuantizeFloats()` (in `tensorflow/lite/kernels/internal/reference/portable_tensor_utils.cc`) .\r\n\r\nThe code change would make this test case pass. However, it would cause regression on test cases like  `//tensorflow/lite/kernels:lstm_test` and `//tensorflow/lite/kernels:svdf_test`. \r\n\r\nI believe the reason of this regression is that there are other code snippets in the execution path of these TCs which also need to be updated, so that the buffer loading (filling) behaviour on BE machines would become consistent, just as @Sidong-Wei mentioned in https://github.com/tensorflow/tensorflow/issues/45009#issuecomment-740008230.\r\n\r\nIt looks like this flatbuffer endianness issue still needs an overall solution. @jdduke @terryheo Any updates from internal tracking? Thank you very much!\r\n", "Hi @jdduke @terryheo , hope all is well.\r\n\r\nWonder if you have any updates reg this issue? Thanks!"]}, {"number": 45005, "title": "Unable to perform subsampling using tf.data.Dataset map() method.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe preprocessing of sequences can easily be a bottleneck in the Deep Learning pipeline. And tf.data.Dataset provides us ways to optimize the input pipeline. But there are limitations to using a custom python function as a preprocessing step. For example, I am trying to train a model to predict the next item(an item can be a movie, song, book etc), given a history of the items consumed. And I want to use a subsampling function to be passed in the map method. The main challenge here is that I can not pass the item2prob dictionary to my custom subsampling function through the map method. And I am not sure if there are any methods available to pull this off.\r\n\r\n**Will this change the current api? How?**\r\nThe current API will support python dictionaries to be passed into a custom python function through the map method.\r\n\r\n**Who will benefit with this feature?**\r\nThis feature will open up possibilities for people trying to optimize their input pipeline using custom preprocessing functions.\r\n\r\n**Any Other info.**\r\nHere is what I have trying to do-\r\n\r\n```\r\n# make a python generator\r\n\r\ndef get_generator_random():\r\n    '''\r\n    yields 10 sequences of varying lengths (3 to 7) with values ranging from 0, 5\r\n    '''\r\n    num_seq = 10\r\n    seq_count = 0\r\n    while seq_count < num_seq:\r\n        rand_seq_len = np.random.randint(3, 8)\r\n        seq = np.random.randint(0,6,rand_seq_len)\r\n        yield seq\r\n        seq_count += 1\r\n\r\npy_gen = get_generator_random()\r\nfor i in py_gen:\r\n    print(i)\r\n\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n[2 2 2 3 5 4]\r\n[3 2 0]\r\n[0 2 1 3 4]\r\n[2 4 4 2 2 4 2]\r\n[5 1 3 0 4 5 4]\r\n[2 5 4 5]\r\n[0 2 2 2 0]\r\n[0 2 3]\r\n[0 4 0]\r\n[2 2 1 4 0 1 0]\r\n```\r\n\r\n```\r\n# make tf dataset from python generator \r\n\r\ndef get_tf_gen():\r\n    tf_gen = tf.data.Dataset.from_generator(\r\n        get_generator,\r\n        output_types=tf.int32, \r\n        output_shapes = (None, )\r\n    )\r\n\r\n    return tf_gen\r\n\r\n\r\ntf_gen = get_tf_gen()\r\nfor i in tf_gen:\r\n  print(i)\r\n```\r\n\r\nOutput:\r\n\r\n```\r\ntf.Tensor([2 2], shape=(2,), dtype=int32)\r\ntf.Tensor([5 1 2 3 4], shape=(5,), dtype=int32)\r\ntf.Tensor([5 1 2 3 4 5], shape=(6,), dtype=int32)\r\ntf.Tensor([5 1 2 2 4 5], shape=(6,), dtype=int32)\r\ntf.Tensor([3 1 2 2 4 2], shape=(6,), dtype=int32)\r\ntf.Tensor([1 1 2 3 3 5], shape=(6,), dtype=int32)\r\ntf.Tensor([2 1 5 3 1 5], shape=(6,), dtype=int32)\r\ntf.Tensor([5 2 2 4 4 2], shape=(6,), dtype=int32)\r\n```\r\n\r\n\r\n```\r\n# define item2prob (prob of removing an item)\r\n\r\nitem2prob = {}\r\nfor i in range(1, 9):\r\n    if i < 5:\r\n        item2prob[i] = 1\r\n    else:\r\n        item2prob[i] = 0.0\r\n\r\nprint(item2prob)\r\n```\r\n\r\n`{1: 1, 2: 1, 3: 1, 4: 1, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0}`\r\n\r\n```\r\ndef subsample(seq):\r\n    seq_new = []\r\n    random_numbers = tf.random.uniform(shape=(len(seq), ), minval=0, maxval=1, dtype=tf.dtypes.float32,).numpy()\r\n    for s_idx, s in enumerate(seq):\r\n        if random_numbers[s_idx] >= item2prob[s]: # this is where the error occurs\r\n            seq_new.append(s)\r\n    return seq_new  \r\n\r\ndef tf_subsample(seq):  \r\n    seq_new = tf.py_function(subsample, inp=[seq], Tout=tf.int32)\r\n    return seq_new\r\n\r\ntf_gen = get_tf_gen()\r\ntf_gen = tf_gen.map(tf_subsample)\r\ntf_gen = tf_gen.padded_batch(4, padded_shapes=5)\r\n\r\nprint('\\nGenerating data...')\r\nfor i in tf_gen:\r\n    print(i)\r\n```\r\n\r\nFollowing is the error message-\r\n\r\n```\r\nGenerating data...\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)\r\n   2101       ctx.executor = executor_new\r\n-> 2102       yield\r\n   2103     finally:\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    757             output_types=self._flat_output_types,\r\n--> 758             output_shapes=self._flat_output_shapes)\r\n    759 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py in iterator_get_next(iterator, output_types, output_shapes, name)\r\n   2609     except _core._NotOkStatusException as e:\r\n-> 2610       _ops.raise_from_not_ok_status(e, name)\r\n   2611     except _core._FallbackException:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6842   # pylint: disable=protected-access\r\n-> 6843   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6844   # pylint: enable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 242, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 131, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 302, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"<ipython-input-79-e5206ab2321d>\", line 5, in subsample\r\n    if random_numbers[s_idx] >= item2prob[s]: # this is where the error occurs\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 823, in __hash__\r\n    raise TypeError(\"Tensor is unhashable. \"\r\n\r\nTypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.\r\n\r\n\r\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-79-e5206ab2321d> in <module>()\r\n     16 \r\n     17 print('\\nGenerating data...')\r\n---> 18 for i in tf_gen:\r\n     19     print(i)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in __next__(self)\r\n    734 \r\n    735   def __next__(self):  # For Python 3 compatibility\r\n--> 736     return self.next()\r\n    737 \r\n    738   def _next_internal(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in next(self)\r\n    770   def next(self):\r\n    771     try:\r\n--> 772       return self._next_internal()\r\n    773     except errors.OutOfRangeError:\r\n    774       raise StopIteration\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    762         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access\r\n    763       except AttributeError:\r\n--> 764         return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n    765 \r\n    766   @property\r\n\r\n/usr/lib/python3.6/contextlib.py in __exit__(self, type, value, traceback)\r\n     97                 value = type()\r\n     98             try:\r\n---> 99                 self.gen.throw(type, value, traceback)\r\n    100             except StopIteration as exc:\r\n    101                 # Suppress StopIteration *unless* it's the same exception that\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)\r\n   2103     finally:\r\n   2104       ctx.executor = executor_old\r\n-> 2105       executor_new.wait()\r\n   2106 \r\n   2107 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py in wait(self)\r\n     65   def wait(self):\r\n     66     \"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\r\n---> 67     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n     68 \r\n     69   def clear_error(self):\r\n\r\nInvalidArgumentError: TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 242, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 131, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 302, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"<ipython-input-79-e5206ab2321d>\", line 5, in subsample\r\n    if random_numbers[s_idx] >= item2prob[s]: # this is where the error occurs\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 823, in __hash__\r\n    raise TypeError(\"Tensor is unhashable. \"\r\n\r\nTypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.\r\n\r\n\r\n\t [[{{node EagerPyFunc}}]]\r\n```", "comments": ["@animesh0794 Are you working on recommender systems?", "@bhack  yes I am.", "> @bhack  yes I am.\n\nWe have a new project in ecosystem:\nhttps://www.tensorflow.org/recommenders\nhttps://blog.tensorflow.org/2020/09/introducing-tensorflow-recommenders.html\nhttps://www.tensorflow.org/recommenders/examples/quickstart", "@bhack I am aware of it. But thanks anyway. I just wanted to try my own code first, before trying TFRS : )  ", "Yes but you can still be inspired on how dataset are managed https://github.com/tensorflow/recommenders/blob/main/tensorflow_recommenders/examples/movielens.py", "@bhack thanks, i would surely check that out."]}, {"number": 44996, "title": "I strongly suspect that we are going to want a dedicated tosa-opt of some form eventually, but we don't need to start there if it is not convenient. We will eventually have a dedicated/minimal binary for importing from TFLite Flatbuffers at least, and I'm keeping an eye on how we can keep the build boundaries sufficient to meet that end.", "body": "I strongly suspect that we are going to want a dedicated tosa-opt of some form eventually, but we don't need to start there if it is not convenient. We will eventually have a dedicated/minimal binary for importing from TFLite Flatbuffers at least, and I'm keeping an eye on how we can keep the build boundaries sufficient to meet that end.\r\n\r\n_Originally posted by @stellaraccident in https://github.com/tensorflow/tensorflow/pull/44851#discussion_r523237473_", "comments": ["@stellaraccident , currently we use lite/flatbuffer_translate to convert from TFLite Flatbuffers to MLIR form. Given the existence of muiltiple translators and pass manager, this sounds like something that could wait until there's a clearer picture of how to do this."]}, {"number": 44995, "title": "TF_RegisterLogListener in C API does not seem to work", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I call TF_RegisterLogListener in the C API, it doesn't seem to work as described. My listener is never called, and TensorFlow log messages still go to the console.  The API description is:\r\n\r\n// Register a listener method that processes printed messages.\r\n//\r\n// If any listeners are registered, the print operator will call all listeners\r\n// with the printed messages and immediately return without writing to the\r\n// logs.\r\n\r\n**Describe the expected behavior**\r\n\r\nMy listener should be called and the messages should not be written to the console.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nextern \"C\"\r\n{\r\n  void logListener(const char *message)\r\n  {\r\n    // This is never called and TensorFlow log messages go to the console instead.\r\n    std::cout << \"TensorFlow: \" << message << std::endl;\r\n  }\r\n}\r\n\r\nTF_RegisterLogListener(logListener);\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi @gehlhaarPfizer , when you say \"log messages\" do you mean all TF logging or specifically messages/tensors printed via `tf.print`?\r\n\r\nOriginally this API was created to make sure `tf.print` specifically would be able to print to jupyter notebook / colab cell outputs. Although there were tentative plans to explore having all TF logging go through it, we didn't really have the bandwidth to do so.\r\n\r\n@rohan100jain to see if tf-core should revisit this.\r\n\r\n", "Ah. I misunderstood. I had expected all TensorFlow logging to go through the listener and not be output.\r\n\r\nI have a small DNN model that I built in Python and then incorporated into an existing C++ program for production deployment. I took the shortest path to put this into C++, which was through the C API. The C++ API is horribly complex. I don't understand why there isn't a C++ API as clean as the Python API. But that is a topic for another conversation.\r\n\r\nWhile it works correctly, I get superfluous output from TensorFlow to my console:\r\n\r\n2020-11-23 14:06:43.917926: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /gpfs/apps/medsci/stacks/noOS/software/zing/latest/bin/../parameters/FitScoreModel.tf\r\n2020-11-23 14:06:44.409069: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2020-11-23 14:06:44.417661: I tensorflow/cc/saved_model/loader.cc:234] Reading SavedModel debug info (if present) from: /gpfs/apps/medsci/stacks/noOS/software/zing/latest/bin/../parameters/FitScoreModel.tf\r\n2020-11-23 14:06:44.556361: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-11-23 14:06:45.745551: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\r\n2020-11-23 14:06:45.815726: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2633b30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-11-23 14:06:45.815758: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-11-23 14:06:46.971689: I tensorflow/cc/saved_model/loader.cc:199] Restoring SavedModel bundle.\r\n2020-11-23 14:06:52.002029: I tensorflow/cc/saved_model/loader.cc:183] Running initialization op on SavedModel bundle at path: /gpfs/apps/medsci/stacks/noOS/software/zing/latest/bin/../parameters/FitScoreModel.tf\r\n2020-11-23 14:06:52.034424: I tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 8134566 microseconds.\r\n\r\nI would really like to redirect this to my program's internal logging mechanism, which goes to a log file. I can suppress this output entirely via an environment variable but I would prefer to redirect it.", "Seconding this on behalf of SIG JVM, we would like to be able to redirect the core logging to a Java logging API (almost certainly SLF4J).  Additionally, it would be **extremely** nice if the callback had some structure, i.e. `(severity: int, message: str)` or similar, maybe with the time stamp, rather than just giving us the strings.", "Related change: https://github.com/tensorflow/tensorflow/pull/41733 cc @avitebskiy \r\n\r\nIIRC, this is not C-API(yet) but could you try `TFAddLogSink(...)` and see if it works for you?", "It seems to work to some extent, but the added sink doesn't get all messages.  For example, if I just echo the data back, I see stuff like:\r\n```\r\n[stderr] 2021-06-26 19:45:11.097075: I external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:361] Device mapping: no known devices.\r\n[stderr] 2021-06-26 19:45:11.102576: I external/org_tensorflow/tensorflow/core/common_runtime/placer.cc:114] testScope/absWithDevice: (Abs): /job:localhost/replica:0/task:0/device:CPU:0\r\n[stderr] 2021-06-26 19:45:11.102898: I external/org_tensorflow/tensorflow/core/common_runtime/placer.cc:114] testScope/Const: (Const): /job:localhost/replica:0/task:0/device:CPU:0\r\nLog message: Severity: 0, Fname: external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc, line: 361, string: Device mapping: no known devices.\r\n\r\n[stderr] 2021-06-26 19:45:11.115882: I external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:361] Device mapping: no known devices.\r\n[stderr] 2021-06-26 19:45:11.119011: I external/org_tensorflow/tensorflow/core/common_runtime/placer.cc:114] testScope/absWithDevice: (Abs): /job:localhost/replica:0/task:0/device:CPU:0\r\n[stderr] 2021-06-26 19:45:11.119308: I external/org_tensorflow/tensorflow/core/common_runtime/placer.cc:114] testScope/Const: (Const): /job:localhost/replica:0/task:0/device:CPU:0\r\nLog message: Severity: 0, Fname: external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc, line: 361, string: Device mapping: no known devices.\r\n\r\n[stderr] 2021-06-26 19:45:11.125814: I external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:361] Device mapping: no known devices.\r\n[stderr] 2021-06-26 19:45:11.128253: I external/org_tensorflow/tensorflow/core/common_runtime/placer.cc:114] testScope1/absWithDevice: (Abs): /job:localhost/replica:0/task:0/device:CPU:0\r\n[stderr] 2021-06-26 19:45:11.128562: I external/org_tensorflow/tensorflow/core/common_runtime/placer.cc:114] testScope2/mulWithDevice: (Mul): /job:localhost/replica:0/task:0/device:CPU:0\r\n[stderr] 2021-06-26 19:45:11.128816: I external/org_tensorflow/tensorflow/core/common_runtime/placer.cc:114] testScope1/Const: (Const): /job:localhost/replica:0/task:0/device:CPU:0\r\n[stderr] 2021-06-26 19:45:11.129037: I external/org_tensorflow/tensorflow/core/common_runtime/placer.cc:114] testScope2/Const: (Const): /job:localhost/replica:0/task:0/device:CPU:0\r\nLog message: Severity: 0, Fname: external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc, line: 361, string: Device mapping: no known devices.\r\n\r\n[stderr] 2021-06-26 19:45:11.134049: I external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:361] Device mapping: no known devices.\r\n[stderr] 2021-06-26 19:45:11.136836: I external/org_tensorflow/tensorflow/core/common_runtime/placer.cc:114] testScope/absWithDevice: (Abs): /job:localhost/replica:0/task:0/device:CPU:0\r\n[stderr] 2021-06-26 19:45:11.137125: I external/org_tensorflow/tensorflow/core/common_runtime/placer.cc:114] testScope/Const: (Const): /job:localhost/replica:0/task:0/device:CPU:0\r\nLog message: Severity: 0, Fname: external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc, line: 361, string: Device mapping: no known devices.\r\n\r\n[stderr] 2021-06-26 19:45:11.150138: I external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:361] Device mapping: no known devices.\r\nLog message: Severity: 0, Fname: external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc, line: 361, string: Device mapping: no known devices.\r\n\r\n[stderr] 2021-06-26 19:45:11.941404: W external/org_tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 400000000 exceeds 10% of free system memory.\r\n[stderr] 2021-06-26 19:45:11.941443: W external/org_tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 400000000 exceeds 10% of free system memory.\r\n\r\n[stderr] 2021-06-26 19:45:12.270591: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: /tmp/tf-saved-model-export-test17892089746029133424\r\n[stderr] 2021-06-26 19:45:12.270678: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\r\n[stderr] 2021-06-26 19:45:12.270688: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tf-saved-model-export-test17892089746029133424\r\n[stderr] 2021-06-26 19:45:12.271437: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\r\n[stderr] 2021-06-26 19:45:12.273924: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 3338 microseconds.\r\n[stderr] 2021-06-26 19:45:12.282239: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: /windows/Users/jimne/Desktop/OtherStuff/tensorflow_java/tensorflow-core/tensorflow-core-api/target/test-classes/saved_model\r\n[stderr] 2021-06-26 19:45:12.282918: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\r\n[stderr] 2021-06-26 19:45:12.282937: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /windows/Users/jimne/Desktop/OtherStuff/tensorflow_java/tensorflow-core/tensorflow-core-api/target/test-classes/saved_model\r\n[stderr] 2021-06-26 19:45:12.283734: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\r\n[stderr] 2021-06-26 19:45:12.287482: I external/org_tensorflow/tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\r\n[stderr] 2021-06-26 19:45:12.287504: I external/org_tensorflow/tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\r\n```\r\n\r\nThe `[stderr]` messages is what is printed to `stderr`, the `Log message:` is my echoing.\r\n\r\nLooking at the code for `direct_session.cc` and `placer.cc`, both look correct, so I'm not sure what's going on.  This is on Linux, 2.5.0, I can make a full issue report if necessary.  The full code is here: https://github.com/tensorflow/java/pull/345\r\n\r\nBuilding with `--define no_default_logger=true` didn't change anything."]}, {"number": 44984, "title": "Incorrect error handling for invalid input names on SavedModel export", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Any\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7.6\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nA_FEATURE_NAME = 'a feature'\r\nB_FEATURE_NAME = 'b_feature'\r\n\r\nclass MyModel(tf.keras.Model):\r\n\r\n    def call(self, inputs):\r\n        return inputs[A_FEATURE_NAME] + inputs[B_FEATURE_NAME]\r\n\r\ninputs = {\r\n    A_FEATURE_NAME: tf.constant([1.]),\r\n    B_FEATURE_NAME: tf.constant([2.]),\r\n}\r\n\r\nmodel = MyModel()\r\noutputs = model(inputs)\r\n\r\nmodel.save('test')\r\n```\r\n\r\nSaving this model gives the following error message\r\n\r\n```\r\nValueError: Got non-flat/non-unique argument names for SavedModel signature 'serving_default': more than one argument to '__inference_signature_wrapper_48' was named 'a feature'. Signatures have one Tensor per named input, so to have predictable names Python functions used to generate these signatures should avoid *args and Tensors in nested structures unless unique names are specified for each. Use tf.TensorSpec(..., name=...) to provide a name for a Tensor input.\r\n```\r\n\r\nThe reason however is that `A_FEATURE_NAME` has an invalid name due to it containing whitespace, so it ends up not matching the placeholder name when tracing the graph for saving: https://github.com/tensorflow/tensorflow/blob/5d8b439eb41fb2373c5b5c5f33c51313cfd9e57f/tensorflow/python/saved_model/save.py#L494\r\n\r\n**Describe the expected behavior**\r\nTensorFlow checks for valid names elsewhere (such as when [declaring an op](https://github.com/tensorflow/tensorflow/blob/00d1dc082eb0fde33e5d8944e4f203248be335f7/tensorflow/python/framework/ops.py#L1804)), so the same logic should be used to return a more relevant error e.g., `ValueError: 'a feature' is not a valid input name` ", "comments": ["I ran the code shared and i am able to replicate the error reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/5bdb0ceb7ad7db8569bc9bd485888a64/untitled463.ipynb).", "@dwyatte \r\n```python\r\nimport tensorflow as tf\r\n\r\nA_FEATURE_NAME = 'a feature'\r\nB_FEATURE_NAME = 'b_feature'\r\n\r\nclass MyModel(tf.keras.Model):\r\n\r\n    def call(self, inputs):\r\n        return inputs['A_FEATURE_NAME'] + inputs['B_FEATURE_NAME']\r\n\r\ninputs = {\r\n    'A_FEATURE_NAME': tf.constant([1.]),\r\n    'B_FEATURE_NAME': tf.constant([2.]),\r\n}\r\n\r\nmodel = MyModel()\r\noutputs = model(inputs)\r\n\r\nmodel.save('test')", "@bhack thanks for the workaround, but I still think the wrong error is raised here. `more than one argument to '__inference_signature_wrapper_48' was named 'a feature'` suggests that I am using the same the same input/op name twice and in fact, the source code mentions `# This should be unreachable, since concrete functions may not be generated with non-unique argument names`. Instead, it might be nicer if tensorflow checked for valid op names before the aforementioned error is reached in `tf.keras.Model.save` and raise a more informative error (something like `ValueError: 'a feature' is not a valid input name`).", "@dwyatte The problem with your original code is the invalid name is replaced to \"Placeholder\" and they will mismatch with e invalid input name \"a feature\".\r\n\r\nOn master you will receive a warning:\r\n\r\n`WARNING:absl:Function _wrapped_model contains input name(s) a feature with unsupported characters which will be renamed to placeholder in the SavedModel.`", "@bhack Thanks for the heads up, I was not aware there were changes in master that might handle address this. With `'2.5.0-dev20201127'` I do see the warning you mention, but I still receive an error which suggests the renaming is not happening:\r\n\r\n```\r\n...\r\nWARNING:absl:Function `_wrapped_model` contains input name(s) a feature with unsupported characters which will be renamed to placeholder in the SavedModel.\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-bc463538c686> in <module>\r\n     17 outputs = model(inputs)\r\n     18 \r\n---> 19 model.save('test')\r\n\r\n~/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\r\n   2046     # pylint: enable=line-too-long\r\n   2047     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n-> 2048                     signatures, options, save_traces)\r\n   2049 \r\n   2050   def save_weights(self,\r\n\r\n~/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\r\n    156   else:\r\n    157     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n--> 158                           signatures, options, save_traces)\r\n    159 \r\n    160 \r\n\r\n~/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options, save_traces)\r\n     94       with utils.keras_option_scope(save_traces):\r\n     95         saved_nodes, node_paths = save_lib.save_and_return_nodes(\r\n---> 96             model, filepath, signatures, options)\r\n     97 \r\n     98     # Save all metadata to a separate file in the SavedModel directory.\r\n\r\n~/venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py in save_and_return_nodes(obj, export_dir, signatures, options, raise_metadata_warning)\r\n   1077   _, exported_graph, object_saver, asset_info, saved_nodes, node_paths = (\r\n   1078       _build_meta_graph(obj, signatures, options, meta_graph_def,\r\n-> 1079                         raise_metadata_warning))\r\n   1080   saved_model.saved_model_schema_version = constants.SAVED_MODEL_SCHEMA_VERSION\r\n   1081 \r\n\r\n~/venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py in _build_meta_graph(obj, signatures, options, meta_graph_def, raise_metadata_warning)\r\n   1263   with save_context.save_context(options):\r\n   1264     return _build_meta_graph_impl(obj, signatures, options, meta_graph_def,\r\n-> 1265                                   raise_metadata_warning)\r\n\r\n~/venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py in _build_meta_graph_impl(obj, signatures, options, meta_graph_def, raise_metadata_warning)\r\n   1197   asset_info, exported_graph = _fill_meta_graph_def(meta_graph_def,\r\n   1198                                                     saveable_view, signatures,\r\n-> 1199                                                     options.namespace_whitelist)\r\n   1200   if options.function_aliases:\r\n   1201     function_aliases = meta_graph_def.meta_info_def.function_aliases\r\n\r\n~/venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py in _fill_meta_graph_def(meta_graph_def, saveable_view, signature_functions, namespace_whitelist)\r\n    699 \r\n    700   with exported_graph.as_default():\r\n--> 701     signatures = _generate_signatures(signature_functions, resource_map)\r\n    702     for concrete_function in saveable_view.concrete_functions:\r\n    703       concrete_function.add_to_graph()\r\n\r\n~/venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py in _generate_signatures(signature_functions, resource_map)\r\n    563     mapped_inputs, exterior_argument_placeholders = (\r\n    564         _map_function_arguments_to_created_inputs(argument_inputs,\r\n--> 565                                                   signature_key, function.name))\r\n    566     outputs = _call_function_with_mapped_captures(\r\n    567         function, mapped_inputs, resource_map)\r\n\r\n~/venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py in _map_function_arguments_to_created_inputs(function_arguments, signature_key, function_name)\r\n    504            \"tf.TensorSpec(..., name=...) to provide a name for a Tensor \"\r\n    505            \"input.\").format(signature_key, compat.as_str_any(function_name),\r\n--> 506                             user_input_name))\r\n    507     arg_placeholder = array_ops.placeholder(\r\n    508         shape=placeholder.shape,\r\n\r\nValueError: Got non-flat/non-unique argument names for SavedModel signature 'serving_default': more than one argument to '__inference_signature_wrapper_42' was named 'a feature'. Signatures have one Tensor per named input, so to have predictable names Python functions used to generate these signatures should avoid *args and Tensors in nested structures unless unique names are specified for each. Use tf.TensorSpec(..., name=...) to provide a name for a Tensor input.\r\n```", "The problem is that you are going to mismatch between `featuere a` and substituted name placeholder at\n\nhttps://github.com/tensorflow/tensorflow/blob/21eb35ad7c4c95343c872f68b7f298b66afc9b0e/tensorflow/python/saved_model/save.py#L494", "Reading the comment above this conditional makes me think it should somehow be rewritten to not capture cases where inputs are simply renamed on saving.\r\nhttps://github.com/tensorflow/tensorflow/blob/21eb35ad7c4c95343c872f68b7f298b66afc9b0e/tensorflow/python/saved_model/save.py#L494 \r\n```\r\n    # If the internal placeholders for a function have names which were\r\n    # uniquified by TensorFlow, then a single user-specified argument name\r\n    # must refer to multiple Tensors. The resulting signatures would be\r\n    # confusing to call. Instead, we throw an exception telling the user to\r\n    # specify explicit names.\r\n```\r\n(for what it's worth, if I have two invalid input names, they are still uniquely renamed `[<tf.Tensor 'Placeholder:0' shape=(None,) dtype=float32>, <tf.Tensor 'Placeholder_1:0' shape=(None,) dtype=float32>]`; perhaps this check is needed for other cases though)\r\n\r\nFurthermore, if I execute the following line which comes after the above conditional, I get the error I would expect:\r\nhttps://github.com/tensorflow/tensorflow/blob/21eb35ad7c4c95343c872f68b7f298b66afc9b0e/tensorflow/python/saved_model/save.py#L507\r\n```\r\narg_placeholder = array_ops.placeholder(shape=placeholder.shape, dtype=placeholder.dtype,name=\"{}_{}\".format(signature_key, user_input_name))\r\n*** ValueError: 'serving_default_a feature' is not a valid scope name\r\n```", "You could probably open a small PR adding an `and` condition for `Placeholder` name an see if the CI will pass tests.", "Still issue exists in 2.5 and Please find the gist [here](https://colab.research.google.com/drive/1RuYXTvTCWpUiUk9di594_xS0gk0XgVWD#scrollTo=Pm15FYLLUbbg).Thanks!", "@dwyatte \r\nThe value error reported does not appear anymore, please refer to the gist [here](https://colab.research.google.com/gist/Saduf2019/f912d30e597dbd83ec297624cfd54531/untitled646.ipynb)", "@Saduf2019 It is working cause you have put my workaround at https://github.com/tensorflow/tensorflow/issues/44984#issuecomment-731283178 in your gist.\r\n\r\nYou need to test instead the original example at https://github.com/tensorflow/tensorflow/issues/44984#issue-745796112\r\n", "Here's a diff with a test for this bug depending on how makes most sense to fix it (I just removed the offending code for now since it doesn't seem to be covered anyway).\r\n\r\nhttps://github.com/tensorflow/tensorflow/compare/master...dwyatte:44984_error_handling\r\n\r\nIt seems the behavior on `master` has changed a bit with some sanitization that converts whitespace to underscores, but only on its tensor info name, and not the name of the input itself. e.g., The following signature is valid\r\n\r\n```\r\n  inputs['a name'] tensor_info:\r\n      dtype: DT_FLOAT\r\n      shape: unknown_rank\r\n      name: serving_default_a_name:0\r\n```\r\n\r\nThis signature can still be called from python as follows (what is effectively in the simple test in the linked diff):\r\n\r\n```\r\nmodel = tf.keras.models.load_model(\"saved_model\")\r\nfeed_dict = {\"a name\": tf.constant(2.)}\r\nmodel.signatures[\"serving_default\"](**feed_dict)\r\n```\r\n\r\nIMO this is a bit of a hack since it exploits invalid identifiers by using **kwargs, so perhaps the name of the input itself should also be sanitized (\"a name\" -> \"a_name\") or by bubbling up the naming changes that trigger `WARNING:absl:Function _wrapped_model contains input name(s)...with unsupported characters...which will be renamed to...in the SavedModel`.", "I got this error when I had a feature named \"_1_day_user_global_dwell_avg\". Do leading underscores also fall under the same error?"]}, {"number": 44980, "title": "Changing self properties does not trigger retracing", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab and Mac\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0-dev20201118\r\n- Python version: 3.6.9 (default, Oct  8 2020, 12:12:24) \r\n\r\n**Describe the current behavior**\r\n\r\nEager and graph mode give different results. This gives rise to bugs very difficult to detect: one just sees a different MSE when switching to graph mode!\r\n\r\n**Describe the expected behavior**\r\n\r\nBoth eager and graph mode should give the same results or raise an error. If `tf.function` uses a variable through `self`, it should also be retraced using the same policy as with input arguments. Else, using properties of `self` should be disallowed in `tf.function`s.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nColab: [here](https://colab.research.google.com/drive/1SCcqXjBx8bivziz5592eW60UKTN9cWWX?usp=sharing).\r\n\r\n````python\r\n\r\nimport tensorflow as tf\r\n\r\nclass Network:\r\n    def call(self, x):\r\n        self.weights = tf.zeros((4,5), dtype=tf.double)\r\n        self.encoder_graph(x)\r\n        self.weights = tf.zeros((2,3), dtype=tf.double)\r\n\r\n        return (self.encoder_graph(x).shape == self.encoder_eager(x).shape)\r\n\r\n    @tf.function\r\n    def encoder_graph(self, x):\r\n        return self.weights \r\n        \r\n    def encoder_eager(self, x):\r\n        return self.weights\r\n\r\nN = Network()\r\n\r\ntf.config.run_functions_eagerly(False)\r\nprint(\"Graph mode: \", N.call(tf.zeros((1,1))))\r\ntf.config.run_functions_eagerly(True)\r\nprint(\"Eager mode: \", N.call(tf.zeros((1,1))))\r\n\r\n````\r\n\r\n**Other info / logs** \r\n\r\nOutput:\r\n\r\n````\r\nGraph mode:  False\r\nEager mode:  True\r\n````", "comments": ["> If you need to force retracing, create a new Function. Separate Function objects are guaranteed not to share traces.\r\n", "Thanks, but that does not solve the problem that the eager mode silently behaves differently from the graph mode.\r\n", "/cc @mdanatg is this similar to https://github.com/tensorflow/tensorflow/issues/38539#issuecomment-618355110", "I agree, this behavior is surprising, confusing and should be fixed. In general, it can be expensive to monitor all objects that the function may depend on. But `self` is a special type of object that should be treated specially.\r\n\r\nNote that this isn't limited to `self`. Only the direct arguments are monitored, and everything else in the function's closure is ignored. For example:\r\n\r\n```\r\nx = tf.constant(1)\r\n\r\n@tf.function\r\ndef f():\r\n  return x\r\n\r\nprint(f())\r\n\r\nx = tf.constant(2)\r\n\r\nprint(f())\r\n```\r\n\r\n```\r\ntf.Tensor(1, shape=(), dtype=int32)\r\ntf.Tensor(1, shape=(), dtype=int32)\r\n```\r\n\r\nMeanwhile, I recommend explicitly passing things as arguments as much as possible (so calling `self.encoder_graph(self.weights, x)`, is safer, as ugly as it looks).\r\n\r\nAlso note that this isn't a problem for `tf.Variable`s because those are changed using a TF op, rather than with a Python assignment (though they have other kinds of limitations):\r\n\r\n```\r\nx = tf.Variable(1)\r\n\r\n@tf.function\r\ndef f():\r\n  return x\r\n\r\nprint(f())\r\n\r\nx.assign(2)\r\n\r\nprint(f())\r\n```\r\n\r\n```\r\ntf.Tensor(1, shape=(), dtype=int32)\r\ntf.Tensor(2, shape=(), dtype=int32)\r\n```", "I have tried in colab with TF version 2.4, nightly version(`2.5.0-dev20201118`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/5d41b5adc15e732d64b017bc8128b362/untitled523.ipynb). Thanks!", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210530, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/50e6ad46355e0ddd01594a6bbe1b94d1/44980.ipynb). Thanks!", "Hi @fachu000 ! I think the results will be the same if eager execution is not disabled or enabled during the process or by printing both eager or graph mode values in the same block . Attaching [gist](https://colab.sandbox.google.com/gist/mohantym/814eb5bd340812636a13b87152db5d5d/44980.ipynb#scrollTo=2uGW9C6x2PTh) for reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 44950, "title": "[Intel MKL] Fuse group of primitive ops for batch normalization to Fu\u2026", "body": "In legacy models, the batch normalization is performed via a group of individual ops instead of a single FusedBatchNorm op. This PR adds fusion support in optimize_for_inference tool to identify a pattern of batch normalization via group of ops in the graph and replace those with single FusedBatchNorm. In inference use case, a FusedBatchNorm op can be further folded with convolution op to reduce the computation and thus increase the performance.\r\nExample of graph transformation via optimize_for_inference tool with change in this PR is shown below. The original subgraph is part of the DenseNet-169. The performance after applying optimize_for_inference is about 1.8x comparing to original graph when measured on CPU for DenseNet-169.\r\n\r\nOriginal subgraph with group of ops made up to batch normalization:\r\n![image](https://user-images.githubusercontent.com/42156420/99428326-57aec900-28bb-11eb-9fa0-4c515f5a4be1.png)\r\n\r\nSubgraph after optimize_for_inference tool, which fused individual batch normalization ops to FusedBatchNorm, then followed by folding FusedBatchNorm with convolution ops\r\n![image](https://user-images.githubusercontent.com/42156420/99428462-875dd100-28bb-11eb-9686-19a274b66219.png)\r\n\r\nAnother example is from GoogleNet-v3, in this model, the gamma factor is 1, multiplication with gamma is not present comparing to the batch normalization ops in DenseNet-169. The performance gain after applying optimze_for_inference is about 1.6x comparing to original graph when measured on CPU for GoogleNet-v3.\r\n\r\nOriginal subgraph with a group of primitive ops made up to batch normalization when gamma is 1:\r\n![image](https://user-images.githubusercontent.com/42156420/104404111-c81a4500-550e-11eb-9e4f-bc06cebc2ba3.png)\r\n\r\nSubgraph after applying optimize_for_inference tool.\r\n![image](https://user-images.githubusercontent.com/42156420/104404188-f861e380-550e-11eb-9c97-8f5cd492ef68.png)\r\n", "comments": ["@penpornk - thanks for your previous comments. I pushed a new commit to: \r\n1) address your comments;\r\n2) add check on the rank of constant value's shape in the pattern matching, and only allow 1D for fusion candidate;\r\n3) for allowing communicative in the pattern, since the breakdown pattern of batch normalization was done by the framework, it is not necessary to add the extra checks to handle input order swapping case. Hope this is ok with you.\r\nThanks for reviewing the changes.", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "This PR is brought up to the latest baseline, and also updated test case on math op function usage.", "@penpornk  Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!"]}, {"number": 44936, "title": "Performance issue with 3070", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.10\r\n- TensorFlow installed from (source or binary): pip install tf-nightly-gpu\r\n- TensorFlow version (use command below): tf-nightly-gpu==2.5.0.dev20201117\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: CUDA 11.1.0 / cuDNN 8.0.4\r\n- GPU model and memory: 3070 driver 455.38\r\n\r\nHi, \r\n\r\nI recently bought an RTX 3070 and, after some struggling I managed to install tf-nightly-gpu with all dependencies (following issue #43947), but the performances compared to my previous 1050 are way worse.\r\n\r\nIs this something relating CUDA and/or NVIDIA drivers (for which I must just wait) or there is something I need to do?\r\n\r\nI also noticed that, in nvidia-smi, the percentage of Volatile GPU-Util is very low\r\n", "comments": ["Can you run a python with:\r\n```\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\nprint(\"Num GPUs:\", len(physical_devices))\r\n```", "Yes, this is the result:\r\n```2020-11-17 13:40:33.927945: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-17 13:40:33.928170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-17 13:40:33.928747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:07:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.725GHz coreCount: 46 deviceMemorySize: 7.76GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-11-17 13:40:33.928780: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-17 13:40:33.928808: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-17 13:40:33.928823: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-17 13:40:33.928838: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-11-17 13:40:33.928852: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-11-17 13:40:33.928867: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2020-11-17 13:40:33.928881: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-11-17 13:40:33.928896: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-17 13:40:33.928966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-17 13:40:33.929509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-17 13:40:33.929994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\nNum GPUs: 1\r\n```\r\n\r\nI tried also TF 2.0.0 with CUDA 10 (installed via conda), but I have the same performance issue", "Do you have a specific a very, very minimal code snippet to reproduce the performance regression issue with this GPU card?", "No actually I can't, it's a reinforcement learning project with a custom environment so I can't provide minimal code snippet", "Ok. Can you try to profile you project with https://www.tensorflow.org/guide/profiler?", "I tried but the profiler doesn't seem to output anything in overviw page, it says:\r\n`No step marker observed and hence the step time is unknown. This may happen if (1) training steps are not instrumented (e.g., if you are not using Keras) or (2) the profiling duration is shorter than the step time. For (1), you need to add step instrumentation; for (2), you may try to profile longer. `\r\n\r\nI'm using a reinforcement learning library based on tensorflow, so I don't have much access to the training code.\r\n\r\nHowever, I do have something in tensorflow_stats page, can it be useful?\r\n\r\n", "If your code highly depend on a large third party library I suggest you to open a ticket like this on the third party library repository (if opensource).", "I already did and they say to me that is a tensorflow problem. I tried on running it on CPU and it goes way faster than it goes on GPU.\r\n\r\nActually, most of the time I use the profiler, it gives me the error:\r\n\r\n`Segmentation fault (core dumped)`\r\n\r\nCan it be related to my problem?", "> I already did and they say to me that is a tensorflow problem. \r\n\r\nJust for reference can you link the ticket here?", "No I can't sorry, I personally know the author and I made a discussion in private.\r\n\r\nAnyway, I run the MNIST test of https://github.com/tensorflow/models and it raise the same error: \r\n\r\n`Segmentation fault (core dumped)`", "Are you in a conda environment? Cause we don't officially support conda here.", "Yes I'm in a conda environment, but I can make a try without it", "Tried it without conda, same result and same error.\r\n\r\nIf I run the same MNIST example in CPU it works correctly.\r\n\r\nEDIT: I tried to run the same example removing the tensorboard callback from the mnist_main.py file, now the error is gone but the test on GPU has the exact same performance as CPU, and I don't think it is right. Plus, in nvidia-smi I always see that Volatile gpu-util is always around 0%, while with the previous GPU it went quite high.", "~~I'm in a similar boat, I spent about 6 hours setting up TF just to run on 3070. However, don't quote me on this but it's possible that we both are suffering from a CPU bottleneck. My CPU core is at 120% while GPU usage jumps between 0% and ~30%~~. ~~Whereas 1070 I was using runs much faster and stays at 90%+ GPU usage.~~\r\n\r\nI can confirm that at least in my case it was CPU/IO bottleneck making it run slower than on a weaker card because it had to constantly go back and forth between GPU and CPU.", "I Have same problem with RTX 3070 after installing tf-nightly-gpu", "@realratchet how can I test if I have a CPU bottleneck? Which CPU are you using?", "@SestoAle I'm using ryzen 5 3600, check CPU usage. CPython is effectively single threaded so it will likely have a spike of 100+% CPU utilization since it eats up entire core. What I did to confirm this was instead of having data being read from IO and then augmented I just passed empty dataset of correct shape (I'm using custom data generator for my dataset). The GPU utilization and speed shot through the roof once I've done that. What I did next was precache the data into regular RAM so it doesn't have to read and augment images on every batch so that there's little work for CPU to do when TF requests for new batch data.", "@realratchet I have the same CPU as yours, so I probably have the same bottleneck.\r\n\r\nAnyway, since I'm using a third party library I don't have much access to how the data are passed into GPU. Can I resolve the issue just changing the CPU?\r\n\r\nit seems strange to me though, I have another workstation with the same CPU and a 2080 SUPER GPU and I don't have this problem. Why I have the bottleneck only with 3070, that should be quite the same of a 2080 in terms of performance?", "I believe second gen cores are a bit faster than first gen, also 30 series has more of them. So while gaming benchmarks might be similar DNN can probably differ. Another issue maybe you using mechanical drive on this machine vs SSD on the this one to store your dataset? I have my data stored in NVMe SSD, so there was no difference in both cases when it comes to IO performance.", "No I have an SSD (not NVMe), and I don't have dataset stored into SSD because I'm using reinforcement learning, so I generate data on the fly. Related to this, could it be a RAM problem (my RAM is quite old, 16GB with 2133 Hz) ? I just checked the CPU usage in the MNIST example of tensorflow and actually it is much higher when I run it in GPU than in CPU..\r\n\r\nIt's quite annoying though, which CPU or RAM I need to get in order to be sure I will have some high performance? ", "It's probably still possible to precache dataset onto disk or RAM before each epoch. Or even during epoch on a child process (this is what I'm planning to do). There's still some kind of interface between that library and TF I assume.\r\n\r\nI don't know your exact use case but if data is generated on the fly and if you're using DDR4 already It's unlikely that changing RAM chips even with low CL and high frequency will give non-negligible performance boost. Generally RAM doesn't differentiate between those two features and the type of heatsink that's attached to it since there's only two or three RAM manufacturers in the world.\r\n\r\nNow swapping out CPU would likely increase the performance. Although I'm not sure how your dataset is generated. If it's single threaded generation you would need a CPU that has higher single thread clock speeds. If you data generator properly manages to utilize multithreading then anything zen3 will see an improvement too most likely.", "Thank you @realratchet  for all your answers.\r\n\r\nHowever, there is still something that bothers me: why does the profiler not work (giving `Segmentation fault (core dumped)` error)? Why the GPU usage is ALWAYS at 0% ~ 3%, never going higher than 3%?\r\n\r\nAnd I repeat that this happens also with the MNIST example of the official tensorflow model, not only with my code. ", "That I cannot tell you, I remember when I was initially setting up I did get some segfaults but that was due to misconfiguration with cuda, although I don't remember what exactly was wrong and what I changed.", "Ok, downgrading CUDA from 11.1 to 11.0 made me able to use the profiler for the MNIST example.\r\n\r\n@bhack here's the result\r\n\r\n![Screenshot from 2020-11-19 19-09-45](https://user-images.githubusercontent.com/28829674/99706212-06f5c680-2a9b-11eb-8211-2cfb917bc18d.png)\r\n\r\nThe thing I notice is that 'Kernel Launch Time' is extremely high. Can this be related to the CPU bottleneck? \r\n", "Hey guys. Checking in here to pitch in to confirm I am facing a similar problem, as I was trying to get my new RTX 3070 to work with TF.\r\n\r\nSymptoms: \r\n- A simple MNIST example running on GPU takes way too long loading until it starts running the epochs (~2 minutes?), while with CPU I don't get this long load time. \r\n- When I monitor with `htop` and `nvidia-smi` in the load time, a single core would be busy running ~100% on the respective python process. Memory occupied increases at this time. Dataset being processed likely.\r\n- Once the first epoch starts running, `nvidia-smi` GPU-util runs only around 5% while single CPU core continues to be busy ~100%. \r\n- I havn't seen any seg fault errors.\r\n- On the same script ran on CPU (`os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"`), python process CPU utilization is around 130% (seems at least more than one core).\r\n\r\nRelevant system specs I have been running:\r\n- Kernel/Distro: Linux 5.9.10 / Manjaro Linux 20.2\r\n- CPU: Ryzen 9 3900X\r\n- GPU: Geforce RTX 3070, 8GB memomry (EVGA GeForce RTX 3070 FTW3 Ultra Gaming)\r\n- nvme SSD\r\n- NVidia driver version: 455.45.01\r\n- tensorflow-gpu version: 2.2.0 (`pip install tensorflow-gpu==2.2.0` on conda environment)\r\n- CUDA & CUDNN version: 10.1 & 7.6 (`cudnn7-cuda10.1` package from [AUR](https://aur.archlinux.org/packages/cudnn7-cuda10.1))\r\n\r\n<details>\r\n<summary>Output of mnist training example (click to expand):</summary>\r\n\r\n```\r\n$ cd /home/jonejkim/Repos/tf2 ; /usr/bin/env /home/jonejkim/.conda/envs/tf2/bin/python /home/jonejkim/.vscode/extensions/ms-python.python-2020.11.371526539/pythonFiles/lib/python/debugpy/launcher 40387 -- /home/jonejkim/Repos/tf2/mnist.py \r\n\r\n2020-11-23 12:37:25.227454: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-11-23 12:37:25.933069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-23 12:37:25.935159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:24:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.815GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-11-23 12:37:25.935293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-11-23 12:37:25.936116: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-11-23 12:37:25.937002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-11-23 12:37:25.937143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-11-23 12:37:25.938046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-11-23 12:37:25.938583: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-11-23 12:37:25.940534: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-11-23 12:37:25.940612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-23 12:37:25.944300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-23 12:37:25.944657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-11-23 12:37:25.945283: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-11-23 12:37:25.950014: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3801860000 Hz\r\n2020-11-23 12:37:25.950565: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f5a64000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-11-23 12:37:25.950576: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-11-23 12:37:26.000455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-23 12:37:26.000935: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563c287508a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-11-23 12:37:26.000956: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3070, Compute Capability 8.6\r\n2020-11-23 12:37:26.001139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-23 12:37:26.001702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:24:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.815GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-11-23 12:37:26.001742: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-11-23 12:37:26.001760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-11-23 12:37:26.001777: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-11-23 12:37:26.001793: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-11-23 12:37:26.001809: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-11-23 12:37:26.001825: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-11-23 12:37:26.001841: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-11-23 12:37:26.001908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-23 12:37:26.002498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-23 12:37:26.003029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-11-23 12:37:26.003064: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-11-23 12:37:26.003768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-23 12:37:26.003785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-11-23 12:37:26.003794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-11-23 12:37:26.003907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-23 12:37:26.004500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-23 12:37:26.005061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7358 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:24:00.0, compute capability: 8.6)\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 784)]             0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 32)                25120     \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 128)               4224      \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 128)               16512     \r\n_________________________________________________________________\r\ndense_3 (Dense)              (None, 128)               16512     \r\n_________________________________________________________________\r\ndense_4 (Dense)              (None, 128)               16512     \r\n_________________________________________________________________\r\ndense_5 (Dense)              (None, 10)                1290      \r\n=================================================================\r\nTotal params: 80,170\r\nTrainable params: 80,170\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\nepoch # 0\r\n2020-11-23 12:38:31.882397: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\nLoss & accuracy at step 0: 2.302, 0.078\r\nLoss & accuracy at step 100: 1.791, 0.647\r\nLoss & accuracy at step 200: 1.726, 0.698\r\nLoss & accuracy at step 300: 1.657, 0.716\r\nLoss & accuracy at step 400: 1.764, 0.727\r\nepoch # 1\r\nLoss & accuracy at step 0: 1.675, 0.740\r\n...\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>MNIST python code I was using (click to expand): </summary>\r\n\r\n```Python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\r\nfrom tensorflow.keras.datasets import mnist\r\n\r\nepochs = 20\r\nbatch_size = 128\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\nx_train = x_train[:].reshape(60000, 28*28).astype('float32') / 255\r\ny_train = tf.one_hot(y_train, 10)\r\ndataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\ndataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\r\n\r\ninputs = tf.keras.Input(shape=(28*28,))\r\nx = Dense(32, activation=tf.nn.relu)(inputs)\r\nx = Dense(128, activation=tf.nn.relu)(x)\r\nx = Dense(128, activation=tf.nn.relu)(x)\r\nx = Dense(128, activation=tf.nn.relu)(x)\r\nx = Dense(128, activation=tf.nn.relu)(x)\r\noutputs = Dense(10, activation=tf.nn.softmax)(x)\r\n\r\nmodel = tf.keras.Model(inputs, outputs)\r\nprint(model.summary())\r\nloss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\r\nmetric = tf.keras.metrics.CategoricalAccuracy()\r\noptimizer = tf.keras.optimizers.Adam()\r\n\r\nfor epoch in range(epochs):\r\n    print(f'epoch # {epoch}')\r\n    for step, (x,y) in enumerate(dataset):\r\n        with tf.GradientTape() as tape:\r\n            logits = model(x)\r\n            loss_value = loss(y, logits)\r\n\r\n        gradients = tape.gradient(loss_value, model.trainable_weights)\r\n        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\r\n        metric.update_state(y, logits)\r\n\r\n        # Logging.\r\n        if step % 100 == 0:\r\n            print('Loss & accuracy at step %d: %.3f, %.3f' % (step, loss_value, metric.result().numpy()))\r\n```\r\n</details>\r\n\r\nComment: \r\n- Am I seeing a CPU bottleneck something related to AMD CPU architecture compatibility with Nvidia/TF? \r\n- Or this is something related to the new RTX 3000 series? \r\n- I am only used to using Intel CPU + Nvidia GPU combination for TF, and it is my first time trying AMD CPU + Nvidia GPU; so I don't have too much experience in how AMD + Nvidia combinations have worked for others in the past.", "Afaik you can't even run Cuda 10 with 30 series and need to use Cuda 11 with nightly tf build.", "An Update: I just got TF working with [Lambda Stack](https://lambdalabs.com/blog/set-up-a-tensorflow-gpu-docker-container-using-lambda-stack-dockerfile/) Ubuntu 20.04 based docker container on the same machine/system I mentioned above. It works flawlessly without hanging so far, so I think it would mostly likely be some setup/configuration issues I have not taken care of properly.\r\n\r\nExample:\r\n\r\n```bash\r\n# build lambda stack docker image\r\ndocker build -t lambda-stack:20.04 -f Dockerfile.focal git://github.com/lambdal/lambda-stack-dockerfiles.git\r\n\r\n# run with MNIST TF python script with GPU\r\ndocker run -it --gpus 1 --rm --name lambda-stack -v ${PWD}/mnist.py:/mnist.py lambda-stack:20.04 python /mnist.py\r\n```\r\nI will look into local setup in time and update here if anything.", "I have the exactly same issue with Nvidia 3080, where the overhead to start the epoch takes a long time. I'm on Archlinux with tensorflow-gpu installed by conda (miniconda). \r\n- Nvidia Driver 460.32.03-2\r\n- Cudnn (from conda) 7.6.5\r\n- Cudatoolkit (from conda) 10.1.243\r\n\r\nOne thing I noticed is that Nvidia now needs CAP_SYS_ADMIN to run profiler. Running as root does not solve the issue above though.\r\n", "> I have the exactly same issue with Nvidia 3080, where the overhead to start the epoch takes a long time. I'm on Archlinux with tensorflow-gpu installed by conda (miniconda).\r\n> \r\n>     * Nvidia Driver 460.32.03-2\r\n> \r\n>     * Cudnn (from conda) 7.6.5\r\n> \r\n>     * Cudatoolkit (from conda) 10.1.243\r\n> \r\n> \r\n> One thing I noticed is that Nvidia now needs CAP_SYS_ADMIN to run profiler. Running as root does not solve the issue above though.\r\n\r\nSome time (1~1.5 months) has passed now since my [last comment above](https://github.com/tensorflow/tensorflow/issues/44936#issuecomment-732324134), and now I am finding myself that I can run it on Manjaro Linux (Arch Linux derivative) without performance issue and natively without using lambda-stack (which is Ubuntu based) docker. \r\n\r\nI suspect the previous problem I was experiencing had to do some RTX 3k series's range of supported CUDA/cuDNN versions and how each TF version requires (or tested stability) different CUDA/cuDNN versions according to the [table of officially tested tensorflow build configurations](https://www.tensorflow.org/install/source). \r\n\r\nOn top of that information, according to [this reddit post](https://www.reddit.com/r/MachineLearning/comments/jepuob/d_it_seems_that_rtx_3080_has_a_issue_with_cuda_101/) 3080 has a compute capability of 8.6 which is only supported on CUDA 11.1 or above. And in my previous case above, I was having unsuccessful attempts with tensorflow-gpu==2.2.0 or 2.3.0 which were the latest version at that time, which according to the table above were tested build compatible with the cudnn 7 and cuda 10.1 combination.\r\n\r\nBut question still lingers on why lambda-stack docker was running without the same problem back then. I honestly didn't bother further looking into it and just happy that it works natively now. It's something that could be looked into, but since it's a new GPU, I'm not sure if it's worth it as things will only get further stabilized over time.\n\n\\+ However, I'm also not clear yet on what this means in terms of backward compatibility with older TF versions.\r\n\r\nFor your reference, my updated working system specs from 1.5 months ago I am using now are:\r\n\r\n- CPU: Ryzen 3900X (same)\r\n- GPU: ~~EVGA GeForce RTX 3070 FTW3 Ultra Gaming~~ -> MSI GeForce RTX 3080 Ventus 3X 10G OC 10GB\r\n- NVidia driver version: 455.45.01 (same)\r\n\r\n- Linux Kernel: ~~5.9.10~~ -> 5.10.2-2\r\n- Distro: Manjaro Linux 20.2 (same)\r\n\r\nAUR Packages: \r\n- ~~cudnn7-cuda10.1~~ -> cudnn8-cuda11.0\r\n- miniconda3 (same)\r\n\r\nConda/Pip packages:\r\n- ~~`pip install tensorflow-gpu==2.2.0`~~ or ~~2.3.0~~ -> `pip install tensorflow-gpu==2.4.0`\r\n\r\nHope this helps. Cheers.", "@jonejkim Thanks a lot for the tips! I'll try to have force conda install cuda11 & cudnn8 to see how it works.", "Found this, \r\n\r\n\r\n1. Talks about benchmarks and how he was able install docker image and make it work\r\nhttps://fsymbols.com/3080-3090-benchmarks/ \r\n2. https://www.youtube.com/watch?v=VVj8EvgVjLo&t=52s&ab_channel=NerdyRodent\r\n\r\nSame as below:  Just change the image to tensorflow v2, you can find in below nvidia link.\r\nhttps://ngc.nvidia.com/catalog/containers/nvidia:tensorflow/tags\r\n\r\ni.e. 20.12-tf2-py3 Image\r\n\r\nI tried with 1.x worked for me. Not happy that it is not natively/directly working with Windows/Linux without docker. Very sad. But atleast I can use the cards for training now. \r\n\r\nHope this helps you guys. And if you are happy with info provided by these guys, give them a thumbs up.\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Adding to my last comment, I was able to install Tensorflow Gpu on windows, but noticed that only at the first run. I get slow speeds. \r\n\r\nPlease refer the [link](https://www.tensorflow.org/install/source_windows). Check the tensorflow table and only install the version of cudann or cuda toolkit to get it working. \r\n\r\nFyi. I'm using a 3070  Nvidia", "tf-nightly-gpu works now with my 3080 with cudatoolkit=11.0 and cudnn=8.0 from conda. However, it seems tf-nightly-gpu does not like cuda 11.1 complaining it could not open libcusolver.", "RTX 3070\r\nshowing error while running CN algorithms", "Some people here say about CPU bottleneck. That is not the case imo.\r\n\r\nI had performance issues since I replaced my 1080ti with a 3090, keeping CPU the same i7-8700K.\r\n\r\nFollowing this post: https://github.com/tensorflow/tensorflow/issues/47875\r\nI ran the script that the user had posted and I got almost the same times. So indeed it is a performance issue. \r\n\r\nI also ran the results using CUDA 11.0, 11.1, and 11.2 with 8.0.5 and 8.1.0 cuDNN. Also, using TF 2.4.1 and TF nightly 2.6.0. Regardless the built, the performance of 3090 is excruciatingly low. \r\nI ran the code also using intel optimised tensorflow (2.4.0) on 8700K; then tensorflow 2.4.1 on MacBook Pro 2017 and mac optimised tensorflow on the same macbook. Everything was on Python 3.8.\r\n\r\nBest performance by far was either with intel-tensorflow MKL on the 8700K or on GPU with 1080ti. \r\n\r\nOne weird thing that happened is when I run the script using intel-tensorflow 2.4.0 MKL, it looks that GPU is utilised too but without using any memory. Whereas, when I use TF-GPU (11.2 and 8.1.0) and TF-nightly-GPU, the GPU unit is not really having a workload (apart from the standard memory allocation).\r\n\r\nTensorflow MKL; 2.4.0:\r\n![Tensorflow MKL; 2.4.0](https://user-images.githubusercontent.com/21122937/113523051-7d541a80-959d-11eb-8202-1b24d034b0cf.png)\r\n\r\nTensorflow GPU; 2.4.1, 11.2, 8.1.0:\r\n![Tensorflow GPU; 2.4.1, 11.2, 8.1.0](https://user-images.githubusercontent.com/21122937/113523073-9ceb4300-959d-11eb-8beb-0d2a378453cf.png)\r\n\r\nTensorflow nightly GPU; 2.6.0, 11.2, 8.1.0:\r\n![Tensorflow nightly GPU; 2.6.0, 11.2, 8.1.0](https://user-images.githubusercontent.com/21122937/113523095-b8564e00-959d-11eb-872d-547cb11c49a5.png)\r\n", "@SestoAle, @DolanDack, Have you solved the problem? I have the same behavior exhihited with a 1080 GTX card", "@MuSamiNaf no, I have just stopped trying at this point.\r\n\r\n", "same issue here.. i have Ryzen 9 3900x and RTX 3070.. python starts and then freezes for 10 minutes before executing something and then suddenly starts working again with good FPS in YoloV4 implementation with Tensorflow backend!\r\n\r\nThis is stupid....\r\n![erroe](https://user-images.githubusercontent.com/40795304/131869711-29870461-cfeb-47b5-9cbf-d9a519851640.PNG)\r\n", "Isn't it possible that this issue is due to a different version of cuDNN you are using?\r\nThe performance was different between cuDDN7 and 8, so I tried to profile it.\r\n\r\nOS: Windows 10 21H2\r\nPython: Python 3.8.10\r\nGPU: Geforce RTX 2070 note\r\nNVIDIA Driver: 510.06\r\nModel: ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03\r\n\r\nProcessing time per step\r\n\r\n|TF2.4.0<br>CUDA11.0<br>cuDNN8.0.5|TF2.3.4<br>CUDA10.1<br>cuDNN7.6.5|\r\n| --: | --: |\r\n| 106.67 msec |52.64 msec|\r\n\r\n\r\nI checked the processing time by changing only the versions of Tensorflow, CUDA, and cuDNN without changing the source code and model.\r\nIn order to reduce the difference between versions, I chose the version of Tensorflow, which is the boundary between cuDNN7 and cuDNN8.\r\n\r\nThe processing time was twice as different.\r\nI used a profiler to compare why this happens, but it's too complicated to figure out where the cause is.\r\nI'm checking it with keras, but it seems that there is a similar tendency.\r\n\r\nIs there anyone who can analyze the profile data to clarify the cause?\r\n![cuda10 1](https://user-images.githubusercontent.com/62313584/153318053-6b96e7b7-689d-4434-8db1-0ea2e3c67cbf.png)\r\n![cuda11 0](https://user-images.githubusercontent.com/62313584/153318060-1091e64c-72e2-4c43-85f6-b07e8f8d5b62.png)\r\n\r\n[profile.zip](https://github.com/tensorflow/tensorflow/files/8029844/profile.zip)\r\n\\* Since the profile results were uneven and constant results could not be obtained, the fastest results will be uploaded for each.\r\n\r\n```python\r\nimport time\r\n\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# TensorFlow 1 Detection Model Zoo\r\n# http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz\r\nmodel_file = \"frozen_inference_graph.pb\"\r\n\r\ntf.compat.v1.reset_default_graph()\r\nwith tf.io.gfile.GFile(model_file, 'rb') as f:\r\n    graph_def = tf.compat.v1.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n\r\nconfig = tf.compat.v1.ConfigProto(\r\n    gpu_options=tf.compat.v1.GPUOptions(\r\n        allow_growth=True\r\n    ),\r\n)\r\nsession = tf.compat.v1.Session(config=config)\r\ntf.import_graph_def(graph_def, name='')\r\n\r\n# https://farm4.staticflickr.com/3643/3507658251_5b250ea5b9_z.jpg\r\nN = 32\r\nimg = cv2.cvtColor(cv2.resize(cv2.imread(\"cat.jpg\"), (640, 640)), cv2.COLOR_BGR2RGB)\r\ntest_one_img = np.expand_dims(img, 0)\r\ntest_n_imgs = np.repeat(test_one_img, N, axis=0)\r\n\r\n\r\ndef predict(title, input_imgs):\r\n    start = time.time()\r\n    n = input_imgs.shape[0]\r\n    classes, scores = session.run([session.graph.get_tensor_by_name('detection_classes:0'),\r\n                                   session.graph.get_tensor_by_name('detection_scores:0')],\r\n                                  feed_dict={'image_tensor:0': input_imgs})\r\n    print(\"[%s] num_imgs=%d, predict=%.2f msec\" % (title, n, (time.time() - start) * 1000 / n))\r\n    # print(\"classes=%s\" % classes[scores > .5])\r\n\r\n\r\n# Skip profile due to memory allocation or warm-up.\r\npredict(\"skip first\", test_n_imgs)\r\n\r\n# Check performance\r\nfor i in range(10):\r\n    predict(f\"check performance %d\" % (i + 1), test_n_imgs)\r\n\r\n# Profile multi step\r\nfor i in range(10):\r\n    tf.profiler.experimental.start(\"logs/ssd_resnet50_fpn_cuda10.1\")\r\n    predict(f\"profile {N}-steps\", test_n_imgs)\r\n    tf.profiler.experimental.stop()\r\n    time.sleep(1)  # Prevent overlap\r\n\r\n# Profile single step\r\nfor i in range(10):\r\n    tf.profiler.experimental.start(\"logs/ssd_resnet50_fpn_cuda10.1\")\r\n    predict(\"profile single-step\", test_one_img)\r\n    tf.profiler.experimental.stop()\r\n    time.sleep(1)\r\n\r\n\r\n```\r\n\r\nRun on CUDA 10.1 (cuDNN7.6.5) and CUDA 11.0 (cuDNN8.0.5)\r\n```\r\n# CUDA 10.1 cuDNN7.6.5\r\n[skip first] num_imgs=32, predict=329.48 msec\r\n[check performance 1] num_imgs=32, predict=51.92 msec\r\n[check performance 2] num_imgs=32, predict=52.16 msec\r\n[check performance 3] num_imgs=32, predict=53.82 msec\r\n[check performance 4] num_imgs=32, predict=53.37 msec\r\n[check performance 5] num_imgs=32, predict=51.13 msec\r\n[check performance 6] num_imgs=32, predict=52.66 msec\r\n[check performance 7] num_imgs=32, predict=52.79 msec\r\n[check performance 8] num_imgs=32, predict=53.44 msec\r\n[check performance 9] num_imgs=32, predict=51.87 msec\r\n[check performance 10] num_imgs=32, predict=53.22 msec\r\n\r\n# CUDA 11.0 cuDNN8.0.5\r\n[skip first] num_imgs=32, predict=491.74 msec\r\n[check performance 1] num_imgs=32, predict=107.72 msec\r\n[check performance 2] num_imgs=32, predict=107.92 msec\r\n[check performance 3] num_imgs=32, predict=108.17 msec\r\n[check performance 4] num_imgs=32, predict=105.07 msec\r\n[check performance 5] num_imgs=32, predict=105.65 msec\r\n[check performance 6] num_imgs=32, predict=107.57 msec\r\n[check performance 7] num_imgs=32, predict=108.58 msec\r\n[check performance 8] num_imgs=32, predict=104.11 msec\r\n[check performance 9] num_imgs=32, predict=104.83 msec\r\n[check performance 10] num_imgs=32, predict=107.07 msec\r\n...\r\n```\r\n\r\nProfiles such as \"Kernal stats\" could not be performed normally in CUDA 10.1, so by renaming and placing cupti64_110.dll in CUDA 11.0, it became possible to profile normally.\r\n```DOS\r\n> copy \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\extras\\CUPTI\\lib64\\cupti64_110.dll\" \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\extras\\CUPTI\\lib64\\cupti64_101.dll\" \r\n```\r\n\r\n"]}, {"number": 44933, "title": "Dataset.unbatch() sets cardinality to -2 when batch remainder is not explicitly dropped", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.8.5\r\n\r\n\r\n**Describe the current behavior**\r\nUsing `tf.data.experimental.cardinality()` yields `-2` (unknown), when batching (without explicitly dropping the remainder) and unbatching a dataset.\r\n\r\n**Describe the expected behavior**\r\n`tf.data.experimental.cardinality()` yields the same result as `len(list(tf.data.Dataset.as_numpy_iterator()))`.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\n    import tensorflow as tf\r\n\r\n    print(tf.version.VERSION)  # 2.3.1\r\n    ds = tf.data.Dataset.range(10)  # shape=()\r\n    ds = ds.batch(2)  # shape=(2,)\r\n    print(tf.data.experimental.cardinality(ds))  # 5\r\n    ds = ds.unbatch()  # shape=()\r\n    print(len(list(ds.as_numpy_iterator()))) # 10\r\n    print(tf.data.experimental.cardinality(ds))  # Should be 10, but is -2 (unknown)\r\n```\r\n\r\n**Other info / logs**\r\nThe issue does not occur (anymore), when using `drop_remainder=True` (see #39136)\r\n", "comments": ["See the difference of the dataset `element_spec` with `True and False`:\r\n```\r\nds = ds.batch(2,drop_remainder=False)\r\nprint(\"Element spec:\",ds.element_spec)\r\n```\r\nAnd check the the `lambda` in the two cases:\r\nhttps://github.com/tensorflow/tensorflow/blob/df86a9308a1112a8fd6cd8369a24ec84fa6cd125/tensorflow/python/data/ops/dataset_ops.py#L3906-L3917\r\n\r\nSo in one case when we are finalizing the `unbatch` operation we have no more the `element_spec`\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/df86a9308a1112a8fd6cd8369a24ec84fa6cd125/tensorflow/python/data/ops/dataset_ops.py#L4665-L4667", "/cc @rachellim", "I am able to replicate this on 2.3 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/74e5d6d0f3578928327db8241740e003/untitled463.ipynb).", "The problem is that the shape passed to `unbatch` is unknown. If `unbatch` can't determine the shape of its inputs, it can't determine its cardinality.\r\n\r\n```\r\n    ds = tf.data.Dataset.range(10)  # shape=()\r\n    ds = ds.batch(2)\r\n    print(ds.element_spec.shape)  # prints (None,)\r\n```\r\n\r\nShape inference is not currently smart enough to realize that because 2 evenly divides 10, the shape will always be (2,). Instead, shape inference is conservative and says the shape is unknown, thinking there may be a partial batch at the end. Fixing this is not straightforward, because cardinalities are not known during shape inference. In particular, when computing the shape of the elements produced by `ds.batch(2)`, all we know is that the input shapes are scalars and that `drop_remainder` is `False`.\r\n\r\nIt is possible to make cardinality information available to shape inference, but for it to work across both eager and graph modes we would need to add Python implementations of cardinality for all types of datasets. Instead of duplicating cardinality logic, we would probably want to make the Python cardinality implementations the source of truth.\r\n\r\nIn summary, I think this is a valid feature request that requires some significant refactoring to achieve. In the short term, it is expected that we can't infer the shape of a batch dataset unless drop_remainder is true.", "@aaudiber:\r\n\r\nIn the meantime what do you think about a small PR like (as it will cover some eager cases):\r\n\r\n```python\r\n    try:\r\n      modulo = self._input_dataset.__len__() % self._batch_size\r\n    except TypeError:\r\n      modulo = True\r\n    # pylint: disable=protected-access\r\n    if constant_drop_remainder or (not modulo) :", "@bhack I'd prefer to keep the behavior consistent between eager and graph mode, so that putting a @tf.function around eager code won't change the result of cardinality/__len__. ", "> @bhack I'd prefer to keep the behavior consistent between eager and graph mode, so that putting a @tf.function around eager code won't change the result of cardinality/**len**.\r\n\r\nOK but `len` is already conditional:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/df86a9308a1112a8fd6cd8369a24ec84fa6cd125/tensorflow/python/data/ops/dataset_ops.py#L447-L449\r\n\r\n", "Ah I see, in that case I think your suggestion makes sense for `len`.", "> Ah I see, in that case I think your suggestion makes sense for `len`.\r\n\r\nOk. Can you check https://github.com/tensorflow/tensorflow/pull/45047"]}, {"number": 44918, "title": "Problems to convert a tensorflow model to TFLite", "body": "HI there, \r\nI am having problems when I transformed my trained model into a TFLite model:\r\n\r\n**System information**\r\n- OS Platform and Distribution: CentOS Linux 7 (Core)\r\n- TensorFlow installed from (source or binary): binary\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nimport numpy as np\r\n#from google.colab import files\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom keras.preprocessing import image\r\nimport glob\r\nimport pickle\r\n\r\nmodel_path = '/home/mymodel.h5'\r\n\r\n# convert the model\r\nmodel = keras.models.load_model(model_path)\r\nprint('model loaded!')\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\nprint('model converted!')\r\n\r\nprint('saving model...')\r\n# save the lite model\r\nwith open('mymodel.tflite','wb') as f:\r\n    f.write(tflite_model)\r\n\r\n# done\r\nprint('convertion finished!')\r\n\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\nThe model is transformed successfully but the converted model does not have the same input dimensions (instead the dimensions are transposed):\r\n\r\n**Any other info / logs**\r\nI am running the converted model using the interpreter (https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp35-cp35m-linux_armv7l.whl) in a Raspberry Pi B+\r\n\r\n'''\r\nfrom tflite_runtime.interpreter import Interpreter\r\n\r\n  self.input_details = self.interpreter.get_input_details()\r\n  self.output_details = self.interpreter.get_output_details()\r\n  # check the type of the input tensor\r\n  self.floating_model = self.input_details[0]['dtype'] == np.float32\r\n  # details about the input image\r\n  self.height = self.input_details[0]['shape'][1]\r\n  self.width = self.input_details[0]['shape'][2]\r\n  self.interpreter.set_tensor(self.input_details[0]['index'],input_data)  \r\n  img = Image.open(path)\r\n  print(img.size)\r\n  input_data = np.expand_dims(img, axis=0)\r\n  print(input_data.shape)\r\n  if self.floating_model:\r\n      input_data = (np.float32(input_data) - input_mean) / input_std\r\n\r\n  self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\r\n  print('tensor set!')\r\n  self.interpreter.invoke()\r\n'''\r\n\r\nThis is the output:\r\n\r\n'''\r\nINFO: Initialized TensorFlow Lite runtime.\r\nmodel loaded!\r\nallocating tensors...\r\n[{'quantization': (0.0, 0), 'shape': array([  1, 640, 480,   3]), 'index': 0, 'name': 'conv2d_input', 'dtype': <class 'numpy.float32'>}]\r\n(640, 480)\r\n(480, 640, 3)\r\nimage expanded shape:\r\n(1, 480, 640, 3)\r\n'''\r\n\r\nand I get this error\r\n'''\r\nValueError: Cannot set tensor: Dimension mismatch. Got 480 but expected 640 for dimension 1 of input 0.\r\n'''\r\nThis error is not present if I run the same code using the original model.\r\n\r\n- If I transpose the image to meet the input expectations of the converted model (480x640 instead of the 640x480 in the original), I can run the model but the output of the model is always [0 1. 0 0 0] regardless of the input (the original model works \r\nwell).\r\n\r\nMany thanks in advance for any idea about how to solve this.\r\n\r\nBest,\r\nEdgar", "comments": ["@edgarbc \r\n\r\nWill it be possible to share `mymodel.h5`. So it helps us in localizing the issue faster.I believe all image inputs are fixed size.Also, please let us know which TF version you are using.Thanks!", "Hi ravikyram,\r\n\r\nYou can have a look at mymodel.h5 [here](https://www.dropbox.com/s/gb78ssrnv94puk9/mymodel.h5?dl=0). I am using TF 2.2.\r\n\r\nThank you for your help.\r\n\r\n", "@edgarbc I tried reproducing the issue. I can see that the image sizes are swapped but that is not related to tensorflow. This size swapping is related to PIL module. Please check [this Stackoverflow example](https://stackoverflow.com/questions/26561800/pil-image-size-returns-the-opposite-width-height). \r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/638301c522c4e095b6bb5b7bfb2c7368/untitled.ipynb) is the gist for our reference. Thanks!", "Hi  jvishnuvardhan,\r\nThanks for looking into this. \r\n\r\nI know that the dimensions when you do img.size (from PIL Image) is transposed compared to a numpy array when you do x.shape when you do input_data = np.expand_dims(img, axis=0) but that is not the issue. The dimensions are correctly handled by the original model.\r\n\r\nI know this because when I run the same code but instead of using the TFLite converted model, I use the original h5 model, I can run it no problem. That is why I think there is something with the converted model. If it helps, [this](https://www.dropbox.com/s/w03lb9g8dujd018/mymodel.tflite?dl=0) is the converted model.\r\n\r\nThanks again,\r\nEdgar\r\n\r\n", "@edgarbc Can you please share entire code so that I can check before and after model conversion to tflite? Thanks!", "This is the code that I use without converting the model:\r\n\r\n\r\n```\r\n\r\nimport numpy as np\r\nfrom tensorflow import keras\r\nfrom PIL import Image\r\nfrom numpy import asarray\r\nimport glob\r\n\r\nmodel_path = 'mymodel.h5'\r\ndatadir = '/datapath/'\r\n\r\n# load model\r\nmodel = keras.models.load_model(model_path)\r\nprint('model loaded!')\r\n\r\nfnames = glob.glob(datadir+'*.jpg')\r\n\r\ni=0\r\nfor fn in fnames:\r\n    path = fn\r\n    print(path)\r\n    img = Image.open(path)\r\n    print(img.size)\r\n\r\n    x = asarray(img)\r\n    print(x.shape)\r\n\r\n    x = np.expand_dims(x, axis=0)\r\n    print(x.shape)\r\n\r\n    pred = model.predict(x)\r\n    print(i,pred)\r\n    i = i + 1\r\n\r\n```\r\n\r\nThis is the code I use on the raspberry pi when I try to use the converted model (mymodel.tflite):\r\n\r\n```\r\nfrom tflite_runtime.interpreter import Interpreter\r\nimport numpy as np\r\nfrom PIL import Image\r\nfrom numpy import asarray\r\nimport glob\r\n\r\ninterpreter = Interpreter(model_path='mymodel.tflite')\r\nprint('model loaded!')\r\nprint('allocating tensors...')\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\ndatadir = 'datapath'\r\nfnames = glob.glob(datadir+'*.jpg')\r\ni=0\r\n\r\n        for fn in fnames:\r\n            path = fn\r\n            print(path)\r\n\r\n            img = Image.open(path)\r\n            print(img.size)\r\n            x = asarray(img)\r\n            print(x.shape)\r\n            \r\n            input_data = np.expand_dims(x, axis=0)\r\n            print(input_data.shape)\r\n            \r\n            interpreter.set_tensor(input_details[0]['index'], input_data)\r\n            print('tensor set!')\r\n            \r\n            interpreter.invoke()            \r\n            print('interpreter invoked!')\r\n            \r\n            output_data = interpreter.get_tensor(output_details[0]['index'])\r\n            print('output data calculated!')\r\n            \r\n            result = np.squeeze(output_data)\r\n            print(result)\r\n            print(i)\r\n            i = i + 1\r\n```\r\n\r\nI hope this helps to clarify this further.\r\nThanks,\r\nEdgar", "@edgarbc Looking at the `input_details[0]['index']`, model is expecting an array of shape `[ 1, 640, 480, 3]`. However, `input_data` is of different size (`[ 1, 480, 640, 3]`. Because of this shape incompatibility it is throwing an error at this line\r\n`interpreter.set_tensor(input_details[0]['index'], input_data)`\r\n\r\nI am not sure why the output always is [0 1. 0 0 0]. Any Idea @karimnosseir . Thanks!", "The model shapes looks fine to me, i don't see differences between the .h5 and the tflite.\r\nAs for the output, @jvishnuvardhan do you have gist with data used that reproduce the problem.", "@karimnosseir [Here](https://colab.research.google.com/gist/jvishnuvardhan/2bad760e2893d2a63ef8f4aa70d9a99a/untitled67.ipynb) is the gist for reference. I downloaded the tflite model from [above](https://github.com/tensorflow/tensorflow/issues/44918#issuecomment-729850629), and downloaded images with (640, 480,   3) shape to run the gist. Thanks! ", "> \r\n> \r\n> @edgarbc Looking at the `input_details[0]['index']`, model is expecting an array of shape `[ 1, 640, 480, 3]`. However, `input_data` is of different size (`[ 1, 480, 640, 3]`. Because of this shape incompatibility it is throwing an error at this line\r\n> `interpreter.set_tensor(input_details[0]['index'], input_data)`\r\n> \r\n> I am not sure why the output always is [0 1. 0 0 0]. Any Idea @karimnosseir . Thanks!\r\n\r\n@jvishnuvardhan thanks for checking. Can you please try to run this code:\r\n\r\n```\r\nimport numpy as np\r\nfrom tensorflow import keras\r\nfrom PIL import Image\r\nfrom numpy import asarray\r\nimport glob\r\n\r\nmodel_path = 'mymodel.h5'\r\ndatadir = '/datapath/'\r\n\r\n# load model\r\nmodel = keras.models.load_model(model_path)\r\nprint('model loaded!')\r\n\r\nfnames = glob.glob(datadir+'*.jpg')\r\n\r\ni=0\r\nfor fn in fnames:\r\n    path = fn\r\n    print(path)\r\n    img = Image.open(path)\r\n    print(img.size)\r\n\r\n    x = asarray(img)\r\n    print(x.shape)\r\n\r\n    x = np.expand_dims(x, axis=0)\r\n    print(x.shape)\r\n\r\n    pred = model.predict(x)\r\n    print(i,pred)\r\n    i = i + 1\r\n```\r\nIn this case (the original h5 model), I do not need to transpose the np array to match the (640, 480, 3) as you did in the gist (and I did when I got [0, 1., 0, 0, 0] as well). In this version, the model runs and predicts correctly. This is the output:\r\n\r\n```\r\n/frames738.jpg\r\n(640, 480)\r\n(480, 640, 3)\r\n(1, 480, 640, 3)\r\n0 [[0. 1. 0. 0. 0.]]\r\n1\r\n/frames794.jpg\r\n(640, 480)\r\n(480, 640, 3)\r\n(1, 480, 640, 3)\r\n1 [[1. 0. 0. 0. 0.]]\r\n2\r\n/frames650.jpg\r\n(640, 480)\r\n(480, 640, 3)\r\n(1, 480, 640, 3)\r\n2 [[0. 1. 0. 0. 0.]]\r\n\r\n...\r\n\r\n/frames791.jpg\r\n(640, 480)\r\n(480, 640, 3)\r\n(1, 480, 640, 3)\r\n21 [[0. 0. 0. 0. 1.]]\r\n22\r\n\r\n```\r\nMaybe the predict() function takes into account the image vs array dimension convention in keras but not when I use an interpreter from tflite_runtime? ", "Hi there, just wondering if you have further suggestions about how to solve this problem?"]}, {"number": 44903, "title": "tflite: why using mali gpu will cause memory leak?", "body": "\r\n\r\n**System information**\r\n\r\n- OS Android 10.0\r\n- Mobile device: mate 30 pro:\r\n- tflite build from source:\r\n- gpu\uff1amali g76:\r\n\r\nI noticed there are some comments about reducing memory leak for mali gpu in tflite opencl and opengl context. And I also observed memory leak on my mate30 pro when using opencl, which does not happens with my previous adreno phone.\r\nmate30 pro\r\n![image](https://user-images.githubusercontent.com/21992233/99243764-162b0900-283c-11eb-98f0-ace9fa61def5.png)\r\n\r\nmi8\r\n![image](https://user-images.githubusercontent.com/21992233/99243935-5a1e0e00-283c-11eb-961a-65c041c473e8.png)\r\n\r\nSo can anyone explain why there may be memory leak for mali gpu? \r\n\r\nThanks!\r\n", "comments": ["Hi @Sunnyday2016 ,\r\n\r\nWe are aware of the existence of this issue, but we don't have a Mali device due to COVID-19 and office accessibility, and it doesn't look like the situation will change any time soon (Santa Clara county went from orange to purple yesterday).\r\n\r\nUnfortunately for GPUs, we can't easily pinpoint the problem location for you to take a look.  It could be a vendor driver bug, a bug in the extensions, or a bug in our code.  The last one is probably most likely, and I would start looking at the code locations where Adreno vs Mali diverge."]}, {"number": 44889, "title": "Make tf.debugging.assert_shapes support agnostic comparisons", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`tf.debugging.assert_shapes` currently states that the tensors must have the shape specified by the constraints e.g.\r\n```\r\n>>> tf.debugging.assert_shapes([([0, 0], [1])])\r\nValueError: .  Specified explicitly.  Tensor shape=(2,) dtype=int32 dimension 0 must have size 1.  Received size 2, shape (2,)\r\n```\r\nThis design makes sense, but doesn't allow for when you want to compare the shapes of two tensors, and you don't know if either of them is right, you just know they have to be the same. If I have two tensors `x` and `y`, and I do `tf.debugging.assert_shapes([(x, y.shape)])`, the error message states that `x` should have the shape of `y` at the call site, but it might be that `y` is wrong as well/instead, in which case the message is misleading. The argument `message` can help here, but it would have to contradict the default message.\r\n\r\n**Will this change the current api? How?**\r\nOnly if we consider error messages public API.\r\n\r\n**Who will benefit with this feature?**\r\nUsers? Not sure I can say anything more useful here", "comments": ["It may be that a new function would be better for this, e.g. `tf.debugging.assert_shapes_equal`", "@joelberkeley,\r\nYou have mentioned:\r\n\r\n> 1. you don't know if either of them is right, you just know they have to be the same\r\n> \r\n> 2. the error message states that x should have the shape of y at the call site, but it might be that y is wrong as well/instead, in which case the message is misleading.\r\n\r\nCan you please explain how the above 2 issues can be addressed with the new function, `tf.debugging.assert_shapes_equal`. \r\n\r\nAlso, can you please specify a Real Time Use Case where the `new function` will be effective compared to that of the existing `Functions/APIs`. Thanks!", "`tf.debugging.assert_shapes_equal` (or whatever it's called) would mean you get an error message like\r\n```\r\n>>> tf.debugging.assert_shapes_equal([([0, 0], [1])])\r\nValueError: .  Specified explicitly.  Tensor shapes must match, but received Tensor shape=(2,) dtype=int32 dimension 0 and Tensor shape=(1,), dtype=int32.\r\n```\r\nI did try to word it so that it works both for \"the tensor must have this specific shape\", and \"the shapes can be anything but they must be equal\", but I can't think of a suitable wording.\r\n\r\nOn further digging, I see that my original use case doesn't actually need it, so I don't have an immediate use for it, but to give an example: an object defined in terms of a number of points in some arbitrary-rank space (like a shape or volume in this space). It would be odd (and in some cases restrictive) to work with points that are merely broadcastable to the same shape, rather than the exact same shape, when you know that they need to mean the same thing, but with different element values.", "@joelberkeley,\r\nWhat I understand from your comment is that we can make the \"Error Message more Explicit/Clear\". \r\n\r\nThe purpose of the API, `tf.debugging.assert_shapes` is to Debug our Code and we can always use the [argument](https://www.tensorflow.org/api_docs/python/tf/debugging/assert_shapes#args), `messsage` to customize the `Error Message` and to make it as clear/explicit as we want.\r\n\r\nPlease let us know if we have answered your question so that we can close this issue. Thanks!", "@rmothukuru not really, no. The limitation of the `message` argument is that the base message is worded in a way that assumes the right hand side of the shape check is correct. I'd like to use the function in a way that _doesn't_ assume this. Adding a `message` doesn't fix the problem because it would have to contradict the default message, and contradictory text in an error message sounds like a bad idea", "I guess one way would be to be able to customize the error message more than we can already.", "I've just found that this works, but it's a bit heavy on the boilerplate\r\n```\r\n>>> x = tf.ones([1, 2])\r\n>>> y = tf.ones([3, 2])\r\n>>> tf.TensorShape(tf.shape(x)).assert_is_compatible_with(tf.shape(y))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/joel.berkeley/.venv/trieste/joelberkeley/dev/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 1134, in assert_is_compatible_with\r\n    raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\nValueError: Shapes (1, 2) and tf.Tensor([3 2], shape=(2,), dtype=int32) are incompatible\r\n```\r\nnote I need `tf.shape(x)` not `x.shape` so as to check runtime shapes"]}, {"number": 44888, "title": "tf.debugging.assert_shapes inconsistent behaviour for scalars", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n`tf.debugging.assert_shapes` has seemingly inconsistent behaviour for scalars\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.debugging.assert_shapes([(0, ())])  # passes as expected\r\ntf.debugging.assert_shapes([(0, (1,))])  # passes unexpectedly\r\n```\r\nI think this is inconsistent, because `tf.constant(0)` does not have shape `(1,)`.\r\n\r\n**Describe the expected behavior**\r\nI would expect `tf.debugging.assert_shapes` to error if the `.shape` attribute is different from the shape constraint", "comments": ["I've just noticed this in the docs\r\n\r\n> Scalar tensors and specified shapes of length zero (excluding the 'inner-most' prefix) are both treated as having a single dimension of size one.\r\n\r\nwhich tells me this is intentional (or at least known). I still think it's a problem, because it's inconsistent", "@joelberkeley \r\nplease share complete standalone code such that we can replicate the issue faced or if possible share a colab gist with the issue reported.", "aside from `import tensorflow as tf`, the example is already standalone", "i've added the import statement", "@joelberkeley \r\nI ran the code and there is no output, please find a [gist here](https://colab.research.google.com/gist/Saduf2019/b8bd2dc4e227cd8d5a2461e91652c13c/untitled462.ipynb).", "yes, there isn't any output. That's my concern. I think it would be more consistent if the second assertion failed", "The issue still exists in Tf Nightly 2.6, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/02ccd12a258474ca84b78c34bdf60440/44888.ipynb). Thanks!"]}, {"number": 44887, "title": "Enable TensorFloat32 with XLA", "body": "**System information**\r\n- TensorFlow version (you are using): `2.5.0-dev20201115`\r\n- Are you willing to contribute it (Yes/No): No (don't know how to)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, TF32 works in the normal run time but not with XLA.\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone using Ampere GPUs and XLA compilation.\r\n\r\n**Any Other info.**\r\nI checked the TF32 was not being used with XLA using the following script:\r\n```python\r\nimport tensorflow as tf\r\ntf.debugging.set_log_device_placement(True)\r\ntf.config.experimental.enable_tensor_float_32_execution(True)\r\n\r\n@tf.function(experimental_compile=True)\r\ndef f(x):\r\n    return x @ x\r\nwith tf.device(\"/GPU:0\"):\r\n    x = tf.cast(tf.random.uniform(shape=(16,16)), tf.float32)\r\nf(x)\r\n```\r\n\r\nI ran this on an A100 with dlprof. With `experimental_compile=False`, the following kernel ran:\r\n![image](https://user-images.githubusercontent.com/12474257/99198812-fb3b9480-2768-11eb-9a39-a417e072a064.png)\r\n\r\nWith `experimental_compile=False` and `tf.config.experimental.enable_tensor_float_32_execution(False)`, the following kernel ran:\r\n![image](https://user-images.githubusercontent.com/12474257/99198825-0c84a100-2769-11eb-8785-599f0527526e.png)\r\n\r\nWith  `experimental_compile=True` and `tf.config.experimental.enable_tensor_float_32_execution(True)`, the same kernel ran:\r\n![image](https://user-images.githubusercontent.com/12474257/99198832-1b6b5380-2769-11eb-93fc-dce2e695d5d3.png)\r\n\r\nThis indicates to me that the TF32 is not enabled with XLA. Please let me know if I'm doing something wrong. Thanks!\r\n", "comments": []}, {"number": 44873, "title": "TF Dataset performance regression / best practices for data augmentation on the accelerator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: P100 (colab)\r\n\r\n**Describe the current behaviour**\r\nIterating over a simple Dataset is very slow (even when using a @tf.function). It is 4 (CPU) - 6 (GPU) times slower than a comparable iteration using numpy.\r\n\r\n**Describe the expected behaviour**\r\nI would expect much faster execution of such a simple code, especially because of the optimisations that could be done on the computational graph. It should at least be on par with iterating in Python over numpy data / operations. I can imagine some share of the GPU overhead is attributable to the CUDA kernel launch overhead. I am working on a much more complex dataset structure (dealing witch random sequences + a batch of random additional data per sequence) and was surprised that a big part of the slowness of my structure can be attributed to the simple example presented here. \r\n\r\nI could probably create a dense dataset (with 100x or more (depending on sequence length) in memory footprint) to increase the speed but I believe this kind of data augmentation (different sequence offsets) should be done on the accelerator to not blow up the memory footprint unnecessary. I did not find any best practices regarding this very common problem I am very much following the philosophy (would rather call it \"hacking\") in the tensorflow function \"timeseries_dataset_from_array\": https://github.com/tensorflow/tensorflow/blob/fcc4b966f1265f466e82617020af93670141b009/tensorflow/python/keras/preprocessing/timeseries.py#L30\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1j14DvChu7FJDyD6D8aZPb7w-h4mdTEVZ?usp=sharing\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nd = 1000\r\nz = tf.zeros((d, d), dtype=tf.float32)\r\nds = tf.data.Dataset.range(100).map(lambda x: z[:10, :10]).repeat().batch(256).take(1000)\r\n@tf.function\r\ndef run():\r\n  s = 0\r\n  for x in ds:\r\n    s = tf.reduce_sum(x)\r\n    pass\r\n\r\nrun()\r\n\r\na = np.ones((d, d), dtype=np.float32)\r\ns = 0\r\nfor _ in range(1000):\r\n  for __ in range(256):\r\n    s += np.sum(a[:10, :10])\r\n```\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.3. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b5978e7b96b2e44a0b473bfd8309ecdb/44873.ipynb). Thanks!", "@jonas-eschmann What is your scope with `s += np.sum(a[:10, :10])`?", "@bhack I wanted to make sure the pipeline is bottlenecked by the dataset throughput and not the reduce_sum. ", "Do you want to apply transformations to the dataset elements?"]}, {"number": 44833, "title": "Optimize Tensorflow for Cortex-R MCUs", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nMost of the Automative sensors uses Cortex-R MCU. To run Tensorflow on Cortex-R MCU, can you please optimize Tensorflow for Cortex-R MCUs as well similar to what you have done for Cortex-M?\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\nBuilding Intelligent sensors for automotive applications as well.\r\n\r\n**Any Other info.**\r\n", "comments": ["Sorry for the late reply. You're asking a good question.\r\n\r\nThe Arm Cortex-R architecture is similar to Cortex-A. Both use [Neon technology](https://developer.arm.com/architectures/instruction-sets/simd-isas/neon). A good starting point for optimizations would be the TFL opt kernels for Cortex-A. For example: [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv.h). But that would require you do build TFL, not TFL for Microcontrollers. Do you have a preference? What are the typical memory sizes (SRAM/flash/DRAM) of the systems you're referring to? Do you have a link to a typical system?\r\n\r\nCheers!", "I should be more clear - Neon technology is _optional_ in some of the Arm Cortex-R, see [link](https://developer.arm.com/-/media/Arm%20Developer%20Community/PDF/Cortex-A%20R%20M%20datasheets/Arm%20Cortex-R%20Comparison%20Table_v2.pdf?revision=74d60ed8-f68b-48c4-a888-9a8b128a2cbe)"]}, {"number": 44814, "title": "what is the difference between tf.nn.fixed_unigram_candidate_sampler and  tf.random.log_uniform_candidate_sampler", "body": "# Problem\r\nThere are two functions for the same problem, negative sampling.\r\n\r\n# detail\r\nIn many examples without your official documentation, we use tf.nn.fixed_unigram_candidate_sampler for negative sampling.\r\nrefs:\r\n- https://github.com/carpedm20/practice-tensorflow/blob/master/embedding/word2vec.py#L226-L234\r\n- https://github.com/chao-ji/tf-word2vec/blob/master/word2vec.py#L146-L153\r\n\r\nbut you want to use tf.random.log_uniform_candidate_sampler for its problem.\r\n\r\nSo, users confused about which is the better one.\r\nand also, tf.rando.log_uniform_candidate_sampler may not serve correct negative sampling #44758\r\n\r\nWe need the correct documentation and tutorial for beginners and researchers.", "comments": ["@MokkeMeguru,\r\nSorry for the delayed response. The API, [tf.nn.fixed_unigram_candidate_sampler](https://www.tensorflow.org/s/results?q=tf.nn.fixed_unigram_candidate_sampler) is no longer present and the only API available is [tf.random.log_uniform_candidate_sampler](https://www.tensorflow.org/api_docs/python/tf/random/log_uniform_candidate_sampler). \r\n\r\nHope there is no ambiguity now. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@rmothukuru `tf.random.fixed_unigram_candidate_sampler`", "According to the docs for `tf.random.fixed_unigram_candidate_sampler` ([doc](https://www.tensorflow.org/api_docs/python/tf/random/fixed_unigram_candidate_sampler)) and `tf.random.log_uniform_candidate_sampler` ([doc](https://www.tensorflow.org/api_docs/python/tf/random/log_uniform_candidate_sampler)), the former uses a discrete distribution defined by the `vocab_file` or the `unigrams` argument, while the latter uses a log-uniform (or Zipfian) distribution defined by the formula `P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)`."]}, {"number": 44806, "title": "[TFLite] Quantization and unfolding of the ADD_N operator", "body": "Hello,\r\n\r\nWhen adding multiple tensors (> 2) in a NN we can either use `tf.keras.layers.Add` or `tf.math.add_n`. The first one is quantizable but not the second one as unfortunatly even if both have the same high-level functionality they aren't converted in the same way when using the TFLiteConverter.\r\n\r\nThe `tf.keras.layers.Add` layer is converted to a cascade of binary ADD operators:\r\n![add_cascade](https://user-images.githubusercontent.com/21028116/98962583-b14f7600-24fe-11eb-8d54-589aa5fd64f7.png)\r\n\r\nThe `tf.math.add_n` on the other hand is converted to a single not yet quantizabe ADD_N operator:\r\n![add_n](https://user-images.githubusercontent.com/21028116/98962472-93821100-24fe-11eb-83f0-36977c558704.png)\r\n\r\nIt seems there is an existing [`LowerAddNOp`](https://github.com/tensorflow/tensorflow/blob/47f87c040292b2038cd807cd60109d44dd6b5b4d/tensorflow/compiler/mlir/tensorflow/transforms/lower_tf.cc#L186) transformation to lower the ADD_N operator into an adder tree of binary ADD operators (instead of a cascade) but for some reasons it doesn't seem to apply during the tests I did.\r\n\r\nAs both functions provide the same functionality it would be ideal for them to be exported in the same way in TFLite to avoid any output difference which could occur due to different kernel implementations and additions order as the floating-point and quantized additions are not associative. Unfolding the ADD_N operator would also allow to easily reuse the optimized ADD kernels (both for HW and SW kernels).\r\n\r\nAs we would like to add support for the quantization of models with `tf.math.add_n` I was wondering what were the plans regarding the ADD_N kernel. Would it be alright to unfold it into a cascade of adds? Or was the plan to unfold it into an adder tree (which would then produce different results than `tf.keras.layers.Add` as the order of additions would not be the same)?\r\n\r\nThanks,\r\nThibaut\r\n\r\n", "comments": ["This is a good example of quantization aware conversion. FYI, @karimnosseir @renjie-liu ", "@Tessil We should be doing the TF Lowering pass and you should see the add Tree. Did you try converting using the nightly build ?", "I think that is the expected behavior. The TF Lower pass will be trigged only when the given TensorFlow core operators are not directly legalized. Since we have the ADD_N op implementation, the legalization procedure prefers adding the the TFLite ADD_N op into the flatbuffer over the ADD op tree addition.\r\n\r\nIf we would do that always, we need to add the the ADD_N op --> ADD op tree lowering rule in the early TensorFlow preparation stage.", "Thanks for checking @abattery. @Tessil we are looking in adding ways to easily customize passes/patterns during conversion. Stay tuned.\r\n", "Thank you very much for your answers. I did my tests with the nightly build. \r\n\r\nNote that if we lower the ADD_N to a series of ADD, it would be nice to keep some coherence with the lowering done in `tf.keras.layers.Add` to avoid numerical differences between the two operators  (either an adder-tree for both or a cascade for both) .\r\n", "Hi @abattery @karimnosseir I was wondering if there was any eventual news on the issue? Should we aim to lower the `tf.math.add_n` operator to a series of add similar to `tf.keras.layers.Add` by default or should we keep the `AddN` operator (though in this case the quantized versions of both operators may produce different results and lack coherence)?", "For the workaround, maybe you create a custom keras layer to meet your needs.", "For now we can just replace `tf.math.add_n` with `tf.keras.layers.Add` as temporary workaround but we're mainly interested by resolving the incoherence between the two operators to make it easier to support both operators on HW platforms.", "@teijeong could you triage this issue?", "FYI, @jianlijianli ", "Hi @abattery, out of interest, what is the difference between \"ModelOptimizationTookit\" and \"TFLiteConverter\" labels? I noticed you re-assigned this issues from the latter to the former above. Quick check of other issues under both seems to suggest that both are applicable to issues with the TFLite Converter.", "TFLite converter is a gateway to the TFLite product. However, it contains several components, TF -> TFLite graph lowering and model optimization techniques. This issue is related to the model optimizations so to be clear, we tried to set the model optimization tag for such cases.", "@Tessil \r\nIs this still an issue", "Hi, yes `tf.keras.layers.Add` and `tf.math.add_n` still behave differently and can produce different quantized results."]}, {"number": 44802, "title": "TFLite: GPU delegate (OpenCL) not copying output data from GPU to CPU.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: -\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\nI have a problem with GPU delegate (OpenCL) not copying output data from GPU to CPU.\r\nIn my model I have two outputs. One of them is consumed by the other but I need to know both both results.\r\nThe problem is that `std::vector<Value*> GraphFloat32::outputs() const` https://github.com/tensorflow/tensorflow/blob/5681c179eff80bce00e526303950b67b23cad14c/tensorflow/lite/delegates/gpu/common/model.cc#L35 returns nodes that do not have any consumers. This is how outputs are defined by this function which means one of my outputs is not recgonized.\r\nWhat follows from that is (in order):\r\n- InferenceBuilderImpl having only one output in outputs_ variable https://github.com/tensorflow/tensorflow/blob/5681c179eff80bce00e526303950b67b23cad14c/tensorflow/lite/delegates/gpu/gl/api2.cc#L478\r\n- InitializeGraph placing only one output in output_refs https://github.com/tensorflow/tensorflow/blob/5681c179eff80bce00e526303950b67b23cad14c/tensorflow/lite/delegates/gpu/delegate.cc#L274\r\n- Prepare creating only one output object definition https://github.com/tensorflow/tensorflow/blob/5681c179eff80bce00e526303950b67b23cad14c/tensorflow/lite/delegates/gpu/delegate.cc#L170\r\n- InferenceRunner having only one output https://github.com/tensorflow/tensorflow/blob/5681c179eff80bce00e526303950b67b23cad14c/tensorflow/lite/delegates/gpu/gl/api2.cc#L425\r\n- InferenceRunner copying only one (out of two) outputs from the GPU to CPU-accessible memory https://github.com/tensorflow/tensorflow/blob/5681c179eff80bce00e526303950b67b23cad14c/tensorflow/lite/delegates/gpu/cl/api.cc#L475\r\nNote: I have tested it without the GPU delegate and I have both outputs available so the inference itself works well apparently.\r\n\r\n**Describe the expected behavior**\r\nMemory is copied from GPU to CPU memory.\r\n", "comments": ["Your analysis of GraphFloat32's output handling is correct.  This comes from a wrong assumption at early stages of when we thought output tensors are those without any consumers, and this apparently was wrong.  There's quite a bit of logic of matching graph input / output tensors as defined in the schema with GraphFloat32, and IIRC, this is the 3rd iteration, but there's still a chance that this is still broken.  I think it's easiest if you can provide the TFLite model (doesn't have to be trained, we only need the architecture).", "@impjdi \r\nThanks for your quick reply.\r\nI'd rather avoid sharing it publicly, please find the model file in this repository (I've granted access to your account): TFLite44802ReproModel", "Thanks, I grabbed the model.  So you have a model which defines 2 output tensors; the first one is an intermediate tensor and the second one is a final tensor.", "While we're fixing to make this work, one quick hack you could do to unblock yourself is adding an almost no-op `ADD` operation (+ eps like 1e-12) to the `output/BiasAdd` tensor and use the output of that `ADD` as the final output.  That way, you have now created a non-intermediate tensor as the graph's output.", "Thanks, I'm happy this will be addressed \ud83d\ude04.\r\nI'm will try this workaround for now.", "> While we're fixing to make this work, one quick hack you could do to unblock yourself is adding an almost no-op `ADD` operation (+ eps like 1e-12) to the `output/BiasAdd` tensor and use the output of that `ADD` as the final output. That way, you have now created a non-intermediate tensor as the graph's output.\r\nhi @impjdi \r\nThanks, i have workaround with Add."]}, {"number": 44799, "title": "TFRecord add total_num", "body": "**System information**\r\n- TensorFlow version (you are using): tf2.3.1\r\n\r\nexample:\r\n```python\r\ndata = tf.data.TFRecordDataset('./data/train.tfrecord').map(parse_function).batch(100)\r\nprint(len(data))\r\n```\r\nI want to get `len(data) = total_num/batch_size`\r\n\r\nI suggest to auto add `total_num` in `train.tfrecord` when created train.tfrecord. Then the TFRecordDataset can direct  to get `len(data) = total_num/batch_size`\r\n\r\n\r\n\r\n", "comments": ["tf.data.experimental.cardinality", "> tf.data.experimental.cardinality\r\n\r\nthanks for your reply.\r\n```python\r\ndata = tf.data.TFRecordDataset('./data/train.tfrecord').map(parse_function).batch(100)\r\nprint(tf.data.experimental.cardinality(data).numpy())\r\n\r\n#just return -2\r\n```\r\n\r\n\r\n\r\nBut it not works well in `TFRecordDataset` . it cant return `total_num/batch_size` .  ", "@SmileTM `-2` is \r\n> `UNKNOWN_CARDINALITY` if the analysis fails to determine the number of elements in dataset (e.g. when the dataset source is a file)", "If you know your tfrecord you can set the cardinality:\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/experimental/assert_cardinality", "@bhack \r\n> If you know your tfrecord you can set the cardinality:\r\n> https://www.tensorflow.org/api_docs/python/tf/data/experimental/assert_cardinality\r\n\r\nyes,  in this way , it works well.\r\n```python\r\ndata = tf.data.Dataset.range(1024).batch(20,drop_remainder=True)\r\nprint(len(data))  # 1024//20\r\nprint(tf.data.experimental.cardinality(data))  # 1024//20\r\n```\r\n\r\nbut it not works in the '.tfrecord' file. I also find\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/experimental/cardinality\r\n\r\nThe operation may return tf.data.experimental.INFINITE_CARDINALITY if dataset contains an infinite number of elements or tf.data.experimental.UNKNOWN_CARDINALITY if the analysis fails to determine the number of elements in dataset (e.g. **when the dataset source is a file**).\r\n\r\nI think  the `cardinality` cant solve this ploblem.  And TFRecordDataset cant support `len()`.\r\n\r\n\r\n**You can assume**\uff1a\r\nwhen you have  a big '.tfrecord' file that dont know it length , how to know its length and length//batch_szie easily ? \r\n\r\n\r\n\r\n\r\n", "See https://github.com/tensorflow/tensorflow/issues/26966#issuecomment-701494276\n\nAnd\nhttps://stackoverflow.com/questions/40472139/obtaining-total-number-of-records-from-tfrecords-file-in-tensorflow\n", "@bhack   Thanks for your replay.\r\nThese links cant solve  this  problem  . But I hope TensorFlow can add this function in the future .\r\nI also found that tensroflow added `len()` function at `Dataset`  in the tf2.3.1, but it not works to `.tfrecord` file  . \r\nI think this is a good beginning .\r\ntensorflow purpose is to provide convenience for developers , isn't it ?\r\n \r\n\r\n\r\n  ", "The stack overflow link can resolve the problem with manually iteration.\r\n\r\nThe first link is a quite recent position by @jsimsa (Dataset api mantainer) that explain why he don't want to add this for tfrecord.\r\n"]}]