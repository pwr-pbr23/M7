[{"number": 28537, "title": "Dataset train terminate and Logging error", "body": "`python retrain.py \\\r\n--bottleneck_dir=/tf_files/bottlenecks \\\r\n--how_many_training_steps 500 \\\r\n--model_dir=/tf_files/inception \\\r\n--output_graph=/tf_files/retrained_graph.pb \\\r\n--output_labels=/tf_files/retrained_labels.txt \\\r\n--image_dir /tf_files/data_set`\r\n\r\nThs is my training code\r\n\r\n> --- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/lib/python3.6/logging/__init__.py\", line 992, in emit\r\n    msg = self.format(record)\r\n  File \"/root/anaconda3/lib/python3.6/logging/__init__.py\", line 838, in format\r\n    return fmt.format(record)\r\n  File \"/root/anaconda3/lib/python3.6/logging/__init__.py\", line 575, in format\r\n    record.message = record.getMessage()\r\n  File \"/root/anaconda3/lib/python3.6/logging/__init__.py\", line 338, in getMessage\r\n    msg = msg % self.args\r\nTypeError: not all arguments converted during string formatting\r\nCall stack:\r\n  File \"retrain.py\", line 1326, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"retrain.py\", line 982, in main\r\n    maybe_download_and_extract(model_info['data_url'])\r\n  File \"retrain.py\", line 339, in maybe_download_and_extract\r\n    'bytes.')\r\n  File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py\", line 116, in info\r\n    _get_logger().info(msg, *args, **kwargs)\r\nMessage: 'Successfully downloaded'\r\nArguments: ('inception-2015-12-05.tgz', 88931400, 'bytes.')\r\nERROR:tensorflow:Image directory '/tf_files/thuru_care_data_set' not found.\r\nTraceback (most recent call last):\r\n  File \"retrain.py\", line 1326, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"retrain.py\", line 989, in main\r\n    class_count = len(image_lists.keys())\r\nAttributeError: 'NoneType' object has no attribute 'keys'", "comments": ["#", "duplicate #28536"]}, {"number": 28536, "title": "Dataset train terminate and Logging error", "body": "`python retrain.py \\\r\n--bottleneck_dir=/tf_files/bottlenecks \\\r\n--how_many_training_steps 500 \\\r\n--model_dir=/tf_files/inception \\\r\n--output_graph=/tf_files/retrained_graph.pb \\\r\n--output_labels=/tf_files/retrained_labels.txt \\\r\n--image_dir /tf_files/data_set`\r\n\r\nThs is my training code\r\n\r\n> --- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/lib/python3.6/logging/__init__.py\", line 992, in emit\r\n    msg = self.format(record)\r\n  File \"/root/anaconda3/lib/python3.6/logging/__init__.py\", line 838, in format\r\n    return fmt.format(record)\r\n  File \"/root/anaconda3/lib/python3.6/logging/__init__.py\", line 575, in format\r\n    record.message = record.getMessage()\r\n  File \"/root/anaconda3/lib/python3.6/logging/__init__.py\", line 338, in getMessage\r\n    msg = msg % self.args\r\nTypeError: not all arguments converted during string formatting\r\nCall stack:\r\n  File \"retrain.py\", line 1326, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"retrain.py\", line 982, in main\r\n    maybe_download_and_extract(model_info['data_url'])\r\n  File \"retrain.py\", line 339, in maybe_download_and_extract\r\n    'bytes.')\r\n  File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py\", line 116, in info\r\n    _get_logger().info(msg, *args, **kwargs)\r\nMessage: 'Successfully downloaded'\r\nArguments: ('inception-2015-12-05.tgz', 88931400, 'bytes.')\r\nERROR:tensorflow:Image directory '/tf_files/thuru_care_data_set' not found.\r\nTraceback (most recent call last):\r\n  File \"retrain.py\", line 1326, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"retrain.py\", line 989, in main\r\n    class_count = len(image_lists.keys())\r\nAttributeError: 'NoneType' object has no attribute 'keys'", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command or code to reproduce the issue. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28535, "title": "1.14-rc0 cherry-pick request: Add cuda runtime 9.0 API to dlopen wrapper.", "body": "This cherrypick is required to build 1.14 with CUDA 9.\r\n\r\nPiperOrigin-RevId: 245304515\r\n", "comments": []}, {"number": 28534, "title": "1.14-rc0 cherry-pick request: Merge pull request #28057 from benbarsdell:fix-const-fold-dyn-shape-noop-reduction", "body": "Cherrypick that significantly improves performance of the auto_mixed_precision grappler pass.\r\n\r\nPiperOrigin-RevId: 246892307", "comments": []}, {"number": 28533, "title": "Simplify reset_uids()", "body": "Remove unnecessary dict clearance code", "comments": ["@I-Hong Could you please resolve the conflicts? Thanks!", "Thanks @gbaned. Updated. ", "@I-Hong Could you please look at the failing checks? Thanks!", "Updated!"]}, {"number": 28532, "title": "ValueError when reading a frozen graph", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution: Linux Fedora 30\r\n- TensorFlow installed from: Binary\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.7.3 (64-bit)\r\n- CUDA/cuDNN version: CUDA 10.0\r\n- GPU model and memory: Nvidia GeForce GTX 1050M [4GB]\r\n\r\n**Current behavior**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/fuzzybatman/.local/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 426, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node bn_data/cond/ones_like/Shape}}\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/fuzzybatman/Workspace/ET4399-Extra_Project_[Okotech]/09 - UNet/frozen_predict.py\", line 30, in <module>\r\n    tf.import_graph_def(trt_graph, name='')\r\n  File \"/home/fuzzybatman/.local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/fuzzybatman/.local/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 430, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node bn_data/cond/ones_like/Shape}}\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nLoad the graph so it can be used for inference.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\noutput_names = ['sigmod/Sigmoid']\r\ninput_names = ['data']\r\n\r\ndef get_frozen_graph(graph_file):\r\n    \"\"\"Read Frozen Graph file from disk.\"\"\"\r\n    with tf.gfile.FastGFile(graph_file, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n    return graph_def\r\n\r\ntrt_graph = get_frozen_graph('<PATH-TO-MODEL>')\r\n\r\n# Create session and load graph\r\ntf_config = tf.ConfigProto()\r\ntf_config.gpu_options.allow_growth = True\r\n\r\ntf_sess = tf.Session(config=tf_config)\r\ntf_sess.run(tf.global_variables_initializer())\r\n\r\ntf.import_graph_def(trt_graph, name='')\r\n```\r\n\r\n**Other info / logs**\r\nThe original model was built in keras\r\n", "comments": ["I used **keras** and **tensorflow.keras** interchangeably.\r\nSo my answer to _Have you written custom code_ is a Yes."]}, {"number": 28531, "title": "Move dataset op names into name_utils namespace", "body": "This PR aims to move all dataset op names into `tensorflow::data::name_utils` namespace and replace all uses of string literals for dataset names.\r\n\r\ncc: @jsimsa ", "comments": ["@jsimsa Thanks for your comments. I rewrite this PR according to your comments. Could you please have a look at the changes (https://github.com/tensorflow/tensorflow/pull/28531/commits/e81ba1947142deb0fb4424cbc30d7f11a8245763)?", "@jsimsa The comments are addressed here (https://github.com/tensorflow/tensorflow/pull/28531/commits/885715e4deeafb9ab20afb959e7f98e561b4dd07), except [this one](https://github.com/tensorflow/tensorflow/pull/28531#discussion_r282693147). More details can be found at the comments. ", "@feihugis thank you for making the changes. After looking at the CL, I realized that it is probably not a good idea to expect all dataset types to be added to the `enum` in `name_utils.h`. After all, there are dataset implementation that do not exist in our code base.\r\n\r\nSo instead of having an enum, I suggest to make the base op name be a static member of the op kernel. We would still keep the `name_utils::XXX` method that take a base string (instead of enum) and return the different versions of it.\r\n\r\nTo make the base op name accessible from unit tests, we will want to create .h files for ops that do not have them that define the externally visible op interface. Take a look at `prefetch_dataset_op.{h, cc}` for a minimal example of how to do this.\r\n\r\nI probably makes sense to do this incrementally for existing ops, starting with ops that already have unit tests. Let me know if you have questions. Thanks!", "@jsimsa I agree with you and have one question in below.\r\n\r\n> After looking at the CL, I realized that it is probably not a good idea to expect all dataset types to be added to the `enum` in `name_utils.h`. After all, there are dataset implementation that do not exist in our code base.\r\n> \r\nYes, it is a good point, e.g. the datasets in tf-io.\r\n\r\n> So instead of having an enum, I suggest to make the base op name be a static member of the op kernel. We would still keep the `name_utils::XXX` method that take a base string (instead of enum) and return the different versions of it.\r\n> \r\n> To make the base op name accessible from unit tests, we will want to create .h files for ops that do not have them that define the externally visible op interface. Take a look at `prefetch_dataset_op.{h, cc}` for a minimal example of how to do this.\r\n>\r\nAs we already have `type_string_` member in `DatasetBase`, could we use `type_string_` as the input for `name_utils::XXX` so that we do not need to create a new static member or .h files. To get the base op name, we can add `name_utils::Basename()`. The dataset op name seems to be named in the format [Basename][Dataset][Version].", "The problem with `type_string_` is that it is derived from the op name and multiple ops can use the same kernel (e.g. `BatchDataset` and `BatchDatasetV2`). In general we do not want to distinguish between different ops executing the same kernel but there are some exceptions to that rule (e.g. `ParallelInterleave` and `ParallelInterleaveV2` which have different op kernels).\r\n\r\nIf you do not want to go through the trouble of creating a new static member (`kDatasetTypeName`) and header files, that's fine. I am okay with not doing this refactoring.", "@jsimsa Thanks for your explanation! I understand now. I do want to try the refactoring with header files. Will submit a commit as an example for your review.", "@jsimsa The {Range, Map, Prefetch} datasets have been refactored with the new static member (`kDatasetTypeName`), header files, and `name_utils`. Could you please have a look at the changes (https://github.com/tensorflow/tensorflow/pull/28531/commits/d2bd4afee7fb31415704f24b0766ab26d0d2a87f) when you have time?", "@jsimsa This commit (https://github.com/tensorflow/tensorflow/pull/28531/commits/ec7f39c4e8665e98e41e6340737327ffb69389d9) is submitted to address your comments, except [this one](https://github.com/tensorflow/tensorflow/pull/28531#discussion_r283102963). \r\n\r\nThanks for letting me know your travel! Have a safe and enjoyable trip!", "@jsimsa The dataset input and attribute names are changed to be named constants, and also make a little refactoring for `RangeDatasetOpTest` and `MapDatasetOpTest` via (https://github.com/tensorflow/tensorflow/pull/28531/commits/60f6c35f6a5dd9405d11c447222178684a766e33). Could you please have a look at the changes when you have time?", "@jsimsa Good suggestion. They have been changed via this commit (https://github.com/tensorflow/tensorflow/pull/28531/commits/91b412bd79f2d521b7b3a3c267c1ca568c83c843).", "@jsimsa The internal checks failed. Could you help paste the log here?", "```\r\nthird_party/tensorflow/core/kernels/data/prefetch_dataset_op_test.cc:16:10: error: module //third_party/tensorflow/core/kernels/data:prefetch_dataset_op_test does not depend on a module exporting 'third_party/tensorflow/core/kernels/data/name_utils.h'\r\nsee http://go/cpp-features#layering_check; to fix run:\r\nbuild_cleaner //third_party/tensorflow/core/kernels/data:prefetch_dataset_op_test\r\n#include \"third_party/tensorflow/core/kernels/data/name_utils.h\"\r\n```", "This import `#include \"tensorflow/core/kernels/data/name_utils.h` is not needed in `prefetch_dataset_op_test.cc` as it has been imported in `DatasetOpsTestBase`. It is removed via this commit (https://github.com/tensorflow/tensorflow/pull/28531/commits/0928e53708715d11da6b425be20ccbe1fadc1770). @jsimsa Could you please have a look at the change?\r\n\r\n", "@jsimsa Thanks for your help on this PR! I will start to refactor the rest of dataset ops. Do you think which way is better to submit the PRs: 1) one PR for one dataset op; 2) one PR for several dataset ops?", "You can submit multiple of them in one PR, but use separate commits for separate dataset ops.", "> You can submit multiple of them in one PR, but use separate commits for separate dataset ops.\r\n\r\nYeah, will do."]}, {"number": 28530, "title": "Python Flow-based Programming UI (graphical designer IDE) similar to FlowHub or Node-red", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13 and 2.0\r\n- Are you willing to contribute it (Yes/No):yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nPython Flow-based Programming UI (graphical designer IDE) similar to NoFLo or others but using Python API  which is more complete . Design the entire model on a canvas by drag-and-drop components on the canvas, then generate the python code and that can be run externally or on the system as a python code. This will make it easy for new users to design new models without coding but benefit from the knowledge \r\n\r\n**Will this change the current api? How?** Not really \r\n\r\n**Who will benefit with this feature?** Tons of new users to Tensorflow AI  without indepth AI low-level programming \r\n\r\n**Any Other info.**\r\n", "comments": ["@mihaimaruseac Can you PTAL? Thanks!", "I think it's better to get PTAL from @martinwicke and @dynamicwebpaige though this feature request seems to not have a lot of details", "In this generality, this is not a feature request we will be working on in the foreseeable future. If there's significant interest in something like this from the community, we would encourage organizing a group and coming up with a more detailed proposal. \r\n\r\nI will close this issue.", "Here are three examples\r\n- https://github.com/Cloud-CV/Fabrik\r\n- https://github.com/TommyX12/tensorbuilder\r\n- https://github.com/sharmalab/tensorflow-gui \r\n\r\n@developeralgo8888 please take a look and see if you can contribute to these projects"]}, {"number": 28529, "title": "[ROCm] Fix for the broken `--config=rocm` build", "body": "The --config=rocm build was broken by the following commit.\r\n\r\nd58b53e19c3d06cea5909c66379d073aea153651\r\n\r\nThe changes made by the above commit did not contain the corresponding changes for the ROCm platform, which was leading to the build failure. Making the corresponding update for ROCm, to make the --config=rocm build working again.\r\n\r\n----------\r\n@tatianashp , @whchung, @timshen91 just FYI\r\n\r\nPlease approve and merge. As with other such PRs this week, the changes here are trivial and only applicable for the --config=rocm build.\r\n\r\nthanks", "comments": ["+ @gunan \r\n\r\nThis is another example of upstream commit to TensorFlow inadvertently break ROCm build. Your idea to send us webhooks is well received and we can take follow-up discussion off-line. Meanwhile could you help review and merge this PR? Thanks.", "This change was rolled back today:\r\nhttps://github.com/tensorflow/tensorflow/commit/98bf24deb21e8d0b5c11aedfe655f6f7a5e022ad\r\n\r\n@chsigg to make sure he is aware the implications of the change."]}, {"number": 28528, "title": "TFLite calibration-and-quantization I/O min/max mismatch on some operators", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nArch Linux 5.0.10\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n\r\n- TensorFlow version (use command below):\r\nRunning: https://github.com/ajarthurs/tensorflow/tree/q_leakyrelu\r\nBase commit: 3ea8756ce6d08a473d78347fb7b876ad5c1be973\r\nRelated PR: https://github.com/tensorflow/tensorflow/pull/27028\r\nRelated issue: https://github.com/tensorflow/tensorflow/issues/28268\r\n\r\n- Python version:\r\n3.7.3\r\n\r\n- Bazel version (if compiling from source):\r\n0.25.0\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n8.3.0\r\n\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nDuring calibration-and-quantization (see `tensorflow/lite/tools/optimize/calibration`), the tensor feeding a `RELU` operator shows that its minimum value is 0 instead of a negative number. This may lead to a quantized model that produces erroneous results; however, I currently cannot confirm if the said behavior is causing erroneous quantization.\r\n\r\n**Describe the expected behavior**\r\nI expect `RELU`'s input minimum value (for quantization) to match the input tensor's actual minimum value (a negative real number).\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nAn MSCOCO-trained floating-point (32bit) model that I am attempting to calibrate-and-quantize is available at https://www.dropbox.com/s/hnkmqzcasb8lu8n/retinanet-float32.tflite?dl=0\r\n\r\n[calibrate_and_quantize.py](https://github.com/tensorflow/tensorflow/files/3154074/calibrate_and_quantize.py.zip) is an example script that runs TF Lite's calibration-and-quantization tool. It can be executed as such:\r\n```\r\n# Adjust paths as needed.\r\n$ PYTHONPATH=path/to/tensorflow/tpu/models/official/retinanet:$PYTHONPATH python calibrate_and_quantize.py --input_tflite_file=retinanet-float32.tflite --output_tflite_file=retinanet-int8.tflite --train_file_pattern=path/to/mscoco-tfrecords/train*\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nWhile running the attached Python script, it reports the following warnings:\r\n```\r\nNote the output min/max is different from the input min/max for op MAX_POOL_2D at index 2 in subgraph 0. This is legal but should happens rarely.\r\nNote the output min/max is different from the input min/max for op RESIZE_BILINEAR at index 77 in subgraph 0. This is legal but should happens rarely.\r\nNote the output min/max is different from the input min/max for op RESIZE_BILINEAR at index 80 in subgraph 0. This is legal but should happens rarely.\r\n```\r\n\r\nAnd during one my `gdb` sessions, I check the I/O min/max values of a `MAX_POOL_2D` operator highlighted by one of the warnings above. The output minimum value of the `MAX_POOL_2D` operator is 0 instead of -19.3, which would throw the said warning.\r\n```\r\nThread 1 \"calibrate_and_q\" hit Breakpoint 2, tflite::optimize::(anonymous namespace)::QuantizeOpOutput (\r\n    model=0x55822760c890, subgraph_idx=0, op_idx=2, property=..., output_idx=0, error_reporter=0x558223944030)\r\n    at tensorflow/lite/tools/optimize/quantize_model.cc:504\r\n504             printf(\r\n(gdb) p min\r\n$5 = -19.2778091\r\n(gdb) p max\r\n$6 = 22.6024609\r\n...\r\n(gdb) p output_tensor->quantization->min[0]\r\n$10 = 0\r\n(gdb) p op_code\r\n$11 = tflite::BuiltinOperator_MAX_POOL_2D\r\n(gdb) p output_tensor->quantization->max[0]\r\n$12 = 22.6024609\r\n(gdb) bt\r\n#0  tflite::optimize::(anonymous namespace)::QuantizeOpOutput (model=0x55822760c890, subgraph_idx=0, op_idx=2,\r\n    property=..., output_idx=0, error_reporter=0x558223944030)\r\n    at tensorflow/lite/tools/optimize/quantize_model.cc:504\r\n#1  0x00007f78cc28497c in tflite::optimize::(anonymous namespace)::QuantizeWeightsInputOutput (\r\n    builder=0x7fff304b4860, model=0x55822760c890, allow_float=false, error_reporter=0x558223944030)\r\n    at tensorflow/lite/tools/optimize/quantize_model.cc:570\r\n#2  0x00007f78cc284faf in tflite::optimize::QuantizeModel (builder=0x7fff304b4860, model=0x55822760c890,\r\n    input_type=@0x7fff304b4844: tflite::TensorType_INT8, output_type=@0x7fff304b4848: tflite::TensorType_INT8,\r\n    allow_float=false, error_reporter=0x558223944030) at tensorflow/lite/tools/optimize/quantize_model.cc:638\r\n#3  0x00007f78cc26a8c1 in tflite::calibration_wrapper::CalibrationWrapper::QuantizeModel (this=0x5582274c7790,\r\n    input_py_type=1, output_py_type=1, allow_float=false)\r\n    at tensorflow/lite/python/optimize/calibration_wrapper.cc:201\r\n#4  0x00007f78cc268f88 in _wrap_CalibrationWrapper_QuantizeModel (args=0x7f78cc697458)\r\n    at bazel-out/k8-dbg/bin/tensorflow/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.cc:3418\r\n#5  0x00007f795c552e68 in _PyMethodDef_RawFastCallKeywords () from /usr/lib/libpython3.7m.so.1.0\r\n#6  0x00007f795c553101 in _PyCFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0\r\n#7  0x00007f795c5c3d19 in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0\r\n#8  0x00007f795c5526db in _PyFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0\r\n#9  0x00007f795c5c36ea in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0\r\n#10 0x00007f795c50bd09 in _PyEval_EvalCodeWithName () from /usr/lib/libpython3.7m.so.1.0\r\n#11 0x00007f795c552882 in _PyFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0\r\n#12 0x00007f795c5bff9c in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0\r\n#13 0x00007f795c50bd09 in _PyEval_EvalCodeWithName () from /usr/lib/libpython3.7m.so.1.0\r\n#14 0x00007f795c552882 in _PyFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0\r\n#15 0x00007f795c5bf22d in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0\r\n#16 0x00007f795c5526db in _PyFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0\r\n#17 0x00007f795c5bf22d in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0\r\n#18 0x00007f795c50bd09 in _PyEval_EvalCodeWithName () from /usr/lib/libpython3.7m.so.1.0\r\n#19 0x00007f795c552882 in _PyFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0\r\n#20 0x00007f795c5c36ea in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0\r\n#21 0x00007f795c50bd09 in _PyEval_EvalCodeWithName () from /usr/lib/libpython3.7m.so.1.0\r\n--Type <RET> for more, q to quit, c to continue without paging--q\r\n```", "comments": ["@ajarthurs , is this problem solved for you ? My understanding is that you are getting min max output as different from input in MAX2Dpool, as toco graph transform does the hardcoding for the Min and max values.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 The calibration-and-quantization tool is reporting different min/max values around `MAX_POOL_2D` as a warning. The source of the warning is at [`tensorflow/lite/tools/optimize/quantize_model.cc`](https://github.com/ajarthurs/tensorflow/blob/q_leakyrelu/tensorflow/lite/tools/optimize/quantize_model.cc#L504-L508). But following that, [the node's output tensor's quant-params are replaced with the input tensor's quant-params](https://github.com/ajarthurs/tensorflow/blob/q_leakyrelu/tensorflow/lite/tools/optimize/quantize_model.cc#L512-L518). Also confirmed with `tensorflow/lite/tools/visualize.py` that `MAX_POOL_2D`'s output minimum is correctly set to a negative value with `RELU` as its activation function.\r\n\r\nSo, this looks to be a non-issue save for the warnings. This issue can be closed unless you think the warnings need to be looked into.\r\n\r\nAtt: [qint8.html.txt](https://github.com/tensorflow/tensorflow/files/3178371/qint8.html.txt) -- Quantized int8 model HTML from `visualize.py`", "@ajarthurs thanks for the confirmation what i meant in my response was that source of the warning is the toco graph transformation which hardcodes the min and max of the maxpool2D, you can refer to HardcodeMinMaxForAverageOrMaxPool function in lite/toco/graph_transformations/hardcode_min_max.cc\r\n\r\nYes this looks like not an issue to me as well. We can close this one.\r\n\r\nRegards\r\nAmit", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28528\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28528\">No</a>\n", "@amitsrivastava78 OK. Thanks, Amit."]}, {"number": 28527, "title": "tfdbg and tf.data API:  Inconsistency with handling dataset iterators' OutOfRangeError", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Mac OS Mojave v10.14.4\r\n- TensorFlow installed from (source or binary):  Binary (PIP)\r\n- TensorFlow version (use command below):  pip install tensorflow==1.13.1\r\n- Python version:  3.6.2\r\n- CUDA/cuDNN version:  N/A (found bug on CPU version)\r\n\r\nprint(tf.GIT_VERSION, tf.VERSION):  ==>  'v1.13.0-rc2-5-g6612da8951' 1.13.1\r\n\r\n\r\n**Describe the current behavior**\r\nDifferent behavior while using tfbdg responding to catching OutOfRangeError required for iteratively looping on dataset iterators from the tf.data API.  Using tfdbg command  'run -t 100', we get execution as normally expected.  Using tfbdg command 'run -f has_inf_or_nan', we get a cascade of errors.\r\n\r\n**Describe the expected behavior**\r\nRunning tfbdg with a filter vs running without a filter produces the same code errors when dealing with catching OutOfRangeErrors.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\ndataset = tf.data.Dataset.range(3)\r\niterator = dataset.make_initializable_iterator()\r\nnext_element = iterator.get_next()\r\nresult = tf.add(next_element, next_element)\r\n\r\nwith tf.Session() as sess:\r\n\r\n  sess = tf_debug.LocalCLIDebugWrapperSession(sess)\r\n\r\n  sess.run(iterator.initializer)\r\n  while(True):\r\n    try:\r\n      output = sess.run(result)\r\n      print(output)\r\n    except tf.errors.OutOfRangeError:\r\n      print(\"End of dataset\")\r\n      break\r\n```\r\n\r\n**Other info / logs**\r\nFYI: This code snippit is a condensed version of a similarly-structured larger project I am working with; in that larger project (which I am unable to condensely reproduce at this time) the difference between running with tfbdg with a filter produces error 'OutOfRangeError is not iterable' and halts execution whereas tfdbg running without a filter does not and executes normally as intended (ie. slightly different than the discrepency displayed in the code snippet above).", "comments": ["I was able to reproduce the reported behavior in TF 1.13.1", "It is no longer needed. Thanks,\nDaniel ReMine\n\n> On Feb 19, 2021, at 2:10 PM, Alfred Sorten Wolf <notifications@github.com> wrote:\n> \n> \ufeff\n> Hi There,\n> \n> We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help.\n> \n> This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28527\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28527\">No</a>\n"]}, {"number": 28526, "title": "build_pip_package broken because of 58b53e19c3d06cea5909c66379d073aea153651", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution ( Linux Ubuntu 16.04):\r\n- TensorFlow version: master at 66b193faeecda4e6bc7e2767c2d927eecd199a34\r\n- Python version: 2\r\n- Installed using virtualenv? pip? conda?: docker container build\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 5.3\r\n\r\n**Describe the problem**\r\nbuild_pip_package is failing: \r\n```\r\n\u001b[0m\u001b[91mTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\n\u001b[0m\u001b[91mINFO: Elapsed time: 559.703s, Critical Path: 225.66s\r\nINFO: 12822 processes: 12822 local.\r\n\u001b[0m\u001b[91mINFO: Build completed successfully, 13671 total actions\r\n\u001b[0m\u001b[91mINFO: Build completed successfully, 13671 total actions\r\n\u001b[0mWed May 8 17:39:25 UTC 2019 : === Preparing sources in dir: /tmp/tmp.kL5rPiZPYi\r\n/tensorflow /tensorflow\r\n/tensorflow\r\n\u001b[91mcp: cannot stat 'bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/external/local_config_cuda/cuda/cuda/cuda_config.h': No such file or directory\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel --bazelrc=/root/.bazelrc build -c opt     tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/whl\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@claynerobison Request you to please follow the steps mentioned in the [link](https://www.tensorflow.org/install). In the log it says could not find cudaconfig.h so if you want to install TF gpu,Please install relevant components as per the OS and let us know how it progresses. Thanks!", "Also, that commit doesn't seem to exist", "This was fixed yesterday by: https://github.com/tensorflow/tensorflow/commit/98bf24deb21e8d0b5c11aedfe655f6f7a5e022ad\r\n\r\nThe GPU builds were working fine, it was the non gpu builds that couldn't find cuda.h :) \r\n\r\nedit: the commit that broke it was d58b53e19c3d06cea5909c66379d073aea153651, the first character is missing from the issue title", "@claynerobison Can you please try the fix suggested by wdirons and let us know. Thanks!", "@claynerobison Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28525, "title": "[TF-TRT] Support SquaredDifference op", "body": "Support SquaredDifference op by using Elementwise layer. Includes unit tests.", "comments": []}, {"number": 28524, "title": "Using subword information in OOV token from fasttext in word embedding layer (keras/tensorflow)", "body": "Serious, there is nothing on the internet I found in this way to handle OOV tokens with custom word vectors! There are some post, but all are saying using eg. fasttext (https://stackoverflow.com/questions/48395570/how-to-initialize-word-embeddings-for-out-of-vocabulary-word?rq=1 ) wih no explanation how to do it.\r\nIt should be somehow possibleto  bypass/hack around to assign a vector for OOVs?\r\nBut I struggle to find the proper position of code like in tf.nn.embedding_lookup where I can do it in a way like `if not in vocab get.fasttext.vector`\r\n\r\nI have my own Fasttext model and trained with it a tensorflow classification model with a word embedding layer.\r\n\r\n```\r\nwith tf.variable_scope('embeddings'):\r\n            word_embeddings = tf.constant(self.embedding_mat, dtype=tf.float32, name=\"embedding\")\r\n            self.embedded_x1 = tf.nn.embedding_lookup(word_embeddings, self.x1)\r\n            self.embedded_x2 = tf.nn.embedding_lookup(word_embeddings, self.x2)\r\n\r\n```\r\nBut, I wonder how I can make use of the subword information of my model for OOV words? Since the word embedding layer operated via indices to look up word vectors and OOV words have no index. Even if a OOV token has a index how would I assign it the proper word vector to this OOV on the fly for an already trained model?\r\n\r\nOne understanding question: If I have a UNK token in my matrix but I did not used this UNK in training, can I assign a custom vector (eg. from fasttext) during prediction? So, for this UNK word which might be similar to a trained word such that the text is classified as same class? Then you could have also more UNK token like UNK1 and UNK2. \r\n\r\nI found two interesting posts, but I do't know if I understand that solution and if that is the solution...\r\nhttps://stackoverflow.com/questions/45113130/how-to-add-new-embeddings-for-unknown-words-in-tensorflow-training-pre-set-fo\r\n\r\nhttps://stackoverflow.com/questions/47022591/use-tensorflow-and-pre-trained-fasttext-to-get-embeddings-of-unseen-words?rq=1\r\n\r\nThanks in advance!", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there and can provide support better for these type of issues. Thanks!", "@achandraa I asked this too on stackoverflow but no answer. I do also get rarely an answer there...", "Closing this issue since its not Bug/Feature Request. [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow) can be good platform to address this question. Thanks!", "@datistiquo  I have same problem with you\r\nI replace embedding layer with DataGenerator\r\nIt's a bit slow but it works fine at least\r\n\r\n`data_generator = DataGenerator(clean_text_list, batch_size=batch_size, shuffle=False)\r\n`\r\n```\r\ndef fasttext2vec(text):\r\n    matrix = np.zeros((max_seq_length, word2vec.get_dimension()))\r\n    for i, word in enumerate(text.split()):\r\n        if i < max_seq_length:\r\n            matrix[i] = word2vec.get_word_vector(word)     \r\n    return matrix\r\n```\r\n\r\n```\r\nimport numpy as np\r\nimport keras\r\n\r\nclass DataGenerator(keras.utils.Sequence):\r\n    'Generates data for Keras'\r\n    def __init__(self,\r\n                 texts,\r\n                 batch_size=256,\r\n                 n_channels=1,\r\n                 dim=(max_seq_length, word2vec.get_dimension()),\r\n                 shuffle=True):\r\n        'Initialization'\r\n        self.dim = dim\r\n        self.batch_size = batch_size\r\n        self.n_channels = n_channels\r\n        self.texts = texts\r\n        self.shuffle = shuffle\r\n        self.on_epoch_end()\r\n\r\n    def __len__(self):\r\n        'Denotes the number of batches per epoch'\r\n        return int(np.floor(len(self.texts) / self.batch_size))\r\n\r\n    def __getitem__(self, index):\r\n        'Generate one batch of data'\r\n        # Generate indexes of the batch\r\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\r\n\r\n        # Find list of IDs\r\n        texts_list = [self.texts[k] for k in indexes]\r\n\r\n        # Generate data\r\n        X = self.__data_generation(texts_list)\r\n\r\n        return X\r\n\r\n    def on_epoch_end(self):\r\n        'Updates indexes after each epoch'\r\n        self.indexes = np.arange(len(self.texts))\r\n        if self.shuffle == True:\r\n            np.random.shuffle(self.indexes)\r\n\r\n    def __data_generation(self, texts):\r\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\r\n        # Initialization\r\n        text_matrix   = np.empty((self.batch_size, *self.dim))\r\n        \r\n        # Generate data\r\n        for i, text in enumerate(texts):\r\n            \r\n            # Store sample\r\n            text_matrix[i,] = vectorized_fastext2vec(text)\r\n\r\n        return text_matrix\r\n```\r\n\r\n\r\n"]}, {"number": 28523, "title": "Tensorflow v2 cannot save with CheckpointManager in tf.function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0a\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.0\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to save a `tf.train.Checkpoint` within a `tf.function` annotated function.\r\nI use the code below:\r\n```python\r\nimport tensorflow as tf\r\n\r\nsw = tf.summary.create_file_writer('models/test/')\r\n\r\ndata = tf.random.normal((1000, 10, 10, 1))\r\ndataset = tf.data.Dataset.from_tensors(data).batch(10)\r\n\r\n\r\nstep = tf.Variable(0)\r\n\r\nckpt = tf.train.Checkpoint(step=step)\r\nmgr = tf.train.CheckpointManager(ckpt, 'models/ckpt/', max_to_keep=20)\r\n\r\n\r\n@tf.function\r\ndef train_step(x):\r\n    return x * x * x\r\n\r\n\r\ndef train(dataset):\r\n    for i in range(10):\r\n        for x in dataset:\r\n            step.assign_add(1)\r\n            print(step)\r\n            print(ckpt.save_counter)\r\n            if tf.equal(step % 2, 0):\r\n                print('save')\r\n                mgr.save()\r\n\r\ntrain = tf.function(train)\r\ntrain(dataset)\r\n```\r\n\r\n<details><summary>Console output</summary>\r\n\r\n```\r\n2019-05-08 17:52:33.083828: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n<tf.Variable 'Variable:0' shape=() dtype=int32>\r\n<tf.Variable 'save_counter:0' shape=() dtype=int64>\r\nsave\r\nTraceback (most recent call last):\r\n  File \".\\test.py\", line 49, in <module>\r\n    train(dataset)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 426, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 370, in _initialize\r\n    *args, **kwds))\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1313, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1580, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1512, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 694, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 317, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 686, in wrapper\r\n    ), args, kwargs)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 392, in converted_call\r\n    result = converted_f(*effective_args, **kwargs)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpw8xyj0j5.py\", line 25, in tf__train\r\n    ag__.for_stmt(ag__.converted_call(range, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (10,), {}), None, loop_body_1, ())\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 81, in for_stmt\r\n    return _py_for_stmt(iter_, extra_test, body, init_state)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 90, in _py_for_stmt\r\n    state = body(target, *state)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpw8xyj0j5.py\", line 23, in loop_body_1\r\n    ag__.for_stmt(dataset, None, loop_body, ())\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 79, in for_stmt\r\n    return _dataset_for_stmt(iter_, extra_test, body, init_state)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 152, in _dataset_for_stmt\r\n    ds.reduce((constant_op.constant(0),), reduce_body_with_dummy_state)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1276, in reduce\r\n    add_to_graph=False)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 2388, in __init__\r\n    self._function = wrapper_fn._get_concrete_function_internal()\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1319, in _get_concrete_function_internal\r\n    *args, **kwargs)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1313, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1580, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1512, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 694, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 2381, in wrapper_fn\r\n    ret = _wrapper_helper(*args)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 2326, in _wrapper_helper\r\n    ret = func(*nested_args)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 150, in reduce_body_with_dummy_state\r\n    reduce_body((), iterate)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 141, in reduce_body\r\n    new_state = body(iterate, *state)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpw8xyj0j5.py\", line 21, in loop_body\r\n    ag__.if_stmt(cond, if_true, if_false)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 245, in if_stmt\r\n    return tf_if_stmt(cond, body, orelse)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 256, in tf_if_stmt\r\n    return control_flow_ops.cond(cond, protected_body, protected_orelse)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 1918, in cond\r\n    return cond_v2.cond_v2(pred, true_fn, false_fn, name)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\ops\\cond_v2.py\", line 74, in cond_v2\r\n    op_return_value=pred)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 694, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 263, in protected_func\r\n    results = func()\r\n  File \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpw8xyj0j5.py\", line 16, in if_true\r\n    ag__.converted_call('save', mgr, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_3, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (), {})\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 267, in converted_call\r\n    return _call_unconverted(f, args, kwargs)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 188, in _call_unconverted\r\n    return f(*args, **kwargs)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py\", line 687, in\r\nsave\r\n    session.run(self._save_counter_assign)\r\nAttributeError: 'NoneType' object has no attribute 'run'\r\n```\r\n</details>\r\n\r\nIt is able to save once and then crashes.\r\n\r\nProabably a topic for @allenlavoie ?", "comments": ["Hrm, I thought we had a better exception here, namely this one: https://github.com/tensorflow/tensorflow/blob/cc0ad492d2752e9a271b6a6b902386458b39016a/tensorflow/python/training/tracking/util.py#L1812\r\n\r\nBut since CheckpointManager calls write() instead of save() (and write() does work on its own inside functions), we'll need to copy the exception over to CheckpointManager too. Thanks for the report.\r\n\r\nWe could implement CheckpointManager in terms of only TensorFlow operations so they work in a graph. Mostly we'd need ops for managing metadata and deleting old checkpoints. (I don't plan to work on this.)", "Glad to help.\r\nSo whats the proper way of saving checkpoints from within a `tf.function`? Only `write`?\r\n", "Yep, you can use `tf.train.Checkpoint.write` since that uses only TF operations. It won't update any protos and so `tf.train.latest_checkpoint` won't work. But the checkpoints themselves are fine.\r\n\r\nPersonally I'd checkpoint outside a `tf.function` at the moment.", "I was just thinking about checkpointing within `tf.function` because this is a thing:\r\nhttps://stackoverflow.com/questions/56038372/does-wrapping-tf-data-dataset-into-tf-function-improve-performance\r\n\r\nAnd when checkpointing outside I do not see how else to get the performance gain of `tf.data.Dataset` inside a `tf.function`.", "You're checkpointing multiple times when iterating over the same dataset? One option is to use a finite dataset that e.g. only does one epoch, then wrap that in a function. After the function runs one epoch you can save another checkpoint. That should have minimial overhead assuming epochs take at least a few minutes.\r\n\r\nIdeally saving would work inside a graph and update all of the metadata. But it's not something that has ever worked in 1.x, so we'll need to add several new ops.", "I am somtimes training on a very large dataset and I would like to have a checkpoint in case something goes wrong within an epoch.\r\nIf that is something for the future than maybe we should rename this issue to something more fitting so that future development can simply use this issue to track it.\r\n\r\nEdit:\r\n@allenlavoie \r\nMaybe it is possible to do a mixture of both `write` and `CheckpointManager`.\r\nOne could write checkpoints within `tf.function` calls and in-between epochs update the checkpoint state that the manager handles.\r\nThis could be implemented like so:\r\n```python\r\nimport tensorflow as tf\r\n\r\nsw = tf.summary.create_file_writer('models/test/')\r\n\r\ndata = tf.random.normal((1000, 10, 10, 1))\r\ndataset = tf.data.Dataset.from_tensors(data).batch(10)\r\n\r\n\r\nstep = tf.Variable(0)\r\n\r\nckpt = tf.train.Checkpoint(step=step)\r\nmgr = tf.train.CheckpointManager(ckpt, 'models/ckpt/', max_to_keep=20)\r\n\r\n\r\n@tf.function\r\ndef train_step(x):\r\n    return x * x * x\r\n\r\n\r\ndef train(dataset):\r\n    for epoch in range(10):\r\n        for x in dataset:\r\n            step.assign_add(1)\r\n            print(step)\r\n            print(ckpt.save_counter)\r\n            if tf.equal(step % 2, 0):\r\n                print('save')\r\n                mgr.write() # This basically does Checkpoint.write() and stores some information about that checkpoint in a variable\r\n        mgr.update_state() # Updates the checkpoint proto with the information gathered when using .write()\r\n        mgr.save() # Nothing changed here\r\n\r\ntrain = tf.function(train)\r\ntrain(dataset)\r\n```\r\n\r\nMaybe my understanding of how the manager works or how variables can be passed between eager and graph is limited but this would be kind of nice to have and does not sound to difficult.", "@sleighsoft Able to reproduce the issue with the first snippet code, the output is as below.\r\n\r\n--> 687       session.run(self._save_counter_assign)\r\n    688     if checkpoint_number is None:\r\n    689       checkpoint_number = save_counter\r\n\r\nAttributeError: 'NoneType' object has no attribute 'run'.\r\n\r\nAfter running the second snippet code, the output is as below.\r\n\r\n     32 train = tf.function(train)\r\n---> 33 train(dataset)\r\n\r\n35 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)\r\n    234       owner = inspect_utils.SuperWrapperForDynamicAttrs(owner)\r\n    235 \r\n--> 236     f = getattr(owner, f)\r\n    237 \r\n    238   if inspect_utils.isbuiltin(f):\r\n\r\nAttributeError: 'CheckpointManager' object has no attribute 'write'\r\n\r\nAs per your pervious comment you meant to say currently the checkpoints are not being updated in between epochs and so you are suggesting this feature?\r\n\r\n\r\n", "As @allenlavoie says https://github.com/tensorflow/tensorflow/issues/28523#issuecomment-490584848\r\nCheckpointManager does currently not support writing checkpoints in graph mode as there are some underlying C++ ops missing to do so.\r\nThe error you are getting is from my point of view basically some old tf1 code leftover copy&paste code.\r\nCorrect me if I am wrong.", "Unfortunately I won't have time to work on this.", "I think a workaround is to use tf.py_function inside tf.function to call CheckpointManager.save, right?", "Yeah, that may be a bit nicer than breaking up the function.", "@alextp Could you provide an example, please? The documentation of https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/py_function seems to be still related to 1.x", "@tf.function\ndef my_function(...):\n  ...\n  tf.py_function(lambda: checkpoint_manager.save(...), [], [])\n  ...\n\n?\n\n*From: *Julian Niedermeier <notifications@github.com>\n*Date: *Wed, May 15, 2019 at 10:43 AM\n*To: *tensorflow/tensorflow\n*Cc: *Alexandre Passos, Mention\n\n@alextp <https://github.com/alextp> Could you provide an example, please?\n> The documentation of\n> https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/py_function\n> seems to be still related to 1.x\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28523?email_source=notifications&email_token=AAABHRKWKM7S67TC3T4CUZDPVRDS3A5CNFSM4HLTBGA2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVPMO7I#issuecomment-492750717>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPFNJSTEVBPJ6O63TLPVRDS3ANCNFSM4HLTBGAQ>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp That's what I tried before I asked, but that throws:\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/user/miniconda3/envs/tf2-gpu-nightly/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 207, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"/home/user/miniconda3/envs/tf2-gpu-nightly/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 121, in __call__\r\n    self._convert(ret, dtype=self._out_dtypes[0]))\r\n\r\nIndexError: list index out of range\r\n\r\n\r\n2019-05-15 21:54:09.442581: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: IndexError: list index out of range\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/user/miniconda3/envs/tf2-gpu-nightly/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 207, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"/home/user/miniconda3/envs/tf2-gpu-nightly/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 121, in __call__\r\n    self._convert(ret, dtype=self._out_dtypes[0]))\r\n\r\nIndexError: list index out of range\r\n\r\n\r\n         [[{{node cond/then/_0/EagerPyFunc}}]]\r\n         [[ReduceDataset_1]]\r\nTraceback (most recent call last):\r\n  File \"chkpt_check.py\", line 31, in <module>\r\n    train(dataset)\r\n  File \"/home/user/miniconda3/envs/tf2-gpu-nightly/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 421, in __call__\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/user/miniconda3/envs/tf2-gpu-nightly/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1320, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/user/miniconda3/envs/tf2-gpu-nightly/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 578, in _filtered_call\r\n    (t for t in nest.flatten((args, kwargs), expand_composites=True)\r\n  File \"/home/user/miniconda3/envs/tf2-gpu-nightly/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 660, in _call_flat\r\n    outputs = self._inference_function.call(ctx, args)\r\n  File \"/home/user/miniconda3/envs/tf2-gpu-nightly/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 434, in call\r\n    ctx=ctx)\r\n  File \"/home/user/miniconda3/envs/tf2-gpu-nightly/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnknownError: IndexError: list index out of range\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/user/miniconda3/envs/tf2-gpu-nightly/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 207, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"/home/user/miniconda3/envs/tf2-gpu-nightly/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 121, in __call__\r\n    self._convert(ret, dtype=self._out_dtypes[0]))\r\n\r\nIndexError: list index out of range\r\n\r\n\r\n         [[{{node cond/then/_0/EagerPyFunc}}]]\r\n         [[ReduceDataset_1]] [Op:__inference_train_529]\r\n```\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nsw = tf.summary.create_file_writer('models/test/')\r\n\r\ndata = tf.random.normal((1000, 10, 10, 1))\r\ndataset = tf.data.Dataset.from_tensors(data).batch(10)\r\n\r\n\r\nstep = tf.Variable(0)\r\n\r\nckpt = tf.train.Checkpoint(step=step)\r\nmgr = tf.train.CheckpointManager(ckpt, 'models/ckpt/', max_to_keep=20)\r\n\r\n\r\n@tf.function\r\ndef train_step(x):\r\n    return x * x * x\r\n\r\n\r\ndef train(dataset):\r\n    for i in range(10):\r\n        for x in dataset:\r\n            step.assign_add(1)\r\n            print(step)\r\n            print(ckpt.save_counter)\r\n            if tf.equal(step % 2, 0):\r\n                print('save')\r\n                tf.py_function(lambda: mgr.save, [], [])\r\n\r\ntrain = tf.function(train)\r\ntrain(dataset)\r\n```", "Shouldn't that be `tf.py_function(mgr.save, [], [])` rather than with the extra lambda? Not related to the py_function-needs-an-output issue that looks like it's causing the exception. Something like this works for me as a workaround:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef main():\r\n  ckpt = tf.train.Checkpoint(v=tf.Variable(0.))\r\n  mgr = tf.train.CheckpointManager(ckpt, '/tmp/tfckpts', max_to_keep=5)\r\n\r\n  def save_and_return_something():\r\n    mgr.save()\r\n    return tf.ones([])\r\n\r\n  @tf.function\r\n  def train_fn():\r\n    data = tf.data.Dataset.range(10)\r\n    for item in data:\r\n      ckpt.v.assign_add(tf.cast(item, tf.float32))\r\n      tf.py_function(save_and_return_something, [], [tf.float32])\r\n\r\n  train_fn()\r\n\r\nif __name__ == '__main__':\r\n  main()\r\n```\r\n\r\nCould you file another bug for py_function not working without outputs? It looks like a trivial fix.", "Oh, it's because it returns a string. `tf.py_function(mgr.save, [], [tf.string])` works. ", "@allenlavoie Thanks for the code!\r\nI completely forgot the brackets or could have removed the lambda... (I am ashamed :D)", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!", "Working code example\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef main():\r\n  ckpt = tf.train.Checkpoint(v=tf.Variable(0.))\r\n  mgr = tf.train.CheckpointManager(ckpt, '/tmp/tfckpts', max_to_keep=5)\r\n\r\n  @tf.function\r\n  def train_fn():\r\n    data = tf.data.Dataset.range(10)\r\n    for item in data:\r\n      ckpt.v.assign_add(tf.cast(item, tf.float32))\r\n      tf.py_function(mgr.save, [], [tf.string])\r\n\r\n  train_fn()\r\n\r\nif __name__ == '__main__':\r\n  main()\r\n```"]}, {"number": 28522, "title": "Runs synchronous execution on NNAPI 1.2+", "body": "This commit updates tensorflow/lite/nnapi_delegate.cc to use synchronous execution(using ANeuralNetworksExecution_compute) on devices with NNAPI 1.2+.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28522) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 28520, "title": "RTX 2080ti bazel build: nvcc fatal: Unsupported gpu architecture 'compute_75' ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.4.0\r\n- Python version: 2.7.15\r\n- Bazel version (if compiling from source): 0.5.4\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: CUDA 8.0 cuDNN 6.0\r\n- GPU model and memory: RTX2080Ti 11G\r\n\r\n\r\n\r\n**Describe the problem**\r\ncommand\r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nError info\r\n```\r\nERROR: /home/googlecamp/.cache/bazel/_bazel_googlecamp/fe4eeee4bfb5545ecaf6693290f5b5f2/external/nccl_archive/BUILD:33:1:\r\n error while parsing .d file: /home/googlecamp/.cache/bazel/_bazel_googlecamp/fe4eeee4bfb5545ecaf6693290f5b5f2/execroot/org_tensorflow/bazel-out/local_linux-opt/bin/external/nccl_archive/_objs/nccl/external/nccl_archive/src/all_gather.cu.pic.d(No such file or directory).\r\nnvcc fatal   : Unsupported gpu architecture 'compute_75'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```\r\n\r\nI wonder whether it's due to the new architecture of RTX2018Ti.", "comments": ["Tensorflow 1.13 requires CUDA 10.0 and cuDNN 7.x.This may be because the old version of CUDA does not support new hardware.", "@Dinghow Did you get chance to upgrade Tensorflow to TF 1.13 as mentioned by rootkitchao , Please find the [link](https://www.tensorflow.org/install/source) for installation. Please let us know how it progresses. Thanks!", "> @Dinghow Did you get chance to upgrade Tensorflow to TF 1.13 as mentioned by rootkitchao , Please find the [link](https://www.tensorflow.org/install/source) for installation. Please let us know how it progresses. Thanks!\r\n\r\nThanks for your suggestion, but it's a pity that I must use tensorflow1.4 for a paper reproduction. And I think this problem is due to the compatibility for new GPU too, anyway, I'll try to edit the codes for TF1.3.", "@Dinghow Please let us know if this issue was resolved after lowering to TF 1.3. If its resolved we will close this issue.Thanks!", "> @Dinghow Please let us know if this issue was resolved after lowering to TF 1.3. If its resolved we will close this issue.Thanks!\r\n\r\nFine, I still encountered this problem with tf1.3, I finally used pip to install compiled tf1.4 without any errors.", "@Dinghow As this is resolved after pop install tf1.4, we will close this issue. Thanks!", "Thanks for your immediate response. In some cases, the TF installed by pip may not be suitable for the users' environment, they must build TF from source, so this issue is still existed, I hope anyone who has solved it totally can share their methods!", "I just ran into this issue. I had CUDA 9.0. I have RTX 2080\r\n\r\nI installed CUDA 10.0, as well as cuDNN 7.5, then build TensorFlow 1.13 with bazel 0.18.1 like\r\n\r\nbazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=cuda -k //tensorflow/tools/pip_package:build_pip_package\r\n\r\nIt built successfully (after hours)."]}, {"number": 28519, "title": "Bug in Keras Guide", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/guide/keras#multiple_gpus\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe last line of this guide produces a bug when ran on the associated colab notebook (maybe everywhere, I haven't checked).\r\n\r\n### Raises listed and defined\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1333     try:\r\n-> 1334       return fn(*args)\r\n   1335     except errors.OpError as e:\r\n\r\n19 frames\r\nInvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by {{node training/TFOptimizer/NcclAllReduce}}with these attrs: [reduction=\"sum\", shared_name=\"c0\", T=DT_FLOAT, num_devices=1]\r\nRegistered devices: [CPU, GPU, XLA_CPU, XLA_GPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[{{node training/TFOptimizer/NcclAllReduce}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1346           pass\r\n   1347       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1348       raise type(e)(node_def, op, message)\r\n   1349 \r\n   1350   def _extend_graph(self):\r\n\r\nInvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by node training/TFOptimizer/NcclAllReduce (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1254) with these attrs: [reduction=\"sum\", shared_name=\"c0\", T=DT_FLOAT, num_devices=1]\r\nRegistered devices: [CPU, GPU, XLA_CPU, XLA_GPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n```\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nNo, I'm only beginning in TF/Keras :) ", "comments": ["@pouannes Thanks for trying TF Keras colab. I was able to execute the colab successfully as is. Are you changing any parameter or using different TF version in the colab?", "I've checked again, the issue was that by habit I used a GPU in colab. Maybe it's not a bug anymore then? I don't really know, but you should be able to reproduce the exception by choosing a GPU as the hardware accelerator.", "I see. You have to install TF-gpu to use gpu accelerator. Closing this issue since it's not a bug and works in colab (without hardware accelerator). Thanks!"]}, {"number": 28518, "title": "Tensorflow v2 tensorflow.python.ops.linalg_ops.svd hangs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0a-gpu\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: RTX 2060\r\n\r\n**Describe the current behavior**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import linalg_ops\r\n\r\nmat = tf.random.uniform([2048, 2048])\r\n\r\ns, u, v = linalg_ops.svd(mat)\r\n```\r\nWhen running this example with my tensorflow-gpu installation it hangs (until I cancel with ctrl+c, which takes very long):\r\n```\r\n2019-05-08 13:20:05.994717: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-05-08 13:20:06.002398: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll\r\n2019-05-08 13:20:06.250271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties:\r\nname: GeForce RTX 2060 major: 7 minor: 5 memoryClockRate(GHz): 1.71\r\npciBusID: 0000:1f:00.0\r\ntotalMemory: 6.00GiB freeMemory: 4.89GiB\r\n2019-05-08 13:20:06.256846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0\r\n2019-05-08 13:20:06.803341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-08 13:20:06.806295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0\r\n2019-05-08 13:20:06.810187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N\r\n2019-05-08 13:20:06.812341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4618 MB memory) -> physical GPU (device: 0,\r\nname: GeForce RTX 2060, pci bus id: 0000:1f:00.0, compute capability: 7.5)\r\n2019-05-08 13:20:07.023250: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles\r\nfor stream 000001C2BFA888A0\r\nTraceback (most recent call last):\r\n  File \".\\svd_check.py\", line 6, in <module>\r\n    s, u, v = linalg_ops.svd(mat)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\linalg_ops.py\", line 418, in svd\r\n    tensor, compute_uv=compute_uv, full_matrices=full_matrices, name=name)\r\n  File \"C:\\Progams\\Miniconda\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_linalg_ops.py\", line 2409, in svd\r\n    \"full_matrices\", full_matrices)\r\nKeyboardInterrupt\r\n```\r\nOn tensorflow most recent v1 version it just works.\r\n\r\nAlso, when running the following it works as well:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import linalg_ops\r\n\r\nmat = tf.random.uniform([2048, 2048])\r\n\r\nwith tf.device('/device:CPU:0'):\r\n    s, u, v = linalg_ops.svd(mat)\r\n```", "comments": ["I tried running the two snippet on Colab using TensorFlow GPU version 2.0.0-alpha0 and noticed that both ran without error but there was difference in execution time.", "Hmm, strange. I don't know if it helps: Nvidia Driver Version 419.67", "@sleighsoft  You mentioned that it worked in most recent v1 version, Can you please confirm the version you used? Also, It was tf-gpu installation correct? In addition to that can you please upgrade your [nvidia driver version](https://www.nvidia.com/Download/index.aspx?lang=en-us#O) and test again. Thanks!", "Working on: tensorflow-gpu==1.13.1\r\n\r\nUpdated NVIDIA driver to 430.64 still not working", "I cannot reproduce. What I did is `pip install tf-nightly-gpu-2.0-preview` then run the python script mentioned above, it took some time to run but it did return. ", "@aaroey I did some more testing. Indeed, it finishes.\r\nBut there is a huge time difference between running SVD on CPU vs GPU.\r\n\r\nTested on Titan X Pascal and Intel(R) Core(TM) i7-6900K CPU @ 3.20GHz\r\n- On CPU: 2.8 seconds\r\n- On GPU: 60 seconds", "@smit-hinsu would you please help to take a look?", "SVD GPU kernel is known to be slower compared to the CPU version.\r\nSee https://github.com/tensorflow/tensorflow/issues/13603#issuecomment-418153277\r\n\r\nStill, it makes sense to see if there was any further regression between graph mode and eager mode or TF version 1.13 and 1.14.\r\n", "Duplicate of #13603\r\n\r\nI don't see any difference in runtime between eager and graph mode or between TensorFlow versions 1.13 and 1.14. All take around 63 seconds. However, I see a regression between versions 1.12 and 1.13. I will update the other bug.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28518\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28518\">No</a>\n"]}, {"number": 28517, "title": "Tensorflow failed to CMake due to Python module not found: tensorflow/contrib/tpu/ops on Windows server 2016", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n  Windows server 2016\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n  Source\r\n- TensorFlow version:\r\n  [d58b53e](https://github.com/tensorflow/tensorflow/commit/d58b53e19c3d06cea5909c66379d073aea153651) \r\n- Python version:\r\nAnaconda 4.1.1 (Python 2.7.14 64-bit)\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nVS2017 15.7.2\r\n- CUDA/cuDNN version:\r\nNVidia CUDA Toolkit 8.0\r\n NVidia CUDNN 5.1\r\n- GPU model and memory:\r\nN/A\r\n\r\nDescribe the problem:\r\nTensorflow failed to CMake due to Python module not found: tensorflow/contrib/tpu/ops. It can be reproduced on latest revision, could you please help take a look at this? Thanks!\r\n\r\nNote: \r\nWe did not find the ops in the D:\\Tensorflow\\src\\tensorflow\\contrib\\tpu path according to the error message. However, the ops path is at D:\\Tensorflow\\src\\tensorflow\\contrib\\tpu\\python\\ops.\r\n\r\nRepro steps:\r\n1. git clone https://github.com/tensorflow/tensorflow D:\\Tensorflow\\src\r\n2. pushd D:\\Tensorflow\r\n3. set PreferredToolArchitecture=x64\r\n4. set rel=Release\r\n5. set cmake_dir=D:\\Tensorflow\\src\\tensorflow\\contrib\\cmake\r\n6. set CUDNN_HOME=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2\\cuda\"\r\n7. set PY=C:\\ProgramData\\Anaconda3\r\n7. set _CL_=/FS\r\n8. mkdir build_x64   && pushd build_x64\r\n9. cmake D:\\Tensorflow\\src\\tensorflow\\contrib\\cmake -A x64 -DCMAKE_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=C:\\ProgramData\\Anaconda3\\python.exe -DPYTHON_LIBRARIES=C:\\ProgramData\\Anaconda3\\libs\\python36.lib -DSWIG_EXECUTABLE=D:\\Tensorflow\\swigwin-3.0.12\\swig.exe -Dtensorflow_BUILD_PYTHON_TESTS=ON -Dtensorflow_BUILD_SHARED_LIB=ON\r\n\r\n[log_x64_cmake.log](https://github.com/tensorflow/tensorflow/files/3156985/log_x64_cmake.log)\r\n\r\nError Message:\r\n[executing command] cmake D:\\Tensorflow\\src\\tensorflow\\contrib\\cmake -A x64 -DCMAKE_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=C:\\ProgramData\\Anaconda3\\python.exe -DPYTHON_LIBRARIES=C:\\ProgramData\\Anaconda3\\libs\\python36.lib -DSWIG_EXECUTABLE=D:\\Tensorflow\\swigwin-3.0.12\\swig.exe -Dtensorflow_BUILD_PYTHON_TESTS=ON -Dtensorflow_BUILD_SHARED_LIB=ON \r\n-- Building for: Visual Studio 15 2017\r\nCMake Warning at CMakeLists.txt:9 (message):\r\n  Your current cmake generator is set to use 32 bit toolset architecture.\r\n  This may cause \"compiler out of heap space\" errors when building.  Consider\r\n  using the flag -Thost=x64 when running cmake.  Ignore this if you are on\r\n  CMake GUI.\r\n-- Selecting Windows SDK version 10.0.17134.0 to target Windows 10.0.14393.\r\n-- The C compiler identification is MSVC 19.22.27706.96\r\n-- The CXX compiler identification is MSVC 19.22.27706.96\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64/cl.exe\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64/cl.exe -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64/cl.exe\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64/cl.exe -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed\r\n-- Performing Test MSVC_OPENMP_SUPPORT\r\n-- Performing Test MSVC_OPENMP_SUPPORT - Success\r\n-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED\r\n-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED - Success\r\n-- D:/Tensorflow/build_x64/abseil_cpp/src/abseil_cpp_build\r\n-- Found PythonInterp: C:/ProgramData/Anaconda3/python.exe (found version \"3.6.5\") \r\n-- Found PythonLibs: C:/ProgramData/Anaconda3/libs/python36.lib (found version \"3.6.5\") \r\nCMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/tpu/ops\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\n-- Found SWIG: D:/Tensorflow/swigwin-3.0.12/swig.exe (found version \"3.0.12\") \r\n-- Configuring incomplete, errors occurred!\r\nSee also \"D:/Tensorflow/build_x64/CMakeFiles/CMakeOutput.log\".\r\nSee also \"D:/Tensorflow/build_x64/CMakeFiles/CMakeError.log\".\r\n[command took 31 seconds] \r\n\r\n\r\n", "comments": ["@spacelg We encourage you to use bazel to build TF, unfortunately we do not support cmake builds for TF. Thanks!\r\n\r\nHere are the links to build TF ,\r\nBuild from source - https://www.tensorflow.org/install/source_windows\r\nBuild from pip - https://www.tensorflow.org/install/pip (select windows in step 1 and 2)\r\n\r\nPlease let us know how it progresses. Thanks!", "Hello, \r\nI need to generate a .sln file so that I can work with Visual Studio 2017. Which I dont think we can do using bazel.\r\nSince there is no folder \"ops\" under the tensorflow/contrib/tpu, I got failure when I configure.\r\n\r\nCould you point me to a resource showing how to use Tensorflow C++ API with Visual Studio 2017.\r\nThank you", "@rounakskm As your requirement is building Tensorflow C++ API with Visual Studio 2017, request you to please open a separate ticket for this.", "@spacelg As there is no response from your side , we will close this issue.\r\n@rounakskm Please open a new issue with your requirements so we will take it forward", "@rounakskm Are you able to compile with vs 2017 or 2019? For me, I am using vs2019, and it complains about `python module not found: tensorflow/contrib/rnn/kernels`.", "I am also using vs2019. and I am facing the following errors:\r\nPython proto directory not found: tensorflow/contrib/tpu/profiler\r\nPython module not found: tensorflow/contrib/rnn/ops\r\nPython module not found: tensorflow/contrib/tpu/profiler\r\n\r\nI checked those folders. In rnn kernels folder is missing also profiler folder is missing from tpu.\r\nDoes any body knows that whether these are moved to somewhere else?", "Same here, has someone figured it out?", "-- Selecting Windows SDK version 10.0.17763.0 to target Windows 10.0.18362.\r\n-- C:/Users/aurel/source/repos/tensorflow/tensorflow/contrib/cmake/build/abseil_cpp/src/abseil_cpp_build\r\nCMake Error at tf_python.cmake:132 (message):\r\n  Python proto directory not found: tensorflow/contrib/gdr\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:132 (message):\r\n  Python proto directory not found: tensorflow/contrib/mpi\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:132 (message):\r\n  Python proto directory not found: tensorflow/contrib/mpi_collectives\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:132 (message):\r\n  Python proto directory not found: tensorflow/contrib/tpu/profiler\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:132 (message):\r\n  Python proto directory not found: tensorflow/contrib/verbs\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/kafka/python\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/kafka/python/ops\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/ignite/python\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/ignite/python/ops\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/kinesis/python\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/kinesis/python/ops\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/mpi_collectives/python\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/mpi_collectives/python/ops\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/rnn/kernels\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/rnn/ops\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/tpu/profiler\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"C:/Users/aurel/source/repos/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/CMakeOutput.log\".\r\nSee also \"C:/Users/aurel/source/repos/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/CMakeError.log\".", "when i using vs2017+cmake+tf-r1.15 to making static lib,i encounter a  problem;\r\nCMake Error at tf_python.cmake:132 (message):\r\n  Python proto directory not found: tensorflow/contrib/tpu/profiler\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n"]}, {"number": 28516, "title": "Issue recognize_commands micro_speech demo", "body": "**System information**\r\nrun on SparkfunEdge Board Apollo3 operating in Burst Mode (96MHz)\r\n\r\nBin build on the following linux env:\r\nLinux edh-VirtualBox 4.18.0-17-generic #18.04.1-Ubuntu SMP Fri Mar 15 15:27:12 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\nTensorFlow installed from (source or binary): Git\r\nTensorFlow version: 1.13.1\r\nPython version:Python 2.7.15rc1\r\nInstalled using virtualenv? pip? conda?: pip\r\nBazel version (if compiling from source): No\r\nGCC/Compiler version (if compiling from source): gcc version 7.3.0 (Ubuntu 7.3.0-27ubuntu1~18.04)\r\nCUDA/cuDNN version: No\r\nGPU model and memory: No\r\n\r\n**Description**\r\nThe micro_speech example does not work.\r\nThe 'Yes' or 'No' are not regognized.\r\nIt seems a logic issue in the source \r\n\r\n> tensorflow/lite/experimental/micro/examples/micro_speech/recognize_commands.cc\r\n\r\nThe TF model returns a good score > 200 following call to` interpreter.Invoke() `and the vector output reurned by `interpreter.output(0)` is most of the time correct in the source \r\n\r\n> main.cc\r\n\r\n **BUT there is a average computation**  that breaks the results in the source \r\n\r\n> recognize_commands.cc\r\n\r\n\r\n\r\nWith the default configuration (`average_window_duration_ms = 1000`) , the average is computed on about 10 model outputs. \r\nThe issue is that when I say 'Yes', only 1or 2 model ouputs detect 'yes with a score > 200. So the average computed on 10 outputs provides a average score always smaller than the threshold equal to 200.\r\nI have tried to reduce the param `average_window_duration_ms = 300`, then the average is computed on 3 or 4 outputs but the result is still < 200.\r\nSo to solve the issue, **I have removed the average and then it works**.\r\n\r\nI don't understand the purpose of the average of the outputs. When I say 'yes', the model detects only one occurence of 'yes', There is no stuttering in the input data?\r\n\r\nCould you please confirm my analyse and my fix.\r\n\r\nThanks in advance\r\n\r\n\r\n\r\n", "comments": ["Hello, no update since one month.\r\nlite/micro speech project is aborted ?\r\nwhat means \"awaiting tensorflower\" ?\r\n\r\nHappy to contribute if i can help", "The averaging is used to try to balance the sensitivity and robustness of the speech detection. If no averaging is used (or the average window is very short), then it's more likely to detect spoken words, but also more likely to make mistakes. It will depend a lot on your application whether you care more about false positives or false negatives.\r\n\r\nWe are also aware of the broader issue that the overall accuracy of the yes/no model isn't as high as we'd like. We believe a lot of improvement can be had by gathering a larger training data set, and one way to help is by contributing your own voice here: https://aiyprojects.withgoogle.com/open_speech_recording", "Closing this for now, since there's been no updates for a while."]}, {"number": 28515, "title": "min/max data missing for FusedBatchNorm op ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): r1.13\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: GeForce 2080Ti\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI'm trying to use contrib/quantize module to perform quantize aware training and convert the trained model to quantize TFlite model. When run `converter.convert()`, the following error will be raised:\r\n```\r\ntensorflow/lite/toco/tooling_util.cc:1702] Array conv0/batch_normalization_v1/FusedBatchNorm, which is an input to the Conv operator producing the output array conv1/re_lu/Relu, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\nAborted (core dumped)\r\n```\r\n\r\nWhen adding ReLU op after the BN, it will be converted without the error massage. But in the tensorboard graph, I cannot see any differences on the FusedBatchNorm part with and without ReLU. \r\n**Describe the expected behavior**\r\nConvert to TFLite without error\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport os\r\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\r\n\r\n\r\ndef build_model(input):\r\n    '''build the model'''\r\n    with tf.name_scope('conv0'):\r\n        x = tf.keras.layers.Conv2D(24, kernel_size=(3, 3), padding='same', use_bias=False)(input)\r\n        x = tf.keras.layers.BatchNormalization(fused=True)(x)\r\n        #x = tf.keras.layers.ReLU()(x)\r\n    with tf.name_scope('conv1'):\r\n        x = tf.keras.layers.Conv2D(48, kernel_size=(3, 3), padding='same', use_bias=False)(x)\r\n        x = tf.keras.layers.BatchNormalization(fused=True)(x)\r\n        x = tf.keras.layers.ReLU()(x)\r\n    x = tf.concat((input, x), axis=-1)\r\n    with tf.name_scope('conv_out'):\r\n        x = tf.keras.layers.Conv2D(10, kernel_size=(1, 1), padding='same', use_bias=True)(x)\r\n        x = tf.keras.layers.ReLU()(x)\r\n    return x\r\n\r\n\r\n# build model\r\ntf.keras.backend.set_learning_phase(1)\r\ninput = tf.placeholder(tf.float32, shape=(None, None, None, 3))\r\nx = build_model(input)\r\n\r\n# quantize\r\ngraph = tf.get_default_graph()\r\ntf.contrib.quantize.create_training_graph(input_graph=graph, quant_delay=0)\r\n\r\n# save\r\nsaver = tf.train.Saver()\r\nwith tf.Session(graph=graph) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    saver.save(sess, './tmp/simple/model.ckpt')\r\n    writer = tf.summary.FileWriter('./tmp/simple/train/', sess.graph)\r\n    writer.flush()\r\n    writer.close()\r\n\r\n\r\n# eval\r\ntf.reset_default_graph()\r\ntf.keras.backend.set_learning_phase(0)\r\ninput = tf.placeholder(tf.float32, shape=(1, 32, 32, 3), name='input')\r\nx = build_model(input)\r\nx = tf.identity(x, 'output')\r\n\r\n# quantize\r\ngraph = tf.get_default_graph()\r\ninit_min=-6, init_max=6)\r\ntf.contrib.quantize.create_eval_graph(graph)\r\nsaver = tf.train.Saver()\r\nwith tf.Session(graph=graph) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    saver.restore(sess, tf.train.latest_checkpoint('./tmp/simple'))\r\n    writer = tf.summary.FileWriter('./tmp/simple/eval/', graph)\r\n    writer.flush()\r\n    writer.close()\r\n\r\n    # freeze graph\r\n    graph_def = graph.as_graph_def()\r\n    froze_graph = tf.graph_util.convert_variables_to_constants(sess, graph_def, ['output'])\r\n    tf.io.write_graph(froze_graph, './tmp/simple/', 'freeze_graph.pb')\r\n\r\n# convert to TFLite\r\ngraph_def_file = './tmp/simple/freeze_graph.pb'\r\ninput_array = [\"input\"]\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_array, ['output'],\r\n                                                      input_shapes={\"input\": [1, 32, 32, 3]})\r\nconverter.allow_custom_ops = True\r\nconverter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\nconverter.inference_input_type = tf.lite.constants.QUANTIZED_UINT8\r\nconverter.quantized_input_stats = {\"input\": (0., 255.)}\r\ntfmodel = converter.convert()\r\nopen(\"./tmp/simple/converted_model.tflite\", \"wb\").write(tfmodel)\r\n\r\n# load the TFLite\r\ninterpreter = tf.lite.Interpreter(model_path=\"./tmp/simple/converted_model.tflite\")\r\ninterpreter.allocate_tensors()\r\nprint(\"TFLite model loadded!!\")\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Looks like we have come across similar issue. Please have a look on #27952. Let us know if it is not the same. You can also have a look on #21725. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28515\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28515\">No</a>\n", "anyone can help? facing the same issue?\r\n", "@achandraa \r\nRecently I tried to do quantization aware training and do Toco converter to Tflite model with a network having batch_norm layers. but I got the error below.\r\n`\"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-08-23 09:53:37.518330: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 83 operators, 127 arrays (0 quantized)\r\n2019-08-23 09:53:37.518848: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 83 operators, 127 arrays (0 quantized)\r\n2019-08-23 09:53:37.520599: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 22 operators, 39 arrays (1 quantized)\r\n2019-08-23 09:53:37.521044: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 21 operators, 38 arrays (1 quantized)\r\n2019-08-23 09:53:37.521219: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 20 operators, 36 arrays (1 quantized)\r\n2019-08-23 09:53:37.521377: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 20 operators, 36 arrays (1 quantized)\r\n2019-08-23 09:53:37.521443: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 14 operators, 30 arrays (1 quantized)\r\n2019-08-23 09:53:37.521516: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 14 operators, 30 arrays (1 quantized)\r\n2019-08-23 09:53:37.521583: F tensorflow/lite/toco/tooling_util.cc:1709] Array layer_1/batch_normalization/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array layer_1/conv1, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\nFatal Python error: Aborted\r\n`\r\nIn this case I need to remain the accuracy of my model. So I think set default_ranges_min/max shouln not be used. Can you or anyone can help me to solve this?\r\n\r\n"]}, {"number": 28514, "title": "GoDoc link error", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/doc.go\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe link doesn't work. It strips the trailing `.md` off the URL when it is redirected through `tensorflow.org/code`\r\n\r\n\r\n### Correct links\r\n\r\nit is correct, I think the redirect is broken. for example, if you change \r\n`https://www.tensorflow.org/code/tensorflow/go/README.md` \r\nto\r\n`https://www.tensorflow.org/code/tensorflow/go/README.md.md` \r\n\r\nit works.\r\n\r\n### Parameters defined\r\n\r\nNA\r\n\r\n### Returns defined\r\n\r\nNA\r\n\r\n### Raises listed and defined\r\n\r\nNA\r\n### Usage example\r\n\r\nNA\r\n\r\n### Request visuals, if applicable\r\n\r\nNA\r\n\r\n### Submit a pull request?\r\n\r\ni don't think a PR is the solution here. if there is a better URL tho i'm happy to submit a PR.\r\n", "comments": ["@theonlydaleking Thanks for finding this link that is not working. I raised a PR to correct the link in the source. Thanks!", "I am closing this as this was resolved by the following https://github.com/tensorflow/tensorflow/pull/28946. Thanks!"]}, {"number": 28513, "title": "bazel test fails \"C++ compilation of rule '@protobuf_archive//:protoc_lib' failed (Exit 2): python.exe failed: error executing command\"", "body": "Env:\r\nOS: windows 10\r\nbazel version: 0.25\r\ntensorflow version:r1.14\r\npython:3.6.4\r\n\r\nI use  command to build .whl file is ok:\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\nbefore action:\r\nI modify third_party/gpus/crosstool/BUILD.tpl.  \r\ncc_toolchain(\r\n    name = \"cc-compiler-local\",\r\n    all_files = \"%{linker_files}\",\r\n    compiler_files = \":empty\",\r\n    #cpu = \"local\",    **#remove cpu attribute**\r\n    dwp_files = \":empty\",\r\n    linker_files = \"%{linker_files}\",\r\n    objcopy_files = \":empty\",\r\n    strip_files = \":empty\",\r\n    # To support linker flags that need to go to the start of command line\r\n    # we need the toolchain to support parameter files. Parameter files are\r\n    # last on the command line and contain all shared libraries to link, so all\r\n    # regular options will be left of them.\r\n    supports_param_files = 1,\r\n    toolchain_identifier = \"local_linux\",\r\n)\r\n\r\ncc_toolchain(\r\n    name = \"cc-compiler-darwin\",\r\n    all_files = \"%{linker_files}\",\r\n    compiler_files = \":empty\",\r\n    #cpu = \"darwin\", **#remove cpu attribute**\r\n    dwp_files = \":empty\",\r\n    linker_files = \"%{linker_files}\",\r\n    objcopy_files = \":empty\",\r\n    strip_files = \":empty\",\r\n    supports_param_files = 0,\r\n    toolchain_identifier = \"local_darwin\",\r\n)\r\nbecause there will make a exception to [#7075](https://github.com/bazelbuild/bazel/issues/7075) \r\n so remove the cpu attribute. \r\n\r\n**But**   when i run bazel test behind command occur exception:\r\nbazel test --noincompatible_disable_crosstool_file tensorflow/contrib/tensor_forest:model_ops_test\r\n\r\nerror:\r\n$ bazel test --noincompatible_disable_crosstool_file tensorflow/contrib/tensor_forest:model_ops_test\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nLoading:\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/contrib/tensor_forest\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/contrib/tensor_forest\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/contrib/tensor_forest\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/contrib/tensor_forest\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/contrib/tensor_forest\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/contrib/tensor_forest\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/contrib/tensor_forest\r\nAnalyzing: target //tensorflow/contrib/tensor_forest:model_ops_test (1 packages loaded, 0 targets configured)\r\nAnalyzing: target //tensorflow/contrib/tensor_forest:model_ops_test (10 packages loaded, 34 targets configured)\r\nAnalyzing: target //tensorflow/contrib/tensor_forest:model_ops_test (31 packages loaded, 847 targets configured)\r\nAnalyzing: target //tensorflow/contrib/tensor_forest:model_ops_test (83 packages loaded, 3373 targets configured)\r\nINFO: Analyzed target //tensorflow/contrib/tensor_forest:model_ops_test (84 packages loaded, 4238 targets configured).\r\nINFO: Found 1 test target...\r\n[0 / 15] [-----] BazelWorkspaceStatusAction stable-status.txt ... (4 actions, 0 running)\r\n[0 / 15] [-----] BazelWorkspaceStatusAction stable-status.txt ... (4 actions, 0 running)\r\nERROR: C:/users/.../7hwgx76p/external/protobuf_archive/BUILD:277:1: C++ compilation of rule '@protobuf_archive//:protoc_lib' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/.../_bazel_.../7hwgx76p/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\shared;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.10240.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\lib\\winv6.3\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x86;;C:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Python/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Python/Anaconda3/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\...\\AppData\\Local\\Temp\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TF_NEED_TENSORRT=0\r\n    SET TMP=C:\\Users\\...\\AppData\\Local\\Temp\r\n  C:/Python/Anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN /DHAVE_PTHREAD /wd4018 /wd4065 /wd4146 /wd4244 /wd4251 /wd4267 /wd4305 /wd4307 /wd4309 /wd4334 /wd4355 /wd4506 /wd4514 /wd4800 /wd4996 /Fobazel-out/x64_windows-opt/bin/external/protobuf_archive/_objs/protoc_lib/java_doc_comment.o /c external/protobuf_archive/src/google/protobuf/compiler/java/java_doc_comment.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nexternal/protobuf_archive/src/google/protobuf/compiler/java/java_doc_comment.cc(35): fatal error C1083: Cannot open include file: 'google/protobuf/compiler/java/java_doc_comment.h': No such file or directory\r\nTarget //tensorflow/contrib/tensor_forest:model_ops_test failed to build\r\nINFO: Elapsed time: 24.179s, Critical Path: 0.55s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n\r\n==============================\r\nhow to fix it?\r\n", "comments": ["@luorixiangyang  Did you solve this ?", "I modify tf_cc_test to cc_test to be ok.\r\ntensorflow/contrib/tensor_forest/BUILD\r\n\r\ncc_test(\r\n    name = \"model_ops_test\",\r\n    size = \"small\",\r\n    srcs = [\r\n        \"kernels/model_ops_test.cc\",\r\n        \"ops/model_ops.cc\",\r\n    ],\r\n    ......\r\n)", "Nit: When posting code, or errors, please wrap multiple lines of that with triple backquotes, so that formatting is nice and output is easy to read.", "TF 1.14 supports cuda 10.0. I see that you are using cuda 9.0 can you please switch to cuda 10.0 if this is still an issue. Thanks!"]}, {"number": 28512, "title": "AttributeError: module 'tensorflow' has no attribute 'placeholder'", "body": "Hello, dear guys.\r\n\r\nI build the tensorflow(CPU) version in Windows 10.\r\n\r\nI install tensorflow-2.0-beta.\r\n\r\nI want to run some tensorflow old-version code:\r\n```\r\n#\r\n#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image\r\n#  Copyright (C) 2017  Christian Zimmermann\r\n#  \r\n#  This program is free software: you can redistribute it and/or modify\r\n#  it under the terms of the GNU General Public License as published by\r\n#  the Free Software Foundation, either version 2 of the License, or\r\n#  (at your option) any later version.\r\n#  \r\n#  This program is distributed in the hope that it will be useful,\r\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\r\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\r\n#  GNU General Public License for more details.\r\n#  \r\n#  You should have received a copy of the GNU General Public License\r\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\r\n#\r\nfrom __future__ import print_function, unicode_literals\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport scipy.misc\r\nimport matplotlib.pyplot as plt\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\n\r\nfrom nets.ColorHandPose3DNetwork import ColorHandPose3DNetwork\r\nfrom utils.general import detect_keypoints, trafo_coords, plot_hand, plot_hand_3d\r\nimport datetime\r\n\r\nif __name__ == '__main__':\r\n    # images to be shown\r\n    image_list = list()\r\n    '''\r\n    image_list.append('./data_new/img.png')\r\n    image_list.append('./data_new/img2.png')\r\n    image_list.append('./data_new/img3.png')\r\n    image_list.append('./data_new/img4.png')\r\n    image_list.append('./data_new/img5.png')\r\n    '''\r\n    starttime = datetime.datetime.now()\r\n    image_list.append('./data_new/demo1.jpg')\r\n    # network input\r\n    image_tf = tf.placeholder(tf.float32, shape=(1, 240, 320, 3))\r\n    hand_side_tf = tf.constant([[1.0, 0.0]])  # left hand (true for all samples provided)\r\n    evaluation = tf.placeholder_with_default(True, shape=())\r\n\r\n    # build network\r\n    net = ColorHandPose3DNetwork()\r\n    hand_scoremap_tf, image_crop_tf, scale_tf, center_tf,\\\r\n    keypoints_scoremap_tf, keypoint_coord3d_tf = net.inference(image_tf, hand_side_tf, evaluation)\r\n\r\n    # Start TF\r\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\r\n    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r\n\r\n    # initialize network\r\n    net.init(sess)\r\n\r\n    # Feed image list through network\r\n    for img_name in image_list:\r\n        starttime = datetime.datetime.now()\r\n        image_raw = scipy.misc.imread(img_name)\r\n        image_raw = scipy.misc.imresize(image_raw, (240, 320))\r\n        image_v = np.expand_dims((image_raw.astype('float') / 255.0) - 0.5, 0)\r\n\r\n        hand_scoremap_v, image_crop_v, scale_v, center_v,\\\r\n        keypoints_scoremap_v, keypoint_coord3d_v = sess.run([hand_scoremap_tf, image_crop_tf, scale_tf, center_tf,\r\n                                                             keypoints_scoremap_tf, keypoint_coord3d_tf],\r\n                                                            feed_dict={image_tf: image_v})\r\n\r\n        hand_scoremap_v = np.squeeze(hand_scoremap_v)\r\n        image_crop_v = np.squeeze(image_crop_v)\r\n        keypoints_scoremap_v = np.squeeze(keypoints_scoremap_v)\r\n        keypoint_coord3d_v = np.squeeze(keypoint_coord3d_v)\r\n\r\n        # post processing\r\n        image_crop_v = ((image_crop_v + 0.5) * 255).astype('uint8')\r\n        coord_hw_crop = detect_keypoints(np.squeeze(keypoints_scoremap_v))\r\n        coord_hw = trafo_coords(coord_hw_crop, center_v, scale_v, 256)\r\n\r\n        # visualize\r\n        fig = plt.figure(1)\r\n        ax1 = fig.add_subplot(221)\r\n        ax2 = fig.add_subplot(222)\r\n        ax3 = fig.add_subplot(223)\r\n        ax4 = fig.add_subplot(224, projection='3d')\r\n        ax1.imshow(image_raw)\r\n        plot_hand(coord_hw, ax1)\r\n        ax2.imshow(image_crop_v)\r\n        plot_hand(coord_hw_crop, ax2)\r\n        ax3.imshow(np.argmax(hand_scoremap_v, 2))\r\n        plot_hand_3d(keypoint_coord3d_v, ax4)\r\n        ax4.view_init(azim=-90.0, elev=-90.0)  # aligns the 3d coord with the camera view\r\n        ax4.set_xlim([-3, 3])\r\n        ax4.set_ylim([-3, 1])\r\n        ax4.set_zlim([-3, 3])\r\n        endtime2 = datetime.datetime.now()\r\n        print(endtime2-starttime)\r\n        plt.show()\r\n\r\n```\r\n\r\nHowever, it reports the error:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"run.py\", line 43, in <module>\r\n>     image_tf = tf.placeholder(tf.float32, shape=(1, 240, 320, 3))\r\n> AttributeError: module 'tensorflow' has no attribute 'placeholder'\r\n\r\nI wonder how can I  make my old code compatible with new version software?\r\n\r\nThanks & Regards!", "comments": ["try running the `tf_upgrade_v2` script against the code to see what changes it suggest.", "Hello, @wdirons , William.\r\n\r\nThanks for your reply.\r\n\r\nHowever, when I run the script, some problems happen.\r\n\r\n> Traceback (most recent call last):\r\n>   File \"c:\\python\\python37\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n>     \"__main__\", mod_spec)\r\n>   File \"c:\\python\\python37\\lib\\runpy.py\", line 85, in _run_code\r\n>     exec(code, run_globals)\r\n>   File \"C:\\Python\\Python37\\Scripts\\tf_upgrade_v2.exe\\__main__.py\", line 9, in <module>\r\n>   File \"c:\\python\\python37\\lib\\site-packages\\tensorflow\\tools\\compatibility\\tf_upgrade_v2_main.py\", line 123, in main\r\n>     args.input_tree, output_tree, args.copy_other_files)\r\n>   File \"c:\\python\\python37\\lib\\site-packages\\tensorflow\\tools\\compatibility\\ast_edits.py\", line 624, in process_tree\r\n>     _, l_report, l_errors = self.process_file(input_path, output_path)\r\n>   File \"c:\\python\\python37\\lib\\site-packages\\tensorflow\\tools\\compatibility\\ast_edits.py\", line 494, in process_file\r\n>     temp_file)\r\n>   File \"c:\\python\\python37\\lib\\site-packages\\tensorflow\\tools\\compatibility\\ast_edits.py\", line 546, in process_opened_file\r\n>     lines = in_file.readlines()\r\n> UnicodeDecodeError: 'gbk' codec can't decode byte 0x98 in position 2352: illegal multibyte sequence\r\n\r\nI am not sure what type of language features in my old code will cause the error-report? How can I fix the problem?\r\nThanks & Regards!\r\n", "@momo1986 : Did you get chance to have a look on this [link](https://www.tensorflow.org/alpha/guide/migration_guide). Let us know if that helps. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "First, Try to uninstall any previous  Keras version and install Keras==2.3.1 using: \r\npip install git+git://github.com/keras-team/keras.git\r\n", "i am using windows 10\r\ntensorflow version 2.0\r\n\r\nfrom data_prepare import gen_vocab\r\nfrom data_prepare import gen_id_seqs\r\nfrom RNNLM import RNNLM\r\nimport os\r\n# It seems that there are some little bugs in tensorflow 1.4.1.\r\n# You can find more details in\r\n# https://github.com/tensorflow/tensorflow/issues/12414\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\nimport tensorflow as tf\r\n\r\n# Set TRAIN to true will build a new model\r\nTRAIN = True\r\n\r\n# If VERBOSE is true, then print the ppl of every sequence when we\r\n# are testing.\r\nVERBOSE = True\r\n\r\n# To indicate your test corpus\r\ntest_file = \"./gap_filling_exercise/gap_filling_exercise\"\r\n\r\nif not os.path.isfile(\"data/vocab\"):\r\n    gen_vocab(\"ptb/train\")\r\nif not os.path.isfile(\"data/train.ids\"):\r\n    gen_id_seqs(\"ptb/train\")\r\n    gen_id_seqs(\"ptb/valid\")\r\n\r\nwith open(\"data/train.ids\") as fp:\r\n    num_train_samples = len(fp.readlines())\r\nwith open(\"data/valid.ids\") as fp:\r\n    num_valid_samples = len(fp.readlines())\r\n\r\nwith open(\"data/vocab\") as vocab:\r\n    vocab_size = len(vocab.readlines())\r\n\r\ndef create_model(sess):\r\n    model = RNNLM(vocab_size=vocab_size,\r\n                  batch_size=64,\r\n                  num_epochs=80,\r\n                  check_point_step=100,\r\n                  num_train_samples=num_train_samples,\r\n                  num_valid_samples=num_valid_samples,\r\n                  num_layers=2,\r\n                  num_hidden_units=600,\r\n                  initial_learning_rate=1.0,\r\n                  final_learning_rate=0.0005,\r\n                  max_gradient_norm=5.0, \r\n                  )\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n    return model\r\n\r\nif TRAIN:\r\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.35)\r\n    with tf.session() as sess:\r\n        model = create_model(sess)\r\n\r\n        saver = tf.train.Saver()\r\n\r\n        model.batch_train(sess, saver)\r\n\r\ntf.compat.v1.reset_default_graph()\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.35)\r\nwith tf.compat.v1.Session() as sess:\r\n    model = create_model(sess)\r\n    saver = tf.compat.v1.train.Saver()\r\n    saver.restore(sess, \"model/best_model.ckpt\")\r\n    predict_id_file = os.path.join(\"data\", test_file.split(\"/\")[-1]+\".ids\")\r\n    if not os.path.isfile(predict_id_file):\r\n        gen_id_seqs(test_file)\r\n    model.predict(sess, predict_id_file, test_file, verbose=VERBOSE)\r\n\r\nand my error is:\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 53, in <module>\r\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.35)\r\nAttributeError: module 'tensorflow' has no attribute 'GPUOptions'\r\n", "kindly help me with the above error\r\n", "try this:\r\nimport keras.backend as K\r\ngpu_options=K. tf.GPUOptions(per_process_gpu_memory_fraction=0.35)"]}, {"number": 28511, "title": "Not found: Op type not registered 'StatefulPartitionedCall'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 9.9\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0\r\n- Python version: 3.5.3\r\n- Installed using virtualenv? pip? conda?: - \r\n- Bazel version (if compiling from source): 0.22\r\n- GCC/Compiler version (if compiling from source): clang from ./configure\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n\r\nI built libtensorflow_cc.so and libtensorflow_framework.so from source using bazel, CPU only. Using the resulting libraries I can compile my small c++ project just fine with cmake (requires some work with dependencies and headers).\r\n\r\nHowever when I load a model via LoadSavedModel into a SavedModelBundle I get the following output:\r\n\r\n2019-05-07 15:21:15.497043: I tensorflow/cc/saved_model/loader.cc:240] Loading SavedModel with tags: { serve }; from: /.../.../model_dir/autoencoder\r\n2019-05-07 15:21:15.511093: I tensorflow/cc/saved_model/loader.cc:289] SavedModel load for tags { serve }; Status: fail. Took 13982 microseconds.\r\nNot found: Op type not registered 'StatefulPartitionedCall' in binary running on ---. Make sure the Op and Kernel are registered in the binary running in this process.\r\n\r\nThe model has been trained in python and is a functional keras model. It was exported via keras.experimental.export_saved_model. My guess is that the 'StatefulPartitionedCall' is added by the export.\r\n\r\nWhen I compile my cpp using bazel inside the tensorflow repo everything goes fine. So is the build of  the shared libraries not up to date or am I doing something wrong?\r\n\r\n", "comments": ["I'm somewhat confused about the setup. You compiled with cmake and StatefulPartitionedCall isn't registered, and then at the same git commit you compiled with bazel and it worked?", "Both times I compiled with bazel from the same commit. Once the libtensorflow_cc.so with the command 'bazel build  //tensorflow:libtensorflow_cc.so'. The resulting shared library is then used, I don't think it matters in which build system. As far as I understand the library is missing the above mentioned op.\r\n\r\nBut when I compile my cpp directly with bazel inside the repo everything works out. My cpp is sitting in tensorflow/cc/examples/test.cpp with a BUILD file:\r\n \r\nload(\"//tensorflow:tensorflow.bzl\", \"tf_cc_binary\")\r\n\r\ntf_cc_binary(\r\n    name = \"test_cc\",\r\n    srcs = [\"test.cpp\"],\r\n    deps = [\r\n        \"//tensorflow/cc/saved_model:loader\",\r\n        \"//tensorflow/cc/saved_model:constants\",\r\n        \"//tensorflow/cc/saved_model:signature_constants\",\r\n        \"//tensorflow/cc/saved_model:tag_constants\",\r\n    ],\r\n)\r\n\r\nThe test.cpp is basically a one liner:\r\n\r\n#include \"tensorflow/cc/saved_model/loader.h\"\r\n#include \"tensorflow/cc/saved_model/tag_constants.h\"\r\n\r\nint main() {  \r\n\r\n    std::string model_path = \"/my/path/to/model\";\r\n    tensorflow::SavedModelBundle bundle;\r\n    \r\n    auto status = tensorflow::LoadSavedModel(\r\n\t\ttensorflow::SessionOptions(),\r\n\t\ttensorflow::RunOptions(),\r\n\t\tmodel_path,\r\n\t\t{tensorflow::kSavedModelTagServe},\r\n\t\t&bundle\r\n\t);\r\n\r\n    std::cout << status << std::endl;\r\n\r\n    return 0;\r\n}\r\n\r\nI just added deps to the BUILD until it worked, no clue what exactly is necessary. The point is that with this setup the StatefulPartitionedCall exists. \r\n\r\n ", "Sorry. Problem was on my side. I moved the resulting .so's which was apparently breaking everything. \r\n\r\nIt would be nice to have an explanation what these libraries are doing and needing somewhere in an official guide. \r\n\r\n I'll close the issue.\r\n"]}, {"number": 28510, "title": "PosixFileSystem::CopyFile: Truncate file before overwrite", "body": "Without O_TRUNC, the original content of the target file will be left\r\nover if the src file is shorter.\r\n\r\nFixes #28508", "comments": ["@tsawada could you please resolve the conflicts? Thanks!", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28510) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28510) for more info**.\n\n<!-- ok -->", "Oops, I force pushed a wrong branch with ill rebasing operation. I just did another force-push that is correctly rebased on the latest master.\r\nI can't remove reviewers as I lack such permission, but I appreciate if anyone can remove  alextp, azaks2, chsigg, frankchn, jhseu and saeta from the reviewers. Sorry for the spam.", "Let me know if I should make another rebase to fix failing builds. It seems those builds were broken on master when I did last rebase."]}, {"number": 28509, "title": "\u3010XLA on Centos\u3011poor performance improvement on Centos than  on Ubuntu", "body": "Linking to XLA libraries has been a default option since version 1.12 .It's a really great job!!\r\nI found training speed was faster, double to speed of tf 1.10, on Ubuntu system(48CPU,0 GPU).\r\n\r\nThen I tried to test this improvement on Centos where I found there is no improvement. I also tried to compile source code with XLA, but no improvement was found either.\r\n\r\nDoes XLA only support Ubuntu system? Or do I have configure anything special before compiling the source code?\r\n\r\nAdditional info:\r\nsource_code: tensorflow-1.12.0-rc2", "comments": ["Will it be possible to provide us the minimal reproducible code that can depict the performance improvement. Thanks!", "requirements: tensorpack / tqdm\r\n```python\r\n#!/usr/bin/env python\r\n# coding=utf8\r\n# File: test_speed.py\r\nimport tensorflow as tf\r\nimport tensorpack\r\nfrom tensorpack import *\r\nfrom tensorpack.dataflow import FakeData\r\nfrom tqdm import tqdm\r\n\r\nACTIONS = 2 # number of valid actions\r\ndef weight_variable(shape):\r\n    initial = tf.truncated_normal(shape, stddev = 0.01)\r\n    return tf.Variable(initial)\r\n\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.01, shape = shape)\r\n    return tf.Variable(initial)\r\n\r\ndef conv2d(x, W, stride):\r\n    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\r\n\r\ndef max_pool_2x2(x):\r\n    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\r\n\r\ndef createNetwork():\r\n    # network weights\r\n    W_conv1 = weight_variable([8, 8, 4, 32])\r\n    b_conv1 = bias_variable([32])\r\n\r\n    W_conv2 = weight_variable([4, 4, 32, 64])\r\n    b_conv2 = bias_variable([64])\r\n\r\n    W_conv3 = weight_variable([3, 3, 64, 64])\r\n    b_conv3 = bias_variable([64])\r\n\r\n    W_fc1 = weight_variable([1600, 512])\r\n    b_fc1 = bias_variable([512])\r\n\r\n    W_fc2 = weight_variable([512, ACTIONS])\r\n    b_fc2 = bias_variable([ACTIONS])\r\n\r\n    # input layer\r\n    s = tf.placeholder(\"float\", [None, 80, 80, 4])\r\n\r\n    # hidden layers\r\n    h_conv1 = tf.nn.relu(conv2d(s, W_conv1, 4) + b_conv1)\r\n    h_pool1 = max_pool_2x2(h_conv1)\r\n    print(\"size:{}\".format(h_pool1))\r\n\r\n    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 2) + b_conv2)\r\n    #h_pool2 = max_pool_2x2(h_conv2)\r\n\r\n    h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1) + b_conv3)\r\n    #h_pool3 = max_pool_2x2(h_conv3)\r\n\r\n    #h_pool3_flat = tf.reshape(h_pool3, [-1, 256])\r\n    h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])\r\n\r\n    h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\r\n\r\n    # readout layer\r\n    readout = tf.matmul(h_fc1, W_fc2) + b_fc2\r\n\r\n    return s, readout, h_fc1\r\n\r\ndef main():\r\n    config = tf.ConfigProto()\r\n    config.inter_op_parallelism_threads = 1\r\n    config.intra_op_parallelism_threads = 32\r\n    sess = tf.Session(config=config)\r\n    s, readout, h_fc1 = createNetwork()\r\n    a = tf.placeholder(\"float\", [None, ACTIONS])\r\n    y = tf.placeholder(\"float\", [None])\r\n    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)\r\n    cost = tf.reduce_mean(tf.square(y - readout_action))\r\n    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)\r\n    sess.run(tf.initialize_all_variables())\r\n\r\n\r\n    # test speed\r\n    BATCH_SIZE = 32\r\n    test_df = FakeData([[BATCH_SIZE, 80, 80, 4], [BATCH_SIZE, ACTIONS], [BATCH_SIZE]],\r\n                        dtype=['float32', 'int', 'float32'],\r\n                        size=1000,\r\n                        random=False)\r\n    test_df.reset_state()\r\n    for d in tqdm(test_df.get_data()):\r\n        sess.run(train_step, feed_dict={s:d[0],\r\n                                  a:d[1],\r\n                                  y:d[2]})\r\nif __name__ == '__main__':\r\n    main()\r\n```", "@sanjoy Can you please take a look? Thanks!\r\n", "How are you enabling XLA (XLA is not *enabled* by default and running a vanilla TF graph does not automatically run XLA.", "Well, I just tested training speed on a CPU machine(Ubuntu System) and a remark difference could be seen between r1.11 and  versions above r1.11\r\nI found that XLA had been a default option in [release log](https://github.com/tensorflow/tensorflow/releases?after=v1.12.0-rc1), then I guess the speed improvment was brought by this option.", "That release note is just saying that TensorFlow binaries have XLA linked in by default (so if you want to use XLA you don't have to rebuild TF).  But XLA is still not enabled by default so the speedup you saw was probably something else.\r\n\r\nI'm closing this issue now, can you please open a separate one if you want to pursue this further?", "Thanks, I will be opening another issue to pursue the reason."]}, {"number": 28508, "title": "gfile.Copy does not overwrite dest file properly on posix filesystems", "body": "**Describe the current behavior**\r\n`gfile.Copy(overwrite=True)` does not truncate the destination file before overwriting. That means if the src file is shorter than the dest file, the resulting dest file contains the mix of the two.\r\n\r\n\r\n**Describe the expected behavior**\r\n`gfile.Copy(overwrite=True)` results in having the exact same content of src file in the dest file.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n$ echo 'aaa' > a.txt\r\n$ echo 'bbbbbb' > b.txt\r\n$ python3 -c \"from tensorflow import gfile; gfile.Copy('a.txt', 'b.txt', overwrite=True)\"\r\n$ cat b.txt\r\naaa\r\nbb\r\n```\r\n\r\nTested with `pip3 install tensorflow==1.13.1`, python 3.5.2\r\n`b.txt` should have `aaa` as  the content, not `aaa\\nbb`.\r\n\r\nRef. https://www.tensorflow.org/api_docs/python/tf/gfile/Copy\r\n", "comments": ["CC @Joeper214 who found this issue", "@tsawada From the above referenced issue, can see a PR was raised, Can you please check if this is resolved after this change got merged.", "I built latest master `8a8a109e` and my `o_trunc` branch within `tensorflow/tensorflow:devel` docker container, and confirmed that the problem reproduces on the `master`, and not on `o_trunc`.", "Oh, the PR fixing this hasn't been yet merged (due to my commit causing a merge conflict, sorry about that)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28508\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28508\">No</a>\n"]}, {"number": 28507, "title": "when I run my model in gpudelegate in android , it build crash", "body": "here is my bug message:\r\n\r\nTfLiteGpuDelegate Prepare: Suitable node shader is not found: Output size is less than input size.Node number 129 (TfLiteGpuDelegate) failed to prepare.\r\n\r\nmy aar is tensorflow-lite-0.0.0-gpu-experimental.aar", "comments": ["@sunzhe09 This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "@sunzhe09 \r\nPlease fill the issue [template](https://github.com/keras-team/keras/issues/new?template=a--tensorflow-backend-users.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28507\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28507\">No</a>\n"]}]