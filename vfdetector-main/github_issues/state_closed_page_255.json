[{"number": 46812, "title": "    TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int64 of argument 'x'.", "body": "[](# -*- coding: utf-8 -*-\r\n\"\"\"Untitled42.ipynb\r\n\r\nAutomatically generated by Colaboratory.\r\n\r\nOriginal file is located at\r\n    https://colab.research.google.com/drive/1LJOb_cY2_aK9adr8i6jNn9QcFzwDZBS8\r\n\"\"\"\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport os\r\nimport random, re, math\r\nimport tensorflow as tf, tensorflow.keras.backend as K\r\nimport tensorflow_addons as tfa\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras import optimizers\r\nfrom tensorflow.keras.models import Sequential\r\nimport tensorflow.keras.layers as L\r\nfrom tensorflow.keras.applications import ResNet152V2, InceptionResNetV2, InceptionV3, Xception, VGG19\r\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D,GlobalMaxPooling2D\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom sklearn.model_selection import train_test_split\r\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\r\nfrom keras import regularizers\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\n!pip install efficientnet\r\nimport efficientnet.tfkeras as efn\r\n\r\nfrom google.colab import files\r\nfiles.upload()\r\n\r\nfrom google.colab import files\r\nfiles.upload()\r\n\r\n! mkdir -p ~/.kaggle\r\n! cp kaggle.json ~/.kaggle/\r\n#change the permission\r\n!chmod 600 ~/.kaggle/kaggle.json\r\n\r\n!kaggle datasets download -d andrewmvd/ocular-disease-recognition-odir5k\r\n\r\nfrom zipfile import ZipFile\r\nfile_name = \"ocular-disease-recognition-odir5k.zip\"\r\nwith ZipFile(file_name, 'r') as zip:\r\n  zip.extractall()\r\n  print('done')\r\n\r\nAUTO = tf.data.experimental.AUTOTUNE\r\ntry:\r\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n    print('Running on TPU ', tpu.master())\r\nexcept ValueError:\r\n    tpu = None\r\n\r\nif tpu:\r\n    tf.config.experimental_connect_to_cluster(tpu)\r\n    tf.tpu.experimental.initialize_tpu_system(tpu)\r\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\nelse:\r\n    strategy = tf.distribute.get_strategy()\r\n\r\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\r\n\r\nGCS_DS_PATH = '/content/ODIR-5K'\r\n\r\ntrain = pd.read_csv('/content/new_df_oc (1).csv')\r\ntrain_paths = train.filename.apply(lambda x: GCS_DS_PATH+ '/ODIR-5K/ODIR-5K/Training Images/' + x).values\r\ntrain_labels = train.target.values\r\n\r\ntrain_paths\r\n\r\ntrain.head(10)\r\n\r\ntrain=train.drop(columns=['D','C','A','M','G','O'],axis=1)\r\n\r\ntrain=train[((train['N']== 1) | (train['H'] == 1))]\r\n\r\ntrain\r\n\r\ntrain,valid = train_test_split(train,test_size = 0.2,random_state = 42)\r\n\r\nBATCH_SIZE = 8* strategy.num_replicas_in_sync\r\nimg_size = 512\r\nEPOCHS = 1\r\nSEED = 42\r\n\r\ndef decode_image(filename, label=None, image_size=(img_size,img_size)):\r\n    bits = tf.io.read_file(filename)\r\n    image = tf.image.decode_jpeg(bits, channels=3) \r\n    image = tf.image.resize(image, image_size)\r\n    image = tf.cast(image, tf.float32)\r\n    image = tf.image.per_image_standardization(image)\r\n    if label is None:\r\n        return image\r\n    else:\r\n        return image, label\r\n    \r\ndef preprocess(df,test=False):\r\n    paths = df.filename.apply(lambda x: GCS_DS_PATH + '/ODIR-5K/Training Images/' + x).values\r\n    labels = df.loc[:, ['N', 'H']].values\r\n    if test==False:\r\n        return paths,labels\r\n    else:\r\n        return paths\r\n    \r\ndef data_augment(image, label=None, seed=SEED):\r\n    image = tf.image.random_flip_left_right(image, seed=seed)\r\n    image = tf.image.random_flip_up_down(image, seed=seed)\r\n           \r\n    if label is None:\r\n        return image\r\n    else:\r\n        return image, label\r\n\r\ndef get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\r\n    rotation = math.pi * rotation / 180.\r\n    shear = math.pi * shear / 180.\r\n\r\n    c1 = tf.math.cos(rotation)\r\n    s1 = tf.math.sin(rotation)\r\n    one = tf.constant([1],dtype='float32')\r\n    zero = tf.constant([0],dtype='float32')\r\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\r\n\r\n    c2 = tf.math.cos(shear)\r\n    s2 = tf.math.sin(shear)\r\n    \r\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \r\n    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\r\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\r\n    \r\n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\r\n\r\ndef transform(image,label=None):\r\n    DIM = img_size\r\n    XDIM = DIM%2 \r\n    \r\n    rot = 15. * tf.random.normal([1],dtype='float32')\r\n    shr = 5. * tf.random.normal([1],dtype='float32') \r\n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\r\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\r\n    h_shift = 8. * tf.random.normal([1],dtype='float32') \r\n    w_shift = 8. * tf.random.normal([1],dtype='float32') \r\n  \r\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \r\n\r\n    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\r\n    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\r\n    z = tf.ones([DIM*DIM],dtype='int32')\r\n    idx = tf.stack( [x,y,z] )\r\n    \r\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\r\n    idx2 = K.cast(idx2,dtype='int32')\r\n    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\r\n              \r\n    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\r\n    d = tf.gather_nd(image,tf.transpose(idx3))\r\n    \r\n    if label is None:\r\n        return tf.reshape(d,[DIM,DIM,3])\r\n    else:\r\n        return tf.reshape(d,[DIM,DIM,3]),label\r\n\r\ntrain_dataset = (tf.data.Dataset\r\n    .from_tensor_slices(preprocess(train))\r\n    .map(decode_image, num_parallel_calls=AUTO)\r\n    #.map(data_augment, num_parallel_calls=AUTO)\r\n    .map(transform,num_parallel_calls=AUTO)\r\n    .shuffle(SEED)\r\n    .batch(BATCH_SIZE)\r\n    .repeat()\r\n    .prefetch(AUTO))\r\n\r\ntest_dataset= (tf.data.Dataset\r\n    .from_tensor_slices(preprocess(valid))\r\n    .map(decode_image, num_parallel_calls=AUTO)\r\n    .batch(BATCH_SIZE)\r\n    .cache()\r\n    .prefetch(AUTO))\r\n\r\nLR_START = 0.00001\r\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\r\nLR_MIN = 0.00001\r\nLR_RAMPUP_EPOCHS = 5\r\nLR_SUSTAIN_EPOCHS = 0\r\nLR_EXP_DECAY = .8\r\n\r\ndef lrfn(epoch):\r\n    if epoch < LR_RAMPUP_EPOCHS:\r\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\r\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\r\n        lr = LR_MAX\r\n    else:\r\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\r\n    return lr\r\n    \r\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\r\n\r\nrng = [i for i in range(EPOCHS)]\r\ny = [lrfn(x) for x in rng]\r\nplt.plot(rng, y)\r\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))\r\n\r\ndef categorical_focal_loss(gamma=2., alpha=.25):\r\n    def categorical_focal_loss_fixed(y_true, y_pred):\r\n        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\r\n        epsilon = K.epsilon()\r\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\r\n        cross_entropy = -y_true * K.log(y_pred)\r\n        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\r\n        return K.sum(loss, axis=1)\r\n    return categorical_focal_loss_fixed\r\n\r\nwith strategy.scope():\r\n    enet = efn.EfficientNetB7(input_shape=(img_size, img_size, 3),weights='noisy-student',include_top=False)\r\n\r\nwith strategy.scope():\r\n    enet.trainable = True\r\n\r\nwith strategy.scope():\r\n    ef7 =tf.keras.Sequential()\r\n    ef7.add(enet)\r\n    ef7.add(tf.keras.layers.MaxPooling2D())\r\n    ef7.add(tf.keras.layers.Conv2D(2048,3,padding='same'))\r\n    ef7.add(tf.keras.layers.BatchNormalization())\r\n    ef7.add(tf.keras.layers.ReLU())\r\n    ef7.add(tf.keras.layers.GlobalAveragePooling2D())\r\n    ef7.add(tf.keras.layers.Flatten())\r\n\r\n    ef7.add(tf.keras.layers.Dense(1024,activation='relu'))\r\n    ef7.add(tf.keras.layers.BatchNormalization())\r\n    ef7.add(tf.keras.layers.LeakyReLU())\r\n    ef7.add(tf.keras.layers.Dropout(0.25))\r\n\r\n    ef7.add(tf.keras.layers.Dense(512,activation='relu'))\r\n    ef7.add(tf.keras.layers.BatchNormalization())\r\n    ef7.add(tf.keras.layers.LeakyReLU())\r\n    ef7.add(tf.keras.layers.Dropout(0.15))\r\n    ef7.add(tf.keras.layers.Dense(2,activation='softmax'))\r\n    ef7.compile(\r\n                optimizer=tf.optimizers.Adam(lr=0.0001),\r\n                loss=categorical_focal_loss(gamma=2., alpha=.25),\r\n                metrics=['categorical_accuracy',\r\n                        tf.keras.metrics.Recall(),\r\n                        tf.keras.metrics.Precision(),   \r\n                        tf.keras.metrics.AUC(),\r\n                        tfa.metrics.F1Score(num_classes=2, average=\"macro\")\r\n                       ])\r\n\r\nh7=ef7.fit(\r\n    train_dataset,\r\n    steps_per_epoch=train_labels.shape[0] // BATCH_SIZE,\r\n    callbacks=[lr_callback],\r\n    epochs=EPOCHS)\r\n\r\n)\r\n\r\n\r\n\r\n\r\nTypeError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\r\n        return step_function(self, iterator)\r\n    <ipython-input-20-4c42ef206b9f>:6 categorical_focal_loss_fixed  *\r\n        cross_entropy = -y_true * K.log(y_pred)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\r\n        raise e\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\r\n        return func(x, y, name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1496 _mul_dispatch\r\n        return multiply(x, y, name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:518 multiply\r\n        return gen_math_ops.mul(x, y, name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:6078 mul\r\n        \"Mul\", x=x, y=y, name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\r\n        inferred_from[input_arg.type_attr]))\r\n\r\n    TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int64 of argument 'x'.](url)\r\n\r\n\r\n[Pls help me on this finding the solution of this error ](url)", "comments": ["@baliakanksha \r\nPlease provide access to the drive shared, here are similar issues #33077,[link](https://stackoverflow.com/questions/59541629/typeerror-input-y-of-mul-op-has-type-float32-that-does-not-match-type-int32)", "i have the same problem please help me", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46812\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46812\">No</a>\n"]}, {"number": 46811, "title": "Encountered \"Segmentation fault (core dumped)\" when convert keras model to tflite model", "body": "I\u2018m trying to convert a pytorch model (SSD) to tflite model. The model transformation path is: pytorch mode >> onnx model >> keras model >> tflite mode. \r\n\r\n### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 16.04**\r\n- TensorFlow installation (pip package or built from source): **pytorch1.6.0+cu101, onnx1.8.0, onnx2keras0.0.24, tensorflow2.3.0, cudatoolkit10.1, cudnn7.6.5** installed via \"**conda install**\" command\r\n\r\n\r\n### 2. Code\r\n\r\npytorch source code in [pytorch-ssd](https://github.com/qfgaohao/pytorch-ssd) and my converting code as follows:\r\n\r\n\r\n```\r\nfrom vision.ssd.mobilenet_v2_ssd_lite import create_mobilenetv2_ssd_lite\r\nimport os\r\nimport torch\r\nimport onnx\r\nimport tensorflow as tf\r\nfrom onnx2keras import onnx_to_keras\r\nimport numpy as np\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\n\r\ntorch_path = \" ... pretrained weights path ...\"\r\ntf_lite_path = \" ... output path ... \"\r\n\r\ncreate_net = lambda num: create_mobilenetv2_ssd_lite(num, is_test=True, device='cpu')\r\nnet = create_net(21)\r\n\r\nstate_dict = torch.load(torch_path, map_location=torch.device('cpu'))\r\nnet.load_state_dict(state_dict, strict=True)\r\n\r\nnet.eval()\r\nimage_size = 300\r\n\r\ninput_np = np.random.uniform(0, 1, (1, 3, image_size, image_size))\r\ninput = torch.FloatTensor(input_np)\r\n\r\ntorch.onnx.export(\r\n        model=net,\r\n        args=input,\r\n        f=\" ... onnx output path ... \",\r\n        export_params=True,\r\n        do_constant_folding=False\r\n        verbose=False,\r\n        input_names=['input'],\r\n        opset_version=10,\r\n        output_names=['output'])\r\n\r\nonnx_model = onnx.load(' .. onnx output path ... ')\r\nkeras_model = onnx_to_keras(onnx_model, ['input'])\r\n\r\nprint(\"======1\")\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\nconverter.experimental_new_converter = True\r\nprint(\"======2\")\r\n\r\ntflite_model = converter.convert() \r\nprint(\"======3\")\r\nwith open(tf_lite_path, 'wb') as f:\r\n        f.write(tflite_model)\r\nprint(\"=====4\")\r\n```\r\n\r\n\r\n\"====1\" and \"====2\" are printed normally, but Segmentation fault occured before \"====3\". No error messages are shown before Segmentation fault occured. Some of the log information is as follows (Total log over 10,000 lines):\r\n\r\n```\r\nTensor(\"inputs/0:0\", shape=(None, 3000, 2), dtype=float32) Tensor(\"inputs/1:0\", shape=(1, 3000, 2), dtype=float32)\r\nINFO:tensorflow:Assets written to: /tmp/tmpt6z4tlht/assets\r\nINFO:tensorflow:Assets written to: /tmp/tmpt6z4tlht/assets\r\nTensor(\"functional_1/444/FusedBatchNormV3:0\", shape=(None, 16, 150, 150), dtype=float32) Tensor(\"functional_1/446_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/469/BiasAdd:0\", shape=(None, 16), dtype=float32) Tensor(\"functional_1/471_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/522/BiasAdd:0\", shape=(None, 96), dtype=float32) Tensor(\"functional_1/524_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/558/BiasAdd:0\", shape=(None, 240), dtype=float32) Tensor(\"functional_1/560_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/595/BiasAdd:0\", shape=(None, 240), dtype=float32) Tensor(\"functional_1/597_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/615/FusedBatchNormV3:0\", shape=(None, 120, 19, 19), dtype=float32) Tensor(\"functional_1/617_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/637/BiasAdd:0\", shape=(None, 120), dtype=float32) Tensor(\"functional_1/639_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/653/FusedBatchNormV3:0\", shape=(None, 48, 19, 19), dtype=float32) Tensor(\"functional_1/655_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/661/FusedBatchNormV3:0\", shape=(None, 144, 19, 19), dtype=float32) Tensor(\"functional_1/663_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/683/BiasAdd:0\", shape=(None, 144), dtype=float32) Tensor(\"functional_1/685_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/699/FusedBatchNormV3:0\", shape=(None, 48, 19, 19), dtype=float32) Tensor(\"functional_1/701_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/746/FusedBatchNormV3:0\", shape=(None, 288, 19, 19), dtype=float32) Tensor(\"functional_1/748_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/768/BiasAdd:0\", shape=(None, 288), dtype=float32) Tensor(\"functional_1/770_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/784/FusedBatchNormV3:0\", shape=(None, 96, 10, 10), dtype=float32) Tensor(\"functional_1/786_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/792/FusedBatchNormV3:0\", shape=(None, 576, 10, 10), dtype=float32) Tensor(\"functional_1/794_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/814/BiasAdd:0\", shape=(None, 576), dtype=float32) Tensor(\"functional_1/816_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/830/FusedBatchNormV3:0\", shape=(None, 96, 10, 10), dtype=float32) Tensor(\"functional_1/832_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/839/FusedBatchNormV3:0\", shape=(None, 576, 10, 10), dtype=float32) Tensor(\"functional_1/841_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/861/BiasAdd:0\", shape=(None, 576), dtype=float32) Tensor(\"functional_1/863_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/877/FusedBatchNormV3:0\", shape=(None, 96, 10, 10), dtype=float32) Tensor(\"functional_1/879_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/899/BiasAdd:0\", shape=(None, 576), dtype=float32) Tensor(\"functional_1/901_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/914/FusedBatchNormV3:0\", shape=(None, 576, 10, 10), dtype=float32) Tensor(\"functional_1/916_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/921/BiasAdd:0\", shape=(None, 1280, 10, 10), dtype=float32) Tensor(\"functional_1/923_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/708/FusedBatchNormV3:0\", shape=(None, 288, 19, 19), dtype=float32) Tensor(\"functional_1/710_const2/Const:0\", shape=(), dtype=float32)\r\nTensor(\"functional_1/1115/mul:0\", shape=(None, 3000, 2), dtype=float32) Tensor(\"functional_1/1117_const2/Const:0\", shape=(1, 3000, 2), dtype=float32)\r\n2021-01-31 14:22:38.501393: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2021-01-31 14:22:38.501533: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2021-01-31 14:22:38.502841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:06:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.62GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2021-01-31 14:22:38.502879: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2021-01-31 14:22:38.502931: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2021-01-31 14:22:38.502954: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2021-01-31 14:22:38.502973: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2021-01-31 14:22:38.502993: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2021-01-31 14:22:38.503024: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2021-01-31 14:22:38.503046: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2021-01-31 14:22:38.504682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2021-01-31 14:22:38.504715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-31 14:22:38.504725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2021-01-31 14:22:38.504733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2021-01-31 14:22:38.506316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21686 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:06:00.0, compute capability: 7.5)\r\n2021-01-31 14:22:38.537184: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n2021-01-31 14:22:38.537202: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.006ms.\r\n2021-01-31 14:22:38.537219: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2021-01-31 14:22:40.299627: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n2021-01-31 14:22:40.299667: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\n2021-01-31 14:22:40.546372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:06:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.62GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2021-01-31 14:22:40.546422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2021-01-31 14:22:40.546476: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2021-01-31 14:22:40.546498: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2021-01-31 14:22:40.546519: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2021-01-31 14:22:40.546539: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2021-01-31 14:22:40.546558: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2021-01-31 14:22:40.546579: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2021-01-31 14:22:40.548140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2021-01-31 14:22:40.548184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-31 14:22:40.548194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2021-01-31 14:22:40.548202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2021-01-31 14:22:40.549800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21686 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:06:00.0, compute capability: 7.5)\r\nSegmentation fault (core dumped)\r\n```\r\n", "comments": ["Hi @ddddddreamcastle \r\n\r\nCould you share the generated keras model in a format the corresponding saved model directory before the TFLite convert API is invoked?\r\n\r\n```\r\nkeras_model = onnx_to_keras(onnx_model, ['input'])\r\nkeras_model.save(\"/tmp/saved_model_dir\")\r\n\r\nprint(\"======1\")\r\n```", "Could you try tf-2.4.1 and tf-nightly version for this script as well?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46810, "title": "StyleGAN2 is not converging in 4x RTX 3090", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): 2.4rc02\r\n- TensorFlow version (use command below): 2.x\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.1\r\n- GPU model and memory: 4x RTX 3090(24 GB)\r\n\r\n\r\n**Describe the current behavior**\r\nI have converted StyleGAN 2 official code(Written in Tensorflow 1.x) to TensorFlow 2.x. It is converging properly till some point, but then it started to decrease the quality. \r\n\r\nFirst I observed a mode collapsing then Eventually, the resulting images were a completely distorted type, kind of noise only. \r\nI'm using data having many categories.\r\n\r\nAnyone knows why it is happing and the solution. Thanks in advance.\r\n", "comments": ["@Thunder003,\r\nInstead of **TensorFlow 2.4rc02**, could you please install **TensorFlow v2.4.1** using the below command and check if it works.\r\n\r\n`pip install tensorflow==2.4.1`\r\n\r\nThanks!", "Also in order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46810\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46810\">No</a>\n"]}, {"number": 46809, "title": "[RNN] LSTM with TimeDistributed layer converts successfully but fails when invoking", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): tf-nightly\r\n\r\n### 2. Code\r\n\r\nhttps://colab.research.google.com/drive/1zgiZN6K1YsT70w-uWshBQA2ESuHSASn3?usp=sharing\r\n\r\n### 3. Failure after conversion\r\n\r\n- Model fails when invoking interpreter.\r\n- When set `batch_size` to 1 during building model, everything works. See `with_converted_lstm.tflite` for example.\r\n", "comments": ["@renjie-liu could you take a look at this?", "@haozha111 could you take a look at this? The problematic concatenation operator is being inserted by LowerStaticTensorList pass.\r\n\r\nIf it is an invalid transformation, it would be better to use flex fallback instead.", "I run the colab and check the converted tflite model, it contains the unidirectional lstm kernel, which suggests that the lower static tensorlist isn't the issue here. The converter will always try to convert to fuse kernels, and only convert to unfused if it fails to do so. Assigning the issue to @renjie-liu to take a further look.", "This should be related to\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/b757b8a8db085408829fb991042fa22bc89d7a8f/tensorflow/compiler/mlir/lite/transforms/lower_static_tensor_list.cc#L402-L419\r\n\r\nIR:\r\n```\r\n%0 = \"tf.Const\"() {value = dense<[-1, 3]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n%1 = \"tf.Const\"() {value = dense<[-1, 4]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n%2 = \"tf.Const\"() {value = dense<1.000000e+00> : tensor<f32>} : () -> tensor<f32>\r\n%3 = \"tf.Const\"() {value = dense<-1> : tensor<1xi32>} : () -> tensor<1xi32>\r\n%4 = \"tf.Const\"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>\r\n%5 = \"tf.Const\"() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>\r\n%6 = \"tf.Const\"() {value = dense<[1, 0, 2]> : tensor<3xi32>} : () -> tensor<3xi32>\r\n%7 = \"tf.Const\"() {value = dense<0> : tensor<i32>} : () -> tensor<i32>\r\n%8 = \"tf.Const\"() {value = dense<-1> : tensor<i32>} : () -> tensor<i32>\r\n%9 = \"tf.Identity\"(%2) {device = \"\"} : (tensor<f32>) -> tensor<f32>\r\n%10 = \"tf.Transpose\"(%arg0, %6) {device = \"\"} : (tensor<?x2x3xf32>, tensor<3xi32>) -> tensor<2x?x3xf32>\r\n%11 = \"tf.Shape\"(%10) {device = \"\"} : (tensor<2x?x3xf32>) -> tensor<3xi32>\r\n%12 = \"tf.StridedSlice\"(%11, %4, %5, %5) {begin_mask = 0 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 1 : i64} : (tensor<3xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>\r\n%13 = \"tf.TensorListReserve\"(%1, %12) {device = \"\"} : (tensor<2xi32>, tensor<i32>) -> tensor<!tf.variant<tensor<?x4xf32>>>\r\n```\r\n\r\n`element_shape` of `TensorListReserve` contains -1 and is rewritten to 1, but that -1 should be `batch_size * lstm_units` instead.", "https://github.com/tensorflow/tensorflow/blob/b757b8a8db085408829fb991042fa22bc89d7a8f/tensorflow/compiler/mlir/lite/transforms/lower_static_tensor_list.cc#L425\r\n\r\nFrom the code here, we only rewrite the first dimension in `element_shape` from -1 to 1, so it won't affect other dimensions in the `element_shape`.", "I think that -1 is exactly the first dimension?", "> I think that -1 is exactly the first dimension?\r\n\r\nthe element_shape is [-1, 4], and this piece of code changes it to [1, 4], which assumes the batch dim is 1.\r\n\r\nIn your model code:\r\ninputs = tf.keras.Input(shape=(10, 2, 3))\r\nlstm = tf.keras.layers.LSTM(4)\r\noutputs = tf.keras.layers.TimeDistributed(lstm)(inputs)\r\nmodel = tf.keras.Model(inputs, outputs)\r\n\r\nI see a few potential issues:\r\n1) `tf.keras.layers.LSTM` expects input shape of [batch, timesteps, features], then you applied `TimeDistributed` layer on top of this LSTM layer. According to https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed, this doesn't look quite right to me. Since `tf.keras.layers.LSTM` already process for each time step, and then you apply TimeDistributed layer again.\r\n2) Could you explain what's the dimension in your input shape (10, 2, 3)? IIUC, 10 is the time steps, but what are 2, 3 for? Isn't LSTM usually only expects only one feature dimension?", "Sorry that it is not a good example. The usage is extracted from a private repo, and is simplied to common used API. Unfortunately, it does not match the example I post here, and I'm afraid that I cannot give more details. However, it basically contains a nested `tf.map_fn`  and some reshape ops are applied resulting in the fact that first dimension is not batch size.", "Thanks. If you could provide a very simple repro of the `tf.map_fn` issue, that will be great. Otherwise you could refer to this post to see if it can resolve your issue:\r\nhttps://github.com/tensorflow/tensorflow/issues/40221#issuecomment-656289196\r\n\r\n(Temporarily closed because we can't exactly reproduce the issue you have, please feel free to reopen if you find a good repro case, thanks)."]}, {"number": 46808, "title": "Multi GPU training - works on Intel CPU but failed on AMD CPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary):binary and source with -march=native\r\n- TensorFlow version (use command below):2.4\r\n- Python version:3.6.9\r\n- Bazel version (if compiling from source):3.1.0\r\n- GCC/Compiler version (if compiling from source):7.5.0\r\n- CUDA/cuDNN version:11.0/8.0.4\r\n- GPU model and memory: 2x GTX1080  Titan/12GB\r\n\r\n**Describe the current behavior**\r\nWith the standard tensorflow-gpu 2.4 distribution installed using\r\n- pip3 install tensorflow-gpu==2.4\"\r\n\r\nWhen the same script is executed on the following two systems:\r\n- System 1: Intel CPU i7 6850k / 2x Titan (Pascal) 12GB / Ubuntu 18.04\r\n- System 2: AMD Threadripper 1950x / 2x Titan (Pascal) 12GB / Ubuntu 18.04\r\n\r\nThe script works on System 1 but stuck on System 2. The script is stuck on the following line with both GPU at 100% utilization:\r\n```\r\nmodel.fit(train_dataset, epochs=12, callbacks=callbacks)\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe multi-gpu training should work on the systems with AMD cpus. System can run the same script successfully with only one GPU assigned to the runtime (export CUDA_VISIBLE_DEVICES=\"0\"). The problem might be CPU related. \r\n\r\nI have also tried Tensorflow compiled from source (version 2.4/GPU with -march=native) and the results are the same (stuck).\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/tutorials/distribute/keras.ipynb\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nSystem 1 (Intel CPU) runs the entire script successfully:\r\n```\r\nEpoch 1/12\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n  3/469 [..............................] - ETA: 2:40 - loss: 2.2512 - accuracy: 0.1276WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0065s vs `on_train_batch_begin` time: 0.0727s). Check your callbacks.\r\nWARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0065s vs `on_train_batch_begin` time: 0.0727s). Check your callbacks.\r\nWARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_train_batch_end` time: 0.0402s). Check your callbacks.\r\nWARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_train_batch_end` time: 0.0402s). Check your callbacks.\r\n```\r\n\r\nSystem 2 (AMD Threadripper CPU) gets stuck on the model.fit line:\r\n```\r\nmodel.fit(train_dataset, epochs=12, callbacks=callbacks)\r\nEpoch 1/12\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n```\r\n", "comments": ["Hi @scotthong, in system 2 you have 2 GPUS? And it runs okay when you just use one of those GPUs, correct? But if you try to use both GPUs, the program hangs (with both GPUs at full utilization). Have I understood correctly?\r\n\r\nTo narrow down the issue, on system two can you try the following experiments:\r\n1. remove all callbacks and try (to see if the issue is with checkpointing or tensorboard etc)\r\n2. try `strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute. ReductionToOneDevice())` to see if NCCL is causing problems.", "Hi @nikitamaia, \r\nThe program runs ok on one of the GPU (export CUDA_VISIBLE_DEVICES=\"0\" or \"1\"). I can also create two virtual GPUs on one physical GPU and it also runs okay.\r\n\r\nThe program is updated as suggested and the program still hangs. The training can complete one training step or sometimes a couple of more steps but it still hangs. When the process is killed forcefully (kill -9 pid#), it also freezes up the x-server (ubuntu 18.04).  When the default MirroredStrategy() is used, the process hangs (100% GPU utilization) but it can be killed without freezing up the xserver.\r\n```\r\nstrategy = tf.distribute.MirroredStrategy()\r\n```\r\n\r\n```\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nimport os\r\n\r\ndef scale(image, label):\r\n  image = tf.cast(image, tf.float32)\r\n  image /= 255\r\n  return image, label\r\n\r\ndef decay(epoch):\r\n  if epoch < 3:\r\n    return 1e-3\r\n  elif epoch >= 3 and epoch < 7:\r\n    return 1e-4\r\n  else:\r\n    return 1e-5\r\n\r\nprint(tf.__version__)\r\ndatasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\nmnist_train, mnist_test = datasets['train'], datasets['test']\r\n\r\n# strategy = tf.distribute.MirroredStrategy()\r\nstrategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice())\r\n\r\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n\r\nnum_train_examples = info.splits['train'].num_examples\r\nnum_test_examples = info.splits['test'].num_examples\r\n\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE_PER_REPLICA = 64\r\nBATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n\r\ntrain_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\neval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)\r\n\r\nwith strategy.scope():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10)\r\n  ])\r\n\r\n  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                optimizer=tf.keras.optimizers.Adam(),\r\n                metrics=['accuracy'])\r\n\r\nmodel.fit(train_dataset, epochs=12, callbacks=[tf.keras.callbacks.LearningRateScheduler(decay)])\r\n```\r\nSave this program as \"multi_gpus.py\" and run it using the following commands:\r\n```\r\n# run on GPU 0 only\r\nexport CUDA_VISIBLE_DEVICES=\"0\"\r\npython3 multi_gpus.py\r\n\r\n# run on GPU 1 only\r\nexport CUDA_VISIBLE_DEVICES=\"1\"\r\npython3 multi_gpus.py\r\n\r\n# run on both GPU 0 and 1\r\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\r\npython3 multi_gpus.py\r\n```\r\n", "Any chance you can get the stack trace from a stuck process (using gdb or some such tool)?", "Hi Sanjoy,\r\n\r\nI've searched similar issues related to Threadripper and Multiple GPUs as reported in the following two threads.\r\n\r\nhttps://github.com/apache/incubator-mxnet/issues/12982\r\nhttps://github.com/pytorch/pytorch/issues/1637\r\n\r\nThe gdb debug output doesn't seem to give any useful information. \r\n\r\nThe good news is that the problem can be resolved by disabling IOMMU in the BIOS regardless of the NUMA configuration.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46808\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46808\">No</a>\n"]}, {"number": 46807, "title": "ImageDataGenerator - Caching not effective when using tf.data.Dataset.from_generator", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro Linux 20.2.1\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: 11.0\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen creating a dataset generator using ImageDataGenerator, as the documentation specifies it:\r\n```\r\nThe data will be looped over (in batches).\r\n```\r\nThe dataset generator is then used in `tf.data.Dataset.from_generator`, on which is then called `.cache()`: we have a problem here, since the `cache()` function caches the data, as the documentation specifies it:\r\n```\r\nThe first time the dataset is iterated over, its elements will be cached either in the specified file or in memory.\r\nSubsequent iterations will use the cached data.\r\n```\r\nThe problem is that the dataset never ends since this is the way ImageDataGenerator works.\r\n\r\n**Describe the expected behavior**\r\n\r\nThere should be a callback or something telling `tf.data.Dataset` that the dataset is OVER when getting the last batch each time, so that data can be cached properly.", "comments": ["@BlueskyFR,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46807\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46807\">No</a>\n"]}, {"number": 46805, "title": "Shuffle Buffer Filled", "body": "When I run the model_main_tf2.py script to train a centernet_resnet50_v1_fpn_512x512 model with a dataset that is of the size 25GB in the tfrecord format.\r\n\r\nIs the dataset the main reason for this issue?\r\n\r\nthe error is\r\n\r\n2021-01-30 19:37:30.641972: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-01-30 19:37:30.642004: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2021-01-30 19:37:41.087656: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-30 19:37:41.087854: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2021-01-30 19:37:41.087870: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-01-30 19:37:41.087898: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kingsman-desktop): /proc/driver/nvidia/version does not exist\r\n2021-01-30 19:37:41.088899: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nWARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\nW0130 19:37:41.089709 140284268705600 cross_device_ops.py:1321] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\nI0130 19:37:41.089887 140284268705600 mirrored_strategy.py:350] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\nINFO:tensorflow:Maybe overwriting train_steps: None\r\nI0130 19:37:41.093331 140284268705600 config_util.py:552] Maybe overwriting train_steps: None\r\nINFO:tensorflow:Maybe overwriting use_bfloat16: False\r\nI0130 19:37:41.093431 140284268705600 config_util.py:552] Maybe overwriting use_bfloat16: False\r\nWARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py:523: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrename to distribute_datasets_from_function\r\nW0130 19:37:43.446819 140284268705600 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py:523: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrename to distribute_datasets_from_function\r\nINFO:tensorflow:Reading unweighted datasets: ['/home/kingsman/deepl/records/train.tfrecord']\r\nI0130 19:37:43.468255 140284268705600 dataset_builder.py:163] Reading unweighted datasets: ['/home/kingsman/deepl/records/train.tfrecord']\r\nINFO:tensorflow:Reading record datasets for input file: ['/home/kingsman/deepl/records/train.tfrecord']\r\nI0130 19:37:43.468415 140284268705600 dataset_builder.py:80] Reading record datasets for input file: ['/home/kingsman/deepl/records/train.tfrecord']\r\nINFO:tensorflow:Number of filenames to read: 1\r\nI0130 19:37:43.468508 140284268705600 dataset_builder.py:81] Number of filenames to read: 1\r\nWARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\r\nW0130 19:37:43.468611 140284268705600 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\r\nWARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\nW0130 19:37:43.471664 140284268705600 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\nWARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map()\r\nW0130 19:37:43.492702 140284268705600 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map()\r\nWARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\nW0130 19:37:51.294010 140284268705600 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\nWARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/inputs.py:281: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\nW0130 19:37:54.907917 140284268705600 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/inputs.py:281: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\n2021-01-30 19:37:58.317565: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-01-30 19:37:58.341385: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3399920000 Hz\r\n2021-01-30 19:38:09.704606: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 68 of 2048\r\n2021-01-30 19:38:18.681284: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 162 of 2048\r\n2021-01-30 19:38:28.579701: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 314 of 2048\r\n2021-01-30 19:38:34.514916: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:230] Shuffle buffer filled.\r\nTraceback (most recent call last):\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 2113, in execution_mode\r\n    yield\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 733, in _next_internal\r\n    output_shapes=self._flat_output_shapes)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2579, in iterator_get_next\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input is empty.\r\n\t [[{{node case/cond/else/_10/case/cond/cond_jpeg/else/_105/case/cond/cond_jpeg/decode_image/DecodeImage}}]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]] [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"object_detection/model_main_tf2.py\", line 113, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"object_detection/model_main_tf2.py\", line 110, in main\r\n    record_summaries=FLAGS.record_summaries)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py\", line 566, in train_loop\r\n    unpad_groundtruth_tensors)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py\", line 344, in load_fine_tune_checkpoint\r\n    features, labels = iter(input_dataset).next()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 628, in next\r\n    return self.__next__()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 632, in __next__\r\n    return self.get_next()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 663, in get_next\r\n    self._iterators[i].get_next_as_list_static_shapes(new_name))\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 1619, in get_next_as_list_static_shapes\r\n    return self._format_data_list_with_options(self._iterator.get_next())\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\", line 585, in get_next\r\n    result.append(self._device_iterators[i].get_next())\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 800, in get_next\r\n    return self._next_internal()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 739, in _next_internal\r\n    return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 2116, in execution_mode\r\n    executor_new.wait()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/executor.py\", line 69, in wait\r\n    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input is empty.\r\n\t [[{{node case/cond/else/_10/case/cond/cond_jpeg/else/_105/case/cond/cond_jpeg/decode_image/DecodeImage}}]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]]\r\n", "comments": ["@2vin2vin \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46804, "title": "Expose TString related C API #46803", "body": "", "comments": ["@Oceania2018  Can you please address Ubuntu Sanity errors? Thanks!", "Hi @mihaimaruseac, when whill this PR be merged?", "This fails to build internally and needs manual import and fixing.\r\n\r\nSadly, I cannot do this at this time, busy with other things. Will likely be able to next week", "@mihaimaruseac Do you have a chance to merge it?", "@mihaimaruseac Sorry to bother you again, May I know when this PR will be merged? This is changes will fix the TString API for other languages binding like TensorFlow .NET since tensorflow v2.4 made a breaking change for string api.", "Hi.\r\n\r\nApologies for the delay. Been quite busy with the upcoming release and didn't get a chance to look into this.\r\n\r\nShould be merged tomorrow."]}, {"number": 46802, "title": "Get path from environment CUDA_PATH ", "body": "**Problem**:\r\nWhen I tried to compile TensorFlow on Arch Linux which CUDA is installed at `/opt/cuda`, `ldconfig` can only locate to its sub-folders. I referred to the official packaging script by Arch Linux and it exports `TF_CUDA_PATH` to bypass this issue, which should not be a permanent solution.\r\n**About this PR**:\r\nPreviously only Windows loads path from environment variable `CUDA_PATH`, but this is also a common variable that most CUDA packages on *nix distributions will export. \r\nThe code will append `CUDA_PATH` value to the search path list, so users who can configure it well originally should not be affected, and will provide convenience to newcomers who's trying to install TensorFlow on their machine.", "comments": ["I am relatively a new contributor here but I think the `nvcc_version` should be retained despite the pylint warning as it ensures easy readability as to what actually is expected of the function to return.\n\nWhat do you think ?", "\r\n\r\n\r\n> I am relatively a new contributor here but I think the `nvcc_version` should be retained despite the pylint warning as it ensures easy readability as to what actually is expected of the function to return.\r\n> \r\n> What do you think ?\r\n\r\nI think it is distracting to introduce unneeded variables inside functions, which in turn complicates readability of the code. \r\n", "Hi Jiang, thanks for your suggestion. \r\n\r\nCould you use `CUDA_TOOLKIT_PATH` instead?\r\n\r\nI know, `CUDA_PATH` is the standard env variable, but this whole business of finding the local CUDA installation is already complicated enough. We would like to simplify it by requiring a single CUDA toolkit directory, similar to how it's done for TFRT. I just never get to it.\r\n\r\nThanks, and sorry for causing you trouble.", "Thanks, I would try your suggestion, and close this PR."]}, {"number": 46801, "title": "thankyou", "body": "thankyou all of you .\r\nI have solved my error by just installing tensorflow==1.5\r\nby the command \"pip install tensorflow==1.5\"", "comments": ["Glad the issue is resolved, moving this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46801\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46801\">No</a>\n"]}, {"number": 46799, "title": "Failed when trying to install TensorFlow 2.x into my laptop. Ubuntu 18.04(Desktop). From source", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04(Desktop)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): Source code\r\n- TensorFlow version: 2.2\r\n- Python version: 3.7.5\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): clang 11.0.0\r\n- CUDA/cuDNN version: CUDA 11.2 / cuDNN 8.0.4\r\n- GPU model and memory: GTX 1050 ti / 4 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nHey, guys. I'm trying to install TensorFlow into my laptop(Dell G3 3579) but failed. There is the error information list below(**Any other info / logs**).\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```bash\r\nsudo bazel build --config=opt --config=v2  --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\n```bash\r\n(base) river@river-G3-3579:~/Downloads/Ubuntu_installation_essential_components/tensorflow$ sudo bazel build --config=opt --config=v2  --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n[sudo] password for river: \r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda_clang, using_cuda, v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=141\r\nINFO: Reading rc options for 'build' from /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.7/site-packages --python_path=/usr/local/bin/python3 --config=xla --config=tensorrt --action_env TF_CUDA_VERSION=11.2 --action_env TF_CUDNN_VERSION=8 --action_env TF_TENSORRT_VERSION=7 --action_env TF_NCCL_VERSION=2.7 --action_env TF_CUDA_PATHS=/usr/local/cuda-11.2/,/usr/local/cuda-11.2/targets/x86_64-linux/include/,/usr/local/include/,/usr/local/lib/,/home/river/Documents/OS_unitility/TensorRT/include/,/home/river/Documents/OS_unitility/TensorRT/util --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --action_env LD_LIBRARY_PATH=:/usr/local/jdk1.8/lib:/usr/local/jdk1.8/jre/lib:/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/extras/Debugger/lib64:/usr/local/cuda-11.2/nvvm/lib64:/usr/local/cuda-11.2/nvvm-prev/lib64 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/local/bin/clang --config=cuda_clang --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:tensorrt in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --action_env TF_NEED_TENSORRT=1\r\nINFO: Found applicable config definition build:cuda_clang in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_clang=true --define=using_clang=true --action_env TF_CUDA_CLANG=1\r\nINFO: Found applicable config definition build:using_cuda in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:cuda_clang in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_clang=true --define=using_clang=true --action_env TF_CUDA_CLANG=1\r\nINFO: Found applicable config definition build:using_cuda in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:opt in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:v2 in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:linux in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Repository local_config_cuda instantiated at:\r\n  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/WORKSPACE:15:10: in <toplevel>\r\n  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/tensorflow/workspace2.bzl:13:20: in workspace\r\n  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/tensorflow/workspace.bzl:95:19: in tf_repositories\r\nRepository rule cuda_configure defined at:\r\n  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/gpus/cuda_configure.bzl:1424:33: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1394, column 38, in _cuda_autoconf_impl\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/gpus/cuda_configure.bzl\", line 977, column 35, in _create_local_cuda_repository\r\n\t\tcuda_config = _get_cuda_config(repository_ctx, find_cuda_config_script)\r\n\tFile \"/home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/gpus/cuda_configure.bzl\", line 666, column 30, in _get_cuda_config\r\n\t\tconfig = find_cuda_config(repository_ctx, find_cuda_config_script, [\"cuda\", \"cudnn\"])\r\n\tFile \"/home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/gpus/cuda_configure.bzl\", line 643, column 41, in find_cuda_config\r\n\t\texec_result = _exec_find_cuda_config(repository_ctx, script_path, cuda_libraries)\r\n\tFile \"/home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/gpus/cuda_configure.bzl\", line 637, column 19, in _exec_find_cuda_config\r\n\t\treturn execute(repository_ctx, [python_bin, \"-c\", decompress_and_execute_cmd])\r\n\tFile \"/home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/remote_config/common.bzl\", line 219, column 13, in execute\r\n\t\tfail(\r\nError in fail: Repository command failed\r\nscript.py:124: DeprecationWarning: invalid escape sequence \\d\r\n  match = re.match(\"#define %s +(\\d+)\" % name, line)\r\nscript.py:260: DeprecationWarning: invalid escape sequence \\d\r\n  pattern = \"Cuda compilation tools, release \\d+\\.\\d+, V(\\d+\\.\\d+\\.\\d+)\"\r\nscript.py:553: DeprecationWarning: invalid escape sequence \\w\r\n  match = re.match(\"^(/[^/ ]*)+/lib/\\w+-linux-gnu/?$\", os.environ[env_name])\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cuda.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cublas_api.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cusolver_common.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/curand.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cufft.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cusparse.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cudnn.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cudnn_version.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nINFO: Repository rules_cc instantiated at:\r\n  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/WORKSPACE:15:10: in <toplevel>\r\n  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/tensorflow/workspace2.bzl:13:20: in workspace\r\n  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/tensorflow/workspace.bzl:1001:20: in tf_repositories\r\nRepository rule tf_http_archive defined at:\r\n  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/repo.bzl:131:34: in <toplevel>\r\nINFO: Repository bazel_skylib instantiated at:\r\n  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/WORKSPACE:15:10: in <toplevel>\r\n  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/tensorflow/workspace2.bzl:13:20: in workspace\r\n  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/tensorflow/workspace.bzl:1065:20: in tf_repositories\r\nRepository rule tf_http_archive defined at:\r\n  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/repo.bzl:131:34: in <toplevel>\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Repository command failed\r\nscript.py:124: DeprecationWarning: invalid escape sequence \\d\r\n  match = re.match(\"#define %s +(\\d+)\" % name, line)\r\nscript.py:260: DeprecationWarning: invalid escape sequence \\d\r\n  pattern = \"Cuda compilation tools, release \\d+\\.\\d+, V(\\d+\\.\\d+\\.\\d+)\"\r\nscript.py:553: DeprecationWarning: invalid escape sequence \\w\r\n  match = re.match(\"^(/[^/ ]*)+/lib/\\w+-linux-gnu/?$\", os.environ[env_name])\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cuda.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cublas_api.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cusolver_common.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/curand.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cufft.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cusparse.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cudnn.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cudnn_version.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Repository command failed\r\nscript.py:124: DeprecationWarning: invalid escape sequence \\d\r\n  match = re.match(\"#define %s +(\\d+)\" % name, line)\r\nscript.py:260: DeprecationWarning: invalid escape sequence \\d\r\n  pattern = \"Cuda compilation tools, release \\d+\\.\\d+, V(\\d+\\.\\d+\\.\\d+)\"\r\nscript.py:553: DeprecationWarning: invalid escape sequence \\w\r\n  match = re.match(\"^(/[^/ ]*)+/lib/\\w+-linux-gnu/?$\", os.environ[env_name])\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cuda.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cublas_api.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cusolver_common.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/curand.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cufft.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cusparse.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cudnn.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cudnn_version.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nINFO: Elapsed time: 7.982s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n    Fetching @local_config_tensorrt; fetching\r\n```\r\n", "comments": ["@25thengineer,\r\nEvery TensorFlow release is compatible with certain CUDA and cuDNN versions. For more information, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu).\r\n\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow-2.4.0 | 3.6-3.8 | GCC 7.3.1 | Bazel 3.1.0 | 8.0 | 11.0\r\ntensorflow-2.3.0 | 3.5-3.8 | GCC 7.3.1 | Bazel 3.1.0 | 7.6 | 10.1\r\ntensorflow-2.2.0 | 3.5-3.8 | GCC 7.3.1 | Bazel 2.0.0 | 7.6 | 10.1\r\ntensorflow-2.1.0 | 2.7, 3.5-3.7 | GCC 7.3.1 | Bazel 0.27.1 | 7.6 | 10.1\r\n\r\n\r\nCould you please try building **TensorFlow v2.4** with **CUDA 11.0** and **cuDNN 8** and let us know if you are facing the same issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46799\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46799\">No</a>\n"]}, {"number": 46798, "title": "Fix region control flow to functional pass with incompatible cast.", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/46656.\r\n\r\nWhen matching call args, do not skip incompatible cast. Otherwise, there will be incompatible args passed to build control flow op.\r\n\r\nAlso note that the https://github.com/tensorflow/tensorflow/issues/46656 is broken after https://github.com/tensorflow/tensorflow/commit/f9818f12c332e8a35dbc9042af55649be64f30c4 (so it works in 2.4.x). Before f9818f12c332e8a35dbc9042af55649be64f30c4, the single call op with few operations is inlined so it won't fall into\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e323dc9a05193c76ba5db1c71d0ef019be676224/tensorflow/compiler/mlir/tensorflow/transforms/region_control_flow_to_functional.cc#L287-L291", "comments": []}, {"number": 46797, "title": "Trying to train a custom object detection model", "body": "Hey guys!\r\nI have been trying to train a model of Object detection using the the Tensorflow models that you guys provide and while i have been trying to do this for the last 3 weeks there are some issues which are coming again and again like there is nor attribute input to convolution box predictor as those issues are resolved now there is a new issue of checkpoint which i am not able to solve or find a way to solve it i would request to update me on this one \r\n![Problem1](https://user-images.githubusercontent.com/51516774/106319221-1e211380-6297-11eb-8395-bf5d3a09478a.jpg)\r\n", "comments": ["@aayushnair1126 \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced].\r\nCould you please paste the error log, not the image it makes it easier to search the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46796, "title": "Change 'must a Tensor' to 'must be a Tensor' in TypeError message", "body": "This is very minor. There was a small typo in a TypeError message.", "comments": []}, {"number": 46795, "title": "Refactor fake_quant.h from reference_ops.h", "body": "PR2 for issue #46783.", "comments": ["Closing this PR, as FAKE_QUANT is currently not required in TFLM per Nat Jeffries."]}, {"number": 46794, "title": "ValueError: Input 0 of layer encoder1_ is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 64, 4, 850)", "body": "          inputs_p = Input(shape=(5,4,850)) #4 time steps, 5 features , num_sds\r\n          decoder_inputs = Input(shape=(5,4,850)) #1time step, 5 features , num_sds\r\n          neighbhourhood = Input(shape=(4,850,850)) #1time step, 5 features , num_sds\r\n\r\n\r\n          encoder_inputs = MyModel_conv_accross_time()(inputs_p , neighbhourhood)\r\n\r\n\r\n          enc = LSTM(cfg['units'],activation='tanh',return_state=True,return_sequences=True,name='encoder1_') \r\n          decoder_outputs, state_h, state_c  = enc(encoder_inputs)\r\n\r\n\r\nValueError: Input 0 of layer encoder1_ is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 64, 4, 850)\r\n\r\n\r\nMyModel_conv_accross_time  returns results with shape (batch,64,4,850)", "comments": ["This is very good", "I believe this is because lstm expects 3d input , not 4d . let me check and get back!", "reshaping the output from MyModel_conv_accross_time to (batch*850,64,4) solved the issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46794\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46794\">No</a>\n"]}, {"number": 46793, "title": "Refactor ParseFakeQuant in lite/core/api/flatbuffer_conversions.cc/h", "body": "PR1 for issue #46783.", "comments": [":frowning_face: Sorry, but only Googlers may change the label `cla: yes`.", "Closing this PR, as FAKE_QUANT is currently not required in TFLM per Nat Jeffries."]}, {"number": 46792, "title": "TFlite conversion wierd Java Script error", "body": "HI, I tried to convert my pytorch model to a deployable tflite model on my coral edge TPU:\r\nAnd my torch model architecture is:\r\n```\r\nEncoderDecoder(\r\n  (encoder): ResNet(\r\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\r\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n    (relu): ReLU(inplace=True)\r\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\r\n    (layer1): Sequential(\r\n      (0): BasicBlock(\r\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (relu): ReLU(inplace=True)\r\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n      )\r\n      (1): BasicBlock(\r\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (relu): ReLU(inplace=True)\r\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n      )\r\n      (2): BasicBlock(\r\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (relu): ReLU(inplace=True)\r\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n      )\r\n    )\r\n    (layer2): Sequential(\r\n      (0): BasicBlock(\r\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\r\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (relu): ReLU(inplace=True)\r\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (downsample): Sequential(\r\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\r\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        )\r\n      )\r\n      (1): BasicBlock(\r\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (relu): ReLU(inplace=True)\r\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n      )\r\n      (2): BasicBlock(\r\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (relu): ReLU(inplace=True)\r\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n      )\r\n      (3): BasicBlock(\r\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (relu): ReLU(inplace=True)\r\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n      )\r\n    )\r\n    (layer3): Sequential(\r\n      (0): BasicBlock(\r\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\r\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (relu): ReLU(inplace=True)\r\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (downsample): Sequential(\r\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\r\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        )\r\n      )\r\n      (1): BasicBlock(\r\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (relu): ReLU(inplace=True)\r\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n      )\r\n      (2): BasicBlock(\r\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (relu): ReLU(inplace=True)\r\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n      )\r\n      (3): BasicBlock(\r\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (relu): ReLU(inplace=True)\r\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n      )\r\n      (4): BasicBlock(\r\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (relu): ReLU(inplace=True)\r\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n      )\r\n      (5): BasicBlock(\r\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (relu): ReLU(inplace=True)\r\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n      )\r\n    )\r\n    (layer4): Sequential(\r\n      (0): BasicBlock(\r\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\r\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (relu): ReLU(inplace=True)\r\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (downsample): Sequential(\r\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\r\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        )\r\n      )\r\n      (1): BasicBlock(\r\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (relu): ReLU(inplace=True)\r\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n      )\r\n      (2): BasicBlock(\r\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        (relu): ReLU(inplace=True)\r\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n      )\r\n    )\r\n  )\r\n  (decoder): CatLinear(\r\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\r\n    (dropout): Dropout(p=0.5, inplace=False)\r\n    (fc1): Linear(in_features=512, out_features=55, bias=True)\r\n  )\r\n  (criterion): SmoothL1Loss()\r\n)\r\n\r\n```\r\n### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): pip package\r\n- Module Versions:\r\nPyTorch: 1.7.1\r\nOnnx: 1.8.0\r\nOnnx_tf: 1.7.0\r\nTensorflow: 2.3.0\r\n\r\n### 2. Code\r\nI followed the instructions and I successfully converted from torch to onnx and to tensorflow.\r\nHowever, when it comes to the tflite model, I met some problem.\r\nThe first method is:\r\n```\r\ndef representative_dataset():\r\n    for _ in range(100):\r\n      data = np_input\r\n      yield [data.astype(np.float32)]\r\n        \r\nconverter = tf.lite.TFLiteConverter.from_saved_model(tfpath)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8  # or tf.uint8\r\nconverter.inference_output_type = tf.int8 \r\ntflite_model = converter.convert()\r\nopen(\"test.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n\r\nDuring the conversion, the page froze and evetually gave me an error: ``Javascript Error: too much recurtion``\r\nI also tried run a python script instead of in a Jupyter notebook, the terminal output seems indicate a successful conversion, but there is no tflite file generated. The output is:\r\n```\r\n  %0 = \"tfl.pad\"(%arg0, %cst) : (tensor<1x3x960x360xf32>, tensor<4x2xi32>) -> tensor<1x3x966x366xf32>\r\n  %1 = \"tfl.transpose\"(%0, %cst_5) : (tensor<1x3x966x366xf32>, tensor<4xi32>) -> tensor<1x966x366x3xf32>\r\n  %2 = \"tfl.split\"(%cst_4, %1) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x966x366x3xf32>) -> tensor<1x966x366x3xf32>\r\n  %3 = \"tfl.conv_2d\"(%2, %cst_12, %cst_48) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x966x366x3xf32>, tensor<64x7x7x3xf32>, tensor<64xf32>) -> tensor<1x480x180x64xf32>\r\n  %4 = \"tfl.transpose\"(%3, %cst_6) : (tensor<1x480x180x64xf32>, tensor<4xi32>) -> tensor<1x64x480x180xf32>\r\n  %5 = \"tfl.pad\"(%4, %cst_0) : (tensor<1x64x480x180xf32>, tensor<4x2xi32>) -> tensor<1x64x482x182xf32>\r\n  %6 = \"tfl.transpose\"(%5, %cst_5) : (tensor<1x64x482x182xf32>, tensor<4xi32>) -> tensor<1x482x182x64xf32>\r\n  %7 = \"tfl.split\"(%cst_4, %6) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x482x182x64xf32>) -> tensor<1x482x182x64xf32>\r\n  %8 = \"tfl.conv_2d\"(%7, %cst_13, %cst_49) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x482x182x64xf32>, tensor<64x3x3x64xf32>, tensor<64xf32>) -> tensor<1x480x180x64xf32>\r\n  %9 = \"tfl.transpose\"(%8, %cst_6) : (tensor<1x480x180x64xf32>, tensor<4xi32>) -> tensor<1x64x480x180xf32>\r\n  %10 = \"tfl.pad\"(%9, %cst_0) : (tensor<1x64x480x180xf32>, tensor<4x2xi32>) -> tensor<1x64x482x182xf32>\r\n  %11 = \"tfl.transpose\"(%10, %cst_5) : (tensor<1x64x482x182xf32>, tensor<4xi32>) -> tensor<1x482x182x64xf32>\r\n  %12 = \"tfl.split\"(%cst_4, %11) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x482x182x64xf32>) -> tensor<1x482x182x64xf32>\r\n  %13 = \"tfl.conv_2d\"(%12, %cst_14, %cst_50) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x482x182x64xf32>, tensor<64x3x3x64xf32>, tensor<64xf32>) -> tensor<1x480x180x64xf32>\r\n  %14 = \"tfl.transpose\"(%13, %cst_6) : (tensor<1x480x180x64xf32>, tensor<4xi32>) -> tensor<1x64x480x180xf32>\r\n  %15 = \"tfl.add\"(%14, %4) {fused_activation_function = \"RELU\"} : (tensor<1x64x480x180xf32>, tensor<1x64x480x180xf32>) -> tensor<1x64x480x180xf32>\r\n  %16 = \"tfl.pad\"(%15, %cst_0) : (tensor<1x64x480x180xf32>, tensor<4x2xi32>) -> tensor<1x64x482x182xf32>\r\n  %17 = \"tfl.transpose\"(%16, %cst_5) : (tensor<1x64x482x182xf32>, tensor<4xi32>) -> tensor<1x482x182x64xf32>\r\n  %18 = \"tfl.split\"(%cst_4, %17) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x482x182x64xf32>) -> tensor<1x482x182x64xf32>\r\n  %19 = \"tfl.conv_2d\"(%18, %cst_15, %cst_51) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x482x182x64xf32>, tensor<64x3x3x64xf32>, tensor<64xf32>) -> tensor<1x480x180x64xf32>\r\n  %20 = \"tfl.transpose\"(%19, %cst_6) : (tensor<1x480x180x64xf32>, tensor<4xi32>) -> tensor<1x64x480x180xf32>\r\n  %21 = \"tfl.pad\"(%20, %cst_0) : (tensor<1x64x480x180xf32>, tensor<4x2xi32>) -> tensor<1x64x482x182xf32>\r\n  %22 = \"tfl.transpose\"(%21, %cst_5) : (tensor<1x64x482x182xf32>, tensor<4xi32>) -> tensor<1x482x182x64xf32>\r\n  %23 = \"tfl.split\"(%cst_4, %22) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x482x182x64xf32>) -> tensor<1x482x182x64xf32>\r\n  %24 = \"tfl.gather\"(%cst_1, %cst_52) {axis = 0 : i32} : (tensor<4xi64>, tensor<0xi64>) -> tensor<0xi64>\r\n  %25 = \"tfl.sparse_to_dense\"(%cst_11, %cst_2, %24, %cst_7) : (tensor<0x1xi64>, tensor<1xi64>, tensor<0xi64>, tensor<i64>) -> tensor<2xi64>\r\n  %26 = \"tf.AddV2\"(%25, %cst_10) {device = \"\"} : (tensor<2xi64>, tensor<2xi64>) -> tensor<2xi64>\r\n  %27 = \"tfl.conv_2d\"(%23, %cst_16, %cst_53) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x482x182x64xf32>, tensor<64x3x3x64xf32>, tensor<64xf32>) -> tensor<1x480x180x64xf32>\r\n  %28 = \"tfl.transpose\"(%27, %cst_6) : (tensor<1x480x180x64xf32>, tensor<4xi32>) -> tensor<1x64x480x180xf32>\r\n  %29 = \"tfl.add\"(%28, %15) {fused_activation_function = \"RELU\"} : (tensor<1x64x480x180xf32>, tensor<1x64x480x180xf32>) -> tensor<1x64x480x180xf32>\r\n  %30 = \"tfl.pad\"(%29, %cst_0) : (tensor<1x64x480x180xf32>, tensor<4x2xi32>) -> tensor<1x64x482x182xf32>\r\n  %31 = \"tfl.transpose\"(%30, %cst_5) : (tensor<1x64x482x182xf32>, tensor<4xi32>) -> tensor<1x482x182x64xf32>\r\n  %32 = \"tfl.split\"(%cst_4, %31) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x482x182x64xf32>) -> tensor<1x482x182x64xf32>\r\n  %33 = \"tfl.conv_2d\"(%32, %cst_17, %cst_54) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x482x182x64xf32>, tensor<64x3x3x64xf32>, tensor<64xf32>) -> tensor<1x480x180x64xf32>\r\n  %34 = \"tfl.transpose\"(%33, %cst_6) : (tensor<1x480x180x64xf32>, tensor<4xi32>) -> tensor<1x64x480x180xf32>\r\n  %35 = \"tfl.pad\"(%34, %cst_0) : (tensor<1x64x480x180xf32>, tensor<4x2xi32>) -> tensor<1x64x482x182xf32>\r\n  %36 = \"tfl.transpose\"(%35, %cst_5) : (tensor<1x64x482x182xf32>, tensor<4xi32>) -> tensor<1x482x182x64xf32>\r\n  %37 = \"tfl.split\"(%cst_4, %36) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x482x182x64xf32>) -> tensor<1x482x182x64xf32>\r\n  %38 = \"tfl.conv_2d\"(%37, %cst_18, %cst_55) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x482x182x64xf32>, tensor<64x3x3x64xf32>, tensor<64xf32>) -> tensor<1x480x180x64xf32>\r\n  %39 = \"tfl.transpose\"(%38, %cst_6) : (tensor<1x480x180x64xf32>, tensor<4xi32>) -> tensor<1x64x480x180xf32>\r\n  %40 = \"tfl.add\"(%39, %29) {fused_activation_function = \"RELU\"} : (tensor<1x64x480x180xf32>, tensor<1x64x480x180xf32>) -> tensor<1x64x480x180xf32>\r\n  %41 = \"tfl.pad\"(%40, %cst_0) : (tensor<1x64x480x180xf32>, tensor<4x2xi32>) -> tensor<1x64x482x182xf32>\r\n  %42 = \"tfl.transpose\"(%41, %cst_5) : (tensor<1x64x482x182xf32>, tensor<4xi32>) -> tensor<1x482x182x64xf32>\r\n  %43 = \"tfl.split\"(%cst_4, %42) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x482x182x64xf32>) -> tensor<1x482x182x64xf32>\r\n  %44 = \"tfl.conv_2d\"(%43, %cst_19, %cst_56) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x482x182x64xf32>, tensor<128x3x3x64xf32>, tensor<128xf32>) -> tensor<1x240x90x128xf32>\r\n  %45 = \"tfl.transpose\"(%44, %cst_6) : (tensor<1x240x90x128xf32>, tensor<4xi32>) -> tensor<1x128x240x90xf32>\r\n  %46 = \"tfl.pad\"(%45, %cst_0) : (tensor<1x128x240x90xf32>, tensor<4x2xi32>) -> tensor<1x128x242x92xf32>\r\n  %47 = \"tfl.transpose\"(%46, %cst_5) : (tensor<1x128x242x92xf32>, tensor<4xi32>) -> tensor<1x242x92x128xf32>\r\n  %48 = \"tfl.split\"(%cst_4, %47) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x242x92x128xf32>) -> tensor<1x242x92x128xf32>\r\n  %49 = \"tfl.conv_2d\"(%48, %cst_20, %cst_57) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x242x92x128xf32>, tensor<128x3x3x128xf32>, tensor<128xf32>) -> tensor<1x240x90x128xf32>\r\n  %50 = \"tfl.transpose\"(%49, %cst_6) : (tensor<1x240x90x128xf32>, tensor<4xi32>) -> tensor<1x128x240x90xf32>\r\n  %51 = \"tfl.transpose\"(%40, %cst_5) : (tensor<1x64x480x180xf32>, tensor<4xi32>) -> tensor<1x480x180x64xf32>\r\n  %52 = \"tfl.split\"(%cst_4, %51) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x480x180x64xf32>) -> tensor<1x480x180x64xf32>\r\n  %53 = \"tfl.conv_2d\"(%52, %cst_21, %cst_58) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x480x180x64xf32>, tensor<128x1x1x64xf32>, tensor<128xf32>) -> tensor<1x240x90x128xf32>\r\n  %54 = \"tfl.transpose\"(%53, %cst_6) : (tensor<1x240x90x128xf32>, tensor<4xi32>) -> tensor<1x128x240x90xf32>\r\n  %55 = \"tfl.add\"(%50, %54) {fused_activation_function = \"RELU\"} : (tensor<1x128x240x90xf32>, tensor<1x128x240x90xf32>) -> tensor<1x128x240x90xf32>\r\n  %56 = \"tfl.pad\"(%55, %cst_0) : (tensor<1x128x240x90xf32>, tensor<4x2xi32>) -> tensor<1x128x242x92xf32>\r\n  %57 = \"tfl.transpose\"(%56, %cst_5) : (tensor<1x128x242x92xf32>, tensor<4xi32>) -> tensor<1x242x92x128xf32>\r\n  %58 = \"tfl.split\"(%cst_4, %57) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x242x92x128xf32>) -> tensor<1x242x92x128xf32>\r\n  %59 = \"tfl.conv_2d\"(%58, %cst_22, %cst_59) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x242x92x128xf32>, tensor<128x3x3x128xf32>, tensor<128xf32>) -> tensor<1x240x90x128xf32>\r\n  %60 = \"tfl.transpose\"(%59, %cst_6) : (tensor<1x240x90x128xf32>, tensor<4xi32>) -> tensor<1x128x240x90xf32>\r\n  %61 = \"tfl.pad\"(%60, %cst_0) : (tensor<1x128x240x90xf32>, tensor<4x2xi32>) -> tensor<1x128x242x92xf32>\r\n  %62 = \"tfl.transpose\"(%61, %cst_5) : (tensor<1x128x242x92xf32>, tensor<4xi32>) -> tensor<1x242x92x128xf32>\r\n  %63 = \"tfl.split\"(%cst_4, %62) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x242x92x128xf32>) -> tensor<1x242x92x128xf32>\r\n  %64 = \"tfl.conv_2d\"(%63, %cst_23, %cst_60) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x242x92x128xf32>, tensor<128x3x3x128xf32>, tensor<128xf32>) -> tensor<1x240x90x128xf32>\r\n  %65 = \"tfl.transpose\"(%64, %cst_6) : (tensor<1x240x90x128xf32>, tensor<4xi32>) -> tensor<1x128x240x90xf32>\r\n  %66 = \"tfl.add\"(%65, %55) {fused_activation_function = \"RELU\"} : (tensor<1x128x240x90xf32>, tensor<1x128x240x90xf32>) -> tensor<1x128x240x90xf32>\r\n  %67 = \"tfl.pad\"(%66, %cst_0) : (tensor<1x128x240x90xf32>, tensor<4x2xi32>) -> tensor<1x128x242x92xf32>\r\n  %68 = \"tfl.transpose\"(%67, %cst_5) : (tensor<1x128x242x92xf32>, tensor<4xi32>) -> tensor<1x242x92x128xf32>\r\n  %69 = \"tfl.split\"(%cst_4, %68) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x242x92x128xf32>) -> tensor<1x242x92x128xf32>\r\n  %70 = \"tfl.conv_2d\"(%69, %cst_24, %cst_61) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x242x92x128xf32>, tensor<128x3x3x128xf32>, tensor<128xf32>) -> tensor<1x240x90x128xf32>\r\n  %71 = \"tfl.transpose\"(%70, %cst_6) : (tensor<1x240x90x128xf32>, tensor<4xi32>) -> tensor<1x128x240x90xf32>\r\n  %72 = \"tfl.pad\"(%71, %cst_0) : (tensor<1x128x240x90xf32>, tensor<4x2xi32>) -> tensor<1x128x242x92xf32>\r\n  %73 = \"tfl.transpose\"(%72, %cst_5) : (tensor<1x128x242x92xf32>, tensor<4xi32>) -> tensor<1x242x92x128xf32>\r\n  %74 = \"tfl.split\"(%cst_4, %73) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x242x92x128xf32>) -> tensor<1x242x92x128xf32>\r\n  %75 = \"tfl.conv_2d\"(%74, %cst_25, %cst_62) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x242x92x128xf32>, tensor<128x3x3x128xf32>, tensor<128xf32>) -> tensor<1x240x90x128xf32>\r\n  %76 = \"tfl.transpose\"(%75, %cst_6) : (tensor<1x240x90x128xf32>, tensor<4xi32>) -> tensor<1x128x240x90xf32>\r\n  %77 = \"tfl.add\"(%76, %66) {fused_activation_function = \"RELU\"} : (tensor<1x128x240x90xf32>, tensor<1x128x240x90xf32>) -> tensor<1x128x240x90xf32>\r\n  %78 = \"tfl.pad\"(%77, %cst_0) : (tensor<1x128x240x90xf32>, tensor<4x2xi32>) -> tensor<1x128x242x92xf32>\r\n  %79 = \"tfl.transpose\"(%78, %cst_5) : (tensor<1x128x242x92xf32>, tensor<4xi32>) -> tensor<1x242x92x128xf32>\r\n  %80 = \"tfl.split\"(%cst_4, %79) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x242x92x128xf32>) -> tensor<1x242x92x128xf32>\r\n  %81 = \"tfl.conv_2d\"(%80, %cst_26, %cst_63) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x242x92x128xf32>, tensor<128x3x3x128xf32>, tensor<128xf32>) -> tensor<1x240x90x128xf32>\r\n  %82 = \"tfl.transpose\"(%81, %cst_6) : (tensor<1x240x90x128xf32>, tensor<4xi32>) -> tensor<1x128x240x90xf32>\r\n  %83 = \"tfl.pad\"(%82, %cst_0) : (tensor<1x128x240x90xf32>, tensor<4x2xi32>) -> tensor<1x128x242x92xf32>\r\n  %84 = \"tfl.transpose\"(%83, %cst_5) : (tensor<1x128x242x92xf32>, tensor<4xi32>) -> tensor<1x242x92x128xf32>\r\n  %85 = \"tfl.split\"(%cst_4, %84) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x242x92x128xf32>) -> tensor<1x242x92x128xf32>\r\n  %86 = \"tfl.conv_2d\"(%85, %cst_27, %cst_64) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x242x92x128xf32>, tensor<128x3x3x128xf32>, tensor<128xf32>) -> tensor<1x240x90x128xf32>\r\n  %87 = \"tfl.transpose\"(%86, %cst_6) : (tensor<1x240x90x128xf32>, tensor<4xi32>) -> tensor<1x128x240x90xf32>\r\n  %88 = \"tfl.add\"(%87, %77) {fused_activation_function = \"RELU\"} : (tensor<1x128x240x90xf32>, tensor<1x128x240x90xf32>) -> tensor<1x128x240x90xf32>\r\n  %89 = \"tfl.pad\"(%88, %cst_0) : (tensor<1x128x240x90xf32>, tensor<4x2xi32>) -> tensor<1x128x242x92xf32>\r\n  %90 = \"tfl.transpose\"(%89, %cst_5) : (tensor<1x128x242x92xf32>, tensor<4xi32>) -> tensor<1x242x92x128xf32>\r\n  %91 = \"tfl.split\"(%cst_4, %90) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x242x92x128xf32>) -> tensor<1x242x92x128xf32>\r\n  %92 = \"tfl.conv_2d\"(%91, %cst_28, %cst_65) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x242x92x128xf32>, tensor<256x3x3x128xf32>, tensor<256xf32>) -> tensor<1x120x45x256xf32>\r\n  %93 = \"tfl.transpose\"(%92, %cst_6) : (tensor<1x120x45x256xf32>, tensor<4xi32>) -> tensor<1x256x120x45xf32>\r\n  %94 = \"tfl.pad\"(%93, %cst_0) : (tensor<1x256x120x45xf32>, tensor<4x2xi32>) -> tensor<1x256x122x47xf32>\r\n  %95 = \"tfl.transpose\"(%94, %cst_5) : (tensor<1x256x122x47xf32>, tensor<4xi32>) -> tensor<1x122x47x256xf32>\r\n  %96 = \"tfl.split\"(%cst_4, %95) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x122x47x256xf32>) -> tensor<1x122x47x256xf32>\r\n  %97 = \"tfl.conv_2d\"(%96, %cst_29, %cst_66) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x122x47x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x120x45x256xf32>\r\n  %98 = \"tfl.transpose\"(%97, %cst_6) : (tensor<1x120x45x256xf32>, tensor<4xi32>) -> tensor<1x256x120x45xf32>\r\n  %99 = \"tfl.transpose\"(%88, %cst_5) : (tensor<1x128x240x90xf32>, tensor<4xi32>) -> tensor<1x240x90x128xf32>\r\n  %100 = \"tfl.split\"(%cst_4, %99) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x240x90x128xf32>) -> tensor<1x240x90x128xf32>\r\n  %101 = \"tfl.conv_2d\"(%100, %cst_30, %cst_67) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x240x90x128xf32>, tensor<256x1x1x128xf32>, tensor<256xf32>) -> tensor<1x120x45x256xf32>\r\n  %102 = \"tfl.transpose\"(%101, %cst_6) : (tensor<1x120x45x256xf32>, tensor<4xi32>) -> tensor<1x256x120x45xf32>\r\n  %103 = \"tfl.add\"(%98, %102) {fused_activation_function = \"RELU\"} : (tensor<1x256x120x45xf32>, tensor<1x256x120x45xf32>) -> tensor<1x256x120x45xf32>\r\n  %104 = \"tfl.pad\"(%103, %cst_0) : (tensor<1x256x120x45xf32>, tensor<4x2xi32>) -> tensor<1x256x122x47xf32>\r\n  %105 = \"tfl.transpose\"(%104, %cst_5) : (tensor<1x256x122x47xf32>, tensor<4xi32>) -> tensor<1x122x47x256xf32>\r\n  %106 = \"tfl.split\"(%cst_4, %105) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x122x47x256xf32>) -> tensor<1x122x47x256xf32>\r\n  %107 = \"tfl.conv_2d\"(%106, %cst_31, %cst_68) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x122x47x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x120x45x256xf32>\r\n  %108 = \"tfl.transpose\"(%107, %cst_6) : (tensor<1x120x45x256xf32>, tensor<4xi32>) -> tensor<1x256x120x45xf32>\r\n  %109 = \"tfl.pad\"(%108, %cst_0) : (tensor<1x256x120x45xf32>, tensor<4x2xi32>) -> tensor<1x256x122x47xf32>\r\n  %110 = \"tfl.transpose\"(%109, %cst_5) : (tensor<1x256x122x47xf32>, tensor<4xi32>) -> tensor<1x122x47x256xf32>\r\n  %111 = \"tfl.split\"(%cst_4, %110) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x122x47x256xf32>) -> tensor<1x122x47x256xf32>\r\n  %112 = \"tfl.conv_2d\"(%111, %cst_32, %cst_69) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x122x47x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x120x45x256xf32>\r\n  %113 = \"tfl.transpose\"(%112, %cst_6) : (tensor<1x120x45x256xf32>, tensor<4xi32>) -> tensor<1x256x120x45xf32>\r\n  %114 = \"tfl.add\"(%113, %103) {fused_activation_function = \"RELU\"} : (tensor<1x256x120x45xf32>, tensor<1x256x120x45xf32>) -> tensor<1x256x120x45xf32>\r\n  %115 = \"tfl.pad\"(%114, %cst_0) : (tensor<1x256x120x45xf32>, tensor<4x2xi32>) -> tensor<1x256x122x47xf32>\r\n  %116 = \"tfl.transpose\"(%115, %cst_5) : (tensor<1x256x122x47xf32>, tensor<4xi32>) -> tensor<1x122x47x256xf32>\r\n  %117 = \"tfl.split\"(%cst_4, %116) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x122x47x256xf32>) -> tensor<1x122x47x256xf32>\r\n  %118 = \"tfl.conv_2d\"(%117, %cst_33, %cst_70) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x122x47x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x120x45x256xf32>\r\n  %119 = \"tfl.transpose\"(%118, %cst_6) : (tensor<1x120x45x256xf32>, tensor<4xi32>) -> tensor<1x256x120x45xf32>\r\n  %120 = \"tfl.pad\"(%119, %cst_0) : (tensor<1x256x120x45xf32>, tensor<4x2xi32>) -> tensor<1x256x122x47xf32>\r\n  %121 = \"tfl.transpose\"(%120, %cst_5) : (tensor<1x256x122x47xf32>, tensor<4xi32>) -> tensor<1x122x47x256xf32>\r\n  %122 = \"tfl.split\"(%cst_4, %121) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x122x47x256xf32>) -> tensor<1x122x47x256xf32>\r\n  %123 = \"tfl.conv_2d\"(%122, %cst_34, %cst_71) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x122x47x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x120x45x256xf32>\r\n  %124 = \"tfl.transpose\"(%123, %cst_6) : (tensor<1x120x45x256xf32>, tensor<4xi32>) -> tensor<1x256x120x45xf32>\r\n  %125 = \"tfl.add\"(%124, %114) {fused_activation_function = \"RELU\"} : (tensor<1x256x120x45xf32>, tensor<1x256x120x45xf32>) -> tensor<1x256x120x45xf32>\r\n  %126 = \"tfl.pad\"(%125, %cst_0) : (tensor<1x256x120x45xf32>, tensor<4x2xi32>) -> tensor<1x256x122x47xf32>\r\n  %127 = \"tfl.transpose\"(%126, %cst_5) : (tensor<1x256x122x47xf32>, tensor<4xi32>) -> tensor<1x122x47x256xf32>\r\n  %128 = \"tfl.split\"(%cst_4, %127) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x122x47x256xf32>) -> tensor<1x122x47x256xf32>\r\n  %129 = \"tfl.conv_2d\"(%128, %cst_35, %cst_72) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x122x47x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x120x45x256xf32>\r\n  %130 = \"tfl.transpose\"(%129, %cst_6) : (tensor<1x120x45x256xf32>, tensor<4xi32>) -> tensor<1x256x120x45xf32>\r\n  %131 = \"tfl.pad\"(%130, %cst_0) : (tensor<1x256x120x45xf32>, tensor<4x2xi32>) -> tensor<1x256x122x47xf32>\r\n  %132 = \"tfl.transpose\"(%131, %cst_5) : (tensor<1x256x122x47xf32>, tensor<4xi32>) -> tensor<1x122x47x256xf32>\r\n  %133 = \"tfl.split\"(%cst_4, %132) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x122x47x256xf32>) -> tensor<1x122x47x256xf32>\r\n  %134 = \"tfl.conv_2d\"(%133, %cst_36, %cst_73) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x122x47x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x120x45x256xf32>\r\n  %135 = \"tfl.transpose\"(%134, %cst_6) : (tensor<1x120x45x256xf32>, tensor<4xi32>) -> tensor<1x256x120x45xf32>\r\n  %136 = \"tfl.add\"(%135, %125) {fused_activation_function = \"RELU\"} : (tensor<1x256x120x45xf32>, tensor<1x256x120x45xf32>) -> tensor<1x256x120x45xf32>\r\n  %137 = \"tfl.pad\"(%136, %cst_0) : (tensor<1x256x120x45xf32>, tensor<4x2xi32>) -> tensor<1x256x122x47xf32>\r\n  %138 = \"tfl.transpose\"(%137, %cst_5) : (tensor<1x256x122x47xf32>, tensor<4xi32>) -> tensor<1x122x47x256xf32>\r\n  %139 = \"tfl.split\"(%cst_4, %138) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x122x47x256xf32>) -> tensor<1x122x47x256xf32>\r\n  %140 = \"tfl.conv_2d\"(%139, %cst_37, %cst_74) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x122x47x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x120x45x256xf32>\r\n  %141 = \"tfl.transpose\"(%140, %cst_6) : (tensor<1x120x45x256xf32>, tensor<4xi32>) -> tensor<1x256x120x45xf32>\r\n  %142 = \"tfl.pad\"(%141, %cst_0) : (tensor<1x256x120x45xf32>, tensor<4x2xi32>) -> tensor<1x256x122x47xf32>\r\n  %143 = \"tfl.transpose\"(%142, %cst_5) : (tensor<1x256x122x47xf32>, tensor<4xi32>) -> tensor<1x122x47x256xf32>\r\n  %144 = \"tfl.split\"(%cst_4, %143) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x122x47x256xf32>) -> tensor<1x122x47x256xf32>\r\n  %145 = \"tfl.conv_2d\"(%144, %cst_38, %cst_75) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x122x47x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x120x45x256xf32>\r\n  %146 = \"tfl.transpose\"(%145, %cst_6) : (tensor<1x120x45x256xf32>, tensor<4xi32>) -> tensor<1x256x120x45xf32>\r\n  %147 = \"tfl.add\"(%146, %136) {fused_activation_function = \"RELU\"} : (tensor<1x256x120x45xf32>, tensor<1x256x120x45xf32>) -> tensor<1x256x120x45xf32>\r\n  %148 = \"tfl.pad\"(%147, %cst_0) : (tensor<1x256x120x45xf32>, tensor<4x2xi32>) -> tensor<1x256x122x47xf32>\r\n  %149 = \"tfl.transpose\"(%148, %cst_5) : (tensor<1x256x122x47xf32>, tensor<4xi32>) -> tensor<1x122x47x256xf32>\r\n  %150 = \"tfl.split\"(%cst_4, %149) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x122x47x256xf32>) -> tensor<1x122x47x256xf32>\r\n  %151 = \"tfl.conv_2d\"(%150, %cst_39, %cst_76) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x122x47x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x120x45x256xf32>\r\n  %152 = \"tfl.transpose\"(%151, %cst_6) : (tensor<1x120x45x256xf32>, tensor<4xi32>) -> tensor<1x256x120x45xf32>\r\n  %153 = \"tfl.pad\"(%152, %cst_0) : (tensor<1x256x120x45xf32>, tensor<4x2xi32>) -> tensor<1x256x122x47xf32>\r\n  %154 = \"tfl.transpose\"(%153, %cst_5) : (tensor<1x256x122x47xf32>, tensor<4xi32>) -> tensor<1x122x47x256xf32>\r\n  %155 = \"tfl.split\"(%cst_4, %154) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x122x47x256xf32>) -> tensor<1x122x47x256xf32>\r\n  %156 = \"tfl.conv_2d\"(%155, %cst_40, %cst_77) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x122x47x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x120x45x256xf32>\r\n  %157 = \"tfl.transpose\"(%156, %cst_6) : (tensor<1x120x45x256xf32>, tensor<4xi32>) -> tensor<1x256x120x45xf32>\r\n  %158 = \"tfl.add\"(%157, %147) {fused_activation_function = \"RELU\"} : (tensor<1x256x120x45xf32>, tensor<1x256x120x45xf32>) -> tensor<1x256x120x45xf32>\r\n  %159 = \"tfl.pad\"(%158, %cst_0) : (tensor<1x256x120x45xf32>, tensor<4x2xi32>) -> tensor<1x256x122x47xf32>\r\n  %160 = \"tfl.transpose\"(%159, %cst_5) : (tensor<1x256x122x47xf32>, tensor<4xi32>) -> tensor<1x122x47x256xf32>\r\n  %161 = \"tfl.split\"(%cst_4, %160) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x122x47x256xf32>) -> tensor<1x122x47x256xf32>\r\n  %162 = \"tfl.conv_2d\"(%161, %cst_41, %cst_78) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x122x47x256xf32>, tensor<512x3x3x256xf32>, tensor<512xf32>) -> tensor<1x60x23x512xf32>\r\n  %163 = \"tfl.transpose\"(%162, %cst_6) : (tensor<1x60x23x512xf32>, tensor<4xi32>) -> tensor<1x512x60x23xf32>\r\n  %164 = \"tfl.pad\"(%163, %cst_0) : (tensor<1x512x60x23xf32>, tensor<4x2xi32>) -> tensor<1x512x62x25xf32>\r\n  %165 = \"tfl.transpose\"(%164, %cst_5) : (tensor<1x512x62x25xf32>, tensor<4xi32>) -> tensor<1x62x25x512xf32>\r\n  %166 = \"tfl.split\"(%cst_4, %165) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x62x25x512xf32>) -> tensor<1x62x25x512xf32>\r\n  %167 = \"tfl.conv_2d\"(%166, %cst_42, %cst_79) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x62x25x512xf32>, tensor<512x3x3x512xf32>, tensor<512xf32>) -> tensor<1x60x23x512xf32>\r\n  %168 = \"tfl.transpose\"(%167, %cst_6) : (tensor<1x60x23x512xf32>, tensor<4xi32>) -> tensor<1x512x60x23xf32>\r\n  %169 = \"tfl.transpose\"(%158, %cst_5) : (tensor<1x256x120x45xf32>, tensor<4xi32>) -> tensor<1x120x45x256xf32>\r\n  %170 = \"tfl.split\"(%cst_4, %169) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x120x45x256xf32>) -> tensor<1x120x45x256xf32>\r\n  %171 = \"tfl.conv_2d\"(%170, %cst_43, %cst_80) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x120x45x256xf32>, tensor<512x1x1x256xf32>, tensor<512xf32>) -> tensor<1x60x23x512xf32>\r\n  %172 = \"tfl.transpose\"(%171, %cst_6) : (tensor<1x60x23x512xf32>, tensor<4xi32>) -> tensor<1x512x60x23xf32>\r\n  %173 = \"tfl.add\"(%168, %172) {fused_activation_function = \"RELU\"} : (tensor<1x512x60x23xf32>, tensor<1x512x60x23xf32>) -> tensor<1x512x60x23xf32>\r\n  %174 = \"tfl.pad\"(%173, %cst_0) : (tensor<1x512x60x23xf32>, tensor<4x2xi32>) -> tensor<1x512x62x25xf32>\r\n  %175 = \"tfl.transpose\"(%174, %cst_5) : (tensor<1x512x62x25xf32>, tensor<4xi32>) -> tensor<1x62x25x512xf32>\r\n  %176 = \"tfl.split\"(%cst_4, %175) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x62x25x512xf32>) -> tensor<1x62x25x512xf32>\r\n  %177 = \"tfl.conv_2d\"(%176, %cst_44, %cst_81) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x62x25x512xf32>, tensor<512x3x3x512xf32>, tensor<512xf32>) -> tensor<1x60x23x512xf32>\r\n  %178 = \"tfl.transpose\"(%177, %cst_6) : (tensor<1x60x23x512xf32>, tensor<4xi32>) -> tensor<1x512x60x23xf32>\r\n  %179 = \"tfl.pad\"(%178, %cst_0) : (tensor<1x512x60x23xf32>, tensor<4x2xi32>) -> tensor<1x512x62x25xf32>\r\n  %180 = \"tfl.transpose\"(%179, %cst_5) : (tensor<1x512x62x25xf32>, tensor<4xi32>) -> tensor<1x62x25x512xf32>\r\n  %181 = \"tfl.split\"(%cst_4, %180) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x62x25x512xf32>) -> tensor<1x62x25x512xf32>\r\n  %182 = \"tfl.conv_2d\"(%181, %cst_45, %cst_82) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x62x25x512xf32>, tensor<512x3x3x512xf32>, tensor<512xf32>) -> tensor<1x60x23x512xf32>\r\n  %183 = \"tfl.transpose\"(%182, %cst_6) : (tensor<1x60x23x512xf32>, tensor<4xi32>) -> tensor<1x512x60x23xf32>\r\n  %184 = \"tfl.add\"(%183, %173) {fused_activation_function = \"RELU\"} : (tensor<1x512x60x23xf32>, tensor<1x512x60x23xf32>) -> tensor<1x512x60x23xf32>\r\n  %185 = \"tfl.pad\"(%184, %cst_0) : (tensor<1x512x60x23xf32>, tensor<4x2xi32>) -> tensor<1x512x62x25xf32>\r\n  %186 = \"tfl.transpose\"(%185, %cst_5) : (tensor<1x512x62x25xf32>, tensor<4xi32>) -> tensor<1x62x25x512xf32>\r\n  %187 = \"tfl.split\"(%cst_4, %186) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x62x25x512xf32>) -> tensor<1x62x25x512xf32>\r\n  %188 = \"tfl.conv_2d\"(%187, %cst_46, %cst_83) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x62x25x512xf32>, tensor<512x3x3x512xf32>, tensor<512xf32>) -> tensor<1x60x23x512xf32>\r\n  %189 = \"tfl.transpose\"(%188, %cst_6) : (tensor<1x60x23x512xf32>, tensor<4xi32>) -> tensor<1x512x60x23xf32>\r\n  %190 = \"tfl.pad\"(%189, %cst_0) : (tensor<1x512x60x23xf32>, tensor<4x2xi32>) -> tensor<1x512x62x25xf32>\r\n  %191 = \"tfl.transpose\"(%190, %cst_5) : (tensor<1x512x62x25xf32>, tensor<4xi32>) -> tensor<1x62x25x512xf32>\r\n  %192 = \"tfl.split\"(%cst_4, %191) {num_splits = 1 : i32} : (tensor<i32>, tensor<1x62x25x512xf32>) -> tensor<1x62x25x512xf32>\r\n  %193 = \"tfl.conv_2d\"(%192, %cst_47, %cst_84) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x62x25x512xf32>, tensor<512x3x3x512xf32>, tensor<512xf32>) -> tensor<1x60x23x512xf32>\r\n  %194 = \"tfl.transpose\"(%193, %cst_6) : (tensor<1x60x23x512xf32>, tensor<4xi32>) -> tensor<1x512x60x23xf32>\r\n  %195 = \"tfl.add\"(%194, %184) {fused_activation_function = \"RELU\"} : (tensor<1x512x60x23xf32>, tensor<1x512x60x23xf32>) -> tensor<1x512x60x23xf32>\r\n  %196 = \"tfl.mean\"(%195, %cst_3) {keep_dims = true} : (tensor<1x512x60x23xf32>, tensor<2xi32>) -> tensor<1x512x1x1xf32>\r\n  %197 = \"tfl.cast\"(%26) : (tensor<2xi64>) -> tensor<2xi32>\r\n  %198 = \"tfl.reshape\"(%196, %197) : (tensor<1x512x1x1xf32>, tensor<2xi32>) -> tensor<1x512xf32>\r\n  %199 = \"tfl.fully_connected\"(%198, %cst_8, %cst_9) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<1x512xf32>, tensor<55x512xf32>, tensor<55xf32>) -> tensor<1x55xf32>\r\n  \"std.return\"(%199) : (tensor<1x55xf32>) -> ()\r\n}) {sym_name = \"main\", tf.entry_function = {control_outputs = \"\", inputs = \"input\", outputs = \"Identity\"}, type = (tensor<1x3x960x360xf32>) -> tensor<1x55xf32>} : () -> ()\r\n```\r\nWhen I use another method, the tflite model seem converted successful,\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(tfpath)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\ntflite_model = converter.convert()\r\nopen(\"test.tflite\", \"wb\").write(tflite_model)\r\n```\r\nBut since this is not quantized, when I deployed the model on my Coral USB TPU, it stated:\r\n`ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.` Which makes sense since I added` tf.lite.OpsSet.SELECT_TF_OPS`. And I used the edgetpu compiler, which gave me the same error. It seems I can only convert my tensorflow to tflite model successfully using this parameter 'tf.lite.OpsSet.SELECT_TF_OPS' (other wise I will have a wierd Javascript Error.\r\nI am kinda stuck between a unsupported, yet converted tflite model and a wierd error that did not even generated a tflite model.\r\n\r\nI also tried a definetaly unreasonable combination:\r\n```\r\ndef representative_dataset():\r\n    for _ in range(100):\r\n      data = np_input\r\n      yield [data.astype(np.float32)]\r\n        \r\nconverter = tf.lite.TFLiteConverter.from_saved_model(tfpath)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset\r\n**converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]**\r\nconverter.inference_input_type = tf.int8  # or tf.uint8\r\nconverter.inference_output_type = tf.int8 \r\ntflite_model = converter.convert()\r\nopen(\"test.tflite\", \"wb\").write(tflite_model)\r\n```\r\nWhich gave me a tflite model, with unsupported ops and uint8 as i/o\r\n\r\nDoes anyone met this problem and know how to solve this? Thank you! \r\n\r\n\r\n", "comments": ["@jingpu could you take a look at this? When EdgeTPU is involved, is the Flex delegate option unavailable?", "@Derricktao Not sure you already did this, but you will also need to compile the converted TFLite model with the Coral Edge TPU compiler. And if you want to accelerate ops on Coral,  they will need to be builtin TFLite ops with INT8/UNIT8 type.\r\n\r\n> When EdgeTPU is involved, is the Flex delegate option unavailable?\r\n\r\nNo. I don't think Flex delegate can be used at the same time. After the Edge TPU compiler, the output is a new TFLite model with edge tpu custom op, which requires a custom delegate for execution. I don't know there is a way to use two delegates at the same time.\r\n", "> I don't know there is a way to use two delegates at the same time.\r\n\r\n@jingpu Technically, possible. TFLite Python API is already enabling multi delegates, for example, XNNPack delegate + Flex delegate if there are Flex ops.", "@jingpu I tried the compiler, and it does showed the model is either not quantized or the ops is not supported.", "Hi @Derricktao , can you provide model file and colab for this? \r\n\r\nSeems `tf.AddV2` is the one that needs TF op - maybe because of its dtype? you can try modify its dtype to int32 and try again. For the model to be compiled for edge TPU, all ops should be quantized. Can you the the code again with tf-nightly, optionally, you can also pass `converter._experimental_new_quantizer = true` to try the new quantizer, if you still encounter error.\r\n\r\n\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(tfpath)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8  # or tf.uint8\r\nconverter.inference_output_type = tf.int8 \r\ntflite_model = converter.convert()\r\nopen(\"test.tflite\", \"wb\").write(tflite_model)\r\n```\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46792\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46792\">No</a>\n"]}, {"number": 46791, "title": "Do not use XLA. Build configuration", "body": "Hi, \r\n\r\nthis is related to building TensorFlow from source, but in case it's my wrongdoing I didn't want to mark it as a bug.\r\n\r\nTensorflow 2.2.1\r\nKeras 2.3.1\r\n\r\nI have a VAE model in which decoder is autoregressive and and one of the layer looks like this:\r\n\r\n```python\r\nclass AutoregressiveGRU(layers.Layer):\r\n\r\n    def __init__(\r\n            self,\r\n            output_dim: int,\r\n            output_len: int,\r\n            recurrent: layers.Recurrent,\r\n            **kwargs,\r\n    ):\r\n        self.output_dim = output_dim\r\n        self.output_len = output_len\r\n        self.initial_state = None\r\n        self.recurrent = recurrent\r\n        super(AutoregressiveGRU, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        super(AutoregressiveGRU, self).build(input_shape)\r\n\r\n    def call(self, x):\r\n        outputs = []\r\n        current_output = backend.zeros_like(backend.repeat(x, 1))\r\n        current_state = x\r\n        for _ in range(self.output_len):\r\n            current_output, current_state = self.recurrent(\r\n                current_output,\r\n                initial_state=current_state,\r\n            )\r\n            outputs.append(current_output)\r\n        result = layers.concatenate(outputs, axis=1)\r\n        result = backend.reshape(result, (-1, self.output_len, self.output_dim))\r\n        return result\r\n```\r\n\r\nI am calculating some higher-order gradients and I am getting errors such as:\r\n\r\n```shell\r\nInvalidArgumentError: Operation 'gradients_2/autoregressive_gru_1_3/gru_3/while_grad/autoregressive_gru_1_3/gru_3/while_grad' has no attr named '_XlaCompile'.\r\n\r\n```\r\n\r\nI looked at [XLA known issues](https://www.tensorflow.org/xla/known_issues) and found out that this might be related to the for-loop that is not bounded to constant number of iterations. I have rewritten the `for-loop` in that layer with `tf.while_loop` that has `maximum_iteration` parameter set, but the problem (error mentioned above) persists:\r\n\r\n```python\r\n    def call(self, x):\r\n        outputs = []\r\n        current_output = backend.zeros_like(backend.repeat(x, 1))\r\n        current_state = x\r\n        i = tf.constant(0)\r\n        c = lambda i, a, b, ta: i < 25\r\n        def turn(i, current_statex, current_outputx, ta):\r\n            current_output_cur, current_state_cur = self.recurrent(\r\n                current_outputx,\r\n                initial_state=current_statex,\r\n            )\r\n            ta.write(i, current_output_cur)\r\n            tf.add(i, 1)\r\n            return [i, current_state_cur, current_output_cur, ta]\r\n\r\n        i, a, b, ta =  tf.while_loop(c, turn, [i, current_state, current_output, tf.TensorArray(tf.float32, size=25)], maximum_iterations=tf.constant(25))\r\n        result = ta.stack()\r\n        return backend.reshape(result, (-1, self.output_len, self.output_dim))\r\n```\r\n\r\nI decided to drop XLA all together and so I compiled TensorFlow from source setting:\r\n```\r\nbuild:xla --action_env=TF_ENABLE_XLA=0\r\nbuild:xla --define=with_xla_support=false\r\n```\r\n\r\nAnd it still  comes with the same error. Is there something wrong with what I am doing?\r\n\r\nI apologize but I am not able to put the entire model here for various reasons. If I remove the autoregressive layer, everything works.\r\n\r\nEDIT:\r\n```\r\ntf.config.list_physical_devices()\r\n```\r\nnow returns only cpu `physical_device:CPU` ano no `physical_device:XLA_CPU`. So why is XLA even used during training? \r\n```\r\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\r\n```", "comments": ["Solved by cherry-picking https://github.com/tensorflow/tensorflow/commit/cf09044d9e7b232080f95f0f910a6803904df1de  and adapting to v.2.2.1. For anyone that cannot update to >v2.2.x:  https://github.com/prokotg/tensorflow/tree/v2.2.1-fix"]}, {"number": 46790, "title": "tf-nightly ResNet50V2 building error", "body": "**System information**\r\nColab\r\n\r\n**Describe the current behavior**\r\nTf-nightly notebook:\r\nhttps://colab.research.google.com/drive/1-A_C9bQL0cRdJ5F2ETlJl9GaZmukYVaq?usp=sharing\r\n\r\n**Describe the expected behavior**\r\nTf 2.4.1 notebook:\r\nhttps://colab.research.google.com/drive/1lRlGfQljmqAwOsP_npItl_aElCI9KPMD?usp=sharing", "comments": ["@ravikyram could you take a look, please?", "The break is introduced in https://github.com/tensorflow/tensorflow/commit/9700d00258607d5a81980a6fe602280dd0f4d2e1. What you do is to override attributes related to output shape inference (somehow risky). So it breaks the static shape inference when you try to build again. Any reason that you have to do that?", "Thank you very much, I was using a pre-trained model, therefore I wanted to leave it as is", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46790\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46790\">No</a>\n"]}, {"number": 46788, "title": "Win10: ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home 15063\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.8 (64-bit)\r\n- Installed using virtualenv? pip? conda?: pip inside virtualenv\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: no cuda (Intel HD Graphics)\r\n- GPU model and memory: Intel HD Graphics Integrated (Memory: N/A)\r\n\r\n**Describe the problem**\r\nIt looks like i am unable to import tensorflow. this is my first time starting with tensorflow and i was trying this code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist\r\n\r\nx_train = tf.keras.utils.normalize(x_train, axis=1)\r\nx_test = tf.keras.utils.normalize(x_test, axis=1)\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\r\nmodel.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\r\nmodel.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\r\n\r\nmodel.compile(optimizer=\"adam\",\r\n              loss=\"sparse_categorical_crossentropy\",\r\n              metrics=[\"accuracy\"])\r\n\r\nmodel.fit(x_train, y_train, epochs=3)\r\n\r\nmodel.save(\"m\\\\mnist_model.model\")\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nvenv\\Scripts\\activate.bat\r\npip install numpy tensorflow\r\npython main.py\r\n```\r\n\r\n\r\n**Any other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"D:\\Xcodz\\tensorflowmachinelearning\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:/Xcodz/tensorflowmachinelearning/main.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"D:\\Xcodz\\tensorflowmachinelearning\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"D:\\Xcodz\\tensorflowmachinelearning\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 39, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"D:\\Xcodz\\tensorflowmachinelearning\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Xcodz\\tensorflowmachinelearning\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "comments": ["@xcodz-dot,\r\nYou might be facing this issue because of the following reasons\r\n\r\n- You are running 32-bit Python or 32-bit OS\r\n- You have not installed the [Microsoft Visual C++ Redistributable](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads) package\r\n- Your CPU does not support AVX instructions. \r\n\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check this similar duplicate issues: #36167\r\n\r\nThanks!", "Ok, your point three is true that my cpu does not support avx instructions. I have 64-bit laptop with 64-bit Windows and 64-bit python/", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46788\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46788\">No</a>\n"]}, {"number": 46787, "title": "Add Torch (Flash light) \"Android\"", "body": "Hi,\r\nI wanna add Torch to this app. sometime light is not good  in area so I have to use flash light in app\r\nBut I can't use this code in app \r\nmCameraManager.setTorchMode(mCameraId, true);\r\nThe error is: camera \"0\" is in use\r\nOr flash light just blink for a second\r\nHow can I use this feature in app?", "comments": ["@MeysamSMH7\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps \r\nfollowed before you ran into this error or stand alone code to reproduce the issue faced]", "@Saduf2019\r\nhi, that is not an error, its just a feature. i`ve found it, its just add these lines to CameraConnectionFragment.java\r\n\r\nif (new sharedPref(getContext()).getTorch())\r\n    previewRequestBuilder.set(CaptureRequest.FLASH_MODE, CaptureRequest.FLASH_MODE_TORCH);\r\nelse\r\n    previewRequestBuilder.set(CaptureRequest.FLASH_MODE, CaptureRequest.FLASH_MODE_OFF);\r\npreviewRequestBuilder.set(CaptureRequest.CONTROL_AE_MODE, CaptureRequest.CONTROL_AE_MODE_ON);\r\n \r\n                                    ", "@MeysamSMH7 where to add this? I am not seeing any class sharedPref in the project. For me it turns on for a second and it stops after that", "Ok seems removing the auto flash in configuration allows the flash to be controlled.\r\n```\r\n// Flash is automatically enabled when necessary.\r\n                previewRequestBuilder.set(\r\n                    CaptureRequest.CONTROL_AE_MODE, CaptureRequest.CONTROL_AE_MODE_ON_AUTO_FLASH);\r\n```\r\nBut this cannot be achieved with `CameraManager manager;` and `manager.setTorchMode(cameraId, true);`, but has to be controlled through the FLASH MODES.\r\n\r\nBasically in CameraActivity - create a variable `  protected CameraConnectionFragment camera2Fragment;` and assign the result of \r\n```\r\ncamera2Fragment =\r\n          CameraConnectionFragment.newInstance(\r\n```\r\n to that new variable;\r\n\r\nAfter that in camera connection fragment i have added\r\n```\r\npublic void turnFlash(boolean flag) {\r\n    previewRequestBuilder.set(CaptureRequest.FLASH_MODE, flag ? CaptureRequest.FLASH_MODE_TORCH : CaptureRequest.FLASH_MODE_OFF);\r\n    previewRequest = previewRequestBuilder.build();\r\n    try {\r\n      captureSession.setRepeatingRequest(\r\n              previewRequest, captureCallback, backgroundHandler);\r\n    } catch (CameraAccessException e) {\r\n      e.printStackTrace();\r\n    }\r\n  }\r\n```\r\nwhich turns the torch mode on or off based on the flag.\r\nThis method is called from `DetectorActivity` - from the onclick button\r\n"]}, {"number": 46786, "title": "C++ compilation of rule failed - Error tensorflow building from source", "body": "\r\n------------------------\r\n\r\n### System information\r\n\r\n\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n-   \r\n-   **TensorFlow installed from (source or binary)**: wanting to install from source\r\n-   **TensorFlow version (use command below)**: 2.3\r\n-   **Python version**: 3.5\r\n-   **Bazel version (if compiling from source)**: 3.1, when using bazel version in the console\r\n-   **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n-   **CUDA/cuDNN version**: 10.1 and 7.6\r\n-   **GPU model and memory**: Nvidia Geforce 740 \r\n-   **Exact command to reproduce**:\r\n\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nIm am trying to install tensorflow from source on a basically fresh installation of Ubuntu 16.04.\r\n\r\nI installed CUDA and CuDNN and installed bazel successfully. I then i used:\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout branch_name  r2.3\r\n./configure\r\n\r\n```\r\n\r\n\r\n### Source code / logs\r\n./configure generated following configuration:\r\n\r\n```\r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.5/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: \r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    /usr/local/cuda-10.1/targets/x86_64-linux/lib\r\n    /usr/local/cuda-10.1/targets/x86_64-linux/include\r\nFound cuDNN 7 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.0]: \r\n\r\n\r\nWARNING: XLA does not support CUDA compute capabilities lower than 3.5. Disable XLA when running on older GPUs.\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n```\r\nAnd then i ran\r\n ```\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nto build my tensorflow.\r\n\r\n\r\nAfter waiting a while i get following error messages:\r\n\r\n```\r\nC++ compilation of rule '//tensorflow/core/kernels:scatter_functor_gpu' failed (Exit 1)\r\n```\r\n\r\nAnyone knows the source of this error and how to fix it or how i could successfully build tensorflow?\r\nThe most important aspect of the installation by source is the gpu support so cuda=y and the compute capability set to 3.0 as my gpu has that compute capability which is the reason i need to build from source in the first place\r\n\r\n\r\n\r\n", "comments": ["@Herminello \r\n\r\nPlease, see tested build configurations from [here](https://www.tensorflow.org/install/source#gpu).Just to verify did you follow all the instructions from [here](https://www.tensorflow.org/install/source).\r\nPlease, share complete error log so it helps us in localizing the issue faster. Thanks!", "@ravikyram \r\n\r\nYes i followed all the instructions, i even tried upgrading gc version 5 to 7 but that didnt work out. I tried the second build configuration from the first link you provided: tf2.3 , python 3.5 , GCC 7, CUDA 10.1, cuDNN 7.6 and bazel 3.1.0\r\n\r\nHere is the the complete error log of the console after trying it again:\r\n```\r\nbazel build --config=cuda --local_ram_resources=2048  //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /home/hreich/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/hreich/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Found applicable config definition build:v2 in file /home/hreich/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda in file /home/hreich/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/hreich/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:linux in file /home/hreich/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/hreich/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /home/hreich/.cache/bazel/_bazel_hreich/ce70f23006de10ef478ad3c980ee8c42/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nDEBUG: /home/hreich/.cache/bazel/_bazel_hreich/ce70f23006de10ef478ad3c980ee8c42/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:5: \r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nWARNING: /home/hreich/tensorflow/tensorflow/core/BUILD:1749:1: in linkstatic attribute of cc_library rule //tensorflow/core:lib_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'cc_library', the error might have been caused by the macro implementation\r\nWARNING: /home/hreich/tensorflow/tensorflow/core/BUILD:2161:1: in linkstatic attribute of cc_library rule //tensorflow/core:framework_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'tf_cuda_library', the error might have been caused by the macro implementation\r\nWARNING: /home/hreich/tensorflow/tensorflow/core/BUILD:1774:1: in linkstatic attribute of cc_library rule //tensorflow/core:lib_headers_for_pybind: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'cc_library', the error might have been caused by the macro implementation\r\nWARNING: /home/hreich/tensorflow/tensorflow/python/BUILD:4662:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/hreich/tensorflow/tensorflow/python/BUILD:115:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nINFO: From ProtoCompile tensorflow/core/util/memmapped_file_system.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/example/example.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/tensor_shape.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/example/feature.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/resource_handle.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/types.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/example/example_parser_configuration.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/tensor.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/log_memory.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/profiler/protobuf/xplane.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/verifier_config.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/remote_fused_graph_execute_info.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/bfc_memory_map.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/queue_runner.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/util/test_log.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/transport_options.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/device_filters.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/data/experimental/snapshot.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/tensor_description.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/saver.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/tensor_slice.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/debug.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/profiler/profiler_options.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/device_properties.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/trackable_object_graph.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/control_flow.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/remote_tensor_handle.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/meta_graph.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/debug_event.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/saved_object_graph.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/graph_debug_info.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/tensor_bundle.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/tensorflow_server.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/struct.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/saved_model.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/rewriter_config.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/lib/core/error_codes.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/named_tensor.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/error_codes.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/config.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/cluster.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/versions.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/allocation_description.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/graph_transfer_info.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/step_stats.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/device_attributes.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/kernel_def.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/summary.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/cost_graph.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/attr_value.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/function.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/api_def.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/variable.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/node_def.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/util/saved_tensor_slice.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/graph.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/op_def.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/reader_base.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/stream_executor/dnn.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/util/event.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From Compiling tensorflow/core/util/proto/proto_utils.cc:\r\ntensorflow/core/util/proto/proto_utils.cc: In function 'bool tensorflow::proto_utils::IsCompatibleType(google::protobuf::FieldDescriptor::Type, tensorflow::DataType)':\r\ntensorflow/core/util/proto/proto_utils.cc:70:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\nINFO: From Compiling tensorflow/python/framework/python_op_gen.cc:\r\ntensorflow/python/framework/python_op_gen.cc: In function 'std::__cxx11::string tensorflow::{anonymous}::VectorToTuple(const std::vector<std::__cxx11::basic_string<char> >&)':\r\ntensorflow/python/framework/python_op_gen.cc:66:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < l.size(); ++i) {\r\n                   ~~^~~~~~~~~~\r\ntensorflow/python/framework/python_op_gen.cc: In function 'void tensorflow::{anonymous}::Unflatten(const string&, const std::vector<std::__cxx11::basic_string<char> >&, const string&, std::__cxx11::string*)':\r\ntensorflow/python/framework/python_op_gen.cc:78:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < output_sizes.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/python/framework/python_op_gen.cc:82:17: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       if (i + 1 < output_sizes.size()) {\r\n           ~~~~~~^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/python/framework/python_op_gen.cc: In member function 'virtual std::__cxx11::string tensorflow::{anonymous}::GenEagerPythonOp::Code()':\r\ntensorflow/python/framework/python_op_gen.cc:298:44: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = op_def_.input_arg_size(); i < params_no_default_.size(); ++i) {\r\n                                          ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/python/framework/python_op_gen.cc:334:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < attrs_.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~\r\ntensorflow/python/framework/python_op_gen.cc: In member function 'bool tensorflow::{anonymous}::GenEagerPythonOp::GetEagerFunctionSetup(const string&, std::__cxx11::string*)':\r\ntensorflow/python/framework/python_op_gen.cc:525:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < attrs_.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/python/framework/python_op_gen_internal.cc:\r\ntensorflow/python/framework/python_op_gen_internal.cc: In member function 'virtual std::__cxx11::string tensorflow::python_op_gen_internal::GenPythonOp::Code()':\r\ntensorflow/python/framework/python_op_gen_internal.cc:564:44: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = op_def_.input_arg_size(); i < params_no_default.size(); ++i) {\r\n                                          ~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/python/framework/python_op_gen_internal.cc:567:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < params_with_default.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From ProtoCompile tensorflow/core/grappler/costs/op_performance_data.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From Compiling tensorflow/core/grappler/optimizers/graph_optimizer_stage.cc:\r\ntensorflow/core/grappler/optimizers/graph_optimizer_stage.cc: In function 'tensorflow::Status tensorflow::grappler::GetTensorProperties(const tensorflow::grappler::GraphOptimizerContext&, const string&, const TensorProperties**)':\r\ntensorflow/core/grappler/optimizers/graph_optimizer_stage.cc:63:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (num_outputs == 0 || tensor_id.index() > num_outputs - 1) {\r\n                           ~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/grappler/optimizers/dependency_optimizer.cc:\r\nIn file included from tensorflow/core/grappler/optimizers/dependency_optimizer.cc:27:0:\r\n./tensorflow/core/grappler/optimizers/constant_folding.h:171:3: warning: multi-line comment [-Wcomment]\r\n   //               /    \\                            /    \\\r\n   ^\r\n./tensorflow/core/grappler/optimizers/constant_folding.h:173:3: warning: multi-line comment [-Wcomment]\r\n   //         /     \\                                       /     \\\r\n   ^\r\ntensorflow/core/grappler/optimizers/dependency_optimizer.cc:322:3: warning: multi-line comment [-Wcomment]\r\n   //    y --^> |      | --^> b           /\\\r\n   ^\r\nINFO: From Compiling tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:\r\ntensorflow/core/grappler/optimizers/auto_mixed_precision.cc: In member function 'std::unique_ptr<tensorflow::grappler::AutoMixedPrecisionLists> tensorflow::grappler::{anonymous}::AutoMixedPrecisionImpl::get_mixed_precision_lists() const':\r\ntensorflow/core/grappler/optimizers/auto_mixed_precision.cc:957:3: warning: control reaches end of non-void function [-Wreturn-type]\r\n   }\r\n   ^\r\ntensorflow/core/grappler/optimizers/auto_mixed_precision.cc: At global scope:\r\ntensorflow/core/grappler/optimizers/auto_mixed_precision.cc:615:6: warning: 'bool tensorflow::grappler::{anonymous}::GraphTypeTopologyView::HasNode(absl::lts_2020_02_25::string_view, const tensorflow::grappler::{anonymous}::TypeAttrId&) const' defined but not used [-Wunused-function]\r\n bool GraphTypeTopologyView::HasNode(absl::string_view node_name,\r\n      ^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/grappler/optimizers/auto_mixed_precision.cc: In member function 'tensorflow::Status tensorflow::grappler::{anonymous}::AutoMixedPrecisionImpl::Optimize()':\r\ntensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1280:5: warning: 'should_process' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if (should_process) {\r\n     ^~\r\nINFO: From Compiling tensorflow/core/grappler/optimizers/function_optimizer.cc:\r\ntensorflow/core/grappler/optimizers/function_optimizer.cc:804:6: warning: 'bool tensorflow::grappler::{anonymous}::CheckStringAttr(const tensorflow::Node*, absl::lts_2020_02_25::string_view)' defined but not used [-Wunused-function]\r\n bool CheckStringAttr(const Node* n, absl::string_view attr_name) {\r\n      ^~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/grappler/optimizers/implementation_selector.cc:\r\ntensorflow/core/grappler/optimizers/implementation_selector.cc: In function 'void tensorflow::grappler::UpdateForwardIdentityNodeDtype(tensorflow::grappler::utils::MutableNodeView*, const DataTypeVector&)':\r\ntensorflow/core/grappler/optimizers/implementation_selector.cc:133:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int pos = 0; pos < fanouts_vector.size(); ++pos) {\r\n                     ~~~~^~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/grappler/utils/graph_view.h:30:0,\r\n                 from ./tensorflow/core/grappler/optimizers/implementation_selector.h:28,\r\n                 from tensorflow/core/grappler/optimizers/implementation_selector.cc:16:\r\n./tensorflow/core/grappler/utils/graph_view_internal.h: At global scope:\r\n./tensorflow/core/grappler/utils/graph_view_internal.h:243:37: warning: inline function 'const FanoutViewT& tensorflow::grappler::utils::internal::NodeViewInternal<FaninViewT, FanoutViewT, GraphViewT, IsConst>::GetMissingFanin() const [with FaninViewT = tensorflow::grappler::utils::MutableFaninView; FanoutViewT = tensorflow::grappler::utils::MutableFanoutView; GraphViewT = tensorflow::grappler::utils::MutableGraphView; bool IsConst = false]' used but never defined\r\n   virtual inline const FanoutViewT& GetMissingFanin() const = 0;\r\n                                     ^~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:\r\ntensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc: In function 'tensorflow::Status tensorflow::grappler::{anonymous}::GetOutputDataType(const std::vector<tensorflow::OpInfo_TensorProperties>&, int, tensorflow::DataType*)':\r\ntensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:76:20: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (output_index >= output_props.size()) {\r\n       ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc: In member function 'tensorflow::Status tensorflow::grappler::UnaryElementwiseRewriter::ConstructScopedAllocatorNode(tensorflow::grappler::ScopedAllocatorOptimizer*, tensorflow::GraphDef*, tensorflow::grappler::NodeMap*, const std::vector<tensorflow::NodeDef*>&, const string&, tensorflow::DataType, int, const string&, const std::vector<tensorflow::TensorShape>&, const std::vector<tensorflow::grappler::{anonymous}::InputDesc>&, const tensorflow::TensorShape&)':\r\ntensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:523:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < inputs.size(); ++i) {\r\n                     ~~^~~~~~~~~~~~~~~\r\ntensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:550:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       for (int i = 0; i < inputs_to_first.size(); ++i) {\r\n                       ~~^~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc: In member function 'tensorflow::Status tensorflow::grappler::UnaryElementwiseRewriter::BuildSAConcatNode(tensorflow::GraphDef*, tensorflow::grappler::NodeMap*, const std::vector<tensorflow::NodeDef*>&, const std::set<std::__cxx11::basic_string<char> >&, const string&, tensorflow::DataType, int, const string&, const string&, const tensorflow::TensorShape&, std::vector<tensorflow::NodeDefBuilder::NodeOut>*)':\r\ntensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:590:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < ops.size(); ++i) {\r\n                     ~~^~~~~~~~~~~~\r\ntensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc: In member function 'tensorflow::Status tensorflow::grappler::UnaryElementwiseRewriter::RewireSubgraph(tensorflow::GraphDef*, tensorflow::grappler::NodeMap*, const std::vector<tensorflow::NodeDef*>&, const std::set<std::__cxx11::basic_string<char> >&, const string&, const string&)':\r\ntensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:711:33: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int op_idx = 0; op_idx < ops.size(); ++op_idx) {\r\n                          ~~~~~~~^~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/grappler/utils/graph_view.cc:\r\nIn file included from ./tensorflow/core/grappler/utils/graph_view.h:30:0,\r\n                 from tensorflow/core/grappler/utils/graph_view.cc:16:\r\n./tensorflow/core/grappler/utils/graph_view_internal.h:243:37: warning: inline function 'const FanoutViewT& tensorflow::grappler::utils::internal::NodeViewInternal<FaninViewT, FanoutViewT, GraphViewT, IsConst>::GetMissingFanin() const [with FaninViewT = tensorflow::grappler::utils::MutableFaninView; FanoutViewT = tensorflow::grappler::utils::MutableFanoutView; GraphViewT = tensorflow::grappler::utils::MutableGraphView; bool IsConst = false]' used but never defined\r\n   virtual inline const FanoutViewT& GetMissingFanin() const = 0;\r\n                                     ^~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/platform/cloud/gcs_file_system.cc:\r\ntensorflow/core/platform/cloud/gcs_file_system.cc: In function 'tensorflow::Status tensorflow::{anonymous}::ParseJson(tensorflow::StringPiece, Json::Value*)':\r\ntensorflow/core/platform/cloud/gcs_file_system.cc:193:16: warning: 'Reader' is deprecated: Use CharReader and CharReaderBuilder instead. [-Wdeprecated-declarations]\r\n   Json::Reader reader;\r\n                ^~~~~~\r\nIn file included from external/jsoncpp_git/include/json/json.h:11:0,\r\n                 from tensorflow/core/platform/cloud/gcs_file_system.cc:33:\r\nexternal/jsoncpp_git/include/json/reader.h:37:63: note: declared here\r\n     \"Use CharReader and CharReaderBuilder instead.\") JSON_API Reader {\r\n                                                               ^~~~~~\r\ntensorflow/core/platform/cloud/gcs_file_system.cc:193:16: warning: 'Json::Reader::Reader()' is deprecated: Use CharReader and CharReaderBuilder instead [-Wdeprecated-declarations]\r\n   Json::Reader reader;\r\n                ^~~~~~\r\nIn file included from external/jsoncpp_git/include/json/json.h:11:0,\r\n                 from tensorflow/core/platform/cloud/gcs_file_system.cc:33:\r\nexternal/jsoncpp_git/include/json/reader.h:56:3: note: declared here\r\n   Reader();\r\n   ^~~~~~\r\ntensorflow/core/platform/cloud/gcs_file_system.cc: In member function 'tensorflow::Status tensorflow::{anonymous}::GcsWritableFile::UploadToSession(const string&, tensorflow::uint64, tensorflow::uint64)':\r\ntensorflow/core/platform/cloud/gcs_file_system.cc:724:52: warning: 'file_size' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n                                          file_size - start_offset));\r\n                                          ~~~~~~~~~~^~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/platform/errors.h:26:0,\r\n                 from ./tensorflow/core/platform/cloud/auth_provider.h:21,\r\n                 from ./tensorflow/core/platform/cloud/gcs_file_system.h:24,\r\n                 from tensorflow/core/platform/cloud/gcs_file_system.cc:16:\r\n./tensorflow/core/platform/strcat.h: In member function 'tensorflow::Status tensorflow::{anonymous}::GcsWritableFile::RequestUploadSessionStatus(const string&, bool*, tensorflow::uint64*)':\r\n./tensorflow/core/platform/strcat.h:114:61: warning: 'file_size' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n       : piece_(digits_, FastUInt64ToBufferLeft(u64, digits_)) {}\r\n                                                             ^\r\ntensorflow/core/platform/cloud/gcs_file_system.cc:652:12: note: 'file_size' was declared here\r\n     uint64 file_size;\r\n            ^~~~~~~~~\r\ntensorflow/core/platform/cloud/gcs_file_system.cc: In member function 'virtual tensorflow::Status tensorflow::{anonymous}::GcsWritableFile::Sync()':\r\ntensorflow/core/platform/cloud/gcs_file_system.cc:598:38: warning: 'file_size' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n                        std::to_string(file_size - start_offset));\r\n                        ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/platform/cloud/gcs_file_system.cc:587:12: note: 'file_size' was declared here\r\n     uint64 file_size;\r\n            ^~~~~~~~~\r\ntensorflow/core/platform/cloud/gcs_file_system.cc: In member function 'tensorflow::Status tensorflow::GcsFileSystem::RenameObject(const string&, const string&)':\r\ntensorflow/core/platform/cloud/gcs_file_system.cc:1755:3: warning: 'done' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   if (!done) {\r\n   ^~\r\nINFO: From Compiling tensorflow/core/platform/cloud/oauth_client.cc:\r\ntensorflow/core/platform/cloud/oauth_client.cc: In member function 'virtual tensorflow::Status tensorflow::OAuthClient::ParseOAuthResponse(tensorflow::StringPiece, tensorflow::uint64, std::__cxx11::string*, tensorflow::uint64*)':\r\ntensorflow/core/platform/cloud/oauth_client.cc:277:16: warning: 'Reader' is deprecated: Use CharReader and CharReaderBuilder instead. [-Wdeprecated-declarations]\r\n   Json::Reader reader;\r\n                ^~~~~~\r\nIn file included from external/jsoncpp_git/include/json/json.h:11:0,\r\n                 from ./tensorflow/core/platform/cloud/oauth_client.h:21,\r\n                 from tensorflow/core/platform/cloud/oauth_client.cc:16:\r\nexternal/jsoncpp_git/include/json/reader.h:37:63: note: declared here\r\n     \"Use CharReader and CharReaderBuilder instead.\") JSON_API Reader {\r\n                                                               ^~~~~~\r\ntensorflow/core/platform/cloud/oauth_client.cc:277:16: warning: 'Json::Reader::Reader()' is deprecated: Use CharReader and CharReaderBuilder instead [-Wdeprecated-declarations]\r\n   Json::Reader reader;\r\n                ^~~~~~\r\nIn file included from external/jsoncpp_git/include/json/json.h:11:0,\r\n                 from ./tensorflow/core/platform/cloud/oauth_client.h:21,\r\n                 from tensorflow/core/platform/cloud/oauth_client.cc:16:\r\nexternal/jsoncpp_git/include/json/reader.h:56:3: note: declared here\r\n   Reader();\r\n   ^~~~~~\r\nINFO: From Compiling tensorflow/core/profiler/lib/profiler_session.cc:\r\nIn file included from tensorflow/core/profiler/lib/profiler_session.cc:37:0:\r\n./tensorflow/core/profiler/utils/derived_timeline.h: In member function 'void tensorflow::profiler::DerivedXLineBuilder::ExpandOrAddEvents(const std::vector<tensorflow::profiler::XEvent>&)':\r\n./tensorflow/core/profiler/utils/derived_timeline.h:40:31: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int level = 0; level < event_per_level.size(); ++level) {\r\n                         ~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/kernels/lookup_util.cc:\r\ntensorflow/core/kernels/lookup_util.cc: In member function 'virtual void tensorflow::lookup::{anonymous}::TextFileLineIterator::Next()':\r\ntensorflow/core/kernels/lookup_util.cc:135:46: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       if (std::max(key_index_, value_index_) >= tokens.size()) {\r\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\r\nINFO: From Compiling external/snappy/snappy-sinksource.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling tensorflow/core/platform/numbers.cc:\r\ntensorflow/core/platform/numbers.cc: In function 'std::__cxx11::string tensorflow::strings::HumanReadableNumBytes(tensorflow::int64)':\r\ntensorflow/core/platform/numbers.cc:459:8: warning: '%lld' directive output may be truncated writing between 1 and 19 bytes into a region of size between 7 and 8 [-Wformat-truncation=]\r\n string HumanReadableNumBytes(int64 num_bytes) {\r\n        ^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/platform/numbers.cc:459:8: note: directive argument in the range [0, 9223372036854775807]\r\nIn file included from /usr/include/stdio.h:936:0,\r\n                 from /usr/include/c++/7/cstdio:42,\r\n                 from /usr/include/c++/7/ext/string_conversions.h:43,\r\n                 from /usr/include/c++/7/bits/basic_string.h:6361,\r\n                 from /usr/include/c++/7/string:52,\r\n                 from ./tensorflow/core/platform/numbers.h:19,\r\n                 from tensorflow/core/platform/numbers.cc:15:\r\n/usr/include/x86_64-linux-gnu/bits/stdio2.h:65:44: note: '__builtin_snprintf' output between 3 and 22 bytes into a destination of size 8\r\n        __bos (__s), __fmt, __va_arg_pack ());\r\n                                            ^\r\nINFO: From Compiling tensorflow/stream_executor/device_description.cc:\r\ntensorflow/stream_executor/device_description.cc: In function 'bool stream_executor::ThreadDimOk(const stream_executor::DeviceDescription&, const stream_executor::ThreadDim&)':\r\ntensorflow/stream_executor/device_description.cc:130:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (total_threads > threads_per_block_limit) {\r\n       ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/util/padding.cc:\r\ntensorflow/core/util/padding.cc: In function 'tensorflow::Status tensorflow::CheckValidPadding(tensorflow::Padding, const std::vector<long long int>&, int, tensorflow::TensorFormat)':\r\ntensorflow/core/util/padding.cc:40:34: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (explicit_paddings.size() != 2 * num_dims) {\r\n         ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/framework/op_def_util.cc:\r\ntensorflow/core/framework/op_def_util.cc: In function 'tensorflow::Status tensorflow::OpDefCompatible(const tensorflow::OpDef&, const tensorflow::OpDef&)':\r\ntensorflow/core/framework/op_def_util.cc:664:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < old_in_ref.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/framework/op_def_util.cc:680:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < old_out_ref.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/framework/shape_inference.cc:\r\ntensorflow/core/framework/shape_inference.cc: In constructor 'tensorflow::shape_inference::InferenceContext::InferenceContext(int, const tensorflow::AttrSlice&, const tensorflow::OpDef&, const std::vector<tensorflow::PartialTensorShape>&, const std::vector<const tensorflow::Tensor*>&, const std::vector<tensorflow::PartialTensorShape>&, const std::vector<std::unique_ptr<std::vector<std::pair<tensorflow::PartialTensorShape, tensorflow::DataType> > > >&)':\r\ntensorflow/core/framework/shape_inference.cc:65:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < input_handle_shapes_and_types.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/framework/shape_inference.cc:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int j = 0; j < v->size(); ++j) {\r\n                     ~~^~~~~~~~~~~\r\ntensorflow/core/framework/shape_inference.cc: In member function 'tensorflow::Status tensorflow::shape_inference::InferenceContext::set_output(tensorflow::StringPiece, const std::vector<tensorflow::shape_inference::ShapeHandle>&)':\r\ntensorflow/core/framework/shape_inference.cc:126:14: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (size != shapes.size()) {\r\n         ~~~~~^~~~~~~~~~~~~~~~\r\ntensorflow/core/framework/shape_inference.cc: In member function 'tensorflow::Status tensorflow::shape_inference::InferenceContext::ExpandOutputs(int)':\r\ntensorflow/core/framework/shape_inference.cc:184:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (new_output_size < outputs_.size()) {\r\n       ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~\r\ntensorflow/core/framework/shape_inference.cc: In member function 'void tensorflow::shape_inference::InferenceContext::PostInputInit(std::vector<std::unique_ptr<std::vector<tensorflow::shape_inference::ShapeAndType> > >)':\r\ntensorflow/core/framework/shape_inference.cc:213:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (inputs_.size() != num_inputs_from_node_def) {\r\n       ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/framework/shape_inference.cc: In member function 'tensorflow::Status tensorflow::shape_inference::InferenceContext::MakeShapeFromShapeTensorTreatScalarAsUnknownShape(int, tensorflow::shape_inference::ShapeHandle*)':\r\ntensorflow/core/framework/shape_inference.cc:721:17: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (input_idx < input_tensors_as_shapes_.size() &&\r\n       ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/framework/shape_inference.cc: In member function 'tensorflow::Status tensorflow::shape_inference::InferenceContext::MakeShapeFromShapeTensor(int, tensorflow::shape_inference::ShapeHandle*)':\r\ntensorflow/core/framework/shape_inference.cc:739:17: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (input_idx < input_tensors_as_shapes_.size() &&\r\n       ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/framework/shape_inference.cc: In member function 'tensorflow::Status tensorflow::shape_inference::InferenceContext::AttachContext(const tensorflow::Status&)':\r\ntensorflow/core/framework/shape_inference.cc:1102:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < inputs_.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~\r\ntensorflow/core/framework/shape_inference.cc:1104:11: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n         i < input_tensors_as_shapes_.size() &&\r\n         ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/framework/shape_inference.cc:1109:48: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     } else if (requested_input_tensor_[i] && i < input_tensors_.size() &&\r\n                                              ~~^~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/framework/shape_inference.cc: In member function 'bool tensorflow::shape_inference::InferenceContext::MergeHandleShapesAndTypes(const std::vector<tensorflow::shape_inference::ShapeAndType>&, std::vector<tensorflow::shape_inference::ShapeAndType>*)':\r\ntensorflow/core/framework/shape_inference.cc:1143:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < shapes_and_types.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/framework/shape_inference.cc:1167:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < new_values.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/framework/shape_inference.cc: In member function 'bool tensorflow::shape_inference::InferenceContext::RelaxHandleShapesAndMergeTypes(const std::vector<tensorflow::shape_inference::ShapeAndType>&, std::vector<tensorflow::shape_inference::ShapeAndType>*)':\r\ntensorflow/core/framework/shape_inference.cc:1202:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < shapes_and_types.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/framework/tensor_util.cc:\r\ntensorflow/core/framework/tensor_util.cc: In function 'bool tensorflow::tensor::internal::PackedValuesNotEqual(T, T) [with T = float]':\r\ntensorflow/core/framework/tensor_util.cc:258:38: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n   return reinterpret_cast<int32_t&>(a) != reinterpret_cast<int32_t&>(b);\r\n                                      ^\r\ntensorflow/core/framework/tensor_util.cc:258:71: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n   return reinterpret_cast<int32_t&>(a) != reinterpret_cast<int32_t&>(b);\r\n                                                                       ^\r\ntensorflow/core/framework/tensor_util.cc: In function 'bool tensorflow::tensor::internal::PackedValuesNotEqual(T, T) [with T = double]':\r\ntensorflow/core/framework/tensor_util.cc:262:38: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n   return reinterpret_cast<int64_t&>(a) != reinterpret_cast<int64_t&>(b);\r\n                                      ^\r\ntensorflow/core/framework/tensor_util.cc:262:71: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n   return reinterpret_cast<int64_t&>(a) != reinterpret_cast<int64_t&>(b);\r\n                                                                       ^\r\nINFO: From Compiling tensorflow/core/framework/op_kernel.cc:\r\nIn file included from tensorflow/core/framework/op_kernel.cc:54:0:\r\n./tensorflow/core/platform/platform_strings.h:96:1: warning: multi-line comment [-Wcomment]\r\n // #define TF_PLAT_STR_(x) \\\r\n ^\r\nINFO: From Compiling tensorflow/core/util/dump_graph.cc:\r\ntensorflow/core/util/dump_graph.cc:169:8: warning: 'tensorflow::Status tensorflow::{anonymous}::WriteTextProtoToUniqueFile(const google::protobuf::MessageLite&, tensorflow::WritableFile*)' defined but not used [-Wunused-function]\r\n Status WriteTextProtoToUniqueFile(\r\n        ^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/framework/run_handler.cc:\r\ntensorflow/core/framework/run_handler.cc: In member function 'void tensorflow::internal::RunHandlerThreadPool::WorkerLoop(int, bool)':\r\ntensorflow/core/framework/run_handler.cc:643:35: warning: 'sub_thread_pool_id' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n         WaitForWorkInSubThreadPool(may_steal_blocking_work, sub_thread_pool_id);\r\n         ~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/common_runtime/lower_case_op.cc:\r\ntensorflow/core/common_runtime/lower_case_op.cc: In member function 'tensorflow::Status tensorflow::{anonymous}::CaseBuilder::AddInputs()':\r\ntensorflow/core/common_runtime/lower_case_op.cc:177:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 1; i < edges.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/common_runtime/gradients.cc:\r\ntensorflow/core/common_runtime/gradients.cc: In function 'tensorflow::Node* tensorflow::AddSymGrad(tensorflow::Graph*, tensorflow::Node*, tensorflow::gtl::ArraySlice<tensorflow::NodeOut>)':\r\ntensorflow/core/common_runtime/gradients.cc:133:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < out_types.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/gradients.cc: In constructor 'tensorflow::SymbolicGradientBuilder::SymbolicGradientBuilder(tensorflow::gtl::ArraySlice<tensorflow::NodeOut>, tensorflow::gtl::ArraySlice<tensorflow::NodeOut>, tensorflow::gtl::ArraySlice<tensorflow::NodeOut>, std::vector<tensorflow::NodeOut>*, tensorflow::Graph*)':\r\ntensorflow/core/common_runtime/gradients.cc:224:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < x_node_outputs_.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/gradients.cc: In member function 'tensorflow::Status tensorflow::SymbolicGradientBuilder::Compute()':\r\ntensorflow/core/common_runtime/gradients.cc:400:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < x_node_outputs_.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/grappler/optimizers/constant_folding.cc:\r\nIn file included from tensorflow/core/grappler/optimizers/constant_folding.cc:18:0:\r\n./tensorflow/core/grappler/optimizers/constant_folding.h:171:3: warning: multi-line comment [-Wcomment]\r\n   //               /    \\                            /    \\\r\n   ^\r\n./tensorflow/core/grappler/optimizers/constant_folding.h:173:3: warning: multi-line comment [-Wcomment]\r\n   //         /     \\                                       /     \\\r\n   ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3054:3: warning: multi-line comment [-Wcomment]\r\n   //                     / \\              / \\\r\n   ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3056:3: warning: multi-line comment [-Wcomment]\r\n   //                   / \\              / \\\r\n   ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3060:3: warning: multi-line comment [-Wcomment]\r\n   //                     / \\              / \\\r\n   ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3062:3: warning: multi-line comment [-Wcomment]\r\n   //                   / \\              / \\\r\n   ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3066:3: warning: multi-line comment [-Wcomment]\r\n   //                     / \\              / \\\r\n   ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3068:3: warning: multi-line comment [-Wcomment]\r\n   //                   / \\              / \\\r\n   ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3072:3: warning: multi-line comment [-Wcomment]\r\n   //                     / \\              / \\\r\n   ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3074:3: warning: multi-line comment [-Wcomment]\r\n   //                   / \\              / \\\r\n   ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3160:3: warning: multi-line comment [-Wcomment]\r\n   //                     / \\              / \\\r\n   ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3162:3: warning: multi-line comment [-Wcomment]\r\n   //                       / \\              / \\\r\n   ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3261:5: warning: multi-line comment [-Wcomment]\r\n     //  / \\\r\n     ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3263:5: warning: multi-line comment [-Wcomment]\r\n     //    / \\\r\n     ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3279:5: warning: multi-line comment [-Wcomment]\r\n     //                / \\      / \\      / \\    / \\     / \\      / \\\r\n     ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3281:5: warning: multi-line comment [-Wcomment]\r\n     //              / \\      / \\      / \\        / \\     / \\      / \\\r\n     ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3313:3: warning: multi-line comment [-Wcomment]\r\n   //                     /   \\                /    \\\r\n   ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3315:3: warning: multi-line comment [-Wcomment]\r\n   //                  / \\                          / \\\r\n   ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc: In function 'bool tensorflow::grappler::{anonymous}::PackedValuesNotEqual(T, T) [with T = float]':\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:145:38: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n   return reinterpret_cast<int32_t&>(a) != reinterpret_cast<int32_t&>(b);\r\n                                      ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:145:71: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n   return reinterpret_cast<int32_t&>(a) != reinterpret_cast<int32_t&>(b);\r\n                                                                       ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc: In function 'bool tensorflow::grappler::{anonymous}::PackedValuesNotEqual(T, T) [with T = double]':\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:150:38: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n   return reinterpret_cast<int64_t&>(a) != reinterpret_cast<int64_t&>(b);\r\n                                      ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:150:71: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n   return reinterpret_cast<int64_t&>(a) != reinterpret_cast<int64_t&>(b);\r\n                                                                       ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc: In member function 'tensorflow::Status tensorflow::grappler::ConstantFolding::MaterializeShapes(const tensorflow::grappler::GraphProperties&)':\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:482:37: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int port_idx = 0; port_idx < output.size(); ++port_idx) {\r\n                            ~~~~~~~~~^~~~~~~~~~~~~~~\r\ntensorflow/core/grappler/optimizers/constant_folding.cc: In member function 'tensorflow::Status tensorflow::grappler::ConstantFolding::MaterializeBroadcastGradientArgs(const tensorflow::NodeDef&, const tensorflow::grappler::GraphProperties&)':\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:644:31: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = common_dims; i < shape1.size(); ++i) {\r\n                             ~~^~~~~~~~~~~~~~~\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:649:31: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = common_dims; i < shape2.size(); ++i) {\r\n                             ~~^~~~~~~~~~~~~~~\r\ntensorflow/core/grappler/optimizers/constant_folding.cc: In function 'bool tensorflow::grappler::{anonymous}::IsValidConstShapeForMulConvPushDown(const string&, const tensorflow::TensorShapeProto&, const tensorflow::TensorShapeProto&)':\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:1168:40: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (mul_const_input_shape.dim_size() <= data_format.size() &&\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/grappler/optimizers/constant_folding.cc: In member function 'tensorflow::Status tensorflow::grappler::ConstantFolding::FoldNode(tensorflow::NodeDef*, tensorflow::GraphDef*, bool*)':\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:1464:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < const_nodes.size(); i++) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:1552:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n           } else if (port < const_nodes.size() &&\r\n                      ~~~~~^~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/grappler/optimizers/constant_folding.cc: In member function 'tensorflow::Status tensorflow::grappler::ConstantFolding::RemoveShuffleOrTranspose(const tensorflow::grappler::GraphProperties&, bool, tensorflow::GraphDef*, tensorflow::NodeDef*)':\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:2071:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (permutation.size() != shape.dim_size()) {\r\n         ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/grappler/optimizers/constant_folding.cc: In member function 'tensorflow::Status tensorflow::grappler::ConstantFolding::SimplifyStridedSlice(const tensorflow::grappler::GraphProperties&, bool, tensorflow::GraphDef*, tensorflow::NodeDef*)':\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:2249:13: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n           j >= ellipsis_index + expanded_ellipsis_indices.size()) {\r\n           ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/grappler/optimizers/constant_folding.cc: In member function 'bool tensorflow::grappler::ConstantFolding::PartialAssocOpConstFolding(tensorflow::GraphDef*, tensorflow::grappler::GraphProperties*, tensorflow::NodeDef*)':\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3482:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (const_inputs.size() == num_non_control_inputs &&\r\n       ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:3490:54: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (const_inputs.size() > 1 && const_inputs.size() < num_non_control_inputs &&\r\n                                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/core/grappler/optimizers/constant_folding.cc:55:0:\r\n./tensorflow/core/util/bcast.h: In instantiation of 'tensorflow::BCastList<N>::BCastList(const Vec (&)[N], bool, bool) [with int N = 2; tensorflow::BCastList<N>::Vec = absl::lts_2020_02_25::InlinedVector<long long int, 4>]':\r\n./tensorflow/core/util/bcast.h:360:52:   required from here\r\n./tensorflow/core/util/bcast.h:142:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (x[i].size() > largest_rank) {\r\n./tensorflow/core/util/bcast.h:179:24: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (copy[i].size() < largest_rank) {\r\nINFO: From Compiling tensorflow/core/common_runtime/propagator_debug_utils.cc:\r\ntensorflow/core/common_runtime/propagator_debug_utils.cc: In function 'const tensorflow::Tensor* tensorflow::GetTensorValueForDump(const tensorflow::Entry&)':\r\ntensorflow/core/common_runtime/propagator_debug_utils.cc:41:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\nINFO: From Compiling tensorflow/core/common_runtime/bfc_allocator.cc:\r\nIn file included from tensorflow/core/common_runtime/bfc_allocator.cc:16:0:\r\n./tensorflow/core/common_runtime/bfc_allocator.h: In member function 'std::__cxx11::string tensorflow::BFCAllocator::Chunk::DebugString(tensorflow::BFCAllocator*, bool)':\r\n./tensorflow/core/common_runtime/bfc_allocator.h:205:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       if (recurse && prev != BFCAllocator::kInvalidChunkHandle) {\r\n                      ~~~~~^~~~~~~~~~~~~~~\r\n./tensorflow/core/common_runtime/bfc_allocator.h:209:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       if (recurse && next != BFCAllocator::kInvalidChunkHandle) {\r\n                      ~~~~~^~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/bfc_allocator.cc: In member function 'tensorflow::BFCAllocator::ChunkHandle tensorflow::BFCAllocator::AllocateChunk()':\r\ntensorflow/core/common_runtime/bfc_allocator.cc:181:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (free_chunks_list_ != kInvalidChunkHandle) {\r\n       ~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/bfc_allocator.cc: In member function 'bool tensorflow::BFCAllocator::DeallocateFreeRegions(size_t)':\r\ntensorflow/core/common_runtime/bfc_allocator.cc:285:14: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     while (h != kInvalidChunkHandle) {\r\n            ~~^~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/bfc_allocator.cc: In member function 'void tensorflow::BFCAllocator::DeallocateRegions(const absl::lts_2020_02_25::flat_hash_set<void*>&)':\r\ntensorflow/core/common_runtime/bfc_allocator.cc:346:14: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     while (h != kInvalidChunkHandle) {\r\n            ~~^~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/bfc_allocator.cc: In member function 'void tensorflow::BFCAllocator::SplitChunk(tensorflow::BFCAllocator::ChunkHandle, size_t)':\r\ntensorflow/core/common_runtime/bfc_allocator.cc:606:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (h_neighbor != kInvalidChunkHandle) {\r\n       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/platform/default/logging.h:29:0,\r\n                 from ./tensorflow/core/platform/logging.h:27,\r\n                 from ./tensorflow/core/platform/status.h:24,\r\n                 from ./tensorflow/core/platform/errors.h:22,\r\n                 from ./tensorflow/core/platform/env.h:27,\r\n                 from ./tensorflow/core/common_runtime/allocator_retry.h:19,\r\n                 from ./tensorflow/core/common_runtime/bfc_allocator.h:27,\r\n                 from tensorflow/core/common_runtime/bfc_allocator.cc:16:\r\ntensorflow/core/common_runtime/bfc_allocator.cc: In member function 'void tensorflow::BFCAllocator::DeallocateRawInternal(void*)':\r\ntensorflow/core/common_runtime/bfc_allocator.cc:631:11: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(h != kInvalidChunkHandle);\r\n         ~~^~~~\r\n./tensorflow/core/platform/macros.h:83:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\ntensorflow/core/common_runtime/bfc_allocator.cc:631:3: note: in expansion of macro 'CHECK'\r\n   CHECK(h != kInvalidChunkHandle);\r\n   ^\r\ntensorflow/core/common_runtime/bfc_allocator.cc: In member function 'void tensorflow::BFCAllocator::Merge(tensorflow::BFCAllocator::ChunkHandle, tensorflow::BFCAllocator::ChunkHandle)':\r\ntensorflow/core/common_runtime/bfc_allocator.cc:677:10: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (h3 != kInvalidChunkHandle) {\r\n       ~~~^~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/bfc_allocator.cc: In member function 'tensorflow::BFCAllocator::ChunkHandle tensorflow::BFCAllocator::TryToCoalesce(tensorflow::BFCAllocator::ChunkHandle, bool)':\r\ntensorflow/core/common_runtime/bfc_allocator.cc:757:15: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (c->next != kInvalidChunkHandle && !ChunkFromHandle(c->next)->in_use()) {\r\n       ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/bfc_allocator.cc:767:15: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (c->prev != kInvalidChunkHandle && !ChunkFromHandle(c->prev)->in_use()) {\r\n       ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/bfc_allocator.cc: In member function 'bool tensorflow::BFCAllocator::MergeTimestampedChunks(size_t)':\r\ntensorflow/core/common_runtime/bfc_allocator.cc:806:11: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (h == kInvalidChunkHandle) {\r\n         ~~^~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/bfc_allocator.cc:835:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int ci = 0; ci < to_merge.size(); ++ci) {\r\n                    ~~~^~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/bfc_allocator.cc:841:11: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (h == kInvalidChunkHandle) continue;\r\n         ~~^~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/platform/default/logging.h:29:0,\r\n                 from ./tensorflow/core/platform/logging.h:27,\r\n                 from ./tensorflow/core/platform/status.h:24,\r\n                 from ./tensorflow/core/platform/errors.h:22,\r\n                 from ./tensorflow/core/platform/env.h:27,\r\n                 from ./tensorflow/core/common_runtime/allocator_retry.h:19,\r\n                 from ./tensorflow/core/common_runtime/bfc_allocator.h:27,\r\n                 from tensorflow/core/common_runtime/bfc_allocator.cc:16:\r\ntensorflow/core/common_runtime/bfc_allocator.cc: In member function 'virtual size_t tensorflow::BFCAllocator::RequestedSize(const void*) const':\r\ntensorflow/core/common_runtime/bfc_allocator.cc:873:11: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(h != kInvalidChunkHandle)\r\n         ~~^~~~\r\n./tensorflow/core/platform/macros.h:83:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\ntensorflow/core/common_runtime/bfc_allocator.cc:873:3: note: in expansion of macro 'CHECK'\r\n   CHECK(h != kInvalidChunkHandle)\r\n   ^\r\ntensorflow/core/common_runtime/bfc_allocator.cc: In member function 'virtual size_t tensorflow::BFCAllocator::AllocatedSize(const void*) const':\r\ntensorflow/core/common_runtime/bfc_allocator.cc:882:11: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(h != kInvalidChunkHandle)\r\n         ~~^~~~\r\n./tensorflow/core/platform/macros.h:83:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\ntensorflow/core/common_runtime/bfc_allocator.cc:882:3: note: in expansion of macro 'CHECK'\r\n   CHECK(h != kInvalidChunkHandle)\r\n   ^\r\ntensorflow/core/common_runtime/bfc_allocator.cc: In member function 'virtual tensorflow::int64 tensorflow::BFCAllocator::AllocationId(const void*) const':\r\ntensorflow/core/common_runtime/bfc_allocator.cc:891:11: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(h != kInvalidChunkHandle)\r\n         ~~^~~~\r\n./tensorflow/core/platform/macros.h:83:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\ntensorflow/core/common_runtime/bfc_allocator.cc:891:3: note: in expansion of macro 'CHECK'\r\n   CHECK(h != kInvalidChunkHandle)\r\n   ^\r\ntensorflow/core/common_runtime/bfc_allocator.cc: In member function 'std::__cxx11::string tensorflow::BFCAllocator::RenderOccupancy()':\r\ntensorflow/core/common_runtime/bfc_allocator.cc:946:14: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     while (h != kInvalidChunkHandle) {\r\n            ~~^~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/bfc_allocator.cc: In member function 'void tensorflow::BFCAllocator::DumpMemoryLog(size_t)':\r\ntensorflow/core/common_runtime/bfc_allocator.cc:1008:14: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     while (h != kInvalidChunkHandle) {\r\n            ~~^~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/bfc_allocator.cc: In member function 'tensorflow::MemoryDump tensorflow::BFCAllocator::RecordMemoryMapInternal()':\r\ntensorflow/core/common_runtime/bfc_allocator.cc:1103:14: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     while (h != kInvalidChunkHandle) {\r\n            ~~^~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/bfc_allocator.cc: In member function 'std::array<tensorflow::BFCAllocator::BinDebugInfo, 21> tensorflow::BFCAllocator::get_bin_debug_info()':\r\ntensorflow/core/common_runtime/bfc_allocator.cc:1157:14: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     while (h != kInvalidChunkHandle) {\r\n            ~~^~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/lib/io/inputbuffer.cc:\r\ntensorflow/core/lib/io/inputbuffer.cc: In member function 'tensorflow::Status tensorflow::io::InputBuffer::ReadNBytes(tensorflow::int64, std::__cxx11::string*)':\r\ntensorflow/core/lib/io/inputbuffer.cc:88:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (bytes_read < bytes_to_read) result->resize(bytes_read);\r\n       ~~~~~~~~~~~^~~~~~~~~~~~~~~\r\ntensorflow/core/lib/io/inputbuffer.cc: In member function 'tensorflow::Status tensorflow::io::InputBuffer::Hint(tensorflow::int64)':\r\ntensorflow/core/lib/io/inputbuffer.cc:207:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (bytes_to_read > size_) {\r\n       ~~~~~~~~~~~~~~^~~~~~~\r\ntensorflow/core/lib/io/inputbuffer.cc:233:46: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (errors::IsOutOfRange(s) && data.size() == bytes_to_read) {\r\n                                  ~~~~~~~~~~~~^~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/platform/platform_strings.cc:\r\nIn file included from tensorflow/core/platform/platform_strings.cc:16:0:\r\n./tensorflow/core/platform/platform_strings.h:96:1: warning: multi-line comment [-Wcomment]\r\n // #define TF_PLAT_STR_(x) \\\r\n ^\r\nINFO: From Compiling tensorflow/core/platform/default/stacktrace_handler.cc:\r\ntensorflow/core/platform/default/stacktrace_handler.cc: In function 'void tensorflow::testing::InstallStacktraceHandler()':\r\ntensorflow/core/platform/default/stacktrace_handler.cc:118:7: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]\r\n       (void)write(STDERR_FILENO, buf, strlen(buf));\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/platform/default/stacktrace_handler.cc:125:7: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]\r\n       (void)write(STDERR_FILENO, buf, strlen(buf));\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/platform/default/stacktrace_handler.cc: In function 'void tensorflow::testing::StacktraceHandler(int, siginfo_t*, void*)':\r\ntensorflow/core/platform/default/stacktrace_handler.cc:79:3: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]\r\n   (void)write(STDERR_FILENO, buf, strlen(buf));\r\n   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/platform/default/stacktrace_handler.cc:89:3: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]\r\n   (void)write(STDERR_FILENO, stacktrace.c_str(), stacktrace.length());\r\n   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/platform/default/stacktrace_handler.cc: In function 'void tensorflow::testing::SafePrintStackTrace()':\r\ntensorflow/core/platform/default/stacktrace_handler.cc:46:3: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]\r\n   (void)write(STDERR_FILENO, begin_msg, strlen(begin_msg));\r\n   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/platform/default/stacktrace_handler.cc:57:3: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]\r\n   (void)write(STDERR_FILENO, end_msg, strlen(end_msg));\r\n   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/lib/io/snappy/snappy_inputstream.cc:\r\ntensorflow/core/lib/io/snappy/snappy_inputstream.cc: In member function 'tensorflow::Status tensorflow::io::SnappyInputStream::Inflate()':\r\ntensorflow/core/lib/io/snappy/snappy_inputstream.cc:88:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < sizeof(uint32); ++i) {\r\n                   ~~^~~~~~~~~\r\ntensorflow/core/lib/io/snappy/snappy_inputstream.cc:90:34: warning: 'compressed_block_length' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n         (compressed_block_length << 8) |\r\n         ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\nINFO: From ProtoCompile tensorflow/core/protobuf/autotuning.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/conv_autotuning.pb.h:\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From Compiling tensorflow/core/common_runtime/placer_inspection_required_ops_utils.cc:\r\ntensorflow/core/common_runtime/placer_inspection_required_ops_utils.cc: In function 'tensorflow::Status tensorflow::{anonymous}::AddOutputIdentities(tensorflow::Node*, tensorflow::Graph*, std::unordered_set<std::__cxx11::basic_string<char> >*)':\r\ntensorflow/core/common_runtime/placer_inspection_required_ops_utils.cc:274:42: warning: 'identity_node' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n             << identity_node->DebugString();\r\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~^~\r\nIn file included from ./tensorflow/core/lib/core/errors.h:19:0,\r\n                 from ./tensorflow/core/framework/tensor_shape.h:23,\r\n                 from ./tensorflow/core/framework/partial_tensor_shape.h:20,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/function.h:30,\r\n                 from ./tensorflow/core/common_runtime/placer_inspection_required_ops_utils.h:35,\r\n                 from tensorflow/core/common_runtime/placer_inspection_required_ops_utils.cc:15:\r\n./tensorflow/core/platform/errors.h:73:48: warning: 'identity_node' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     ::tensorflow::Status _status = (__VA_ARGS__);        \\\r\n                                                ^\r\ntensorflow/core/common_runtime/placer_inspection_required_ops_utils.cc:252:11: note: 'identity_node' was declared here\r\n     Node* identity_node;\r\n           ^~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/common_runtime/immutable_executor_state.cc:\r\ntensorflow/core/common_runtime/immutable_executor_state.cc:268:6: warning: 'bool tensorflow::{anonymous}::ExtractScopedAllocatorAttr(const std::vector<int>&, int, tensorflow::AllocatorAttributes*)' defined but not used [-Wunused-function]\r\n bool ExtractScopedAllocatorAttr(const std::vector<int>& sc_attr,\r\n      ^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/common_runtime/inline_function_utils.cc:\r\ntensorflow/core/common_runtime/inline_function_utils.cc: In lambda function:\r\ntensorflow/core/common_runtime/inline_function_utils.cc:330:3: warning: control reaches end of non-void function [-Wreturn-type]\r\n   };\r\n   ^\r\nERROR: /home/hreich/tensorflow/tensorflow/core/util/BUILD:351:1: error executing shell command: '/bin/bash -c bazel-out/k8-opt-exec-50AE0418-ST-e0f78fafe98f9cd48032f8d6a9732902b210aba3b30f85849d3455fe9d9fb327/bin/tensorflow/tools/git/gen_git_source --generate \"$@\" --git_tag_override=${GIT_TAG_...' failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/hreich/.cache/bazel/_bazel_hreich/ce70f23006de10ef478ad3c980ee8c42/execroot/org_tensorflow/bazel-out/k8-opt-exec-50AE0418-ST-e0f78fafe98f9cd48032f8d6a9732902b210aba3b30f85849d3455fe9d9fb327/bin/tensorflow/tools/git/gen_git_source.runfiles/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 32, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 416.719s, Critical Path: 105.04s\r\nINFO: 2049 processes: 2049 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "```\r\nERROR: /home/hreich/tensorflow/tensorflow/core/util/BUILD:351:1: error executing shell command: '/bin/bash -c bazel-out/k8-opt-exec-50AE0418-ST-e0f78fafe98f9cd48032f8d6a9732902b210aba3b30f85849d3455fe9d9fb327/bin/tensorflow/tools/git/gen_git_source --generate \"$@\" --git_tag_override=${GIT_TAG_...' failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/hreich/.cache/bazel/_bazel_hreich/ce70f23006de10ef478ad3c980ee8c42/execroot/org_tensorflow/bazel-out/k8-opt-exec-50AE0418-ST-e0f78fafe98f9cd48032f8d6a9732902b210aba3b30f85849d3455fe9d9fb327/bin/tensorflow/tools/git/gen_git_source.runfiles/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 32, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n```\r\n\r\nThis signals that your python installation is broken.", "@Herminello \r\nCan you try updating to the latest stable version 2.6.0 and let us know if the issue still persists? ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46786\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46786\">No</a>\n"]}, {"number": 46785, "title": "Op with name (StatefulPartitionedCall/sequential/conv2d/BiasAdd) and type (Conv) kernel not found in CPUExecutionProvider", "body": "I followed the practices described in to build onnxruntime for all 4 targets (in android) https://github.com/microsoft/onnxruntime/blob/master/docs/ONNX_Runtime_for_Mobile_Platforms.md\r\n\r\n1. converted onnx to ort files with basic optimization via --optimization_level basic  & another ort files allowing onnx optimization in ort.\r\n\r\n2. Tried Release build with \r\n./build.sh --config Release--android --android_sdk_path /Android --android_ndk_path /Android/ndk/21.1.6352462/ --android_abi x86 --android_api 29 --minimal_build extended --use_nnapi  --disable_exceptions --build_shared_lib --skip_tests --include_ops_by_config <config file produced by step 1>\r\n(For all ABI types)\r\n3. My config files contain \r\n`ai.onnx;9;MatMul,Sub`\r\n`ai.onnx;12;Add,Conv,Gemm,MaxPool,Mul,Relu,Reshape,Softmax,Transpose`\r\n\r\nI am running on Api 30 on x86 emulator but getting exception on both types of (ort: one with basioc optimization & one without optimization)\r\n`exception of type Ort::Exception: Failed to find kernel for Conv(11) (node StatefulPartitionedCall/sequential/conv2d/BiasAdd). Op with name (StatefulPartitionedCall/sequential/conv2d/BiasAdd) and type (Conv) kernel not found in CPUExecutionProvider. No matching hash for 8328794455908578232\"`\r\nIs there kernel named \"BiasAdd\" ? \r\n\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46785\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46785\">No</a>\n"]}, {"number": 46784, "title": "Enabling GPU support for Tensorflow in virtualenv", "body": "I am attempting to setup `Tensorflow` with GPU support on virtualenv. \r\nI have a native version with system interpreter - `Tensorflow 2.4.1`, which works fine and does see the `GPU`. \r\nAnd  in virtualenv I use `Tensorflow 2.0.0` with Python 3.7.9. \r\n\r\nWhen running in the interpreter on the system with `Tensorflow 2.4.1` I receive following greeting message : \r\n\r\n`I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0`\r\n\r\nAnd the command `tf.config.experimental.list_physical_devices(\"GPU\")` shows my GPU. \r\nHowever perforing the same actions in virtualenv I do not receive this greeting message, and the output of the command above is an empty list. \r\n\r\n**System information**\r\n- OS Platform and Distribution : (Linux Ubuntu 20.04):\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7.9\r\n- Installed using pip in virtualenv\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: RTX 2080Ti\r\n\r\nI've installed the `Tensorflow 2.0.0` via pip inside virtualenv. And the `CUDA` setup was performed as described here \r\nhttps://www.tensorflow.org/install/gpu?hl=ur with some changes for `Ubuntu 20.04`.\r\n\r\nMy question is : what needs to be done in order to enable GPU support inside virtualenv? \r\n\r\n\r\n", "comments": ["@Godofnothing \r\nCan you please upgrade your tensorflow version on virtual env same as on your system where gpu is detected also verify [here](https://www.tensorflow.org/install/source#gpu).", "The problem is that the project I am working on and for which I have created this `virtualenv` requires `TF 2.0`, its scripts use some core functionality of `TF` like from `tensorflow.python`, `tensorflow.framework`, which were removed in later versions. ", "@Godofnothing\r\nThe cuda version you are using is 11.0 which is compatible with tf 2.4 and onward, please refer to the gpu compatibility link shared above, where tf 2.0 uses cuda 10 and cudnn 7.4", "Ok, thanks a lot for help!", "@Godofnothing\r\nCould you please move this issue to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46784\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46784\">No</a>\n"]}, {"number": 46783, "title": "Micro: port op FAKE_QUANT from Lite", "body": "@tensorflow/micro\r\n\r\n**System information**\r\nHost OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\nTensorFlow installed from (source or binary): source\r\nTensorflow version (commit SHA if source): master\r\nTarget platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge\r\n\r\n**Describe the problem**\r\nI am about to port The TF Lite kernel op FAKE_QUANT to TFLite Micro.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nPR 1: refactor flatbuffer_conversions parsing function\r\nPR 2: refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header with only the changes to pass internal CI build checks.\r\nPR 3: copy the kernel from lite to micro any make the micro op and its testing code to work.\r\n", "comments": ["As far as I know, this op is only used to calibrate models for quantization within TFLite. Since the TFLM runtime expects quantized models, I cannot think of a use case for this operator within TFLM.\r\n\r\nLet me know if I'm missing something here.", "Closing this issue per Nat's comment.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46783\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46783\">No</a>\n", "Hallo, I used quantisation-aware training according to [this](https://www.tensorflow.org/model_optimization/guide/quantization/training_example) official example. After converting the model to a tflite model it still has this \"fake_quant\" layers. Is there a way to get rid of this layers in the tflite model or convert them into somthing else like \"Quantize\"? Or is this quantization-aware training just not possible with Tflite Micro?"]}, {"number": 46782, "title": "tf.keras.Model.fit() training works fine but custom training loop fails for identical model, optimizer, and loss function", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Anaconda\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nv2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n\r\n**Describe the current behavior**\r\nCode\r\n`from tensorflow.python.eager import backprop\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport math\r\n\r\nprint(\"TensorFlow version: {}\".format(tf.__version__))\r\nprint(\"Eager execution: {}\".format(tf.executing_eagerly()))\r\n\r\n\r\n#Equation to generate data\r\n#y = x * 0.1\r\ndef calulate(x):\r\n    return sum([i*0.1 for i in x])\r\n\r\n#Generate samples in batch\r\ndef samples(size):\r\n    x = np.random.random((size, 4))\r\n    y = np.zeros((size))\r\n    for i in range(size):\r\n        y[i] = calulate(x[i])\r\n    return x,y\r\n\r\n#Model\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.Dense(1, input_shape=(4,))\r\n])\r\n\r\n#loss function and optimizer\r\nloss = tf.keras.losses.MeanSquaredError(reduction=\"auto\")\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\r\n\r\n#Validation function\r\ndef validate():\r\n    x = np.random.random((5, 4))\r\n    y = [calulate(i) for i in x]\r\n    e = model.predict(x)\r\n    print(\"{} ==> {} , {}\".format(x, y, e))\r\n\r\n#Model.fit\r\ndef train_default(epoch) :\r\n    x, y = samples(64000)\r\n    model.compile(optimizer=optimizer, loss=loss)\r\n    history = model.fit(x, y, epochs=epoch, validation_split = 0.2)\r\n\r\n\r\n#Custom train\r\ndef train_custom(steps):\r\n    x = []\r\n    y = []\r\n    for _ in range(2000):\r\n        xe, ye = samples(32)\r\n        x.append(xe)\r\n        y.append(ye)\r\n\r\n    for episod in range(steps):\r\n        episod_loss = tf.keras.metrics.Mean()\r\n        for i in range(2000):\r\n            with tf.GradientTape() as tape:\r\n                e = model(x[i], training=True)\r\n                l = loss(y[i], e)\r\n            grads = tape.gradient(l, model.trainable_weights)\r\n            #optimizer.minimize(l, model.trainable_variables, tape=tape)\r\n            optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n            episod_loss.update_state(l)\r\n        print(\"episod: {} loss: {}\".format(episod, episod_loss.result()))\r\n        print(model.trainable_weights)\r\n\r\n\r\n#Tensorflow extension\r\nloss_tracker = tf.keras.metrics.Mean(name=\"loss\")\r\nmae_metric = tf.keras.metrics.MeanAbsoluteError(name=\"mae\")\r\n\r\nclass CustomModel(tf.keras.Model):\r\n    def train_step(self, data):\r\n        x, y = data\r\n\r\n        with tf.GradientTape() as tape:\r\n            y_pred = self(x, training=True)  # Forward pass\r\n            l = loss(y, y_pred)\r\n\r\n        #Compute gradients\r\n        trainable_vars = self.trainable_variables\r\n        gradients = tape.gradient(l, trainable_vars)\r\n\r\n        #Update weights\r\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n\r\n        #Compute our own metrics\r\n        loss_tracker.update_state(l)\r\n        mae_metric.update_state(y, y_pred)\r\n        return {\"loss\": loss_tracker.result(), \"mae\": mae_metric.result()}\r\n\r\n    @property\r\n    def metrics(self):\r\n        return [loss_tracker, mae_metric]\r\n\r\ndef train_extended(steps):\r\n    #Construct an instance of CustomModel\r\n    inputs = tf.keras.Input(shape=(4,))\r\n    outputs = tf.keras.layers.Dense(1)(inputs)\r\n    model = CustomModel(inputs, outputs)\r\n\r\n    #We don't passs a loss or metrics here.\r\n    model.compile(optimizer=optimizer)\r\n\r\n    x = []\r\n    y = []\r\n    for _ in range(2000):\r\n        xe, ye = samples(32)\r\n        x.append(xe)\r\n        y.append(ye)\r\n\r\n    for episod in range(steps):\r\n        for i in range(2000):\r\n            model.fit(x[i], y[i], epochs=1, verbose= 0)\r\n        print(\"episod: {}\".format(episod))\r\n        print(model.trainable_variables)\r\n\r\n\r\n##Works\r\n#train_default(5)\r\n#validate()\r\n\r\n#Works\r\n#train_extended(5)\r\n#validate()\r\n\r\n#Don't work\r\ntrain_custom(5)\r\nvalidate()`\r\n**Describe the expected behavior**\r\ntrain_custom function should also works like other \r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@Georji-Ghosh,\r\nI did not face any errors while running the code with TF v2.4. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/fa5666b41e856603f5260dd6f343fef3/46782.ipynb). \r\n\r\nCould you please let us know the expected output, so that we can look into it. Thanks!", "@amahendrakar Thanks for taking time to look into the issue.\r\nThe code runs properly and there is no issue in execution. But the issue is in the result generated.\r\n1. When we train the model from scratch using either of \"train_default()\" or \"train_extended()\"  methods it works fine and all the weights goes ~0.1 as expected as the data generated using y=0.1x method and model has only one dense layer mapping input to output.\r\n`episod: 4\r\n[<tf.Variable 'dense_2/kernel:0' shape=(4, 1) dtype=float32, numpy=\r\narray([[0.10025591],\r\n       [0.09995212],\r\n       [0.10002392],\r\n       [0.10034888]], dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.00022549], dtype=float32)>]`\r\n2. When we train the model using custom training loop \"train_custom()\" it doesn't converges and the weights gets wrong values.\r\n`episod: 4 loss: 0.0034574554301798344\r\n[<tf.Variable 'dense_1/kernel:0' shape=(4, 1) dtype=float32, numpy=\r\narray([[0.00126073],\r\n       [0.00269012],\r\n       [0.00523321],\r\n       [0.0034958 ]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([0.19942063], dtype=float32)>]`\r\n\r\nMy expectation is that both ways of training with same amount of training data should generate the same kind of weights in the model. \r\n\r\nactually to be more specific model.fit is not working same as\r\n\r\n           with tf.GradientTape() as tape:\r\n                e = model(x[i], training=True)\r\n                l = loss(y[i], e)\r\n            grads = tape.gradient(l, model.trainable_weights)\r\n            #optimizer.minimize(l, model.trainable_variables, tape=tape)\r\n            optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n            episod_loss.update_state(l)", "Can someone please help me understand why the custom training has a different set of weights after training then model.fit training?  or am I missing some points here?", "@ymodak,\r\nWas able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/abb333c21663dcda56e0b4b896907b99/46782.ipynb). Thanks!", "@Georji-Ghosh Please take a look at [SO thread](https://stackoverflow.com/questions/63550752/tf2-gradienttape-vs-model-fit-why-does-gradienttape-doesnt-work) discussion and see if it helps. Thanks!", "Thanks for your input it resolve the issue for me I missed to convert the numpy array to tensor before sending it to the model and loss function.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46782\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46782\">No</a>\n"]}, {"number": 46781, "title": "cudnn 8.1.0 support ( + Bazel 4.0.0 )", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master 2.5.0\r\n- Python version: 3.9.1\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): Visual Studio 2019\r\n- CUDA/cuDNN version: 11.2/8.1.0\r\n- GPU model and memory: RTX3090 GDDR6 24GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nbuild failed\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nERROR: D:/repo/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:496:11: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:dot_op_emitter' failed (Exit 2): python.exe failed: error executing command\r\n```\r\n\r\n```\r\ncl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release\r\ncl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\include\\utility(246): warning C4244: 'initializing': conversion from '_Ty' to '_Ty1', possible loss of data\r\n        with\r\n        [\r\n            _Ty=uint64_t\r\n        ]\r\n        and\r\n        [\r\n            _Ty1=unsigned int\r\n        ]\r\nexternal/llvm-project/llvm/include\\llvm/ADT/SmallBitVector.h(725): note: see reference to function template instantiation 'std::pair<unsigned int,llvm::ArrayRef<uint64_t>>::pair<unsigned __int64,llvm::ArrayRef<uint64_t>,0>(std::pair<unsigned __int64,llvm::ArrayRef<uint64_t>> &&) noexcept' being compiled\r\nexternal/llvm-project/llvm/include\\llvm/ADT/SmallBitVector.h(724): note: see reference to function template instantiation 'std::pair<unsigned int,llvm::ArrayRef<uint64_t>>::pair<unsigned __int64,llvm::ArrayRef<uint64_t>,0>(std::pair<unsigned __int64,llvm::ArrayRef<uint64_t>> &&) noexcept' being compiled\r\nexternal/llvm-project/mlir/include\\mlir/Dialect/Linalg/Transforms/CodegenStrategy.h(74): error C2668: 'mlir::linalg::sfinae_enqueue': ambiguous call to overloaded function\r\nexternal/llvm-project/mlir/include\\mlir/Dialect/Linalg/Transforms/CodegenStrategy.h(51): note: could be 'void mlir::linalg::sfinae_enqueue<mlir::linalg::LinalgTilingPattern,LinalgOpType,mlir::linalg::LinalgTilingOptions>(mlir::OwningRewritePatternList &,OptionsType,mlir::MLIRContext *,llvm::StringRef,mlir::linalg::LinalgTransformationFilter)'\r\n        with\r\n        [\r\n            LinalgOpType=mlir::linalg::GenericOp,\r\n            OptionsType=mlir::linalg::LinalgTilingOptions\r\n        ]\r\nexternal/llvm-project/mlir/include\\mlir/Dialect/Linalg/Transforms/CodegenStrategy.h(38): note: or       'void mlir::linalg::sfinae_enqueue<mlir::linalg::LinalgTilingPattern,LinalgOpType,mlir::linalg::LinalgTilingOptions,std::enable_if<false,void>>(mlir::OwningRewritePatternList &,OptionsType,mlir::MLIRContext *,llvm::StringRef,mlir::linalg::LinalgTransformationFilter)'\r\n        with\r\n        [\r\n            LinalgOpType=mlir::linalg::GenericOp,\r\n            OptionsType=mlir::linalg::LinalgTilingOptions\r\n        ]\r\nexternal/llvm-project/mlir/include\\mlir/Dialect/Linalg/Transforms/CodegenStrategy.h(75): note: while trying to match the argument list '(mlir::OwningRewritePatternList, mlir::linalg::LinalgTilingOptions, mlir::MLIRContext *, std::string, mlir::linalg::LinalgTransformationFilter)'\r\nexternal/llvm-project/mlir/include\\mlir/Dialect/Linalg/Transforms/CodegenStrategy.h(72): note: while compiling class template member function 'mlir::OwningRewritePatternList mlir::linalg::Tile<LinalgOpType>::buildRewritePatterns(mlir::MLIRContext *,mlir::linalg::LinalgTransformationFilter)'\r\n        with\r\n        [\r\n            LinalgOpType=mlir::linalg::GenericOp\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\include\\memory(2536): note: see reference to class template instantiation 'mlir::linalg::Tile<LinalgOpType>' being compiled\r\n        with\r\n        [\r\n            LinalgOpType=mlir::linalg::GenericOp\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\include\\memory(2535): note: while compiling class template member function 'void std::default_delete<mlir::linalg::Tile<LinalgOpType>>::operator ()(_Ty *) noexcept const'\r\n        with\r\n        [\r\n            LinalgOpType=mlir::linalg::GenericOp,\r\n            _Ty=mlir::linalg::Tile<mlir::linalg::GenericOp>\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\include\\memory(2647): note: see reference to function template instantiation 'void std::default_delete<mlir::linalg::Tile<LinalgOpType>>::operator ()(_Ty *) noexcept const' being compiled\r\n        with\r\n        [\r\n            LinalgOpType=mlir::linalg::GenericOp,\r\n            _Ty=mlir::linalg::Tile<mlir::linalg::GenericOp>\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\include\\memory(2574): note: see reference to class template instantiation 'std::default_delete<mlir::linalg::Tile<LinalgOpType>>' being compiled\r\n        with\r\n        [\r\n            LinalgOpType=mlir::linalg::GenericOp\r\n        ]\r\nexternal/llvm-project/mlir/include\\mlir/Dialect/Linalg/Transforms/CodegenStrategy.h(160): note: see reference to class template instantiation 'std::unique_ptr<mlir::linalg::Tile<LinalgOpType>,std::default_delete<mlir::linalg::Tile<LinalgOpType>>>' being compiled\r\n        with\r\n        [\r\n            LinalgOpType=mlir::linalg::GenericOp\r\n        ]\r\ntensorflow/compiler/xla/service/cpu/dot_op_emitter.cc(325): note: see reference to function template instantiation 'mlir::linalg::CodegenStrategy &mlir::linalg::CodegenStrategy::tile<mlir::linalg::GenericOp>(mlir::linalg::LinalgTilingOptions,mlir::linalg::LinalgTransformationFilter::FilterFunction)' being compiled\r\n```", "comments": ["Hey, did you find some solution for it? I am facing the same issue on Fedora 32.", "Can you sync back to head and rebuild after `bazel clean --expunge`? Also, please report what commit are you building from", "success with latest master, closing the issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46781\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46781\">No</a>\n"]}, {"number": 46780, "title": "TFLite Converter Version  InCompatibility Issue", "body": "\r\n\r\n### 2. Code\r\n\r\nPlease find this [notebook](https://colab.research.google.com/drive/1slj7OwIZ8M567BDUyNLUI7w-Eag7b55R?usp=sharing)\r\n\r\n### 3. Failure after conversion\r\n\r\nModel conversion working with `tensorflow-2.3.0` and `tf-nightly` versions and not working with `tensorflow=2.4`(default colab version). Similary model inference is working only with `tensorflow-2.3.0` and not working with both `tensorflow-2.4` and `tf-nightly`\r\n\r\nCC: @abattery @khanhlvg ", "comments": ["I have tried in colab with TF 2.4 version and I am seeing converter error.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/01fbf9eed0166e6af71f59f0ec46ef10/untitled643.ipynb).Thanks!", "@ravikyram That is my point. Conversion is successful with `tensorflow-2.3` but erroring out with `tensorflow-2.4`", "You need to enable select TF op for this case in the conversion. We added more concise logics to fallback to TF ops. This is an intended behavior.", "TFLite's MaxPool op kernel does not support all inputs like the model's inputs in the gist. However, the older versions converted them without checking its capability to TFLite MaxPool operator. That was a wrong behavior. To reflect the capability of the TFLite MaxPool, the select TF option is needed since the TFLite MaxPool can not handle those inputs and it should select the fallback option via the select TF option.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46780\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46780\">No</a>\n", "Thanks, @abattery for a detailed explanation. It's working as expected."]}, {"number": 46778, "title": "fixed issue template link", "body": "When creating a new GitHub issue using the \"bug\" template, the `GitHub Policy` link results in a `404`.\r\nI am assuming it is meant to link to `ISSUE_TEMPLATE.md` instead of `ISSUES.md`.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46778) for more info**.\n\n<!-- need_sender_cla -->", "@de-code  Can you please sign CLA. Thanks!", "Is it possible to sign the CLA with a `@users.noreply.github.com` email on the commit? It looks like one needs to have a google account associated with it.", "@de-code Can you please make sure to use same GitHub username and email-id associated with it.", "I gave in and used another email address in order to be able to sign the cla.", "Links works for me. `ISSUES.md` is not the same as `ISSUE_TEMPLATE.md`", "Okay, looks like `ISSUES.md` has been moved back from `opensource_only` by a [commit 6 days ago](https://github.com/tensorflow/tensorflow/commit/556ab089ea32328845cee36c1745aa931d1e88ac#diff-ccf25e28bb6b96800a92313dfceb6c1a89d7ab1d932606565f2921bdeed08864)."]}]