[{"number": 36772, "title": "Error converting GraphDef to tflite - Check failed: GetOpWithOutput(model, output_array) Specified output array \"'TFLite_Detection_PostProcess'\" is not produced by any op in this graph", "body": "**System information**\r\n- OS Platform and Distribution = Win10\r\n- TensorFlow installed from binary - pip\r\n- TensorFlow version 1.13.1\r\n\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\n# Copy and paste here the exact command\r\ntflite_convert \\\r\n --output_file=/tmp/detect.tflite \\ \r\n --graph_def_file=tflite2/tflite_graph.pb \\ \r\n --input_shapes=1,300,300,3 \\\r\n --input_arrays=image_tensor \\ \r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n(tensorflow1) C:\\tensorflow1\\models\\research\\object_detection>tflite_convert \\ --output_file=/tmp/fuck.tflite \\ --graph_def_file=tflite2/tflite_graph.pb \\ --input_shapes=1,300,300,3 \\ --input_arrays=image_tensor \\ --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'\r\n2020-02-15 00:33:22.884597: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2020-02-15 00:33:24.579223: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-02-15 00:33:24.608227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce RTX 2080 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.845\r\npciBusID: 0000:2d:00.0\r\n2020-02-15 00:33:24.611885: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2020-02-15 00:33:24.616006: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2020-02-15 00:33:24.621212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll\r\n2020-02-15 00:33:24.623686: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll\r\n2020-02-15 00:33:24.628228: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll\r\n2020-02-15 00:33:24.631880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll\r\n2020-02-15 00:33:24.640338: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-02-15 00:33:24.642489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-02-15 00:33:24.644141: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-02-15 00:33:24.649750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce RTX 2080 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.845\r\npciBusID: 0000:2d:00.0\r\n2020-02-15 00:33:24.653135: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2020-02-15 00:33:24.656487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2020-02-15 00:33:24.658613: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll\r\n2020-02-15 00:33:24.660695: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll\r\n2020-02-15 00:33:24.662859: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll\r\n2020-02-15 00:33:24.665900: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll\r\n2020-02-15 00:33:24.670329: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-02-15 00:33:24.672979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-02-15 00:33:25.178111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-15 00:33:25.179912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2020-02-15 00:33:25.181515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2020-02-15 00:33:25.184222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6269 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 SUPER, pci bus id: 0000:2d:00.0, compute capability: 7.5)\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\jacks\\Anaconda3\\envs\\tensorflow1\\Scripts\\tflite_convert-script.py\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"C:\\Users\\jacks\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\lite\\python\\tflite_convert.py\", line 515, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"C:\\Users\\jacks\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\jacks\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\jacks\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"C:\\Users\\jacks\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\lite\\python\\tflite_convert.py\", line 511, in run_main\r\n    _convert_tf1_model(tflite_flags)\r\n  File \"C:\\Users\\jacks\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\lite\\python\\tflite_convert.py\", line 199, in _convert_tf1_model\r\n    output_data = converter.convert()\r\n  File \"C:\\Users\\jacks\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\lite\\python\\lite.py\", line 989, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\jacks\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py\", line 412, in toco_convert_graph_def\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"C:\\Users\\jacks\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py\", line 200, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-02-15 00:33:25.515637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2020-02-15 00:33:27.222702: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TFLite_Detection_PostProcess\r\n2020-02-15 00:33:27.230047: F tensorflow/lite/toco/tooling_util.cc:935] Check failed: GetOpWithOutput(model, output_array) Specified output array \"'TFLite_Detection_PostProcess'\" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00002c24 (most recent call first):\r\n  File \"C:\\Users\\jacks\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 52 in execute\r\n  File \"C:\\Users\\jacks\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\absl\\app.py\", line 250 in _run_main\r\n  File \"C:\\Users\\jacks\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\absl\\app.py\", line 299 in run\r\n  File \"C:\\Users\\jacks\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40 in run\r\n  File \"C:\\Users\\jacks\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 89 in main\r\n  File \"C:\\Users\\jacks\\Anaconda3\\envs\\tensorflow1\\Scripts\\toco_from_protos-script.py\", line 10 in <module>\r\n", "comments": ["@Jack-Hewson Did you try recent `TF1.15` version?. Can you please share a standalone code to reproduce the issue? share the model also. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36772\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36772\">No</a>\n", "Sorry I made the mistake in creating an inference graph and not the graph for tflite", "How do you solve it?", "@Jack-Hewson \r\n"]}, {"number": 36771, "title": "Building libtensorflow for v2.1.0 fails on mac with unrecognized flag \"-fno-constant-cfstrings\"", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OSX 10.15.3\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.1.0 (Version on branch git/r2.1)\r\n- Python version: Python 3.7.4\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): CC=gcc-8, CXX=g++8, version 8.3.0, \r\n- CUDA/cuDNN version: N/A, not installing with GPU\r\n- GPU model and memory: N/A, not installing with GPU\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nAt Airbnb we want to build by source the C++ API for tensorflow so we can integrate it with internal libraries. We want to use the latest C++ API and building for mac is a first step. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nHere are the minimal steps taken to reproduce the issue on my machine:\r\n\r\n```\r\n\u276f echo $PWD\r\n/Users/ray_zhang/home/tensorflow\r\n\u276f CXX=g++-8 CC=gcc-8 ./configure\r\n... (chose all the default options)\r\n\u276f bazel clean --expunge\r\n\u276f BAZEL_USE_CPP_ONLY_TOOLCHAIN=1 bazel build --action_env CC=gcc-8 --action_env CXX=g++-8 //tensorflow:libtensorflow.so\r\n```\r\n\r\nAnd I get the error:\r\n\r\n```\r\nERROR: /private/var/tmp/_bazel_ray_zhang/af5fde86ccd7201c52a3f430d09db563/external/curl/BUILD.bazel:32:1: C++ compilation of rule '@curl//:curl' failed (Exit 1)\r\ngcc-8: error: unrecognized command line option '-fno-constant-cfstrings'; did you mean '-mno-constant-cfstrings'?\r\nTarget //tensorflow:libtensorflow.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 244.628s, Critical Path: 53.75s\r\nINFO: 754 processes: 754 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nHere's a larger trace with the WARNING messages shortened(I'm guessing that is redundant):\r\n\r\n```\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=188\r\nINFO: Reading rc options for 'build' from /Users/ray_zhang/home/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --config=v2\r\nINFO: Reading rc options for 'build' from /Users/ray_zhang/home/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/Users/ray_zhang/anaconda/envs/bh/bin/python --action_env PYTHON_LIB_PATH=/Users/ray_zhang/anaconda/envs/bh/lib/python3.7/site-packages --python_path=/Users/ray_zhang/anaconda/envs/bh/bin/python --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /Users/ray_zhang/home/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /Users/ray_zhang/home/tensorflow/.tf_configure.bazelrc: --define with_xla_support=true\r\nWARNING: /Users/ray_zhang/home/tensorflow/tensorflow/core/BUILD:2174:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:abi.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /Users/ray_zhang/home/tensorflow/tensorflow/core/BUILD:2174:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:byte_order.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /Users/ray_zhang/home/tensorflow/tensorflow/core/BUILD:2174:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:context.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /Users/ray_zhang/home/tensorflow/tensorflow/core/BUILD:2174:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:cord.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /Users/ray_zhang/home/tensorflow/tensorflow/core/BUILD:2174:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:cpu_feature_guard.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n...\r\n'//tensorflow/core/platform:profile_utils/cpu_utils.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /Users/ray_zhang/home/tensorflow/tensorflow/core/BUILD:2367:12: in srcs attribute of cc_library rule //tensorflow/core:jpeg_internal: please do not import '//tensorflow/core/platform:jpeg.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /Users/ray_zhang/home/tensorflow/tensorflow/core/BUILD:2347:12: in srcs attribute of cc_library rule //tensorflow/core:gif_internal: please do not import '//tensorflow/core/platform:gif.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nINFO: Analyzed target //tensorflow:libtensorflow.so (165 packages loaded, 15597 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base /private/var/tmp/_bazel_ray_zhang/af5fde86ccd7201c52a3f430d09db563/sandbox\r\nINFO: From Compiling external/llvm/lib/Support/UnicodeCaseFold.cpp [for host]:\r\nexternal/llvm/lib/Support/UnicodeCaseFold.cpp:8:1: warning: multi-line comment [-Wcomment]\r\n //   utils/unicode-case-fold.py \\\r\n ^\r\nINFO: From Compiling external/llvm/lib/Support/VirtualFileSystem.cpp [for host]:\r\nexternal/llvm/lib/Support/VirtualFileSystem.cpp: In member function 'std::unique_ptr<llvm::vfs::RedirectingFileSystem::Entry> llvm::vfs::RedirectingFileSystemParser::parseEntry(llvm::yaml::Node*, llvm::vfs::RedirectingFileSystem*, bool)':\r\nexternal/llvm/lib/Support/VirtualFileSystem.cpp:1448:5: warning: 'Kind' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     switch (Kind) {\r\n     ^~~~~~\r\nINFO: From Compiling external/llvm/lib/MC/XCOFFObjectWriter.cpp [for host]:\r\nexternal/llvm/lib/MC/XCOFFObjectWriter.cpp: In member function 'void {anonymous}::XCOFFObjectWriter::writeSymbolName(const llvm::StringRef&)':\r\nexternal/llvm/lib/MC/XCOFFObjectWriter.cpp:349:17: warning: 'char* strncpy(char*, const char*, size_t)' specified bound 8 equals destination size [-Wstringop-truncation]\r\n     std::strncpy(Name, SymbolName.data(), XCOFF::NameSize);\r\n     ~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling external/llvm/lib/MC/MachObjectWriter.cpp [for host]:\r\nexternal/llvm/lib/MC/MachObjectWriter.cpp: In member function 'void llvm::MachObjectWriter::writeNlist(llvm::MachObjectWriter::MachSymbolData&, const llvm::MCAsmLayout&)':\r\nexternal/llvm/lib/MC/MachObjectWriter.cpp:381:13: warning: 'AliaseeInfo' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     Address = AliaseeInfo->StringIndex;\r\n     ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling external/llvm/lib/MC/ELFObjectWriter.cpp [for host]:\r\nexternal/llvm/lib/MC/ELFObjectWriter.cpp: In function 'uint64_t {anonymous}::ELFWriter::writeObject(llvm::MCAssembler&, const llvm::MCAsmLayout&)':\r\nexternal/llvm/lib/MC/ELFObjectWriter.cpp:1216:36: warning: 'AddrsigSection' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n       SectionOffsets[AddrsigSection] = std::make_pair(SecStart, SecEnd);\r\n                                    ^\r\nINFO: From Compiling external/nsync/internal/cv.c [for host]:\r\nexternal/nsync/internal/cv.c: In function 'void nsync::nsync_cv_init(nsync::nsync_cv*)':\r\nexternal/nsync/internal/cv.c:30:36: warning: 'void* memset(void*, int, size_t)' clearing an object of type 'nsync::nsync_cv' {aka 'struct nsync::nsync_cv_s_'} with no trivial copy-assignment; use value-initialization instead [-Wclass-memaccess]\r\n         memset (cv, 0, sizeof (*cv));\r\n                                    ^\r\nIn file included from external/nsync/public/nsync.h:20,\r\n                 from external/nsync/internal/cv.c:19:\r\nexternal/nsync/public/nsync_cv.h:86:16: note: 'nsync::nsync_cv' {aka 'struct nsync::nsync_cv_s_'} declared here\r\n typedef struct nsync_cv_s_ {\r\n                ^~~~~~~~~~~\r\nINFO: From Compiling external/nsync/internal/counter.c [for host]:\r\nexternal/nsync/internal/counter.c: In function 'nsync::nsync_counter_s_* nsync::nsync_counter_new(uint32_t)':\r\nexternal/nsync/internal/counter.c:39:28: warning: 'void* memset(void*, int, size_t)' clearing an object of type 'struct nsync::nsync_counter_s_' with no trivial copy-assignment; use value-initialization instead [-Wclass-memaccess]\r\n   memset (c, 0, sizeof (*c));\r\n                            ^\r\nexternal/nsync/internal/counter.c:29:8: note: 'struct nsync::nsync_counter_s_' declared here\r\n struct nsync_counter_s_ {\r\n        ^~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/platform/numbers.cc:\r\ntensorflow/core/platform/numbers.cc: In instantiation of 'T tensorflow::{anonymous}::locale_independent_strtonum(const char*, const char**) [with T = double]':\r\ntensorflow/core/platform/numbers.cc:197:60:   required from here\r\ntensorflow/core/platform/numbers.cc:65:21: warning: comparison of integer expressions of different signedness: 'int' and 'std::__cxx11::basic_string<char>::size_type' {aka 'long unsigned int'} [-Wsign-compare]\r\n   for (int i = 0; i < special_num_str.length(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/lib/strings/proto_serialization.cc:\r\ntensorflow/core/lib/strings/proto_serialization.cc: In function 'bool tensorflow::SerializeToBufferDeterministic(const google::protobuf::MessageLite&, char*, size_t)':\r\ntensorflow/core/lib/strings/proto_serialization.cc:75:44: warning: comparison of integer expressions of different signedness: 'size_t' {aka 'long unsigned int'} and 'int' [-Wsign-compare]\r\n   return !output_stream.HadError() && size == output_stream.ByteCount();\r\n                                       ~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy-sinksource.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nERROR: /private/var/tmp/_bazel_ray_zhang/af5fde86ccd7201c52a3f430d09db563/external/curl/BUILD.bazel:32:1: C++ compilation of rule '@curl//:curl' failed (Exit 1)\r\ngcc-8: error: unrecognized command line option '-fno-constant-cfstrings'; did you mean '-mno-constant-cfstrings'?\r\nTarget //tensorflow:libtensorflow.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 244.628s, Critical Path: 53.75s\r\nINFO: 754 processes: 754 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["@OneRaynyDay,\r\nIs this still an issue?\r\n\r\nCould you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same error. Thanks!", "Hi there, I no longer have the setup to reproduce this error unfortunately. I think we should mark it as closed :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36771\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36771\">No</a>\n", "@OneRaynyDay i got same problem now.\r\n\r\nIt seems \"-fno-constant-cfstrings\" is **clang**-specific option, not **GCC**. I'll try to build without this argument"]}, {"number": 36770, "title": "Memory logging if option is set.", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36770) for more info**.\n\n<!-- need_sender_cla -->", "Please ignore I entered the wrong address meant to merge this into my fork for public use.\r\nIf someone is interested in what I'm trying to do (output tensorflow memory data in a csv, let me know!", "@aninrusimha I am trying to log memory usage for each TF Operations. I am. unable to find a reliable way to do it. Is this what this commit is about ?", "Yes! TL;DR there was another project I was working on where I needed to look at the allocation data. the file [bfc_allocator.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/bfc_allocator.cc) has a method called `void BFCAllocator::MaybeWriteMemoryMap()` which writes a memory dump to 'TF_BFC_MEMORY_DUMP'. By default this behavior only occurs when there is an OOM error, but you can modify this behavior in order to better understand memory usage.\r\n"]}, {"number": 36769, "title": "Tensorflow installation error", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Dear Tensorflow  expert,\r\n\r\nI have tried to use miniconda to install Tensorflow cpu for Windows10. It went smoothly. However when I test it with simple line:\r\nimport tensorflow as tf\r\nThere is Import Error: DLL load failed: The specified module could not be found.\r\n\r\nHow can we fix it and have tensorflow installed completely? \r\n\r\nThank you in advance!\r\n![Tensorflow_installation_err1](https://user-images.githubusercontent.com/56452335/74577593-9a6ef980-4f55-11ea-8892-49e4bee39f43.PNG)\r\n![Tensorflow_installation_err2](https://user-images.githubusercontent.com/56452335/74577597-a0fd7100-4f55-11ea-9144-64a30dbf75c0.PNG)\r\n\r\n\r\n\r\n", "@Pinkmei  Please refer the [link](https://github.com/tensorflow/tensorflow/issues/36167) for similar issue and let us know if it helps.", "Thank you. I have installed it successfully.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36769\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36769\">No</a>\n"]}, {"number": 36768, "title": "Pix2Pix tutorial error: \"Cannot convert a symbolic Tensor (truediv:0) to a numpy array.\"", "body": "## URL(s) with the issue: \r\n\r\nhttps://www.tensorflow.org/tutorials/generative/pix2pix#load_the_dataset\r\n\r\n## Description of issue (what needs changing):\r\n\r\nplt.imshow(inp/255.0) is not valid in latest TF. Instead you must call .eval() on the Tensor objects to convert them to numpy.\r\n\r\n### Clear description\r\n\r\nThe existing code throws this error: \"Cannot convert a symbolic Tensor (truediv:0) to a numpy array.\"\r\nfrom this line: plt.imshow(inp/255.0)\r\n\r\nI was able to fix this by calling inp.eval(). But in order to call eval, you also must set up a session, which has not yet been explained at that point in the tutorial.\r\n\r\nThere may be a better way to get around this error -- I am new to TensorFlow. \r\nI'm guessing the tutorial contains legacy code from 1.x ? I hope there isn't more because it took me quite a while to find a fix!", "comments": ["@lcdavis13, I tried with Tf 2.1 and Latest Tf nightly version, its working as expected no error seen. Thanks ", "Hi, thanks for the response. It seems to work as expected if I add the line \"tf.enable_eager_execution()\" , which is not included in the tutorial. But it then throws a warning about this method being deprecated. I've tested on 3 different environments and all show the same issue when following the tutorial. I'm not using the nightly builds though, but rather the latest Anaconda releases. Tested on one linux (redhat) and two Windows 10 environments.", "@lcdavis13, Can you include gist or full error log to analyze the issue. Thanks! ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36767, "title": "Changes for DNNL1.x fused_batch_norm and Pooling Ops(Max and Avg).", "body": "", "comments": ["This PR has changes needed to support DNNL1.x fused_batch_norm and Pooling Ops(Max & Avg). This goes with other changes for DNNL1.x from Intel", "Heads-up: we have just found some unit test failure (with MKL DNN v1.0 build) and will provide fix next Monday. Thanks!", "@gzmkl Appreciate the heads up! Please mention me when you open the PR and I'll review it.", "@jojivk73 can you please resolve conflicts? Thanks!", "@penpornk conflicts and formatting should be resolved now. Please wait for @jojivk73 to ping you again for re-review. Thank you for again!", "Changes are complete now! Thank you", "Not able to see the reformating diff with \r\nclang+llvm-8.0.1-x86_64-linux-sles11.3/bin/clang-format -style=Google\r\n", "Avoiding any functional changes since the team is debugging issues. Will use execute_primitive for part of the code. Added TODOs"]}, {"number": 36766, "title": "TensorFlowLiteC.framework in flutter : Undefined symbols for architecture & using TensorFlowLiteC directly for iOS", "body": "**System information** - \r\nbazel --version 2.0.0\r\nTensorFlowLiteC.framework built as per [these instructions](https://github.com/tensorflow/tensorflow/blob/2fc54c5eacd547ba2cd9a6acf61ff131bb5f1bfd/tensorflow/lite/g3doc/guide/build_ios.md) \r\n\r\nflutter doctor\r\n```\r\nDoctor summary (to see all details, run flutter doctor -v):\r\n[\u2713] Flutter (Channel stable, v1.12.13+hotfix.8, on Mac OS X 10.15.3 19D76, locale en)\r\n[\u2717] Android toolchain - develop for Android devices\r\n    \u2717 Unable to locate Android SDK.\r\n      Install Android Studio from: https://developer.android.com/studio/index.html\r\n      On first launch it will assist you in installing the Android SDK components.\r\n      (or visit https://flutter.dev/setup/#android-setup for detailed instructions).\r\n      If the Android SDK has been installed to a custom location, set ANDROID_HOME to that location.\r\n      You may also want to add it to your PATH environment variable.\r\n\r\n \r\n[\u2713] Xcode - develop for iOS and macOS (Xcode 11.1)\r\n[!] Android Studio (not installed)\r\n[\u2713] VS Code (version 1.42.1)\r\n[\u2713] Connected device (1 available)\r\n\r\n! Doctor found issues in 2 categories.\r\n```\r\n\r\n**Steps to reproduce:**\r\n\r\n`git clone https://github.com/am15h/tflite_flutter_plugin`\r\n`cd tflite_flutter_plugin`\r\n`git checkout ios_support`\r\n`flutter pub get`\r\n`cd example/ios && pod install`\r\n`cd .. && flutter run`\r\n\r\n\r\n<details>\r\n<summary><b>Error log when running on iOS simulator</b> similar error log with iPhone arm64 device</summary>\r\n<pre>\r\nMDGs-iMac:example amish$ flutter run\r\n \r\nLaunching lib/main.dart on iPhone 11 Pro Max in debug mode...\r\nWarning: Missing build name (CFBundleShortVersionString).\r\nWarning: Missing build number (CFBundleVersion).\r\nAction Required: You must set a build name and number in the pubspec.yaml file version field before submitting to the App\r\nStore.\r\n \r\nRunning pod install...                                              3.5s\r\nRunning Xcode build...                                                  \r\n                                                   \r\n \u251c\u2500Assembling Flutter resources...                           7.8s\r\n \u2514\u2500Compiling, linking and signing...                         6.0s\r\nXcode build done.                                           32.7s\r\nFailed to build iOS app\r\nError output from Xcode build:\r\n\u21b3\r\n    ** BUILD FAILED **\r\n\r\n\r\nXcode's output:\r\n\u21b3\r\n    Undefined symbols for architecture x86_64:\r\n      \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::assign(char const*, unsigned\r\n      long)\", referenced from:\r\n          absl::time_internal::cctz::TimeZoneInfo::Load(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> > const&, absl::time_internal::cctz::ZoneInfoSource*) in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          absl::time_internal::cctz::ParsePosixSpec(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> > const&, absl::time_internal::cctz::PosixTimeZone*) in\r\n          TensorFlowLiteC(time_zone_posix_69842208220ebf392e752081d8d0bbbb.o)\r\n      \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::compare(unsigned long, unsigned\r\n      long, char const*) const\", referenced from:\r\n          absl::time_internal::cctz::TimeZoneIf::Load(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> > const&) in TensorFlowLiteC(time_zone_if_b1201e42997e5298e8b9b342eb441cd9.o)\r\n          absl::time_internal::cctz::(anonymous namespace)::FileZoneInfoSource::Open(std::__1::basic_string<char,\r\n          std::__1::char_traits<char>, std::__1::allocator<char> > const&) in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          absl::time_internal::cctz::(anonymous namespace)::AndroidZoneInfoSource::Open(std::__1::basic_string<char,\r\n          std::__1::char_traits<char>, std::__1::allocator<char> > const&) in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::mutex::~mutex()\", referenced from:\r\n          __GLOBAL__sub_I_time_zone_impl.cc in TensorFlowLiteC(time_zone_impl_70088ae3adaade7bd54e47cd293b4fcc.o)\r\n      \"typeinfo for std::__1::basic_streambuf<char, std::__1::char_traits<char> >\", referenced from:\r\n          typeinfo for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          typeinfo for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::basic_streambuf<char, std::__1::char_traits<char> >::imbue(std::__1::locale const&)\", referenced from:\r\n          vtable for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          vtable for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::mutex::lock()\", referenced from:\r\n          absl::time_internal::cctz::time_zone::Impl::LoadTimeZone(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> > const&, absl::time_internal::cctz::time_zone*) in\r\n          TensorFlowLiteC(time_zone_impl_70088ae3adaade7bd54e47cd293b4fcc.o)\r\n          absl::time_internal::cctz::time_zone::Impl::ClearTimeZoneMapTestOnly() in\r\n          TensorFlowLiteC(time_zone_impl_70088ae3adaade7bd54e47cd293b4fcc.o)\r\n      \"std::__1::basic_streambuf<char, std::__1::char_traits<char> >::setbuf(char*, long)\", referenced from:\r\n          vtable for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          vtable for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"vtable for __cxxabiv1::__class_type_info\", referenced from:\r\n          typeinfo for absl::time_internal::cctz::TimeZoneIf in\r\n          TensorFlowLiteC(time_zone_if_b1201e42997e5298e8b9b342eb441cd9.o)\r\n          typeinfo for std::__1::__function::__base<std::__1::unique_ptr<absl::time_internal::cctz::ZoneInfoSource,\r\n          std::__1::default_delete<absl::time_internal::cctz::ZoneInfoSource> > (std::__1::basic_string<char,\r\n          std::__1::char_traits<char>, std::__1::allocator<char> > const&)> in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          typeinfo for absl::time_internal::cctz::TimeZoneInfo::Load(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> > const&)::$_1 in TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          typeinfo for absl::time_internal::cctz::ZoneInfoSource in\r\n          TensorFlowLiteC(zone_info_source_d0d641f6600cbc71bed9faf61afe560f.o)\r\n      NOTE: a missing vtable usually means the first non-inline virtual member function has no definition.\r\n      \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::erase(unsigned long, unsigned\r\n      long)\", referenced from:\r\n          absl::time_internal::cctz::FixedOffsetToAbbr(std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> > const&)\r\n          in TensorFlowLiteC(time_zone_fixed_12d6eb8ff52945a37ee8f97c1ea02823.o)\r\n      \"std::__1::basic_streambuf<char, std::__1::char_traits<char> >::sync()\", referenced from:\r\n          vtable for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          vtable for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::basic_streambuf<char, std::__1::char_traits<char> >::uflow()\", referenced from:\r\n          vtable for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          vtable for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::basic_ostream<char, std::__1::char_traits<char> >::operator<<(unsigned long)\", referenced from:\r\n          absl::time_internal::cctz::TimeZoneInfo::Description() const in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::basic_streambuf<char, std::__1::char_traits<char> >::xsputn(char const*, long)\", referenced from:\r\n          vtable for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          vtable for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::basic_ostream<char, std::__1::char_traits<char> >::~basic_ostream()\", referenced from:\r\n          construction vtable for std::__1::basic_ostream<char, std::__1::char_traits<char>\r\n          >-in-std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          construction vtable for std::__1::basic_ostream<char, std::__1::char_traits<char>\r\n          >-in-std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::basic_ostream<char, std::__1::char_traits<char> >::~basic_ostream()\", referenced from:\r\n          construction vtable for std::__1::basic_ostream<char, std::__1::char_traits<char>\r\n          >-in-std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          construction vtable for std::__1::basic_ostream<char, std::__1::char_traits<char>\r\n          >-in-std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"virtual thunk to std::__1::basic_ostream<char, std::__1::char_traits<char> >::~basic_ostream()\", referenced from:\r\n          construction vtable for std::__1::basic_ostream<char, std::__1::char_traits<char>\r\n          >-in-std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          construction vtable for std::__1::basic_ostream<char, std::__1::char_traits<char>\r\n          >-in-std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"virtual thunk to std::__1::basic_ostream<char, std::__1::char_traits<char> >::~basic_ostream()\", referenced from:\r\n          construction vtable for std::__1::basic_ostream<char, std::__1::char_traits<char>\r\n          >-in-std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          construction vtable for std::__1::basic_ostream<char, std::__1::char_traits<char>\r\n          >-in-std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::basic_ostream<char, std::__1::char_traits<char> >::sentry::sentry(std::__1::basic_ostream<char,\r\n      std::__1::char_traits<char> >&)\", referenced from:\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::ios_base::clear(unsigned int)\", referenced from:\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::basic_streambuf<char, std::__1::char_traits<char> >::xsgetn(char*, long)\", referenced from:\r\n          vtable for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          vtable for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::basic_ostream<char, std::__1::char_traits<char> >::sentry::~sentry()\", referenced from:\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::resize(unsigned long, char)\",\r\n      referenced from:\r\n          std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> >::overflow(int) in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> >::overflow(int) in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::clog\", referenced from:\r\n          absl::time_internal::cctz::TimeZoneInfo::CheckTransition(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> > const&, absl::time_internal::cctz::TransitionType const&, int, bool,\r\n          std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          absl::time_internal::cctz::TimeZoneInfo::ExtendTransitions(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> > const&, absl::time_internal::cctz::TimeZoneInfo::Header const&) in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::mutex::unlock()\", referenced from:\r\n          absl::time_internal::cctz::time_zone::Impl::LoadTimeZone(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> > const&, absl::time_internal::cctz::time_zone*) in\r\n          TensorFlowLiteC(time_zone_impl_70088ae3adaade7bd54e47cd293b4fcc.o)\r\n          absl::time_internal::cctz::time_zone::Impl::ClearTimeZoneMapTestOnly() in\r\n          TensorFlowLiteC(time_zone_impl_70088ae3adaade7bd54e47cd293b4fcc.o)\r\n      \"___cxa_end_catch\", referenced from:\r\n          std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> >::overflow(int) in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::vector<absl::time_internal::cctz::Transition, std::__1::allocator<absl::time_internal::cctz::Transition>\r\n          >::shrink_to_fit() in TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> >::overflow(int) in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::basic_ostream<char, std::__1::char_traits<char> >::operator<<(int)\", referenced from:\r\n          absl::time_internal::cctz::TimeZoneInfo::CheckTransition(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> > const&, absl::time_internal::cctz::TransitionType const&, int, bool,\r\n          std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"operator new(unsigned long)\", referenced from:\r\n          -[TFLBufferConvert convertWithEncoder:shape:sourceBuffer:convertedBuffer:] in\r\n          TensorFlowLiteC(buffer_convert_1d2d90cc5f78b8992cc785cc8c58fd0c.o)\r\n          -[TFLInferenceContext setInputDimensions:outputDimensions:taskDescriptors:] in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::__1::vector<unsigned int, std::__1::allocator<unsigned int> >::__vallocate(unsigned long) in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          void std::__1::vector<TFLComputeTask* __strong, std::__1::allocator<TFLComputeTask* __strong>\r\n          >::__emplace_back_slow_path<TFLComputeTask* __strong&>(TFLComputeTask* __strong&) in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::__1::__tree_iterator<std::__1::__value_type<unsigned int, tflite::gpu::StrongShape<(tflite::gpu::Layout)10> >,\r\n          std::__1::__tree_node<std::__1::__value_type<unsigned int, tflite::gpu::StrongShape<(tflite::gpu::Layout)10> >,\r\n          void*>*, long> std::__1::__tree<std::__1::__value_type<unsigned int,\r\n          tflite::gpu::StrongShape<(tflite::gpu::Layout)10> >, std::__1::__map_value_compare<unsigned int,\r\n          std::__1::__value_type<unsigned int, tflite::gpu::StrongShape<(tflite::gpu::Layout)10> >, std::__1::less<unsigned\r\n          int>, true>, std::__1::allocator<std::__1::__value_type<unsigned int,\r\n          tflite::gpu::StrongShape<(tflite::gpu::Layout)10> > > >::__emplace_hint_unique_key_args<unsigned int,\r\n          std::__1::pair<unsigned int const, tflite::gpu::StrongShape<(tflite::gpu::Layout)10> >\r\n          const&>(std::__1::__tree_const_iterator<std::__1::__value_type<unsigned int,\r\n          tflite::gpu::StrongShape<(tflite::gpu::Layout)10> >, std::__1::__tree_node<std::__1::__value_type<unsigned int,\r\n          tflite::gpu::StrongShape<(tflite::gpu::Layout)10> >, void*>*, long>, unsigned int const&, std::__1::pair<unsigned int\r\n          const, tflite::gpu::StrongShape<(tflite::gpu::Layout)10> > const&) in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::__1::pair<std::__1::__tree_iterator<id<MTLBuffer> __strong, std::__1::__tree_node<id<MTLBuffer> __strong,\r\n          void*>*, long>, bool> std::__1::__tree<std::__1::__value_type<unsigned int, id<MTLBuffer> __strong>,\r\n          std::__1::__map_value_compare<unsigned int, id<MTLBuffer> __strong, std::__1::less<unsigned int>, true>,\r\n          std::__1::allocator<id<MTLBuffer> __strong> >::__emplace_unique_key_args<unsigned int,\r\n          std::__1::piecewise_construct_t const&, std::__1::tuple<unsigned int const&>, std::__1::piecewise_construct_t\r\n          const&<> >(unsigned int const&, std::__1::piecewise_construct_t const&, std::__1::tuple<unsigned int const&>&&,\r\n          std::__1::piecewise_construct_t const&<>&&) in TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::__1::pair<std::__1::__tree_iterator<unsigned int, std::__1::__tree_node<unsigned int, void*>*, long>, bool>\r\n          std::__1::__tree<unsigned int, std::__1::less<unsigned int>, std::__1::allocator<unsigned int>\r\n          >::__emplace_unique_key_args<unsigned int, unsigned int const&>(unsigned int const&, unsigned int const&) in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          ...\r\n      \"std::__1::ios_base::init(void*)\", referenced from:\r\n          absl::operator<<(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, absl::uint128) in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          absl::time_internal::cctz::TimeZoneInfo::Description() const in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"typeinfo for std::length_error\", referenced from:\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(memory_management_b2355fe02092d3e625e8a39708405196.o)\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(internal_177af47ac7f30482d4096214acf6ba06.o)\r\n          std::__1::__throw_length_error(char const*) in\r\n          TensorFlowLiteC(min_cost_flow_assignment_3433aed886ccf32d0ddc192b47dca509.o)\r\n          std::__1::__throw_length_error(char const*) in\r\n          TensorFlowLiteC(greedy_by_size_assignment_9fa33a720f1e8acb4a8f6c0d76110984.o)\r\n          std::__1::__throw_length_error(char const*) in\r\n          TensorFlowLiteC(greedy_by_breadth_assignment_9652ad845cf0f94df849627b34cc7bc4.o)\r\n          ...\r\n      \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::append(unsigned long, char)\",\r\n      referenced from:\r\n          absl::operator<<(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, absl::uint128) in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          absl::time_internal::cctz::TimeZoneInfo::ResetToBuiltinUTC(std::__1::chrono::duration<long long, std::__1::ratio<1l,\r\n          1l> > const&) in TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::length_error::~length_error()\", referenced from:\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(memory_management_b2355fe02092d3e625e8a39708405196.o)\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(internal_177af47ac7f30482d4096214acf6ba06.o)\r\n          std::__1::__throw_length_error(char const*) in\r\n          TensorFlowLiteC(min_cost_flow_assignment_3433aed886ccf32d0ddc192b47dca509.o)\r\n          std::__1::__throw_length_error(char const*) in\r\n          TensorFlowLiteC(greedy_by_size_assignment_9fa33a720f1e8acb4a8f6c0d76110984.o)\r\n          std::__1::__throw_length_error(char const*) in\r\n          TensorFlowLiteC(greedy_by_breadth_assignment_9652ad845cf0f94df849627b34cc7bc4.o)\r\n          ...\r\n      \"std::__1::ios_base::getloc() const\", referenced from:\r\n          absl::operator<<(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, absl::uint128) in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>\r\n      >::basic_string(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned\r\n      long, unsigned long, std::__1::allocator<char> const&)\", referenced from:\r\n          absl::time_internal::cctz::TimeZoneIf::Load(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> > const&) in TensorFlowLiteC(time_zone_if_b1201e42997e5298e8b9b342eb441cd9.o)\r\n          absl::time_internal::cctz::(anonymous namespace)::FileZoneInfoSource::Open(std::__1::basic_string<char,\r\n          std::__1::char_traits<char>, std::__1::allocator<char> > const&) in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          absl::time_internal::cctz::(anonymous namespace)::AndroidZoneInfoSource::Open(std::__1::basic_string<char,\r\n          std::__1::char_traits<char>, std::__1::allocator<char> > const&) in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"___gxx_personality_v0\", referenced from:\r\n          -[TFLBufferConvert initWithDevice:isFloat16:convertToPBHWC4:] in\r\n          TensorFlowLiteC(buffer_convert_1d2d90cc5f78b8992cc785cc8c58fd0c.o)\r\n          -[TFLBufferConvert convertWithEncoder:shape:sourceBuffer:convertedBuffer:] in\r\n          TensorFlowLiteC(buffer_convert_1d2d90cc5f78b8992cc785cc8c58fd0c.o)\r\n          -[TFLInferenceContext compileModelWithDevice:taskDescriptors:outputBufferIDs:runtimeOptions:] in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          -[TFLInferenceContext setInputDimensions:outputDimensions:taskDescriptors:] in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          -[TFLInferenceContext encodeWithEncoder:inputOutputBuffers:encoderBlock:] in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::__1::vector<id<MTLBuffer> __strong, std::__1::allocator<id<MTLBuffer> > >::vector(unsigned long) in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          ...\r\n      \"std::__1::basic_streambuf<char, std::__1::char_traits<char> >::~basic_streambuf()\", referenced from:\r\n          absl::operator<<(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, absl::uint128) in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >::~basic_ostringstream()\r\n          in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >::~basic_ostringstream()\r\n          in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          virtual thunk to std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char>\r\n          >::~basic_ostringstream() in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          virtual thunk to std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char>\r\n          >::~basic_ostringstream() in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> >::~basic_stringbuf() in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> >::~basic_stringbuf() in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          ...\r\n      \"std::__1::basic_ostream<char, std::__1::char_traits<char> >::~basic_ostream()\", referenced from:\r\n          absl::operator<<(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, absl::uint128) in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >::~basic_ostringstream()\r\n          in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >::~basic_ostringstream()\r\n          in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          virtual thunk to std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char>\r\n          >::~basic_ostringstream() in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          virtual thunk to std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char>\r\n          >::~basic_ostringstream() in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          absl::time_internal::cctz::TimeZoneInfo::Description() const in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >::~basic_ostringstream()\r\n          in TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          ...\r\n      \"std::__1::basic_ostream<char, std::__1::char_traits<char> >::operator<<(unsigned long long)\", referenced from:\r\n          absl::operator<<(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, absl::uint128) in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n      \"std::__1::thread::hardware_concurrency()\", referenced from:\r\n          absl::base_internal::NumCPUs() in TensorFlowLiteC(sysinfo_7400a2bc43f21361805bc698a3f484fd.o)\r\n          absl::base_internal::NominalCPUFrequency() in TensorFlowLiteC(sysinfo_7400a2bc43f21361805bc698a3f484fd.o)\r\n      \"std::__1::ctype<char>::id\", referenced from:\r\n          absl::operator<<(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, absl::uint128) in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::chrono::steady_clock::now()\", referenced from:\r\n          absl::base_internal::CycleClock::Now() in TensorFlowLiteC(cycleclock_5b17d6cd121f92a8007a903553fab797.o)\r\n      \"___cxa_begin_catch\", referenced from:\r\n          ___clang_call_terminate in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> >::overflow(int) in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::vector<absl::time_internal::cctz::Transition, std::__1::allocator<absl::time_internal::cctz::Transition>\r\n          >::shrink_to_fit() in TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> >::overflow(int) in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::__vector_base_common<true>::__throw_length_error() const\", referenced from:\r\n          std::__1::enable_if<(__is_forward_iterator<unsigned int*>::value) && (is_constructible<unsigned int,\r\n          std::__1::iterator_traits<unsigned int*>::reference>::value), void>::type std::__1::vector<unsigned int,\r\n          std::__1::allocator<unsigned int> >::assign<unsigned int*>(unsigned int*, unsigned int*) in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::__1::vector<unsigned int, std::__1::allocator<unsigned int> >::__vallocate(unsigned long) in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          void std::__1::vector<TFLComputeTask* __strong, std::__1::allocator<TFLComputeTask* __strong>\r\n          >::__emplace_back_slow_path<TFLComputeTask* __strong&>(TFLComputeTask* __strong&) in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          void std::__1::vector<tflite::gpu::TensorUsageRecord<unsigned long>,\r\n          std::__1::allocator<tflite::gpu::TensorUsageRecord<unsigned long> > >::__emplace_back_slow_path<int, unsigned int&,\r\n          unsigned int&>(int&&, unsigned int&, unsigned int&) in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::__1::vector<id<MTLBuffer> __strong, std::__1::allocator<id<MTLBuffer> > >::__vallocate(unsigned long) in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          void std::__1::vector<InputBuffer, std::__1::allocator<InputBuffer>\r\n          >::__emplace_back_slow_path<InputBuffer>(InputBuffer&&) in\r\n          TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          void std::__1::vector<UniformBuffer, std::__1::allocator<UniformBuffer>\r\n          >::__emplace_back_slow_path<UniformBuffer>(UniformBuffer&&) in\r\n          TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          ...\r\n      \"std::__1::locale::use_facet(std::__1::locale::id&) const\", referenced from:\r\n          absl::operator<<(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, absl::uint128) in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::__next_prime(unsigned long)\", referenced from:\r\n          std::__1::__hash_table<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> >, absl::time_internal::cctz::time_zone::Impl const*>,\r\n          std::__1::__unordered_map_hasher<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>\r\n          >, std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >,\r\n          absl::time_internal::cctz::time_zone::Impl const*>, std::__1::hash<std::__1::basic_string<char,\r\n          std::__1::char_traits<char>, std::__1::allocator<char> > >, true>,\r\n          std::__1::__unordered_map_equal<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>\r\n          >, std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >,\r\n          absl::time_internal::cctz::time_zone::Impl const*>, std::__1::equal_to<std::__1::basic_string<char,\r\n          std::__1::char_traits<char>, std::__1::allocator<char> > >, true>,\r\n          std::__1::allocator<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> >, absl::time_internal::cctz::time_zone::Impl const*> > >::rehash(unsigned long) in\r\n          TensorFlowLiteC(time_zone_impl_70088ae3adaade7bd54e47cd293b4fcc.o)\r\n      \"std::__1::chrono::system_clock::now()\", referenced from:\r\n          absl::Now() in TensorFlowLiteC(clock_59d4d44bffb6d368cb7c779169be0652.o)\r\n          absl::GetCurrentTimeNanos() in TensorFlowLiteC(clock_59d4d44bffb6d368cb7c779169be0652.o)\r\n      \"std::__1::basic_streambuf<char, std::__1::char_traits<char> >::showmanyc()\", referenced from:\r\n          vtable for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          vtable for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::ios_base::__set_badbit_and_consider_rethrow()\", referenced from:\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"___cxa_guard_acquire\", referenced from:\r\n          absl::container_internal::Sample() in TensorFlowLiteC(memory_management_b2355fe02092d3e625e8a39708405196.o)\r\n          absl::container_internal::HashtablezSampler::Global() in\r\n          TensorFlowLiteC(hashtablez_sampler_f7a58fda31cdb479dd28749081bd30a1.o)\r\n          absl::time_internal::cctz::time_zone::Impl::UTCImpl() in\r\n          TensorFlowLiteC(time_zone_impl_70088ae3adaade7bd54e47cd293b4fcc.o)\r\n          absl::time_internal::cctz::TimeZoneLibC::MakeTime(absl::time_internal::cctz::detail::civil_time<absl::time_internal::\r\n          cctz::detail::second_tag> const&) const in TensorFlowLiteC(time_zone_libc_1c4053d35db44a839fcd158b0a3f97d5.o)\r\n      \"std::__1::basic_ios<char, std::__1::char_traits<char> >::~basic_ios()\", referenced from:\r\n          absl::operator<<(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, absl::uint128) in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >::~basic_ostringstream()\r\n          in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >::~basic_ostringstream()\r\n          in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          virtual thunk to std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char>\r\n          >::~basic_ostringstream() in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          virtual thunk to std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char>\r\n          >::~basic_ostringstream() in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          absl::time_internal::cctz::TimeZoneInfo::Description() const in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >::~basic_ostringstream()\r\n          in TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          ...\r\n      \"___cxa_guard_abort\", referenced from:\r\n          absl::container_internal::Sample() in TensorFlowLiteC(memory_management_b2355fe02092d3e625e8a39708405196.o)\r\n          absl::container_internal::HashtablezSampler::Global() in\r\n          TensorFlowLiteC(hashtablez_sampler_f7a58fda31cdb479dd28749081bd30a1.o)\r\n          absl::time_internal::cctz::time_zone::Impl::UTCImpl() in\r\n          TensorFlowLiteC(time_zone_impl_70088ae3adaade7bd54e47cd293b4fcc.o)\r\n      \"std::__1::__basic_string_common<true>::__throw_length_error() const\", referenced from:\r\n          std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>\r\n          >::basic_string<std::nullptr_t>(char const*) in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>\r\n          >::basic_string<std::nullptr_t>(char const*) in TensorFlowLiteC(common_cd6e0002c2cc438c490b1d8b3375cdd6.o)\r\n          std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> >::str() const in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          absl::time_internal::cctz::local_time_zone() in TensorFlowLiteC(time_zone_lookup_1013b5b9ce7824eb44fb9b51c7e98f70.o)\r\n          absl::time_internal::cctz::FixedOffsetToName(std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> > const&)\r\n          in TensorFlowLiteC(time_zone_fixed_12d6eb8ff52945a37ee8f97c1ea02823.o)\r\n          absl::time_internal::cctz::(anonymous namespace)::AndroidZoneInfoSource::Open(std::__1::basic_string<char,\r\n          std::__1::char_traits<char>, std::__1::allocator<char> > const&) in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> >::str() const in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          ...\r\n      \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::insert(unsigned long, unsigned\r\n      long, char)\", referenced from:\r\n          absl::operator<<(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, absl::uint128) in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n      \"std::exception::what() const\", referenced from:\r\n          vtable for std::__1::bad_function_call in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          vtable for std::__1::bad_function_call in TensorFlowLiteC(hashtablez_sampler_f7a58fda31cdb479dd28749081bd30a1.o)\r\n          vtable for std::__1::bad_function_call in TensorFlowLiteC(zone_info_source_d0d641f6600cbc71bed9faf61afe560f.o)\r\n      \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::assign(char const*)\", referenced\r\n      from:\r\n          -[TFLBufferConvert initWithDevice:isFloat16:convertToPBHWC4:] in\r\n          TensorFlowLiteC(buffer_convert_1d2d90cc5f78b8992cc785cc8c58fd0c.o)\r\n          absl::FormatDuration(absl::Duration) in TensorFlowLiteC(duration_23d5e2ad34b3d4e9d4db6be0369385c2.o)\r\n      \"vtable for __cxxabiv1::__si_class_type_info\", referenced from:\r\n          typeinfo for std::__1::bad_function_call in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          typeinfo for std::__1::bad_function_call in TensorFlowLiteC(hashtablez_sampler_f7a58fda31cdb479dd28749081bd30a1.o)\r\n          typeinfo for std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          typeinfo for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          typeinfo for absl::time_internal::cctz::TimeZoneInfo in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          typeinfo for std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          typeinfo for std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          ...\r\n      NOTE: a missing vtable usually means the first non-inline virtual member function has no definition.\r\n      \"std::__1::__vector_base_common<true>::__throw_out_of_range() const\", referenced from:\r\n          -[TFLComputeTask assignBuffers:outputIds:usageRecordIds:sharedBufferIds:sharedBuffers:] in\r\n          TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n      \"std::exception::~exception()\", referenced from:\r\n          std::__1::bad_function_call::~bad_function_call() in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          std::__1::bad_function_call::~bad_function_call() in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          std::__1::bad_function_call::~bad_function_call() in\r\n          TensorFlowLiteC(hashtablez_sampler_f7a58fda31cdb479dd28749081bd30a1.o)\r\n          std::__1::bad_function_call::~bad_function_call() in\r\n          TensorFlowLiteC(hashtablez_sampler_f7a58fda31cdb479dd28749081bd30a1.o)\r\n          std::__1::bad_function_call::~bad_function_call() in\r\n          TensorFlowLiteC(zone_info_source_d0d641f6600cbc71bed9faf61afe560f.o)\r\n          std::__1::bad_function_call::~bad_function_call() in\r\n          TensorFlowLiteC(zone_info_source_d0d641f6600cbc71bed9faf61afe560f.o)\r\n      \"std::__1::to_string(unsigned int)\", referenced from:\r\n          -[TFLComputeTask setInputDimensionsWithDevice:dimensions:] in\r\n          TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n      \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::append(char const*)\", referenced\r\n      from:\r\n          -[TFLComputeTask setInputDimensionsWithDevice:dimensions:] in\r\n          TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          absl::FormatDuration(absl::Duration) in TensorFlowLiteC(duration_23d5e2ad34b3d4e9d4db6be0369385c2.o)\r\n          absl::(anonymous namespace)::AppendNumberUnit(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> >*, double, absl::(anonymous namespace)::DisplayUnit) in\r\n          TensorFlowLiteC(duration_23d5e2ad34b3d4e9d4db6be0369385c2.o)\r\n          absl::time_internal::cctz::(anonymous namespace)::FileZoneInfoSource::Open(std::__1::basic_string<char,\r\n          std::__1::char_traits<char>, std::__1::allocator<char> > const&) in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::to_string(unsigned long)\", referenced from:\r\n          -[TFLComputeTask setInputDimensionsWithDevice:dimensions:] in\r\n          TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n      \"vtable for std::length_error\", referenced from:\r\n          std::length_error::length_error(char const*) in TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::length_error::length_error(char const*) in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          std::length_error::length_error(char const*) in TensorFlowLiteC(memory_management_b2355fe02092d3e625e8a39708405196.o)\r\n          std::length_error::length_error(char const*) in TensorFlowLiteC(internal_177af47ac7f30482d4096214acf6ba06.o)\r\n          std::length_error::length_error(char const*) in\r\n          TensorFlowLiteC(min_cost_flow_assignment_3433aed886ccf32d0ddc192b47dca509.o)\r\n          std::length_error::length_error(char const*) in\r\n          TensorFlowLiteC(greedy_by_size_assignment_9fa33a720f1e8acb4a8f6c0d76110984.o)\r\n          std::length_error::length_error(char const*) in\r\n          TensorFlowLiteC(greedy_by_breadth_assignment_9652ad845cf0f94df849627b34cc7bc4.o)\r\n          ...\r\n      NOTE: a missing vtable usually means the first non-inline virtual member function has no definition.\r\n      \"___cxa_allocate_exception\", referenced from:\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          std::__1::__throw_bad_function_call() in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(memory_management_b2355fe02092d3e625e8a39708405196.o)\r\n          std::__1::__throw_bad_function_call() in TensorFlowLiteC(hashtablez_sampler_f7a58fda31cdb479dd28749081bd30a1.o)\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(internal_177af47ac7f30482d4096214acf6ba06.o)\r\n          std::__1::__throw_length_error(char const*) in\r\n          TensorFlowLiteC(min_cost_flow_assignment_3433aed886ccf32d0ddc192b47dca509.o)\r\n          ...\r\n      \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::compare(unsigned long, unsigned\r\n      long, char const*, unsigned long) const\", referenced from:\r\n          absl::FormatDuration(absl::Duration) in TensorFlowLiteC(duration_23d5e2ad34b3d4e9d4db6be0369385c2.o)\r\n          absl::time_internal::cctz::FixedOffsetFromName(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> > const&, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >*) in\r\n          TensorFlowLiteC(time_zone_fixed_12d6eb8ff52945a37ee8f97c1ea02823.o)\r\n          absl::time_internal::cctz::TimeZoneInfo::CheckTransition(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> > const&, absl::time_internal::cctz::TransitionType const&, int, bool,\r\n          std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          absl::time_internal::cctz::TimeZoneLibC::TimeZoneLibC(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> > const&) in TensorFlowLiteC(time_zone_libc_1c4053d35db44a839fcd158b0a3f97d5.o)\r\n          absl::time_internal::cctz::TimeZoneLibC::TimeZoneLibC(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> > const&) in TensorFlowLiteC(time_zone_libc_1c4053d35db44a839fcd158b0a3f97d5.o)\r\n      \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::push_back(char)\", referenced\r\n      from:\r\n          absl::(anonymous namespace)::AppendNumberUnit(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> >*, double, absl::(anonymous namespace)::DisplayUnit) in\r\n          TensorFlowLiteC(duration_23d5e2ad34b3d4e9d4db6be0369385c2.o)\r\n          std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> >::overflow(int) in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          absl::time_internal::cctz::TimeZoneInfo::Load(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> > const&, absl::time_internal::cctz::ZoneInfoSource*) in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          std::__1::basic_stringbuf<char, std::__1::char_traits<char>, std::__1::allocator<char> >::overflow(int) in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          absl::time_internal::cctz::(anonymous namespace)::FileZoneInfoSource::Open(std::__1::basic_string<char,\r\n          std::__1::char_traits<char>, std::__1::allocator<char> > const&) in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::terminate()\", referenced from:\r\n          ___clang_call_terminate in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n      \"std::__1::chrono::system_clock::from_time_t(long)\", referenced from:\r\n          absl::Now() in TensorFlowLiteC(clock_59d4d44bffb6d368cb7c779169be0652.o)\r\n          absl::GetCurrentTimeNanos() in TensorFlowLiteC(clock_59d4d44bffb6d368cb7c779169be0652.o)\r\n          absl::Time::In(absl::TimeZone) const in TensorFlowLiteC(time_663724b99f08c35ff81d5280df9c0131.o)\r\n          absl::FromChrono(std::__1::chrono::time_point<std::__1::chrono::system_clock, std::__1::chrono::duration<long long,\r\n          std::__1::ratio<1l, 1000000l> > > const&) in TensorFlowLiteC(time_663724b99f08c35ff81d5280df9c0131.o)\r\n          absl::ToChronoTime(absl::Time) in TensorFlowLiteC(time_663724b99f08c35ff81d5280df9c0131.o)\r\n          absl::TimeZone::At(absl::Time) const in TensorFlowLiteC(time_663724b99f08c35ff81d5280df9c0131.o)\r\n          absl::(anonymous namespace)::MakeTimeWithOverflow(std::__1::chrono::time_point<std::__1::chrono::system_clock,\r\n          std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> > > const&,\r\n          absl::time_internal::cctz::detail::civil_time<absl::time_internal::cctz::detail::second_tag> const&,\r\n          absl::time_internal::cctz::time_zone const&, bool*) in TensorFlowLiteC(time_663724b99f08c35ff81d5280df9c0131.o)\r\n          ...\r\n      \"typeinfo for std::__1::basic_ostream<char, std::__1::char_traits<char> >\", referenced from:\r\n          construction vtable for std::__1::basic_ostream<char, std::__1::char_traits<char>\r\n          >-in-std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          typeinfo for std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          construction vtable for std::__1::basic_ostream<char, std::__1::char_traits<char>\r\n          >-in-std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n          typeinfo for std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> > in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>\r\n      >::basic_string(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\",\r\n      referenced from:\r\n          -[TFLInferenceContext setInputDimensions:outputDimensions:taskDescriptors:] in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          -[TFLComputeTask compileWithDevice:taskDescriptor:runtimeOptions:] in\r\n          TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          -[TFLComputeTask setInputDimensionsWithDevice:dimensions:] in\r\n          TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          -[TFLComputeTask assignBuffers:outputIds:usageRecordIds:sharedBufferIds:sharedBuffers:] in\r\n          TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          tflite::gpu::Status tflite::gpu::AssignObjectsToTensors<unsigned\r\n          long>(std::__1::vector<tflite::gpu::TensorUsageRecord<unsigned long>,\r\n          std::__1::allocator<tflite::gpu::TensorUsageRecord<unsigned long> > > const&, tflite::gpu::MemoryStrategy,\r\n          tflite::gpu::ObjectsAssignment<unsigned long>*, std::__1::vector<std::__1::vector<unsigned long,\r\n          std::__1::allocator<unsigned long> >, std::__1::allocator<std::__1::vector<unsigned long,\r\n          std::__1::allocator<unsigned long> > > > const*) in\r\n          TensorFlowLiteC(memory_management_b2355fe02092d3e625e8a39708405196.o)\r\n          tflite::gpu::Status tflite::gpu::GreedyInOrderAssignment<unsigned\r\n          long>(std::__1::vector<tflite::gpu::TensorUsageRecord<unsigned long>,\r\n          std::__1::allocator<tflite::gpu::TensorUsageRecord<unsigned long> > > const&, tflite::gpu::ObjectsAssignment<unsigned\r\n          long>*, std::__1::vector<std::__1::vector<unsigned long, std::__1::allocator<unsigned long> >,\r\n          std::__1::allocator<std::__1::vector<unsigned long, std::__1::allocator<unsigned long> > > > const*) in\r\n          TensorFlowLiteC(memory_management_b2355fe02092d3e625e8a39708405196.o)\r\n          tflite::gpu::Status tflite::gpu::AssignObjectsToTensors<tflite::gpu::StrongShape<(tflite::gpu::Layout)10>\r\n          >(std::__1::vector<tflite::gpu::TensorUsageRecord<tflite::gpu::StrongShape<(tflite::gpu::Layout)10> >,\r\n          std::__1::allocator<tflite::gpu::TensorUsageRecord<tflite::gpu::StrongShape<(tflite::gpu::Layout)10> > > > const&,\r\n          tflite::gpu::MemoryStrategy, tflite::gpu::ObjectsAssignment<tflite::gpu::StrongShape<(tflite::gpu::Layout)10> >*,\r\n          std::__1::vector<std::__1::vector<unsigned long, std::__1::allocator<unsigned long> >,\r\n          std::__1::allocator<std::__1::vector<unsigned long, std::__1::allocator<unsigned long> > > > const*) in\r\n          TensorFlowLiteC(memory_management_b2355fe02092d3e625e8a39708405196.o)\r\n          ...\r\n      \"std::__1::basic_streambuf<char, std::__1::char_traits<char> >::basic_streambuf()\", referenced from:\r\n          absl::operator<<(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, absl::uint128) in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          absl::time_internal::cctz::TimeZoneInfo::Description() const in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"___cxa_free_exception\", referenced from:\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(memory_management_b2355fe02092d3e625e8a39708405196.o)\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(internal_177af47ac7f30482d4096214acf6ba06.o)\r\n          std::__1::__throw_length_error(char const*) in\r\n          TensorFlowLiteC(min_cost_flow_assignment_3433aed886ccf32d0ddc192b47dca509.o)\r\n          std::__1::__throw_length_error(char const*) in\r\n          TensorFlowLiteC(greedy_by_size_assignment_9fa33a720f1e8acb4a8f6c0d76110984.o)\r\n          std::__1::__throw_length_error(char const*) in\r\n          TensorFlowLiteC(greedy_by_breadth_assignment_9652ad845cf0f94df849627b34cc7bc4.o)\r\n          ...\r\n      \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::append(char const*, unsigned\r\n      long)\", referenced from:\r\n          -[TFLComputeTask setInputDimensionsWithDevice:dimensions:] in\r\n          TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          absl::FormatDuration(absl::Duration) in TensorFlowLiteC(duration_23d5e2ad34b3d4e9d4db6be0369385c2.o)\r\n          absl::(anonymous namespace)::AppendNumberUnit(std::__1::basic_string<char, std::__1::char_traits<char>,\r\n          std::__1::allocator<char> >*, double, absl::(anonymous namespace)::DisplayUnit) in\r\n          TensorFlowLiteC(duration_23d5e2ad34b3d4e9d4db6be0369385c2.o)\r\n          absl::time_internal::cctz::(anonymous namespace)::FileZoneInfoSource::Open(std::__1::basic_string<char,\r\n          std::__1::char_traits<char>, std::__1::allocator<char> > const&) in\r\n          TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::__1::__shared_weak_count::__release_weak()\", referenced from:\r\n          -[TFLInferenceContext compileModelWithDevice:taskDescriptors:outputBufferIDs:runtimeOptions:] in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n      \"___cxa_throw\", referenced from:\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          std::__1::__throw_bad_function_call() in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(memory_management_b2355fe02092d3e625e8a39708405196.o)\r\n          std::__1::__throw_bad_function_call() in TensorFlowLiteC(hashtablez_sampler_f7a58fda31cdb479dd28749081bd30a1.o)\r\n          std::__1::__throw_length_error(char const*) in TensorFlowLiteC(internal_177af47ac7f30482d4096214acf6ba06.o)\r\n          std::__1::__throw_length_error(char const*) in\r\n          TensorFlowLiteC(min_cost_flow_assignment_3433aed886ccf32d0ddc192b47dca509.o)\r\n          ...\r\n      \"std::__1::locale::~locale()\", referenced from:\r\n          absl::operator<<(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, absl::uint128) in\r\n          TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(int128_df0a637a93531d213521c15fc19bd7c1.o)\r\n          std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::__put_character_sequence<char,\r\n          std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*, unsigned\r\n          long) in TensorFlowLiteC(time_zone_info_ac39a6fa55f13bccf521f748f28e02bd.o)\r\n      \"std::logic_error::logic_error(char const*)\", referenced from:\r\n          std::length_error::length_error(char const*) in TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::length_error::length_error(char const*) in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          std::length_error::length_error(char const*) in TensorFlowLiteC(memory_management_b2355fe02092d3e625e8a39708405196.o)\r\n          std::length_error::length_error(char const*) in TensorFlowLiteC(internal_177af47ac7f30482d4096214acf6ba06.o)\r\n          std::length_error::length_error(char const*) in\r\n          TensorFlowLiteC(min_cost_flow_assignment_3433aed886ccf32d0ddc192b47dca509.o)\r\n          std::length_error::length_error(char const*) in\r\n          TensorFlowLiteC(greedy_by_size_assignment_9fa33a720f1e8acb4a8f6c0d76110984.o)\r\n          std::length_error::length_error(char const*) in\r\n          TensorFlowLiteC(greedy_by_breadth_assignment_9652ad845cf0f94df849627b34cc7bc4.o)\r\n          ...\r\n      \"typeinfo for std::exception\", referenced from:\r\n          typeinfo for std::__1::bad_function_call in TensorFlowLiteC(compute_task_7d9740b6bb033cffaa24e4dcd6c98207.o)\r\n          typeinfo for std::__1::bad_function_call in TensorFlowLiteC(hashtablez_sampler_f7a58fda31cdb479dd28749081bd30a1.o)\r\n          typeinfo for std::__1::bad_function_call in TensorFlowLiteC(zone_info_source_d0d641f6600cbc71bed9faf61afe560f.o)\r\n      \"___cxa_guard_release\", referenced from:\r\n          absl::container_internal::Sample() in TensorFlowLiteC(memory_management_b2355fe02092d3e625e8a39708405196.o)\r\n          absl::container_internal::HashtablezSampler::Global() in\r\n          TensorFlowLiteC(hashtablez_sampler_f7a58fda31cdb479dd28749081bd30a1.o)\r\n          absl::time_internal::cctz::time_zone::Impl::UTCImpl() in\r\n          TensorFlowLiteC(time_zone_impl_70088ae3adaade7bd54e47cd293b4fcc.o)\r\n          absl::time_internal::cctz::TimeZoneLibC::MakeTime(absl::time_internal::cctz::detail::civil_time<absl::time_internal::\r\n          cctz::detail::second_tag> const&) const in TensorFlowLiteC(time_zone_libc_1c4053d35db44a839fcd158b0a3f97d5.o)\r\n      \"___cxa_pure_virtual\", referenced from:\r\n          vtable for absl::time_internal::cctz::TimeZoneIf in TensorFlowLiteC(time_zone_if_b1201e42997e5298e8b9b342eb441cd9.o)\r\n          vtable for absl::time_internal::cctz::ZoneInfoSource in\r\n          TensorFlowLiteC(zone_info_source_d0d641f6600cbc71bed9faf61afe560f.o)\r\n      \"operator delete(void*)\", referenced from:\r\n          -[TFLBufferConvert initWithDevice:isFloat16:convertToPBHWC4:] in\r\n          TensorFlowLiteC(buffer_convert_1d2d90cc5f78b8992cc785cc8c58fd0c.o)\r\n          -[TFLBufferConvert convertWithEncoder:shape:sourceBuffer:convertedBuffer:] in\r\n          TensorFlowLiteC(buffer_convert_1d2d90cc5f78b8992cc785cc8c58fd0c.o)\r\n          -[TFLInferenceContext compileModelWithDevice:taskDescriptors:outputBufferIDs:runtimeOptions:] in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          -[TFLInferenceContext setInputDimensions:outputDimensions:taskDescriptors:] in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          -[TFLInferenceContext .cxx_destruct] in TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::__1::__tree<std::__1::__value_type<unsigned int, unsigned long>, std::__1::__map_value_compare<unsigned int,\r\n          std::__1::__value_type<unsigned int, unsigned long>, std::__1::less<unsigned int>, true>,\r\n          std::__1::allocator<std::__1::__value_type<unsigned int, unsigned long> >\r\n          >::destroy(std::__1::__tree_node<std::__1::__value_type<unsigned int, unsigned long>, void*>*) in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          std::__1::__tree<unsigned int, std::__1::less<unsigned int>, std::__1::allocator<unsigned int>\r\n          >::destroy(std::__1::__tree_node<unsigned int, void*>*) in\r\n          TensorFlowLiteC(inference_context_0de3088040d9569800f4171c1bc2996f.o)\r\n          ...\r\n    ld: symbol(s) not found for architecture x86_64\r\n    clang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n    note: Using new build systemnote: Planning buildnote: Constructing build description\r\n\r\nCould not build the application for the simulator.\r\nError launching application on iPhone 11 Pro Max.\r\n</pre>\r\n</details>", "comments": ["adding,\r\n`s.library = 'c++'`\r\nto `ios/tflite_flutter_plugin.podspec` file in my project ([link](https://github.com/am15h/tflite_flutter_plugin/blob/ios_support/ios/tflite_flutter_plugin.podspec#L24)) resolves this error.", "@yyoon I am unable to directly use `s.dependency 'TensorFlowLiteC', '2.1.0'` in iOS, dart:ffi dynamic lookup fails with this, although there are no issues when I use \r\n[`s.dependency 'TensorFlowLiteObjC', '2.1.0'`](https://github.com/am15h/tflite_flutter_plugin/blob/master/ios/tflite_flutter_plugin.podspec#L19). The same problem occurs when trying to use `TensorFlowLiteC.framework`, I am aware that TensorFlowLiteC is meant to be used internally for ObjC and swift, but is there a way to get TensorFlowLiteC working on iOS. I think that adding the required attributes to [TensorFlowLiteC.podspec](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ios/TensorFlowLiteC.podspec) might be the way.\r\n", "@am15h Hi Amish. Could you clarify which `.podspec` file you were referring to?\r\nThe `TensorFlowLiteC.podspec` file should already have the `s.library = 'c++'` line:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ios/TensorFlowLiteC.podspec#L21\r\n\r\nIf you're just trying to use the pure C API, there is no reason for you to use the Obj-C CocoaPods. Anyways, I'll take a look and try reproducing the issue as you instructed above.", "@@am15h please let us know if the above comment helps resolve the issue.", "Thanks @yyoon, actually I was referring to `ios/tflite_flutter_plugin.podspec` file in my project ([link](https://github.com/am15h/tflite_flutter_plugin/blob/ios_support/ios/tflite_flutter_plugin.podspec#L24)). \r\n\r\nJust to be clear Undefined symbols for architecture error was resolved and now I am facing **flutter dynamic lookup failed** using the TensorFlowLiteC library.\r\n\r\nI don't have much knowledge about iOS development and CocoaPods, initially I thought that I might not be able to use `s.dependency 'TensorFlowLiteC', '2.1.0'` in place of [`s.dependency 'TensorFlowLiteObjC', '2.1.0'`](https://github.com/am15h/tflite_flutter_plugin/blob/master/ios/tflite_flutter_plugin.podspec#L19) because unlike [`TensorFlowLiteObjC.podspec`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/objc/TensorFlowLiteObjC.podspec),  [`TensorFlowLiteC.podspec`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ios/TensorFlowLiteC.podspec) does not specify module_map, header, source etc, (probably we don't need to specify these while using [`s.vendored_frameworks = 'Frameworks/TensorFlowLiteC.framework'`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ios/TensorFlowLiteC.podspec#L22) ??).  \r\nThe second thing is that I face the same error when I use TensorFlowLiteC.framework folder (you can reproduce this using the [steps above](https://github.com/tensorflow/tensorflow/issues/36766#issue-565585013).) I tried dynamic linking in flutter using [the ObjectBox.framework file](https://github.com/objectbox/objectbox-swift/releases/download/v1.2.0/ObjectBox-framework-1.2.0.zip) and It worked fine.\r\n", "And what is your Xcode version? If you're using Xcode 10 or below, could you try using Xcode 11?\r\nIn the meantime, I'll try to reproduce your case.", "> And what is your Xcode version? If you're using Xcode 10 or below, could you try using Xcode 11?\r\n\r\nAlready using latest version Xcode.\r\n\r\n", "Hmm, I'm not seeing the build error you pasted above, and seeing a different error when the app is launched. See below.\r\n\r\n```\r\nLaunching lib/main.dart on YoungSeok Yoon\u2019s iPhone in debug mode...\r\nWarning: Missing build name (CFBundleShortVersionString).\r\nWarning: Missing build number (CFBundleVersion).\r\nAction Required: You must set a build name and number in the pubspec.yaml file version field before submitting to the App Store.\r\nAutomatically signing iOS for device deployment using specified development team in Xcode project: EQHXZ8M8AV\r\nRunning Xcode build...\r\n \u251c\u2500Assembling Flutter resources...                           3.3s\r\n \u2514\u2500Compiling, linking and signing...                         6.4s\r\nXcode build done.                                           13.1s\r\nInstalling and launching...\r\nflutter: \u2550\u2550\u2561 EXCEPTION CAUGHT BY WIDGETS LIBRARY \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\r\n\r\nflutter: The following ArgumentError was thrown attaching to the render tree:\r\n\r\nflutter: Invalid argument(s): Failed to lookup symbol (dlsym(RTLD_DEFAULT, TfLiteVersion): symbol not found)\r\n\r\nflutter:\r\n\r\nflutter: When the exception was thrown, this was the stack:\r\n\r\nflutter: #0      DynamicLibrary.lookup (dart:ffi-patch/ffi_dynamic_library_patch.dart:33:29)\r\n\r\nflutter: #1      TfLiteVersion (package:tflite_flutter_plugin/src/bindings/bindings.dart:13:6)\r\n\r\nflutter: #2      TfLiteVersion (package:tflite_flutter_plugin/src/bindings/bindings.dart:12:26)\r\n\r\nflutter: #3      version (package:tflite_flutter_plugin/tflite.dart:18:37)\r\n\r\nflutter: #4      _MyAppState.initState (package:tflite_flutter_plugin_example/main.dart:22:39)\r\n\r\nflutter: #5      StatefulElement._firstBuild (package:flutter/src/widgets/framework.dart:4355:58)\r\n\r\nflutter: #6      ComponentElement.mount (package:flutter/src/widgets/framework.dart:4201:5)\r\n\r\nflutter: #7      Element.inflateWidget (package:flutter/src/widgets/framework.dart:3194:14)\r\n\r\nflutter: #8      Element.updateChild (package:flutter/src/widgets/framework.dart:2988:12)\r\n\r\nflutter: #9      RenderObjectToWidgetElement._rebuild (package:flutter/src/widgets/binding.dart:1028:16)\r\n\r\nflutter: #10     RenderObjectToWidgetElement.mount (package:flutter/src/widgets/binding.dart:999:5)\r\n\r\nflutter: #11     RenderObjectToWidgetAdapter.attachToRenderTree.<anonymous closure> (package:flutter/src/widgets/binding.dart:942:17)\r\n\r\nflutter: #12     BuildOwner.buildScope (package:flutter/src/widgets/framework.dart:2412:19)\r\n\r\nflutter: #13     RenderObjectToWidgetAdapter.attachToRenderTree (package:flutter/src/widgets/binding.dart:941:13)\r\n\r\nflutter: #14     WidgetsBinding.attachRootWidget (package:flutter/src/widgets/binding.dart:819:7)\r\n\r\nflutter: #15     WidgetsBinding.scheduleAttachRootWidget.<anonymous closure> (package:flutter/src/widgets/binding.dart:804:7)\r\n\r\nflutter: #24     _Timer._runTimers (dart:isolate-patch/timer_impl.dart:384:19)\r\n\r\nflutter: #25     _Timer._handleMessage (dart:isolate-patch/timer_impl.dart:418:5)\r\n\r\nflutter: #26     _RawReceivePortImpl._handleMessage (dart:isolate-patch/isolate_patch.dart:174:12)\r\n\r\nflutter: (elided 8 frames from package dart:async and package dart:async-patch)\r\n\r\nflutter: \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\r\nInstalling and launching...                                        15.8s\r\nSyncing files to device YoungSeok Yoon\u2019s iPhone...               4,561ms (!)\r\n```\r\n\r\nHere's my `flutter doctor` output from my mac, just in case.\r\n\r\n```\r\n 19:50:05 \ue0b0 ~/Programming/tflite_flutter_plugin/example \ue0b0 \ue0a0 ios_support \u25cf \ue0b0 flutter doctor\r\nDoctor summary (to see all details, run flutter doctor -v):\r\n[\u2713] Flutter (Channel stable, v1.12.13+hotfix.8, on Mac OS X 10.15.2 19C57, locale en-US)\r\n[!] Android toolchain - develop for Android devices (Android SDK version 29.0.0)\r\n    ! Some Android licenses not accepted.  To resolve this, run: flutter doctor --android-licenses\r\n[\u2713] Xcode - develop for iOS and macOS (Xcode 11.3.1)\r\n[!] Android Studio (version 3.2)\r\n    \u2717 Flutter plugin not installed; this adds Flutter specific functionality.\r\n    \u2717 Dart plugin not installed; this adds Dart specific functionality.\r\n[\u2713] VS Code (version 1.42.0)\r\n[\u2713] Connected device (1 available)\r\n\r\n! Doctor found issues in 2 categories.\r\n```\r\n\r\nSeeing the exact same error when I try to run it on iOS Simulator.\r\nIs this error expected? Or should it be displaying some Flutter UI instead?", "Yes @yyoon the log you have shared is the expected error. I apologise for not updating the latest error log.\r\n> \r\n> Just to be clear Undefined symbols for architecture error was resolved and now I am facing **flutter dynamic lookup failed** using the TensorFlowLiteC library.\r\n> \r\n\r\nas I said in a previous comment I had shared log for the error _Unresolved symbol for architecture_ which I was able to resolve, now we are facing **Failed to lookup symbol** (as you have shared)", "Ah, so we're on the same page now. Thanks for clarifying.\r\nLooks like I'll need to understand dart:ffi and how it works in flutter a bit to investigate this further.\r\nLet me get back to you when I find any clues.", "Thanks @yyoon, These are some related issues https://github.com/dart-lang/ffi/issues/28, https://github.com/flutter/flutter/issues/33227,  https://github.com/flutter/flutter/issues/46887, after going through the comments I suspect that this might be a dart:ffi and flutter related problem, still I can't say that with surety as,\r\n**linking works fine with ObjectBox.framework and not TensorFlowLiteC.framework**\r\nI have added code here https://github.com/am15h/tflite_flutter_plugin/tree/other_framework, in case you want to test.\r\n\r\nSteps to run with ObjectBox.framework: \r\n`git clone https://github.com/am15h/tflite_flutter_plugin/`\r\n`cd tflite_flutter_plugin && git checkout other_framework`\r\n`cd example && flutter run`\r\n\r\n<details>\r\n<summary>Expected Output when running with ObjectBox.framework</summary>\r\n<pre>\r\nMDGs-iMac:example amish$ flutter run\r\n \r\nLaunching lib/main.dart on iPhone 11 Pro Max in debug mode...\r\nWarning: Missing build name (CFBundleShortVersionString).\r\nWarning: Missing build number (CFBundleVersion).\r\nAction Required: You must set a build name and number in the pubspec.yaml file version field before\r\nsubmitting to the App Store.\r\n \r\nRunning pod install...                                              2.8s\r\nRunning Xcode build...                                                  \r\n                                                   \r\n \u251c\u2500Assembling Flutter resources...                          11.7s\r\n \u2514\u2500Compiling, linking and signing...                        24.5s\r\nXcode build done.                                           58.7s\r\nSyncing files to device iPhone 11 Pro Max...                            \r\n<b>flutter: Printing Object-Box version 0.8.1</b>                  \r\nSyncing files to device iPhone 11 Pro Max...                                                         6,485ms (!)                                       \r\n\r\n\ud83d\udd25  To hot reload changes while running, press \"r\". To hot restart (and rebuild state), press \"R\".\r\nAn Observatory debugger and profiler on iPhone 11 Pro Max is available at:\r\nhttp://127.0.0.1:56759/BVhK6kknhJY=/\r\nFor a more detailed help message, press \"h\". To detach, press \"d\"; to quit, press \"q\".\r\n</pre>\r\n</details>", "@am15h Could you try this? Modify your `ios/tflite_flutter_plugin.podspec` file to include `-all_load` flag in the `OTHER_LDFLAGS` section.\r\n\r\n```rb\r\n  s.xcconfig = { 'OTHER_LDFLAGS' => '-framework TensorFlowLiteC -all_load' }\r\n```\r\n\r\nAnd then, run `pod install` again inside `example/ios` and do `flutter run`\r\nThis seems to have fixed the problem for me.", "Thanks a lot, @yyoon things are now working fine with TensorFlowLiteC.framework. ", "Awesome! Thanks for verifying.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36766\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36766\">No</a>\n", "So I have \"-all_load\" in podspec but still get this error."]}, {"number": 36765, "title": "[ROCm] Workaround for a known CPU/GPU kernel interface args passing bug", "body": "I am working on submitting an update to Eigen to enable fp16 packet optimization for the ROCm platform. When testing those updates with TF, they result in the manifestation of a known bug in the CPU/GPU kernel interface args passing bug.\r\n\r\nBasically the CPU code (`.cc` files) is compiled with an old version of gcc (`v5.4`, i.e. the that comes default with `Ubuntu 16.04`) and the GPU code (`.cu.cc` files) is compiled with HCC (which is clang10 based). This results in the GPU kernel arguments sometimes getting corrupted malformed. We do not have a fix for the issue, short of using a newer gcc version which does not have this \"bug\".  We have discovered that packing all the arguments within a struct seems to workaround the bug, and that is what this PR does.\r\n\r\n----------------\r\n\r\n/cc @whchung @chsigg @nvining-work \r\n", "comments": ["gentle ping", "@rmlarsen "]}, {"number": 36764, "title": "tf.linalg.svd not supporting tf.half as dtype on Windows CPU with python3.5", "body": "Hello!\r\n\r\nI am currently writing a test file for an optimizer toward tensorflow_addons. I use tf.linalg.svd function in the test file and I test with the case of using tf.half as dtype. \r\n\r\nThe test can pass the given CI test with Mac CPU python 3.5 and Ubuntu CPU python 3. But for the given CI test on Windows CPU with python3.5, it throws following error message: \r\n\r\n\"\r\nERROR:tensorflow:No OpKernel was registered to support Op 'Svd' used by node Svd (defined at \\\\?\\C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\Bazel.runfiles_pt5sug_g\\runfiles\\__main__\\tensorflow_addons\\optimizers\\conditional_gradient_test.py:37) with these attrs: [full_matrices=false, compute_uv=true, T=DT_HALF]\r\n\r\nRegistered devices: [CPU]\r\n\r\nRegistered kernels:\r\n\r\n  device='CPU'; T in [DT_FLOAT]\r\n\r\n  device='CPU'; T in [DT_DOUBLE]\r\n\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n\r\n  device='GPU'; T in [DT_FLOAT]\r\n\r\n  device='GPU'; T in [DT_DOUBLE]\r\n\r\n\r\n\r\n\t [[Svd]]\r\n\"\r\n\r\nThe code I am running with is as following:\r\n\"\r\ndef top_singular_vector( m):\r\n        # handle the case where m is a tensor of rank 0 or rank 1.\r\n        n = tf.cond(\r\n            tf.equal(tf.rank(m), 0),\r\n            lambda: tf.expand_dims(tf.expand_dims(m, [0]), [0]),\r\n            lambda: m,\r\n        )\r\n        n = tf.cond(tf.equal(tf.rank(m), 1), lambda: tf.expand_dims(m, [0]), lambda: n,)\r\n        st, ut, vt = tf.linalg.svd(n, full_matrices=False)\r\n        m_size = tf.shape(n)\r\n        ut = tf.reshape(ut[:, 0], [m_size[0], 1])\r\n        vt = tf.reshape(vt[:, 0], [m_size[1], 1])\r\n        st = tf.matmul(ut, tf.transpose(vt))\r\n        # when we return the top singular vector, we have to remove the\r\n        # dimension we have added on\r\n        st = tf.cond(\r\n            tf.equal(tf.rank(m), 0),\r\n            lambda: tf.squeeze(tf.squeeze(st, [0]), [0]),\r\n            lambda: st,\r\n        )\r\n        st = tf.cond(tf.equal(tf.rank(m), 1), lambda: tf.squeeze(st, [0]), lambda: st,)\r\n        return st\r\n\r\ngrads0 = tf.constant([0.1, 0.1], dtype=tf.half)\r\ngrads1 = tf.constant([0.01, 0.01], dtype=tf.half)\r\ntop_singular_vector0 = top_singular_vector(grads0)\r\ntop_singular_vector1 = top_singular_vector(grads1)\r\n\"\r\n\r\nIt will be great if tf.linalg.svd function can have more compatible or similar behavior on dtype over different systems.\r\n\r\nThank you!\r\n", "comments": ["@pkan2 Please provide the code snippet  in proper format  as  we are facing indentation error.  Also, please provide the TF version. Thanks!", "Hello @sai110594 !\r\n\r\nThe code is as following:\r\n```\r\ndef top_singular_vector(m):\r\n        # handle the case where m is a tensor of rank 0 or rank 1.\r\n        n = tf.cond(\r\n            tf.equal(tf.rank(m), 0),\r\n            lambda: tf.expand_dims(tf.expand_dims(m, [0]), [0]),\r\n            lambda: m,\r\n        )\r\n        n = tf.cond(tf.equal(tf.rank(m), 1), lambda: tf.expand_dims(m, [0]), lambda: n,)\r\n        st, ut, vt = tf.linalg.svd(n, full_matrices=False)\r\n        m_size = tf.shape(n)\r\n        ut = tf.reshape(ut[:, 0], [m_size[0], 1])\r\n        vt = tf.reshape(vt[:, 0], [m_size[1], 1])\r\n        st = tf.matmul(ut, tf.transpose(vt))\r\n        # when we return the top singular vector, we have to remove the\r\n        # dimension we have added on\r\n        st = tf.cond(\r\n            tf.equal(tf.rank(m), 0),\r\n            lambda: tf.squeeze(tf.squeeze(st, [0]), [0]),\r\n            lambda: st,\r\n        )\r\n        st = tf.cond(tf.equal(tf.rank(m), 1), lambda: tf.squeeze(st, [0]), lambda: st,)\r\n        return st\r\n\r\ngrads0 = tf.constant([0.1, 0.1], dtype=tf.half)\r\ngrads1 = tf.constant([0.01, 0.01], dtype=tf.half)\r\ntop_singular_vector0 = top_singular_vector(grads0)\r\ntop_singular_vector1 = top_singular_vector(grads1)\r\n```\r\n\r\nAnd I think the version used by the Windows CPU Python3 test is:\r\n```\r\nCollecting tensorflow==2.1.0\r\nDownloading tensorflow-2.1.0-cp35-cp35m-win_amd64.whl (355.8 MB)\r\n```\r\n\r\nThank you!\r\n", "I tried to reproduce the issue in colab  with TF 2.1 and didn't face any issues. Please find the [gist](https://colab.sandbox.google.com/drive/1oI3bNG72gHc7MjnpcwHiv6i3M_jDCVO9#scrollTo=l4K9fKHwfwci) here. Thanks!", "Is it possibly caused by the fact the code is tested with graph mode rather than eager execution mode?\r\n\r\nHere is the full file:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow_addons.utils.types import FloatTensorLike\r\n\r\nfrom typeguard import typechecked\r\nfrom typing import Union, Callable\r\n\r\n\r\n@tf.keras.utils.register_keras_serializable(package=\"Addons\")\r\nclass ConditionalGradient(tf.keras.optimizers.Optimizer):\r\n    \"\"\"Optimizer that implements the Conditional Gradient optimization.\r\n    This optimizer helps handle constraints well.\r\n    Currently only supports frobenius norm constraint or nuclear norm\r\n    constraint.\r\n    See https://arxiv.org/pdf/1803.06453.pdf\r\n    ```\r\n    variable -= (1-learning_rate) * (variable + lambda_ * gradient\r\n        / (frobenius_norm(gradient) + epsilon))\r\n    ```\r\n    Note that `lambda_` here refers to the constraint \"lambda\" in\r\n    the paper. `epsilon` is constant with tiny value as compared to\r\n    the value of frobenius norm of gradient. The purpose of `epsilon`\r\n    here is to avoid the case that the value of frobenius norm of\r\n    gradient is 0.\r\n    In this implementation, `epsilon` defaults to $10^{-7}$.\r\n    For nucler norm constraint, the formula is as following:\r\n    ```\r\n    variable -= (1-learning_rate) * (variable\r\n    + lambda_ * top_singular_vector(gradient))\r\n    ```\r\n    \"\"\"\r\n\r\n    @typechecked\r\n    def __init__(\r\n        self,\r\n        learning_rate: Union[FloatTensorLike, Callable],\r\n        lambda_: Union[FloatTensorLike, Callable] = 0.01,\r\n        epsilon: FloatTensorLike = 1e-7,\r\n        ord: str = \"fro\",\r\n        use_locking: bool = False,\r\n        name: str = \"ConditionalGradient\",\r\n        **kwargs\r\n    ):\r\n        \"\"\"Construct a new conditional gradient optimizer.\r\n        Args:\r\n            learning_rate: A `Tensor` or a floating point value. or a schedule\r\n                that is a `tf.keras.optimizers.schedules.LearningRateSchedule`\r\n                The learning rate.\r\n            lambda_: A `Tensor` or a floating point value. The constraint.\r\n            epsilon: A `Tensor` or a floating point value. A small constant\r\n                for numerical stability when handling the case of norm of\r\n                gradient to be zero.\r\n            ord: Order of the norm. Supported values are `'fro'`\r\n                and `'nuclear'`. Default is `'fro'`, which is frobenius norm.\r\n            use_locking: If `True`, use locks for update operations.\r\n            name: Optional name prefix for the operations created when\r\n                applying gradients. Defaults to 'ConditionalGradient'.\r\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`,\r\n                `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients\r\n                by norm; `clipvalue` is clip gradients by value, `decay` is\r\n                included for backward compatibility to allow time inverse\r\n                decay of learning rate. `lr` is included for backward\r\n                compatibility, recommended to use `learning_rate` instead.\r\n        \"\"\"\r\n        super().__init__(name=name, **kwargs)\r\n        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\r\n        self._set_hyper(\"lambda_\", lambda_)\r\n        self.epsilon = epsilon or tf.keras.backend.epsilon()\r\n        supported_norms = [\"fro\", \"nuclear\"]\r\n        if ord not in supported_norms:\r\n            raise ValueError(\r\n                \"'ord' must be a supported matrix norm in %s, got '%s' instead\"\r\n                % (supported_norms, ord)\r\n            )\r\n        self.ord = ord\r\n        self._set_hyper(\"use_locking\", use_locking)\r\n\r\n    def get_config(self):\r\n        config = {\r\n            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\r\n            \"lambda_\": self._serialize_hyperparameter(\"lambda_\"),\r\n            \"epsilon\": self.epsilon,\r\n            \"ord\": self.ord,\r\n            \"use_locking\": self._serialize_hyperparameter(\"use_locking\"),\r\n        }\r\n        base_config = super().get_config()\r\n        return {**base_config, **config}\r\n\r\n    def _create_slots(self, var_list):\r\n        for v in var_list:\r\n            self.add_slot(v, \"conditional_gradient\")\r\n\r\n    def _prepare_local(self, var_device, var_dtype, apply_state):\r\n        super()._prepare_local(var_device, var_dtype, apply_state)\r\n        apply_state[(var_device, var_dtype)][\"learning_rate\"] = tf.identity(\r\n            self._get_hyper(\"learning_rate\", var_dtype)\r\n        )\r\n        apply_state[(var_device, var_dtype)][\"lambda_\"] = tf.identity(\r\n            self._get_hyper(\"lambda_\", var_dtype)\r\n        )\r\n        apply_state[(var_device, var_dtype)][\"epsilon\"] = tf.convert_to_tensor(\r\n            self.epsilon, var_dtype\r\n        )\r\n\r\n    def _resource_apply_dense(self, grad, var, apply_state=None):\r\n        def frobenius_norm(m):\r\n            return tf.math.reduce_sum(m ** 2) ** 0.5\r\n\r\n        def top_singular_vector(m):\r\n            # handle the case where m is a tensor of rank 0 or rank 1.\r\n            n = tf.cond(\r\n                tf.equal(tf.rank(m), 0),\r\n                lambda: tf.expand_dims(tf.expand_dims(m, [0]), [0]),\r\n                lambda: m,\r\n            )\r\n            n = tf.cond(\r\n                tf.equal(tf.rank(m), 1), lambda: tf.expand_dims(m, [0]), lambda: n,\r\n            )\r\n            st, ut, vt = tf.linalg.svd(n, full_matrices=False)\r\n            m_size = tf.shape(n)\r\n            ut = tf.reshape(ut[:, 0], [m_size[0], 1])\r\n            vt = tf.reshape(vt[:, 0], [m_size[1], 1])\r\n            st = tf.matmul(ut, tf.transpose(vt))\r\n            # when we return the top singular vector, we have to remove the\r\n            # dimension we have added on\r\n            st = tf.cond(\r\n                tf.equal(tf.rank(m), 0),\r\n                lambda: tf.squeeze(tf.squeeze(st, [0]), [0]),\r\n                lambda: st,\r\n            )\r\n            st = tf.cond(\r\n                tf.equal(tf.rank(m), 1), lambda: tf.squeeze(st, [0]), lambda: st,\r\n            )\r\n            return st\r\n\r\n        ord = self.ord\r\n        if ord == \"fro\":\r\n            var_device, var_dtype = var.device, var.dtype.base_dtype\r\n            coefficients = (apply_state or {}).get(\r\n                (var_device, var_dtype)\r\n            ) or self._fallback_apply_state(var_device, var_dtype)\r\n            norm = tf.convert_to_tensor(\r\n                frobenius_norm(grad), name=\"norm\", dtype=var.dtype.base_dtype\r\n            )\r\n            lr = coefficients[\"learning_rate\"]\r\n            lambda_ = coefficients[\"lambda_\"]\r\n            epsilon = coefficients[\"epsilon\"]\r\n            var_update_tensor = tf.math.multiply(var, lr) - (\r\n                1 - lr\r\n            ) * lambda_ * grad / (norm + epsilon)\r\n            var_update_kwargs = {\r\n                \"resource\": var.handle,\r\n                \"value\": var_update_tensor,\r\n            }\r\n            var_update_op = tf.raw_ops.AssignVariableOp(**var_update_kwargs)\r\n            return tf.group(var_update_op)\r\n\r\n        else:\r\n            var_device, var_dtype = var.device, var.dtype.base_dtype\r\n            coefficients = (apply_state or {}).get(\r\n                (var_device, var_dtype)\r\n            ) or self._fallback_apply_state(var_device, var_dtype)\r\n            top_singular_vector = tf.convert_to_tensor(\r\n                top_singular_vector(grad),\r\n                name=\"top_singular_vector\",\r\n                dtype=var.dtype.base_dtype,\r\n            )\r\n            lr = coefficients[\"learning_rate\"]\r\n            lambda_ = coefficients[\"lambda_\"]\r\n            epsilon = coefficients[\"epsilon\"]\r\n            var_update_tensor = (\r\n                tf.math.multiply(var, lr) - (1 - lr) * lambda_ * top_singular_vector\r\n            )\r\n            var_update_kwargs = {\r\n                \"resource\": var.handle,\r\n                \"value\": var_update_tensor,\r\n            }\r\n            var_update_op = tf.raw_ops.AssignVariableOp(**var_update_kwargs)\r\n            return tf.group(var_update_op)\r\n\r\n    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\r\n        def frobenius_norm(m):\r\n            return tf.reduce_sum(m ** 2) ** 0.5\r\n\r\n        def top_singular_vector(m):\r\n            # handle the case where m is a tensor of rank 0 or rank 1.\r\n            n = tf.cond(\r\n                tf.equal(tf.rank(m), 0),\r\n                lambda: tf.expand_dims(tf.expand_dims(m, [0]), [0]),\r\n                lambda: m,\r\n            )\r\n            n = tf.cond(\r\n                tf.equal(tf.rank(m), 1), lambda: tf.expand_dims(m, [0]), lambda: n,\r\n            )\r\n            st, ut, vt = tf.linalg.svd(n, full_matrices=False)\r\n            m_size = tf.shape(n)\r\n            ut = tf.reshape(ut[:, 0], [m_size[0], 1])\r\n            vt = tf.reshape(vt[:, 0], [m_size[1], 1])\r\n            st = tf.matmul(ut, tf.transpose(vt))\r\n            # when we return the top singular vector, we have to remove the\r\n            # dimension we have added on\r\n            st = tf.cond(\r\n                tf.equal(tf.rank(m), 0),\r\n                lambda: tf.squeeze(tf.squeeze(st, [0]), [0]),\r\n                lambda: st,\r\n            )\r\n            st = tf.cond(\r\n                tf.equal(tf.rank(m), 1), lambda: tf.squeeze(st, [0]), lambda: st,\r\n            )\r\n            return st\r\n\r\n        ord = self.ord\r\n        if ord == \"fro\":\r\n            var_device, var_dtype = var.device, var.dtype.base_dtype\r\n            coefficients = (apply_state or {}).get(\r\n                (var_device, var_dtype)\r\n            ) or self._fallback_apply_state(var_device, var_dtype)\r\n            norm = tf.convert_to_tensor(\r\n                frobenius_norm(grad), name=\"norm\", dtype=var.dtype.base_dtype\r\n            )\r\n            lr = coefficients[\"learning_rate\"]\r\n            lambda_ = coefficients[\"lambda_\"]\r\n            epsilon = coefficients[\"epsilon\"]\r\n            var_slice = tf.gather(var, indices)\r\n            var_update_value = tf.math.multiply(var_slice, lr) - (\r\n                1 - lr\r\n            ) * lambda_ * grad / (norm + epsilon)\r\n            var_update_kwargs = {\r\n                \"resource\": var.handle,\r\n                \"indices\": indices,\r\n                \"updates\": var_update_value,\r\n            }\r\n            var_update_op = tf.raw_ops.ResourceScatterUpdate(**var_update_kwargs)\r\n            return tf.group(var_update_op)\r\n\r\n        else:\r\n            var_device, var_dtype = var.device, var.dtype.base_dtype\r\n            coefficients = (apply_state or {}).get(\r\n                (var_device, var_dtype)\r\n            ) or self._fallback_apply_state(var_device, var_dtype)\r\n            top_singular_vector = tf.convert_to_tensor(\r\n                top_singular_vector(grad),\r\n                name=\"top_singular_vector\",\r\n                dtype=var.dtype.base_dtype,\r\n            )\r\n            lr = coefficients[\"learning_rate\"]\r\n            lambda_ = coefficients[\"lambda_\"]\r\n            var_slice = tf.gather(var, indices)\r\n            var_update_value = (\r\n                tf.math.multiply(var_slice, lr)\r\n                - (1 - lr) * lambda_ * top_singular_vector\r\n            )\r\n            var_update_kwargs = {\r\n                \"resource\": var.handle,\r\n                \"indices\": indices,\r\n                \"updates\": var_update_value,\r\n            }\r\n            var_update_op = tf.raw_ops.ResourceScatterUpdate(**var_update_kwargs)\r\n            return tf.group(var_update_op)\r\n```\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow_addons.utils import test_utils\r\nimport numpy as np\r\nimport conditional_gradient as cg_lib\r\n\r\n\r\n@test_utils.run_all_in_graph_and_eager_modes\r\nclass ConditionalGradientTest(tf.test.TestCase):\r\n    def _update_conditional_gradient_numpy(self, var, norm, g, lr, lambda_):\r\n        var = var * lr - (1 - lr) * lambda_ * g / norm\r\n        return var\r\n\r\n    def top_singular_vector(self, m):\r\n        # handle the case where m is a tensor of rank 0 or rank 1.\r\n        n = tf.cond(\r\n            tf.equal(tf.rank(m), 0),\r\n            lambda: tf.expand_dims(tf.expand_dims(m, [0]), [0]),\r\n            lambda: m,\r\n        )\r\n        n = tf.cond(tf.equal(tf.rank(m), 1), lambda: tf.expand_dims(m, [0]), lambda: n,)\r\n        st, ut, vt = tf.linalg.svd(n, full_matrices=False)\r\n        m_size = tf.shape(n)\r\n        ut = tf.reshape(ut[:, 0], [m_size[0], 1])\r\n        vt = tf.reshape(vt[:, 0], [m_size[1], 1])\r\n        st = tf.matmul(ut, tf.transpose(vt))\r\n        # when we return the top singular vector, we have to remove the\r\n        # dimension we have added on\r\n        st = tf.cond(\r\n            tf.equal(tf.rank(m), 0),\r\n            lambda: tf.squeeze(tf.squeeze(st, [0]), [0]),\r\n            lambda: st,\r\n        )\r\n        st = tf.cond(tf.equal(tf.rank(m), 1), lambda: tf.squeeze(st, [0]), lambda: st,)\r\n        return st\r\n\r\n    def doTestBasicNuclear(self, use_resource=False, use_callable_params=False):\r\n        for i, dtype in enumerate([tf.half, tf.float32, tf.float64]):\r\n            if use_resource:\r\n                var0 = tf.Variable([1.0, 2.0], dtype=dtype, name=\"var0_%d\" % i)\r\n                var1 = tf.Variable([3.0, 4.0], dtype=dtype, name=\"var1_%d\" % i)\r\n            else:\r\n                var0 = tf.Variable([1.0, 2.0], dtype=dtype)\r\n                var1 = tf.Variable([3.0, 4.0], dtype=dtype)\r\n\r\n            grads0 = tf.constant([0.1, 0.1], dtype=dtype)\r\n            grads1 = tf.constant([0.01, 0.01], dtype=dtype)\r\n            top_singular_vector0 = self.top_singular_vector(grads0)\r\n            top_singular_vector1 = self.top_singular_vector(grads1)\r\n\r\n            def learning_rate():\r\n                return 0.5\r\n\r\n            def lambda_():\r\n                return 0.01\r\n\r\n            ord = \"nuclear\"\r\n\r\n            if not use_callable_params:\r\n                learning_rate = learning_rate()\r\n                lambda_ = lambda_()\r\n\r\n            cg_opt = cg_lib.ConditionalGradient(\r\n                learning_rate=learning_rate, lambda_=lambda_, ord=ord\r\n            )\r\n            cg_update = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\r\n\r\n            if not tf.executing_eagerly():\r\n                self.evaluate(tf.compat.v1.global_variables_initializer())\r\n                # Fetch params to validate initial values\r\n                self.assertAllClose([1.0, 2.0], self.evaluate(var0))\r\n                self.assertAllClose([3.0, 4.0], self.evaluate(var1))\r\n\r\n            # Check we have slots\r\n            self.assertEqual([\"conditional_gradient\"], cg_opt.get_slot_names())\r\n            slot0 = cg_opt.get_slot(var0, \"conditional_gradient\")\r\n            self.assertEquals(slot0.get_shape(), var0.get_shape())\r\n            slot1 = cg_opt.get_slot(var1, \"conditional_gradient\")\r\n            self.assertEquals(slot1.get_shape(), var1.get_shape())\r\n\r\n            if not tf.executing_eagerly():\r\n                self.assertFalse(slot0 in tf.compat.v1.trainable_variables())\r\n                self.assertFalse(slot1 in tf.compat.v1.trainable_variables())\r\n\r\n            if not tf.executing_eagerly():\r\n                self.evaluate(cg_update)\r\n\r\n            top_singular_vector0 = self.evaluate(top_singular_vector0)\r\n            top_singular_vector1 = self.evaluate(top_singular_vector1)\r\n\r\n            self.assertAllCloseAccordingToType(\r\n                np.array(\r\n                    [\r\n                        1.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[0],\r\n                        2.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[1],\r\n                    ]\r\n                ),\r\n                self.evaluate(var0),\r\n            )\r\n            self.assertAllCloseAccordingToType(\r\n                np.array(\r\n                    [\r\n                        3.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0],\r\n                        4.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[1],\r\n                    ]\r\n                ),\r\n                self.evaluate(var1),\r\n            )\r\n\r\n            # Step 2: the conditional_gradient contain the previous update.\r\n            if tf.executing_eagerly():\r\n                cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\r\n            else:\r\n                self.evaluate(cg_update)\r\n            self.assertAllCloseAccordingToType(\r\n                np.array(\r\n                    [\r\n                        (1.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[0]) * 0.5\r\n                        - (1 - 0.5) * 0.01 * top_singular_vector0[0],\r\n                        (2.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[1]) * 0.5\r\n                        - (1 - 0.5) * 0.01 * top_singular_vector0[1],\r\n                    ]\r\n                ),\r\n                self.evaluate(var0),\r\n            )\r\n            self.assertAllCloseAccordingToType(\r\n                np.array(\r\n                    [\r\n                        (3.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0]) * 0.5\r\n                        - (1 - 0.5) * 0.01 * top_singular_vector1[1],\r\n                        (4.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0]) * 0.5\r\n                        - (1 - 0.5) * 0.01 * top_singular_vector1[1],\r\n                    ]\r\n                ),\r\n                self.evaluate(var1),\r\n            )\r\n\r\n\r\n    def testBasicNuclear(self):\r\n        with self.cached_session():\r\n            self.doTestBasicNuclear(use_resource=False)\r\n\r\n    def testResourceBasicNuclear(self):\r\n        self.doTestBasicNuclear(use_resource=True)\r\n\r\n    def testBasicCallableParamsNuclear(self):\r\n        self.doTestBasicNuclear(use_resource=True, use_callable_params=True)\r\n\r\n    def testVariablesAcrossGraphsNuclear(self):\r\n        optimizer = cg_lib.ConditionalGradient(0.01, 0.5, ord=\"nuclear\")\r\n        with tf.Graph().as_default():\r\n            var0 = tf.Variable([1.0, 2.0], dtype=tf.float32, name=\"var0\")\r\n            var1 = tf.Variable([3.0, 4.0], dtype=tf.float32, name=\"var1\")\r\n\r\n            def loss():\r\n                return tf.math.reduce_sum(var0 + var1)\r\n\r\n            optimizer.minimize(loss, var_list=[var0, var1])\r\n            optimizer_variables = optimizer.variables()\r\n            # There should be three items. The first item is iteration,\r\n            # and one item for each variable.\r\n            self.assertStartsWith(\r\n                optimizer_variables[1].name, \"ConditionalGradient/var0\"\r\n            )\r\n            self.assertStartsWith(\r\n                optimizer_variables[2].name, \"ConditionalGradient/var1\"\r\n            )\r\n            self.assertEqual(3, len(optimizer_variables))\r\n\r\n    # Based on issue #347 in the following link,\r\n    #        \"https://github.com/tensorflow/addons/issues/347\"\r\n    # tf.half is not registered for 'ResourceScatterUpdate' OpKernel\r\n    # for 'GPU' devices.\r\n    # So we have to remove tf.half when testing with gpu.\r\n    # The function \"_DtypesToTest\" is from\r\n    #       \"https://github.com/tensorflow/tensorflow/blob/5d4a6cee737a1dc6c20172a1dc1\r\n    #        5df10def2df72/tensorflow/python/kernel_tests/conv_ops_3d_test.py#L53-L62\"\r\n\r\n    def _DtypesToTest(self, use_gpu):\r\n        if use_gpu:\r\n            return [tf.float32, tf.float64]\r\n        else:\r\n            return [tf.half, tf.float32, tf.float64]\r\n\r\n    def testMinimizeSparseResourceVariableNuclear(self):\r\n        # This test invokes the ResourceSparseApplyConditionalGradient\r\n        # operation. And it will call the 'ResourceScatterUpdate' OpKernel\r\n        # for 'GPU' devices. However, tf.half is not registered in this case,\r\n        # based on issue #347.\r\n        # Thus, we will call the \"_DtypesToTest\" function.\r\n        #\r\n        # TODO:\r\n        #       Wait for the solving of issue #347. After that, we will test\r\n        #       for the dtype to be tf.half, with 'GPU' devices.\r\n        for dtype in self._DtypesToTest(use_gpu=tf.test.is_gpu_available()):\r\n            var0 = tf.Variable([[1.0, 2.0]], dtype=dtype)\r\n\r\n            def loss():\r\n                x = tf.constant([[4.0], [5.0]], dtype=dtype)\r\n                pred = tf.matmul(tf.nn.embedding_lookup([var0], [0]), x)\r\n                return pred * pred\r\n\r\n            # the gradient based on the current loss function\r\n            grads0_0 = 32 * 1.0 + 40 * 2.0\r\n            grads0_1 = 40 * 1.0 + 50 * 2.0\r\n            grads0 = tf.constant([[grads0_0, grads0_1]], dtype=dtype)\r\n            top_singular_vector0 = self.top_singular_vector(grads0)\r\n\r\n            learning_rate = 0.1\r\n            lambda_ = 0.1\r\n            ord = \"nuclear\"\r\n            opt = cg_lib.ConditionalGradient(\r\n                learning_rate=learning_rate, lambda_=lambda_, ord=ord\r\n            )\r\n            cg_op = opt.minimize(loss, var_list=[var0])\r\n            self.evaluate(tf.compat.v1.global_variables_initializer())\r\n\r\n            # Run 1 step of cg_op\r\n            self.evaluate(cg_op)\r\n\r\n            # Validate updated params\r\n            top_singular_vector0 = self.evaluate(top_singular_vector0)\r\n            self.assertAllCloseAccordingToType(\r\n                [\r\n                    [\r\n                        1.0 * learning_rate\r\n                        - (1 - learning_rate) * lambda_ * top_singular_vector0[0][0],\r\n                        2.0 * learning_rate\r\n                        - (1 - learning_rate) * lambda_ * top_singular_vector0[0][1],\r\n                    ]\r\n                ],\r\n                self.evaluate(var0),\r\n            )\r\n\r\n    def testMinimizeWith2DIndiciesForEmbeddingLookupNuclear(self):\r\n        # This test invokes the ResourceSparseApplyConditionalGradient\r\n        # operation.\r\n        var0 = tf.Variable(tf.ones([2, 2]))\r\n\r\n        def loss():\r\n            return tf.math.reduce_sum(tf.nn.embedding_lookup(var0, [[1]]))\r\n\r\n        # the gradient for this loss function:\r\n        grads0 = tf.constant([[0, 0], [1, 1]], dtype=tf.float32)\r\n        top_singular_vector0 = self.top_singular_vector(grads0)\r\n\r\n        learning_rate = 0.1\r\n        lambda_ = 0.1\r\n        ord = \"nuclear\"\r\n        opt = cg_lib.ConditionalGradient(\r\n            learning_rate=learning_rate, lambda_=lambda_, ord=ord\r\n        )\r\n        cg_op = opt.minimize(loss, var_list=[var0])\r\n        self.evaluate(tf.compat.v1.global_variables_initializer())\r\n\r\n        # Run 1 step of cg_op\r\n        self.evaluate(cg_op)\r\n        top_singular_vector0 = self.evaluate(top_singular_vector0)\r\n        self.evaluate(var0)\r\n        self.assertAllCloseAccordingToType(\r\n            [\r\n                learning_rate * 1\r\n                - (1 - learning_rate) * lambda_ * top_singular_vector0[1][0],\r\n                learning_rate * 1\r\n                - (1 - learning_rate) * lambda_ * top_singular_vector0[1][1],\r\n            ],\r\n            self.evaluate(var0[1]),\r\n        )\r\n\r\n    def testTensorLearningRateAndConditionalGradientNuclear(self):\r\n        for dtype in [tf.half, tf.float32, tf.float64]:\r\n            with self.cached_session():\r\n                var0 = tf.Variable([1.0, 2.0], dtype=dtype)\r\n                var1 = tf.Variable([3.0, 4.0], dtype=dtype)\r\n                grads0 = tf.constant([0.1, 0.1], dtype=dtype)\r\n                grads1 = tf.constant([0.01, 0.01], dtype=dtype)\r\n                top_singular_vector0 = self.top_singular_vector(grads0)\r\n                top_singular_vector1 = self.top_singular_vector(grads1)\r\n                ord = \"nuclear\"\r\n                cg_opt = cg_lib.ConditionalGradient(\r\n                    learning_rate=tf.constant(0.5), lambda_=tf.constant(0.01), ord=ord\r\n                )\r\n                cg_update = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\r\n                if not tf.executing_eagerly():\r\n                    self.evaluate(tf.compat.v1.global_variables_initializer())\r\n                    # Fetch params to validate initial values\r\n                    self.assertAllClose([1.0, 2.0], self.evaluate(var0))\r\n                    self.assertAllClose([3.0, 4.0], self.evaluate(var1))\r\n\r\n                # Check we have slots\r\n                self.assertEqual([\"conditional_gradient\"], cg_opt.get_slot_names())\r\n                slot0 = cg_opt.get_slot(var0, \"conditional_gradient\")\r\n                self.assertEquals(slot0.get_shape(), var0.get_shape())\r\n                slot1 = cg_opt.get_slot(var1, \"conditional_gradient\")\r\n                self.assertEquals(slot1.get_shape(), var1.get_shape())\r\n\r\n                if not tf.executing_eagerly():\r\n                    self.assertFalse(slot0 in tf.compat.v1.trainable_variables())\r\n                    self.assertFalse(slot1 in tf.compat.v1.trainable_variables())\r\n\r\n                if not tf.executing_eagerly():\r\n                    self.evaluate(cg_update)\r\n                # Check that the parameters have been updated.\r\n                top_singular_vector0 = self.evaluate(top_singular_vector0)\r\n                top_singular_vector1 = self.evaluate(top_singular_vector1)\r\n                self.assertAllCloseAccordingToType(\r\n                    np.array(\r\n                        [\r\n                            1.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[0],\r\n                            2.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[1],\r\n                        ]\r\n                    ),\r\n                    self.evaluate(var0),\r\n                )\r\n                self.assertAllCloseAccordingToType(\r\n                    np.array(\r\n                        [\r\n                            3.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0],\r\n                            4.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[1],\r\n                        ]\r\n                    ),\r\n                    self.evaluate(var1),\r\n                )\r\n                # Step 2: the conditional_gradient contain the\r\n                # previous update.\r\n                if tf.executing_eagerly():\r\n                    cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\r\n                else:\r\n                    self.evaluate(cg_update)\r\n                # Check that the parameters have been updated.\r\n                self.assertAllCloseAccordingToType(\r\n                    np.array(\r\n                        [\r\n                            (1.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[0])\r\n                            * 0.5\r\n                            - (1 - 0.5) * 0.01 * top_singular_vector0[0],\r\n                            (2.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[1])\r\n                            * 0.5\r\n                            - (1 - 0.5) * 0.01 * top_singular_vector0[1],\r\n                        ]\r\n                    ),\r\n                    self.evaluate(var0),\r\n                )\r\n                self.assertAllCloseAccordingToType(\r\n                    np.array(\r\n                        [\r\n                            (3.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0])\r\n                            * 0.5\r\n                            - (1 - 0.5) * 0.01 * top_singular_vector1[0],\r\n                            (4.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[1])\r\n                            * 0.5\r\n                            - (1 - 0.5) * 0.01 * top_singular_vector1[1],\r\n                        ]\r\n                    ),\r\n                    self.evaluate(var1),\r\n                )\r\n\r\n    def _dbParamsNuclearCG01(self):\r\n        \"\"\"Return dist-belief conditional_gradient values.\r\n        Return values been generated from the dist-belief\r\n        conditional_gradient unittest, running with a learning rate of 0.1\r\n        and a lambda_ of 0.1.\r\n        These values record how a parameter vector of size 10, initialized\r\n        with 0.0, gets updated with 10 consecutive conditional_gradient\r\n        steps.\r\n        It uses random gradients.\r\n        Returns:\r\n            db_grad: The gradients to apply\r\n            db_out: The parameters after the conditional_gradient update.\r\n        \"\"\"\r\n        db_grad = [[]] * 10\r\n        db_out = [[]] * 10\r\n        db_grad[0] = [\r\n            0.00096264342,\r\n            0.17914793,\r\n            0.93945462,\r\n            0.41396621,\r\n            0.53037018,\r\n            0.93197989,\r\n            0.78648776,\r\n            0.50036013,\r\n            0.55345792,\r\n            0.96722615,\r\n        ]\r\n        db_out[0] = [\r\n            -4.1552783e-05,\r\n            -7.7334875e-03,\r\n            -4.0554535e-02,\r\n            -1.7870164e-02,\r\n            -2.2895109e-02,\r\n            -4.0231861e-02,\r\n            -3.3951234e-02,\r\n            -2.1599628e-02,\r\n            -2.3891764e-02,\r\n            -4.1753381e-02,\r\n        ]\r\n        db_grad[1] = [\r\n            0.17075552,\r\n            0.88821375,\r\n            0.20873757,\r\n            0.25236958,\r\n            0.57578111,\r\n            0.15312378,\r\n            0.5513742,\r\n            0.94687688,\r\n            0.16012503,\r\n            0.22159521,\r\n        ]\r\n        db_out[1] = [\r\n            -0.00961733,\r\n            -0.0507779,\r\n            -0.01580694,\r\n            -0.01599489,\r\n            -0.03470477,\r\n            -0.01264373,\r\n            -0.03443632,\r\n            -0.05546713,\r\n            -0.01140388,\r\n            -0.01665068,\r\n        ]\r\n        db_grad[2] = [\r\n            0.35077485,\r\n            0.47304362,\r\n            0.44412705,\r\n            0.44368884,\r\n            0.078527533,\r\n            0.81223965,\r\n            0.31168157,\r\n            0.43203235,\r\n            0.16792089,\r\n            0.24644311,\r\n        ]\r\n        db_out[2] = [\r\n            -0.02462724,\r\n            -0.03699233,\r\n            -0.03154433,\r\n            -0.03153357,\r\n            -0.00876844,\r\n            -0.05606324,\r\n            -0.02447166,\r\n            -0.03469437,\r\n            -0.0124694,\r\n            -0.01829169,\r\n        ]\r\n        db_grad[3] = [\r\n            0.9694621,\r\n            0.75035888,\r\n            0.28171822,\r\n            0.83813518,\r\n            0.53807181,\r\n            0.3728098,\r\n            0.81454384,\r\n            0.03848977,\r\n            0.89759839,\r\n            0.93665648,\r\n        ]\r\n        db_out[3] = [\r\n            -0.04124615,\r\n            -0.03371741,\r\n            -0.0144246,\r\n            -0.03668303,\r\n            -0.02240246,\r\n            -0.02052062,\r\n            -0.03503307,\r\n            -0.00500922,\r\n            -0.03715545,\r\n            -0.0393002,\r\n        ]\r\n        db_grad[4] = [\r\n            0.38578293,\r\n            0.8536852,\r\n            0.88722926,\r\n            0.66276771,\r\n            0.13678469,\r\n            0.94036359,\r\n            0.69107032,\r\n            0.81897682,\r\n            0.5433259,\r\n            0.67860287,\r\n        ]\r\n        db_out[4] = [\r\n            -0.01979207,\r\n            -0.0380417,\r\n            -0.03747472,\r\n            -0.0305847,\r\n            -0.00779536,\r\n            -0.04024221,\r\n            -0.03156913,\r\n            -0.0337613,\r\n            -0.02578116,\r\n            -0.03148951,\r\n        ]\r\n        db_grad[5] = [\r\n            0.27885768,\r\n            0.76100707,\r\n            0.24625534,\r\n            0.81354135,\r\n            0.18959245,\r\n            0.48038563,\r\n            0.84163809,\r\n            0.41172323,\r\n            0.83259648,\r\n            0.44941229,\r\n        ]\r\n        db_out[5] = [\r\n            -0.01555188,\r\n            -0.04084422,\r\n            -0.01573331,\r\n            -0.04265549,\r\n            -0.01000746,\r\n            -0.02740575,\r\n            -0.04412147,\r\n            -0.02341569,\r\n            -0.0431026,\r\n            -0.02502293,\r\n        ]\r\n        db_grad[6] = [\r\n            0.27233034,\r\n            0.056316052,\r\n            0.5039115,\r\n            0.24105175,\r\n            0.35697976,\r\n            0.75913221,\r\n            0.73577434,\r\n            0.16014607,\r\n            0.57500273,\r\n            0.071136251,\r\n        ]\r\n        db_out[6] = [\r\n            -0.01890448,\r\n            -0.00767214,\r\n            -0.03367592,\r\n            -0.01962219,\r\n            -0.02374278,\r\n            -0.05110246,\r\n            -0.05128598,\r\n            -0.01254396,\r\n            -0.04094184,\r\n            -0.00703416,\r\n        ]\r\n        db_grad[7] = [\r\n            0.58697265,\r\n            0.2494842,\r\n            0.08106143,\r\n            0.39954534,\r\n            0.15892942,\r\n            0.12683646,\r\n            0.74053431,\r\n            0.16033,\r\n            0.66625422,\r\n            0.73515922,\r\n        ]\r\n        db_out[7] = [\r\n            -0.03772915,\r\n            -0.01599993,\r\n            -0.00831695,\r\n            -0.0263572,\r\n            -0.01207801,\r\n            -0.01285448,\r\n            -0.05034329,\r\n            -0.01104364,\r\n            -0.04477356,\r\n            -0.04558992,\r\n        ]\r\n        db_grad[8] = [\r\n            0.8215279,\r\n            0.41994119,\r\n            0.95172721,\r\n            0.68000203,\r\n            0.79439718,\r\n            0.43384039,\r\n            0.55561525,\r\n            0.22567581,\r\n            0.93331909,\r\n            0.29438227,\r\n        ]\r\n        db_out[8] = [\r\n            -0.03919835,\r\n            -0.01970845,\r\n            -0.04187151,\r\n            -0.03195836,\r\n            -0.03546333,\r\n            -0.01999326,\r\n            -0.02899324,\r\n            -0.01083582,\r\n            -0.04472339,\r\n            -0.01725317,\r\n        ]\r\n        db_grad[9] = [\r\n            0.68297005,\r\n            0.67758518,\r\n            0.1748755,\r\n            0.13266537,\r\n            0.70697063,\r\n            0.055731893,\r\n            0.68593478,\r\n            0.50580865,\r\n            0.12602448,\r\n            0.093537711,\r\n        ]\r\n        db_out[9] = [\r\n            -0.04510314,\r\n            -0.04282944,\r\n            -0.0147322,\r\n            -0.0111956,\r\n            -0.04617687,\r\n            -0.00535998,\r\n            -0.0442614,\r\n            -0.031584,\r\n            -0.01207165,\r\n            -0.00736567,\r\n        ]\r\n        return db_grad, db_out\r\n\r\n    def testLikeDistBeliefNuclearCG01(self):\r\n        with self.cached_session():\r\n            db_grad, db_out = self._dbParamsNuclearCG01()\r\n            num_samples = len(db_grad)\r\n            var0 = tf.Variable([0.0] * num_samples)\r\n            grads0 = tf.constant([0.0] * num_samples)\r\n            ord = \"nuclear\"\r\n            cg_opt = cg_lib.ConditionalGradient(learning_rate=0.1, lambda_=0.1, ord=ord)\r\n            if not tf.executing_eagerly():\r\n                cg_update = cg_opt.apply_gradients(zip([grads0], [var0]))\r\n                self.evaluate(tf.compat.v1.global_variables_initializer())\r\n\r\n            for i in range(num_samples):\r\n                if tf.executing_eagerly():\r\n                    grads0 = tf.constant(db_grad[i])\r\n                    cg_opt.apply_gradients(zip([grads0], [var0]))\r\n                else:\r\n                    cg_update.run(feed_dict={grads0: db_grad[i]})\r\n                self.assertAllClose(np.array(db_out[i]), self.evaluate(var0))\r\n\r\n    def testSparseNuclear(self):\r\n        # TODO:\r\n        #       To address the issue #347.\r\n        for dtype in self._DtypesToTest(use_gpu=tf.test.is_gpu_available()):\r\n            with self.cached_session():\r\n                var0 = tf.Variable(tf.zeros([4, 2], dtype=dtype))\r\n                var1 = tf.Variable(tf.constant(1.0, dtype, [4, 2]))\r\n                grads0 = tf.IndexedSlices(\r\n                    tf.constant([[0.1, 0.1]], dtype=dtype),\r\n                    tf.constant([1]),\r\n                    tf.constant([4, 2]),\r\n                )\r\n                grads1 = tf.IndexedSlices(\r\n                    tf.constant([[0.01, 0.01], [0.01, 0.01]], dtype=dtype),\r\n                    tf.constant([2, 3]),\r\n                    tf.constant([4, 2]),\r\n                )\r\n                top_singular_vector0 = tf.constant(\r\n                    [[0.0, 0.0], [0.7071067, 0.7071067], [0.0, 0.0], [0.0, 0.0]],\r\n                    dtype=dtype,\r\n                )\r\n                top_singular_vector1 = tf.constant(\r\n                    [\r\n                        [-4.2146844e-08, -4.2146844e-08],\r\n                        [0.0000000e00, 0.0000000e00],\r\n                        [4.9999994e-01, 4.9999994e-01],\r\n                        [4.9999994e-01, 4.9999994e-01],\r\n                    ],\r\n                    dtype=dtype,\r\n                )\r\n                learning_rate = 0.1\r\n                lambda_ = 0.1\r\n                ord = \"nuclear\"\r\n                cg_opt = cg_lib.ConditionalGradient(\r\n                    learning_rate=learning_rate, lambda_=lambda_, ord=ord\r\n                )\r\n                cg_update = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\r\n\r\n                if not tf.executing_eagerly():\r\n                    self.evaluate(tf.compat.v1.global_variables_initializer())\r\n                    # Fetch params to validate initial values\r\n                    self.assertAllClose([0, 0], self.evaluate(var0)[0])\r\n                    self.assertAllClose([0, 0], self.evaluate(var0)[1])\r\n                    self.assertAllClose([1, 1], self.evaluate(var1)[2])\r\n                # Check we have slots\r\n                self.assertEqual([\"conditional_gradient\"], cg_opt.get_slot_names())\r\n                slot0 = cg_opt.get_slot(var0, \"conditional_gradient\")\r\n                self.assertEquals(slot0.get_shape(), var0.get_shape())\r\n                slot1 = cg_opt.get_slot(var1, \"conditional_gradient\")\r\n                self.assertEquals(slot1.get_shape(), var1.get_shape())\r\n                if not tf.executing_eagerly():\r\n                    self.assertFalse(slot0 in tf.compat.v1.trainable_variables())\r\n                    self.assertFalse(slot1 in tf.compat.v1.trainable_variables())\r\n\r\n                # Step 1:\r\n                if not tf.executing_eagerly():\r\n                    self.evaluate(cg_update)\r\n                # Check that the parameters have been updated.\r\n\r\n                top_singular_vector0 = self.evaluate(top_singular_vector0)\r\n                top_singular_vector1 = self.evaluate(top_singular_vector1)\r\n\r\n                self.assertAllCloseAccordingToType(\r\n                    np.array(\r\n                        [\r\n                            0\r\n                            - (1 - learning_rate)\r\n                            * lambda_\r\n                            * top_singular_vector0[0][0],\r\n                            0\r\n                            - (1 - learning_rate)\r\n                            * lambda_\r\n                            * top_singular_vector0[0][1],\r\n                        ]\r\n                    ),\r\n                    self.evaluate(var0)[0],\r\n                )\r\n                self.assertAllCloseAccordingToType(\r\n                    np.array(\r\n                        [\r\n                            0\r\n                            - (1 - learning_rate)\r\n                            * lambda_\r\n                            * top_singular_vector0[1][0],\r\n                            0\r\n                            - (1 - learning_rate)\r\n                            * lambda_\r\n                            * top_singular_vector0[1][1],\r\n                        ]\r\n                    ),\r\n                    self.evaluate(var0)[1],\r\n                )\r\n                self.assertAllCloseAccordingToType(\r\n                    np.array(\r\n                        [\r\n                            1.0 * learning_rate\r\n                            - (1 - learning_rate)\r\n                            * lambda_\r\n                            * top_singular_vector1[2][0],\r\n                            1.0 * learning_rate\r\n                            - (1 - learning_rate)\r\n                            * lambda_\r\n                            * top_singular_vector1[2][1],\r\n                        ]\r\n                    ),\r\n                    self.evaluate(var1)[2],\r\n                )\r\n                # Step 2: the conditional_gradient contain the\r\n                # previous update.\r\n                if tf.executing_eagerly():\r\n                    cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\r\n                else:\r\n                    self.evaluate(cg_update)\r\n                # Check that the parameters have been updated.\r\n                self.assertAllClose(np.array([0, 0]), self.evaluate(var0)[0])\r\n                self.assertAllCloseAccordingToType(\r\n                    np.array(\r\n                        [\r\n                            (\r\n                                0\r\n                                - (1 - learning_rate)\r\n                                * lambda_\r\n                                * top_singular_vector0[1][0]\r\n                            )\r\n                            * learning_rate\r\n                            - (1 - learning_rate)\r\n                            * lambda_\r\n                            * top_singular_vector0[1][0],\r\n                            (\r\n                                0\r\n                                - (1 - learning_rate)\r\n                                * lambda_\r\n                                * top_singular_vector0[1][1]\r\n                            )\r\n                            * learning_rate\r\n                            - (1 - learning_rate)\r\n                            * lambda_\r\n                            * top_singular_vector0[1][1],\r\n                        ]\r\n                    ),\r\n                    self.evaluate(var0)[1],\r\n                )\r\n                self.assertAllCloseAccordingToType(\r\n                    np.array(\r\n                        [\r\n                            (\r\n                                1.0 * learning_rate\r\n                                - (1 - learning_rate)\r\n                                * lambda_\r\n                                * top_singular_vector1[2][0]\r\n                            )\r\n                            * learning_rate\r\n                            - (1 - learning_rate)\r\n                            * lambda_\r\n                            * top_singular_vector1[2][0],\r\n                            (\r\n                                1.0 * learning_rate\r\n                                - (1 - learning_rate)\r\n                                * lambda_\r\n                                * top_singular_vector1[2][1]\r\n                            )\r\n                            * learning_rate\r\n                            - (1 - learning_rate)\r\n                            * lambda_\r\n                            * top_singular_vector1[2][1],\r\n                        ]\r\n                    ),\r\n                    self.evaluate(var1)[2],\r\n                )\r\n\r\n    def testSharingNuclear(self):\r\n        for dtype in [tf.half, tf.float32, tf.float64]:\r\n            with self.cached_session():\r\n                var0 = tf.Variable([1.0, 2.0], dtype=dtype)\r\n                var1 = tf.Variable([3.0, 4.0], dtype=dtype)\r\n                grads0 = tf.constant([0.1, 0.1], dtype=dtype)\r\n                grads1 = tf.constant([0.01, 0.01], dtype=dtype)\r\n                top_singular_vector0 = self.top_singular_vector(grads0)\r\n                top_singular_vector1 = self.top_singular_vector(grads1)\r\n                learning_rate = 0.1\r\n                lambda_ = 0.1\r\n                ord = \"nuclear\"\r\n                cg_opt = cg_lib.ConditionalGradient(\r\n                    learning_rate=learning_rate, lambda_=lambda_, ord=ord\r\n                )\r\n                cg_update1 = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\r\n                cg_update2 = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\r\n                if not tf.executing_eagerly():\r\n                    self.evaluate(tf.compat.v1.global_variables_initializer())\r\n                    # Fetch params to validate initial values\r\n                    self.assertAllClose([1.0, 2.0], self.evaluate(var0))\r\n                    self.assertAllClose([3.0, 4.0], self.evaluate(var1))\r\n\r\n                # Check we have slots\r\n                self.assertEqual([\"conditional_gradient\"], cg_opt.get_slot_names())\r\n                slot0 = cg_opt.get_slot(var0, \"conditional_gradient\")\r\n                self.assertEquals(slot0.get_shape(), var0.get_shape())\r\n                slot1 = cg_opt.get_slot(var1, \"conditional_gradient\")\r\n                self.assertEquals(slot1.get_shape(), var1.get_shape())\r\n\r\n                if not tf.executing_eagerly():\r\n                    self.assertFalse(slot0 in tf.compat.v1.trainable_variables())\r\n                    self.assertFalse(slot1 in tf.compat.v1.trainable_variables())\r\n                # Because in the eager mode, as we declare two cg_update\r\n                # variables, it already altomatically finish executing them.\r\n                # Thus, we cannot test the param value at this time for\r\n                # eager mode. We can only test the final value of param\r\n                # after the second execution.\r\n                if not tf.executing_eagerly():\r\n                    self.evaluate(cg_update1)\r\n                    # Check that the parameters have been updated.\r\n                    top_singular_vector0 = self.evaluate(top_singular_vector0)\r\n                    top_singular_vector1 = self.evaluate(top_singular_vector1)\r\n                    self.assertAllCloseAccordingToType(\r\n                        np.array(\r\n                            [\r\n                                1.0 * learning_rate\r\n                                - (1 - learning_rate)\r\n                                * lambda_\r\n                                * top_singular_vector0[0],\r\n                                2.0 * learning_rate\r\n                                - (1 - learning_rate)\r\n                                * lambda_\r\n                                * top_singular_vector0[1],\r\n                            ]\r\n                        ),\r\n                        self.evaluate(var0),\r\n                    )\r\n                    self.assertAllCloseAccordingToType(\r\n                        np.array(\r\n                            [\r\n                                3.0 * learning_rate\r\n                                - (1 - learning_rate)\r\n                                * lambda_\r\n                                * top_singular_vector1[0],\r\n                                4.0 * learning_rate\r\n                                - (1 - learning_rate)\r\n                                * lambda_\r\n                                * top_singular_vector1[1],\r\n                            ]\r\n                        ),\r\n                        self.evaluate(var1),\r\n                    )\r\n\r\n                # Step 2: the second conditional_gradient contain\r\n                # the previous update.\r\n                if not tf.executing_eagerly():\r\n                    self.evaluate(cg_update2)\r\n                # Check that the parameters have been updated.\r\n                self.assertAllCloseAccordingToType(\r\n                    np.array(\r\n                        [\r\n                            (\r\n                                1.0 * learning_rate\r\n                                - (1 - learning_rate)\r\n                                * lambda_\r\n                                * top_singular_vector0[0]\r\n                            )\r\n                            * learning_rate\r\n                            - (1 - learning_rate) * lambda_ * top_singular_vector0[0],\r\n                            (\r\n                                2.0 * learning_rate\r\n                                - (1 - learning_rate)\r\n                                * lambda_\r\n                                * top_singular_vector0[1]\r\n                            )\r\n                            * learning_rate\r\n                            - (1 - learning_rate) * lambda_ * top_singular_vector0[1],\r\n                        ]\r\n                    ),\r\n                    self.evaluate(var0),\r\n                )\r\n                self.assertAllCloseAccordingToType(\r\n                    np.array(\r\n                        [\r\n                            (\r\n                                3.0 * learning_rate\r\n                                - (1 - learning_rate)\r\n                                * lambda_\r\n                                * top_singular_vector1[0]\r\n                            )\r\n                            * learning_rate\r\n                            - (1 - learning_rate) * lambda_ * top_singular_vector1[0],\r\n                            (\r\n                                4.0 * learning_rate\r\n                                - (1 - learning_rate)\r\n                                * lambda_\r\n                                * top_singular_vector1[1]\r\n                            )\r\n                            * learning_rate\r\n                            - (1 - learning_rate) * lambda_ * top_singular_vector1[1],\r\n                        ]\r\n                    ),\r\n                    self.evaluate(var1),\r\n                )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tf.test.main()\r\n```\r\n\r\nThe code is from the following page:\r\n[https://github.com/pkan2/addons/blob/master/tensorflow_addons/optimizers/conditional_gradient.py](url)\r\n[https://github.com/pkan2/addons/blob/master/tensorflow_addons/optimizers/conditional_gradient_test.py](url)\r\n\r\nAnd the error message and system info is from here:\r\n[https://github.com/pkan2/addons/runs/446766976?check_suite_focus=true](url)\r\n\r\nThank you!", "@pkan2 Is this still an issue for you? Can you please test with recent `tf-nightly` and share a simple standalone code to reproduce the issue. It is difficult to follow long codes. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36764\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36764\">No</a>\n"]}, {"number": 36763, "title": "Missing dependencies in image_classification android example for instrumented test ClassifierTest", "body": "**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nIn the tensorflow lite image classification example for android; gradle dependencies for the instrumented test have to be added\r\n \r\nhttps://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/android/app/build.gradle\r\n\r\nFix:\r\nneeds to be added in the dependencies section: \r\n\r\n    androidTestImplementation 'androidx.test.ext:junit:1.1.1'\r\n    androidTestImplementation 'com.android.support.test:rules:1.0.2'\r\n    androidTestImplementation 'com.google.truth:truth:1.0.1'\r\n\r\n\r\n ", "comments": ["The fix is given...", "Thank you for your comment! We have fixed this. :-)", "https://github.com/tensorflow/examples/commit/92a4dcd9e5db3cb6755f19d9d9450bb07b2f90b5\r\n\r\nThanks a lot for the feedback!"]}, {"number": 36762, "title": "[ROCm] Updating ROCm CI scripts to include large tests", "body": "This PR updates the ROCm CI scripts to include `large` tests.\r\n\r\nOn the ROCm platform, the CI scripts run tests with `sharding` disabled, in order to prevent the same test from running more than one subtest concurrently on the same GPU . Note that tests are still run in parallel (with number of tests in parellel == the number of GPUs available on the testing machine), just not on individual GPUs.\r\n\r\nBecause `sharding` is disabled (and all subtests within a given test execute serially on the GPU), some tests that have a lot of subtests, take much longer to run with the `sharding` disabled. Some of these tests were getting timed out, and need to re-classified as `large` (to raise their timeout limit). \r\n\r\nThere does not seem to be a way to conditionally specify the value of the `size` arg (i.e. `large` for ROCm, but `medium` for everybody else), and hence the change to `large` is unconditional.\r\n\r\n------\r\n\r\n/cc @whchung @chsigg @nvining-work ", "comments": ["re-based PR to resolve merge-conflicts", "gentle ping", "gentle ping", "@deven-amd Can you please resolve conflicts? Thanks!", "@gbaned done\r\n\r\n@cheshire please re-approve.\r\n\r\nthanks again", "We cannot accept converting these tests to `large` as then they won't run in internal CI", "@mihaimaruseac is there a way to make them conditionally large (i.e. large only for --config=rocm? )", "I don't think so. We'll need to ask for support from Bazel", "Closing out this PR for now. Maybe revisit it sometime in the future once bazel has the requisite support."]}, {"number": 36761, "title": "[INTEL MKL] Updating QuantizeV2 and Dequantize Ops with API changes in MKLDNN 1.0", "body": "This PR updates QuantizeV2 and Dequantize Ops for API changes in MKLDNN 1.0.", "comments": ["@penpornk Is there any issue with  \r\n\r\n> import/copybara \u2014 An error happened while migrating the change", "@mdfaijul Yeah, something seems wrong. Let me rerun the tests again to re-trigger import/copybara.", "@penpornk Thanks for rerunning. Let me know if there is any issue from our side."]}, {"number": 36760, "title": "[INTEL MKL] Upgrading RequantizePerChannel Op with API changes in MKLDNN 1.0", "body": "This PR ports RequantizePerChannel operator to MKLDNN 1.0", "comments": ["@mdfaijul do you want to add test cases around new changes ?", "@penpornk Thanks! Have changed it. Please take a look."]}, {"number": 36759, "title": "posenet negative output scores", "body": "@tensorflow/micro\r\n\r\n\r\n**Describe the problem**\r\nHello,\r\n\r\nI'm working on a C++ implementation for the posenet model. (using multi_person_mobilenet_v1_075_float.tflite).\r\n\r\nI can't seem to find much documentation on how to interpret the outputs of the model, but I did come across [this medium post](https://medium.com/@prajwalsingh_48273/posenet-for-android-8b6dede9fa2f). My outputs adhere to the dimensions that the article outlines, however, I'm getting negative scores.\r\n\r\nI've also tried with the starter model file from the [Tensorflow Pose estimation page](https://www.tensorflow.org/lite/models/pose_estimation/overview) with similar negative scores.\r\n\r\nMy question is regarding the range of values I can expect for each of these outputs (score, offsets, displacements). \r\n\r\nProbably a better question to ask: Is there updated models and comprehensive documentation somewhere for any pretrained posenet model?", "comments": ["Is there any update/comments on this?", "What device would the model be running on? (android/ios/microcontrollers)?", "Microcontroller - but right now it's just running in a docker environment, with the code cross compiled for ARM64", "Sorry - accidentally closed this ticket. ", "^ Bump - \r\nAny potential solutions? Relatively blocked on this one", "Hi @aselva-eb ! We are checking to see if you still need help in this issue ,Have you visited this two threads yet ? [Link1](https://docs.openvinotoolkit.org/2020.4/omz_demos_human_pose_estimation_demo_README.html),[Link2 ](https://github.com/Qengineering/TensorFlow_Lite_Pose_RPi_32-bits),[Link3](https://www.tensorflow.org/lite/examples/pose_estimation/overview). Please post on [discuss](https://discuss.tensorflow.org/) to proceed further as there is a larger community there to help you out.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36759\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36759\">No</a>\n"]}, {"number": 36758, "title": "Tensorflow-GPU 2.0 Docker Python script is just returning a message without executing", "body": "I installed \"tensorflow:2.0.1-gpu-py3\" docker image on my Ubuntu 18.04 with nvidia-driver-435 and mx110 video card. It works and even runs the following code after few minutes of initialization\r\n`python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"`\r\nbut when I'm trying to execute this file \r\n\r\nhttps://github.com/vadimen/algorithms/blob/master/code.py\r\n\r\nit just returns \r\n\r\n`python3 ssd_mobilenet_tf_for_video.py \r\n2020-02-14 17:56:28.801358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-02-14 17:56:28.803092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-14 17:56:28.803374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce MX110 major: 5 minor: 0 memoryClockRate(GHz): 1.006\r\npciBusID: 0000:01:00.0\r\n2020-02-14 17:56:28.803547: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-02-14 17:56:28.804471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-02-14 17:56:28.805256: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-02-14 17:56:28.805476: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-02-14 17:56:28.806716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-02-14 17:56:28.807675: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-02-14 17:56:28.810783: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-14 17:56:28.810947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-14 17:56:28.811578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-14 17:56:28.812142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-02-14 17:56:30.518085: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-02-14 17:56:30.540075: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1800000000 Hz\r\n2020-02-14 17:56:30.540367: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4f31d60 executing computations on platform Host. Devices:\r\n2020-02-14 17:56:30.540380: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2020-02-14 17:56:30.577626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-14 17:56:30.577961: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4eda270 executing computations on platform CUDA. Devices:\r\n2020-02-14 17:56:30.577977: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce MX110, Compute Capability 5.0\r\n2020-02-14 17:56:30.578121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-14 17:56:30.578356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce MX110 major: 5 minor: 0 memoryClockRate(GHz): 1.006\r\npciBusID: 0000:01:00.0\r\n2020-02-14 17:56:30.578399: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-02-14 17:56:30.578417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-02-14 17:56:30.578431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-02-14 17:56:30.578446: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-02-14 17:56:30.578462: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-02-14 17:56:30.578477: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-02-14 17:56:30.578489: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-14 17:56:30.578537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-14 17:56:30.578769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-14 17:56:30.578972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-02-14 17:56:30.579001: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-02-14 17:56:30.579466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-14 17:56:30.579477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2020-02-14 17:56:30.579483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2020-02-14 17:56:30.579558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-14 17:56:30.579815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-14 17:56:30.580074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1535 MB memory) -> physical GPU (device: 0, name: GeForce MX110, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n{'detection_classes': TensorShape([None, 100]), 'num_detections': TensorShape([None]), 'detection_boxes': TensorShape([None, 100, 4]), 'detection_scores': TensorShape([None, 100])}\r\n`\r\n", "comments": ["@vadimen Can you plese check using the latest version of tensorflow-gpu docker image and let me know if it works. Thanks!", "@gowthamkpr I installed tensorflow/tensorflow:nightly-gpu-py3 image released 8 hours ago and everything did work. I couldn't get GUI output from opencv but it's clear why, at least I got console output from python which told me that my code is executed 5 times slower than on CPU on host machine. I think it's because of not enough memory, my mx110 card gas 2GB of memory but output says it's all used \r\n\r\n2020-02-20 10:48:00.273011: W tensorflow/core/common_runtime/bfc_allocator.cc:245] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n\r\nwhy is all memory being used ? \r\nanyway the initial issue seems to be solved. Thank you so much for that.\r\n\r\nI tried to limit memory with this but it's still using everything.\r\n```\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\n\r\ntf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\r\n\r\n```\r\n\r\n\r\n", "@vadimen I am glad that our 1st issue has been resolved. Please create an other issue as this issue has been resolved . Thanks!"]}, {"number": 36757, "title": "Add more information about eager/graph context for Keras layer.call()", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#call\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThis is a key function that user will implement with they custom layer. Currently it is poorly documented, especially w.r.t the execution context of eager/graph. In TF 2.0, we are advocating eager execution by default. However, the call() body in keras is executed with graph context by default unless configured otherwise. It will raise error if user try to add print/debug related to items into the call body, eg print(eager_tensor.numpy()), etc.\r\n\r\nSome related question raised in  https://github.com/tensorflow/tensorflow/issues/27519.\r\n\r\n### Correct links\r\n\r\nYes\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nYes\r\n\r\n### Usage example\r\n\r\nNo\r\n\r\n### Request visuals, if applicable\r\n\r\nNo\r\n\r\n### Submit a pull request?\r\nNo\r\n", "comments": ["Just FYI, keras-team is working on some documents that clearly defines the API contract for all the important interfaces for Keras, and we will update the docs related very soon.", "I think #37574 only cover part of the functionnality for call(), and the major part for eager/graph context is not covered yet.", "Any pointers on the proper way to enable eager execution on layer.call()? I hit this issue converting a model that uses a custom layer to tflite. The conversion fails since the custom layer call function is not being executed eagerly (tf.executing_eagerly() is returning False). I have been looking around on the documentation, and just can't find how to force my custom layer to use eager mode...", "@adelcast, to run the layer.call() body in eager context, you need to invoke model.compile(run_eagerly=True). Note that this will have performance slow down due to eager runtime, and should only used for debug purpose.", "@qlzh727 Do we want to close this or are we waiting for a contribution?", "Though the TF webpage doesn't show the recent updates, I think this was resolved in the `master` branch. The description is much more clear now. I added a PR to add description about running in eager mode as described in https://github.com/tensorflow/tensorflow/issues/36757#issuecomment-609914819. Thanks", "@qlzh727 Can we add this as part of FAQs in Keras website? Thanks\r\n\r\nThere is a related question in FAQs https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call ", "Sure.", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! "]}, {"number": 36756, "title": "Update rmsprop.py API docs for TF 2.x", "body": "Hello and TGIF. This PR updates the RMSProp API docs for TF 2.x:\r\n\r\n- Added a new description: sourced from Alex Graves's 2013 [paper](https://arxiv.org/abs/1308.0850) and Hinton's 2012 [lecture](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf). The latter is said to be the first public mention of the optimizer. The description is pretty much word for word, hope that's ok. Feel free to modify it.\r\n- Modified the reference using Alex Graves' reference style from the 2013 [paper](https://arxiv.org/pdf/1308.0850.pdf) that followed the first \"unpublished\" introduction of RMSProp (a.k.a. _rmsprop_) (`T. Tieleman and G. Hinton. Lecture 6.5 - rmsprop:...`).\r\n- Other minor fixes (indentation, colon, etc).\r\n\r\nLet me know if that looks good @lamberta and team. I'm combing through another platform's optimizer API docs, so will be sending you more PRs if I spot anything here\u2014hope that's ok.", "comments": ["@8bitmp3 Can you please resolve conflicts? Thanks!", "@8bitmp3 can you please check build failures and resolve conflicts ?", "@rthadur @tanzhenyu @gbaned The changes to the docs were made on Feb 14th and they didn't touch the Python code. So, the the internal build failures may be caused by the fact when you started reviewing the PR in April, the original file already went through some distinct changes, as @fchollet already introduced new improvements to the docstring  - see https://github.com/tensorflow/tensorflow/commit/f6302e4ec726225036b3419af7d218e93aa06783 (especially, the new detailed args section). If Francois doesn't mind, I can submit new a PR with a small addition of references at the end and close this PR.", "@8bitmp3 we can close this PR, if there are new doc changes please open a new PR."]}, {"number": 36755, "title": "Unify the code for GpuCompiler::OptimizeHloPostLayoutAssignment for AMD and NVidia in XLA", "body": "The code in `GpuCompiler::OptimizeHloPostLayoutAssignment` subclasses is essentially duplicated for AMD and NVidia in XLA. This already leads to subtle bugs: TreeReductionRewriterPass is applied to NVidia, but not to AMD.\r\n\r\n Would it be possible to unify those, put the code in `gpu_compiler.cc`, and just check the platform to dynamically choose whether to apply NVidia-specific or AMD-specific passes?", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36755\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36755\">No</a>\n"]}, {"number": 36754, "title": "[MLIR][XLA] HLO to LHLO buffer assignment using live variable analysis", "body": "This PR adds blocks signature conversion and buffer allocation using live variable analysis.", "comments": ["@dfki-ehna Can you please address the pifon2a's comments and resolve conflicts? Thanks!"]}, {"number": 36753, "title": "[XLA] Fix a latent bug. ", "body": "Currently I think the code is fine, but the function doesn't do what its name say.\r\n\r\n@cheshire ", "comments": ["Now this is obviously a bug, but have you checked the usages? Do they actually need a block ID zero? Any ideas on why it was working before?", "I checked the usage rapidly:\r\n\r\n```\r\n$ git grep IsBlock0Thread0\r\ntensorflow/compiler/xla/service/gpu/ir_emission_utils.cc:llvm::Value* IsBlock0Thread0(llvm::IRBuilder<>* b) {\r\ntensorflow/compiler/xla/service/gpu/ir_emission_utils.h:llvm::Value* IsBlock0Thread0(llvm::IRBuilder<>* b);\r\ntensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:  KernelSupportLibrary{&b_}.If(\"emit_mof_tuple\", IsBlock0Thread0(&b_), [&] {\r\ntensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:    KernelSupportLibrary{&b_}.If(\"emit_mof_tuple\", IsBlock0Thread0(&b_), [&] {\r\n```\r\n\r\nIt is only used to write the Tuple ptr at the start of the kernel from the comment above its usage:\r\n\r\n```\r\n // Emit the tuple pointers in one thread.  We could do this at any point in\r\n  // the kernel, but we do it at the beginning in the hopes of reducing register\r\n  // pressure, since we touch threadIdx.x and blockIdx.x at the beginning of the\r\n  // kernel *anyway*.\r\n  std::vector<IrArray> output_arrays = ConstructIrArrayForOutputs(hlo);\r\n  KernelSupportLibrary{&b_}.If(\"emit_mof_tuple\", IsBlock0Thread0(&b_), [&] {\r\n    llvm_ir::EmitTuple(GetIrArray(hlo, hlo), output_arrays, &b_);\r\n  });\r\n```\r\n\r\nand the second place\r\n```\r\n  // For multioutput fusion, one thread needs to output a tuple\r\n  // with pointers to all the individual outputs.  We could do this\r\n  // at any point in the kernel, but we do it at the beginning in\r\n  // the hopes of reducing register pressure, since we touch\r\n  // threadIdx.x and blockIdx.x at the beginning of the kernel\r\n  // *anyway*.\r\n  if (hlo->IsMultiOutputFusion()) {\r\n    KernelSupportLibrary{&b_}.If(\"emit_mof_tuple\", IsBlock0Thread0(&b_), [&] {\r\n      llvm_ir::EmitTuple(GetIrArray(*hlo, *hlo),\r\n                         ConstructIrArrayForOutputs(*hlo), &b_);\r\n    });\r\n  }\r\n```\r\n\r\nSo I think it just cause more threads then needed to write the same data to the same destination.\r\n\r\nSo I think in the current code, it didn't cause problems, except probably a very small slowdown that we probably can't mesure."]}, {"number": 36752, "title": "[XLA] Speed up row reduction on P100/V100 for float16 and [u]int8", "body": "This PR make the unroll factor per thread dependent of the dtype and the GPU generation.\r\nIdem for the number of threads per blocks.\r\n\r\nThis give me an 1.82x speed up for float16 and 3x for [u]int8 on V100.\r\nThis give me an 1.33x speed up for float16 and 2.39x for [u]int8 on P100.\r\n\r\nUPDATE: this PR only contain the change about the unroll factor. The blocks size changes will be moved to another PR.", "comments": ["I redid the timing. So this PR is ready to merge from my point of view.", "I just force pushed to this PR to only kept the unroll factor changes as requested.", "Let's have a separate PR for changes to num_threads_x, and discuss the trade-offs there.", "I'm looking into why do we get Windows failures, seems unrelated."]}, {"number": 36751, "title": "Allocator (GPU_0_bfc) ran out of memory trying to allocate 32.0KiB (rounded to 32768)", "body": "**System information**\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): -  Windows 10 1909  \r\nTensorFlow version (use command below): -  2.1.0 - gpu version  \r\nPython version: -  3.6.9  \r\nCUDA/cuDNN version: -  10.1.243  \r\nGPU model and memory:  GTX 1660 Ti 6GB and 32GB (28GB available)  \r\n\r\n**Describe the current behavior**  \r\nFirstly it is some weighted model which I used for google-quest-challenge and since the contest date is out for submission I ran kaggle kernel not normal but with GPU. And then I got score, want to increase by chaning the model and few things and when I ran this in my computer, surprisingly I am getting this error. Please somebody help me how to rectify this error. I need it and I am so desperate. Because I work all day long with this computer on machine learning models. So, now that the error occurred I cannot do it.  \r\n\r\n![1](https://user-images.githubusercontent.com/44919399/74539156-921dbb00-4f63-11ea-955d-2e46e171387d.jpg)\r\n![2](https://user-images.githubusercontent.com/44919399/74539180-9944c900-4f63-11ea-9c08-6271c8f0e60e.jpg)\r\n\r\n**Code to reproduce the issue** \r\n[google-quest-challenge.txt](https://github.com/tensorflow/tensorflow/files/4204830/google-quest-challenge.txt)\r\n[data.zip](https://github.com/tensorflow/tensorflow/files/4204848/data.zip)\r\n\r\n\r\n**Other info / logs**  \r\nI uploaded the code as text file and datasets as zip file. Please find it and response as soon as possible. I am very desperate.\r\n\r\n", "comments": ["ran-out-of-memory occurs because your model on training data couldn't run and fit in memory your GPU have. Most of the time the default gpu memory allocates (approx. 90% of total). So you can manually set gpu limit to a fraction say 0.95 which uses 95% likewise. And the other is, you definitely have to decrease batch_size first and then reduce parameters in model.layers or remove few layers. Thank You!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36751\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36751\">No</a>\n"]}, {"number": 36750, "title": "Loading the in-memory loaded saved model ", "body": "Hi all,\r\nCan someone tell me a way to load an in-memory loaded tensorflow model (a protobuf file) into Tensorflow?\r\n\r\nFor example, the current code I have is as follows: \r\n```\r\nmodel_path = 'path/to/the/model/'\r\nsession = tf.Session(graph=tf.Graph())\r\ntf.saved_model.loader.load(session, ['serve'], model_path)\r\n```\r\n\r\nHowever, instead of loading the model by providing its path, I would like to load a model by providing a bytebuffer of the loaded model file.\r\n\r\nCan anyone tell me how I can do this? Thanks!", "comments": ["@harshithdwivedi, Please find the [doc](https://www.tensorflow.org/tutorials/keras/save_and_load) for Tensorflow save and load model. Thanks!", "I can't find the answer to my issue in the doc that you linked @gadagashwini \r\n", "@harshithdwivedi What version of tensorflow are you using?", "In 1.x we have import_graph_def: https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/graph_util/import_graph_def\r\n\r\nThat doesn't work with variables, since we don't put variable values in protobufs. (Although you could load the variables yourself.)\r\n\r\nIn 2.x I'd just keep the SavedModel on an in-memory filesystem and use the file-based APIs. ", "Hi @allenlavoie , that sounds like a good idea.\r\nCan you point me towards any docs or code examples that let load the an in-memory model in 2.x?\r\n\r\nThanks again!", "I think `/run/shm/` is in-memory on most Linux systems. This works on my machine:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\npath = \"/run/shm/saved_model/\"\r\n\r\nm = tf.Module()\r\nm.v = tf.Variable(1.)\r\ntf.saved_model.save(m, path)\r\n\r\nimported = tf.saved_model.load(path)\r\nassert imported.v == 1.\r\n```\r\n\r\nBut on any operating system you should be able to configure one somehow, and once you do that you can just feed the path to TensorFlow.", "This is great, thanks!\r\nI'll try it out and let you know.\r\ncc: @aayusharora"]}, {"number": 36749, "title": "[tflite] add xnnpack delegate to label_image", "body": "The [XNNPACK Delegate](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack) uses [XNNPACK](https://github.com/google/XNNPACK), a fairly optimized floating point library, to run some inference operators (see the delegate's [README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md) for currently supported ops). With XNNPACK, I was able to get performance numbers similar to what @Maratyszcza described at [XNNPACK's readme](https://github.com/google/XNNPACK/README.md). I also got good numbers on Pixel 4 and Oppo Reno 3. Numbers on x86 machines are also good.\r\n\r\n- **Single-threaded**, `label_image -m MODEL_NAME -x 1 -c 50 -t 1`\r\n\r\nModel |  Pixel 3a, ms | Pixel 4, ms| Oppo Reno 3, ms |\r\n-- | --: | --: | --:\r\nMobileNet v1 1.0X |  88.02 | 29.95 | 36.23 |\r\nMobileNet v2 1.0X |  55.13 | 19.13 | 21.69 |\r\nMobileNet v3 Large |  44.84 | 15.69 | 17.65 |\r\nMobileNet v3 Small |   14.23 | 5.58 | 5.66|\r\n\r\n- **multi-threaded**, the number of threads is set to be the number of big cores. `label_image -m MODEL_NAME -x 1 -c 50 -t NUMBER_OF_BIG_CORES`\r\n\r\nModel |  Pixel 3a, ms | Pixel 4, ms| Oppo Reno 3, ms|\r\n-- | --: | --: | --:\r\nMobileNet v1 1.0X |  46.00 | 10.60 | 13.23 |\r\nMobileNet v2 1.0X |  28.57 | 6.89 | 7.56 |\r\nMobileNet v3 Large |  24.62 | 6.52 | 6.88 |\r\nMobileNet v3 Small |   8.24 | 2.47 |  2.37 |\r\n\r\n", "comments": ["@freedomtan Can you please check srjoglekar246's comments and keep us posted? Thanks!", "@srjoglekar246  and @gbaned: thanks. I updated it.", "@gbaned Can you help get this in with the new change?", "I am not sure how it passed the presubmits, but it looks like the build on macos is now failing after this cl:\r\n\r\n```\r\nERROR: /Volumes/BuildData/tmpfs/tensor_flow/tensorflow/lite/tools/evaluation/stages/BUILD:171:1: Couldn't build file tensorflow/lite/tools/evaluation/stages/inference_profiler_stage_test: Linking of rule '//tensorflow/lite/tools/evaluation/stages:inference_profiler_stage_test' failed (Exit 1)\r\nld: warning: option -s is obsolete and being ignored\r\nUndefined symbols for architecture x86_64:\r\n  \"__cvtu32_mask16\", referenced from:\r\n      _xnn_f32_clamp_ukernel__avx512f in libavx512f_ukernels.a(avx512f_f5094ccab65aa097758ad595fc013aa6.o)\r\n      _xnn_f32_dwconv_ukernel_up16x25__avx512f in libavx512f_ukernels.a(up16x25-avx512f_858bc72ba4ab0c22a96b33f8c986da4d.o)\r\n      _xnn_f32_dwconv_ukernel_up16x4__avx512f in libavx512f_ukernels.a(up16x4-avx512f_bda6330d2e7849b699d64546d5554778.o)\r\n      _xnn_f32_dwconv_ukernel_up16x9__avx512f in libavx512f_ukernels.a(up16x9-avx512f_b7bac09f3bd6e9da99c3bd67f089f13b.o)\r\n      _xnn_f32_gemm_ukernel_1x16__avx512f_broadcast in libavx512f_ukernels.a(1x16-avx512f-broadcast_9b0d6ef6fa981981cc969d9d4c7172fe.o)\r\n      _xnn_f32_gemm_ukernel_7x16__avx512f_broadcast in libavx512f_ukernels.a(7x16-avx512f-broadcast_453776902dcf985bc6b1450a4b846b2e.o)\r\n      _xnn_f32_hswish_ukernel__avx512f_x32 in libavx512f_ukernels.a(avx512f-x32_f8deb97e68fad5860a602672bfd8cdf3.o)\r\n      ...\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\nI will revert this for now. we can then look into why the presubmits here did not catch the problem.", "@gunan How do I reproduce the error? I guess some recent XNNPACK related changes broke it. With my ` freedomtan:add_xnnpack_to_label_image` branch, I can build `tensorflow/lite/tools/evaluation/stages/inference_profiler_stage_test`, `tensorflow/lite/examples/label_image:label_image`, and others. It also works after rebasing my branch to recent 690c33e.", "@Maratyszcza FYI", "I suspect the issue is due to Apple Clang lacking support for `_cvtu32_mask16` AVX512 intrinsic. XNNPACK has a [polyfill for this case](https://github.com/google/XNNPACK/blob/4ea95bef8cdd942895f23f5cc09c778d10500551/src/xnnpack/intrinsics-polyfill.h#L15), but it is enabled only for Apple Clang pre-10, while it seems that Apple Clang 10 also lacks `_cvtu32_mask16`. Presumed fix landed in google/XNNPACK@b18783570f0643560be641b193367d3906955141"]}, {"number": 36748, "title": "Ubuntu 18.04 with RTX 2070 SUPER with tensorflow 1.13, Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERRO", "body": "**System information**\r\n- Have I written custom code : No\r\n- OS: Linux 5.3.0-28 18.04.1-Ubuntu\r\n- TensorFlow installed from pip3\r\n- TensorFlow version: tried 13.1 and 13.2\r\n- Python version: Python 3.6.9\r\n- Bazel: /\r\n- GCC/Compiler: gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n- CUDA/cuDNN version: CUDA 10.0, cuDNN 7.4.2\r\n- GPU model and memory: GeForce RTX 2070 SUPER, 8GB\r\n\r\n**Describe the current behavior**\r\nI downloaded the [MNIST \"tensorflow without a PhD example\"](https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd) from git and tried to run one of the examples with\r\n`python3 mnist_3.1_convolutional_bigger_dropout.py`\r\nafter downloading tensorflow and tensorflow gpu with\r\n`pip3 install tensorflow==1.13.2 tensorflow-gpu==1.13.2`.\r\n\r\n**Describe the expected behavior**\r\nI have not altered the code in any way and could successfully run it on the CPU. I assumed it would work without problems with the GPU, too, after I installed proprietary NVidia drivers (435) and the CUDA and cuDNN libraries.\r\nThere was a [workaround in a different issue relating to the same problem](https://github.com/tensorflow/tensorflow/issues/24496#issuecomment-464909727) but this did not fix the issue for me.\r\n\r\n**Code to reproduce the issue**\r\nJust clone the repository above and run the code from the \"tensorflow-mnist-tutorial\" directory.\r\n\r\n**Other info / logs**\r\nThis is the output:\r\n```\r\n$ python3 mnist_3.1_convolutional_bigger_dropout.py \r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nINFO:tensorflow:Tensorflow version 1.13.2\r\nTensorflow version 1.13.2\r\n2020-02-14 11:56:04.391977: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x16d9470 executing computations on platform CUDA. Devices:\r\n2020-02-14 11:56:04.392023: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2070 SUPER, Compute Capability 7.5\r\n2020-02-14 11:56:04.412696: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600105000 Hz\r\n2020-02-14 11:56:04.413472: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1718880 executing computations on platform Host. Devices:\r\n2020-02-14 11:56:04.413512: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2020-02-14 11:56:04.413749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce RTX 2070 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.815\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 7.79GiB freeMemory: 7.56GiB\r\n2020-02-14 11:56:04.413786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2020-02-14 11:56:04.414980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-14 11:56:04.415008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2020-02-14 11:56:04.415022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2020-02-14 11:56:04.415176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7350 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:03:00.0, compute capability: 7.5)\r\n2020-02-14 11:56:08.593018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2020-02-14 11:56:08.593064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-14 11:56:08.593075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2020-02-14 11:56:08.593087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2020-02-14 11:56:08.593176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7350 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:03:00.0, compute capability: 7.5)\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:From mnist_3.1_convolutional_bigger_dropout.py:88: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\nWARNING:tensorflow:From mnist_3.1_convolutional_bigger_dropout.py:95: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n\r\nFuture major versions of TensorFlow will allow gradients to flow\r\ninto the labels input on backprop by default.\r\n\r\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\r\n\r\n2020-02-14 11:56:10.165980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2020-02-14 11:56:10.166030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-14 11:56:10.166039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2020-02-14 11:56:10.166049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2020-02-14 11:56:10.166133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7350 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:03:00.0, compute capability: 7.5)\r\n2020-02-14 11:56:10.651240: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\n2020-02-14 11:56:11.534908: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-02-14 11:56:11.581923: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nException in Tkinter callback\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node Conv2D}}]]\r\n         [[{{node convert_image}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/tkinter/__init__.py\", line 1705, in __call__\r\n    return self.func(*args)\r\n  File \"/usr/lib/python3.6/tkinter/__init__.py\", line 749, in callit\r\n    func(*args)\r\n  File \"/usr/lib/python3/dist-packages/matplotlib/backends/backend_tkagg.py\", line 95, in _on_timer\r\n    TimerBase._on_timer(self)\r\n  File \"/usr/lib/python3/dist-packages/matplotlib/backend_bases.py\", line 1383, in _on_timer\r\n    ret = func(*args, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/matplotlib/animation.py\", line 1542, in _step\r\n    still_going = Animation._step(self, *args)\r\n  File \"/usr/lib/python3/dist-packages/matplotlib/animation.py\", line 1277, in _step\r\n    self._draw_next_frame(framedata, self._blit)\r\n  File \"/usr/lib/python3/dist-packages/matplotlib/animation.py\", line 1296, in _draw_next_frame\r\n    self._draw_frame(framedata)\r\n  File \"/usr/lib/python3/dist-packages/matplotlib/animation.py\", line 1814, in _draw_frame\r\n    self._drawn_artists = self._func(framedata, *self._args)\r\n  File \"/home/myuser/repository/tensorflow-without-a-phd/tensorflow-mnist-tutorial/tensorflowvisu.py\", line 364, in animate_step\r\n    compute_step(n, request_test_data_update, request_data_update)\r\n  File \"mnist_3.1_convolutional_bigger_dropout.py\", line 131, in training_step\r\n    feed_dict={X: batch_X, Y_: batch_Y, pkeep: 1.0, step: i})\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node Conv2D (defined at mnist_3.1_convolutional_bigger_dropout.py:78) ]]\r\n         [[node convert_image (defined at /home/myuser/repository/tensorflow-without-a-phd/tensorflow-mnist-tutorial/tensorflowvisu.py:62) ]]\r\n\r\nCaused by op 'Conv2D', defined at:\r\n  File \"mnist_3.1_convolutional_bigger_dropout.py\", line 78, in <module>\r\n    Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1026, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nUnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node Conv2D (defined at mnist_3.1_convolutional_bigger_dropout.py:78) ]]\r\n         [[node convert_image (defined at /home/myuser/repository/tensorflow-without-a-phd/tensorflow-mnist-tutorial/tensorflowvisu.py:62) ]]\r\n```\r\n\r\nBy the way, there was a helpful table showing which tensorflow version required what python version, gcc compiler, bazel version and cuDNN/CUDA version. What happened to that?\r\n\r\nCheers", "comments": ["I also now tested this with TensorFlow 1.4 and Cuda 10.0 and tried cuDNN 7.4.2 and cuDNN 7.6.5 and the same problem happens.\r\n\r\n[edit]\r\nI also ran `watch -n 1 'nvidia-smi'` and it seems that, for some reason, the memory usage reaches the maximum available memory of the card, which is strange.\r\n```\r\nEvery 1,0s: nvidia-smi                                                                                   ail-Test4: Tue Feb 18 10:32:06 2020\r\n\r\nTue Feb 18 10:32:06 2020\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 207...  Off  | 00000000:03:00.0  On |                  N/A |\r\n|  0%   29C    P2    43W / 215W |   7973MiB /  7977MiB |      2%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1071      G   /usr/lib/xorg/Xorg                            78MiB |\r\n|    0      1271      G   /usr/bin/gnome-shell                          69MiB |\r\n|    0      4885      C   python3                                     7813MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n```", "Alright, I think I found a working configuration. I installed tensorflow 1.11 and am using cuda 9.0 with cuDNN 7.4.2 and it seems to be working. I tried both the MNIST examples and the Superman vs Batman logo classificator mentioned above.\r\n\r\n[edit]\r\nThis problem still occurs sometimes, though. So far I cannot figure out why. Sometimes when that happens I get a segmentation fault.\r\nThe segmentation fault happens in the Batman vs Superman logo classifier I linked above, in predict.py. It just says\r\n```\r\n2020-02-18 15:26:39.896608: E tensorflow/stream_executor/cuda/cuda_dnn.cc:353] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nSegmentation fault (core dumped)\r\n```\r\nand that's it. [edit] And just to clarify: while training a model I can use it to validate a set of pictures to see what results they are giving me but when loading the model from disk and trying to do the same I get the segmentation fault.\r\n\r\n[edit]\r\nJust for testing I installed tensorflow 1.12 without changing the version of cuda and cuDNN and here I also get the CUDNN_STATUS_INTERNAL_ERROR that started this issue in the first place.", "Can you please try by killing all active python sessions/jupyter notebook.\r\nYou need exit() all python interpreters.\r\nStart new python instance and add following lines on top `mnist_3.1_convolutional_bigger_dropout.py ` script.\r\n```python\r\nimport tensorflow as tf\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n\r\n# rest of mnist_3.1_convolutional_bigger_dropout.py code\r\n```", "Hey, I actually had tried that before and it didn't work but I did the mistake of trying to put it into my code. When I put the code that you mentioned before ANYTHING else in my code it seems to work.\r\nCould you please tell me what it does and do you know what the underlying problem is?\r\n\r\nThank you very much!\r\n\r\n[edit]\r\nInterestingly enough adding that piece of code to my routines also gets rid of the segmentation fault with tensorflow 1.11 error that I have mentioned above, at least as far I can see.", "Glad it worked for you. By default, TensorFlow maps nearly all of the GPU memory. In some cases it is desirable for the process to only allocate a subset of the available memory, or to only grow the memory usage as is needed by the process. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36748\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36748\">No</a>\n"]}, {"number": 36747, "title": "Micro: Return error if tensor allocation fails.", "body": "", "comments": ["@jvistrand Can you please resolve conflicts? Thanks!", "Fixed. Also corrected some indentation on master.", "@jvistrand Can you please resolve conflicts? Thanks!"]}, {"number": 36746, "title": "Best Method To import Multiple Tensorflow Models in Inference", "body": "suppose i have 3 Models in Tensorflow each for a specific problem while inference with a web cam or in real time processing, my question is how can i import multiple models in one session with their frozen graph and labelpb files ? or what is the best method for doing running the 3 models at the same time ?\r\n\r\n", "comments": ["@Abdelrahmanalii you can create three different [graphs](https://www.tensorflow.org/api_docs/python/tf/Graph) and run them in one session. \r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 36745, "title": "Micro: Return error if tensor allocation fails.", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36745) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 36744, "title": "[INTEL MKL] DNN1.0 integration - concat op", "body": "DNN1.0 changes for the concat operator", "comments": ["@penpornk  Thanks for the review comments. I will address and submit the patch ASAP", "@penpornk  Address the review comments and pushed the changes. Please review", "@penpornk  Address the review comments. Please check"]}, {"number": 36743, "title": "Fix example speech_command: v2 compatibility", "body": "Update Python script `test_streaming_accuracy.py` to make it compatible with v2.", "comments": ["Thanks for doing this!"]}]