[{"number": 54282, "title": "TFLite model working in multi-processing but not multi-threading", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur (M1)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): binary (pip package)\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.12\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: M1 Chip\r\n\r\n**Describe the current behavior**\r\nTensorflow lite model doesn't work when using multi-threading using `multiprocessing.dummy.Pool` in Python, while it does work in multi-processing.\r\n\r\n**Describe the expected behavior**\r\nTensorflow Lite model should have worked in both.\r\n\r\n**Standalone code to reproduce the issue**\r\n(Sample TFLite model attached here: [model.tflite.zip](https://github.com/tensorflow/tensorflow/files/8008907/model.tflite.zip))\r\n\r\n```\r\nimport math\r\nimport numpy as np\r\nimport os\r\nimport tensorflow as tf\r\nimport time\r\n\r\n#use below one for multi-threading\r\nfrom multiprocessing.dummy import Pool \r\n\r\n#use below one for multi-processing\r\n#from multiprocessing import Pool \r\n\r\nfrom multiprocessing import cpu_count\r\nfrom tqdm import tqdm\r\n\r\ninterpreter = None\r\ninput_index = None\r\noutput_index = None\r\n\r\ndef init_interpreter(model_path):\r\n    global interpreter\r\n    global input_index\r\n    global output_index\r\n    interpreter = tf.lite.Interpreter(model_path=model_path)\r\n    input_index = interpreter.get_input_details()\r\n    output_index = interpreter.get_output_details()\r\n    interpreter.allocate_tensors()\r\n    print('done init')\r\n\r\ndef get_prediction(img_name):\r\n    img = np.random.randn(1, 256, 256, 3).astype(np.float32)\r\n    interpreter.set_tensor(input_index[0][\"index\"], img)\r\n    interpreter.invoke()\r\n    pred = interpreter.tensor(output_index[0][\"index\"])()[0][0]\r\n    return pred\r\n\r\ndef main():\r\n    samples = list(range(100))\r\n    model_path = 'model.tflite'\r\n    with Pool(processes=2, initializer=init_interpreter, initargs=(model_path,)) as pool:\r\n        preds = list(tqdm(pool.imap(get_prediction, samples)))\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\n\r\n**Other info / logs**  It gives the following error when using TFLite model with multi-threading\r\n```\r\nRuntimeError: There is at least 1 reference to internal data\r\n      in the interpreter in the form of a numpy array or slice. Be sure to\r\n      only hold the function returned from tensor() if you are using raw\r\n      data access.\r\n```\r\n\r\nThe problem that we are working on is memory sensitive and requires many models to be loaded into the memory and thus, we can't use multi-processing as it leads to the same model being loaded as many times as the number of processes. Is there any way to use TFLite models in multi-threading? Any help would be much appreciated. Thanks!", "comments": ["@rohanmishra21 Could you please try with the latest TF v2.8.0 and refer this [link](https://www.tensorflow.org/lite/performance/best_practices) ?Please let us know if it helps.\r\nThank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 54281, "title": "train_step method of custom model is not called in graph execution (i.e., non eager) mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, but I modified only a minor portion from the stock example.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ArchLinux.\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA.\r\n- TensorFlow installed from (source or binary): via `pip install tensorflow tensorflow-gpu keras`, probably from binary.\r\n- TensorFlow version (use command below): `v2.8.0-rc1-32-g3f878cff5b6 2.8.0`\r\n- Python version: `Python 3.9.7`\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: CUDA 11.5\r\n- GPU model and memory: NVIDIA GeForce GTX 1060 6GB.\r\n\r\n**Describe the current behavior**\r\nCustom model's `train_step` is not being used in non-eager execution mode.\r\n\r\n**Describe the expected behavior**\r\nCustom model's `train_step` is used regardless of whether eager execution is enabled or not.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport keras\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass CustomModel(keras.Model):\r\n\tdef train_step(self, data):\r\n\t\treturn {m.name: m.result() for m in self.metrics}\r\n\r\n\r\nif __name__ == '__main__':\r\n\t# config\r\n\t# tf.compat.v1.enable_eager_execution()\r\n\ttf.compat.v1.disable_eager_execution()\r\n\r\n\tprint(\"TensorFlow version: {}\".format(tf.__version__))\r\n\tprint(\"Eager execution: {}\".format(tf.executing_eagerly()))\r\n\r\n\t# Construct and compile an instance of CustomModel\r\n\tinputs = keras.Input(shape=(32,))\r\n\toutputs = keras.layers.Dense(1)(inputs)\r\n\tmodel = CustomModel(inputs, outputs)\r\n\tmodel.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\r\n\r\n\t# Just use `fit` as usual\r\n\tx = np.random.random((1000, 32))\r\n\ty = np.random.random((1000, 1))\r\n\r\n\tprint(model.evaluate(x, y))\r\n\tmodel.fit(x, y, epochs=3)\r\n\tprint(model.evaluate(x, y))\r\n```\r\n\r\n**Other info / logs**\r\n\r\nWhen eager execution is enabled, `train_step` gets called, which means the model **isn't** trained as **expected**.\r\n\r\n```\r\n32/32 [==============================] - 0s 1ms/step - loss: 0.3040 - mae: 0.4428\r\n[0.3039644658565521, 0.442813515663147]\r\nEpoch 1/3\r\n32/32 [==============================] - 0s 874us/step - loss: 0.0000e+00 - mae: 0.0000e+00\r\nEpoch 2/3\r\n32/32 [==============================] - 0s 810us/step - loss: 0.0000e+00 - mae: 0.0000e+00\r\nEpoch 3/3\r\n32/32 [==============================] - 0s 762us/step - loss: 0.0000e+00 - mae: 0.0000e+00\r\n32/32 [==============================] - 0s 1ms/step - loss: 0.3040 - mae: 0.4428\r\n[0.3039644658565521, 0.442813515663147]\r\n```\r\n\r\nWhen eager execution is disabled, `train_step` is ignored, and the model is trained normally and `train_step` is ignored.\r\nThis is **not expected**.\r\n\r\n```\r\n[0.26861959040164946, 0.41127136]\r\nTrain on 1000 samples\r\nEpoch 1/3\r\n1000/1000 [==============================] - 0s 71us/sample - loss: 0.2598 - mae: 0.4037\r\nEpoch 2/3\r\n1000/1000 [==============================] - 0s 40us/sample - loss: 0.2432 - mae: 0.3912\r\nEpoch 3/3\r\n1000/1000 [==============================] - 0s 37us/sample - loss: 0.2296 - mae: 0.3805\r\n[0.2220638926625252, 0.3743403]\r\n```\r\n\r\nrelated issues: #45922 #40880\r\n\r\nthe snippet is modified from stock example [here](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit/#a_first_simple_example).\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54281\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54281\">No</a>\n", "The culprit is here.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/993403945efc9e7a933e9c8bd3ad029c36c6882b/tensorflow/python/keras/utils/version_utils.py#L53\r\n\r\nThere are training.py and training_v1.py.\r\n\r\ntraining_v1 does not support custom `train_step`.\r\n\r\nThe new training.py is only used when executed eagerly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54281\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54281\">No</a>\n"]}, {"number": 54280, "title": "Unintuitive error in learning rate type when saving then loading weights after further fitting", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution: Ubuntu 20.04.3 LTS (or the colab-env)\r\n- TensorFlow installed from (source or binary): binary (through pip) (or the colab-env)\r\n- TensorFlow version: 2.7.0\r\n- Python version: 3.8.10 (or the colab-env)\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n\r\nThe following procedure causes a fairly unintuitive `tensorflow.python.framework.errors_impl.InvalidArgumentError`:\r\n\r\n1. Setup model\r\n2. Compile model\r\n3. Fit model with e.g. learning-rate scheduler callback (saving the history)\r\n4. Save weights of model\r\n5. Compile model using `learning_rate` from last epoch from previous fit (using saved history)\r\n6. Fit model\r\n7. Load the saved weights, which triggers `InvalidArgumentError `\r\n\r\nsee linked colab notebook for details.\r\n\r\nWhat happens is that the type for the learning-rate changes when compiling the model the second time, causing a mismatch and the saved learning-rate fails to load. This appears unintuitive since both types for the learning-rate are apparently valid (can be used to perform a fit), but you cannot load if you accidentally changed the type.\r\n\r\n**Describe the expected behavior**\r\n\r\nStep 7. in the above should not produce an error.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThis example code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nX, y = tf.random.uniform((50,)), tf.random.uniform((50,))\r\nX_val, y_val = tf.random.uniform((5,)), tf.random.uniform((5,))\r\n\r\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(schedule=lambda epoch,lr: lr*0.995, verbose=1)\r\n\r\nmodel = tf.keras.models.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\r\n\r\nmodel.compile(optimizer=tf.keras.optimizers.SGD(), loss='mse')\r\n\r\nhistory = model.fit(X, y, epochs=1, validation_data=(X_val,y_val), callbacks=[lr_scheduler])\r\n\r\nmodel.save_weights('tmp_model')\r\n\r\nmodel.compile(\r\n    optimizer=tf.keras.optimizers.SGD(learning_rate=history.history['lr'][-1]*0.1),\r\n    #optimizer=tf.keras.optimizers.SGD(learning_rate=float(history.history['lr'][-1]*0.1)), # work-around\r\n    loss='mse'\r\n)\r\n\r\nmodel.fit(X, y, epochs=1, validation_data=(X_val,y_val), callbacks=[lr_scheduler])\r\nmodel.load_weights('tmp_model')\r\n```\r\noutputs:\r\n```\r\nEpoch 00001: LearningRateScheduler setting learning rate to 0.00994999977760017.\r\n2/2 [==============================] - 0s 126ms/step - loss: 0.6959 - val_loss: 0.9985 - lr: 0.0099\r\n\r\nEpoch 00001: LearningRateScheduler setting learning rate to 0.0009900249862112105.\r\n2/2 [==============================] - 0s 157ms/step - loss: 0.6430 - val_loss: 0.9910 - lr: 9.9002e-04\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-3d1de671fc08> in <module>()\r\n     21 \r\n     22 model.fit(X, y, epochs=1, validation_data=(X_val,y_val), callbacks=[lr_scheduler])\r\n---> 23 model.load_weights(f'tmp_model')\r\n\r\n1 frames\r\n/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)\r\n     65     except Exception as e:  # pylint: disable=broad-except\r\n     66       filtered_tb = _process_traceback_frames(e.__traceback__)\r\n---> 67       raise e.with_traceback(filtered_tb) from None\r\n     68     finally:\r\n     69       del filtered_tb\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     57     ctx.ensure_initialized()\r\n     58     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 59                                         inputs, attrs, num_outputs)\r\n     60   except core._NotOkStatusException as e:\r\n     61     if name is not None:\r\n\r\nInvalidArgumentError: tensor_name = optimizer/learning_rate/.ATTRIBUTES/VARIABLE_VALUE; expected dtype double does not equal original dtype float [Op:RestoreV2]\r\n```\r\n\r\nnotebook with code:\r\n\r\nhttps://colab.research.google.com/drive/1FxwBUjUcw0U0FiL1LDGFbfCY7ocGQ8ij?usp=sharing\r\n\r\n", "comments": ["Hi @johanbluecreek !  You have to save weights before loading weights. Attached resolved [gist](https://colab.sandbox.google.com/gist/mohantym/209fb08281d158a0b9a42013c6b7a781/invalidargumenterror-bug.ipynb#scrollTo=gqh2NGrMfRld) and relevant [thread](https://keras.io/api/models/model_saving_apis/#saveweights-method) for reference.   Thanks!", "> Hi @johanbluecreek ! You have to save weights before loading weights. Attached resolved [gist](https://colab.sandbox.google.com/gist/mohantym/209fb08281d158a0b9a42013c6b7a781/invalidargumenterror-bug.ipynb#scrollTo=gqh2NGrMfRld) and relevant [thread](https://keras.io/api/models/model_saving_apis/#saveweights-method) for reference. Thanks!\r\n\r\nThe purpose was to load weights from before the second fit, not after.\r\n\r\nPotential scenario is when doing fine-tuning after transfer learning: You set your downloadable model to `trainable=False`, and add and train your top layers, then set the layers of the whole model to `trainable=True`, what you then may want to do is to find an appropriate learning rate, which does not leave the already found minima too much. To do so you may want to load weights from after first fit, try one epoch with a new learning rate a few times before you find an appropriate one. Under such an approach, the error is triggered.", "Ok @johanbluecreek ! Thanks for the clarification . Please post this [Keras](https://github.com/keras-team/keras) repo too. Thanks!", "Ok @johanbluecreek ! Closing this issue here as it will be tracked in Keras repo. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54280\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54280\">No</a>\n"]}, {"number": 54277, "title": "Option to avoid caching with bijectors.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.7.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe current behaviour for `tensorflow_probability.bijectors.Bijector` is to cache the input to be used if the inverse function is called. I would very much like a keyword being able to turn this caching off to ensure the inverse function of the bijector is always called. Whilst setting the property of `_is_injective` can accomplish this, you lose other features.\r\n\r\n**Will this change the current api? How?**\r\nYes, a keyword in the definition of the Bijector class.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wishes to have bijectors where the inverse function is always called rather than the input that's been cached.\r\n\r\n**Any Other info.**\r\n", "comments": ["@alexhepburn ,\r\nThis issue is more related to tensorflow/probability than tensorflow/tensorflow. Please post it on Tensorflow probability repo from [here](https://github.com/tensorflow/probability). Thanks!\r\n", "Thanks!", "@alexhepburn ,\r\nPlease feel free to move this issue to closed status as it is related to different repo.Thanks!"]}, {"number": 54275, "title": "Do not run scorecards analysis in forks", "body": "Since this job has side effects, I am guessing it shouldn't be run by any forks? The analysis is probably going to fail due to permissions errors and would confuse contributors.", "comments": []}, {"number": 54274, "title": "AutoGraph could not transform <function ...>", "body": "```\r\nINFO:tensorflow:<function train_main.<locals>.train at 0x2976dd700> is not cached for subkey ConversionOptions[{}]\r\nINFO:tensorflow:Source code of <function train_main.<locals>.train at 0x2976dd700>:\r\n\r\n@tf.function\r\ndef train(batch):\r\n    s    = batch[:, start_s:    end_s]\r\n    a    = batch[:, start_a:    end_a]\r\n    s_   = batch[:, start_s_:   end_s_]\r\n    r    = batch[:, start_r:    end_r]\r\n    done = batch[:, start_done: end_done]\r\n\r\n    noise = tf.random.normal([args.batch_size, env.action_dim])\r\n    a_, log_\u03c0_ = actor([s, noise])\r\n    y = r + args.gamma * (1-done) * (\r\n    tf.minimum(critic_1_target([s_, a_]),\r\n    critic_2_target([s_, a_]))\r\n    - args.alpha * log_\u03c0_)\r\n\r\n    with tf.GradientTape() as tape:\r\n        MSBE_1 = (1/args.batch_size) * tf.reduce_sum((critic_1([s, a]) - y)**2)\r\n    MSBE_1_grads = tape.gradient(MSBE_1, critic_1.trainable_weights)\r\n    critic_1_optimizer.apply_gradients(zip(MSBE_1_grads, critic_1.trainable_weights))\r\n\r\n    with tf.GradientTape() as tape:\r\n        MSBE_2 = (1/args.batch_size) * tf.reduce_sum((critic_2([s, a]) - y)**2)\r\n    MSBE_2_grads = tape.gradient(MSBE_2, critic_2.trainable_weights)\r\n    critic_2_optimizer.apply_gradients(zip(MSBE_2_grads, critic_2.trainable_weights))\r\n\r\n    noise = tf.random.normal([args.batch_size, env.action_dim])\r\n    with tf.GradientTape() as tape:\r\n        log_\u03c0_\u03b8: tf.Tensor\r\n        a_\u03b8 , log_\u03c0_\u03b8 = actor([s, noise])\r\n        expected_reward = (1/args.batch_size) *                 tf.reduce_sum(critic_1([s, a_\u03b8]) - args.alpha * log_\u03c0_\u03b8)\r\n        neg_expected_reward = -expected_reward\r\n    expected_reward_grad = tape.gradient(neg_expected_reward, actor.trainable_weights)\r\n    actor_optimizer.apply_gradients(zip(expected_reward_grad, actor.trainable_weights))\r\n\r\n    polyak_average(critic_1_target.variables, critic_1.variables)\r\n    polyak_average(critic_2_target.variables, critic_2.variables)\r\n\r\n    if args.model_name:\r\n        MSBE_1_log(MSBE_1)\r\n        MSBE_2_log(MSBE_2)\r\n        expected_reward_log(expected_reward)\r\n\r\n\r\nINFO:tensorflow:Error transforming entity <function train_main.<locals>.train at 0x2976dd700>\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 433, in converted_call\r\n    converted_f = _convert_actual(target_entity, program_ctx)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 275, in _convert_actual\r\n    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 286, in transform\r\n    return self.transform_function(obj, user_context)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 470, in transform_function\r\n    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 363, in transform_function\r\n    result = self.transform_ast(node, context)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 243, in transform_ast\r\n    node = self.initial_analysis(node, ctx)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 231, in initial_analysis\r\n    node = activity.resolve(node, ctx, None)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 709, in resolve\r\n    return ActivityAnalyzer(context, parent_scope).visit(node)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py\", line 445, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/opt/homebrew/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ast.py\", line 407, in visit\r\n    return visitor(node)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 601, in visit_FunctionDef\r\n    node.body = self.visit_block(node.body)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py\", line 340, in visit_block\r\n    replacement = self.visit(node)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py\", line 445, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/opt/homebrew/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ast.py\", line 407, in visit\r\n    return visitor(node)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 651, in visit_With\r\n    node = self.generic_visit(node)\r\n  File \"/opt/homebrew/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ast.py\", line 483, in generic_visit\r\n    value = self.visit(value)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py\", line 445, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/opt/homebrew/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ast.py\", line 407, in visit\r\n    return visitor(node)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 394, in visit_AnnAssign\r\n    node.value = self.visit(node.value)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py\", line 431, in visit\r\n    raise ValueError(msg)\r\nValueError: invalid value for \"node\": expected \"ast.AST\", got \"<class 'NoneType'>\"; to visit lists of nodes, use \"visit_block\" instead\r\nWARNING:tensorflow:AutoGraph could not transform <function train_main.<locals>.train at 0x2976dd700> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: invalid value for \"node\": expected \"ast.AST\", got \"<class 'NoneType'>\"; to visit lists of nodes, use \"visit_block\" instead\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n```\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 12\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary (`pip3 install tensorflow-macos`)\r\n- TensorFlow version (use command below): `unknown 2.7.0`\r\n- Python version: 3.9.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nSorry, but I can't provide the full source code.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I did some investigation and the problem seems to be on the line `log_\u03c0_\u03b8: tf.Tensor` giving a type annotation for `log_\u03c0_\u03b8`. If I comment that line out, the issue goes away.", "@andmis \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "I cannot provide you with the entire Python program. As I said, the line that introduces the problem is the type annotation `log_\u03c0_\u03b8: tf.Tensor`, and it seems that the parser in TensorFlow cannot handle that line.\r\n\r\nTo reproduce the problem, I think it should be enough to add a line that looks like `foo_bar: int` or `foo_bar: tf.Tensor` to any `@tf.function`.", "@andmis \r\nWhen i tried to replicate your issue, I got `SyntaxError: invalid syntax` at `log_\u03c0_\u03b8: tf.Tensor`. \r\nHelp us in replicating your issue to investigate the root cause. Thanks!\r\n", "What version of Python are you using? That line of code is a variable type annotation. Python type hinting support was introduced in 3.5, and variable type annotations specifically in 3.6. You can read more about Python type hints at <https://docs.python.org/3/library/typing.html>. See also this StackOverflow thread: <https://stackoverflow.com/questions/51789120/type-hints-syntax-error-on-python-3-5>.\r\n\r\nDo you get that error with Python >= 3.7?", "@andmis, I am using Python version 3.7.\r\nI get the syntax error when i am using `log_\u03c0_\u03b8: tf.Tensor`.\r\nThanks!\r\n", "Do you get a syntax error if you run the following Python program?\r\n\r\n```python\r\nn: int\r\nprint('Hello, world')\r\n```", "@andmis,\r\nI didn\u2019t get syntax error on Python ~=3.8 for both below code snippet.\r\n```\r\nn: int\r\nprint('Hello, world')\r\n```\r\n```\r\nimport tensorflow as tf\r\nlog_\u03c0_\u03b8: tf.Tensor\r\n```", "Ok, now try this:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef f():\r\n  n: int\r\n  return 0\r\n\r\nf()\r\n```", "@sachinprasadhs, I am able to replicate the issue with Tensorflow 2.8. Please find the Colab gist [here](https://colab.research.google.com/drive/1Za8SUtBmyXrfnlHYAspG8Fz1VXfcqsDF?usp=sharing).  Seems Autograph function failed to read `n: int`. Thanks!", "The operation which you are trying is not supported in tf.function. You can silence this warning by decorating with `@tf.autograph.experimental.do_not_convert` like below.\r\n\r\n import tensorflow as tf\r\n\r\n```\r\n@tf.function\r\n@tf.autograph.experimental.do_not_convert\r\ndef f():\r\n  n: int\r\n  return 0\r\n\r\nf()\r\n```", "- It's not an operation, it's a type annotation.\r\n- The error message says, `Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, 'export AUTOGRAPH_VERBOSITY=10') and attach the full output.`\r\n- Type annotations like `n: int = 3` work fine, with no error. Why is `n: int = 3` ok but `n: int` not ok?\r\n- This is obviously a bug in the parser code.", "Yes, this is a bug. A temporary workaround would be to initialize it, e.g. `log_\u03c0_\u03b8: tf.Tensor = None`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54274\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54274\">No</a>\n"]}, {"number": 54273, "title": "Build broken on AARCH64 by XNNPACK update", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: git HEAD\r\n- Python version:3.8.10\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 5.0.0\r\n- GCC/Compiler version (if compiling from source): 10.3.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nbuild_pip_package fails with\r\n\r\nERROR: /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/external/XNNPACK/BUILD.bazel:9264:19: Compiling src/f32-igemm/1x8-aarch64-neonfma-cortex-a75.cc failed: (Exit 1): gcc failed: error executing command \r\n  (cd /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/lib64:/usr/local/lib \\\r\n    PATH=/home/builder/.cache/bazelisk/downloads/bazelbuild/bazel-5.0.0-linux-arm64/bin:/home/builder/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /usr/local/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/aarch64-opt/bin/external/XNNPACK/_objs/jit/1/1x8-aarch64-neonfma-cortex-a75.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/XNNPACK/_objs/jit/1/1x8-aarch64-neonfma-cortex-a75.pic.o' -fPIC '-DXNN_LOG_LEVEL=0' -DPTHREADPOOL_NO_DEPRECATED_API -iquoteexternal/XNNPACK -iquotebazel-out/aarch64-opt/bin/external/XNNPACK -iquoteexternal/FP16 -iquotebazel-out/aarch64-opt/bin/external/FP16 -iquoteexternal/clog -iquotebazel-out/aarch64-opt/bin/external/clog -iquoteexternal/pthreadpool -iquotebazel-out/aarch64-opt/bin/external/pthreadpool -iquoteexternal/FXdiv -iquotebazel-out/aarch64-opt/bin/external/FXdiv -iquoteexternal/cpuinfo -iquotebazel-out/aarch64-opt/bin/external/cpuinfo -Ibazel-out/aarch64-opt/bin/external/FP16/_virtual_includes/FP16 -Ibazel-out/aarch64-opt/bin/external/clog/_virtual_includes/clog -Ibazel-out/aarch64-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/aarch64-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/aarch64-opt/bin/external/cpuinfo/_virtual_includes/cpuinfo -isystem external/XNNPACK/include -isystem bazel-out/aarch64-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/aarch64-opt/bin/external/XNNPACK/src -isystem external/FP16/include -isystem bazel-out/aarch64-opt/bin/external/FP16/include -isystem external/pthreadpool/include -isystem bazel-out/aarch64-opt/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/aarch64-opt/bin/external/FXdiv/include -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -Iinclude -Isrc -O2 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/XNNPACK/src/f32-igemm/1x8-aarch64-neonfma-cortex-a75.cc -o bazel-out/aarch64-opt/bin/external/XNNPACK/_objs/jit/1/1x8-aarch64-neonfma-cortex-a75.pic.o)\r\n# Configuration: ce8f4d34c0d44ddff120a1f5ad2cdfaaab653410aefcb8fe64f7a37d66c62619\r\n# Execution platform: @local_execution_config_platform//:platform\r\nexternal/XNNPACK/src/f32-igemm/1x8-aarch64-neonfma-cortex-a75.cc: In member function 'void xnnpack::aarch64::{anonymous}::Generator::generate(bool, size_t, size_t, size_t, float, float)':\r\nexternal/XNNPACK/src/f32-igemm/1x8-aarch64-neonfma-cortex-a75.cc:48:39: error: 'numeric_limits' is not a member of 'std'\r\n   48 |   const bool clamp_min = min != -std::numeric_limits<float>::infinity();\r\n      |                                       ^~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f32-igemm/1x8-aarch64-neonfma-cortex-a75.cc:48:54: error: expected primary-expression before 'float'\r\n   48 |   const bool clamp_min = min != -std::numeric_limits<float>::infinity();\r\n      |                                                      ^~~~~\r\nexternal/XNNPACK/src/f32-igemm/1x8-aarch64-neonfma-cortex-a75.cc:49:39: error: 'numeric_limits' is not a member of 'std'\r\n   49 |   const bool clamp_max = max != +std::numeric_limits<float>::infinity();\r\n      |                                       ^~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f32-igemm/1x8-aarch64-neonfma-cortex-a75.cc:49:54: error: expected primary-expression before 'float'\r\n   49 |   const bool clamp_max = max != +std::numeric_limits<float>::infinity();\r\n      |                                                      ^~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 387.892s, Critical Path: 129.24s\r\nINFO: 2857 processes: 206 internal, 2651 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nSeems to be caused by \r\nhttps://github.com/tensorflow/tensorflow/commit/b9f4c111ce9301240bd3d3b97449bc485ca52b15", "comments": ["@cfRod @nSircombe ", "CC @Maratyszcza", "Will be fixed by https://github.com/google/XNNPACK/pull/2539", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54273\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54273\">No</a>\n"]}, {"number": 54272, "title": "[TF:TRT] Remove warning message about missing TRTEngineCacheResource", "body": "This PR removes a confusing warning message that was printed while saving the model.\r\n\r\nUntil now, if we convert and save the model without calling build(), then the following message is printed to the console:\r\n```\r\nW tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:196 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_0)\r\n```\r\n\r\nThis message is confusing to new users of TF-TRT: it describes a failure, which is printed as a warning `W`, and it is not clear for the user if anything went wrong (in fact nothing went wrong). This PR removes this warning message. ", "comments": ["Tagging @bixia1 for review.", "Good work, Tamas! Thanks for the detail explanation [here](https://github.com/tensorflow/tensorflow/pull/54272#issue-1124358603).\r\nSince the first block of the conversation will go to the PR as a commit message, would you please copy those details to another comment block (the second comment block when you create a new PR in the future), and only leave the message that you want to go into the commit message in the first conversation block?", "Good point @bixia1, I have shortened the commit message. Adding the details here:\r\n\r\nThe following code converts a model and saves it, without actually building the engines\r\n```python\r\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir=model_dir)\r\nconverter.convert()\r\n# converter.build(input_fn) is not called\r\nconverter.save(trt_model_dir)\r\n```\r\nWhen we execute this, the following message is printed to the console:\r\n```\r\nW tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:196 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_0)\r\n```\r\n\r\n\r\nThe PR changes the error handling from `OP_REQUIRES_OK` to `OP_REQUIRES` https://github.com/tensorflow/tensorflow/blob/29177d41e6a5cce8a81eabe58e73dcbe30c7c538/tensorflow/core/framework/op_requires.h#L41-L56 which only VLOGs the message, but does not issue a warning https://github.com/tensorflow/tensorflow/blob/29177d41e6a5cce8a81eabe58e73dcbe30c7c538/tensorflow/core/framework/op_kernel.cc#L1703-L1711\r\n\r\nThe exception is handled in the converter, where we print an informative message https://github.com/tensorflow/tensorflow/blob/e3a36c96323b132c3fc3c74f1503974a144e21cc/tensorflow/python/compiler/tensorrt/trt_convert.py#L1362-L1367"]}, {"number": 54271, "title": "model.fit() bug when using a zipped Dataset as input for a multiple-input model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.9.0.dev20220202\r\n- Python version: 3.10.2\r\n\r\n**Describe the current behavior**\r\nI have a custom model which takes 3 images as input\r\nI have 3 separate (currently unbatched as I debug this error) datasets, classes encoded as categorical, meaning each input tensor has shape ((x, y, z), (c,))\r\nTrying to input the 3 datasets separately fails, either by inputting them as a dict mapping each ds to a named input `{\"Input1\": ds1, {\"Input2\": ds2, {\"Input3\": ds3}`, or using a list `[ds, ds2, ds3]`\r\n\r\nI zip the three datasets. Testing the resulting dataset with [(using the docs as guidance)](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip):\r\n```\r\nfor element in zipped_ds.as_numpy_iterator():\r\nprint(\"element\", element)\r\n```\r\nOutputs:\r\n```\r\nelement [[[x1, y1, z1], [c1,]], [[x2, y2, z2], [c2,]], [[x3, y3, z3], [c3,]]] \r\nelement [[[x1, y1, z1], [c1,]], [[x2, y2, z2], [c2,]], [[x3, y3, z3], [c3,]]] \r\n...\r\n\r\n```\r\nSeems to work, right? Every call to the iterator returns 3 elements.\r\nWell, when I use the zipped dataset as input of model_fit(), the first element in the tuple returned by the dataset object is treated as the input for the whole model, meaning that instead of using [[[x1, y1, z1], [c1,]], [[x2, y2, z2], [c2,]], [[x3, y3, z3], [c3,]]] as the input to the model, it uses [[x1, y1, z1], [c1,]], and the training fails.\r\n\r\nI've tried many approaches, like using `zipped_ds.as_numpy_iterator()` or `([ds1, ds2, ds3] for idx, (ds1, ds2, ds3) in enumerate(zipped_ds))`, but both fail as the returned item is empty\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# %%\r\nimport os\r\n\r\nimport tensorflow as tf # tensorflow nightly, version>=2.5\r\nfrom tensorflow import keras\r\nfrom tensorflow.image import crop_to_bounding_box as tfimgcrop\r\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\r\n\r\nBATCH_SIZE=32 # Adjust?\r\n\r\nIMG_SIZE=(224, 224)\r\nIMG_SHAPE = IMG_SIZE + (3,)\r\n\r\n# %%\r\n_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\r\npath_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\r\nPATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\r\n\r\ntrain_dir = os.path.join(PATH, 'train')\r\nvalidation_dir = os.path.join(PATH, 'validation')\r\n\r\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\r\n                                             shuffle=False,\r\n                                             label_mode='categorical',\r\n                                             batch_size=32,\r\n                                             image_size=IMG_SIZE)\r\nvalidation_dataset = tf.keras.preprocessing.image_dataset_from_directory(validation_dir,\r\n                                             shuffle=False,\r\n                                             label_mode='categorical',\r\n                                             batch_size=32,\r\n                                             image_size=IMG_SIZE)\r\n\r\n# %%\r\nbase_model1 = tf.keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\r\n                                               include_top=False,\r\n                                               weights='imagenet',\r\n                                               minimalistic=False,\r\n                                               pooling=max,\r\n                                               dropout_rate=0.2)\r\nbase_model2 = tf.keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\r\n                                               include_top=False,\r\n                                               weights='imagenet',\r\n                                               minimalistic=False,\r\n                                               pooling=max,\r\n                                               dropout_rate=0.2)\r\nbase_model3 = tf.keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\r\n                                               include_top=False,\r\n                                               weights='imagenet',\r\n                                               minimalistic=False,\r\n                                               pooling=max,\r\n                                               dropout_rate=0.2)\r\n\r\n# %%\r\npre_concat_layer1 = tf.keras.layers.Dense(64, \r\n                                        activation='relu', \r\n                                        kernel_initializer='random_uniform', \r\n                                        bias_initializer='zeros')\r\npre_concat_layer2 = tf.keras.layers.Dense(64, \r\n                                        activation='relu', \r\n                                        kernel_initializer='random_uniform', \r\n                                        bias_initializer='zeros')\r\npre_concat_layer3 = tf.keras.layers.Dense(64, \r\n                                        activation='relu', \r\n                                        kernel_initializer='random_uniform', \r\n                                        bias_initializer='zeros')\r\n\r\npost_concat_layer = tf.keras.layers.Dense(128, \r\n                                        activation='relu', \r\n                                        kernel_initializer='random_uniform', \r\n                                        bias_initializer='zeros')\r\nprediction_layer = tf.keras.layers.Dense(2, \r\n                                        activation='softmax', \r\n                                        kernel_initializer='random_uniform', \r\n                                        bias_initializer='zeros')\r\n\r\n# %%\r\ninput1 = tf.keras.Input(shape=(64, 64, 3), name=\"First\")\r\ninput2 = tf.keras.Input(shape=(64, 64, 3), name=\"Second\")\r\ninput3 = tf.keras.Input(shape=(64, 64, 3), name=\"Third\")\r\n\r\nx = base_model1(input1, training=False)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\r\nx = tf.keras.layers.Dropout(0.2)(x)\r\nx = tf.keras.layers.BatchNormalization()(x)\r\nx = pre_concat_layer1(x)\r\nx = tf.keras.layers.Dropout(0.2)(x)\r\noutputs = tf.keras.layers.BatchNormalization()(x)\r\nbody1 = tf.keras.Model(input1, outputs)\r\n\r\nx = base_model2(input2, training=False)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\r\nx = tf.keras.layers.Dropout(0.2)(x)\r\nx = tf.keras.layers.BatchNormalization()(x)\r\nx = pre_concat_layer2(x)\r\nx = tf.keras.layers.Dropout(0.2)(x)\r\noutputs = tf.keras.layers.BatchNormalization()(x)\r\nbody2 = tf.keras.Model(input2, outputs)\r\n\r\nx = base_model3(input3, training=False)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\r\nx = tf.keras.layers.Dropout(0.2)(x)\r\nx = tf.keras.layers.BatchNormalization()(x)\r\nx = pre_concat_layer3(x)\r\nx = tf.keras.layers.Dropout(0.2)(x)\r\noutputs = tf.keras.layers.BatchNormalization()(x)\r\nbody3 = tf.keras.Model(input3, outputs)\r\n\r\n# %%\r\nbody1.get_layer(\"MobilenetV3large\")._name = \"MobilenetV3large1\"\r\nbody2.get_layer(\"MobilenetV3large\")._name = \"MobilenetV3large2\"\r\nbody3.get_layer(\"MobilenetV3large\")._name = \"MobilenetV3large3\"\r\n\r\n# %%\r\ncombinedInput = tf.keras.layers.concatenate([body1.output, body2.output, body3.output])\r\nx = post_concat_layer(combinedInput)\r\nx = tf.keras.layers.Dropout(0.2)(x)\r\nx = tf.keras.layers.BatchNormalization()(x)\r\nfoutput = prediction_layer(x)\r\nfinal_model = tf.keras.Model(inputs=[body1.input, body2.input, body3.input], outputs=foutput)\r\n\r\n# %%\r\ndef resize_data1(images, classes):\r\n    return (tfimgcrop(images,\r\n                        offset_height=0,\r\n                        offset_width=0,\r\n                        target_height=64,\r\n                        target_width=64),\r\n                    classes)\r\ndef resize_data2(images, classes):\r\n    return (tfimgcrop(images,\r\n                        offset_height=0,\r\n                        offset_width=64,\r\n                        target_height=64,\r\n                        target_width=64),\r\n                    classes)\r\ndef resize_data3(images, classes):\r\n    return (tfimgcrop(images,\r\n                        offset_height=0,\r\n                        offset_width=128,\r\n                        target_height=64,\r\n                        target_width=64),\r\n                    classes)\r\n\r\n# %%\r\ntrain_dataset_unb = train_dataset.unbatch()\r\ntrain_dataset1 = train_dataset_unb.map(resize_data1)\r\ntrain_dataset2 = train_dataset_unb.map(resize_data2)\r\ntrain_dataset3 = train_dataset_unb.map(resize_data3)\r\ntrain_dataset_zip = tf.data.Dataset.zip((train_dataset1, train_dataset2, train_dataset3))\r\n\r\nvalidation_dataset_unb = validation_dataset.unbatch()\r\nvalidation_dataset1 = validation_dataset_unb.map(resize_data1)\r\nvalidation_dataset2 = validation_dataset_unb.map(resize_data2)\r\nvalidation_dataset3 = validation_dataset_unb.map(resize_data3)\r\nvalidation_dataset_zip = tf.data.Dataset.zip((validation_dataset1, validation_dataset2, validation_dataset3))\r\n\r\n# %%\r\nfinal_model.compile()\r\n\r\n# %%\r\nhistory = final_model.fit(train_dataset_zip,\r\n                        epochs=999, \r\n                        validation_data=validation_dataset_zip,\r\n                        validation_steps=32\r\n                        )\r\n```", "comments": ["I would like to work on this. I am new to this community could anyone please guide me how to go about solving this issue. ", "Hi @dwipddalal ! You can go through[ model ](https://www.tensorflow.org/api_docs/python/tf/keras/Model)documentation to start contributing in this issue.", "@ghylander !\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "Ok @ghylander ! Closing this issue here as it will be tracked in Keras repo . Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54271\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54271\">No</a>\n"]}, {"number": 54267, "title": "asd", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54267\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54267\">No</a>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54267\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54267\">No</a>\n"]}, {"number": 54266, "title": "The gif encoding and decoding is not lossless in tf and tfio", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.7.11\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: running on CPU\r\n- GPU model and memory: running on CPU\r\n\r\n**Describe the current behavior**\r\nThe gif encoding and decoding is not lossless. The input after gif encoding and decoding is not equivalent to the original input.\r\n\r\n**Describe the expected behavior**\r\nThe image encoded by gif and then decoded should be lossless.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nneed to install tensorflow=2.7.0 and tensorflow_io=0.22.0\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_io as tfio\r\nimport numpy as np\r\n\r\ninput = np.array(\r\n    [[[[13, 54, 87,],\r\n    [56, 210, 195,],\r\n    [230, 135, 61,],],\r\n    [[13, 54, 87,],\r\n    [56, 210, 195,],\r\n    [230, 135, 61,],],\r\n    [[13, 54, 87,],\r\n    [56, 210, 195,],\r\n    [230, 135, 61,]]]]\r\n)\r\n\r\ninput_uint8 = tf.cast(input, tf.uint8)\r\nencoded_file = tfio.image.encode_gif(input_uint8)\r\ninput_decoded = tf.io.decode_gif(encoded_file) \r\nprint(np.allclose(input_uint8, input_decoded))  # False\r\n```\r\nThe input after gif encoding and decoding is not equivalent to the original input. But gif itself should be lossless.\r\n", "comments": ["@jiannanWang ,\r\nI was facing different error while executing the mentioned code.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/361adb568f56a2339301b18749337680/untitled215.ipynb) and confirm.", "Hi tilakrayal! Thank you for your response. I found a bug in my code. It lies in the last line and I've fixed that. I confirmed on the gist that the fixed code is working as I expected. Can you please try it again? ", "There might be some misunderstanding. The problem is not solved. I fixed my code so now it can trigger the bug of gif encoding and decoding.\r\n", "@jiannanWang ,\r\nI was able to get the same error result with the old and updated code.Please find the [gist](https://colab.research.google.com/gist/tilakrayal/b8387b8b5aa40c7948872f31426f9a21/untitled216.ipynb).If possible can you please provide the error log which helps to analyse the issue.", "Hi tilakrayal. I created a gist(https://colab.research.google.com/drive/16Cd-eiB5KnGR_nLF4KK0XmB8zSZtVUMM?usp=sharing) here. You can check it. On my computer, the code will print \"False\" which indicates the encoded and decoded input is not equivalent to the original input.", "@sachinprasadhs ,\r\nI was able to reproduce the issue in tf v2.7, v2.8 and different error in nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/2e647699835f7fdb0131931bfbe226ee/54266.ipynb).", "Thanks for the report and investigation!\r\n\r\nI think this is fine, after digging a bit deeper.\r\n\r\nFirst, image formats in general are not lossless, so \"saving as image in tensorflow\" is not intended to be a lossless operation.  And the docs (https://www.tensorflow.org/io/api_docs/python/tfio/image/encode_gif) don't suggest that GIF uses a lossless variation.\r\n\r\nFor saving and loading data (without loss), there are other mechanisms.  \r\n\r\nSpecifically for GIF, there are a few potential sources of loss.  While we don't (I believe) use a lossy encryption, we do use a palettized encoding.  This CAN be exact/lossless for small int8 tensors, but a simple heuristic like histogram each color is perceptually more accurate if you quantize to a smaller color space first.  In fact, this is what the common giflib does and we do here; quantize from 8 to 5 bits per channel first.\r\n\r\nI don't think we're likely to change this commmitment (i.e. lossy) without a strong reason, so I'm closing the ticket for now, but always happy to hear suggestions.  Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54266\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54266\">No</a>\n"]}, {"number": 54264, "title": "tf.image.non_max_suppression produces wrong results in tf.function when the input is float16.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.7.11\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: running on CPU\r\n- GPU model and memory: running on CPU\r\n\r\n**Describe the current behavior**\r\ntf.image.non_max_suppression will produce wrong results in tf.function when the input is float16.\r\n\r\n**Describe the expected behavior**\r\nThe results should be equivalent to eager mode.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nargument = {'max_output_size': 3673497876354746723,\r\n            'scores': np.array([61920., 31020., 2234., 41280., -25490.], dtype=np.float16),\r\n            'boxes': np.array([[-40500., -59940., -106.3, 41800.],\r\n                               [-37700., 37500., -53630., 49920.],\r\n                               [-44900., -5356., 28080., 24940.],\r\n                               [-44770., -18110., 28940., 7900.],\r\n                               [-32380., 36200., -40640., 22850.]], dtype=np.float16)}\r\n\r\nfun = tf.image.non_max_suppression\r\n\r\noutput1 = fun(**argument)\r\nprint(output1)  # the results of eager mode is correct\r\n\r\n@tf.function\r\ndef fun_wrapper(arg):\r\n    return fun(**arg)\r\n\r\noutput2 = fun_wrapper(argument) \r\nprint(output2)  # the results of tf function is wrong\r\n```\r\n\r\nThe results are:\r\n\r\ntf.Tensor([0 3 1 2 4], shape=(5,), dtype=int32)\r\ntf.Tensor([0], shape=(1,), dtype=int32)", "comments": ["@chunduriv Was able to replicate the issue on colab using TF v2.7.0 ,2.8.0 and tf-nightly, please find the attached gist [here](https://colab.research.google.com/gist/sushreebarsa/f2e8a8bf3f8d60f7f7255741315cfd52/untitled566.ipynb).Thanks!", "Triage Notes: The maximum value that float16 can represent is `65504`. The values in this example are large enough that it is a reasonable guess that overflow is occurring. This appears to be a testing only case to find numeric bounds, the box coordinates do not look like either typical absolute pixel coordinates in an image or [0,1] normalized coordinates. Using `float32` avoids this bug. While the `NonMaxSuppressionV3` currently does not support `bfloat16`, using `bfloat16` instead of `float16` might solve this problem.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54264\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54264\">No</a>\n"]}, {"number": 54263, "title": "Why is inference time so slow when using tensorflow.compat.v1?", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source(pip install)\r\n- TensorFlow version (use command below): TFv1(1.13.1 / 1.14.0 /1.15.4), TFv2(2.7.0)\r\n- Python version: Python 3.6(TFv1), Python 3.7(TFv2)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 11.3 / cuDNN 8.2.1\r\n- GPU model and memory: GTX 1080ti 11G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: 1.13.1 / 1.14.0 / 1.15.4\r\n2. TF 2.0: 2.7.0\r\n\r\n**Describe the current behavior**\r\nInference time is slow when loading and inferring models using tf.compat.v1.\r\nI tested 4 cases about one model.\r\nthe model is one of tensorflow object detection model zoo and trained using own dataset(fine tuning). \r\n\r\n1 case) inference using TF 1.13.1 -> inference time : 0.05\r\n2 case) inference using TF 1.14.0 -> inference time : 0.05\r\n3 case) inference using TF 1.15.4 -> inference time : 0.27\r\n4 case) inference using TF 2.7.0 -> inference time : 0.27\r\n\r\nLooking at the above results, I think version of tf.compat.v1 is 1.15.4.\r\nCan I change version of tf.compat.v1 from 1.15 -> 1.14?\r\n\r\nOur company solution is using tensorflow 1.13 to load and infer models. And I am migrating to tf2 to support RTX 3000 series.\r\n\r\n**Describe the expected behavior**\r\n\r\nExpected result,\r\n1 case) inference using TF 1.13.1 -> inference time : 0.05\r\n2 case) inference using TF 1.14.0 -> inference time : 0.05\r\n3 case) inference using TF 1.15.4 -> inference time : 0.05\r\n4 case) inference using TF 2.7.0 -> inference time : 0.05\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@SinDongHwan ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.", "@tilakrayal ,\r\nI would like to send you the sample code and weight file by email.\r\nCould you tell me your email?", "@SinDongHwan ,\r\nCan you please attach the files here.We can download from the same.Thanks!", "@tilakrayal \r\nFirst, Sorry. I can't attach the weight file because we are using it for our solution product. \r\n\r\nI found two things.\r\nOne uses the \"ssd resnet 50\" pretrained model.\r\nThe other has different number of nodes and inference time between the weight file and the weight file of the model zoo.([our_weight.txt](https://github.com/tensorflow/tensorflow/files/8031477/our_weight.txt), [ssd_resnet50.txt](https://github.com/tensorflow/tensorflow/files/8031480/ssd_resnet50.txt))\r\n- average inference time (tensorflow_gpu==1.13.1):  \r\n our weight - 0.054s\r\n ssd_resnet50 - 0.067s\r\n- average inference time (tensorflow_gpu==1.15.0):\r\n our weight - 0.27s\r\n ssd_resnet50 - 0.06s\r\n\r\n\r\nAnd I want to download following.\r\n\r\n1. I want to download a repository of a specific tree (branch). \r\ne.g.) https://github.com/tensorflow/models/tree/23b5b4227dfa1b23d7c21f0dfaf0951b16671f43\r\n\r\n2. I would like to download the old version of the weights file if it exists.\r\n\r\nCan I download it?\r\nIf possible, how can I download it?\r\n", "@SinDongHwan ,\r\nThis issue is more suitable for TensorFlow Models repo. Please post it on Tensorflow Models repo from [here](https://github.com/tensorflow/models/issues/new/choose). Thanks!", "@tilakrayal \r\nThank you. I also wrote this issue there."]}, {"number": 54262, "title": "q", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 54261, "title": "keras.layers.BatchNormalization/Reshape produces dynamic tensors when converted to tflite", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (converter), Linux 5.10.17 (tflite)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4 Model B Rev 1.4\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8 (converter) 3.7 (tflite)\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: \r\n\r\n**Describe the current behavior**\r\nConverting a model containing a `BatchNormalization` & `Reshape` layers (allowed via `SELECT_TF_OPS`) produces tensors which have dynamic size. When running this using a tflite delegate which only supports static tensors the following error occurs:\r\n\r\n```\r\nRuntimeError: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#420 is a dynamic-sized tensor).\r\n```\r\n\r\nwhere `tensor#420` is:\r\n\r\n```\r\n{\r\n    'name': 'model_3/batch_normalization_4/FusedBatchNormV3', \r\n    'index': 420, \r\n    'shape': array([   1,    5,    1,    1, 1024]), \r\n    'shape_signature': array([   1,    5,    1,    1, 1024]), \r\n    'dtype': <class 'numpy.float32'>, \r\n    'quantization': (0.0, 0), \r\n    'quantization_parameters': {\r\n      'scales': array([], dtype=float32), \r\n      'zero_points': array([], dtype=int32), \r\n      'quantized_dimension': 0\r\n    }, \r\n    'sparsity_parameters': {}\r\n}\r\n```\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nThe converted model runs successfully on a static delegate.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\nThis is reproducible with:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Sequential, Input, Model\r\nfrom tensorflow.keras.layers import BatchNormalization, Dense, Reshape\r\n\r\ninputs = Input(shape=(10,))\r\nMLP = Sequential()\r\nMLP.add(Dense(1024*5))\r\nMLP.add(Reshape((5,1,1,1024)))\r\nMLP.add(BatchNormalization(axis=1))\r\nMLP.add(Dense(1024*5))\r\n\r\noutput = MLP(inputs)\r\n\r\nmodel = Model(inputs=inputs, outputs=output)\r\nmodel.save('reproduce_bug')\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('reproduce_bug')\r\n\r\nconverter._experimental_lower_tensor_list_ops = False\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\n\r\ntflite_model = converter.convert()\r\n\r\nwith open(\"reproduce_bug.tflite\", \"wb\") as f:\r\n    f.write(tflite_model)\r\n```\r\n\r\nand then on the tflite device:\r\n\r\n```python\r\nimport tflite_runtime.interpreter as tflite\r\n\r\ninterpreter = tflite.Interpreter(model_path='reproduce_bug.tflite')\r\n\r\nfor tdetails in interpreter.get_tensor_details():\r\n    if tdetails['index'] == 19: # or your failing index\r\n        print(tdetails)\r\n\r\ninterpreter.allocate_tensors()\r\n```\r\n\r\nNote that unlike our real failing case this fails on a tensor in reshape rather than in `BatchNormalization`.\r\n\r\n**Other info / logs** \r\nFull traceback:\r\n\r\n```python-traceback\r\nTraceback (most recent call last):\r\n  File \"inference.py\", line 20, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 521, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\nRuntimeError: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#420 is a dynamic-sized tensor).Ignoring failed application of the default TensorFlow Lite delegate indexed at 0.\r\n```\r\n", "comments": ["@zacps By default, keras sets `batch_size` or the first argument of the input to be dynamic (i.e, -1). In order to fix this, please update your code as follows:\r\n\r\nInitial code: ```inputs = Input(shape=(10,))```\r\nUpdated code: ```inputs = Input(shape=(10,), batch_size=1)```\r\n\r\n\r\n\r\nUpdated standalone code:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Sequential, Input, Model\r\nfrom tensorflow.keras.layers import BatchNormalization, Dense, Reshape\r\n\r\ninputs = Input(shape=(10,), batch_size=1)\r\nMLP = Sequential()\r\nMLP.add(Dense(1024*5))\r\nMLP.add(Reshape((5,1,1,1024)))\r\nMLP.add(BatchNormalization(axis=1))\r\nMLP.add(Dense(1024*5))\r\n\r\noutput = MLP(inputs)\r\n\r\nmodel = Model(inputs=inputs, outputs=output)\r\nmodel.save('reproduce_bug')\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('reproduce_bug')\r\n\r\nconverter._experimental_lower_tensor_list_ops = False\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\n\r\ntflite_model = converter.convert()\r\n\r\nwith open(\"reproduce_bug.tflite\", \"wb\") as f:\r\n    f.write(tflite_model)\r\n```\r\n\r\nThe issue should be resolved.\r\n\r\nAdditionally, did you happen to modify the `tensor#420` output?\r\n\r\nIncorrect output (as you've mentioned above):\r\n```\r\n{\r\n    'name': 'model_3/batch_normalization_4/FusedBatchNormV3', \r\n    'index': 420, \r\n    'shape': array([   1,    5,    1,    1, 1024]), \r\n    'shape_signature': array([   1,    5,    1,    1, 1024]), \r\n    'dtype': <class 'numpy.float32'>, \r\n    'quantization': (0.0, 0), \r\n    'quantization_parameters': {\r\n      'scales': array([], dtype=float32), \r\n      'zero_points': array([], dtype=int32), \r\n      'quantized_dimension': 0\r\n    }, \r\n    'sparsity_parameters': {}\r\n}\r\n```\r\n\r\nCorrect output (notice that the `shape_signature` has a -1 at the beginning):\r\n\r\n```\r\n{\r\n    'name': 'model_3/batch_normalization_4/FusedBatchNormV3', \r\n    'index': 420, \r\n    'shape': array([   1,    5,    1,    1, 1024]), \r\n    'shape_signature': array([   -1,    5,    1,    1, 1024]), \r\n    'dtype': <class 'numpy.float32'>, \r\n    'quantization': (0.0, 0), \r\n    'quantization_parameters': {\r\n      'scales': array([], dtype=float32), \r\n      'zero_points': array([], dtype=int32), \r\n      'quantized_dimension': 0\r\n    }, \r\n    'sparsity_parameters': {}\r\n}\r\n```\r\n\r\n\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54261\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54261\">No</a>\n"]}, {"number": 54260, "title": "huhuuh", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54260\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54260\">No</a>\n"]}, {"number": 54259, "title": "inference/prediction batch is non deterministic and overlap data", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7/2.2\r\n- Python version: 3.10,3.8\r\n\r\n**Describe the current behavior**\r\nwe create tensor dataset from parquet files using make_batch_reader() & make_petastorm_dataset().\r\nStrategy - Multiworker mirror strategy\r\nwe use tf native functions like unbatch, batch, autosharding etc \r\ntf.data.experimental.AutoShardPolicy.DATA\r\n\r\nds = dataset.\r\n        .unbatch()\r\n        .batch(xcc_batch_size)\r\n        .with_options(options)\r\n         .prefetch(AUTO)\r\n\r\nstrategy.experimental_distribute_dataset(ds)\r\n\r\ntensor data have overlap between batches and we expect the data shouldn't overlap between batches. \r\n\r\n**Describe the expected behavior**\r\n\r\ndata shouldn't overlap between batches. if we batch 100 records in 10 batch then i would expect all 10 batches should have unique data. \r\n", "comments": ["Adding clarification that @kabilan6 provided in #53846: This issue occurs on a single GPU, so multi-GPU strategy is not relevant.\r\n\r\n@kabilan6, before we ask you for a simple reproducer script, are you able to first try running on TF 2.8? There were some potentially relevant determinism fixes between 2.7 and 2.8 related to `tf.data.Dataset` that might resolve at least the nondeterminism element of your issue.", "@duncanriach we noticed issue with both single and multi gpu workers. i tried tf 2.8 version and still same issue. we create tensor dataset from parquet file. then we use tf unbatch and batch function, then distribute with experimental_distribute_dataset", "@kabilan6,\r\n\r\n## Single vs multi-GPU\r\n\r\nWhen considering determinism, if there is not determinism with a single GPU then there will also not be determinism with multi-GPU. (The inverse is not necessarily true, however.) Once a single GPU is operating deterministically, multi-GPU will automatically operate deterministically, unless you are running into an independent issue with multi-GPU, which is unlikely.\r\n\r\n**Therefore, this issue is, at least and for now, a single GPU issue.**\r\n\r\nIn other words, we can proceed with attempting to get to the simplest reproducer code possible while running on only one GPU.\r\n\r\nI think we can probably also rule out `experimental_distribute_dataset` since this is multi-device functionality, unless its use with a single device is somehow introducing a bug (which is unlikely). However, I think it should be easy to include in a reproducer script.\r\n\r\n## TF Version\r\n\r\nThank you for confirming that the issue is occurring with TF2.8.\r\n\r\n## Next Steps\r\n\r\nIn your original comment, you wrote, \"we create tensor dataset from parquet files using make_batch_reader() & make_petastorm_dataset().\" I assume that you are referring to `petastorm.reader.make_batch_reader` ([documentation](https://petastorm.readthedocs.io/en/latest/api.html#module-petastorm.reader)) and `petastorm.tf_utils.make_petastorm_dataset` ([use example](https://petastorm.readthedocs.io/en/latest/readme_include.html#tensorflow-api)).\r\n\r\n**Have you confirmed that the Petastorm-provided dataset is generating data both correctly and deterministically/reproducibly?**\r\n\r\n**For `petastorm.reader.make_batch_reader` have you set the `shard_seed`?**\r\n\r\nIn your original comment, you also mention that you're using `tf.data.experimental.AutoShardPolicy.DATA`. Note that in the [documentation](https://www.tensorflow.org/api_docs/python/tf/data/experimental/AutoShardPolicy), it states that \"for this mode to correctly partition the dataset elements, the dataset needs to produce elements in a deterministic order.\" If the Petastorm-provided dataset is not deterministic, then this TensorFlow data sharding functionality may lead to the final batches not having unique data (as you're seeing).\r\n\r\n**Have you experimented with disabling TF sharding, by setting `AutoShardPolicy` to `OFF`?**\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Thanks for your timely response. If the auto sharding is set to OFF , how tf batch data will be distributed to different worker and multiple gpu with in a worker in multi worker strategy?\r\n\r\nAnother question, If i try mirror strategy ( with single worker and single GPU), we noticed data non determinstic  data(tf2.7), do you think this should be resolved in 2.8 ? \r\n\r\n Have you confirmed that the Petastorm-provided dataset is generating data both correctly and deterministically/reproducibly? we set the worker_count = 1 and ran multiworker startegy, i hope this should produce data in determinstic order. ", "> If the auto sharding is set to OFF , how tf batch data will be distributed to different worker and multiple gpu with in a worker in multi worker strategy?\r\n\r\nMy intention for asking you about setting `AutoShardPolicy` to `OFF` was as an experiment to see if that led to the non-uniqueness problem being resolved. If it did, then it would support the hypothesis that the problem of nondeterminism and non-uniqueness in the dataset was related to the Petastorm-provided dataset being nondeterministic/non-reproducible.\r\n\r\nThis is just an experiment to help isolate the source of the problems you're seeing. I'm making hypotheses and asking you to run experiments to test those hypotheses.\r\n\r\n> Another question, If i try mirror strategy ( with single worker and single GPU), we noticed data non determinstic data(tf2.7), do you think this should be resolved in 2.8 ?\r\n\r\nI would imagine that the nondeterminism you're seeing is from the same source as before. You have not isolated the source and eliminated it, have you? It seems like you're changing too many variables. We just need to focus on isolating the source of the problems in the system as it stands. We're not even trying to find a solution yet. We're just trying to isolate the problem.\r\n\r\nWhat I first want us to do is rule out this being a Petastorm-related issue. Everything I asked about in my previous comment is focused on that.", "I think you should run one experiment at a time (on a single GPU); only run *one* of the following at a time, not necessarily in the order listed, and then we can use the results to infer what might be happening. You don't necessarily have to run all of these experiments. It sounds like you're starting with experiment two, although I'm concerned that you've changed the multi-GPU strategy, which could be confounding (and also suggests you're not running these experiments on a single GPU, as you should now be).\r\n\r\n1. Look at the sequence of data coming out of the dataset produced by Petastorm to see if it's deterministic.\r\n2. Set `petastorm.reader.make_batch_reader` `worker_count` to 1 to see if that makes the batches both deterministic and unique.\r\n3. Set TF `AutoShardPolicy` to `OFF` to see if this makes the batch contents unique.\r\n\r\nAll these three independent experiments are attempting to discover the same thing: is the Petastorm-produced dataset deterministic/reproducible or not.", "Also, to debug these kinds of problems, to run the above experiments, you shouldn't need to run for very long. You should be able to see the nondeterminism after only a few steps, maybe 10 steps at the very most. You can check for determinism by performing a running hash on the examples coming out of the dataset. At this point, you'll want to focus on confirming determinism of the dataset coming out of Petastorm, because it's the furthest upstream we can easily look. And nondeterminism will flow downstream. We want to make sure that input to TensorFlow is deterministic.", "And remember, as with any change you make, if setting `worker_count` to 1 (on its own, in the absence of other changes) leads to the batches being deterministic and unique, then it will merely provide certainty that the nondeterminism is coming from the Petastorm section of your data pipeline. It will not necessarily be the ultimate solution you will want to use. There might be a best-practice for getting maximum deterministic performance from Petastorm.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54259\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54259\">No</a>\n", "For future reference, here is the summary of my current understanding of this issue.\r\n\r\nUntil it's ruled out, we should assume that the observed/reported nondeterminism is coming from the upstream [Petastorm](https://petastorm.readthedocs.io/en/latest/index.html) section of the input pipeline. Although I have not written code to reproduce it, the [documentation](https://petastorm.readthedocs.io/en/latest/api.html#module-petastorm.reader) for `petastorm.reader.make_batch_reader` (and `petastorm.reader.make_reader`) has pseudo-random functionality that must be initialized in order to obtain determinism.\r\n\r\nAdditionally, the observed \"overlap data\" is probably due to feeding TensorFlow's DATA-mode autosharding functionality with the nondeterministic data from Petastorm. See [this documentation](https://www.tensorflow.org/api_docs/python/tf/data/experimental/AutoShardPolicy).\r\n\r\n@mohantym, please will you change the title of this issue to \"Batch is nondeterministic and has overlapping data when using Petastorm make_batch_reader\"."]}, {"number": 54258, "title": "Build with exception \"no such package '@llvm-raw//utils/bazel':\" .", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS monterey 12.0.1\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\nTensorFlow installed from (source or binary):source\r\nTensorFlow version: r2.8\r\nPython version: python3.9\r\nInstalled using virtualenv? pip? conda?:pip\r\nBazel version (if compiling from source): bazel 3.7.2\r\nGCC/Compiler version (if compiling from source): Apple clang version 13.0.0 (clang-1300.0.29.3)\r\nCUDA/cuDNN version: no enabled\r\nGPU model and memory:: no enabled(Just CPU model)\r\n\r\n\r\n\r\n**Describe the problem**\r\nThe \"bazel build --config=dbg --strip=never -c dbg --copt='-g' --cxxopt='-g' //tensorflow/tools/pip_package:build_pip_package\" always failed at the step pull and install the package from url \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz\" with msg \"no such package '@llvm-raw//utils/bazel':\" , maybe  it is too large , can it be solved by download the package  to local and install from local ? If so ,  I want to download it to local firstly and install from local , but how to install it  from local ? any file need to be modified ?\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nERROR: An error occurred during the fetch of repository 'llvm-raw':\r\n   Traceback (most recent call last):\r\n\tFile \"/Users/xxx/ai/tf/tensorflow/third_party/repo.bzl\", line 64, column 33, in _tf_http_archive_impl\r\n\t\tctx.download_and_extract(\r\nError in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz, https://github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz] to /private/var/tmp/_bazel_shenyufeng/a4eaa5fcc3d32c786eb9bb79fa0163a2/external/llvm-raw/temp10925281720111290723/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz: Premature EOF\r\nERROR: Error fetching repository: Traceback (most recent call last):\r\n\tFile \"/Users/xxx/ai/tf/tensorflow/third_party/repo.bzl\", line 64, column 33, in _tf_http_archive_impl\r\n\t\tctx.download_and_extract(\r\nError in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz, https://github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz] to /private/var/tmp/_bazel_xxxx/a4eaa5fcc3d32c786eb9bb79fa0163a2/external/llvm-raw/temp10925281720111290723/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz: Premature EOF\r\nERROR: no such package '@llvm-raw//utils/bazel': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz, https://github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz] to /private/var/tmp/_bazel_shenyufeng/a4eaa5fcc3d32c786eb9bb79fa0163a2/external/llvm-raw/temp10925281720111290723/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz: Premature EOF\r\nINFO: Elapsed time: 1738.882s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)", "comments": ["@shenyufeng1234,\r\nIssue is with Bazel version. Latest Tensorflow version r2.8 requires Bazel 4.2.1 or 5.0.0.\r\nFor Xcode version is 13, download llvm 12 from brew and tried to compile using the llvm 12 as per build configuration. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54258\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54258\">No</a>\n"]}, {"number": 54257, "title": "How to install llvm-project from local ? ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS monterey 12.0.1\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\nTensorFlow installed from (source or binary):source\r\nTensorFlow version: r2.7\r\nPython version: python3.9\r\nInstalled using virtualenv? pip? conda?:pip\r\nBazel version (if compiling from source): bazel 3.7.2\r\nGCC/Compiler version (if compiling from source): Apple clang version 13.0.0 (clang-1300.0.29.3)\r\nCUDA/cuDNN version: no enabled\r\nGPU model and memory:: no enabled(Just CPU model)\r\n\r\n\r\n**Describe the problem**\r\nWhen I run the command \" bazel build --config=dbg --strip=never -c dbg --copt='-g' --cxxopt='-g' //tensorflow/tools/pip_package:build_pip_package\", it would pull package from \"https://github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz\" ,maybe it is too large,  the bazel build  command always failed at this step, so how to install this package from local ? As I can download this package  to local successfully.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nERROR: An error occurred during the fetch of repository 'llvm-raw':\r\n   Traceback (most recent call last):\r\n\tFile \"/Users/xxx/ai/tf/tensorflow/third_party/repo.bzl\", line 64, column 33, in _tf_http_archive_impl\r\n\t\tctx.download_and_extract(\r\nError in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz, https://github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz] to /private/var/tmp/_bazel_shenyufeng/a4eaa5fcc3d32c786eb9bb79fa0163a2/external/llvm-raw/temp10925281720111290723/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz: Premature EOF\r\nERROR: Error fetching repository: Traceback (most recent call last):\r\n\tFile \"/Users/xxxx/ai/tf/tensorflow/third_party/repo.bzl\", line 64, column 33, in _tf_http_archive_impl\r\n\t\tctx.download_and_extract(\r\nError in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz, https://github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz] to /private/var/tmp/_bazel_shenyufeng/a4eaa5fcc3d32c786eb9bb79fa0163a2/external/llvm-raw/temp10925281720111290723/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz: Premature EOF\r\nERROR: no such package '@llvm-raw//utils/bazel': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz, https://github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz] to /private/var/tmp/_bazel_shenyufeng/a4eaa5fcc3d32c786eb9bb79fa0163a2/external/llvm-raw/temp10925281720111290723/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz: Premature EOF\r\nINFO: Elapsed time: 1738.882s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n", "comments": ["@shenyufeng1234 ,\r\nCan you please take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/53503) with the similar error and it is fixed in the latest build.It helps.Thanks!\r\n", "> tilakrayal\r\nThe issue also happened at r2.8, which has the fix.so they are different issue. My question is is there is any way to install the llvm-project from local ? such as download it to local manually and install it from there .\r\n", "@shenyufeng1234 ,\r\nCan you please take a look at this [link](https://llvm.org/docs/CMake.html) to install llvm-project.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54257\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54257\">No</a>\n"]}, {"number": 54256, "title": "Error: Shape error While Using tf.image.multiscale_SSIM", "body": "```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-21-37ba827d3ac9> in <module>()\r\n    11 gan_model = define_gan(g_model, d_model, image_shape)\r\n    12 # train model\r\n---> 13 train(d_model, g_model, gan_model, dataset)\r\n\r\n 2 frames\r\n /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py in assert_is_compatible_with(self, other)\r\n 1169     \"\"\"\r\n 1170     if not self.is_compatible_with(other):\r\n -> 1171       raise ValueError(\"Shapes %s and %s are incompatible\" % (self,other))\r\n 1172 \r\n 1173   def most_specific_compatible_shape(self, other):\r\n\r\n ValueError: Shapes (256, 256, 3) and (16, 16, 1) are incompatible\r\n```", "comments": ["Hi @Hjiwnain ! Could you please update the template with a sample stand alone code too ?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 54255, "title": "InvalidArgumentError:  Size 1 must be non-negative, not -13 \t [[{{node gradient_tape/mean_squared_error/sub-1-ReshapeNHWCToNCHW-LayoutOptimizer}}]] [Op:__inference_train_function_7208]", "body": "I am trying to train my neuronal Network but get this error everytime and i dont know what it means.\r\n\r\nI am using tenserfow 2.7.0 in google colab.\r\nHere is my NN architecture (i am training a CNN to output two informations - steering and speed.\r\nIs something wrong with the mean_squared_error? \r\n`class v2Model():\r\n    def __init__(self):\r\n        self.logdir = os.path.join(TRAIN_LOGS_PATH, datetime.datetime.now().strftime(\"run-V2_%d-%b-%Y__%H-%M-%S_%f\"))\r\n        self.epochs = 10\r\n        self.batchSizeTrain = 100\r\n        self.batchSizeVal = 25\r\n\r\n    def createModel(self) -> None:\r\n        inputs = Input(shape=(80, 320, 1))\r\n        self.model = Rescaling(scale=1./127.5, offset=-1.)(inputs)\r\n        self.model = Conv2D(filters=16, kernel_size=(5, 5), padding='VALID', activation='elu')(self.model)\r\n        self.model = MaxPooling2D(pool_size=(2, 2), padding='VALID')(self.model)\r\n        self.model = Conv2D(filters=32, kernel_size=(3, 3), padding='VALID', activation='elu')(self.model)\r\n        self.model = MaxPooling2D(pool_size=(2, 2), padding='VALID')(self.model)\r\n        self.model = Conv2D(filters=64, kernel_size=(3, 3), padding='VALID', activation='elu')(self.model)\r\n        self.model = MaxPooling2D(pool_size=(2, 2), padding='VALID')(self.model)\r\n        self.model = Dense(19456, use_bias=True)(self.model)\r\n        self.model = Dropout(rate=0.2)(self.model)\r\n        self.model = Dense(500, use_bias=True)(self.model)\r\n        dropV2 = Dropout(rate=0.2)(self.model)\r\n\r\n        headSteeringV2 = Dense(1, activation=\"linear\", name=\"output_ster\")(dropV2)\r\n        headSpeedV2 = Dense(1, activation=\"linear\", name=\"output_acc\")(dropV2)\r\n\r\n        self.model = Model(inputs=inputs, outputs=[headSteeringV2, headSpeedV2])\r\n        #print(self.model.summary())\r\n      \r\n        lossMse = MeanSquaredError()\r\n        optimizer = Adam()\r\n\r\n        self.model.compile(optimizer=optimizer, loss=\"mean_squared_error\", metrics=['accuracy'])\r\n    \r\n    def trainModel(self, x_train, y_train_ster, y_train_acc, x_val, y_val_ster, y_val_acc) -> None:\r\n\r\n        tensorboardCallback = tf.keras.callbacks.TensorBoard(self.logdir, histogram_freq=1)\r\n        earlyStopCallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=2, mode='auto')\r\n        # bei zittern 1e-2, 1e-3\r\n\r\n        stepsPerEpoch = len(x_train) // self.batchSizeTrain\r\n        stepsVal = len(x_val) // self.batchSizeVal\r\n\r\n        self.history = self.model.fit(genData(x_train, {\"output_ster\":y_train_ster, \"output_acc\":y_train_acc}, self.batchSizeTrain, True, \"v2\"),\r\n                                  steps_per_epoch=stepsPerEpoch,\r\n                                  epochs=self.epochs ,\r\n                                  validation_data=genData(x_val, {\"output_ster\": y_val_ster, \"output_acc\": y_val_acc}, self.batchSizeVal, False, \"v2\"),\r\n                                  validation_steps=stepsVal,\r\n                                  callbacks=[tensorboardCallback, earlyStopCallback])\r\n\r\n    def uploadTraining(self):\r\n      !tensorboard dev upload --logdir {self.logdir} \\\r\n        --name f\"Training Results from CNN V2 at {datetime.datetime.now().strftime('%d-%b-%Y__%H-%M-%S')}\" \\\r\n        --description \"Accuracy and loss graphs\" \\\r\n        --one_shot\r\n    \r\n    def saveModel(self):\r\n      self.model.save(\"/content/drive/MyDrive/model-v2.h5\")`\r\n\r\n`\r\nFull Error message.\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n[<ipython-input-22-33a8561955b9>](https://localhost:8080/#) in <module>()\r\n     74 v2CNN = v2Model()\r\n     75 v2CNN.createModel()\r\n---> 76 v2CNN.trainModel(x_train, y_train_ster, y_train_acc, x_val, y_val_ster, y_val_acc)\r\n     77 v2CNN.uploadTraining()\r\n     78 v2CNN.saveModel()\r\n\r\n2 frames\r\n[/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py](https://localhost:8080/#) in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     57     ctx.ensure_initialized()\r\n     58     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 59                                         inputs, attrs, num_outputs)\r\n     60   except core._NotOkStatusException as e:\r\n     61     if name is not None:\r\n\r\nInvalidArgumentError:  Size 1 must be non-negative, not -12\r\n\t [[{{node gradient_tape/mean_squared_error_1/sub-1-ReshapeNHWCToNCHW-LayoutOptimizer}}]] [Op:__inference_train_function_5310]\r\n`", "comments": ["@Salman-F  \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "@sushreebarsa \r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): google colab pre installed\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): no idea\r\n- GCC/Compiler version (if compiling from source): no idea\r\n- CUDA/cuDNN version: google colab pre installed\r\n- GPU model and memory: google colab switches sometimes\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n3. v2.7.0-0-gc256c071bb2 2.7.0\r\n\r\n**Describe the current behavior**\r\nI can't train my CNN because of an error i can't explain (or see where it is)\r\n\r\n**Describe the expected behavior**\r\nThe training of my CNN should start and the epochs should work\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no idea\r\n- Briefly describe your candidate solution(if contributing): no idea\r\n\r\n**Standalone code to reproduce the issue**\r\nHere is my notebook\r\nhttps://gist.github.com/Salman-F/015a7658333e8afe8d6c80c7b8ce1d2a\r\n\r\n**Other info / logs** \r\nEpoch 1/10\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n[<ipython-input-22-33a8561955b9>](https://localhost:8080/#) in <module>()\r\n     74 v2CNN = v2Model()\r\n     75 v2CNN.createModel()\r\n---> 76 v2CNN.trainModel(x_train, y_train_ster, y_train_acc, x_val, y_val_ster, y_val_acc)\r\n     77 v2CNN.uploadTraining()\r\n     78 v2CNN.saveModel()\r\n\r\n2 frames\r\n[/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py](https://localhost:8080/#) in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     57     ctx.ensure_initialized()\r\n     58     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 59                                         inputs, attrs, num_outputs)\r\n     60   except core._NotOkStatusException as e:\r\n     61     if name is not None:\r\n\r\nInvalidArgumentError:  Size 1 must be non-negative, not -12\r\n\t [[{{node gradient_tape/mean_squared_error_1/sub-1-ReshapeNHWCToNCHW-LayoutOptimizer}}]] [Op:__inference_train_function_5310]\r\n", "Sorry for not filling it out immediately :) \r\n@sushreebarsa -- The problem just appears when i try to train the v2 CNN. The CNN v1 is working", "@Salman-F Thank you for the update!\r\nI tried to run your code on colab using TF v2.7.0,2.8.0  and faced a different error .Could you please have a look at the [gist1](https://colab.research.google.com/gist/sushreebarsa/d5c3ca1517f7466c011b746dab8ab22d/trainingv2.ipynb#scrollTo=BThu9cCBrpIt),  [gist2](https://colab.research.google.com/gist/sushreebarsa/90752372d189019a6aabbe02281694d7/trainingv2.ipynb#scrollTo=dbSAFS9wp0gI) and confirm the same? Let me know if I'm missing something to replicate the issue.Thanks!", "@sushreebarsa Hi, yes i think you are missing my data. I can try to give you some data that you have to load into your google colab enviorment. A problem may accur with the paths i decleared at the beginning. You have to change them to access the data (or if you put them in your google drive  and conect it to google colab it should be the same)\r\nDownload a few data samples here: https://nextcloud.dhbw-stuttgart.de/index.php/s/akmrSWTFA6zDiQm\r\nMaybe you can also create some dummy data.\r\nPlease feel free to contact me, if you need more :)", "@gadagashwini Was able to reproduce the issue on colab using TF v[2.7.0](https://colab.research.google.com/gist/sushreebarsa/1158d73576da91e6fc8d164de8172fc5/gist54255.ipynb) and [2.8.0](https://colab.research.google.com/gist/sushreebarsa/52ff328b88545cfecb6ca410c81408c2/gist54255.ipynb) ,please find the attached gists for reference.Thanks!", "@Salman-F, Looks like issue is with your input data shape\r\nI tried to replicate your issue with minimal code. Below sample code works\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras import datasets\r\nimport tensorflow.keras as keras\r\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dropout, Dense, Rescaling\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.losses import MeanSquaredError\r\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\r\nfrom tensorflow.keras.models import Model\r\n\r\n\r\nX_train = np.random.random((235,65, 320, 1))\r\ny_train = np.random.random((235, 65, 320, 1))\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\r\ntrain_data = dataset.shuffle(len(X_train)).batch(32)\r\ntrain_data = train_data.prefetch(\r\n        buffer_size=tf.data.experimental.AUTOTUNE)\r\n\r\ninput_shape = (65, 320, 1)\r\ninitializer = tf.keras.initializers.HeUniform()\r\n\r\nmodel = tf.keras.Sequential(\r\n[\r\n    tf.keras.Input(shape=(65, 320, 1)),\r\n    keras.layers.Rescaling(1./125,offset=1., input_shape=(65,320,1)),\r\n    keras.layers.Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu', kernel_initializer=initializer),\r\n    keras.layers.Dense(20, use_bias=True),\r\n    keras.layers.Dropout(rate=0.7),\r\n    keras.layers.Dense(1, activation='linear')\r\n])\r\n\r\nmodel.summary()\r\nlossMse = MeanSquaredError()\r\nmodel.compile(optimizer='adam',\r\n                loss=lossMse,\r\n                metrics=['accuracy'])\r\n\r\nmodel.fit(X_train, y_train, epochs=5, verbose=1)\r\n```\r\n\r\n**Output**\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\n Layer (type)                Output Shape              Param #   \r\n=================================================================\r\n rescaling (Rescaling)       (None, 65, 320, 1)        0         \r\n                                                                 \r\n conv2d (Conv2D)             (None, 65, 320, 16)       416       \r\n                                                                 \r\n dense (Dense)               (None, 65, 320, 20)       340       \r\n                                                                 \r\n dropout (Dropout)           (None, 65, 320, 20)       0         \r\n                                                                 \r\n dense_1 (Dense)             (None, 65, 320, 1)        21        \r\n                                                                 \r\n=================================================================\r\nTotal params: 777\r\nTrainable params: 777\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nEpoch 1/5\r\n8/8 [==============================] - 6s 47ms/step - loss: 1.7397 - accuracy: 0.0000e+00\r\nEpoch 2/5\r\n8/8 [==============================] - 0s 25ms/step - loss: 0.7736 - accuracy: 0.0000e+00\r\nEpoch 3/5\r\n8/8 [==============================] - 0s 23ms/step - loss: 0.5147 - accuracy: 0.0000e+00\r\nEpoch 4/5\r\n8/8 [==============================] - 0s 22ms/step - loss: 0.4024 - accuracy: 0.0000e+00\r\nEpoch 5/5\r\n8/8 [==============================] - 0s 26ms/step - loss: 0.3066 - accuracy: 0.0000e+00\r\n<keras.callbacks.History at 0x7f66000bcb90>\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54255\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54255\">No</a>\n"]}, {"number": 54253, "title": "TensorFlow Lite kernel tests CMake build fix - duplicated symbols", "body": "Fixing the kernel tests CMake build fix (duplicated symbols) introduced as a side-effect of https://github.com/tensorflow/tensorflow/commit/4c09a48c7129ed448c36f8dae82b3e5f54d486d8.\r\n", "comments": ["@madaosik Can you please resolve conflicts? Thanks!"]}, {"number": 54252, "title": "Update ops.py", "body": "In this removing `'s'` doesn't make any change in the explanation", "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac, @ymodak \r\n"]}, {"number": 54250, "title": "Isn't tf.compat.v1.get_variable the default trainable variable?", "body": "I am implementing deep learning model using tensorflow keras where I need \u03b3 learnable parameter. For that I am utilizing\r\n\r\n`gamma = tf.compat.v1.get_variable(\"gamma\", [1], initializer=tf.constant_initializer(0.0))`\r\n\r\nI read the [Official Documentation of tf.compat.v1.get_variable](https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable) but could not get the complete idea. In the official documentation it seems that by default this variable is non trainable. am I right? If yes then should I need to true this function in order to make trainable, like:\r\n` gamma = tf.compat.v1.get_variable(\"gamma\", [1], trainable=True, initializer=tf.constant_initializer(0.0))`\r\n\r\nCould someone please clear it?", "comments": ["Hi @Nafees-060 ! I agree with your point on getting trainable variables by enabling **trainable=True**.  Have attached a [thread ](https://www.programcreek.com/python/example/112642/tensorflow.compat.v1.get_variable)for comparing different use cases on tf.compat.v1.get_variable(). Thanks!", "> Hi @Nafees-060 ! I agree with your point on getting trainable variables by enabling **trainable=True**. Have attached a [thread ](https://www.programcreek.com/python/example/112642/tensorflow.compat.v1.get_variable)for comparing different use cases on tf.compat.v1.get_variable(). Thanks!\r\n\r\nOkay, thanks. May I know the difference between `initializer=v.initial_value` and  `initializer=tf.constant_initializer(0.0)`", "> Hi @Nafees-060 ! I agree with your point on getting trainable variables by enabling **trainable=True**. Have attached a [thread ](https://www.programcreek.com/python/example/112642/tensorflow.compat.v1.get_variable)for comparing different use cases on tf.compat.v1.get_variable(). Thanks!\r\n\r\nOOPs, I just tried trainable=True, and the number of trainable parameters remains the same as it was previously. Shouldn't the total number of parameters be increased by one additional learnable parameter?", "@Nafees-060 ! Number of trainable parameters will remain same but their values will vary once set trainable=True. Regarding this [query](https://github.com/tensorflow/tensorflow/issues/54250#issuecomment-1028719878) , Please visit [tf.constant.initializer ](https://www.tensorflow.org/api_docs/python/tf/constant_initializer)and [tf.variable](https://www.tensorflow.org/api_docs/python/tf/Variable#args) to get more details.                                                                   \r\nPlease move this issue to closed status if it helped .Thank you!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 54248, "title": "Tensorflow accesses .hdf5 weights file after it's been loaded", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux - Redhat 7.9\r\n- TensorFlow installed from (source or binary): binary (conda)\r\n- TensorFlow version (use command below): On all versions from 2.2 to 2.7 (2.7.0 for the latest)\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.2.0\r\n\r\n**Describe the current behavior**\r\n\r\nWhen loading a keras model from a temporary `.hdf5` weights file, the weights are read successfully read into the model, but Tensorflow outputs a DATA_LOSS warning after the `load_weights` method has finished executing indicating that Tensorflow is still trying to read the file.\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect Tensorflow to not touch the weights file once the `load_weights` function as returned.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport tempfile\r\nimport os\r\nimport shutil\r\n\r\n# Load MNIST Datasets\r\nprint(\"Load MNIST Dataset\")\r\nmnist_datasets, ds_info = tfds.load('mnist', as_supervised=True, with_info=True)\r\ntrain_ds = mnist_datasets['train']\r\ntest_ds = mnist_datasets['test']\r\n\r\nbatch_size = 32\r\n\r\ndef normalize_img(image, label):\r\n  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\r\n  return tf.cast(image, tf.float32) / 255., label\r\n\r\ntry:\r\n    autotune_opt = tf.data.AUTOTUNE\r\nexcept:\r\n    autotune_opt = tf.data.experimental.AUTOTUNE\r\n\r\ntrain_ds = train_ds.map(\r\n    normalize_img, num_parallel_calls=autotune_opt)\r\ntrain_ds = train_ds.cache()\r\ntrain_ds = train_ds.shuffle(ds_info.splits['train'].num_examples)\r\ntrain_ds = train_ds.batch(batch_size)\r\n\r\ntest_ds = test_ds.map(\r\n    normalize_img, num_parallel_calls=autotune_opt)\r\ntest_ds = test_ds.batch(batch_size)\r\ntest_ds = test_ds.cache()\r\ntest_ds = test_ds.prefetch(autotune_opt)\r\n\r\n# Define function to create functional keras model\r\ndef create_keras_model(layer_dims):\r\n    inp = tf.keras.layers.Input((28,28,1), name=\"Input_Name\")\r\n    last_layer = inp\r\n    last_layer = tf.keras.layers.Reshape((28*28*1,), name=\"Input_Reshape\")(last_layer)\r\n    for i in range(len(layer_dims)):\r\n        dim = layer_dims[i]\r\n        last_layer = tf.keras.layers.Dense(dim, name=f\"Layer_{i}\")(last_layer)\r\n    outp = tf.keras.layers.Dense(10, name=\"Output_Layer\", activation='sigmoid')(last_layer)\r\n\r\n    return tf.keras.Model(inputs=inp, outputs=outp)\r\n\r\n# Create and train initial model:\r\n\r\nprint(\"Create and train initial keras model\")\r\nmdl_1 = create_keras_model([32,32,32])\r\n\r\nmdl_1.compile(\r\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\r\n    metrics=tf.keras.metrics.SparseCategoricalAccuracy())\r\n\r\nmdl_1.fit(train_ds, epochs=1)\r\n\r\n# Create function to test models\r\ndef eval_on_test(mdl):\r\n    num_matches = 0\r\n    total_examples = 0\r\n\r\n    for X, Y in test_ds:\r\n        # Eval model\r\n        Y_eval = tf.argmax(mdl(X),axis=1).numpy()\r\n\r\n        # Count\r\n        total_examples += len(Y_eval)\r\n        num_matches += (Y.numpy() == Y_eval).sum()\r\n\r\n    return num_matches/total_examples\r\n\r\nprint(\"Initial model performance\")\r\nprint(eval_on_test(mdl_1))\r\n\r\n# Save weights to disk\r\nmdl_weight_file = 'weights.hdf5'\r\nmdl_1.save_weights(mdl_weight_file)\r\n\r\n# Create new model and load from a temporary file\r\nmdl_2 = create_keras_model([32,32,32])\r\n\r\nwith tempfile.NamedTemporaryFile('w+b') as temp_f:\r\n    # Copy content to temporary file\r\n    print(\"Copy weights to temp file\")\r\n    with open(mdl_weight_file, 'rb') as f:\r\n        temp_f.write(f.read())\r\n\r\n    # Flush to disk\r\n    temp_f.flush()\r\n\r\n    print(\"Load weights from temp file\")\r\n    # Load weights from temp file\r\n    mdl_2.load_weights(temp_f.name)\r\n\r\nprint(\"temp file gone\")\r\n\r\nprint(\"Loaded model performance\")\r\nprint(eval_on_test(mdl_2))\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThis the output I get by running this code locally:\r\n\r\n```\r\nLoad MNIST Dataset\r\n2022-02-02 15:45:23.299505: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2022-02-02 15:45:23.299702: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (iforge123): /proc/driver/nvidia/version does not exist\r\nCreate and train initial keras model\r\n1875/1875 [==============================] - 6s 2ms/step - loss: 0.3753 - sparse_categorical_accuracy: 0.8929 \r\nInitial model performance\r\n0.9172\r\nCopy weights to temp file\r\nLoad weights from temp file\r\n2022-02-02 15:45:32.765376: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open /tmp/tmpfpsuz_25: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\r\ntemp file gone\r\nLoaded model performance\r\n0.9172\r\n```\r\n\r\nI tried running this code on colab, however I can't see any of the tensorflow logging lines I see on my local machine.", "comments": ["This might be expected behavior, but then I'd like a way to determine that tensorflow is finished reading the file before releasing the temporary file.", "@krafczyk ,\r\nI was able to execute the mentioned code with out any Data loss warnings/issues.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/4f18c11a8f4c2d065dbb0bc27dce8e10/untitled211.ipynb).", "@tilakrayal Yes, I think its because none of the TF logging messages are visible on colab. I ran it on colab before submitting as well, and also didn't see the warnings. That said, every version of tensorflow I've tried on my local machine produces this error message.", "@tilakrayal I worked around the colab logging issue by first writing the script to a file, and then using `!` magic commands to execute the script. If you know a proper way to show logging on colab please let me know.\r\n\r\nHere is my [gist](https://colab.research.google.com/drive/1JE6NuBB2oJ_0MMiEcomP4kfXJOidp9Pd?usp=sharing)", "@chunduriv ,\r\nI was able to reproduce the issue in tf v2.5, v2.7 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/56e11b6f728e3449c61a90a9c1a6456a/untitled214.ipynb).", "@krafczyk,\r\n\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999).Thanks!", "@chunduriv Sure. Since the error message is originating from the tensorflow C library, i figured this would be a better place to start.", "I've opened the issue at keras-team/keras here: keras-team/keras#16048", "@krafczyk, Can you please close this issue, since it is tracked [there](https://github.com/keras-team/keras/issues/16048). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54248\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54248\">No</a>\n"]}, {"number": 54247, "title": "Properly set the bounds for estimator", "body": "Not affecting TF 2.8 release much (as the nightly that we pinned to is the same we build 2.8 from) but we should fix.", "comments": []}, {"number": 54245, "title": "ResourceExhaustedError", "body": "Hey everyone. I am training resnet50v2 on my dataset, I have 18746 images in the training set and 4000+ images in the validation set. I am using a batch size of 32. The first time I tried to train my model with this batch size, it worked. When I try to train my model it gave me the following error this time.\r\n![error](https://user-images.githubusercontent.com/61932757/152167745-4ce0449f-6530-41d7-b4c3-b1afd00d82e7.png)\r\n\r\nI tried a lot, but nothing worked for me.", "comments": ["sorry this is the error.\r\n![eror](https://user-images.githubusercontent.com/61932757/152168025-a31d73a4-786e-44cc-991b-50694ce1e335.png)\r\nplease see this, i uploaded the above picture mistakenly.", "@faizan1234567 could you please tell your system and software specifications?", "I am using google colab", "You can refer to this answer here: https://stackoverflow.com/questions/59394947/how-to-fix-resourceexhaustederror-oom-when-allocating-tensor/59395251 <br> This is basically because your colab gpu is running out of memory for your use case", "@faizan1234567 Could you please refer this [thread](https://www.kaggle.com/questions-and-answers/62946) , similar [issue](https://github.com/tensorflow/models/issues/8487) ,and also have a look at the above comment ?Please let us know if it helps?\r\nFor any further queries please post this issue on TF [forum](https://discuss.tensorflow.org/) where there is a larger community to help.\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 54243, "title": "Tenserflow recommenders : tfrs.metrics.FactorizedTopK", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Colab**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n**2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`**\r\n\r\n**Describe the current behavior**\r\n\r\n![image](https://user-images.githubusercontent.com/57206771/152147536-8bedf93a-5594-4c46-af94-55ba8b88e812.png)\r\n\r\n```\r\n\r\n!pip install -q tensorflow-recommenders\r\n!pip install -q --upgrade tensorflow_addons\r\n!pip install -q --upgrade tensorflow-datasets\r\n!pip install -q scann\r\n\r\nimport os\r\nimport pprint\r\nimport tempfile\r\nfrom typing import Dict, Text\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow_recommenders as tfrs\r\n\r\nimport tensorflow_addons as tfa\r\nfrom tqdm.keras import TqdmCallback\r\nfrom tensorflow.keras import callbacks as tf_callbacks\r\n\r\n# === \uc5f0\uc2b5\uc6a9 Rawdata \ub85c\ub529 ===\r\n# \ud574\ub2f9 \uc608\uc81c\uc5d0\uc11c\ub294 ratings \ud14c\uc774\ube14\uc5d0 \uc788\ub294 \ub370\uc774\ud130\ub97c '\uc720\uc800\uac00 \ud574\ub2f9 \uc601\ud654\ub97c \ubcf4\uc558\ub2e4'\ub294 \uac83\uc73c\ub85c \uac04\uc8fc\ud558\uace0\r\n# Ratings data.\r\nratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\r\n# Features of all the available movies.\r\nmovies = tfds.load(\"movielens/100k-movies\", split=\"train\")\r\n\r\nratings = ratings.map(lambda x: {\r\n    \"movie_title\": x[\"movie_title\"],\r\n    \"user_id\": x[\"user_id\"],\r\n})\r\nmovies = movies.map(lambda x: x[\"movie_title\"])\r\n\r\n# embedding \uc2dc, vocabulary size \uc124\uc815\uc744 \uc704\ud574\r\n# user \uc218\uc640 movie \uc218 \ud30c\uc545\r\nmovie_titles = movies\r\nuser_ids = ratings.map(lambda x: x[\"user_id\"])\r\n# movie_titles = movies\r\n# user_ids = ratings.map(lambda x: x[\"user_id\"])\r\n\r\nunique_movie_titles = np.unique(np.concatenate(list(movie_titles.batch(1))))\r\nunique_user_ids = np.unique(np.concatenate(list(user_ids.batch(1))))\r\n\r\nembedding_dimension = 64\r\n\r\nuser_model = tf.keras.Sequential([\r\n  # embedding layer\uc5d0 \ub300\ud55c input \uc6a9 \uc774\ubbc0\ub85c \ub808\uc774\ube14\uc778\ucf54\ub529 \uc218\ud589\r\n  tf.keras.layers.StringLookup(vocabulary=unique_user_ids, output_mode=\"int\", mask_token=None),\r\n  # We add an additional embedding to account for unknown tokens.\r\n  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension),\r\n  tf.keras.layers.Dense(32, activation=\"relu\")\r\n])\r\n\r\nmovie_model = tf.keras.Sequential([\r\n  # embedding layer\uc5d0 \ub300\ud55c input \uc6a9 \uc774\ubbc0\ub85c \ub808\uc774\ube14\uc778\ucf54\ub529 \uc218\ud589\r\n  tf.keras.layers.StringLookup(vocabulary=unique_movie_titles, mask_token=None),\r\n  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension),\r\n  tf.keras.layers.Dense(32, activation=\"relu\")\r\n])\r\n\r\n# learning parameter setting\r\nbatch_size = 1024\r\neta = 1e-3\r\nweight_decay = 1e-5\r\n# model_save_flag = False\r\n# checkpoint_filepath = folder_path + 'models/tmp_checkpoint/'\r\n\r\ntask = tfrs.tasks.Retrieval(\r\n  loss=tf.keras.losses.CategoricalCrossentropy(),\r\n  metrics=tfrs.metrics.FactorizedTopK(candidates=movies.batch(batch_size).map(movie_model), k=200, name=\"top_k_acc\")\r\n)\r\n\r\ncb_reduceLR = tf_callbacks.ReduceLROnPlateau(patience=1, factor=0.6, min_lr=1e-7)\r\ncb_earlyStopping = tf_callbacks.EarlyStopping(patience=10, monitor='val_top_k_acc/top_100_categorical_accuracy', mode='max')\r\n\r\ntf.random.set_seed(42)\r\nshuffled = ratings.shuffle(batch_size, seed=42, reshuffle_each_iteration=True)\r\n\r\ntrain = shuffled.take(80000)\r\ntest = shuffled.skip(80000).take(2000)\r\n\r\ncached_train = train.shuffle(batch_size, seed=42, reshuffle_each_iteration=True).batch(batch_size).prefetch(tf.data.AUTOTUNE)\r\ncached_test = test.batch(batch_size).prefetch(tf.data.AUTOTUNE)\r\n\r\n\r\n# tfrs.Model \ud074\ub798\uc2a4\ub97c \uc0c1\uc18d\ubc1b\uc544 \ubaa8\ub378 \ube4c\ub4dc\r\nclass MovielensModel(tfrs.Model):\r\n\r\n  def __init__(self, user_model, movie_model):\r\n    super().__init__()\r\n    self.movie_model: tf.keras.Model = movie_model\r\n    self.user_model: tf.keras.Model = user_model\r\n    self.task: tf.keras.layers.Layer = task\r\n\r\n  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\r\n    # We pick out the user features and pass them into the user model.\r\n    user_embeddings = self.user_model(features[\"user_id\"])\r\n    # And pick out the movie features and pass them into the movie model,\r\n    # getting embeddings back.\r\n    positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\r\n\r\n    # The task computes the loss and the metrics.\r\n    return self.task(user_embeddings, positive_movie_embeddings)\r\n\r\nmodel = MovielensModel(user_model, movie_model)\r\n\r\nmodel.compile(\r\n    optimizer=tfa.optimizers.AdamW(learning_rate=eta, weight_decay=weight_decay),\r\n    # loss=tf.keras.losses.CategoricalCrossentropy(),\r\n    # metrics=tfrs.metrics.FactorizedTopK(candidates=movies.batch(batch_size).map(movie_model), k=200, name=\"top_k_acc\")\r\n)\r\nhistory = model.fit(\r\n  cached_train, validation_data=cached_test, epochs=30, verbose=0,\r\n  callbacks=[cb_reduceLR, cb_earlyStopping, TqdmCallback(verbose=0)]\r\n)\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n**I wannted to use the tfrs.metrics.FactorizedTopK(**k=200**).\r\nbut like above, the output shows the result only with k=1,5,10,100 (always 4 cases).**\r\n\r\n**Describe the expected behavior**\r\n\r\n**I want to see the result with k=200**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi @Cafelatte1 ! Could you please update complete stand alone code instead of screenshot? Thanks!", "> Hi @Cafelatte1 ! Could you please update complete stand alone code instead of screenshot? Thanks!\r\n\r\nI updated my issue ! I attached the codes which is a little transformed from tensorflow recommender tutorial.", "Hi @Cafelatte1 ! Could you please post this in [tfrs ](https://github.com/tensorflow/recommenders/issues)repo ? ", "> Hi @Cafelatte1 ! Could you please post this in [tfrs ](https://github.com/tensorflow/recommenders/issues)repo ?\r\n\r\nOkay ! Thanks", "Ok @Cafelatte1 ! Closing this issue here then. Thanks!"]}, {"number": 54241, "title": "FIFOQueue as a part of custom layer", "body": "Hello,\r\n\r\n   I need a queue as a part of my custom input layer in Transformer for Short-term memory purposes. I need to store the coming features \"in first in first out\" manner and remove the oldies features when it's full and clean all content if it's needed. I see [here](https://www.tensorflow.org/api_docs/python/tf/queue/FIFOQueue) something like that for working with Tensor, but is it a good choice for me and contains all that I need? Is the FIFOQueue faster than using Python's deque or Numpy based queue directly in a custom layer?\r\n\r\nCompare Python's deque collection with TF's FIFOQueue:\r\n| Python | TensorFlow |\r\n|--------|-------------|\r\n| deque(maxlen=max_size) | tf.queue.FIFOQueue(capacity, dtypes, shapes=None, names=None, shared_name=None)\r\n| append(x)<br># if it's full discard from the left end | enqueue(vals) |\r\n| clear() | ??? |\r\n| len(queue) | size() |\r\n| pop() | dequeue() |\r\n| queue[i] | ??? |\r\n\r\nOr alternatively do it with Numpy like:\r\n```python\r\nqueue = np.zeros((maxlen, features)) # my queue\r\nx = np.random.normal(size=features) # new features\r\n\r\n# append()\r\nqueue = np.concatenate([queue[:-1], x[np.newaxis, :]])\r\n\r\n# predict\r\ny = model(queue[np.newaxis, :, :])\r\n```\r\n\r\nWith Python's timeit I get a benchmark on 100000 cycles with maxlen=1000 and features=6:\r\n| Method | Time |\r\n|--------|-------------|\r\n| Numpy | 0.1765849579999994  s |\r\n| deque | 12.360834958000002  s |\r\n\r\nThanks, have a nice day.", "comments": ["@fchollet ", "Thanks for opening this feature request. Development of keras moved to separate repository https://github.com/keras-team/keras/issues\r\n\r\nPlease post this issue on keras-team/keras repo.\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThank you!", "Okej, I create a feature request there: https://github.com/keras-team/keras/issues/16015.\r\n\r\nHave a nice day."]}, {"number": 54240, "title": "checking pid of the process using tpu", "body": null, "comments": ["@skye Could you please review this?", "@sshahrokhi  Can you please check @skye's comments and keep us posted ? Thanks!", "I don't know how I missed the email with @skye's comments on this PR, but thanks a lot for all the great reviews! ", "I added a two more comments. Feel free to resolve, but this is also fine as-is.", "Thanks @skye for the comments, I applied them. "]}]