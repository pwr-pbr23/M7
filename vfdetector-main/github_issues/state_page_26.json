[{"number": 50436, "title": "DistributedDataset does not work on Python 3.8", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `no`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `docker ubuntu:focal`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `no`\r\n- TensorFlow installed from (source or binary): `binary`\r\n- TensorFlow version (use command below): `v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0`\r\n- Python version: `3.8.5`\r\n- Bazel version (if compiling from source): `n/a`\r\n- GCC/Compiler version (if compiling from source): `n/a`\r\n- CUDA/cuDNN version: `n/a`\r\n- GPU model and memory: `n/a`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I use Python 3.8.5,\r\n\r\nMirroredStrategy:\r\n\r\n```\r\nException ignored in: <function Pool.__del__ at 0x7fbd5c04d280>\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 268, in __del__\r\n  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 362, in put\r\nAttributeError: 'NoneType' object has no attribute 'dumps'\r\n```\r\n\r\nMultiWorkerMirroredStrategy:\r\n\r\n```\r\nterminate called without an active exception\r\n```\r\n\r\nNothing happens in Python 3.6.9.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe process exits with code 0.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport sys\r\nimport tensorflow as tf\r\n\r\n\r\nprint(sys.version_info)\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\n\r\n\r\ndef distribute_dataset_fn(input_context: tf.distribute.InputContext):\r\n  return tf.data.Dataset.from_tensors(([1.0])).repeat(64).batch(2)\r\n\r\n\r\n@tf.function\r\ndef step(x):\r\n  return x\r\n\r\n\r\nif __name__ == '__main__':\r\n  strategy = tf.distribute.MultiWorkerMirroredStrategy()\r\n  #strategy = tf.distribute.MirroredStrategy()\r\n  with strategy.scope():\r\n    dist_ds = strategy.distribute_datasets_from_function(distribute_dataset_fn)\r\n\r\n  for x in dist_ds:\r\n    strategy.run(step, args=[x])\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n2021-06-24 04:46:17.224210: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nWARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\nWARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\r\n2021-06-24 04:46:17.280529: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\r\n2021-06-24 04:46:17.281658: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2199970000 Hz\r\nsys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)\r\nv2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\nException ignored in: <function Pool.__del__ at 0x7fbd5c04d280>\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 268, in __del__\r\n  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 362, in put\r\nAttributeError: 'NoneType' object has no attribute 'dumps'\r\n```\r\n\r\n```\r\n2021-06-24 04:52:19.023657: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-06-24 04:52:19.031611: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> tf-multi-worker-v5xfh-worker-0.pipelines.svc:2222, 1 -> tf-multi-worker-v5xfh-worker-1.pipelines.svc:2222}\r\n2021-06-24 04:52:19.033039: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://tf-multi-worker-v5xfh-worker-0.pipelines.svc:2222\r\n2021-06-24 04:52:20.069924: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\r\n2021-06-24 04:52:20.071033: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2199970000 Hz\r\nsys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)\r\nv2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\nterminate called without an active exception\r\n```", "comments": ["@ornew Can you please explain details about your setup like how many workers? GPUs? CPUs? I tried on a system with single GPU and don't face that error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/996daf2a6ddf9f813c16bb7519845893/untitled.ipynb). Thanks!", "Please see attached logs. now I'm using tensorflow-cpu binary, on two workers with grpc ring communications.\r\n\r\nI'm using kubeflow for running:\r\n\r\n```yaml\r\napiVersion: kubeflow.org/v1\r\nkind: TFJob\r\nmetadata:\r\n  generateName: tf-multi-worker-\r\nspec:\r\n  backoffLimit: 0\r\n  tfReplicaSpecs:\r\n    Worker:\r\n      replicas: 2\r\n      restartPolicy: Never\r\n      template:\r\n        spec:\r\n          containers:\r\n            - name: tensorflow\r\n              image: <python 3.8 base image>\r\n              command: [\"/bin/bash\"]\r\n              args:\r\n                - -exc\r\n                - python3 /example.py\r\n```"]}, {"number": 50424, "title": "[TFLite] Quantized int8/16 support for TILE operator", "body": "Added quantized int8 and int16x8 support for TILE operator.", "comments": ["Can you add a test in tensorflow/lite/testing/op_tests/tile.py with \"fully_quantize\"  equals true.\r\n", "@georgeedward2000  Can you please check @thaink's comments and keep us posted ? Thanks!", "@gbaned @thaink I will add a test and update the PR as fast as possible. Thanks!", "I am OK with the kernel changes but would like to have someone's opinion on tensorflow/lite/tools/optimize/operator_property.cc.", "@thaink @gbaned I updated the PR. Thanks!", "Hi Tei, Could you take a look at the quantization-related part?\r\n", "@georgeedward2000 Can you please approve this PR by click on Approve and run button.  Thank you.", "@gbaned The \"approve\" button is disabled and has the pop up message \"pull request author cannot approve their own pull request\". What should I do regarding it? Thanks!", "> @gbaned The \"approve\" button is disabled and has the pop up message \"pull request author cannot approve their own pull request\". What should I do regarding it? Thanks!\r\n\r\n@georgeedward2000  Sorry, nothing to do from your end. I have clicked on Approve and run button. Thank you.\r\n", "@teijeong  Can you please review this PR ? Thanks!", "@teijeong Can you please review this PR ? Thanks!", "@thaink It seems int8 support for the TILE operator was added today by commit https://github.com/tensorflow/tensorflow/commit/2145b1da6f70da8cb27d57359dde59450ee33dab Unfortunately int16 is still missing, should we update this PR to only add int16 support or are there plans on your side to add it? We'd like to avoid doing duplicate work on the operator. \r\n\r\nThanks.", "Right. It was added today. Could you update this PR for int16 support only. I'll quickly review the PR after that.", "@georgeedward2000 Can you please check @thaink's comments and resolve conflicts?. Thanks!", "@georgeedward2000 Can you please resolve conflicts? Thanks!", "> @georgeedward2000 Can you please resolve conflicts? Thanks!\r\n\r\nHi, the merge conflicts have now been resolved.\r\nBest regards,\r\nSaoirse"]}, {"number": 50423, "title": "\u201cCloud TPU\u201d console spam on every TensorFlow import", "body": "**System information**\r\n\r\n  *  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n  *  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n  *  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n  *  TensorFlow installed from (source or binary): binary\r\n  *  TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n  *  Python version: Python 3.9.5\r\n  *  Bazel version (if compiling from source): N/A\r\n  *  GCC/Compiler version (if compiling from source): N/A\r\n  *  CUDA/cuDNN version: N/A\r\n  *  GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nImporting TensorFlow prints an unnecessary and unhelpful warning:\r\n\r\n   ` tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.`\r\n\r\n**Describe the expected behavior**\r\n\r\nImporting TensorFlow should not print any messages about Cloud TPUs.\r\nThis is a normal desktop installation that doesn\u2019t have anything to do\r\nwith TPUs, and doesn\u2019t need them.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\n$ python -c \"import logging; logging.basicConfig(level=logging.DEBUG); import tensorflow\"\r\nDEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\r\nDEBUG:h5py._conv:Creating converter from 7 to 5\r\nDEBUG:h5py._conv:Creating converter from 5 to 7\r\nDEBUG:h5py._conv:Creating converter from 7 to 5\r\nDEBUG:h5py._conv:Creating converter from 5 to 7\r\n\r\n```\r\n**Other info / logs**\r\n\r\nLikely introduced by 5364121.\r\n\r\n\r\nThe commit that supposedly fixes #35547 doesn't actually fixes the issue but only changes the log level from warning to debug.\r\n\r\nIt should instead not print any messages on a non TPU environment.\r\n\r\n\r\nThanks for your report! It should be fixed starting with the nightly build tomorrow morning (`2.1.0-dev20200104`).\r\n\r\n_Originally posted by @frankchn in https://github.com/tensorflow/tensorflow/issues/35547#issuecomment-570694131_", "comments": ["@jpfarias \r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\nThanks\r\n", "> \r\n> \r\n> @jpfarias\r\n> \r\n> We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n> Thanks\r\n\r\nI updated the issue to match the template\r\n", "Got the same info message on `TensorFlow 2.5.0`. \r\nIt is quite annoying to get debug messages of something that I have never asked to see. \r\n\r\nWhy is it using the `logging` and not the `standard tensorflow logger`? "]}, {"number": 50418, "title": "TFlite memory persistence", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS (GNU/Linux 5.4.0-73-lowlatency x86_64)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Desktop\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: N/A\r\n- Bazel version (if compiling from source): 4.0.0\r\n- GCC/Compiler version (if compiling from source): GCC 10.2.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nDoing multiple inference runs directly after each other indicates good speed (after the initial warmup run). However, if I add a 5 seconds delay between inference runs, the speed is much worse. My guess is that this has something to do with memory persistence?\r\n\r\nNo delay between inference runs:\r\n```\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nPrediction 1: 105 us\r\nPrediction 2: 32 us\r\nPrediction 3: 26 us\r\nPrediction 4: 23 us\r\nPrediction 5: 25 us\r\n```\r\n\r\n5 seconds delay between inference runs:\r\n```\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nPrediction 1: 72 us\r\nPrediction 2: 94 us\r\nPrediction 3: 97 us\r\nPrediction 4: 192 us\r\nPrediction 5: 100 us\r\n```\r\n\r\n**Describe the expected behavior**\r\nInference speed should be similar even if there is a few seconds delay between runs.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n#include <tensorflow/lite/interpreter.h>\r\n#include <tensorflow/lite/model.h>\r\n#include <tensorflow/lite/kernels/register.h>\r\n#include <iomanip>\r\n#include <iostream>\r\n#include <mutex>\r\n#include <fstream>\r\n#include <chrono>\r\n#include <thread>\r\n\r\nstd::mutex mtx;\r\n\r\nusing namespace std;\r\nusing namespace std::chrono_literals;\r\nusing std::chrono::high_resolution_clock;\r\nusing std::chrono::duration_cast;\r\nusing std::chrono::duration;\r\nusing std::chrono::microseconds;\t\r\n\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\nstd::unique_ptr<tflite::FlatBufferModel> model;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\n\r\nfloat predict() {\r\n\tstd::scoped_lock lock{mtx};\r\n\tint* input;\r\n\tfloat* output; \r\n\t\r\n\tinput = interpreter->typed_input_tensor<int>(0);\r\n\t\r\n\tfor (int i=0; i<20; ++i) {\r\n\t\tinput[i] = rand()%((25000 - 0) + 1) + 0; \r\n\t}\r\n\t\r\n\tinterpreter->Invoke();\r\n\t\r\n\toutput = interpreter->typed_output_tensor<float>(0);\r\n\r\n\treturn output[0];\r\n}\r\n\r\nvoid init_model(){\r\n\r\n\tmodel = tflite::FlatBufferModel::BuildFromFile(\"/home/gustaf/cprojects/pacific/nlp/sv/model.tflite\");\r\n\r\n\ttflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n\r\n\t//interpreter->SetNumThreads(1);\r\n\r\n\tinterpreter->AllocateTensors();\r\n\t\r\n\tinterpreter->Invoke();\r\n\r\n}\r\n\r\n\r\nint main (){\r\n\t\r\n\tinit_model();\r\n\t\r\n\tint delay = 5;\r\n\t\r\n\tauto t1 = high_resolution_clock::now();\r\n\tpredict();\r\n\tauto t2 = high_resolution_clock::now();\r\n\t\r\n\tstd::this_thread::sleep_for(std::chrono::seconds(delay));\r\n\t\r\n\tauto t3 = high_resolution_clock::now();\r\n\tpredict();\r\n\tauto t4 = high_resolution_clock::now();\r\n\t\r\n\tstd::this_thread::sleep_for(std::chrono::seconds(delay));\r\n\t\r\n\tauto t5 = high_resolution_clock::now();\r\n\tpredict();\r\n\tauto t6 = high_resolution_clock::now();\r\n\t\r\n\tstd::this_thread::sleep_for(std::chrono::seconds(delay));\r\n\t\r\n\tauto t7 = high_resolution_clock::now();\r\n\tpredict();\r\n\tauto t8 = high_resolution_clock::now();\r\n\t\r\n\tstd::this_thread::sleep_for(std::chrono::seconds(delay));\r\n\t\r\n\tauto t9 = high_resolution_clock::now();\r\n\tpredict();\r\n\tauto t10 = high_resolution_clock::now();\r\n\r\n\r\nauto ms1 = duration_cast<microseconds>(t2 - t1);\r\nauto ms2 = duration_cast<microseconds>(t4 - t3);\r\nauto ms3 = duration_cast<microseconds>(t6 - t5);\r\nauto ms4 = duration_cast<microseconds>(t8 - t7);\r\nauto ms5 = duration_cast<microseconds>(t10 - t9);\r\n\r\ncout << \"Prediction 1: \" << ms1.count() << \" us\" << endl;\r\ncout << \"Prediction 2: \" << ms2.count() << \" us\" << endl;\r\ncout << \"Prediction 3: \" << ms3.count() << \" us\" << endl;\r\ncout << \"Prediction 4: \" << ms4.count() << \" us\" << endl;\r\ncout << \"Prediction 5: \" << ms5.count() << \" us\" << endl;\r\n\r\n}\r\n\r\n\r\n```\r\n", "comments": ["A few random thoughts. \r\n\r\nIt seems the model is tiny. A few hundred us is blazing fast, and the difference might be just noise. Do you have a real use case that's affected by the performance difference? \r\n\r\nIs it possible because the CPU is doing other things in the 5 second wait, so the data is squeeze out of L1/L2 cache? and this doesn't happen if you're invoking the interpreter continuously. ", "> A few random thoughts. \n> \n> \n> \n> It seems the model is tiny. A few hundred us is blazing fast, and the difference might be just noise. Do you have a real use case that's affected by the performance difference? \n> \n> \n> \n> Is it possible because the CPU is doing other things in the 5 second wait, so the data is squeeze out of L1/L2 cache? and this doesn't happen if you're invoking the interpreter continuously. \n\nMaking sure that there are no cache misses is a good thought. To test it I've limited the code to run on one core and closed all other processes using the same cpu. The results are the same unfortunately. Speed is degrading fast after the latest invoke. \n\nThe model is tiny by purpose. Latency is extremely important for my use case, event driven high-frequency trading. Cutting a hundred microseconds is a huge improvement!\n\n"]}, {"number": 50417, "title": "TensorRT Converter *Core Dumped*", "body": "**System information**\r\n`tensorflow:2.5.0-gpu` base Docker container\r\n\r\n**Describe the current behavior**\r\nExecution is aborted (attached image)\r\n<img width=\"603\" alt=\"Capture\" src=\"https://user-images.githubusercontent.com/45488144/123081935-2be25b80-d427-11eb-8952-2afa8e5475c2.PNG\">\r\n\r\n**Describe the expected behavior**\r\nPerformed a successful model conversion 2 weeks ago with the same script.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n        converter = trt.TrtGraphConverterV2(\r\n            input_saved_model_dir = self.model_dir)\r\n        converter.convert()\r\n\r\n        saved_model_dir_trt = os.path.join(self.model_dir, self.model_id + '_trt')\r\n        converter.save(saved_model_dir_trt)\r\n```\r\n", "comments": ["@pkanwar23 is this the same `LD_LIBRARY_PATH` issue we've seen elsewhere?", "Yes- same issue. This should be fixed soon. ", "@pkanwar23 Any update on that, bc for my recently downloaded image it was still not working.\r\nIt works for image: ``nvcr.io/nvidia/tensorflow:21.07-tf2-py3`` while it crashes for:\r\n- ``tensorflow/tensorflow:2.5.0-gpu-jupyter``\r\n- ``tensorflow/tensorflow:2.5.1-gpu-jupyter``\r\n- ``tensorflow/tensorflow:2.6.0-gpu-jupyter``"]}, {"number": 50407, "title": "Issue with custom metrics with model class inheritance and using GPU\u2019s with mirrored strategy with TF version 2.4.1 and TFP version 0.12.1 (Also tested and found same issue on TF 2.5.0/TFP 0.12.1).", "body": "\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Modified Google Colab Example\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: NA\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**: 2.4.1 (and 2.5 via colab)\r\n-   **Python version**: 3.8.8\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**: \r\n  nvcc: NVIDIA (R) Cuda compiler driver\r\n  Copyright (c) 2005-2020 NVIDIA Corporation\r\n  Built on Thu_Jun_11_22:26:38_PDT_2020\r\n  Cuda compilation tools, release 11.0, V11.0.194\r\n  Build cuda_11.0_bu.TC445_37.28540450_0\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\n\r\nI am having issues getting custom metrics to work nice with tensorflow models designed with model class inheritance and distributed across GPUs with mirrored strategy. This is on TF version 2.4.1 and TFP 0.12.1.  The model fits and metric works on fitting. However, when I rerun the model to retrieve the estimated distribution object the following error is returned:\r\n**_ValueError: SyncOnReadVariable does not support `assign_add` in cross-replica context when aggregation is set to `tf.VariableAggregation.SUM`_**\r\n\r\n If I remove the custom metric, then no error is returned. If I use a tensorflow.keras defined metric such as \"mse,\" no error is returned.  A similar issue was raised previously but closed as resolved: [tensorflow issue 40366](https://github.com/tensorflow/tensorflow/issues/40366.)\r\n\r\nI am using code from the google tutorial on google colab called Probabilistic Layers Regression. It can be found here: [colab link](https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Probabilistic_Layers_Regression.ipynb\r\n)\r\n\r\nI have modified the linked code slightly to incorporate class inheritance because in my project I want to return the distribution object, and be able to calculate the negative log likelihood in a metric and a modified negative log likelihood in the loss (using custom metrics and custom losses).\r\n\r\nFor example, the following code will run and the metric works when calling model.fit:\r\n\r\n```\r\n\r\n## define custom loss\r\ndef negloglik_loss(y_true, y_pred):\r\n    nll,_,_ = y_pred\r\n    return nll\r\n## define custom metric\r\ndef negloglik_metric(y_true, y_pred):\r\n    nll,_,_ = y_pred\r\n    return nll\r\n    \r\n    \r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\r\n\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    for gpu in gpus:\r\n        tf.config.experimental.set_memory_growth(gpu,True)\r\n\r\ndevices_names = [d.name.split('e:')[1] for d in gpus]\r\n\r\nprint(devices_names)\r\nstrategy = tf.distribute.MirroredStrategy(\r\n           devices=devices_names)\r\n\r\n\r\n## create dataset\r\nw0 = 0.125\r\nb0 = 5.\r\nx_range = [-20, 60]\r\n\r\ndef load_dataset(n=150, n_tst=150):\r\n  np.random.seed(43)\r\n  def s(x):\r\n    g = (x - x_range[0]) / (x_range[1] - x_range[0])\r\n    return 3 * (0.25 + g**2.)\r\n  x = (x_range[1] - x_range[0]) * np.random.rand(n) + x_range[0]\r\n  eps = np.random.randn(n) * s(x)\r\n  y = (w0 * x * (1. + np.sin(x)) + b0) + eps\r\n  #x = x[..., np.newaxis]\r\n  x_tst = np.linspace(*x_range, num=n_tst).astype(np.float32)\r\n  #x_tst = x_tst[..., np.newaxis]\r\n  return y, x, x_tst\r\n\r\ny, x, x_tst = load_dataset()\r\n\r\n## build model from model class inheritance\r\nclass tfp_prob_reg(tf.keras.Model):\r\n    \r\n    def __init__(self):\r\n        super(tfp_prob_reg, self).__init__()\r\n        self.block_1 = tf.keras.layers.Dense(1, activation='relu')\r\n        \r\n    def call(self, inputs):\r\n        input_x, input_y = inputs\r\n        x_mu = self.block_1(input_x)\r\n        dist = tfp.layers.DistributionLambda(lambda x_mu: tfd.Normal(loc=x_mu, scale=1))(x_mu)\r\n        \r\n        \r\n        return -dist.log_prob(input_y), dist, x_mu\r\n\r\n## run on gpus\r\nwith strategy.scope():\r\n    ## define inputs\r\n    input_x = tf.keras.Input(shape=(1,))\r\n    input_y = tf.keras.Input(shape=(1,))\r\n\r\n    ## define output\r\n    outputs_x = tfp_prob_reg()([input_x, input_y])\r\n\r\n    ## build model\r\n    tfp_model = tf.keras.Model(inputs = [input_x, input_y], outputs=outputs_x)\r\n    tfp_model.add_loss(negloglik_loss(input_y, outputs_x))\r\n    tfp_model.add_metric(negloglik_metric(input_y, outputs_x), name='metric')\r\n\r\n    ## compile model\r\n    tfp_model.compile(optimizer=tf.optimizers.Nadam(learning_rate=1e-5))\r\n\r\n## fit model\r\ntfp_model.fit([x,y],\r\n              epochs=10,\r\n              validation_split = .1,\r\n              verbose=True)\r\n```\r\n\r\nHowever, re-running the fitted model on data to retrieve the estimated distribution does not run and returns the following error:\r\n\r\n\r\n`return_outputs_on_new_data = tfp_model([x_tst, y])`\r\n```\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-38-10667b80afe2> in <module>\r\n----> 1 return_outputs_on_new_data = tfp_model([x_tst, y])\r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n   1010         with autocast_variable.enable_auto_cast_variables(\r\n   1011             self._compute_dtype_object):\r\n-> 1012           outputs = call_fn(inputs, *args, **kwargs)\r\n   1013 \r\n   1014         if self._activity_regularizer:\r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py in call(self, inputs, training, mask)\r\n    422         a list of tensors if there are more than one outputs.\r\n    423     \"\"\"\r\n--> 424     return self._run_internal_graph(\r\n    425         inputs, training=training, mask=mask)\r\n    426 \r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py in _run_internal_graph(self, inputs, training, mask)\r\n    558 \r\n    559         args, kwargs = node.map_arguments(tensor_dict)\r\n--> 560         outputs = node.layer(*args, **kwargs)\r\n    561 \r\n    562         # Update tensor_dict.\r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n   1010         with autocast_variable.enable_auto_cast_variables(\r\n   1011             self._compute_dtype_object):\r\n-> 1012           outputs = call_fn(inputs, *args, **kwargs)\r\n   1013 \r\n   1014         if self._activity_regularizer:\r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in call(self, inputs)\r\n   3264 \r\n   3265   def call(self, inputs):\r\n-> 3266     self.add_metric(inputs, aggregation=self.aggregation, name=self.metric_name)\r\n   3267     return inputs\r\n   3268 \r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in add_metric(self, value, name, **kwargs)\r\n   1748 \r\n   1749       if should_update_state:\r\n-> 1750         metric_obj(value)\r\n   1751     else:\r\n   1752       if from_metric_obj:\r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py in __call__(self, *args, **kwargs)\r\n    234 \r\n    235     from tensorflow.python.keras.distribute import distributed_training_utils  # pylint:disable=g-import-not-at-top\r\n--> 236     return distributed_training_utils.call_replica_local_fn(\r\n    237         replica_local_fn, *args, **kwargs)\r\n    238 \r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/distribute/distributed_training_utils.py in call_replica_local_fn(fn, *args, **kwargs)\r\n     64     with strategy.scope():\r\n     65       return strategy.extended.call_for_each_replica(fn, args, kwargs)\r\n---> 66   return fn(*args, **kwargs)\r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py in replica_local_fn(*args, **kwargs)\r\n    215         update_op = None\r\n    216       else:\r\n--> 217         update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\r\n    218       update_ops = []\r\n    219       if update_op is not None:\r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/utils/metrics_utils.py in decorated(metric_obj, *args, **kwargs)\r\n     88 \r\n     89     with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):\r\n---> 90       update_op = update_state_fn(*args, **kwargs)\r\n     91     if update_op is not None:  # update_op will be None in eager execution.\r\n     92       metric_obj.add_update(update_op)\r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py in update_state_fn(*args, **kwargs)\r\n    175         control_status = ag_ctx.control_status_ctx()\r\n    176         ag_update_state = autograph.tf_convert(obj_update_state, control_status)\r\n--> 177         return ag_update_state(*args, **kwargs)\r\n    178     else:\r\n    179       if isinstance(obj.update_state, def_function.Function):\r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    665       try:\r\n    666         with conversion_ctx:\r\n--> 667           return converted_call(f, args, kwargs, options=options)\r\n    668       except Exception as e:  # pylint:disable=broad-except\r\n    669         if hasattr(e, 'ag_error_metadata'):\r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, args, kwargs, caller_fn_scope, options)\r\n    348   if conversion.is_in_allowlist_cache(f, options):\r\n    349     logging.log(2, 'Allowlisted %s: from cache', f)\r\n--> 350     return _call_unconverted(f, args, kwargs, options, False)\r\n    351 \r\n    352   if ag_ctx.control_status_ctx().status == ag_ctx.Status.DISABLED:\r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs, options, update_cache)\r\n    476 \r\n    477   if kwargs is not None:\r\n--> 478     return f(*args, **kwargs)\r\n    479   return f(*args)\r\n    480 \r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py in update_state(self, values, sample_weight)\r\n    377     value_sum = math_ops.reduce_sum(values)\r\n    378     with ops.control_dependencies([value_sum]):\r\n--> 379       update_total_op = self.total.assign_add(value_sum)\r\n    380 \r\n    381     # Exit early if the reduction doesn't have a denominator.\r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/distribute/values.py in assign_add(self, value, use_locking, name, read_value)\r\n   1186           not values_util.in_replica_update_context()):\r\n   1187         values_util.mark_as_unsaveable()\r\n-> 1188         return values_util.on_read_assign_add_cross_replica(\r\n   1189             self, value, read_value=read_value)\r\n   1190       else:\r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/distribute/values_util.py in on_read_assign_add_cross_replica(var, value, read_value)\r\n    199     if ds_context.in_cross_replica_context():\r\n    200       if var.aggregation == vs.VariableAggregation.SUM:\r\n--> 201         raise ValueError(\r\n    202             \"SyncOnReadVariable does not support `assign_add` in \"\r\n    203             \"cross-replica context when aggregation is set to \"\r\n\r\nValueError: SyncOnReadVariable does not support `assign_add` in cross-replica context when aggregation is set to `tf.VariableAggregation.SUM`\r\n\r\n```\r\nIf you copy this code to the google colab link and modify it to run in that notebook, you get the same errors. meaning, this error is also present on TF version 2.5.0/TFP 0.12.1.\r\n", "comments": ["@clr-iv \r\nCould you please refer to [this link](https://stackoverflow.com/questions/65413136/tensorflow-cannot-call-tf-keras-model-add-metric-when-tf-distribute-mirrore) and let us know if it helps. #40366 ", "@Saduf2019 \r\n\r\nthanks for taking the time to look at this issue. \r\n\r\nI have known about the solution at #40366. The only solution present in that issue is to upgrade to TF 2.4. However, we are using TF 2.4.1 and are still seeing the same reported error. Am I missing something within #40366?\r\n\r\n\r\n\r\n"]}, {"number": 50401, "title": "Support input of temporal sample_weights for model training on ragged tensors", "body": "**System information**\r\n- TensorFlow version (you are using): 2.5.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently tensorflow throws an error if we input temporal [sample_weights](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) for a model that's fitting inputs/outputs that are in the format of ragged tensors. Example:\r\n```python\r\n#input in general has shape (N_inputs, variable length, N_input_channels)    \r\nX = [[[4.,3,2],[2,1,3],[-1,2,1]],\r\n     [[1,2,3],[3,2,4]]]\r\nX = tf.ragged.constant(X, ragged_rank=1, dtype=tf.float64)\r\n\r\n#output in general has shape (N_inputs, variable but same as corresponding input, N_classification_classes)\r\nY = [[[0,0,1],[0,1,0],[1,0,0]],\r\n     [[0,0,1],[1,0,0]]]\r\nY = tf.ragged.constant(Y, ragged_rank=1)\r\n\r\n#Documentation says for temporal data we can pass 2D array with shape (samples, sequence_length) \r\nweights = [[100,1,1],\r\n           [100,1]]\r\nweights = np.array(weights)\r\n\r\nmodel = SimpleModel(width=16, in_features=3, out_features=3)\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\r\nmodel.fit(X,Y) #works fine\r\nmodel.fit(X,Y, sample_weight=weights) #throws error\r\n```\r\nWhere the error thrown is `ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list)`. If we do the equivalent operator for a non-ragged tensors \r\n```python\r\n#input in general has shape (N_inputs, 2, N_input_channels)    \r\nX = [[[4.,3,2],[2,1,3]],\r\n     [[1,2,3],[3,2,4]]]\r\nX = tf.constant(X, dtype=tf.float64)\r\n\r\n#output in general has shape (N_inputs, 2, N_classification_classes)\r\nY = [[[0,0,1],[0,1,0]],\r\n     [[0,0,1],[1,0,0]]]\r\nY = tf.constant(Y)\r\n\r\n#Documentation says for temporal data we can pass 2D array with shape (samples, sequence_length) \r\nweights = [[100,1],\r\n           [100,1]]\r\nweights=np.array(weights)\r\n\r\nmodel = SimpleModel(width=16, in_features=3, out_features=3)\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\r\nmodel.fit(X,Y) #works fine\r\nmodel.fit(X,Y, sample_weight=weights) #also works fine\r\n```\r\nEverything works fine.  The desired feature would allow passing of sample_weights for ragged tensors in the same way we could pass sample_weights for non-ragged tensors \r\n\r\n\r\n**Will this change the current api? How?**\r\nThis would change the `tf.keras.Model.fit` api so that ragged sample_weights are supported \r\n\r\n\r\n**Who will benefit with this feature?**\r\nPeople working with variable length data. This occurs in areas like computer vision and applications of deep learning to particle physics. This feature would allow people working with ragged tensors to deal with underrepresented classes in temporal data via reweighing. \r\n\r\n\r\n**Any Other info.**\r\nDefinition of SimpleLayer and SimpleModel used above\r\n```python\r\nclass SimpleLayer(tf.keras.layers.Layer):\r\n    \"\"\"Just dummy layer to illustrate sample_weight for layer\"\"\"\r\n    def __init__(self, in_features, out_features, n):\r\n        super(SimpleLayer, self).__init__()\r\n        self.out_features = out_features\r\n        self.in_features = in_features\r\n\r\n        self.Gamma = self.add_weight(name='Gamma'+str(n),\r\n                shape=(in_features, out_features), \r\n                initializer='glorot_normal', trainable=True)\r\n\r\n    def call(self, inputs):\r\n        #uses ragged map_flat_values for Ragged tensors to handle\r\n        #variable number of jet\r\n        xG = tf.ragged.map_flat_values(tf.matmul, inputs, self.Gamma)\r\n        return xG\r\n\r\n    \r\nclass SimpleModel(tf.keras.Model):\r\n    \"\"\"Composes SimpleLayer above to create simple network for ragged tensors\"\"\"\r\n    def __init__(self, width, in_features, out_features, Sigma=tf.nn.leaky_relu):\r\n        super(SimpleModel, self).__init__()\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        self.width = width\r\n        self.first_layer = SimpleLayer(self.in_features, self.width, 0)\r\n        self.hidden = SimpleLayer(self.width, self.width, 1)\r\n        self.last_layer = SimpleLayer(self.width, self.out_features, 2)\r\n        self.Sigma = Sigma\r\n\r\n    def call(self, inputs):\r\n        #use map_flat_values to apply activation to ragged tensor\r\n        x = tf.ragged.map_flat_values(self.Sigma, self.first_layer(inputs))\r\n        x = tf.ragged.map_flat_values(self.Sigma, self.hidden(x))\r\n        x = tf.ragged.map_flat_values(tf.nn.softmax, self.last_layer(x))\r\n        return x\r\n```\r\nFull traceback of error\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-32-ce7d043b6b3f> in <module>\r\n     60 model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n     61 model.fit(X,Y) #works fine\r\n---> 62 model.fit(X,Y, sample_weight=weights) #throws error\r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1131          training_utils.RespectCompiledTrainableState(self):\r\n   1132       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\r\n-> 1133       data_handler = data_adapter.get_data_handler(\r\n   1134           x=x,\r\n   1135           y=y,\r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in get_data_handler(*args, **kwargs)\r\n   1362   if getattr(kwargs[\"model\"], \"_cluster_coordinator\", None):\r\n   1363     return _ClusterCoordinatorDataHandler(*args, **kwargs)\r\n-> 1364   return DataHandler(*args, **kwargs)\r\n   1365 \r\n   1366 \r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\r\n   1152     adapter_cls = select_data_adapter(x, y)\r\n   1153     self._verify_data_adapter_compatibility(adapter_cls)\r\n-> 1154     self._adapter = adapter_cls(\r\n   1155         x,\r\n   1156         y,\r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, sample_weight_modes, batch_size, steps, shuffle, **kwargs)\r\n    584                **kwargs):\r\n    585     super(CompositeTensorDataAdapter, self).__init__(x, y, **kwargs)\r\n--> 586     x, y, sample_weights = _process_tensorlike((x, y, sample_weights))\r\n    587     sample_weight_modes = broadcast_sample_weight_modes(\r\n    588         sample_weights, sample_weight_modes)\r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in _process_tensorlike(inputs)\r\n   1044     return x\r\n   1045 \r\n-> 1046   inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)\r\n   1047   return nest.list_to_tuple(inputs)\r\n   1048 \r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)\r\n    865 \r\n    866   return pack_sequence_as(\r\n--> 867       structure[0], [func(*x) for x in entries],\r\n    868       expand_composites=expand_composites)\r\n    869 \r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/util/nest.py in <listcomp>(.0)\r\n    865 \r\n    866   return pack_sequence_as(\r\n--> 867       structure[0], [func(*x) for x in entries],\r\n    868       expand_composites=expand_composites)\r\n    869 \r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in _convert_numpy_and_scipy(x)\r\n   1039       if issubclass(x.dtype.type, np.floating):\r\n   1040         dtype = backend.floatx()\r\n-> 1041       return ops.convert_to_tensor_v2_with_dispatch(x, dtype=dtype)\r\n   1042     elif _is_scipy_sparse(x):\r\n   1043       return _scipy_sparse_to_sparse_tensor(x)\r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    204     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    205     try:\r\n--> 206       return target(*args, **kwargs)\r\n    207     except (TypeError, ValueError):\r\n    208       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2_with_dispatch(value, dtype, dtype_hint, name)\r\n   1428     ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\r\n   1429   \"\"\"\r\n-> 1430   return convert_to_tensor_v2(\r\n   1431       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\r\n   1432 \r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)\r\n   1434 def convert_to_tensor_v2(value, dtype=None, dtype_hint=None, name=None):\r\n   1435   \"\"\"Converts the given `value` to a `Tensor`.\"\"\"\r\n-> 1436   return convert_to_tensor(\r\n   1437       value=value,\r\n   1438       dtype=dtype,\r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py in wrapped(*args, **kwargs)\r\n    161         with Trace(trace_name, **trace_kwargs):\r\n    162           return func(*args, **kwargs)\r\n--> 163       return func(*args, **kwargs)\r\n    164 \r\n    165     return wrapped\r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n   1564 \r\n   1565     if ret is None:\r\n-> 1566       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1567 \r\n   1568     if ret is NotImplemented:\r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)\r\n     50 def _default_conversion_function(value, dtype, name, as_ref):\r\n     51   del as_ref  # Unused.\r\n---> 52   return constant_op.constant(value, dtype, name=name)\r\n     53 \r\n     54 \r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n    262     ValueError: if called on a symbolic tensor.\r\n    263   \"\"\"\r\n--> 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n    265                         allow_broadcast=True)\r\n    266 \r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    274       with trace.Trace(\"tf.constant\"):\r\n    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n--> 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n    277 \r\n    278   g = ops.get_default_graph()\r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):\r\n    300   \"\"\"Implementation of eager constant.\"\"\"\r\n--> 301   t = convert_to_eager_tensor(value, ctx, dtype)\r\n    302   if shape is None:\r\n    303     return t\r\n\r\n~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum\r\n     97   ctx.ensure_initialized()\r\n---> 98   return ops.EagerTensor(value, ctx.device_name, dtype)\r\n     99 \r\n    100 \r\n\r\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).\r\n```\r\n", "comments": []}, {"number": 50400, "title": "NaNs/Infs in gradient of tf.image.ssim_multiscale", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Installed from pip\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 11.2/8.1.1\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n**Describe the current behavior**\r\nCalculating gradient of `tf.image.ssim_multiscale` fails (resulting in NaNs and +Infs) in case of MS-SSIM being equal to 0 (e.g. with negated images as inputs). \r\n\r\n**Describe the expected behavior**\r\nCalculated gradient does not contain NaNs and Infs.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):  no\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.debugging.enable_check_numerics()\r\n\r\nx = tf.Variable(tf.random.uniform(shape=(10, 64, 64, 1), minval=0, maxval=1))\r\n\r\nwith tf.GradientTape() as tape:\r\n    y = tf.image.ssim_multiscale(x, 1 - x, max_val=1, filter_size=11, filter_sigma=1.5, power_factors=(0.07105472, 0.45297383, 0.47597145), k1=0.01, k2=0.045)\r\n\r\ntape.gradient(y, x)\r\n```\r\n\r\n**Other info / logs**\r\nOutput of code above:\r\n```\r\n2021-06-22 13:49:31.098596: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll\r\n2021-06-22 13:49:33.278314: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll\r\n2021-06-22 13:49:33.305643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 10 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2021-06-22 13:49:33.305856: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll\r\n2021-06-22 13:49:33.313716: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll\r\n2021-06-22 13:49:33.313841: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-06-22 13:49:33.317789: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll\r\n2021-06-22 13:49:33.319230: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll\r\n2021-06-22 13:49:33.324269: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusolver64_11.dll\r\n2021-06-22 13:49:33.327990: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll\r\n2021-06-22 13:49:33.328966: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll\r\n2021-06-22 13:49:33.329199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n2021-06-22 13:49:33.329609: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-06-22 13:49:33.330158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 10 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2021-06-22 13:49:33.330405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n2021-06-22 13:49:33.925802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-06-22 13:49:33.925979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0\r\n2021-06-22 13:49:33.926031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N\r\n2021-06-22 13:49:33.926529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4624 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2021-06-22 13:49:34.160168: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll\r\n2021-06-22 13:49:34.532402: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8101\r\n2021-06-22 13:49:34.874100: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll\r\n2021-06-22 13:49:35.251241: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll\r\nTraceback (most recent call last):\r\n  File \"ssim_multiscale.py\", line 12, in <module>\r\n    tape.gradient(y, x)\r\n  File \"C:\\python36_ver\\cuda11_2_gpu_5_1_trunk\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\", line 1080, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \"C:\\python36_ver\\cuda11_2_gpu_5_1_trunk\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\", line 77, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n  File \"C:\\python36_ver\\cuda11_2_gpu_5_1_trunk\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\", line 159, in _gradient_function\r\n    return grad_fn(mock_op, *out_grads)\r\n  File \"C:\\python36_ver\\cuda11_2_gpu_5_1_trunk\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 1519, in _PowGrad\r\n    gx = grad * y * math_ops.pow(x, y - 1)\r\n  File \"C:\\python36_ver\\cuda11_2_gpu_5_1_trunk\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"C:\\python36_ver\\cuda11_2_gpu_5_1_trunk\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 688, in pow\r\n    return gen_math_ops._pow(x, y, name=name)\r\n  File \"C:\\python36_ver\\cuda11_2_gpu_5_1_trunk\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 6722, in _pow\r\n    _ctx, \"Pow\", name, x, y)\r\n  File \"C:\\python36_ver\\cuda11_2_gpu_5_1_trunk\\lib\\site-packages\\tensorflow\\python\\debug\\lib\\check_numerics_callback.py\", line 301, in callback\r\n    path_length_limit=self._path_length_limit))\r\n  File \"C:\\python36_ver\\cuda11_2_gpu_5_1_trunk\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1003, in check_numerics_v2\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"C:\\python36_ver\\cuda11_2_gpu_5_1_trunk\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 6897, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:\r\n\r\n!!! Detected Infinity or NaN in output 0 of eagerly-executing op \"Pow\" (# of outputs: 1) !!!\r\n  dtype: <dtype: 'float32'>\r\n  shape: (10, 1, 3)\r\n  # of +Inf elements: 20\r\n\r\n  Input tensors (2):\r\n         0: tf.Tensor(\r\n[[[0.         0.         0.6353643 ]]\r\n\r\n [[0.         0.         0.53395146]]\r\n\r\n [[0.         0.         0.535903  ]]\r\n\r\n [[0.         0.         0.67927223]]\r\n\r\n [[0.         0.         0.5929147 ]]\r\n\r\n [[0.         0.         0.5948703 ]]\r\n\r\n [[0.         0.         0.6313471 ]]\r\n\r\n [[0.         0.         0.62584317]]\r\n\r\n [[0.         0.         0.71081793]]\r\n\r\n [[0.         0.         0.56649375]]], shape=(10, 1, 3), dtype=float32)\r\n         1: tf.Tensor([-0.9289453  -0.54702616 -0.52402854], shape=(3,), dtype=float32)\r\n\r\n : Tensor had +Inf values [Op:CheckNumericsV2]\r\n```\r\n\r\nDuring diagnosing this problem, I have discovered that adding epsilon to `mcs_and_ssim` (e.g. in `image_ops_impl.py:4531`) seems to solve a problem. Probably, this is not a valid solution, but maybe it provides some useful information.\r\n", "comments": ["@Saduf2019 \r\n\r\nI was able to replicate the issue reported here.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/572f7415ddb97f4008d0cede840335f5/untitled109.ipynb) .Thanks", "@TrevorG \r\nCould you please refer to [this link](https://stackoverflow.com/questions/38810424/how-does-one-debug-nan-values-in-tensorflow), which explains all the reasons and let us know if it helps.", "I have read that post and, frankly speaking, I don't know how it relates to this specific problem. I have already (at least, I think so) located the source of Infs (I admit, it might not be clear from the post): it happens when at least one value in `mcs_and_ssim` is 0. It leads to Infs during calculating gradient of `math_ops.pow(mcs_and_ssim, power_factors)` in `image_ops_impl.py:4522`. The gradient is calculated with `pow(mcs_and_ssim, power_factors - 1)` (see `math_grad.py:1512`), AFAIU. And `power_factors` contains values in (0, 1) interval.", "I just encountered the same issue. My code worked with version 2.4.3 started to produce Nan loss from the first epoch. The implementation in `image_ops_impl.py` has not changed. The issue persists in 2.6.0.", "The issue still exists, only solved by adding a very small number as mentioned in the original post, another workaround is to train for some iterations using MSE and then start the training from a saved checkpoint and continue using MS-SSIM", "I have the same issue. I followed @MahmoudAshraf97 and trained using MSE for couple epochs then started a new training from MSE checkpoints with MS-SSIM, which went well for one epoch, before falling back to NaN on the second epoch. I am using tensorflow version 2.4.1.", "@gabrielpeixoto-cvai \r\nIt's totally dependent on the model and the learning rate but in general the ms-ssim requires a good initial state or else it will fail to NaN regardless of the epoch when the output and the target have a very low ms-ssim score", "It is still replicating in [2.8](https://colab.sandbox.google.com/gist/mohantym/ea143fafbf3acb4eee5671d97632903c/github_50400.ipynb) .  Output of y values were 0's  in the list which is causing **inf** gradient in this case."]}, {"number": 50394, "title": "Model can be benchmarked with GPU but can't be run with GPU inside android app", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 7.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: RK3399\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI use the `android_aarch64_benchmark_model_plus_flex` tool and it can run perfectly with gpu and I see performance gain. \r\nBut when I follow this link (https://www.tensorflow.org/lite/performance/gpu_advanced) to load the same model inside my android app, the `isDelegateSupportedOnThisDevice` function returned false. \r\n**Describe the expected behavior**\r\n`isDelegateSupportedOnThisDevice` should return true\r\n\r\n\r\n\r\nAdditionally, I figured with the benchmark script that I currently only have valid opencl installed. (if i set --gpu_backend=gl, the benchmark script doesn't work). However, in android, it seems it's not detecting the opencl runtime correctly. Not sure if I need to load any runtime libs for opencl manually or not but it wasn't in any documentation, so assume I don't need to do that. Just wanna check if this is indeed a bug or it's by design. \r\n", "comments": ["without a model attached, it's hard to tell.\r\n\r\nfor android 12, you need to update the manifest file so that the delegate can detect libOpenCL.so", "> without a model attached, it's hard to tell.\r\n> \r\n> for android 12, you need to update the manifest file so that the delegate can detect libOpenCL.so\r\n\r\nHere is the model. https://drive.google.com/file/d/1IzvcG9p4sXyWqoOqIcIjkM4larrgvhYC/view?usp=sharing\r\n\r\nWe are still on Android 7.1 (we build embedded systems, not customer app). For the manifest file, do you have an example? i couldn't find any in the doc. ", "Did able to finger out what was the problem? @ysyyork "]}, {"number": 50393, "title": "A name conflict bug caused by LoweringFunctionalOps pass", "body": "\r\n**System information**\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.6\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running testSkipEagerCaseLoweringPreservesNameForFetch test from tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/functional_ops_test.py on a device that isn\u2019t CPU/GPU the test fails due to a naming conflict in TF:\r\n\u201cInvalid argument: Node 'Case/branch_index/_3' is not unique\u201d\r\n\r\nDuring LoweringFunctionalOps pass, the Case node is replaced with a graph that contains a _SwitchN nodes with the name 'Case/branch_index/_3' with an input that is named 'Case/branch_index/\u2019.\r\nLater, during graph partitioning, a _Send node is added between the Const input and the _SwitchN node which happens to get the name 'Case/branch_index/_3' thus causing a conflict.\r\n\r\nHere is a schematic of the situation:\r\n![case2](https://user-images.githubusercontent.com/17768507/122736642-eb4cdb80-d288-11eb-8c0c-75cfb949df51.png)\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Other info / logs** \r\nThe _SwitchN name given during the lowering phase happens at:\r\ntensorflow/core/common_runtime/lower_case_op.cc:122\r\nThe _Send name given during partitioning happens at:\r\ntensorflow/core/graph/graph_partition.cc:241\r\n\r\n", "comments": ["Hi!  Can you clarify the reproduction conditions a little more?  Specifically, you said that it fails that test in `functional_ops_test.py` \"on a device that isn\u2019t CPU/GPU\" -- is there a specific setup that we can try?  Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hello,\r\nUnfortunately, it isn't possible to provide you with a specific setup that would reproduce this, however, I can provide you with the pre-partitioning and post-partitioning graphs where it is possible to see the problem unfolding.\r\n(These are .pbtxt files, for some reason couldn't upload .pbtxt files here)\r\n\r\n[before.txt](https://github.com/tensorflow/tensorflow/files/6767985/before.txt)\r\n[after.txt](https://github.com/tensorflow/tensorflow/files/6767984/after.txt)\r\n\r\nIn the after.txt graph, if you look for 'Case/branch_index/_3' you would see two nodes with that name.: a _SwitchN and a _Recv.\r\n\r\nOne thing you can try is maybe placing the _SwitchN on CPU, the reason this doesn't reproduce on GPU is due to the fact that both the _SwitchN and the Const before it are placed on GPU so a _Recv isn't added.\r\n\r\nHope this gives you a lead, let me know if you need any extra information!", "Same issue is also seen in another test \"testCaseLowering\" in the same file if you execute the test on another device other than CPU or GPU.\r\n"]}, {"number": 50392, "title": "groups parameter for convolution layers is broken", "body": "### System information\r\n\r\n- Custom code\r\n- Linux Debian buster\r\n- tensorflow from binary 2.6.0-dev20210621\r\n- python 3-7\r\n- CPU only\r\n\r\n### Code to reproduce the problem\r\n\r\n```\r\n  import tensorflow as tf\r\n  import numpy as np\r\n  \r\n \r\n  def try_call_with_gradient(model, input_shape):\r\n      try:\r\n          for _ in range(2):\r\n              with tf.GradientTape() as tape:\r\n                  x = np.random.normal(size=input_shape).astype(np.float32)\r\n                  y = np.random.randint(2, size=(input_shape[0])).astype(np.int32)\r\n  \r\n                  out = model(x)\r\n                  out = tf.reduce_mean(out, list(range(1, len(input_shape))))\r\n  \r\n                  loss = tf.keras.losses.binary_crossentropy(y, out, from_logits=True)\r\n  \r\n              grads = tape.gradient(loss, model.trainable_variables)\r\n      except Exception as e:\r\n          print(e)\r\n\r\n  input_shape_1d = (3,12,4)\r\n  model_1d = tf.keras.layers.Conv1D(filters=6, kernel_size=3, groups=2)\r\n  try_call_with_gradient(model_1d, input_shape_1d)\r\n  \r\n\r\n  input_shape_2d = (3,12,12,4)\r\n  model_2d = tf.keras.layers.Conv2D(filters=6, kernel_size=3, groups=2)\r\n  try_call_with_gradient(model_2d, input_shape_2d)\r\n  \r\n\r\n  input_shape_3d = (3,12,12,12,4)\r\n  model_3d = tf.keras.layers.Conv3D(filters=6, kernel_size=3, groups=2)\r\n  try_call_with_gradient(model_3d, input_shape_3d)\r\n\r\n```\r\nOutput:\r\n```\r\nComputed input depth 4 doesn't match filter input depth 2 [Op:Conv2DBackpropInput]\r\nComputed input depth 4 doesn't match filter input depth 2 [Op:Conv2DBackpropInput]\r\nNumber of channels in filter (2) must match last dimension of input (4) [Op:Conv3D]\r\n```\r\n\r\n### Describe the problem\r\nConvolution layers with non-trivial groups parameter are broken. \r\n\r\nAdditionally Conv1D and Conv2D have different behavior than Conv3D, regarding when an error is thrown. The former break at the gradient computation, the latter at the forward call.\r\n\r\nFrom my understanding and from the documentation, the only requirement for the **groups** parameter is that **filters** and **input channels** must be a multiple of **groups**. So the parameter choices look just fine to me. \r\n\r\n### Full Tracebacks\r\n\r\nOnce for 1D\r\n```\r\nTraceback (most recent call last):\r\n  File \".../scratches/scratch.py\", line 23, in <module>\r\n    try_call_with_gradient(model_1d, input_shape_1d)\r\n  File \".../scratches/scratch.py\", line 17, in try_call_with_gradient\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n  File \".../python3.7/site-packages/tensorflow/python/eager/backprop.py\", line 1090, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \".../python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\", line 77, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n  File \".../python3.7/site-packages/tensorflow/python/eager/backprop.py\", line 159, in _gradient_function\r\n    return grad_fn(mock_op, *out_grads)\r\n  File \".../python3.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 590, in _Conv2DGrad\r\n    data_format=data_format),\r\n  File \"...python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1247, in conv2d_backprop_input\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \".../python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6941, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Computed input depth 4 doesn't match filter input depth 2 [Op:Conv2DBackpropInput]\r\n\r\nProcess finished with exit code 1\r\n```\r\nand once for 3D\r\n```\r\nTraceback (most recent call last):\r\n  File \".../scratches/scratch.py\", line 33, in <module>\r\n    try_call_with_gradient(model_3d, input_shape_3d)\r\n  File \".../scratches/scratch.py\", line 12, in try_call_with_gradient\r\n    out = model(x)\r\n  File \".../python3.7/site-packages/keras/engine/base_layer.py\", line 1037, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \".../python3.7/site-packages/keras/layers/convolutional.py\", line 249, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \".../python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \".../python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1138, in convolution_v2\r\n    name=name)\r\n  File \".../python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1268, in convolution_internal\r\n    name=name)\r\n  File \".../python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 3136, in _conv3d_expanded_batch\r\n    name=name)\r\n  File \"...python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1400, in conv3d\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \".../python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6941, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Number of channels in filter (2) must match last dimension of input (4) [Op:Conv3D]\r\n\r\nProcess finished with exit code 1\r\n```\r\n", "comments": ["I had some deeper look into the generic 'Conv' class in tensorflow/python/keras/layers/convolutional.py.\r\n\r\nBesides some checks the **groups** parameter is only used to adjust the size of the allocated kernel variable. It has no impact on the created convolution op as far as I can see.\r\n\r\nBasically the use of grouped convolutions is not implemented...\r\n\r\nEDIT: apparently tf.nn.conv2d handles grouped convolutions implicitly just from filter shape. See https://github.com/tensorflow/tensorflow/pull/36773#discussion_r404294541\r\n\r\nFor tf.nn.conv3d this doesn't work at all, but is implemented as if it worked that way. For 1d and 2d it seems to break at gradient computation.", "Was able to reproduce the issue. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/0f3c77993e5d39082329bfcdaebe4306/untitled99.ipynb).Thanks!", "@fischeru: I'm wondering if I have stumbled on the same issue:\r\n```python\r\nimport tensorflow as tf                 \r\ncg2=tf.keras.layers.Conv1D(4,4,groups=2)\r\nprint(\"cg2\",cg2(tf.zeros((2,16,4))))    \r\n```\r\ncauses error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1030, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 249, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1019, in convolution_v2\r\n    name=name)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1149, in convolution_internal\r\n    name=name)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 602, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 602, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1892, in conv1d\r\n    name=name)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 932, in conv2d\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6897, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: input and filter must have the same depth: 4 vs 2 [Op:Conv2D]\r\n```", "@RemiLacroix-IDRIS: strange, for me this runs just fine with tensorflow 2.5. can this be a difference between tensorflow and tensorflow-gpu? I didn't know that tensorflow-gpu still exists as a separate package.", "@fischeru: `tensorflow-gpu` is just the name of the Conda env here but it's the regular TensorFlow package.\r\n\r\nAnyway turns out the issue is probably different as it happens only for our own builds, see #50628."]}, {"number": 50383, "title": "Didn't find op for builtin opcode 'SUM' version '1'", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 11.4\r\n- TensorFlow installed from (source or binary): Source\r\n- Tensorflow version (commit SHA if source): 2.5 (a4dfb8d1a71)\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): STM Cortex M4\r\n\r\n**Describe the problem**\r\nThe SUM operator is not supported in TF Lite Microcontroller, even though it is supported in TF Lite. I assume it's a pretty easy operation to get ported to MCU? I'm getting \r\n`Didn't find op for builtin opcode 'SUM' version '1'`\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nRun any TF Model with SUM operator, `AllocateTensors()` will print out `Didn't find op for builtin opcode 'SUM' version '1'`\r\n", "comments": ["I also need this. Is this something that's possible to contribute?"]}, {"number": 50381, "title": "support CSR (data, indices, indptr), [shape=(M, N)] format in tf.sparse ", "body": "**System information**\r\n- TensorFlow version (you are using): 2.5\r\n- Are you willing to contribute it (Yes/No): YES\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe current `tf.sparse` constructor only support COO format `((data, (row_ind, col_ind)), [shape=(M, N)])`. However, the standard sparse representation is CSR `(((data, indices, indptr), [shape=(M, N)]))`. It would nice to see both options in `tf.sparse`. [Scipy](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html) and [pytorch](https://pytorch.org/docs/stable/generated/torch._sparse_csr_tensor.html) already support both these cases. \r\nI investigate a bit more and find out there are some functionalities that already implemented at [repo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse/) but apparently, they are no longer maintain or accessible:\r\n\r\ne.g., ```tf.compat.v1.raw_ops.DenseToCSRSparseMatrix``` is not working:\r\n\r\n```\r\nimport tensorflow.compat.v1 as tf\r\n\r\ntf.disable_v2_behavior()\r\n\r\ntmp = tf.constant([[1, 2, 3, 4, 5], [0, 0, 0, 2, 1]], dtype=tf.float32)\r\nindices = tf.where(tf.not_equal(tmp, 0))\r\nb = tf.raw_ops.DenseToCSRSparseMatrix(dense_input=ss, indices=indices)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    print(sess.run(b)) \r\n```\r\n\r\n**Will this change the current api? How?** No, it can be handle with if-else condition based on args \r\n\r\n**Who will benefit with this feature?** tf.sparse\r\n\r\n", "comments": []}, {"number": 50376, "title": "question about the roadmap of xla send/recv ops supporting.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TensorFlow master branch with commit id: 3ef6fbfd02716f774024afae711383fdb0d8a30c\r\n- Are you willing to contribute it (Yes/No): Yes\r\n \r\n\r\n**Describe the feature and the current behavior/state.**\r\nI'm starting my project on top of tensorflow with cross-machine communication support. Currently, we want to use the send/recv ops for multicast/p2p communication in HLO graph. However, we find that the latest tensorflow has not fully supported such ops in xla. I would like to query the roadmap for the support.\r\nThe send/recv hlo file for test is as follows:\r\n\r\nHloModule TwoSendRecvBothWayRecvFist_module\r\n\r\nENTRY %TwoSendRecvBothWayRecvFist.v3 () -> (f32[], token[]) {\r\n  %token0 = token[] after-all()\r\n  %recv = (f32[], u32[], token[]) recv(token[] %token0), channel_id=15, sharding={maximal device=1}\r\n  ROOT %recv-done = (f32[], token[]) recv-done((f32[], u32[], token[]) %recv), channel_id=15, sharding={maximal device=1}\r\n  %constant = f32[] constant(2.1), sharding={maximal device=0}\r\n  %send = (f32[], u32[], token[]) send(f32[] %constant, token[] %token0), channel_id=16, sharding={maximal device=0}, control-predecessors={%recv}\r\n  %send-done = token[] send-done((f32[], u32[], token[]) %send), channel_id=16, sharding={maximal device=0}\r\n}\r\n\r\nThe current tensorflow  would end up with an error when compiling the hlo graph as follows:\r\nAttempting to fetch value instead of handling error Internal: LHLO opcode recv is not supported.\r\n\r\n\r\nWe notice that the \"xla all_to_all\" communication primitive was added last month (https://github.com/tensorflow/tensorflow/blob/636833c8ff5f716dfa2eddd60a9eae905d8f715a/tensorflow/compiler/xla/service/gpu/nccl_all_to_all_thunk.cc#L65). So are the support of xla send/recv ops in scheduling?\r\n\r\nThanks a lot.\r\n\r\n\r\n**Will this change the current api? How?**\r\nNot sure.\r\n**Who will benefit with this feature?**\r\nThis feature would enable us to develop more flexible parallelisms for large model training. such as pipeline parallelism support in xla level.\r\n\r\n**Any Other info.**\r\nNone\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@NEWPLAN ,\r\n\r\nCan you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also can you please elaborate about your Feature and please specify the Use Cases for this feature. Thanks!", "> @NEWPLAN ,\r\n> \r\n> Can you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also can you please elaborate about your Feature and please specify the Use Cases for this feature. Thanks!\r\n\r\nHi @tilakrayal , \r\n\r\nThanks for your help. We have updated the issue with given template as above."]}, {"number": 50363, "title": "Does not furhter progress after `Successfully opened dynamic library libcublasLt` using multi NVIDIA V100 GPUs", "body": "TensorFlow program runs successfully with one GPU. It also works with multiple NVIDIA GTX Titian GPUs within 2 minutes. However, when more than one NVIDIA V100 GPU should be used, TensorFlow does not further progress after `Successfully opened dynamic library libcublasLt.so.11` and the GPU utilization raises to 100% according to `nvidia-smi`. \r\n\r\n**System information**\r\n- OS Platform and Distribution:\r\n  - CentOS 7\r\n- TensorFlow installed from pip\r\n- TensorFlow version 2.5.0\r\n- Python version 3.6.8\r\n- CUDA version 11.2\r\n- cuDNN version 8.2.1.32\r\n- GPU NVIDIA V100 32GB\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n#!/usr/bin/env python3\r\nimport tensorflow as tf\r\nimport os\r\n\r\nprint(\"Tensorflow version: {}\".format(tf.__version__))\r\nimport tensorflow_datasets as tfds\r\n\r\n# load dataset\r\ntfds.download.DownloadManager(download_dir=os.path.dirname(os.path.realpath(__file__)))\r\ndatasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True,\r\n                           data_dir=os.path.dirname(os.path.realpath(__file__)))\r\nmnist_train, mnist_test = datasets['train'], datasets['test']\r\n\r\n# Set Strategy\r\nstrategy = tf.distribute.MirroredStrategy()\r\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\nnum_train_examples = info.splits['train'].num_examples\r\nnum_test_examples = info.splits['test'].num_examples\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE_PER_REPLICA = 64\r\nBATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n\r\ndef scale(image, label):\r\n  image = tf.cast(image, tf.float32)\r\n  image /= 255\r\n  return image, label\r\n\r\ntrain_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\neval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)\r\n# Create model\r\nwith strategy.scope():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10)\r\n  ])\r\n\r\n  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                optimizer=tf.keras.optimizers.Adam(),\r\n                metrics=['accuracy'])\r\n## Define the checkpoint directory to store the checkpoints\r\ncheckpoint_dir = './training_checkpoints'\r\n## Name of the checkpoint files\r\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n\r\n# Function for decaying the learning rate.\r\ndef decay(epoch):\r\n  if epoch < 3:\r\n    return 1e-3\r\n  elif epoch >= 3 and epoch < 7:\r\n    return 1e-4\r\n  else:\r\n    return 1e-5\r\n\r\n# Callback for printing the LR at the end of each epoch.\r\nclass PrintLR(tf.keras.callbacks.Callback):\r\n  def on_epoch_end(self, epoch, logs=None):\r\n    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\r\n                                                      model.optimizer.lr.numpy()))\r\ncallbacks = [\r\n    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\r\n    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\r\n                                       save_weights_only=True),\r\n    tf.keras.callbacks.LearningRateScheduler(decay),\r\n    PrintLR()\r\n]\r\n\r\n# Train and evaluate\r\nmodel.fit(train_dataset, epochs=12, callbacks=callbacks)\r\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\r\neval_loss, eval_acc = model.evaluate(eval_dataset)\r\nprint('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\r\n\r\n\r\n# Export to SavedModel\r\npath = 'saved_model/'\r\nmodel.save(path, save_format='tf')\r\nunreplicated_model = tf.keras.models.load_model(path)\r\nunreplicated_model.compile(\r\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n    optimizer=tf.keras.optimizers.Adam(),\r\n    metrics=['accuracy'])\r\n\r\neval_loss, eval_acc = unreplicated_model.evaluate(eval_dataset)\r\nprint('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\r\n\r\n\r\nwith strategy.scope():\r\n  replicated_model = tf.keras.models.load_model(path)\r\n  replicated_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                           optimizer=tf.keras.optimizers.Adam(),\r\n                           metrics=['accuracy'])\r\n  eval_loss, eval_acc = replicated_model.evaluate(eval_dataset)\r\n  print ('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\r\n```\r\n\r\n**LOG with multiple V100 GPUs**\r\n```\r\n2021-06-19 11:38:35.846512: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-06-19 11:38:37.567052: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Failed precondition: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\r\nExtraction completed...: 100%|##########| 4/4 [00:00<00:00,  5.59 file/s]\r\nDl Size...: 100%|##########| 10/10 [00:00<00:00, 13.96 MiB/s]\r\nDl Completed...: 100%|##########| 4/4 [00:00<00:00,  5.58 url/s]9 file/s]\r\nGenerating splits...:   0%|          | 0/2 [00:00<?, ? splits/2021-06-19 11:38:38.388762: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\r\n2021-06-19 11:38:38.495458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: NVIDIA Tesla V100S-PCIE-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\r\n2021-06-19 11:38:38.499057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \r\npciBusID: 0000:25:00.0 name: NVIDIA Tesla V100S-PCIE-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\r\n2021-06-19 11:38:38.499141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-06-19 11:38:38.505167: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\r\n2021-06-19 11:38:38.505300: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\r\n2021-06-19 11:38:38.507215: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\r\n2021-06-19 11:38:38.507603: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\r\n2021-06-19 11:38:38.509115: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\r\n2021-06-19 11:38:38.510381: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\r\n2021-06-19 11:38:38.510627: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\r\n2021-06-19 11:38:38.521523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\r\n2021-06-19 11:38:38.522242: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-06-19 11:38:39.059808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: NVIDIA Tesla V100S-PCIE-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\r\n2021-06-19 11:38:39.065165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \r\npciBusID: 0000:25:00.0 name: NVIDIA Tesla V100S-PCIE-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\r\n2021-06-19 11:38:39.079312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\r\n2021-06-19 11:38:39.079492: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-06-19 11:38:39.860307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-06-19 11:38:39.860413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 \r\n2021-06-19 11:38:39.860431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y \r\n2021-06-19 11:38:39.860442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N \r\n2021-06-19 11:38:39.876738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30995 MB memory) -> physical GPU (device: 0, name: NVIDIA Tesla V100S-PCIE-32GB, pci bus id: 0000:01:00.0, compute capability: 7.0)\r\n2021-06-19 11:38:39.880520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30995 MB memory) -> physical GPU (device: 1, name: NVIDIA Tesla V100S-PCIE-32GB, pci bus id: 0000:25:00.0, compute capability: 7.0)\r\nWARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\r\nWARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\r\n2021-06-19 11:38:58.854683: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\r\n2021-06-19 11:38:58.854764: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\r\n2021-06-19 11:38:58.854849: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1611] Profiler found 2 GPUs\r\n2021-06-19 11:38:58.856387: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcupti.so.11.2\r\n2021-06-19 11:38:59.211473: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\r\n2021-06-19 11:38:59.216559: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1743] CUPTI activity buffer flushed\r\n2021-06-19 11:38:59.323866: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:461] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\r\n2021-06-19 11:38:59.406847: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\r\n2021-06-19 11:38:59.407366: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1999970000 Hz\r\n2021-06-19 11:39:04.122930: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\r\n2021-06-19 11:39:04.528403: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8201\r\n2021-06-19 11:39:05.177074: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8201\r\n2021-06-19 11:39:05.622628: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\r\n2021-06-19 11:39:06.126034: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\r\n```\r\n", "comments": ["Can you try to get a backtrace of the stuck process?  You can do that by starting `gdb`, [attaching](https://sourceware.org/gdb/onlinedocs/gdb/Attach.html) to the stuck python process and [getting a backtrace](https://sourceware.org/gdb/current/onlinedocs/gdb/Backtrace.html).", "Hi sanjoy,\r\nThanks for the helpful comment. Here is the backtrace of my program:\r\n```\r\n#0  0x00007f80fc073d19 in syscall () from /usr/lib64/libc.so.6\r\n#1  0x00007f7eb7b2e9ab in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()\r\n   from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007f7eb7b2dfc9 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()\r\n   from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007f7eb7b2b58b in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()\r\n   from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007f7eb7b2ba63 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007f7eb2b0eaac in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<absl::lts_2020_09_23::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<absl::lts_2020_09_23::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tensorflow::CancellationManager*, absl::lts_2020_09_23::optional<tensorflow::EagerRemoteFunctionParams> const&, absl::lts_2020_09_23::optional<tensorflow::ManagedStackTrace> const&) ()\r\n   from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007f7eac7aad84 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_2020_09_23::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, absl::lts_2020_09_23::optional<tensorflow::EagerRemoteFunctionParams> const&, std::unique_ptr<tensorflow::KernelAndDevice, tensorflow::core::RefCountDeleter> const&, tensorflow::GraphCollector*, tensorflow::CancellationManager*, absl::lts_2020_09_23::Span<tensorflow::TensorHandle*>, absl::lts_2020_09_23::optional<tensorflow::ManagedStackTrace> const&) () from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007f7eac7ab9f1 in tensorflow::ExecuteNode::Run() ()\r\n   from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007f7eb305e45f in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) ()\r\n   from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007f7eac7a8095 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) () from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007f7eac7a90f0 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) ()\r\n   from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007f7eac50ff89 in tensorflow::EagerOperation::Execute(absl::lts_2020_09_23::Span<tensorflow::AbstractTensorHandle*>, int*)\r\n    () from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#12 0x00007f7eb2b1d951 in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) ()\r\n   from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#13 0x00007f7eabf194ed in TFE_Execute ()\r\n   from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#14 0x00007f7eabe850a5 in TFE_Py_ExecuteCancelable(TFE_Context*, char const*, char const*, absl::lts_2020_09_23::InlinedVector<TFE_TensorHandle*, 4ul, std::allocator<TFE_TensorHandle*> >*, _object*, TFE_CancellationManager*, absl::lts_2020_09_23::InlinedVector<TFE_TensorHandle*, 2ul, std::allocator<TFE_TensorHandle*> >*, TF_Status*) ()\r\n   from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#15 0x00007f7ea5751167 in tensorflow::TFE_Py_ExecuteCancelable_wrapper(pybind11::handle const&, char const*, char const*, pybind11::handle const&, pybind11::handle const&, tensorflow::CancellationManager*, pybind11::handle const&) ()\r\n   from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tfe.so\r\n#16 0x00007f7ea5753ee8 in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::handle const&, char const*, char const*, pybind11::handle const&, pybind11::handle const&, pybind11::handle const&)#42}, pybind11::object, pybind11::handle const&, char const*, char const*, pybind11::handle const&, pybind11::handle const&, pybind11::handle const&, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::handle const&, char const*, char const*, pybind11::handle const&, pybind11::handle const&, pybind11::handle const&)#42}&&, pybind11::object (*)(pybind11::handle const&, char const*, char const*, pybind11::handle const&, pybind11::handle const&, pybind11::handle const&), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call) () from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tfe.so\r\n#17 0x00007f7ea576bc64 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()\r\n   from /home/schnepf/venv/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tfe.so\r\n#18 0x00007f80fcd53f91 in _PyCFunction_FastCallDict () from /usr/lib64/libpython3.6m.so.1.0\r\n#19 0x00007f80fcdbe66f in call_function () from /usr/lib64/libpython3.6m.so.1.0\r\n#20 0x00007f80fcdb2db7 in _PyEval_EvalFrameDefault () from /usr/lib64/libpython3.6m.so.1.0\r\n#21 0x00007f80fcdbd5fa in _PyEval_EvalCodeWithName () from /usr/lib64/libpython3.6m.so.1.0\r\n#22 0x00007f80fcdbe50a in fast_function () from /usr/lib64/libpython3.6m.so.1.0\r\n#23 0x00007f80fcdbe793 in call_function () from /usr/lib64/libpython3.6m.so.1.0\r\n#24 0x00007f80fcdb403b in _PyEval_EvalFrameDefault () from /usr/lib64/libpython3.6m.so.1.0\r\n#25 0x00007f80fcdbd5fa in _PyEval_EvalCodeWithName () from /usr/lib64/libpython3.6m.so.1.0\r\n#26 0x00007f80fcdbe50a in fast_function () from /usr/lib64/libpython3.6m.so.1.0\r\n#27 0x00007f80fcdbe793 in call_function () from /usr/lib64/libpython3.6m.so.1.0\r\n#28 0x00007f80fcdb403b in _PyEval_EvalFrameDefault () from /usr/lib64/libpython3.6m.so.1.0\r\n#29 0x00007f80fcdbd5fa in _PyEval_EvalCodeWithName () from /usr/lib64/libpython3.6m.so.1.0\r\n#30 0x00007f80fcdbe50a in fast_function () from /usr/lib64/libpython3.6m.so.1.0\r\n#31 0x00007f80fcdbe793 in call_function () from /usr/lib64/libpython3.6m.so.1.0\r\n#32 0x00007f80fcdb403b in _PyEval_EvalFrameDefault () from /usr/lib64/libpython3.6m.so.1.0\r\n#33 0x00007f80fcdbd5fa in _PyEval_EvalCodeWithName () from /usr/lib64/libpython3.6m.so.1.0\r\n#34 0x00007f80fcdbfa1e in _PyFunction_FastCallDict () from /usr/lib64/libpython3.6m.so.1.0\r\n#35 0x00007f80fcd161fe in _PyObject_FastCallDict () from /usr/lib64/libpython3.6m.so.1.0\r\n#36 0x00007f80fcd16311 in _PyObject_Call_Prepend () from /usr/lib64/libpython3.6m.so.1.0\r\n#37 0x00007f80fcd15f83 in PyObject_Call () from /usr/lib64/libpython3.6m.so.1.0\r\n#38 0x00007f80fcd6998d in slot_tp_call () from /usr/lib64/libpython3.6m.so.1.0\r\n#39 0x00007f80fcd15f83 in PyObject_Call () from /usr/lib64/libpython3.6m.so.1.0\r\n#40 0x00007f80fcdb4a41 in _PyEval_EvalFrameDefault () from /usr/lib64/libpython3.6m.so.1.0\r\n#41 0x00007f80fcdbda33 in _PyEval_EvalCodeWithName () from /usr/lib64/libpython3.6m.so.1.0\r\n#42 0x00007f80fcdbfa1e in _PyFunction_FastCallDict () from /usr/lib64/libpython3.6m.so.1.0\r\n#43 0x00007f80fcd161fe in _PyObject_FastCallDict () from /usr/lib64/libpython3.6m.so.1.0\r\n#44 0x00007f80fcd16311 in _PyObject_Call_Prepend () from /usr/lib64/libpython3.6m.so.1.0\r\n#45 0x00007f80fcd15f83 in PyObject_Call () from /usr/lib64/libpython3.6m.so.1.0\r\n#46 0x00007f80fcdb4a41 in _PyEval_EvalFrameDefault () from /usr/lib64/libpython3.6m.so.1.0\r\n#47 0x00007f80fcdbd5fa in _PyEval_EvalCodeWithName () from /usr/lib64/libpython3.6m.so.1.0\r\n#48 0x00007f80fcdbfa1e in _PyFunction_FastCallDict () from /usr/lib64/libpython3.6m.so.1.0\r\n#49 0x00007f80fcd161fe in _PyObject_FastCallDict () from /usr/lib64/libpython3.6m.so.1.0\r\n#50 0x00007f80fcd16311 in _PyObject_Call_Prepend () from /usr/lib64/libpython3.6m.so.1.0\r\n#51 0x00007f80fcd15f83 in PyObject_Call () from /usr/lib64/libpython3.6m.so.1.0\r\n#52 0x00007f80fcd6998d in slot_tp_call () from /usr/lib64/libpython3.6m.so.1.0\r\n#53 0x00007f80fcd16180 in _PyObject_FastCallDict () from /usr/lib64/libpython3.6m.so.1.0\r\n#54 0x00007f80fcdbe81c in call_function () from /usr/lib64/libpython3.6m.so.1.0\r\n#55 0x00007f80fcdb2db7 in _PyEval_EvalFrameDefault () from /usr/lib64/libpython3.6m.so.1.0\r\n#56 0x00007f80fcdbd5fa in _PyEval_EvalCodeWithName () from /usr/lib64/libpython3.6m.so.1.0\r\n#57 0x00007f80fcdbe50a in fast_function () from /usr/lib64/libpython3.6m.so.1.0\r\n#58 0x00007f80fcdbe793 in call_function () from /usr/lib64/libpython3.6m.so.1.0\r\n#59 0x00007f80fcdb403b in _PyEval_EvalFrameDefault () from /usr/lib64/libpython3.6m.so.1.0\r\n#60 0x00007f80fcdbea35 in PyEval_EvalCodeEx () from /usr/lib64/libpython3.6m.so.1.0\r\n#61 0x00007f80fcdbf7cb in PyEval_EvalCode () from /usr/lib64/libpython3.6m.so.1.0\r\n#62 0x00007f80fce47dfe in run_mod () from /usr/lib64/libpython3.6m.so.1.0\r\n#63 0x00007f80fccf4f43 in PyRun_FileExFlags () from /usr/lib64/libpython3.6m.so.1.0\r\n#64 0x00007f80fccf5315 in PyRun_SimpleFileExFlags () from /usr/lib64/libpython3.6m.so.1.0\r\n#65 0x00007f80fce4e2d2 in Py_Main () from /usr/lib64/libpython3.6m.so.1.0\r\n#66 0x0000000000400ab9 in main ()\r\n```"]}, {"number": 50345, "title": "ROCM: Segmentation fault late in build process", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro, completely updated\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): compiling from source\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.9.5\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source): 4.0.0\r\n- GCC/Compiler version (if compiling from source):10.2.0-3\r\n- CUDA/cuDNN version: cuda: 11.3.0-2,m, cudnn: 8.2.0.53-1\r\n- GPU model and memory: AMD rx570 4GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nProblem is the build fails\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nBuilding this aur package https://github.com/rocm-arch/tensorflow-rocm\r\n\r\nAfter many hours of compilation I get a segmentation fault.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n    compile tensorflow/core/kernels/mlir_generated/is_inf_gpu_f16_i1_kernel_generator_kernel.o [for host]; 2s local\r\n    compile tensorflow/core/kernels/mlir_generated/is_inf_gpu_f64_i1_kernel_generator_kernel.o [fERROR: /tmp/trizen-mario/tensorflow-rocm/src/tensorflow-2.5.0-rocm/tensorflow/core/kernels/mlir_generated/BUILD:957:23: compile tensorflow/core/kernels/mlir_generated/is_finite_gpu_f16_i1_kernel_generator_kernel.o [for host] failed: (Segmentation fault): tf_to_kernel failed: error executing command bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel '--unroll_factors=4' '--tile_sizes=256' '--arch=gfx701,gfx702,gfx803,gfx900,gfx904,gfx906,gfx908' ... (remaining 4 argument(s) skipped)\r\n[20,437 / 21,317] 11 actions running\r\n    compile tensorflow/core/kernels/mlir_generated/is_finite_gpu_f64_i1_kernel_generator_kernel.o [for host]; 3s local\r\n    compile tensorflow/core/kernels/mlir_generated/is_inf_gpu_f16_i1_kernel_generator_kernel.o [for host]; 2s local\r\n    compile tensorflow/core/kernels/mlir_generated/is_inf_gpu_f64_i1_kernel_generator_kernel.o [for host]; 2s local\r\n    compile tensorflow/core/kernels/mlir_generated/is_nan_gpu_f16_i1_kernel_generator_kernel.o [f2021-06-18 15:06:05.299828: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n0x556475d2e878: i1 = FP_CLASS 0x5564779f2e08, Constant:i32<504>TensorFlow crashed, please file a bug on https://github.com/tensorflow/tensorflow/issues with the trace below.\r\nStack dump:\r\n0.\tProgram arguments: bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel --unroll_factors=4 --tile_sizes=256 --arch=gfx701,gfx702,gfx803,gfx900,gfx904,gfx906,gfx908 --input=bazel-out/host/bin/tensorflow/core/kernels/mlir_generated/is_finite_gpu_f16_i1.mlir --output=bazel-out/host/bin/tensorflow/core/kernels/mlir_generated/is_finite_gpu_f16_i1_kernel_generator_kernel.o --enable_ftz=False --cpu_codegen=False\r\n1.\t2.\tRunning pass 'CallGraph Pass Manager' on module 'acme'.\r\n3.\tRunning pass 'AMDGPU DAG->DAG Pattern Instruction Selection' on function '@IsFinite_GPU_DT_HALF_DT_BOOL_kernel'\r\nStack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x408d943)[0x556473344943]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x408bb0d)[0x556473342b0d]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x408bc94)[0x556473342c94]\r\n/usr/lib/libpthread.so.0(+0x13870)[0x7f9763e9a870]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2b070e8)[0x556471dbe0e8]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x18acc23)[0x556470b63c23]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2a9adf2)[0x556471d51df2]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2b6f3b6)[0x556471e263b6]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2bb88c6)[0x556471e6f8c6]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2b6fa1e)[0x556471e26a1e]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2b6fb98)[0x556471e26b98]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2a80a73)[0x556471d37a73]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2a837e0)[0x556471d3a7e0]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2a856a6)[0x556471d3c6a6]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2e0f83f)[0x5564720c683f]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x3f03645)[0x5564731ba645]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x3bc50e7)[0x556472e7c0e7]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x3f030a1)[0x5564731ba0a1]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x169de1d)[0x556470954e1d]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x16a2bef)[0x556470959bef]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0xbe3199)[0x55646fe9a199]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x361381d)[0x5564728ca81d]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x361394a)[0x5564728ca94a]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x361427b)[0x5564728cb27b]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x3612b6f)[0x5564728c9b6f]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x36133ac)[0x5564728ca3ac]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x361394a)[0x5564728ca94a]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x3615c06)[0x5564728ccc06]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x7f78dd)[0x55646faae8dd]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x6b5eb8)[0x55646f96ceb8]\r\n/usr/lib/libc.so.6(__libc_start_main+0xd5)[0x7f9763348b25]\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x7f060e)[0x55646faa760e]\r\n[20,437 / 21,317] 11 actions running\r\n    compile tensorflow/core/kernels/mlir_generated/is_finite_gpu_f64_i1_kernel_generator_kernel.o [for host]; 3s local\r\n    compile tensorflow/core/kernels/mlir_generated/is_inf_gpu_f16_i1_kernel_generator_kernel.o [for host]; 2s local\r\n    compile tensorflow/core/kernels/mlir_generated/is_inf_gpu_f64_i1_kernel_generator_kernel.o [for host]; 2s local\r\n    compile tensorflow/core/kernels/mlir_generated/is_nan_gpu_f16_i1_kernel_generator_kernel.o [fERROR: /tmp/trizen-mario/tensorflow-rocm/src/tensorflow-2.5.0-rocm/tensorflow/tools/pip_package/BUILD:284:10 Middleman _middlemen/tensorflow_Stools_Spip_Upackage_Sbuild_Upip_Upackage-runfiles failed: (Segmentation fault): tf_to_kernel failed: error executing command bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel '--unroll_factors=4' '--tile_sizes=256' '--arch=gfx701,gfx702,gfx803,gfx900,gfx904,gfx906,gfx908' ... (remaining 4 argument(s) skipped)\r\nINFO: Elapsed time: 11598.345s, Critical Path: 268.21s\r\nINFO: 20448 processes: 1439 internal, 19009 local.\r\nFAILED: Build did NOT complete successfully\r\n==> ERROR: A failure occurred in build().\r\n```", "comments": ["I've also been seeing the same error while building TF 2.5 with CUDA 10.2 on Linux ppc64le containers. On baremetal, the build works just fine. Could someone please help us on this?", "Has anyone got any fix for this issue?\r\n", "@chsigg Could you please help reassign to the right person? Thank you!", "In one of the issues https://github.com/tensorflow/tensorflow/issues/46680#issuecomment-775447310, I found that passing `--define=tensorflow_enable_mlir_generated_gpu_kernels=0` in the bazel invocation command fixes the above problem. For ROCM, it should be set to 0 even if cuda is on. For cuda only builds it is true by default.\r\n\r\nIt fixed the problem in my case too. But I don't have RoCM enabled, just have cuda enabled. So, I'm not sure what would be impact of this change functionality wise. Could someone please help me understand the effect and need of this flag in bazelrc? ", "Maybe @frgossen would be able to help?", "The tool that fails here is the kernel generator `tools/kernel_gen/tf_to_kernel`, which creates the new MLIR-generated kernels. With `--define=tensorflow_enable_mlir_generated_gpu_kernels=0` you basically disable this path and fall back to the old Eigen kernels. This is probably not what you want to do long term but may be a temporary fix. \r\nI am trying to reproduce this now to find out more. ", "@deven-amd\r\nDeven, could you maybe help here?\r\nIn the ROCM RBE build, this seems to work fine.", "builds are working fine on our end too.\r\n\r\nI suspect the error maybe because the build is trying to specify some `gfx` archs that are old / not supported\r\n\r\nFrom the logfile snippet pasted above, we see that the tool is being used to generated code for the following AMDGPU archs\r\n`--arch=gfx701,gfx702,gfx803,gfx900,gfx904,gfx906,gfx908 `\r\n\r\nCan we trim this list down to just `gfx900, gfx906, gfx908` (i.e. the list of AMDGPU arch we currently/official support for TF) and check if the error is resolved.", "@frgossen Do you mind further elaboration on what's the potential impact of using this workaround(`--define=tensorflow_enable_mlir_generated_gpu_kernels=0`)? Does it hurt performance? Or will some TF models fail to run?", "`--define=tensorflow_enable_mlir_generated_gpu_kernels=0` disables the new MLIR-generated kernels. This concerns the kernels that you use in TF eager mode. Instead, TF will fall back to use the old Eigen-based kernels. I would expect this to hurt performance and you may have fewer kernels and data types, i.e. type coverage that we recently extended will not be available to you. Depending on the models, it is in principle possible that some would not run. ", "@frgossen Thank you - that helps a lot.\r\n\r\nDo you happen to know if they have changed the flag name? I tried the workaround but same errors:\r\n```\r\nERROR: /home/yay/tensorflow-rocm/src/tensorflow-2.6.2-opt-rocm/tensorflow/core/kernels/mlir_generated/BUILD:1123:23: compile tensorflow/core/kernels/mlir_generated/is_inf_gpu_f16_i1_kernel[4/1088]\r\nor_kernel.o failed (Segmentation fault): tf_to_kernel failed: error executing command bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel '--unroll_factors=4' '--tile_sizes\r\n=256' '--max-supported-rank=5' '--arch=gfx701,gfx702,gfx803,gfx900,gfx904,gfx906,gfx908' ... (remaining 4 argument(s) skipped)\r\n2021-11-23 17:47:48.283929: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n0x562fb32e1a00: i1 = FP_CLASS 0x562fb3243c50, Constant:i32<516>TensorFlow crashed, please file a bug on https://github.com/tensorflow/tensorflow/issues with the trace below.\r\nStack dump:\r\n0.      Program arguments: bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel --unroll_factors=4 --tile_sizes=256 --max-supported-rank=5 --arch=gfx701,gfx702,gfx803,gfx900\r\n,gfx904,gfx906,gfx908 --input=bazel-out/k8-opt/bin/tensorflow/core/kernels/mlir_generated/is_inf_gpu_f16_i1.mlir --output=bazel-out/k8-opt/bin/tensorflow/core/kernels/mlir_generated/is_inf_gpu_f16\r\n_i1_kernel_generator_kernel.o --enable_ftz=False --cpu_codegen=False\r\n1.      Running pass 'CallGraph Pass Manager' on module 'acme'.\r\n2.      Running pass 'AMDGPU DAG->DAG Pattern Instruction Selection' on function '@IsInf_GPU_DT_HALF_DT_BOOL_kernel'\r\nStack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x4684c68)[0x562fb00d4c68]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x4682ffd)[0x562fb00d2ffd]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x4683184)[0x562fb00d3184]\r\n/usr/lib/libpthread.so.0(+0x13870)[0x7f97c7208870]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x355ebd8)[0x562faefaebd8]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x1f361dc)[0x562fad9861dc]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x34f1ee2)[0x562faef41ee2]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x35c9d56)[0x562faf019d56]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x3605f55)[0x562faf055f55]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x35ca3be)[0x562faf01a3be]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x35ca538)[0x562faf01a538]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x34d8a33)[0x562faef28a33]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x34db6d3)[0x562faef2b6d3]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x34dd62f)[0x562faef2d62f]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x386edf7)[0x562faf2bedf7]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x44ecb9c)[0x562faff3cb9c]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x41b8d5f)[0x562fafc08d5f]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x44ec5d7)[0x562faff3c5d7]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x1a04ebd)[0x562fad454ebd]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x1a09ee6)[0x562fad459ee6]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0xeb0d19)[0x562fac900d19]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x3022785)[0x562faea72785]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x302290a)[0x562faea7290a]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x30231f5)[0x562faea731f5]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x30216d3)[0x562faea716d3]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x30222b0)[0x562faea722b0]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x302290a)[0x562faea7290a]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x3023855)[0x562faea73855]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0xad66e0)[0x562fac5266e0]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x999141)[0x562fac3e9141]\r\n/usr/lib/libc.so.6(__libc_start_main+0xd5)[0x7f97c5dc7b25]\r\nbazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0xace7ce)[0x562fac51e7ce]\r\n```\r\nHere is some debug info from bazel:\r\n```\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=196\r\nINFO: Reading rc options for 'build' from /home/yay/tensorflow-rocm/src/tensorflow-2.6.2-opt-rocm/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/yay/tensorflow-rocm/src/tensorflow-2.6.2-opt-rocm/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain\r\n--define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --ena\r\nble_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from /home/yay/tensorflow-rocm/src/tensorflow-2.6.2-opt-rocm/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/usr/lib/python3.9/site-packages --python_path=/usr/bin/python --define=with_xla_support=true --config=\r\nrocm --action_env TF_SYSTEM_LIBS=boringssl,curl,cython,gif,icu,libjpeg_turbo,lmdb,nasm,png,pybind11,zlib\r\nINFO: Found applicable config definition build:short_logs in file /home/yay/tensorflow-rocm/src/tensorflow-2.6.2-opt-rocm/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/yay/tensorflow-rocm/src/tensorflow-2.6.2-opt-rocm/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:rocm in file /home/yay/tensorflow-rocm/src/tensorflow-2.6.2-opt-rocm/.bazelrc: --crosstool_top=@local_config_rocm//crosstool:toolchain --define=using\r\n_rocm=true --define=using_rocm_hipcc=true --repo_env TF_NEED_ROCM=1\r\nINFO: Found applicable config definition build:avx2_linux in file /home/yay/tensorflow-rocm/src/tensorflow-2.6.2-opt-rocm/.bazelrc: --copt=-mavx2\r\nINFO: Found applicable config definition build:mkl in file /home/yay/tensorflow-rocm/src/tensorflow-2.6.2-opt-rocm/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorfl\r\now_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt\r\nINFO: Found applicable config definition build:linux in file /home/yay/tensorflow-rocm/src/tensorflow-2.6.2-opt-rocm/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFI\r\nX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=fa\r\nlse\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/yay/tensorflow-rocm/src/tensorflow-2.6.2-opt-rocm/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNA\r\nMIC_KERNELS\r\n```\r\nStrangely the above doesn't contain the workaround flag I passed to it:\r\n```\r\nriaqn@bedroom ~> ps -aux|grep -i mlir_\r\nriaqn     622225  0.0  0.0 342936  6808 ?        Sl+  07:45   0:00 bazel build --config=avx2_linux --config=mkl -c opt --define=tensorflow_enable_mlir_generated_gpu_kernels=0 //tensorflow:libtensorflow.so //tensorflow:libtensorflow_cc.so //tensorflow:install_headers //tensorflow/tools/pip_package:build_pip_package\r\n```", "Just checked, and at this point `mlir_generated_gpu_kernels_enabled` seems to be an alias for `gpu_enabled` (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/mlir_generated/build_defs.bzl#L116). \r\nEither way, disabling MLIR is not very sustainable. I suggest to follow Deven's advice and try reduce the GPU architectures. ", "@frgossen Thank you for the reply. I limited to `gfx803` (the GPU I'm using) and it builds successfully. "]}, {"number": 50334, "title": "Error build hello_world targeting RISCV ", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version: v2.4.2\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): GCC 7.5.0, riscv64-unknown-elf-g++ (GCC) 9.2.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\nWe are failed to build a runnable binary for RISCV as our targeting architecture following the steps described in this page.\r\nhttps://www.tensorflow.org/lite/microcontrollers/library\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. clone the repository\r\n2. make -f tensorflow/lite/micro/tools/make/Makefile hello_world_bin (this is okay)\r\n3. make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mcu_riscv hello_world_bin (this builds, but it uses g++ instead of the riscv64-unknown-elf-g++)\r\n\r\n4. So we copy mcu_riscv_makefile.inc to riscv32_mcu_makefile.inc\r\n5. make -f tensorflow/lite/micro/tools/make/Makefile TARGET=riscv32_mcu hello_world_bin (then there are errors, asm related, the log is attached below)\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\ncc1: warning: command line option '-fno-threadsafe-statics' is valid for C++/ObjC++ but not for C                                                               \r\ncc1: warning: command line option '-fno-use-cxa-atexit' is valid for C++/ObjC++ but not for C                                                                           \r\nIn file included from tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:6:                                                  \r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c: In function 'measure_cpu_freq':\r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:170:3: error: 'asm' undeclared (first use in this function)                                \r\n  170 |   asm volatile (\"csrr %0, \" #reg : \"=r\"(__tmp)); \\                                                                                                      \r\n      |   ^~~                                                                                                                                                          \r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:152:32: note: in expansion of macro 'read_csr'               \r\n  152 |   unsigned long start_mcycle = read_csr(mcycle);                                                                                                        \r\n      |                                ^~~~~~~~                                                                                                                         \r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:170:3: note: each undeclared identifier is reported only once for each function it appears in\r\n  170 |   asm volatile (\"csrr %0, \" #reg : \"=r\"(__tmp)); \\                                                                                    \r\n      |   ^~~                                                                                                                                                                          \r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:152:32: note: in expansion of macro 'read_csr'                                      \r\n  152 |   unsigned long start_mcycle = read_csr(mcycle);                                                                                                                                \r\n      |                                ^~~~~~~~                                                                                                                                     \r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:170:7: error: expected ';' before 'volatile'                                                              \r\n  170 |   asm volatile (\"csrr %0, \" #reg : \"=r\"(__tmp)); \\                                                                                                                               \r\n      |       ^~~~~~~~                                                                                                                                                                  \r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:152:32: note: in expansion of macro 'read_csr'                                           \r\n  152 |   unsigned long start_mcycle = read_csr(mcycle);                                                                                                                              \r\n      |                                ^~~~~~~~                                                                                                                                          \r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:170:7: error: expected ';' before 'volatile'                                                            \r\n  170 |   asm volatile (\"csrr %0, \" #reg : \"=r\"(__tmp)); \\                                                                                    \r\n      |       ^~~~~~~~                                                         \r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:158:32: note: in expansion of macro 'read_csr'\r\n  158 |   unsigned long delta_mcycle = read_csr(mcycle) - start_mcycle;                                                    \r\n      |                                ^~~~~~~~                            \r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c: In function '_init':\r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:225:25: warning: format '%d' expects argument of type 'int', but argument 2 has type 'long unsigned int' [-W\r\nformat=]                                                                       \r\n  225 |   printf(\"core freq at %d Hz\\n\", get_cpu_freq());\r\n      |                        ~^        ~~~~~~~~~~~~~~                                                                                                                                                     \r\n      |                         |        |    \r\n      |                         int      long unsigned int                                                                                                                          \r\n      |                        %ld \r\nIn file included from tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:6:\r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:175:5: error: 'asm' undeclared (first use in this function)\r\n  175 |     asm volatile (\"csrw \" #reg \", %0\" :: \"i\"(val)); \\        \r\n      |     ^~~                                                          \r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:227:3: note: in expansion of macro 'write_csr'\r\n  227 |   write_csr(mtvec, &trap_entry);                                                                                                                                                                    \r\n      |   ^~~~~~~~~                              \r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:175:9: error: expected ';' before 'volatile'\r\n  175 |     asm volatile (\"csrw \" #reg \", %0\" :: \"i\"(val)); \\\r\n      |         ^~~~~~~~\r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:227:3: note: in expansion of macro 'write_csr'\r\n  227 |   write_csr(mtvec, &trap_entry);\r\n      |   ^~~~~~~~~\r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:177:9: error: expected ';' before 'volatile'\r\n  177 |     asm volatile (\"csrw \" #reg \", %0\" :: \"r\"(val)); })\r\n      |         ^~~~~~~~\r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:227:3: note: in expansion of macro 'write_csr'\r\n  227 |   write_csr(mtvec, &trap_entry);\r\n      |   ^~~~~~~~~\r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:170:7: error: expected ';' before 'volatile'\r\n  170 |   asm volatile (\"csrr %0, \" #reg : \"=r\"(__tmp)); \\\r\n      |       ^~~~~~~~\r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:228:7: note: in expansion of macro 'read_csr'\r\n  228 |   if (read_csr(misa) & (1 << ('F' - 'A'))) { // if F extension is present\r\n      |       ^~~~~~~~\r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:175:9: error: expected ';' before 'volatile'\r\n  175 |     asm volatile (\"csrw \" #reg \", %0\" :: \"i\"(val)); \\\r\n      |         ^~~~~~~~\r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:229:5: note: in expansion of macro 'write_csr'\r\n  229 |     write_csr(mstatus, MSTATUS_FS); // allow FPU instructions without trapping\r\n      |     ^~~~~~~~~\r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:177:9: error: expected ';' before 'volatile'\r\n  177 |     asm volatile (\"csrw \" #reg \", %0\" :: \"r\"(val)); })\r\n      |         ^~~~~~~~\r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:229:5: note: in expansion of macro 'write_csr'\r\n  229 |     write_csr(mstatus, MSTATUS_FS); // allow FPU instructions without trapping                                                                                                 \r\n      |     ^~~~~~~~~                                                                                                                                                               \r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:177:9: error: expected ';' before 'volatile'                                                             \r\n  177 |     asm volatile (\"csrw \" #reg \", %0\" :: \"r\"(val)); })                                                                                                                      \r\n      |         ^~~~~~~~                                                                                                                                                                 \r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:229:5: note: in expansion of macro 'write_csr'                                           \r\n  229 |     write_csr(mstatus, MSTATUS_FS); // allow FPU instructions without trapping                                                                                                  \r\n      |     ^~~~~~~~~                                                                                                                                                                    \r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:175:9: error: expected ';' before 'volatile'                                                           \r\n  175 |     asm volatile (\"csrw \" #reg \", %0\" :: \"i\"(val)); \\\r\n      |         ^~~~~~~~\r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:230:5: note: in expansion of macro 'write_csr'\r\n  230 |     write_csr(fcsr, 0); // initialize rounding mode, undefined at reset\r\n      |     ^~~~~~~~~\r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:177:9: error: expected ';' before 'volatile'\r\n  177 |     asm volatile (\"csrw \" #reg \", %0\" :: \"r\"(val)); })\r\n      |         ^~~~~~~~\r\ntensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:230:5: note: in expansion of macro 'write_csr'\r\n  230 |     write_csr(fcsr, 0); // initialize rounding mode, undefined at reset\r\n      |     ^~~~~~~~~\r\ntensorflow/lite/micro/tools/make/Makefile:318: recipe for target 'tensorflow/lite/micro/tools/make/gen/riscv32_mcu_riscv32_mcu/obj/tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freed\r\nom-e300-hifive1/init.o' failed\r\nmake: *** [tensorflow/lite/micro/tools/make/gen/riscv32_mcu_riscv32_mcu/obj/tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.o] Error 1\r\n", "comments": []}, {"number": 50325, "title": "Grappler ImplementationSelector fails to use CUDNN (for GRU & LSTM) when using tf.data.experimental.scan", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary (latest-gpu docker)\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0 (but this also reproduces in earlier versions).\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 11.2 / 8.1.0\r\n- GPU model and memory: NVIDIA Quadro P1000\r\n\r\n**Describe the current behavior**\r\nAs you can see in the code [here](https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/layers/recurrent_v2.py#L832), the way CUDNN LSTM & GRU are implemented, is by adding two functions with the same name to the graph. Grappler's ImplementationSelector optimizer runs later and selects the correct implementation. However, when using `tf.data.experimental.scan`, from what I've previously seen (I don't quite remember well), the function gets renamed - which causes the implementation selector to fail.\r\nWhat actually happens is the non-CUDNN version is the default implementation - so it falls back to that one.\r\n\r\n**Describe the expected behavior**\r\nThe layers should run using CUDNN, when possible.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): This might be a bit over my head, but if the solution is simple, I could implement it.\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nUnfortunately I couldn't think of a way to display this issue without Tensorboard, but here it is:\r\n```\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.GRU(4),\r\n])\r\n\r\n@tf.function\r\ndef foo(dataset):\r\n    a = 1.\r\n    # Autograph converts this to tf.data.experimental.scan\r\n    for i in dataset:\r\n        a = model(i)\r\n    return a # return the value to avoid pruning\r\n\r\ninp_tensor = tf.random.normal([32, 10, 8])\r\ndataset = tf.data.Dataset.from_tensor_slices([inp_tensor])\r\n\r\ntf.profiler.experimental.start(\"/tmp/tensorboard\")\r\n\r\n# This successfully uses CUDNN\r\nmodel.predict(inp_tensor)\r\n\r\n# This doesn't use CUDNN\r\nfoo(dataset)\r\n\r\ntf.profiler.experimental.stop()\r\n```\r\nHere you can see `model.predict` uses CUDNN:\r\n![image](https://user-images.githubusercontent.com/51128928/122393399-8427e200-cf7d-11eb-98ef-28ec9cd75cf6.png)\r\n\r\nAnd here you can see `foo(dataset)` does not use CUDNN:\r\n![image](https://user-images.githubusercontent.com/51128928/122393625-bcc7bb80-cf7d-11eb-839d-2b9fc5437872.png)", "comments": ["I was able to reproduce the code in tf v2.4 and v2.5.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/093d3289a3660f8e4f813f8821c2ff84/untitled50325.ipynb)."]}, {"number": 50320, "title": "Can i implement a model that learns weights that fit the quantization step size?", "body": "I want to make model what can train quantization weights. Is there such a way?\r\n\r\nI already know how to take out the weights of the trained model,\r\nquantize them in an external tool, and then load them again.\r\n\r\nThanks for reading this question and have a nice day", "comments": []}, {"number": 50308, "title": "deserialize pb stream to feature is too slow.", "body": "We run tensorflow on A100 machine, and our model currently consumes TFRecord very fast, we find deserialize varint64 is hot spot, so I want to replace from varint64 to fixed64.\r\nI find :\r\n    1. TF don't support fixed64 in feature.proto.\r\n    2. I have a test that fixed64 is faster(10x) than varint64.\r\nIs there any way we can implement fixed64 without modifying the TF code?", "comments": ["Some of our confidential data needs to be converted into hash64, and parse varint64 is unfriendly to hash64.", "Now TF not support Fixed64, Do we have a plan to add a fixed64 type to deserialize hash64?", "If you think this part is meaningful and you have no plans to implement this part, I can contribute this part of the code.", "@zhaozheng09 Contributions are always welcome. Please feel free to raise a PR. Thanks!"]}, {"number": 50305, "title": "How to create Tensorflow data ingestion pipeline for multiple related CSVs?", "body": "Let us say we have some Relational data. \r\nMaking a simple example for a retail store chain:\r\n* Dataset 1 --> Store_id, Daily_sales\r\n* Dataset 2 --> Customer_id, store_id, Time in, Time out\r\n\r\nLet us say the task is to predict ```Daily_sales```.\r\n\r\n\r\nI know how how to create data batches for one single CSV. I can use ```tf.data.experimental.make_csv_dataset``` and iterate over the dataset iterable that it returns to read the batches lazily. \r\n\r\nHowever, I want to read in the batches from ```Dataset 1``` and ```Dataset 2``` described above where the common id is ```store_id``` such that the batch reads the rows with same ```store_id```s from both the datasets. I want to do this because I will run two networks (RNN on ```Dataset 2``` and a single Fully connected layer on ```Dataset 1```) on both datasets and then merge them in the final fully connected layer.\r\n\r\nCan you please guide me on how to approach this problem in scenarios where:\r\n * The datasets can fit into the memory\r\n * The datasets can not fit into the memory\r\n\r\nHere is a concrete example of the behaviour I am looking  for:\r\n```\r\nimport pandas as pd\r\nDataset_1 = pd.DataFrame({'id':['a','b','c','d'],'col1':[1,2,3,4]})\r\nprint(Dataset_1)\r\n  id  col1\r\n0  a     1\r\n1  b     2\r\n2  c     3\r\n3  d     4\r\nDataset_2 = pd.DataFrame({'id':['a','a','b','c','c','c','d'],'col1':[10,11,12,13,14,15,16]})\r\nprint(Dataset_2)\r\n\tid\tcol1\r\n0\ta\t10\r\n1\ta\t11\r\n2\tb\t12\r\n3\tc\t13\r\n4\tc\t14\r\n5\tc\t15\r\n6\td\t16\r\n#Let us say i want to create 2 batches. The following dataframes are how i want my batches to look like\r\nbatch_1 = (pd.DataFrame({'id':['a','b'],'col1':[1,2]}),pd.DataFrame({'id':['a','a','b'],'col1':[10,11,12]}))\r\nprint(batch_1[0])\r\n\tid\tcol1\r\n0\ta\t1\r\n1\tb\t2\r\nprint(batch_1[1])\r\n  id  col1\r\n0  a    10\r\n1  a    11\r\n2  b    12\r\nbatch_2 = (pd.DataFrame({'id':['c','d'],'col1':[3,4]}),pd.DataFrame({'id':['c','c','c','d'],'col1':[13,14,15,16]}))\r\nprint(batch_2[0])\r\nid  col1\r\n0  c     3\r\n1  d     4\r\n\r\nprint(batch_2[1])\r\n id  col1\r\n0  c    13\r\n1  c    14\r\n2  c    15\r\n3  d    16\r\n```", "comments": ["@nitinmnsn \r\nCan you please tell if [this](https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset?version=nightly) helps?", "No, it does not insofar as I can see the behaviours CsvDataset API makes possible for me.\r\nHere is a concrete example of the behaviour I am looking  for:\r\n```\r\nimport pandas as pd\r\nDataset_1 = pd.DataFrame({'id':['a','b','c','d'],'col1':[1,2,3,4]})\r\nprint(Dataset_1)\r\n  id  col1\r\n0  a     1\r\n1  b     2\r\n2  c     3\r\n3  d     4\r\nDataset_2 = pd.DataFrame({'id':['a','a','b','c','c','c','d'],'col1':[10,11,12,13,14,15,16]})\r\nprint(Dataset_2)\r\n\tid\tcol1\r\n0\ta\t10\r\n1\ta\t11\r\n2\tb\t12\r\n3\tc\t13\r\n4\tc\t14\r\n5\tc\t15\r\n6\td\t16\r\n#Let us say i want to create 2 batches. The following dataframes are how i want my batches to look like\r\nbatch_1 = (pd.DataFrame({'id':['a','b'],'col1':[1,2]}),pd.DataFrame({'id':['a','a','b'],'col1':[10,11,12]}))\r\nprint(batch_1[0])\r\n\tid\tcol1\r\n0\ta\t1\r\n1\tb\t2\r\nprint(batch_1[1])\r\n  id  col1\r\n0  a    10\r\n1  a    11\r\n2  b    12\r\nbatch_2 = (pd.DataFrame({'id':['c','d'],'col1':[3,4]}),pd.DataFrame({'id':['c','c','c','d'],'col1':[13,14,15,16]}))\r\nprint(batch_2[0])\r\nid  col1\r\n0  c     3\r\n1  d     4\r\n\r\nprint(batch_2[1])\r\n id  col1\r\n0  c    13\r\n1  c    14\r\n2  c    15\r\n3  d    16\r\n```\r\nAdding this example to the original post as well"]}, {"number": 50298, "title": "tf2 could NOT start more than 2 processes/sessions (one process one tf session) on intel i9 processor while tf1.15.4 is ok", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n   ```\r\n   No\r\n   ```\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n   ```\r\n   Linux 289d7745a47e 5.10.25-linuxkit #1 SMP Tue Mar 23 09:27:39 UTC 2021 x86_64 GNU/Linux\r\n   ```\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n   ```\r\n   N.A.\r\n   ```\r\n- TensorFlow installed from (source or binary):\r\n   ```\r\n   binary, direct pip install, so far tested with version 2.3.1 and 2.5.0\r\n   ```\r\n- TensorFlow version (use command below):\r\n   ```\r\n   2.3.1, 2.5.0\r\n   ```\r\n- Python version:\r\n   ```\r\n   Python 3.6.12\r\n   ```\r\n- Bazel version (if compiling from source):\r\n   ```\r\n   N.A.\r\n   ```\r\n- GCC/Compiler version (if compiling from source):\r\n   ```\r\n   gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\r\n   Copyright (C) 2016 Free Software Foundation, Inc.\r\n   This is free software; see the source for copying conditions.  There is NO\r\n   warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n   ```\r\n- CUDA/cuDNN version:\r\n   ```\r\n   N.A.\r\n   ```\r\n- GPU model and memory:\r\n   ```\r\n   N.A.\r\n   ```\r\n\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n   ```\r\n   N.A.\r\n   ```\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n   ```\r\n   v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n   ```\r\n\r\n**Describe the current behavior**\r\n\r\ntensorflow2 could **NOT** start more than 2 processes on **intel i9 processor**? (within docker container), i am not quite sure i9 processors is the issue here, but this is working great on my old laptop.\r\n\r\nAnd i tested the exact behaviour with tensorflow 1.15.4 and it is ok.\r\n\r\n```\r\nDocker version 20.10.6, build 370c289\r\n```\r\n```\r\nprocessor\t: 0\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 158\r\nmodel name\t: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\r\n```\r\nSo far i tested on tensorflow `2.3.1` and `2.5.0`,  both cannot start more than 2 tf sessions, then behaviour is that for the first 2 tf sessions it is pretty quick, like a few seconds, but when i tried to start the 3rd tf session, it took quite long, like 5 or 6 mins+, eventually it still cant start, but one of the previous started tf session will get killed silently.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n`tf2.py` to load model:\r\n```\r\nimport tensorflow as tf\r\n\r\nmodel = tf.saved_model.load(\"/models/tf2_models/od_ssd_mobilenet/1\")\r\n\r\nprint('233333-------------')\r\n\r\nwhile True:\r\n  pass\r\n```\r\n\r\nthe `od_ssd_mobilenet` model i download from here: [http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz)\r\n\r\n- Step 1: do `python tf2.py &` to load the model and put the process to the background, and wait the `233333-------------` message get printed;\r\n- Step 2: then use `ps -ef | grep tf2.py` to check the process is running;\r\n- Repeat step 1 and 2 to start 2 ok processes;\r\n- Repeat step 1 and 2 to try to start the 3rd process, you will find out it will take super long, and when eventually you see the `233333-------------`  message get printed, one of the previous 2 ok process was killed silently.\r\n\r\nAnd i tested the exact behaviour with tensorflow 1.15.4 and it is ok.\r\n\r\nthe logs i had when the model was loading:\r\n```\r\n2021-06-16 04:33:57.384790: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-06-16 04:33:57.384844: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2021-06-16 04:34:00.171257: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2021-06-16 04:34:00.171318: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-06-16 04:34:00.171409: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (289d7745a47e): /proc/driver/nvidia/version does not exist\r\n2021-06-16 04:34:00.171848: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n233333-------------\r\n```\r\n", "comments": ["@lnshi \r\n\r\nLooking at the error log,seems like it is similar to this [#45930](https://github.com/tensorflow/tensorflow/issues/45930),and [#4196](https://github.com/tensorflow/tensorflow/issues/4196) and let me know if it helps.Thanks\r\n", "@UsharaniPagadala \r\n\r\nI think the concerns are totally different:\r\n\r\n1. This issue/bug is happening on CPU deployment not GPU, and i am installing `tensorflow` not `tensorflow-gpu`;\r\n2. The script i pasted is the simplified one to reproduce the issue, with my full procedure with **2** processes all the inferences are happening properly, no issues/errors;\r\n3. Just previously i am able to run **more than 2** tf.Sessions/processes based on the number of the CPU cores for one model without any problem, now with this new laptop and intel i9 processors i am not able to do that anymore.\r\n\r\n**That is why i raised this issue to check is this a bug in tf2, or some intel i9 hardware compatibilities issues? coz previously either with tf1 and tf2 i didn't encounter such issue.**\r\n\r\nHope i made myself clear.", "Update1\r\n---\r\nTested on aws [g4dn.xlarge](https://aws.amazon.com/ec2/instance-types/g4/) instance with tensorflow 2.4.1 and 2.5.0:\r\n> G4dn instances feature NVIDIA T4 GPUs and custom Intel Cascade Lake CPUs\r\n\r\n- Without utilising the GPU device, **NO such issues**, can start 3 processes and subsequent load tests are all fine;\r\n- With GPU device, no enough GPU memory to start the 3rd process:\r\n   ```\r\n   2021-06-17 09:55:53.379762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13509 MB memory) -physical GPU (device: 0, name: NVIDIA Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)\r\n- With GPU device, with a smaller inception model and  the [ssd_mobilenet od model](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz), **NO such issues**, can start 3 processes (1 for inception, 2 for the ssd_mobilenet od model) and subsequent load tests are all fine;\r\n  ```\r\n  Fri Jun 18 03:43:50 2021\r\n  +-----------------------------------------------------------------------------+\r\n  | NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\r\n  |-------------------------------+----------------------+----------------------+\r\n  | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n  | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n  |                               |                      |               MIG M. |\r\n  |===============================+======================+======================|\r\n  |   0  NVIDIA Tesla T4     On   | 00000000:00:1E.0 Off |                    0 |\r\n  | N/A   39C    P0    27W /  70W |  13322MiB / 15109MiB |      0%      Default |\r\n  |                               |                      |                  N/A |\r\n  +-------------------------------+----------------------+----------------------+\r\n  \r\n  +-----------------------------------------------------------------------------+\r\n  | Processes:                                                                  |\r\n  |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n  |        ID   ID                                                   Usage      |\r\n  |=============================================================================|\r\n  |    0   N/A  N/A     19746      C   python3                          4185MiB |\r\n  |    0   N/A  N/A     19747      C   python3                          4567MiB |\r\n  |    0   N/A  N/A     19755      C   python3                          4567MiB |\r\n  +-----------------------------------------------------------------------------+\r\n   ```\r\n\r\nUpdate2\r\n---\r\nTested on aws [c4.4xlarge](https://aws.amazon.com/about-aws/whats-new/2015/01/11/amazon-ec2-c4-instances-now-available/) instance with tensorflow 2.3.1, **NO such issues**, can start any number of processes as long the memory is sufficient, and subsequent load tests are all fine;\r\n> C4 instances are based on Intel Xeon E5-2666 v3 (codename Haswell) processors that run at a base frequency of 2.9 GHz, and can deliver clock speeds as high as 3.5 GHz with Intel \u00ae Turbo Boost.\r\n\r\nUpdate3\r\n---\r\nTested on aws [m5.4xlarge](https://aws.amazon.com/ec2/instance-types/m5/) instance with tensorflow 2.5.0, **NO such issues**, can start any number of processes as long the memory is sufficient, and subsequent load tests are all fine;\r\n> M5 and M5d instances feature either the 1st or 2nd generation Intel Xeon Platinum 8000 series processor (Skylake-SP or Cascade Lake)\r\n\r\n@UsharaniPagadala looks it is indeed specific to the `Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz`? any idea?", "@lnshi \r\nThe tested [build](https://www.tensorflow.org/install/source#gpu) Cuda  compatibility for tf2.5 is 11.2 .Please  check the latest [release](https://github.com/tensorflow/tensorflow/releases/tag/v2.5.0).This is also might be the reason and let us know if it helps.Thanks\r\n \r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow-2.5.0 | 3.6-3.9 | GCC 7.3.1 | Bazel 3.7.2 | 8.1 | 11.2\r\ntensorflow-2.4.0 | 3.6-3.8 | GCC 7.3.1 | Bazel 3.1.0 | 8.0 | 11.0\r\ntensorflow-2.3.0 | 3.5-3.8 | GCC 7.3.1 | Bazel 3.1.0 | 7.6 | 10.1\r\n\r\n\r\n<br class=\"Apple-interchange-newline\">", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "`pip install tensorflow` and `pip install tensorflow_gpu` will result in the exact same code in TF 2.x\r\n\r\nIf you want CPU-only code you have to use `tensorflow_cpu` package.", "The issue here seems to be a silent OOM killer when you run out of memory. It's not really the fault of TF.", "@mihaimaruseac  that is even more weird, i gave 7GB memory to the container, and it happens also on my local mac which has 32GB memory....\r\n\r\nto highlight again, i never installed tensorflow-gpu, and exactly same code base working great on all my other MPBs, only on this new MBP 2021 with i9 processor i encounter this problem, t**hat is why i am checking whether TF has some compatible issues with this latest intel i9 processors**", "If you did `pip install tensorflow` then you installed `tensorflow-gpu`. They are exactly the same package."]}, {"number": 50278, "title": "Second gradient return None using tf.while_loop in TF2.x", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Example provided below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from binary\r\n- TensorFlow version (use command below): 2.5 and 1.15.2 both in Colab\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nIn tf2.5, when using `tf.while_loop`, we are able to call `.gradients()` in the `body()` of `tf.while_loop`. \r\nHowever, if we call `tf.GradientTape()`to get the first gradient outside the `tf.while_loop`, and then call `.gradients()` to get the second gradient in the `body()` of the `tf.while_loop`, the second gradient will be `None`.\r\n\r\n**Standalone code to reproduce the issue**\r\n[colab](https://colab.research.google.com/drive/1xV2vN_fZkkZz47AmoH-y-Fl1B6lmtpGB?usp=sharing)\r\n```python3 \r\n# step function\r\n@tf.function\r\ndef step(dummy):\r\n  with tf.GradientTape(persistent=True) as tape:\r\n    loss = tf.reduce_sum(tf.pow(var, 2))  # loss = w^2\r\n  grad = tape.gradient(loss, var)  # 1st gradient, grad = 2w\r\n  train = loop(grad, var, dummy)\r\n\r\n# Set loop funtion\r\ndef loop(grad, var, dummy):\r\n  def cond(dummy):\r\n    return True\r\n\r\n  def body(dummy):\r\n    s = tf.constant(1.)\r\n    elemwise = math_ops.multiply(grad, array_ops.stop_gradient(s))\r\n    grad_value = tf.gradients(elemwise, var) # 2nd gradient \r\n    # return None to grad_value, but expect grad_value is not None\r\n    print('grad_value=', grad_value)\r\n    return dummy\r\n\r\n  return tf.while_loop(cond, body, [dummy])\r\n\r\n# Set variables\r\nvar = tf.Variable(2., name= 'w1')\r\ndummy = tf.constant(1.)\r\nstep(dummy)\r\n```\r\n\r\n**Describe the expected behavior**\r\nExpect the grad_value is not `None`\r\n\r\n\r\n\r\n**Other info / logs** \r\n - tf2.5(**I focus on**) with `@tf.function` fails\r\n - tf1.15 In default graph mode works, but in eager mode with `@tf.function` fails\r\n - similar issue [this](https://github.com/tensorflow/tensorflow/issues/24866#issue-398513482) and [this](https://stackoverflow.com/questions/49555016/compute-gradients-for-each-time-step-of-tf-while-loop) \r\n", "comments": ["@sachinprasadhs ,\r\nI was able to reproduce the issue in tf v2.4,v2.5 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/22b1834cffc0c92156e43d54bd157dfa/untitled50278.ipynb).", "Although it's not officially deprecated (yet), `tf.gradients()` is TF1-style graph only API, could you try reformulating your code to use only `tf.GradientTape` ?", "> Although it's not officially deprecated (yet), `tf.gradients()` is TF1-style graph only API, could you try reformulating your code to use only `tf.GradientTape` ?\r\n\r\nThanks for suggestion\r\nI tried `tf.GradientTape` in the second gradient, it can't work neither(See the following code). \r\nI want to let `step()` function and its sub functions run under the graph, that's why I use `@tf.funtion()`\r\nMy main concern is that why it can't work even `tf.funtion()` puts everything into graph mode in TF2.X. Instead, in TF1.X,  it works also in graph mode. \r\n\r\n```python \r\n# step function\r\n@tf.function\r\ndef step(dummy):\r\n  with tf.GradientTape(persistent=True) as tape:\r\n    loss = tf.reduce_sum(tf.pow(var, 2))  # loss = w^2\r\n  grad = tape.gradient(loss, var)  # 1st gradient, grad = 2w\r\n  train = loop(grad, var, dummy)\r\n\r\n# Set loop funtion\r\ndef loop(grad, var, dummy):\r\n  def cond(dummy):\r\n    return True\r\n\r\n  def body(dummy):\r\n    s = tf.constant(1.)\r\n    with tf.GradientTape(persistent=True) as tape:\r\n      elemwise = math_ops.multiply(grad, array_ops.stop_gradient(s))\r\n    grad_value = tape.gradient(elemwise, var) # 2nd gradient , grad_value= None\r\n    # return None to grad_value, but expect grad_value is not None\r\n    print('grad_value=', grad_value)\r\n    return dummy\r\n\r\n  return tf.while_loop(cond, body, [dummy])\r\n\r\n# Set variables\r\nvar = tf.Variable(2., name= 'w1')\r\ndummy = tf.constant(1.)\r\nstep(dummy)\r\n```"]}, {"number": 50274, "title": "ResNet giving different outputs in TF and TFLite in C++", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 8\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.2\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): 8.3.1\r\n\r\n\r\n**Describe the current behavior**\r\nResNet36 (and ResNet50) models give different outputs when running inference on TensorFlow and TensorFlow-Lite in C++.\r\n\r\n**Describe the expected behavior**\r\nOutputs should be same across all platforms and across different APIs.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nDue to certain restrictions, I won't be able to provide fully reproducible code. Following are some snippets : \r\nModel : \r\n```\r\nbase_model = tf.keras.applications.ResNet50(\r\n    include_top=False, weights='imagenet',\r\n    input_shape=[96,112,3])\r\n\r\nx = base_model.output\r\nx = tf.keras.layers.Flatten()(x)\r\nx = tf.keras.layers.Dense(128, use_bias=True, name='Bottleneck')(x)\r\n   \r\nmodel = tf.keras.models.Model( base_model.input, x )\r\n```\r\n\r\nTFLite inference : \r\n```\r\nstd::unique_ptr<tflite::FlatBufferModel> flat_buffer_model = tflite::FlatBufferModel::BuildFromBuffer((const char*)model_buffer->data(), model_buffer->size(), &err_reporter);\r\ntflite::ops::builtin::BuiltinOpResolver builtins;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\ntflite::InterpreterBuilder(*flat_buffer_model, builtins)(&interpreter);\r\n\r\nconst std::vector<int>& outputs = thread_interpreter.interpreter_->outputs();\r\nTfLiteStatus tf_stat = thread_interpreter.interpreter_->Invoke();\r\n\r\n```\r\n\r\n**Other info / logs** \r\nI have a ResNet36 model trained using TFv1 and saved into a protobuf file format. This is used to generate inference on sample image using TensorFlow C API on windows. \r\nI used ```tf.compat.v1.lite.converter.from_frozen_graph()``` to convert this protobuf to a .tflite file.\r\nI ran inference from TFLite using C++ API (on x86_64 architecture).\r\nAdditionally, I also ran inference from the TFLite model using Python.\r\n\r\n```***All the 3 outputs are different.***```\r\n\r\nInterestingly, I also have an InceptionResNet model (.pb file from TF1 converted into .tflite file using TF2) and it gives the exact same output when I run inference in TF (C) and TFLite (C++). So, this makes me question that something is wrong with tflite converter/API specifically when dealing with ResNet architecture.\r\n\r\nNote : I also tested ResNet50 architecture which was trained in TF2 and got similar anomalous results. (This is just to rule out any functionalities that may not have been supported by backwards compatible converter).\r\n\r\n\r\nI am interested in knowing pointers such as gcc compiler optimizations or anything else that may cause different outputs across TensroFlow and TensorFlow-Lite in C++.\r\n\r\nAny help is much appreciated.", "comments": ["@saikumarchalla Let me know if you need any more information.", "Hi @suraj-maniyar , can you include the code/command you used to convert the model? In the past, the main source of different outputs in vision models is differences in the preprocessing pipeline for the image (how it is scaled and/or cropped), so if you want to include the representative code you used that would be helpful. You might also try running the TFLite Interpreter from Python with the same input to see if you get different results.", "@jdduke \r\n\r\nI used this code to convert .pb to .tflite : \r\n```\r\n\r\nimport tensorflow as tf\r\ntf.__version__\r\n# 2.3.2\r\n\r\ngraph_def_file = \"resface36_L2Norm128_at_epoch_168.pb\"\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(graph_def_file,\r\n                                              [\"input\"],\r\n                                              [\"resface36/Bottleneck/BiasAdd\"],\r\n                                              input_shapes={'input': [1, 96, 112, 3]})\r\ntflite_model = converter.convert()\r\n\r\nopen(\"resface36_L2Norm128_at_epoch_168.tflite\", \"wb\").write(tflite_model)\r\n\r\n```\r\n\r\n\r\nAs for the preprocessing pipeline, I am not even using any preprocessing on the image. I simply created a matrix of zeros of dimension 96x112 and fed it to network in TF and TFLite. I have also tested Inception ResNet and it gave exact same output in TF and TFLite. I only have problems with ResNet where the outputs don't match.\r\n\r\nAnd yes, I do get different output when running TFLite interpreter from Python. \r\n\r\nSo basically, I have 3 different outputs for the same resnet model with the same input (zero matrix) on different configurations (TF on C++, TFLite on C++, TFLite on Python)", "Is the output correct when using TFLite via Python? Or does TFLite provide wrong (but different) outputs via C++ & Python? It would be helpful to see the TFLite model (untrained is fine) you have, so I can take a look.\r\n\r\nAlso I assume you are doing `interpreter->AllocateTensors()` when using C++, before any invoke?", "I have similar problem with you though my model is different from yours. Have you solved it?\r\nI have 3 different outputs for **the same tflite model** with the same input on different configurations (TFLite on C++ on Windows, TFLite on python3 on Windows, TFLite on Python3 on linux) \r\ntf version is 2.4.0\r\n\r\n\r\n", "@lxsyz its difficult to know what might be without any idea of how you are doing inference on the model :-). Are any of the three settings you mentioned giving you the correct output?", "@srjoglekar246 I'm doing inference in translation task with batch inputs(about 16 sentences). 12 outputs are same but 4 outputs are similar with few different words. It's very strange.\r\n"]}, {"number": 50260, "title": "make_csv_dataset fails inside Dataset.interleave()", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): debian\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n**Describe the current behavior**\r\n`make_csv_dataset`  fails  when used as a map function inside `data.Dataset.interleave()`\r\n\r\n\r\n**Describe the expected behavior**\r\nIt must create datasets\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nfiles = tf.data.Dataset.list_files(\"*.csv\")\r\nds3 = files.interleave(lambda x: tf.data.experimental.make_csv_dataset(x, batch_size=1))\r\n```\r\n\r\n[link to colab](https://colab.research.google.com/gist/eli-osherovich/ac1e62d1c2ba919fd450dfec4c178ab9/test.ipynb)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nRunning this code:\r\n```python\r\nfiles = tf.data.Dataset.list_files(\"*.csv\")\r\nds3 = files.interleave(lambda x: tf.data.experimental.make_csv_dataset(x, batch_size=1))\r\n```\r\nI get the following error:\r\n```\r\nOperatorNotAllowedInGraphError: in user code:\r\n\r\n    <ipython-input-5-fcdae26c49e4>:2 None  *\r\n        lambda x: tf.data.experimental.make_csv_dataset(x, batch_size=1))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/data/experimental/ops/readers.py:480 make_csv_dataset_v2  **\r\n        filenames = _get_file_names(file_pattern, False)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/data/experimental/ops/readers.py:1135 _get_file_names\r\n        file_names = list(gfile.Glob(file_pattern))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py:383 get_matching_files\r\n        return get_matching_files_v2(filename)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py:449 get_matching_files_v2\r\n        for single_filename in pattern\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:520 __iter__\r\n        self._disallow_iteration()\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:516 _disallow_iteration\r\n        self._disallow_in_graph_mode(\"iterating over `tf.Tensor`\")\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:496 _disallow_in_graph_mode\r\n        \" this function with @tf.function.\".format(task))\r\n\r\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n```", "comments": ["@saikumarchalla  friendly ping", "@rmothukuru ,\r\nI was able to reproduce the issue in tf v2.4,v2.5 and nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/6f817f8b3efce2b2415462f05e20dad3/50260.ipynb).", "@jvishnuvardhan \r\nAny update on this?", "Thanks for reporting this @eli-osherovich, this looks like a real bug that I'll work on fixing.\r\n\r\nThe reason this hasn't come up before is that calling `make_csv_dataset` inside of `interleave` isn't usually necessary. `make_csv_dataset` will incorporate its own `interleave` when you set `num_parallel_reads`. i.e., instead of writing\r\n\r\n```python\r\nfiles = tf.data.Dataset.list_files(\"*.csv\")\r\nds3 = files.interleave(lambda x: tf.data.experimental.make_csv_dataset(x, batch_size=1))\r\n```\r\n\r\nyou could write\r\n\r\n```python\r\nds3 = tf.data.experimental.make_csv_dataset(\"*.csv\", batch_size=1, num_parallel_reads=tf.data.AUTOTUNE)\r\n```\r\n\r\nThe second version may also be more performant, because it will use multiple threads to read from CSV files in parallel.", "Thanks, @aaudiber \r\n\r\nMay real code is a bit more complex: in my case each CSV file represents a  different class (with big skew between classes) and I wanted to do sort of round-robin sampling. ", "@eli-osherovich [CsvDataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset) accepts `Tensor` filenames, so you could use it directly from inside of `interleave`. `make_csv_dataset` on the other hand requires `string` filenames, so that it can infer column names based on the filenames. It would be convenient if `make_csv_dataset` could support `Tensor` filenames as well, but right now your best option is to use `CsvDataset` instead.", "Thanks, @aaudiber  I will try CsvDataset, even though, I would prefer the automation of `make_csv_dataset` "]}, {"number": 50259, "title": "Monolithic build for Tf Lite with CMake", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.5\r\n- Python version: \r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): Cmake 3.16\r\n- GCC/Compiler version (if compiling from source): 7.4\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\nI want to create a monolithin build of the libtensorflow-lite.so library file. \r\n\r\nWith bazel, you used to just add `--config=monolithic` to the `bazel build` command in order to stop having to ensure libraries like libflatbuffers.a, libruy.a, etc. had to be passed on along with libtensorflow-lite.so.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nSteps provided from here:\r\nhttps://www.tensorflow.org/lite/guide/build_cmake\r\n", "comments": ["@terryheo could you take a look at this?", "A monolithic static build is not available now.\r\n\r\nBut you can use a monolithic shared library for C API.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake#build_tensorflow_lite_c_library", "We already have a codebase that is specifically designed for the C++ API. I don't think it will be trivial to change. Furthermore, I think the C API has considerably more limitations than that of the standard normal API. Would I be correct in these statements?\r\n\r\nIs there any way for me to remove the dependency of certain libraries such as `libruy.a`. I figured that if I set it to off in CMAKE, I will not need to link to the libtensorflow-all.so file afterwards. I set it to off within the CMAKE settings, but it provokes failures during the android build:\r\n\r\n```\r\nCMARGS=\"-DTFLITE_ENABLE_XNNPACK=OFF \\\r\n        -DTFLITE_ENABLE_RUY=OFF \\\r\n        -DBUILD_SHARED_LIBS=ON \\\r\n        -DTFLITE_ENABLE_NNAPI=OFF \\\r\n        -DTFLITE_ENABLE_MMAP=OFF\"\r\n        \r\nCMARGS=\"$CMARGS -DCMAKE_TOOLCHAIN_FILE=$PKG_DIR/android-ndk-r18/build/cmake/android.toolchain.cmake \\\r\n        -DANDROID_ABI=$ANDROID_ARM_ARCH \\\r\n        -DANDROID_PLATFORM=$ANDROID_PLATFORM\"\r\n\r\n$CMAKETOOLS/cmake $CMARGS $TFLITE_ROOT_DIR\r\n```\r\n\r\nThis provokes the following set of errors: \r\n\r\n```\r\nIn file included from /home/keidav01/devenv/tensorflow/tensorflow/lite/core/api/flatbuffer_conversions.cc:16:\r\nIn file included from /home/keidav01/devenv/tensorflow/tensorflow/lite/core/api/flatbuffer_conversions.h:28:\r\n/home/keidav01/devenv/tensorflow/tensorflow/lite/schema/schema_generated.h:21:10: fatal error: 'flatbuffers/flatbuffers.h' file not found\r\n#include \"flatbuffers/flatbuffers.h\"\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n[ 58%] Building CXX object CMakeFiles/tensorflow-lite.dir/kernels/dequantize.cc.o\r\n[ 58%] Building CXX object CMakeFiles/tensorflow-lite.dir/kernels/depthwise_conv.cc.o\r\n[ 60%] Building CXX object CMakeFiles/tensorflow-lite.dir/kernels/detection_postprocess.cc.o\r\n[ 60%] Building CXX object CMakeFiles/tensorflow-lite.dir/kernels/div.cc.o\r\nIn file included from /home/keidav01/devenv/tensorflow/tensorflow/lite/kernels/conv.cc:15:\r\n/home/keidav01/devenv/tensorflow/tensorflow/lite/kernels/internal/optimized/integer_ops/conv.h:18:10: fatal error: 'ruy/profiler/instrumentation.h' file not found\r\n#include \"ruy/profiler/instrumentation.h\"  // from @ruy\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n[ 60%] Building CXX object CMakeFiles/tensorflow-lite.dir/kernels/eigen_support.cc.o\r\nIn file included from /home/keidav01/devenv/tensorflow/tensorflow/lite/delegates/telemetry.cc:16:\r\nIn file included from /home/keidav01/devenv/tensorflow/tensorflow/lite/delegates/telemetry.h:22:\r\n/home/keidav01/devenv/tensorflow/tensorflow/lite/experimental/acceleration/configuration/configuration_generated.h:21:10: fatal error: 'flatbuffers/flatbuffers.h' file not found\r\n#include \"flatbuffers/flatbuffers.h\"\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n[ 60%] Building CXX object CMakeFiles/tensorflow-lite.dir/kernels/elementwise.cc.o\r\n[ 60%] Building CXX object CMakeFiles/tensorflow-lite.dir/kernels/embedding_lookup.cc.o\r\n1 error generated.\r\nCMakeFiles/tensorflow-lite.dir/build.make:192: recipe for target 'CMakeFiles/tensorflow-lite.dir/delegates/telemetry.cc.o' failed\r\nmake[2]: *** [CMakeFiles/tensorflow-lite.dir/delegates/telemetry.cc.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\n/home/keidav01/devenv/tensorflow/tensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc:26:10: fatal error: 'ruy/ruy.h' file not found\r\n#include \"ruy/ruy.h\"  // from @ruy\r\n         ^~~~~~~~~~~\r\nIn file included from /home/keidav01/devenv/tensorflow/tensorflow/lite/c/c_api_experimental.cc:24:\r\nIn file included from /home/keidav01/devenv/tensorflow/tensorflow/lite/c/c_api_internal.h:25:\r\nIn file included from /home/keidav01/devenv/tensorflow/tensorflow/lite/core/api/op_resolver.h:23:\r\n/home/keidav01/devenv/tensorflow/tensorflow/lite/schema/schema_generated.h:21:10: fatal error: 'flatbuffers/flatbuffers.h' file not found\r\n#include \"flatbuffers/flatbuffers.h\"\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/keidav01/devenv/tensorflow/tensorflow/lite/core/api/op_resolver.cc:16:\r\nIn file included from /home/keidav01/devenv/tensorflow/tensorflow/lite/core/api/op_resolver.h:23:\r\n/home/keidav01/devenv/tensorflow/tensorflow/lite/schema/schema_generated.h:21:10: fatal error: 'flatbuffers/flatbuffers.h' file not found\r\n#include \"flatbuffers/flatbuffers.h\"\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/keidav01/devenv/tensorflow/tensorflow/lite/c/c_api.cc:20:\r\nIn file included from /home/keidav01/devenv/tensorflow/tensorflow/lite/c/c_api_internal.h:25:\r\nIn file included from /home/keidav01/devenv/tensorflow/tensorflow/lite/core/api/op_resolver.h:23:\r\n/home/keidav01/devenv/tensorflow/tensorflow/lite/schema/schema_generated.h:21:10: fatal error: 'flatbuffers/flatbuffers.h' file not found\r\n#include \"flatbuffers/flatbuffers.h\"\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n1 error generated.\r\nCMakeFiles/tensorflow-lite.dir/build.make:75: recipe for target 'CMakeFiles/tensorflow-lite.dir/core/api/flatbuffer_conversions.cc.o' failed\r\nmake[2]: *** [CMakeFiles/tensorflow-lite.dir/core/api/flatbuffer_conversions.cc.o] Error 1\r\n/home/keidav01/devenv/tensorflow/tensorflow/lite/kernels/audio_spectrogram.cc:22:10: fatal error: 'flatbuffers/flexbuffers.h' file not found\r\n#include \"flatbuffers/flexbuffers.h\"  // from @flatbuffers\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/keidav01/devenv/tensorflow/tensorflow/lite/kernels/depthwise_conv.cc:16:\r\n/home/keidav01/devenv/tensorflow/tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv.h:23:10: fatal error: 'ruy/profiler/instrumentation.h' file not found\r\n#include \"ruy/profiler/instrumentation.h\"  // from @ruy\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/keidav01/devenv/tensorflow/tensorflow/lite/kernels/detection_postprocess.cc:24:10: fatal error: 'flatbuffers/flexbuffers.h' file not found\r\n#include \"flatbuffers/flexbuffers.h\"  // from @flatbuffers\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```"]}, {"number": 50246, "title": "Inconsistent gather_nd behavior on empty tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n- Python version: 3.9.4\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nUsing tf.gather_nd with empty tensors raises an invalid argument error, but works if tensors are not empty. tf.gather does not have this problem.\r\n\r\nFor example, this raises \"Invalid argument: Requested more than 0 entries, but params is empty.\"\r\n```python\r\ntf.gather_nd(tf.zeros([2, 0, 5]), [[0]])\r\n```\r\n\r\nThis alternative using tf.gather should be equivalent, but does not raise any error.\r\nIt returns an empty tensor of the right shape (1, 0, 5).\r\n```python\r\ntf.gather(tf.zeros([2, 0, 5]), [0])\r\n```\r\n\r\nThe tf.gather_nd example works fine if the tensor is not empty. This returns a tensor of shape (1, 1, 5).\r\n```python\r\ntf.gather_nd(tf.zeros([2, 1, 5]), [[0]])\r\n```\r\n\r\n**Describe the expected behavior**\r\ntf.gather_nd should be returning empty tensors of the appropriate shape just as tf.gather does. Note that while in this example tf.gather is an alternative, this is not always the case. For example, gathering based on multi-dimensional indices would force using tf.gather_nd, and any code that might face empty tensors now needs to handle these as corner cases.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe single line examples above are enough to reproduce the issue.", "comments": ["@leandro-gracia-gil \r\n\r\nCould you please refer [link1](https://www.tensorflow.org/api_docs/python/tf/gather_nd), [link2](https://www.programmersought.com/article/31143915811/), hope it helps.Thanks", "I am aware of the documentation. That is precisely why I'm reporting this as an inconsistent behavior.", "@leandro-gracia-gil,\r\nCan you please let us know why do you want to use Empty Tensors? Thanks! ", "Rather than using empty tensors, what I want is not to have to code workarounds for tf.gather_nd if one of my tensors happens to be empty.\r\n\r\nMy use case involves using tf.gather_nd with tensors in a nested structure. Depending on the situation, some of the tensors might have an inner dimension (not indexed by tf.gather_nd) with size 0, effectively making the whole tensor empty. However, because tf.gather_nd fails with empty tensors even if indexing does not happen in empty dimensions, the code fails in these cases. This is not the case of tf.gather, which correctly returns an empty tensor of the right shape after indexing the appropriate non-empty axes.\r\n\r\nUnfortunately, I actually need to index 2 dimensions at once based on tensor indices, which means I must use tf.gather_nd. So, I'm being forced to apply ugly workarounds with tf.cond and tf.size to detect when a tensor is empty and avoid using tf.gather_nd altogether in these cases. The fact that tf.gather works fine with empty tensors but tf.gather_nd does not in equivalent scenarios is why I decided to submit this as an inconsistent behavior issue.\r\n\r\nHope this clarifies things a bit. The code I showed above is just the simplest way to reproduce the issue I could find.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Replying to avoid it from closing automatically. Let me know if any further info is required."]}, {"number": 50244, "title": "Weird loss-inconsistencies and worse accuracy compared to older versions", "body": "\r\n**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Windows 10\r\n- Mobile device: -\r\n- TensorFlow installed from: pip install tensorflow-gpu\r\n- TensorFlow version: v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.7.9\r\n- Bazel version: -\r\n- GCC/Compiler version: -\r\n- CUDA/cuDNN version: 11.3\r\n- GPU model and memory: Nvidia GTX 1060\r\n\r\n\r\nI am working with ResNet50 from keras and am currently unable to achieve the same results I do with TF 1.13.1 in the newer TF 2.4.1.\r\nFirstly the weights in ResNet50 have a very different range of values compared to TF 1.13.1.\r\nIn the old version for example one layer (i.e. model.layers[-6]) with 512*2048 weights has values between -0.15 and +0.15 while TF 2.4.1 has only values between -0.05 and +0.05. I also let the he_normal initializer generate 100 random numbers in TF 1.13.1 and TF 2.4.1 each and the old version also produces a range of values that is 3 times higher than the new version's one.\r\n**Why did that change?**\r\n\r\nI now tried to save the weights in TF 1.13.1 and import them in TF 2.4.1 to see if this is the breaking change that makes my accuracy and training progress worse.\r\nHowever when I evaluate the untrained, randomly initialized model I get weird CategoricalCrossentropy loss values from model.evaluate.\r\n\r\n\r\n\r\nOn TF 1.13.1 I executed the following code to generate the saved weights file:\r\n```\r\nmodel = keras.applications.resnet50.ResNet50(weights=None, include_top=True, classes=5)\r\nmodel.compile(loss=\"categorical_crossentropy\", metrics=['accuracy','categorical_crossentropy'])\r\nmodel.save(\"TF-1-13-1before_training\")\r\n```\r\n\r\nOn TF 2.4.1 I loaded a custom dataset and the model:\r\n```\r\nmodel = keras.models.load_model(\"TF-1-13-1before_training\")\r\nprint(x_train.shape)  # (10, 224, 224, 3) 10 color-images, 2 for each of the 5 classes 224x224\r\nprint(y_train.shape)  # (10, 5) 10 times a one-hot encoded vector for 5 classes\r\n\r\npredictions = model.predict(x_train)\r\ncce = keras.losses.CategoricalCrossentropy()\r\n\r\ncce(y_train, predictions).eval(session=tf.compat.v1.Session())  # Yields 12.894476\r\nmodel.evaluate(x=x_train, y=y_train)  # Yields [463.46978759765625, 0.2, 463.4698]\r\nmodel.metrics_names  # gives ['loss', 'acc', 'categorical_crossentropy']\r\n```\r\n```\r\nprint(y_train)\r\n[[1. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0.]\r\n [0. 1. 0. 0. 0.]\r\n [0. 1. 0. 0. 0.]\r\n [0. 0. 1. 0. 0.]\r\n [0. 0. 1. 0. 0.]\r\n [0. 0. 0. 1. 0.]\r\n [0. 0. 0. 1. 0.]\r\n [0. 0. 0. 0. 1.]\r\n [0. 0. 0. 0. 1.]]\r\n```\r\n```\r\nprint(predictions)\r\n[[1. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0.]]\r\n```\r\n\r\n**Why does model.evaluate return different categorical_crossentropy loss values than the standalone CCE function for the same data? How can the high value of ~463 be explained in contrast to the expected value of 12.89?**\r\nThe standalone CategoricalCrossentropy function applied on predictions and target values seems correct, the return value of model.evaluate on the other hand does not.\r\n\r\nThank you in advance for your help", "comments": ["@M-Wolff ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code of both versions and dataset  or colab link to reproduce the issue reported here.\r\n\r\nThanks!", "@tilakrayal \r\nI created a github repo with 2 code files, one for TF 2.4.1 and one for TF 1.13.1.\r\nPlease run TF 1.13.1 first because this will save the weights of this version to the disk to be used in TF 2.4.1 code.\r\nIt also includes the output generated on my machine.\r\nTo be a bit more specific, I am planning on training on a HPC Cluster, thus the available software is somewhat limited. For running with TF 1.13.1 I have to load:\r\nGCC/8.2.0-2.31.1\r\nCUDA/10.1.105\r\nOpenMPI/3.1.3\r\nTensorFlow/1.13.1-Python-3.7.2\r\n\r\nFor TF 2.4.1 the software is:\r\nGCC/10.2.0\r\nCUDA/11.1.1\r\nOpenMPI/4.0.5\r\nTensorFlow/2.4.1\r\n\r\nGPU for TF 1.13.1 and TF 2.4.1 was a RTX2080 Ti by Nvidia this time.\r\n\r\nWhat I find odd about this issue:\r\n\r\n- initialized weights in TF 1.13.1 are between -0.14 ... 0.14, in TF 2.4.1 just between -0.05 ... 0.05\r\n- Metric (CCE via model.evaluate) is at 12.89 for TF 1.13.1, and at 1.61 for TF 2.4.1 with a randomly initialized Resnet50\r\n- Predictions in TF 1.13.1 are vectors with exactly one value at 1.0 and all others at 0.0, in TF 2.4.1 the probability spreads out evenly across all classes (~0.2 for each of the 5 classes)\r\n- Loading saved model (from TF 1.13.1) in TF 2.4.1 produces ridicoulusly high loss-values (~742) but produces the same predictions as in TF 1.13.1\r\n- Standalone loss in TF 2.4.1 (with loaded model from Tf 1.13.1) applied on the predictions of the ResNet50 and y_train labels (categorical: i.e. [0,0,0,0,1] for class number 5) produces same loss value as model.evaluate in TF 1.13.1, but very different from model.evaluate in TF 2.4.1 for the same model.\r\n\r\nThe mentioned github repo can be found here, dataset consists of a very small subset (10 images total for 5 classes) of SwedishLeaves dataset with images in .tif format, also uploaded in my github repo:\r\nhttps://github.com/M-Wolff/TF-Bug\r\nThe saved initial weights file of TF 1.13.1 is also uploaded there\r\n\r\n\r\nThank you for taking the time to look into this issue!\r\n\r\n\r\nEDIT:\r\nWeight differences in randomly initialized models are not just unlucky outliers. This is a scatterplot of the weight values in layer [-6]. Red for TF 2.4.1, Blue for TF 1.13.1.\r\n![tfdiscord2](https://user-images.githubusercontent.com/56885160/121949125-67f72b80-cd58-11eb-8420-6e7014d8f9c8.PNG)\r\n", "Can you try with `tf.compat.v1.keras.applications.resnet50` in your TF 2 and see if the results are still different.", "I tried with `tf.compat.v1.keras.applications.resnet50.ResNet50(...)` for ResNet50 initialization and `tf.compat.v1.keras.models.load_model(...)` for loading the model of TF 1.13.1 into TF 2.4.1 and it produces the exact same results as using TF 2.4.1 without the `compat.v1` part. So no improvement there.\r\n\r\nCode file as well as output generated on my machine are also included in the [github repo](https://github.com/M-Wolff/TF-Bug)", "Update:\r\nI tried the same thing with the InceptionV3 network `tf.keras.applications.inception_v3.InceptionV3` in both versions as well as with the `compat.v1` part in TF 2.4.1 and all three of them behave as expected and produce the same results.\r\nConsistent loss values of ~1.6 and spread out probabilities of ~20% for each class.\r\n\r\nBecause ResNet50 also has a loss of ~1.6 and spread out probabilities of ~20% for each class in the new version I assume that the old version (TF 1.13.1) has the bug and the saved (faulty?) model of that version can't be loaded into newer versions (?).\r\n\r\nWhat I find surprising is that the old Version which I suppose has the bug performs way better than the new version.\r\nUsing very few images as a training set (like in my [github repo](https://github.com/M-Wolff/TF-Bug)) the old version reaches 50-60% accuracy on a test set, the new version gets stuck at ~20% for 5 classes and is thus not better than randomly guessing.\r\n\r\nExample of my training progress:\r\nTF 1.13.1\r\n![TF-1-13-1-picklePlot](https://user-images.githubusercontent.com/56885160/122429837-5f3e6980-cf93-11eb-952c-75cf50fe6e12.png)\r\nTF 2.4.1\r\n![TF-2-4-1-picklePlot](https://user-images.githubusercontent.com/56885160/122429867-66657780-cf93-11eb-9de8-8a13e699319f.png)\r\n\r\nThe top graph is TF 1.13.1 with keras 2.2.4, starts with a high loss value on test set (~12.8), the loss then drops and the accuracy increases as expected.\r\nThe lower graph is TF 2.4.1 loss on validation set starts low and even increases, accuracy has one small peak but remains somewhat unchanged.\r\n\r\nAlso wondering why train and test set have very different loss values in TF 1.13.1 in a randomly initialized model.\r\nGraphs were produced by reading the history returned by model.fit_generator() on both versions.\r\nFor these graphs the train set consisted of `keras.preprocessing.image.ImageDataGenerator` and the test set was just read with `np.asarray(tflearn.data_utils.image_preloader(...))`", "Another Update:\r\nI just tried out the same experiment without using the `ImageDataGenerator`. I simply load the images (2 per class for the training set, 50 per class for the validation set; 5 classes total) into a numpy array and start the training on ResNet50 and InceptionV3 in the old and new version.\r\n\r\nSurprisingly TF 2.4.1 is significantly worse than the old version as this graph shows.\r\nTop is old version, bottom is new version. Left ResNet50 and right InceptionV3.\r\n\r\nLoss in the new version on the validation set increases, in the old version it decreases as expected. Also the range of values for the loss is very different even at the beginning of training the randomly initialized model.\r\n![results](https://user-images.githubusercontent.com/56885160/122450664-8b63e580-cfa7-11eb-8808-e3a203367c54.png)\r\n\r\n**Am I doing something terribly wrong in [my training script](https://github.com/M-Wolff/TF-Bug/blob/main/full_training/training_comparison.py) or is the old version of TF simply better (for small training sets)?**\r\nWhen using a larger training set size (25 images for each class as training set, validation set is still 50 images per class without overlapping the train set) the differences between the version seem to matter less:\r\n![results](https://user-images.githubusercontent.com/56885160/122453093-42616080-cfaa-11eb-8f64-9d1948fd4a10.png)\r\n\r\nThe newer version seems smoother and more stable this time with a bigger training set, however the loss values still are very different (up to 80 in new ResNet50, only ~13 in old one).\r\n\r\nWhat could cause this odd behaviour?", "@M-Wolff , I'm reading through the thread, and I'm having trouble understanding what the bug exactly is. It's clear that there are some subtle (and some not so subtle) differences between TF1.x and TF2.x but that no surprise, and can't be totally eliminated. It's also clear that saving a model in TF1.x and loading it in TF2.x also has some subtle issues (and AFAIC is not a supported feature). Many of the examples I see here are comparing training behavior on very small datasets that can't be used to reason about modern NN optimization algorithms.\r\n\r\nCould you please summarize the bug more precisely?", "@deeb02 Thank you for your reply. Loading old models into newer versions not being a supported feature explains the high loss values, and it seems that the weird output prediction of a randomly initialized network for ResNet50 in TF 1.13.1 (i.e. for 5 classes [0, 0, 0, 1, 0]) has been fixed in newer versions and now produces in TF 2.4.1 [0.2, 0.2, 0.2, 0.2, 0.2] as expected.\r\n\r\nBut I still wonder what change would result in TF 2.4.1 being far worse at training with few training samples than the old TF 1.13.1.\r\nFor example the swedish leaves dataset (images of swedish leaves centered on a plain white background; very high similarity within one class) performs just fine in the old version of TF with 2-4 samples per class (always above 55%) and the new version can't do much better than randomly guessing (10% accuracy for 10 classes). I do understand that this is probably an edge case and not a general bug in TensorFlow, but I would definitely say this is undesired behaviour.\r\n\r\nI spent the last few days training lots of networks on different datasets in both versions of TF for comparison (~5000 GPU hours total) and it seems that on larger training splits the accuracy for One-Hot encoded predictions increased in the newer version compared to an older version of TF. For very small training splits the accuracy always got very much worse.\r\nWhen applying a custom encoding (what I actually planned to work on) in the output layer, and not the default One-Hot encoding, the newer version of TF is producing worse accuracies most of the time. But this is also a very specific use-case that Tensorflow was probably not designed for.\r\n\r\n\r\nSo to summarize the _bug_ or weird behaviour: \r\nAm I correct that unless there is some severe mistake in my [code](https://github.com/M-Wolff/TF-Bug/blob/main/full_training/training_comparison.py) it might be a good approach to try the same code on the same dataset again in an older version of TF when one can't produce good results in a more recent version (especially when working on very small training sets or when using custom encodings / loss functions)?\r\n", "@M-Wolff This is a stale issue. \r\n\r\nCan you please check with recent TF versions and let us know whether this was resolved with recent TF versions? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@jvishnuvardhan\r\n![new_comparison](https://user-images.githubusercontent.com/56885160/153570859-2a4509ff-5a71-4b9c-a7de-036aca3385e4.png)\r\n\r\nI took the time to reproduce the same environment and dataset as when I created this issue\r\n 8 months ago. By now I had already cleaned up my working directory as my project was completed months ago.\r\n\r\nIn TensorFlow 2.8.0 (latest release) the behavior still matches the one of TF 2.4.1 described in my previous posts. In TF 1.13.1 the CNN (ResNet-50 and InceptionV3) can achieve 50-70% accuracy on a test split, while in newer TF versions there is no learning progress at all, accuracy stays at 20% for 5 classes (as good as randomly guessing). Results can be compared in the picture above. The dataset consists of the Swedish-Leaf dataset (high similarity of images within a given class) with 2 images per class for training and 50 images per class for testing. 5 classes total. The code is the same one I linked earlier.\r\n\r\nI do understand that this seems like an edge case, but in my project I trained 3770 CNN's in TF 1.13.1 and TF 2.4.1 **each** and got to the result that something weird happened somewhere between TF 1.13.1 and TF 2.4.1 (or I did something terribly wrong and nobody has noticed yet).\r\nI analyzed the effects of some changes in the output layer of the CNN's: TF 1.13.1 and PyTorch behaved similarly and as expected, however TF 2.4.1 did not. This goes to show that something must have changed in TensorFlow that affects it's performance in a bad way as far as I can tell.\r\n\r\nCurrently it seems like if you are training on a very small dataset, and don't get good results, trying the same exact experiments in an older version of TensorFlow might be a good idea to improve the accuracies.\r\n", "@M-Wolff Can you please check if you are implementing normalization twice on your inputs? I see you have a normalization in your preprocessing (normalize=True).\r\n\r\nI will looks into this later today but if you can run normalize=False, do you get any better results? Thanks!", "@jvishnuvardhan\r\nI used normalize=True in the image_preloader call twice, because I load training and testing data separately from different folders.\r\nThat flag should just change the intervals I get in the loaded image arrays from 0...255 to 0...1 according to the documentation of tflearn.\r\n\r\nInterestingly, setting it to normalize=False did change my results.\r\nThe graph shows TF 2.8.0 with normalize=False, the graphs for TF 1.13.1 are the same ones I posted earlier (with normalize=True) as the goal is to improve the results in TF 2.8.0 and not make them worse in TF 1.13.1 so they match.\r\n![new_comparison_TF280-normalize-False](https://user-images.githubusercontent.com/56885160/153697776-5c1b6990-7ee2-43ef-b920-43d601d9c8ba.png)\r\n\r\nFocusing on the graphs for TF 2.8.0 **without** normalization (as comparison with TF 1.13.1 is now limited, since the graphs for TF 1.13.1 still show normalize=True) the following questions arise:\r\n\r\n1. Why does the test loss go up zo 7000 or 2500 without the test accuracy changing at all (before epoch 50)?\r\n2. test accuracy spikes up to good values in the middle, but drops just as fast again. Why?\r\n3. The InceptionV3 CNN actually managed to go below 20% accuracy in the very end for 5 classes. It is now even worse than randomly guessing. Why is that happening?\r\n4. The Resnet-50 CNN goes up to good accuracies as well, but then drops and ends at 25% (better as before but still not as good as TF 1.13.1). Why does it drop at all, shouldnt it stay in an upwards trend, as training tries to improve the accuracy of predictions?\r\n", "Could you try with different dataset and check if you notice the same behavior. \r\nThe results in different versions are not expected even when you set the seed, there will always be minor error difference due to floating point precision error.\r\nAdditionally, you can also try with different [initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers?version=nightly) which is best suited for your usecase.", "I am aware that I can not expect the same exact accuracy due to limited floating point precision and other factors.\r\n\r\nAlso I am not trying to improve performance in TF 2.4.1 by using an initializer that is best suited for my usecase, I am only trying to understand why switching from TF 1.13.1 to TF 2.4.1 and leaving every other parameter as it was has such a big impact on validation accuracy.\r\nThe code was producing good results in TF 1.13.1 and just by switching to TF 2.4.1, using the same exact code and datasets, the accuracy got way worse in some cases.\r\n\r\nI trained on 5 different datasets. Other parameters that were varied in each run were the amount of classes used, size of the train-split, training from scratch vs. using pre-trained networks, different networks (ResNet-50, InceptionV3 and InceptionV3 with changes), ...\r\nThis resulted in 3770 different combinations to train neural networks on.\r\n\r\nI did this in TF 2.4.1 and did not get the results I was expecting, so I used the same exact setup of datasets and parameters and trained using the same exact code in TF 1.13.1 and suddenly the results were better and produced trends as desired (regarding some changes in parameters).\r\n\r\nTo visualize this, here is a scatterplot showing the difference between both versions. Each dot represents the difference in validation accuracy of two **exact same** training runs in TF 2.4.1 vs. TF 1.13.1. Positive values mean that TF 2.4.1 is better, negative values that TF 1.13.1 is performing better (accuracy on test-split).\r\n![AllDatasets](https://user-images.githubusercontent.com/56885160/162606647-8b271fa5-ffb9-4663-bb3f-ca18249d613f.png)\r\n\r\nAnd yes, I double-checked, the performance in TF 2.4.1 is sometimes really 80%-90% worse (e.g. 96% accuracy in TF 1.13.1 and only 6% accuracy in TF 2.4.1 with the same setup).\r\n\r\nGeneral trend in that diagram is that TF 2.4.1 is in many cases performing approximately as good as TF 1.13.1 but there are cases where there is a huge difference, and those cases don't seem too rare in my analysis.\r\nSeems to happen mostly when there is only a small amount of training images.\r\n\r\nRestricting the scatterplot from above to only the Cifar dataset results in this scatterplot:\r\n![Cifar](https://user-images.githubusercontent.com/56885160/162606836-7bc554b4-b742-40ac-b2c8-4e6c7aba0dfe.png)\r\nProbably because Cifar has a lot of images, even when the training-split is limited to 10% of the images there are enough pictures present so that this weird behaviour doesn't occur. Training on larger datasets (e.g. Cifar) works well in both versions.\r\n\r\n\r\nHowever the main question or issue here still persists: Training on small datasets (using very few images in the train-split) is working fine in TF 1.13.1 and in TF 2.4.1 validation accuracy decreases by a lot.\r\n\r\nSo when faced with a problem that requires training on a small dataset, it seems that one should prefer the old version above the newer TF versions. \r\nHow can this strange behaviour be explained?\r\n", "Thanks for the detailed insight, could you please create this issue in keras-team/keras repo for faster resolution.\r\nYou can link this issue to the new issue as reference to all your insights.\r\n\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I opened [this issue](https://github.com/keras-team/keras/issues/16444) in keras-team/keras.\r\nIf the google-ml-butler bot doesn't decide otherwise I will comment with an update here as well, if the issue gets resolved at some point in the future.\r\n\r\nI certainly find it a bit annoying, that the bot starts to threaten to close this issue if I don't find the time to reply within one week, especially since there was no response from your side for **7 months** at some point...\r\n\r\nAdditionally, after reading through my latest response again, I noticed that I messed up the y-axis labels in the plots. It should of course be the other way around: \"TF 2.4.1 minus TF 1.13.1\", so negative values actually correspond to TF 1.13.1 being better (higher accuracy). Sorry!"]}, {"number": 50220, "title": "Make tf.io.gfile / tf.data separate (standalone) packages?", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.x\r\n- Are you willing to contribute it (Yes/No): No (time constraints) \r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nAt the moment we have to install `tensorflow` as a whole even though our data-processing pipeline only requires access to `tf.gfile` and/or `tf.data`.\r\n\r\nImo it could be beneficial if it was possible to install these two packages separately (standalone) without having to install the rest of Tensorflow. Especially `tf.gfile` could be useful in projects where not a single line related to machine-learning is written so this would be a candidate to become its own `pip`-installable package maybe with the option to install the `tf.data` functionalities. \r\n\r\n```\r\npip install tensorflow_gfile        # gfile only\r\npip install tensorflow_gfile[data]  # gfile + data\r\npip install tensorflow              # All the things\r\n```\r\n\r\n**Will this change the current api? How?**\r\n\r\nThe API should not change, only the fact that packages can be selectively installed.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone who works on larger projects.\r\n", "comments": ["There is no plan for this as it does not bring enough benefits to TF team to justify spending ~2 quarters for the separation.", "I can't really comment on how long this would take and whether or not the TF team itself would benefit from this but I have the feeling that particularly `tf.io.gfile` could be a separate thing. It's really useful and I think this is true even outside of a machine-learning setting. \r\n\r\nI personally would also like to be able to e.g. write shards in a separate pipeline without having to install the entire Tensorflow framework.\r\n\r\nHowever, please close this issue if my suggestion doesn't make sense whatsoever. ", "This would be a target of modularization efforts but so far all such efforts in the past few years have been discontinued. When our team grows again we can try launching a new process. If approved, we can work on this.\r\n\r\nFor now, I'd keep this open, as a reminder that we could do this.", "How i can contribute\r\nplease guide me\r\nI like to solve this problem"]}, {"number": 50208, "title": "Tensorflow-2.5.0 does not build with tensorrt", "body": "Tried to build tensorflow-2.5.0 with tensorrt but failed with\r\n```\r\n/home/bernard/opt/python38/tensorflow-2.5.0/tensorflow/compiler/tf2tensorrt/BUILD:39:11: C++ compilation of rule '//tensorflow/compiler/tf2tensorrt:tensorrt_stub' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/tf2tensorrt/_objs/tensorrt_stub/nvinfer_stub.pic.d ... (remaining 149 argument(s) skipped)\r\nIn file included from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:56:\r\n./tensorflow/compiler/tf2tensorrt/stub/NvInfer_5_0.inc:5:7: error: declaration of \u2018void* createInferBuilder_INTERNAL(void*, int)\u2019 has a different exception specifier\r\n    5 | void* createInferBuilder_INTERNAL(void* logger, int version) {\r\n      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:17:\r\nbazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:8083:30: note: from previous declaration \u2018void* createInferBuilder_INTERNAL(void*, int32_t) noexcept\u2019\r\n 8083 | extern \"C\" TENSORRTAPI void* createInferBuilder_INTERNAL(void* logger, int32_t version) noexcept;\r\n      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:56:\r\n./tensorflow/compiler/tf2tensorrt/stub/NvInfer_5_0.inc:12:7: error: declaration of \u2018void* createInferRuntime_INTERNAL(void*, int)\u2019 has a different exception specifier\r\n   12 | void* createInferRuntime_INTERNAL(void* logger, int version) {\r\n      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from bazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:54,\r\n                 from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:17:\r\nbazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInferRuntime.h:2253:30: note: from previous declaration \u2018void* createInferRuntime_INTERNAL(void*, int32_t) noexcept\u2019\r\n 2253 | extern \"C\" TENSORRTAPI void* createInferRuntime_INTERNAL(void* logger, int32_t version) noexcept;\r\n      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:56:\r\n./tensorflow/compiler/tf2tensorrt/stub/NvInfer_5_0.inc:33:28: error: declaration of \u2018nvinfer1::IPluginRegistry* getPluginRegistry()\u2019 has a different exception specifier\r\n   33 | nvinfer1::IPluginRegistry* getPluginRegistry() {\r\n      |                            ^~~~~~~~~~~~~~~~~\r\nIn file included from bazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:54,\r\n                 from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:17:\r\nbazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInferRuntime.h:2264:51: note: from previous declaration \u2018nvinfer1::IPluginRegistry* getPluginRegistry() noexcept\u2019\r\n 2264 | extern \"C\" TENSORRTAPI nvinfer1::IPluginRegistry* getPluginRegistry() noexcept;\r\n      |                                                   ^~~~~~~~~~~~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 7109.058s, Critical Path: 184.50s\r\nINFO: 23489 processes: 8232 internal, 15257 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\nIt builds successfully if tensorrt is not chosen when configure.\r\n\r\nSystem Ubuntu 20.04\r\n\r\npython-3.8.10\r\n\r\ngcc 9.3.0\r\n\r\nbazel 3.7.2\r\n\r\nNvidia GTX1070\r\n\r\ncuda 11.0\r\n\r\ntesorrt 8 \r\n\r\ncudnn 8", "comments": ["TF 2.5 is compatible with cuda 11.2 and cudnn 8.1 you may want to try upgrading those.", "Yes, but cuda 11.2 breaks other stuffs.  Many downstream projects barely support  cuda 11.0. If TF25 is compatible with 11.2 shouldn't it be compatible with 11.0?", "@ymodak\r\nAccording to Nvidia's websites  tensorrt doesn't support 11.2 + cudnn 8.1, but 11.2 update2 + cuddnn 8.2. I don't see how your combination can work.\r\n\r\nhttps://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html\r\n\r\nhttps://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html\r\n\r\n"]}]