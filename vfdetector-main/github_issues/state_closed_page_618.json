[{"number": 35109, "title": "add version information to the dll, such as tensorlow_112.dll", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): msvc2015\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: NVIDIA GTX 1080TI   X 11G\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI can build the tensorflow_cc.dll and use the tensorflow_C++ with GPU.\r\nThe problem is that I want to add the version information to the .dll File so that I can use two different version of tensorflow (tensorflow_CC_112.dll and tensorflow_CC_14.dll) in a single .exe.\r\n\r\n\r\n\r\n", "comments": []}, {"number": 35108, "title": "tf.py_function is unusable in map function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.0/7\r\n- GPU model and memory: Tesla T4, 16gb\r\n\r\n\r\n**Describe the current behavior**\r\nAttempting to use `tf.py_function` inside a `tf.data.Dataset.map`. Failure occurs because the  tensor returned from py_function is accessed during build time of the graph, but function is expected to be evaluated during runtime of the graph. \r\n\r\n**Attempt 1**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n#make magical data that requires python\r\ndef make_data(i): \r\n    return np.cast[np.uint8](i) * np.ones([20,256,256,3], dtype=np.float32) / 10.\r\n\r\n#clean up magical data\r\n@tf.function \r\ndef make_clean_data(i): \r\n    ones = tf.py_function(make_data,[i],tf.float32) \r\n    ones = tf.reshape(ones,ones.get_shape()) \r\n    ones =tf.image.resize(ones,[224,224]) \r\n    return ones\r\n\r\nds = tf.data.Dataset.range(10)\r\nds.map(make_clean_data)\r\n```\r\nError: \r\n```\r\nValueError: in converted code:\r\n\r\n    <ipython-input-13-6dd8c09d760a>:8 make_clean_data  *\r\n        ones = tf.reshape(ones,ones.get_shape())\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:131 reshape\r\n        result = gen_array_ops.reshape(tensor, shape, name)\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py:8117 reshape\r\n        \"Reshape\", tensor=tensor, shape=shape, name=name)\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py:545 _apply_op_helper\r\n        (input_name, err))\r\n\r\n    ValueError: Tried to convert 'shape' to a tensor and failed. Error: Cannot convert a partially known TensorShape to a Tensor: <unknown>\r\n```\r\n\r\n**Attempt 2**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n#make magical data that requires python\r\ndef make_data(i): \r\n    return np.cast[np.uint8](i) * np.ones([20,256,256,3], dtype=np.float32) / 10.\r\n\r\n#clean up magical data\r\n@tf.function \r\ndef make_clean_data(i): \r\n    ones = tf.py_function(make_data,[i],tf.float32) \r\n    ones.set_shape(ones.get_shape()) \r\n    ones =tf.image.resize(ones,[224,224]) \r\n    return ones\r\n\r\nds = tf.data.Dataset.range(10)\r\nds.map(make_clean_data)\r\n```\r\nError\r\n```\r\nValueError: in converted code:\r\n\r\n    <ipython-input-14-311bd0ebc814>:9 make_clean_data  *\r\n        ones =tf.image.resize(ones,[224,224])\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1319 resize_images_v2\r\n        skip_resize_if_same=False)\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1034 _resize_images_common\r\n        raise ValueError('\\'images\\' contains no shape.')\r\n\r\n    ValueError: 'images' contains no shape.\r\n```\r\n**Attempt 3**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n#make magical data that requires python\r\ndef make_data(i): \r\n    data = np.cast[np.uint8](i) * np.zeros([20,256,256,3], dtype=np.float32) / 10.\r\n    return  data, data.shape\r\n\r\n#clean up magical data\r\n@tf.function \r\ndef make_clean_data(i): \r\n    ones, shape = tf.py_function(make_data,[i],(tf.float32, tf.int32)) \r\n    ones.set_shape(shape) \r\n    ones =tf.image.resize(ones,[224,224]) \r\n    return ones\r\n\r\nds = tf.data.Dataset.range(10)\r\nds.map(make_clean_data)\r\n```\r\nError\r\n```\r\nOperatorNotAllowedInGraphError: in converted code:\r\n\r\n    <ipython-input-11-a2af44684940>:9 make_clean_data  *\r\n        ones.set_shape(shape)\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:626 set_shape\r\n        shape = tensor_shape.TensorShape(shape)\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:776 __init__\r\n        self._dims = [as_dimension(d) for d in dims_iter]\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:776 <listcomp>\r\n        self._dims = [as_dimension(d) for d in dims_iter]\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:547 __iter__\r\n        self._disallow_iteration()\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:540 _disallow_iteration\r\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:518 _disallow_when_autograph_enabled\r\n        \" decorating it directly with @tf.function.\".format(task))\r\n\r\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n```\r\n**Attempt 4**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n#make magical data that requires python\r\ndef make_data(i): \r\n    data = np.cast[np.uint8](i) * np.zeros([20,256,256,3], dtype=np.float32) / 10.\r\n    return  data, data.shape\r\n\r\n#clean up magical data\r\n@tf.function \r\ndef make_clean_data(i): \r\n    ones, shape = tf.py_function(make_data,[i],(tf.float32, tf.int32)) \r\n    ones = tf.reshape(ones, shape) \r\n    ones =tf.image.resize(ones,[224,224]) \r\n    return ones\r\n\r\nds = tf.data.Dataset.range(10)\r\nds.map(make_clean_data)\r\n```\r\nError\r\n```\r\nValueError: in converted code:\r\n\r\n    <ipython-input-16-e967bbec561a>:13 make_clean_data  *\r\n        ones =tf.image.resize(ones,[224,224])\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1319 resize_images_v2\r\n        skip_resize_if_same=False)\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1034 _resize_images_common\r\n        raise ValueError('\\'images\\' contains no shape.')\r\n\r\n    ValueError: 'images' contains no shape.\r\n```\r\n**Attempt 5**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n#make magical data that requires python\r\ndef make_data(i): \r\n    data = np.cast[np.uint8](i) * np.zeros([20,256,256,3], dtype=np.float32) / 10.\r\n    return  data, data.shape\r\n\r\n#clean up magical data\r\n@tf.function \r\ndef make_clean_data(ones, shape): \r\n    ones.set_shape(shape) \r\n    ones =tf.image.resize(ones,[224,224]) \r\n    return ones\r\n\r\nds = tf.data.Dataset.range(10)\r\nds = ds.map(lambda i:  tf.py_function(make_data,[i],(tf.float32, tf.int32)))\r\nds.map(make_clean_data)\r\n```\r\nError\r\n```\r\nOperatorNotAllowedInGraphError: in converted code:\r\n\r\n    <ipython-input-12-6bfeded17ffe>:8 make_clean_data  *\r\n        ones.set_shape(shape)\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:626 set_shape\r\n        shape = tensor_shape.TensorShape(shape)\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:776 __init__\r\n        self._dims = [as_dimension(d) for d in dims_iter]\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:776 <listcomp>\r\n        self._dims = [as_dimension(d) for d in dims_iter]\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:547 __iter__\r\n        self._disallow_iteration()\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:540 _disallow_iteration\r\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\r\n    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:518 _disallow_when_autograph_enabled\r\n        \" decorating it directly with @tf.function.\".format(task))\r\n\r\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n```", "comments": ["Issue is replicating on Colab with Tf 2.0. \r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/d0078d66c22d6f80cb6af30c713fc01b/untitled307.ipynb). Thanks!", "@hollowgalaxy What is the expected output of `make_data` function and from where that functional argument  `i` is coming from? Thanks! ", "Not sure which of the attempts you mean. But the `make_data` function is described for each of the attempts. ", "`py_function` produces tensors of unknown shape and rank (as shape inference does not work on arbitrary Python code) and if you need the shape (e.g. for `tf.image.resize`) you need to set it based on statically available information (i.e. it cannot be something that `py_function` returns).\r\n\r\nFor example, you can use `set_shape` to set the rank of the `py_function` output to 4, which will be sufficient for `tf.image.resize` to operate correctly:\r\n\r\n```\r\nimport tensorflow.compat.v2 as tf\r\nimport numpy as np\r\n\r\ntf.enable_v2_behavior()\r\n\r\n#make magical data that requires python\r\ndef make_data(i): \r\n    return np.cast[np.uint8](i) * np.ones([20,256,256,3], dtype=np.float32) / 10.\r\n    \r\n\r\n#clean up magical data\r\ndef make_clean_data(i): \r\n    ones = tf.py_function(make_data,[i],tf.float32) \r\n    ones.set_shape(tf.TensorShape([None, None, None, None]))\r\n    ones = tf.image.resize(ones, [224,224])\r\n    return ones\r\n\r\nds = tf.data.Dataset.range(10)\r\nds = ds.map(make_clean_data)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35108\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35108\">No</a>\n"]}, {"number": 35107, "title": "tf.keras.layers.BatchNormalization() may not work in tf=2.0 and eager model is disable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 in Docker\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 10.0 / 7\r\n- GPU model and memory: GTX 1080Ti / 11175MiB\r\n\r\n**Describe the current behavior**\r\n\r\nHi authors and developers,\r\n\r\nI am developing our project in tf=2.0.0 and eager_mode is disable.\r\n\r\nThe main reason is tf=1.x will not be maintained but third party libraries have not been ready for tf=2.0 yet.\r\n\r\nThis issues is a separate issues from [#35050](https://github.com/tensorflow/tensorflow/issues/35050#issuecomment-565395512)\r\n\r\nThis potential issue is somethine wrong if users do custom training with level API which includes `tf.keras.layers.BatchNormalization()` in tf=2.0 and eager model is disable.\r\n\r\nI summary the testcaset as the following:\r\n\r\n```python\r\n#%%\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n#tf.compat.v1.disable_v2_behavior()\r\n\r\nimport numpy as np\r\n\r\nbatch_size = 100\r\n\r\ndef download_data():\r\n\r\n    # get raw data\r\n    (trainX, trainY), (testX, testY) = tf.keras.datasets.cifar10.load_data()\r\n    trainX = trainX.astype(np.float32)\r\n    testX  = testX.astype(np.float32)\r\n\r\n    # ont-hot\r\n    trainY = tf.keras.utils.to_categorical(trainY, 10)\r\n    testY  = tf.keras.utils.to_categorical(testY , 10)\r\n\r\n    # get validation sets\r\n    training_size = 45000\r\n    validX = trainX[training_size:,:]\r\n    validY = trainY[training_size:,:]\r\n\r\n    trainX = trainX[:training_size,:]\r\n    trainY = trainY[:training_size,:]\r\n\r\n    return trainX, trainY, validX, validY, testX, testY\r\n\r\ndef data_pipeline(dataX, dataY):\r\n\r\n        dataset = tf.data.Dataset.from_tensor_slices( (dataX, dataY) )\r\n        dataset = dataset.shuffle(batch_size * 8)\r\n        dataset = dataset.repeat()\r\n        dataset = dataset.batch(batch_size)\r\n        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n        return dataset\r\n\r\nclass custom_model():\r\n    def __init__(self):\r\n\r\n        def Acc():\r\n            acc = tf.keras.metrics.categorical_accuracy(label_ref, clf_out)\r\n            return tf.math.reduce_mean(acc)\r\n\r\n        def c_loss():\r\n            loss = tf.keras.losses.categorical_crossentropy(label_ref, clf_out)\r\n            loss = tf.math.reduce_mean(loss)\r\n            return loss\r\n\r\n        # create model\r\n        clf_input = tf.keras.layers.Input(shape=(32,32,3), name=\"model/input\")\r\n        model = tf.keras.applications.resnet_v2.ResNet50V2(include_top=True, weights=None, input_tensor=clf_input, pooling='max', classes=10)\r\n        #model = tf.keras.applications.vgg16.VGG16(include_top=True, weights=None, input_tensor=clf_input, pooling='max', classes=10)\r\n        model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\r\n\r\n        label_ref = tf.keras.layers.Input(shape=(10,) , name='label_ref')\r\n        clf_out = model(clf_input)\r\n\r\n        # using tf.keras.optimizers.Nadam would get error\r\n        #optimizer = tf.keras.optimizers.Nadam(lr=0.0005)\r\n        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)\r\n        self.train_op = optimizer.minimize(c_loss(), var_list=[model.trainable_variables])\r\n\r\n        self.clf_model = model\r\n        self.clf_input = clf_input\r\n        self.label_ref = label_ref\r\n        self.op_acc = Acc()\r\n        self.c_loss = c_loss()\r\n\r\nif __name__ == '__main__':\r\n\r\n    # set GPU\r\n    import os\r\n    if os.environ.get(\"CUDA_VISIBLE_DEVICES\") is None:\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\n    # reset tf session\r\n    tf.compat.v1.keras.backend.clear_session()\r\n    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\r\n    sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\r\n    tf.compat.v1.keras.backend.set_session(sess) \r\n\r\n    # prepare data\r\n    trainX, trainY, validX, validY, testX, testY = download_data()\r\n    train_gen = data_pipeline(trainX, trainY)\r\n    valid_gen = data_pipeline(validX, validY)\r\n    test_gen = data_pipeline(testX, testY)\r\n\r\n    # build targeted model\r\n    model = tf.keras.applications.resnet_v2.ResNet50V2(include_top=True, weights=None, input_shape=(32,32,3), pooling='max', classes=10)\r\n    #model = tf.keras.applications.vgg16.VGG16(include_top=True, weights=None, input_shape=(32,32,3), pooling=None, classes=10)\r\n    model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\r\n\r\n    # fit and evalutate\r\n    model.fit(train_gen,\r\n            steps_per_epoch = trainY.shape[0] // batch_size,\r\n            validation_data = valid_gen,\r\n            validation_steps= validY.shape[0] // batch_size,\r\n            epochs=5,\r\n            verbose=2)\r\n    model.evaluate(testX, testY, verbose=2, batch_size=batch_size)\r\n\r\n    # create a new model\r\n    print('Make sure that we create a new model.')\r\n    model = custom_model()\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n    model.clf_model.evaluate(testX, testY, verbose=2, batch_size=batch_size)\r\n\r\n    # train model\r\n    num_epoch = 5\r\n    total_len = trainY.shape[0] // batch_size\r\n    tf_iter = tf.compat.v1.data.make_initializable_iterator(train_gen)\r\n    tf_next = tf_iter.get_next()\r\n    sess.run(tf_iter.initializer)\r\n    for epoch in range(num_epoch):\r\n        c_loss, acc = 0.0, 0.0\r\n        for ii in range(total_len):\r\n            X, Y = sess.run(tf_next)\r\n            [b_c_loss, b_acc, _] = sess.run([model.c_loss, model.op_acc, model.train_op],\r\n                                                feed_dict={ model.clf_input: X,\r\n                                                            model.label_ref: Y,\r\n                                                            tf.keras.backend.learning_phase(): 1})\r\n            c_loss = c_loss + b_c_loss\r\n            acc = acc + b_acc\r\n        \r\n        c_loss = c_loss / total_len\r\n        acc = acc / total_len\r\n        print('[Training]Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss, acc) )\r\n\r\n    print('Show loss and accuracy with keras API')\r\n    model.clf_model.evaluate(trainX, trainY, verbose=2, batch_size=batch_size)\r\n    model.clf_model.evaluate(validX, validY, verbose=2, batch_size=batch_size)\r\n    model.clf_model.evaluate(testX, testY, verbose=2, batch_size=batch_size)\r\n\r\n    print('Show loss and accuracy with low level API')\r\n    # evaluate\r\n    num_epoch = 1\r\n    total_len = validY.shape[0] // batch_size\r\n    tf_iter = tf.compat.v1.data.make_initializable_iterator(valid_gen)\r\n    tf_next = tf_iter.get_next()\r\n    sess.run(tf_iter.initializer)\r\n    for epoch in range(num_epoch):\r\n        c_loss_t, acc_t, c_loss_f, acc_f = 0.0, 0.0, 0.0, 0.0\r\n        for ii in range(total_len):\r\n            X, Y = sess.run(tf_next)\r\n            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],\r\n                                        feed_dict={ model.clf_input: X,\r\n                                                    model.label_ref: Y,\r\n                                                    tf.keras.backend.learning_phase(): 1})\r\n            c_loss_t = c_loss_t + b_c_loss\r\n            acc_t = acc_t + b_acc\r\n\r\n            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],\r\n                                        feed_dict={ model.clf_input: X,\r\n                                                    model.label_ref: Y,\r\n                                                    tf.keras.backend.learning_phase(): 0})\r\n            c_loss_f = c_loss_f + b_c_loss\r\n            acc_f = acc_f + b_acc\r\n\r\n        c_loss_t = c_loss_t / total_len\r\n        c_loss_f = c_loss_f / total_len\r\n        acc_t = acc_t / total_len\r\n        acc_f = acc_f / total_len\r\n        print('[Validation][learning_phase=1] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_t, acc_t) )\r\n        print('[Validation][learning_phase=0] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_f, acc_f) )\r\n\r\n    # evaluate\r\n    num_epoch = 1\r\n    total_len = testY.shape[0] // batch_size\r\n    tf_iter = tf.compat.v1.data.make_initializable_iterator(test_gen)\r\n    tf_next = tf_iter.get_next()\r\n    sess.run(tf_iter.initializer)\r\n    for epoch in range(num_epoch):\r\n        c_loss_t, acc_t, c_loss_f, acc_f = 0.0, 0.0, 0.0, 0.0\r\n        for ii in range(total_len):\r\n            X, Y = sess.run(tf_next)\r\n            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],\r\n                                        feed_dict={ model.clf_input: X,\r\n                                                    model.label_ref: Y,\r\n                                                    tf.keras.backend.learning_phase(): 1})\r\n            c_loss_t = c_loss_t + b_c_loss\r\n            acc_t = acc_t + b_acc\r\n\r\n            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],\r\n                                        feed_dict={ model.clf_input: X,\r\n                                                    model.label_ref: Y,\r\n                                                    tf.keras.backend.learning_phase(): 0})\r\n            c_loss_f = c_loss_f + b_c_loss\r\n            acc_f = acc_f + b_acc\r\n\r\n        c_loss_t = c_loss_t / total_len\r\n        c_loss_f = c_loss_f / total_len\r\n        acc_t = acc_t / total_len\r\n        acc_f = acc_f / total_len\r\n        print('[Testing][learning_phase=1] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_t, acc_t) )\r\n        print('[Testing][learning_phase=0] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_f, acc_f) )\r\n\r\n```\r\n\r\nThe first part of testing case is training model with high leval API and the result is as expected.\r\n```\r\n450/450 - 39s - loss: 1.9658 - accuracy: 0.2993 - val_loss: 1.7215 - val_accuracy: 0.3738\r\nEpoch 2/5\r\n450/450 - 28s - loss: 1.5722 - accuracy: 0.4334 - val_loss: 1.5897 - val_accuracy: 0.4152\r\nEpoch 3/5\r\n450/450 - 27s - loss: 1.3876 - accuracy: 0.4993 - val_loss: 1.4867 - val_accuracy: 0.4770\r\nEpoch 4/5\r\n450/450 - 28s - loss: 1.2564 - accuracy: 0.5477 - val_loss: 1.3498 - val_accuracy: 0.5060\r\nEpoch 5/5\r\n450/450 - 27s - loss: 1.1488 - accuracy: 0.5888 - val_loss: 1.3380 - val_accuracy: 0.5232\r\n10000/10000 - 3s - loss: 1.3523 - accuracy: 0.5289\r\n```\r\n\r\nI got a strange loss and the ourput can be seen the following:\r\n```\r\nMake sure that we create a new model.\r\n10000/10000 - 3s - loss: 10.2004 - accuracy: 0.1048\r\n[Training]Epoch: 1/5 - loss: 2.288 - acc: 0.268\r\n[Training]Epoch: 2/5 - loss: 1.513 - acc: 0.448\r\n[Training]Epoch: 3/5 - loss: 1.285 - acc: 0.537\r\n[Training]Epoch: 4/5 - loss: 1.426 - acc: 0.487\r\n[Training]Epoch: 5/5 - loss: 1.306 - acc: 0.535\r\nShow loss and accuracy with keras API\r\n45000/45000 - 9s - loss: nan - accuracy: 0.1002\r\n5000/5000 - 1s - loss: nan - accuracy: 0.0986\r\n10000/10000 - 2s - loss: nan - accuracy: 0.1000\r\nShow loss and accuracy with low level API\r\n[Validation][learning_phase=1] Epoch: 1/1 - loss: 1.163 - acc: 0.585\r\n[Validation][learning_phase=0] Epoch: 1/1 - loss: nan - acc: 0.099\r\n[Testing][learning_phase=1] Epoch: 1/1 - loss: 1.179 - acc: 0.587\r\n[Testing][learning_phase=0] Epoch: 1/1 - loss: nan - acc: 0.100\r\n```\r\n\r\nObviously, after training custom model with low level API, the result would be wrong when setting `tf.keras.backend.learning_phase(): 0`\r\n\r\nAlso, the result from keras API is wrong too.\r\n\r\n`tf.keras.backend.learning_phase(): 0` may affect the behavior of `tf.keras.layers.BatchNormalization()` but I'm not sure whether this is root cause.\r\n\r\nI have tried a small custom model without `tf.keras.layers.BatchNormalization()` for MNIST dataset and the result is normal.\r\n\r\nThe testcase for MNIST as shown in the following:\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n#tf.compat.v1.disable_v2_behavior()\r\n\r\nimport numpy as np\r\n\r\nbatch_size = 100\r\n\r\ndef download_data():\r\n\r\n    # get raw data\r\n    (trainX, trainY), (testX, testY) = tf.keras.datasets.mnist.load_data()\r\n    trainX = trainX.astype(np.float32)\r\n    testX  = testX.astype(np.float32)\r\n\r\n    # ont-hot\r\n    trainY = tf.keras.utils.to_categorical(trainY, 10)\r\n    testY  = tf.keras.utils.to_categorical(testY , 10)\r\n\r\n    # get validation sets\r\n    training_size = 55000\r\n    validX = trainX[training_size:,:]\r\n    validY = trainY[training_size:,:]\r\n\r\n    trainX = trainX[:training_size,:]\r\n    trainY = trainY[:training_size,:]\r\n\r\n    # expand dimesion\r\n    trainX = np.expand_dims(trainX, axis=3)\r\n    validX = np.expand_dims(validX, axis=3)\r\n    testX  = np.expand_dims(testX , axis=3)\r\n\r\n    return trainX, trainY, validX, validY, testX, testY\r\n\r\ndef data_pipeline(dataX, dataY):\r\n\r\n        dataset = tf.data.Dataset.from_tensor_slices( (dataX, dataY) )\r\n        dataset = dataset.shuffle(batch_size * 8)\r\n        dataset = dataset.repeat()\r\n        dataset = dataset.batch(batch_size)\r\n        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n        return dataset\r\n\r\nclass custom_model():\r\n    def __init__(self):\r\n\r\n        def Acc():\r\n            acc = tf.keras.metrics.categorical_accuracy(label_ref, clf_out)\r\n            return tf.math.reduce_mean(acc)\r\n\r\n        def c_loss():\r\n            loss = tf.keras.losses.categorical_crossentropy(label_ref, clf_out)\r\n            loss = tf.math.reduce_mean(loss)\r\n            return loss\r\n\r\n        # declare variables\r\n        self.init_op = tf.compat.v1.keras.initializers.he_normal()\r\n        model_layers = [ tf.keras.layers.Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=self.init_op, name=\"clf/c1\"),\r\n                         tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=self.init_op, name=\"clf/c2\"),\r\n                         tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"clf/p1\"),\r\n                         tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=self.init_op, name=\"clf/c3\"),\r\n                         tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=self.init_op, name=\"clf/c4\"),\r\n                         tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"clf/p2\"),\r\n                         tf.keras.layers.Flatten(name=\"clf/f1\"),\r\n                         tf.keras.layers.Dense(256, activation=\"relu\", kernel_initializer=self.init_op, name=\"clf/d1\"),\r\n                         tf.keras.layers.Dense(10 , activation=None  , kernel_initializer=self.init_op, name=\"clf/d2\"),\r\n                         tf.keras.layers.Activation('softmax', name=\"clf/a1\")\r\n                        ]\r\n\r\n        # clf_model\r\n        clf_input = tf.keras.layers.Input(shape=(28,28,1 ), name=\"model/input\")\r\n        clf_out   = clf_input\r\n        for ii in model_layers:\r\n            clf_out = ii(clf_out)\r\n        clf_model = tf.keras.models.Model(inputs=clf_input, outputs=clf_out, name='clf_model')\r\n        clf_model.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\r\n\r\n\r\n        label_ref = tf.keras.layers.Input(shape=(10,) , name='label_ref')\r\n        clf_out = clf_model(clf_input)\r\n\r\n        # using tf.keras.optimizers.Nadam would get error\r\n        #optimizer = tf.keras.optimizers.Nadam(lr=0.0005)\r\n        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)\r\n        self.train_op = optimizer.minimize(c_loss(), var_list=[clf_model.trainable_variables])\r\n\r\n        self.clf_model = clf_model\r\n        self.clf_input = clf_input\r\n        self.label_ref = label_ref\r\n        self.op_acc = Acc()\r\n        self.c_loss = c_loss()\r\n\r\nif __name__ == '__main__':\r\n\r\n    # set GPU\r\n    import os\r\n    if os.environ.get(\"CUDA_VISIBLE_DEVICES\") is None:\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\n    # reset tf session\r\n    tf.compat.v1.keras.backend.clear_session()\r\n    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\r\n    sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\r\n    tf.compat.v1.keras.backend.set_session(sess) \r\n\r\n    # prepare data\r\n    trainX, trainY, validX, validY, testX, testY = download_data()\r\n    train_gen = data_pipeline(trainX, trainY)\r\n    valid_gen = data_pipeline(validX, validY)\r\n    test_gen = data_pipeline(testX, testY)\r\n\r\n    # create a new model\r\n    print('Make sure that we create a new model.')\r\n    model = custom_model()\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n    model.clf_model.evaluate(testX, testY, verbose=2, batch_size=batch_size)\r\n\r\n    # train model\r\n    num_epoch = 5\r\n    total_len = trainY.shape[0] // batch_size\r\n    tf_iter = tf.compat.v1.data.make_initializable_iterator(train_gen)\r\n    tf_next = tf_iter.get_next()\r\n    sess.run(tf_iter.initializer)\r\n    for epoch in range(num_epoch):\r\n        c_loss, acc = 0.0, 0.0\r\n        for ii in range(total_len):\r\n            X, Y = sess.run(tf_next)\r\n            [b_c_loss, b_acc, _] = sess.run([model.c_loss, model.op_acc, model.train_op],\r\n                                                feed_dict={ model.clf_input: X,\r\n                                                            model.label_ref: Y,\r\n                                                            tf.keras.backend.learning_phase(): 1})\r\n            c_loss = c_loss + b_c_loss\r\n            acc = acc + b_acc\r\n        \r\n        c_loss = c_loss / total_len\r\n        acc = acc / total_len\r\n        print('[Training]Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss, acc) )\r\n\r\n    print('Show loss and accuracy with keras API')\r\n    model.clf_model.evaluate(trainX, trainY, verbose=2, batch_size=batch_size)\r\n    model.clf_model.evaluate(validX, validY, verbose=2, batch_size=batch_size)\r\n    model.clf_model.evaluate(testX, testY, verbose=2, batch_size=batch_size)\r\n\r\n    print('Show loss and accuracy with low level API')\r\n    # evaluate\r\n    num_epoch = 1\r\n    total_len = validY.shape[0] // batch_size\r\n    tf_iter = tf.compat.v1.data.make_initializable_iterator(valid_gen)\r\n    tf_next = tf_iter.get_next()\r\n    sess.run(tf_iter.initializer)\r\n    for epoch in range(num_epoch):\r\n        c_loss_t, acc_t, c_loss_f, acc_f = 0.0, 0.0, 0.0, 0.0\r\n        for ii in range(total_len):\r\n            X, Y = sess.run(tf_next)\r\n            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],\r\n                                        feed_dict={ model.clf_input: X,\r\n                                                    model.label_ref: Y,\r\n                                                    tf.keras.backend.learning_phase(): 1})\r\n            c_loss_t = c_loss_t + b_c_loss\r\n            acc_t = acc_t + b_acc\r\n\r\n            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],\r\n                                        feed_dict={ model.clf_input: X,\r\n                                                    model.label_ref: Y,\r\n                                                    tf.keras.backend.learning_phase(): 0})\r\n            c_loss_f = c_loss_f + b_c_loss\r\n            acc_f = acc_f + b_acc\r\n\r\n        c_loss_t = c_loss_t / total_len\r\n        c_loss_f = c_loss_f / total_len\r\n        acc_t = acc_t / total_len\r\n        acc_f = acc_f / total_len\r\n        print('[Validation][learning_phase=1] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_t, acc_t) )\r\n        print('[Validation][learning_phase=0] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_f, acc_f) )\r\n\r\n    # evaluate\r\n    num_epoch = 1\r\n    total_len = testY.shape[0] // batch_size\r\n    tf_iter = tf.compat.v1.data.make_initializable_iterator(test_gen)\r\n    tf_next = tf_iter.get_next()\r\n    sess.run(tf_iter.initializer)\r\n    for epoch in range(num_epoch):\r\n        c_loss_t, acc_t, c_loss_f, acc_f = 0.0, 0.0, 0.0, 0.0\r\n        for ii in range(total_len):\r\n            X, Y = sess.run(tf_next)\r\n            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],\r\n                                        feed_dict={ model.clf_input: X,\r\n                                                    model.label_ref: Y,\r\n                                                    tf.keras.backend.learning_phase(): 1})\r\n            c_loss_t = c_loss_t + b_c_loss\r\n            acc_t = acc_t + b_acc\r\n\r\n            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],\r\n                                        feed_dict={ model.clf_input: X,\r\n                                                    model.label_ref: Y,\r\n                                                    tf.keras.backend.learning_phase(): 0})\r\n            c_loss_f = c_loss_f + b_c_loss\r\n            acc_f = acc_f + b_acc\r\n\r\n        c_loss_t = c_loss_t / total_len\r\n        c_loss_f = c_loss_f / total_len\r\n        acc_t = acc_t / total_len\r\n        acc_f = acc_f / total_len\r\n        print('[Testing][learning_phase=1] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_t, acc_t) )\r\n        print('[Testing][learning_phase=0] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_f, acc_f) )\r\n```\r\n\r\nDefinitely, we got a very normal output:\r\n\r\n```\r\nMake sure that we create a new model.\r\n10000/10000 - 1s - loss: 398.0696 - acc: 0.1151\r\n[Training]Epoch: 1/5 - loss: 11.997 - acc: 0.558\r\n[Training]Epoch: 2/5 - loss: 0.474 - acc: 0.849\r\n[Training]Epoch: 3/5 - loss: 0.282 - acc: 0.914\r\n[Training]Epoch: 4/5 - loss: 0.213 - acc: 0.935\r\n[Training]Epoch: 5/5 - loss: 0.181 - acc: 0.945\r\nShow loss and accuracy with keras API\r\n55000/55000 - 1s - loss: 0.1555 - acc: 0.9535\r\n5000/5000 - 0s - loss: 0.1501 - acc: 0.9584\r\n10000/10000 - 0s - loss: 0.1687 - acc: 0.9539\r\nShow loss and accuracy with low level API\r\n[Validation][learning_phase=1] Epoch: 1/1 - loss: 0.150 - acc: 0.958\r\n[Validation][learning_phase=0] Epoch: 1/1 - loss: 0.150 - acc: 0.958\r\n[Testing][learning_phase=1] Epoch: 1/1 - loss: 0.169 - acc: 0.954\r\n[Testing][learning_phase=0] Epoch: 1/1 - loss: 0.169 - acc: 0.954\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should work properly.\r\n\r\n**Code to reproduce the issue**\r\n\r\nPlease see the section of **Describe the current behavior**\r\n\r\n**Other info / logs**\r\n\r\nskip ...", "comments": ["I have tried on colab with TF version 2.0  and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/bf147df257b9103cd0b58604cb0b70c9/untitled481.ipynb). Thanks!", "@CNOCycle Similar issue https://github.com/tensorflow/tensorflow/issues/32477", "any update for this issue?", "In general, if you want to maintain v1 behavior, it's best to use the v1 apis as well:\r\n```\r\nimport tensorflow.compat.v1 as tf\r\n```\r\nIn this case, the behavior of the trainable arg of BatchNorm is slightly different in v1 versus v2, so it may be that the trainable arg or learning phase is not set correctly above. The docstring for BatchNorm details some of the differences further: https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization#used-in-the-notebooks", "@karmel , thanks your reply.\r\n\r\nAfter applying your suggestion, all things run correctly.\r\n\r\n\r\nThe following result is produced in docker image `tensorflow/tensorflow:2.1.0-gpu-py3`\r\n```\r\n10000/10000 - 1s - loss: 689.0110 - accuracy: 0.1010\r\n[Training]Epoch: 1/5 - loss: 11.044 - acc: 0.718\r\n[Training]Epoch: 2/5 - loss: 0.312 - acc: 0.897\r\n[Training]Epoch: 3/5 - loss: 0.232 - acc: 0.924\r\n[Training]Epoch: 4/5 - loss: 0.203 - acc: 0.933\r\n[Training]Epoch: 5/5 - loss: 0.192 - acc: 0.939\r\nShow loss and accuracy with keras API\r\n55000/55000 - 1s - loss: 0.2015 - accuracy: 0.9365\r\n5000/5000 - 0s - loss: 0.1974 - accuracy: 0.9390\r\n10000/10000 - 0s - loss: 0.2321 - accuracy: 0.9289\r\nShow loss and accuracy with low level API\r\n[Validation][learning_phase=1] Epoch: 1/1 - loss: 0.197 - acc: 0.939\r\n[Validation][learning_phase=0] Epoch: 1/1 - loss: 0.197 - acc: 0.939\r\n[Testing][learning_phase=1] Epoch: 1/1 - loss: 0.232 - acc: 0.929\r\n[Testing][learning_phase=0] Epoch: 1/1 - loss: 0.232 - acc: 0.929\r\n```\r\n", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35107\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35107\">No</a>\n", "Unfortunately, I forget to check a model with BatchNorm.\r\n\r\nFor this case, the API is still broken. I did the test on `tf-nightly-gpu:2.2.0.dev20200314`\r\n\r\nThe following output shows that loss is `nan` if users use low level API\r\n\r\n```\r\nTrain on 450 steps, validate on 50 steps\r\nEpoch 1/5\r\n450/450 - 26s - loss: 1.9441 - accuracy: 0.3058 - val_loss: 1.7556 - val_accuracy: 0.3694\r\nEpoch 2/5\r\n450/450 - 25s - loss: 1.5582 - accuracy: 0.4386 - val_loss: 1.6236 - val_accuracy: 0.4176\r\nEpoch 3/5\r\n450/450 - 25s - loss: 1.3817 - accuracy: 0.5054 - val_loss: 1.4167 - val_accuracy: 0.4940\r\nEpoch 4/5\r\n450/450 - 25s - loss: 1.2448 - accuracy: 0.5544 - val_loss: 1.3870 - val_accuracy: 0.5090\r\nEpoch 5/5\r\n450/450 - 25s - loss: 1.1224 - accuracy: 0.5992 - val_loss: 1.4285 - val_accuracy: 0.4988\r\nMake sure that we create a new model.\r\n[Training]Epoch: 1/5 - loss: 2.415 - acc: 0.234\r\n[Training]Epoch: 2/5 - loss: 1.648 - acc: 0.401\r\n[Training]Epoch: 3/5 - loss: 1.427 - acc: 0.483\r\n[Training]Epoch: 4/5 - loss: 1.279 - acc: 0.541\r\n[Training]Epoch: 5/5 - loss: 1.134 - acc: 0.596\r\nShow loss and accuracy with keras API\r\nShow loss and accuracy with low level API\r\n[Validation][learning_phase=1] Epoch: 1/1 - loss: 1.082 - acc: 0.614\r\n[Validation][learning_phase=0] Epoch: 1/1 - loss: nan - acc: 0.095\r\n[Testing][learning_phase=1] Epoch: 1/1 - loss: 1.119 - acc: 0.600\r\n[Testing][learning_phase=0] Epoch: 1/1 - loss: nan - acc: 0.094\r\n```\r\n", "Okay, will take a look", "Me too\r\nMaybe related to [this](https://github.com/tensorflow/tensorflow/issues/36700)", "This can be fixed by [this](https://github.com/tensorflow/tensorflow/issues/36366#issuecomment-601985968)", "@raghavab1992 suggested that replace `tensorflow.python.keras.layers` with `tensorflow.keras.layers`.\r\n\r\nThis suggestion is mention by @liuxingbaoyu, too .\r\n\r\nUnfortunately, this method did not work. \r\n\r\nI also force ask BN layer to use V1 instead of V2. But the loss is still `nan` if users run low level API.\r\n", "@CNOCycle Yes, I also think there may be other problems, so I am waiting for tf official.", "Hi @CNOCycle, really sorry about the delay! I've finally had a chance to take a look at this.\r\n\r\nAs far as I can tell, it's actually totally unrelated from the other referenced issues (about which layers are passed to keras applications). Instead, it seems like internal changes we've made to the implementation of learning_phase over time may have broken some usages of learning phase when constructing tf.compat.v1 sessions and running them.\r\n\r\nUnfortunately, this is far enough from how we recommend people use Keras in TF2 that our team doesn't have the bandwidth to fix this. And, if we were to try fixing this it would have to wait until 2.3\r\n\r\nYou *may* be able to set learning phase explicitly and have it work by using keras.backend.set_learning_phase/keras.backend.learning_phase_scope\r\n(I'm not sure on that front). You may also be able to pass in `training` explicitly to the layers that use learning phase.\r\n\r\nBut, our overall recommendation would be: If you can't upgrade to 2.x style code and it's crucial for you to keep using sessions directly with keras layers, you should probably just use TensorFlow 1.x.", "(Closing because we don't have the bandwidth to fix this)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35107\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35107\">No</a>\n"]}, {"number": 35106, "title": "can dev a lua version", "body": "so many game dev in lua,can't find the lua version of tensorflow", "comments": ["We do not have lua bindings for tensorflow.\r\nSee https://github.com/tensorflow/tensorflow/issues/1318 for more info."]}, {"number": 35105, "title": "Tensorboard Embedding Projector Custom Search Bug", "body": "Context: I am using the online version of the [Tensorboard Embedding Projector](https://projector.tensorflow.org/). I uploaded my own checkpoint and metadata TSVs with 681909 rows each. I clicked on the \"CUSTOM\" tab, and then found a bug when I tried to search. \r\n\r\nCurrent Behavior/Bug: I am unable to click on the search bar that says \"Search by\". When I do click anywhere in the search bar, a drop down appears with the option \"label\". If I click label and try to search again, there is no difference. There is no error message.\r\n\r\nExpected Behavior: I would be able to search, or be given an error message indicating why I'm unable to.", "comments": ["@pujaarajan ,\r\nThis looks like tensorboard related issue, could you please report it in [Tensorboard](https://github.com/tensorflow/tensorboard/issues/new/choose) repo. Also close this issue once its reported there. Thanks!"]}, {"number": 35104, "title": "Print kernel sizes too", "body": "This only concerns convolution layers. But this change will enable\r\ncomputing theoretical flops just looking at the dot graph.\r\n\r\nNot sure if anyone else will find this useful but discard if you think is not\r\nuseful.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35104) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35104) for more info**.\n\n<!-- ok -->", "Will you be able to add a test case for the use case described? The test is also to make sure this change is compatible with existing use case.", "@pavithrasv, to add test, I'll add some help. Do you know how I can test changes in python files without building Tensorflow from source? Currently, I just changed the python files in the lib folder of TensorFlow installed via pip.\r\nI'll greatly appreciate the help :).", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@vmiheer @pavithrasv Any update on this PR, please. Thanks!", "@vmiheer So your test will need to go in vis_utils_test.py. What you can do is \r\n1. pip install tensorflow\r\n2. create a separate python test module and put your test in that and make sure it passes\r\n3. move this test into vis_utils_test.py with required updates\r\n4. commit and let the PR process run the test now in TF source code\r\n\r\nIf you want to directly put your test in vis_utils_test.py which is part of TF source code, you will need to build TF from source.", "@vmiheer Any update on this PR, please. Thanks!", "I am currently busy and I won't be able to work on it.\r\nClosing it, Thank you so much @pavithrasv for help."]}, {"number": 35103, "title": "Async functions cause tf_upgrade_v2 to crash", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro Linux (kernel 4.19)\r\n- TensorFlow installed from (source or binary): binary? I'm not sure, I used `pip install tensorflow`\r\n- TensorFlow version: 2.0.0\r\n- Python version: Python 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip. This is in a virtualenv environment though.\r\n\r\n**Describe the problem**\r\nRunning `tf_upgrade_v2` on a file that has async functions causes it to crash. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```tf_upgrade_v2 --infile ./image_classification/test_tf.py --outfile ./image_classification_v2/test_tf.py\r\nTraceback (most recent call last):\r\n  File \"/home/alex/git-repos/bic-bot-py/bin/tf_upgrade_v2\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/tensorflow_core/tools/compatibility/tf_upgrade_v2_main.py\", line 139, in main\r\n    args.input_file, output_file, upgrade)\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/tensorflow_core/tools/compatibility/tf_upgrade_v2_main.py\", line 40, in process_file\r\n    upgrader.process_file(in_filename, out_filename)\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/tensorflow_core/tools/compatibility/ast_edits.py\", line 900, in process_file\r\n    temp_file)\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/tensorflow_core/tools/compatibility/ast_edits.py\", line 960, in process_opened_file\r\n    self.update_string_pasta(\"\".join(lines), in_filename))\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/tensorflow_core/tools/compatibility/ast_edits.py\", line 916, in update_string_pasta\r\n    t = pasta.parse(text)\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/__init__.py\", line 25, in parse\r\n    annotator.visit(t)\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py\", line 1201, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py\", line 133, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/home/alex/.pyenv/versions/3.6.8/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py\", line 47, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py\", line 225, in visit_Module\r\n    self.generic_visit(node)\r\n  File \"/home/alex/.pyenv/versions/3.6.8/lib/python3.6/ast.py\", line 261, in generic_visit\r\n    self.visit(item)\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py\", line 1201, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py\", line 133, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/home/alex/.pyenv/versions/3.6.8/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/home/alex/.pyenv/versions/3.6.8/lib/python3.6/ast.py\", line 261, in generic_visit\r\n    self.visit(item)\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py\", line 1201, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py\", line 133, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/home/alex/.pyenv/versions/3.6.8/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py\", line 47, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py\", line 673, in visit_Return\r\n    self.token('return')\r\n  File \"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py\", line 1340, in token\r\n    token_val, token.src, token.start[0], token.line))\r\npasta.base.annotate.AnnotationError: Expected 'return' but found 'async'\r\nline 1: async def f():\r\n```\r\n\r\n**Any other info / logs**\r\ntest_tf.py:\r\n```\r\nasync def f():\r\n    return\r\n```", "comments": ["@TheBicPen, Could you post the complete standalone code to reproduce the issue. Thanks!", "Here is the command I ran: `tf_upgrade_v2 --infile test_tf.py --outfile test_tf_v2.py `. The contents of the test_tf.py are the same as I posted above:\r\n```\r\nasync def f():\r\n    return\r\n```\r\nIt also crashed when I tested with a file containing more code (same command) :\r\n```\r\nfrom image_classification import image_classify as ic\r\n\r\ntf_sess = None\r\nclassifications = None\r\n\r\n\r\ndef start_tf():\r\n    global tf_sess\r\n    global classifications\r\n    classifications = ic.get_classifications()\r\n    if tf_sess is not None:\r\n        print(\"TensorFlow session already exists\")\r\n        return \"TensorFlow session already exists\"\r\n    tf_sess = ic.main()\r\n    return \"started TensorFlow session\"\r\n\r\n\r\ndef stop_tf():\r\n    global tf_sess\r\n    global classifications\r\n    tf_sess.close()  # free resources used by the tf session\r\n    tf_sess = None  # remove the reference to it\r\n    classifications = None\r\n    print(\"ended TensorFlow session\")\r\n    return \"ended TensorFlow session\"\r\n\r\n\r\nasync def classify_attachment(message, tf_sess, classifications):\r\n    if classifications is None:\r\n        return \"image classifications not found\"\r\n    if tf_sess is None:\r\n        return \"TensorFlow session not found\"\r\n    if len(message.attachments) == 0:\r\n        return \"no valid attachments in message\"\r\n\r\n    # check image using image-classification\r\n    attachment = message.attachments[0]\r\n    print(\"message contains attachment at:{0}\".format(attachment.url))\r\n    img = await attachment.read()\r\n    result = ic.classify_image(img, tf_sess, classifications)\r\n    return (result, attachment)\r\n```\r\n", "@TheBicPen, Looks like `image_classification` is not defined and tensorflow import is missing. Please provide the more information to reproduce the issue. Thanks!", "I don't have access to my computer right now, but the specific code that caused the issue is here: https://github.com/TheBicPen/bic-bot-py/tree/master/image_classification . The command I ran was `tf_upgrade_v2 --intree image_classification --outtree image_classification_v2`. I ran this command from the root of the repo. Additionally, the `test_tf.py` file that I used to try to isolate the issue is located here: https://github.com/TheBicPen/bic-bot-py/blob/master/test/test_tf.py . I hope this information is enough to reproduce the issue", "@TheBicPen, Tried reproducing the reported issue with provided code.But I didn't get error message.\r\nPlease find the [gist](https://colab.sandbox.google.com/gist/gadagashwini/3e5e9ba8c8982b276584c3176919334d/untitled325.ipynb) and confirm the issue. Thanks! ", "@gadagashwini, The issue can be reproduced as seen [here](https://colab.research.google.com/gist/TheBicPen/4f32482213a0797f0e4a9d4c2228a913/untitled325.ipynb).", "This is an issue in google_pasta, a library we use to write that script in. This is fixed as shown in the following [comment](https://github.com/tensorflow/tensorflow/issues/26486#issuecomment-497027128). Please go though it. Thanks!", "Same issue occurs with pasta 0.1.8:\r\n```\r\n>pip show google-pasta\r\nName: google-pasta\r\nVersion: 0.1.8\r\nSummary: pasta is an AST-based Python refactoring library\r\nHome-page: UNKNOWN\r\nAuthor: Nick Smith\r\nAuthor-email: smithnick@google.com\r\nLicense: Apache 2.0\r\nLocation: ....\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\r\nRequires: six\r\nRequired-by: tensorflow\r\n```\r\nThis is on Windows this time.", "@TheBicPen Lets make things clear here. If you use pasta 0.1.8 are you still facing the issue in Manjaro Linux?", "I am facing the same issue with pasta 0.1.8 on Windows. I haven't checked yet on Manjaro.", "Initially the issue has been created for **Manjaro Linux** So, lets stick to that for now. If you are looking into windows, please create a new issue for windows. Thanks!", "> Initially the issue has been created for **Manjaro Linux** So, lets stick to that for now. If you are looking into windows, please create a new issue for windows. Thanks!\r\n\r\nI had a hardware change over the holidays so I only have a Windows machine now. I'll spin up a Manjaro VM tonight, but I'm pretty sure that the same issue occurs across both platforms.", "```\r\n...\r\npasta.base.annotate.AnnotationError: Expected 'message' but found 'async'\r\nline 29: async def classify_attachment(message, tf_sess, classifications):\r\n\r\n[manjaro@manjaro-i3 bic-bot-py]$ pip show google-pasta\r\nName: google-pasta\r\nVersion: 0.1.8\r\nSummary: pasta is an AST-based Python refactoring library\r\nHome-page: UNKNOWN\r\nAuthor: Nick Smith\r\nAuthor-email: smithnick@google.com\r\nLicense: Apache 2.0\r\nLocation: /home/manjaro/.local/lib/python3.7/site-packages\r\nRequires: six\r\nRequired-by: tensorflow\r\n```\r\nSame issue on Manjaro.", "Its failing with pasta 0.1.7 too..!!", "Pasta has been upgraded to 0.2 and this should be fixed now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35103\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35103\">No</a>\n"]}, {"number": 35102, "title": "Adam implementation differs from paper (applies bias B_2 correction to \\epsilon)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: V10.0.130\r\n- GPU model and memory: Tesla V100-SXM2\r\n\r\n**Describe the current behavior**\r\nIn the Adam paper, we subtract the following quantity from our current gradient [0]:\r\n\\alpha * \\hat{m_t} / (\\sqrt{v_t / (1-\\beta^t_2)}  + \\epsilon)\r\n\r\n<img width=\"475\" alt=\"Screen Shot 2019-12-13 at 2 32 58 PM\" src=\"https://user-images.githubusercontent.com/54961543/70836600-7f490780-1db5-11ea-9669-27c50fe48cae.png\">\r\n\r\nThe Tensorflow implementation subtracts a subtly different quantity ([1]):\r\n\\alpha * \\hat{m_t} * \\sqrt{1-\\beta^T_2} / (\\sqrt{v_t} + \\epsilon)\r\n\r\n<img width=\"438\" alt=\"Screen Shot 2019-12-13 at 2 34 36 PM\" src=\"https://user-images.githubusercontent.com/54961543/70836654-c1724900-1db5-11ea-9260-5fc0678f6f39.png\">\r\n\r\nThe difference between the two expressions is that in the first, we de-bias only the moving average of the squared gradient, v_t. In the second, this bias correction is also applied to \\epsilon. This manifests as scaling up epilson quite a lot in very early training steps, reducing the magnitude of the gradient update.\r\n\r\nNote that the same bug was present in PyTorch prior to v1.3. It was fixed in this PR: https://github.com/pytorch/pytorch/pull/22628. That PR description provides a useful visualization.\r\n\r\n**Describe the expected behavior**\r\nImplement the algorithm as described in the paper. If the old implementation is necessary to preserve back-compat, providing a flag to trigger the correct implementation would be most helpful.\r\n\r\n\r\n[0] https://arxiv.org/pdf/1412.6980.pdf, see final 2 lines of Algorithm 1\r\n[1] Notable lines in TF implementation regarding this issue: \r\nhttps://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/adam.py#L162\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/adam.py#L245\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc#L373\r\n\r\n", "comments": ["Thanks for the report. We have noticed this, and decided we can only make behavior change in 3.0", "Before 3.0 we can flag-gate this behavior, though, and warn if the incompatible-with-the-paper flag is used.\r\n\r\n@tanzhenyu can you implement the right behavior and gate it behind an optimizer constructor argument / different class?", "> Before 3.0 we can flag-gate this behavior, though, and warn if the incompatible-with-the-paper flag is used.\r\n> \r\n> @tanzhenyu can you implement the right behavior and gate it behind an optimizer constructor argument / different class?\r\n\r\nSounds good.", "@bmc2-stripe Closing as duplicate.Can you please take a look at this [issue](https://github.com/pytorch/pytorch/pull/22628).Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35102\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35102\">No</a>\n", "@saikumarchalla The issue you linked to is a PyTorch issue. I don't think this should have been closed.", "This issue makes senses, and could affect the stability of Bert finetuning on small datasets.\r\nPytorch team has already fixed this.", "I just saw this issue, was about to report it myself. Until 3.0 is available, it might be worth mentioning this issue in the docstring of `class Adam` so that those expecting paper-correct results will have a note to refer to (and won't be expected to compare the research paper with the TensorFlow implementation).", "@bmc2-stripe It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi, @sushreebarsa, I've verified that the incorrect formula is still being applied in master. Notably, the comment here references exactly the formula I listed above (which is not consistent with the paper):\r\nhttps://github.com/memo/ofxMSATensorFlow/blob/master/libs/tensorflow/include/tensorflow/cc/ops/training_ops.h#L1008\r\n", "Hi There,\r\n\r\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \r\n\r\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35102\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35102\">No</a>\n"]}, {"number": 35101, "title": "AutoGraph could not transform <bound method TopLevelFeature.decode_example of FeaturesDict", "body": "**System information**\r\n- OS Platform and Distribution: Arch Linux, 5.4.2-arch1-1-ARCH\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.1.0rc0-1\r\n- Keras version: 2.2.4-tf\r\n- Python version: 3.8\r\n- GPU model and memory: 2x GTX 1080 Ti 11GB\"`\r\n\r\n**Describe the current behavior**\r\nexecuting Tensorflow's MNIST handwriting example produces warning:\r\n\r\n> WARNING:tensorflow:AutoGraph could not transform <bound method TopLevelFeature.decode_example of FeaturesDict({\r\n>     'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\r\n>     'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\r\n> })> and will run it as-is.\r\n> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n> Cause: 'arguments' object has no attribute 'defaults'\r\n\r\n\r\n**Code to reproduce the issue**\r\n import tensorflow as tf\r\n  import tensorflow_datasets as tfds\r\n  \r\n  from tensorflow.keras.optimizers import Adam\r\n  \r\n  def build_model():\r\n      filters = 48\r\n      units = 24\r\n      kernel_size = 7\r\n      learning_rate = 1e-4\r\n      model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(units, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n      ])\r\n      model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\r\n      return model\r\n  \r\n  datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n  mnist_train, mnist_test = datasets['train'], datasets['test']\r\n  \r\n  num_train_examples = info.splits['train'].num_examples\r\n  num_test_examples = info.splits['test'].num_examples\r\n  \r\n  BUFFER_SIZE = 10000\r\n  BATCH_SIZE = 32\r\n  \r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n  \r\n  train_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n  eval_dataset = mnist_test.map(scale).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n  \r\nmodel = build_model()\r\n  \r\n  epochs=2\r\n  model.fit(\r\n          train_dataset,\r\n          validation_data=eval_dataset,\r\n          steps_per_epoch=num_train_examples/epochs,\r\n          validation_steps=num_test_examples/epochs,\r\n          epochs=epochs)", "comments": ["@olk ,\r\nHi,i tried running the code in TF-2.1rc-1 and I didn't face any error. Kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/0f8885a8bfc428c6fe96dfdc5df70952/35068.ipynb) of colab for your reference.Thanks!", "```\r\nprint('TensorFlow version: %s' % tf.__version__)\r\nprint('Keras version: %s' % tf.keras.__version__)\r\n```\r\nprints:\r\n> TensorFlow version: 2.1.0-rc0\r\n> Keras version: 2.2.4-tf\r\n\r\nDoes RC1 fix the issue?", "Tensorflow at Arch Linux upgraded to new version\r\n> TensorFlow version: 2.1.0-rc1\r\n> Keras version: 2.2.4-tf\r\n\r\nbut same warnings are displayed:\r\n\r\n> WARNING:tensorflow:AutoGraph could not transform <bound method TopLevelFeature.decode_example of FeaturesDict({\r\n>     'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\r\n>     'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\r\n> })> and will run it as-is.\r\n> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n> Cause: 'arguments' object has no attribute 'defaults'\r\n> WARNING:tensorflow:AutoGraph could not transform <bound method TopLevelFeature.decode_example of FeaturesDict({\r\n>     'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\r\n>     'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\r\n> })> and will run it as-is.\r\n> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n> Cause: 'arguments' object has no attribute 'defaults'\r\n> WARNING:tensorflow:AutoGraph could not transform <bound method TopLevelFeature.decode_example of FeaturesDict({\r\n>     'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\r\n>     'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\r\n> })> and will run it as-is.\r\n> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n> Cause: 'arguments' object has no attribute 'defaults'\r\n> WARNING:tensorflow:AutoGraph could not transform <bound method TopLevelFeature.decode_example of FeaturesDict({\r\n>     'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\r\n>     'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\r\n> })> and will run it as-is.\r\n> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n> Cause: 'arguments' object has no attribute 'defaults'\r\n> WARNING:tensorflow:AutoGraph could not transform <function scale at 0x7f1877956940> and will run it as-is.\r\n> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n> Cause: invalid value for \"node\": expected \"ast.AST\", got \"<class 'NoneType'>\"; to visit lists of nodes, use \"visit_block\" instead\r\n> WARNING:tensorflow:AutoGraph could not transform <function scale at 0x7f1877956940> and will run it as-is.\r\n> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n> Cause: invalid value for \"node\": expected \"ast.AST\", got \"<class 'NoneType'>\"; to visit lists of nodes, use \"visit_block\" instead\r\n> Train for 30000.0 steps, validate for 5000.0 steps\r\n", "I guess this issue is related to using Tensorflow with Python-3.8.", "The error vanishes if I downgraded my system to:\r\n- Python 3.7.4\r\n- Tensorflow-2.1.0-rc1\r\n\r\nAfter system upgrade (Python 3.8.0) I get the error messages again:\r\n> WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\n> WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\n> WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\n> WARNING:tensorflow:AutoGraph could not transform <bound method TopLevelFeature.decode_example of FeaturesDict({\r\n>     'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\r\n>     'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\r\n> })> and will run it as-is.\r\n> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n> Cause: 'arguments' object has no attribute 'defaults'\r\n> WARNING:tensorflow:AutoGraph could not transform <bound method TopLevelFeature.decode_example of FeaturesDict({\r\n>     'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\r\n>     'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\r\n> })> and will run it as-is.\r\n> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n> Cause: 'arguments' object has no attribute 'defaults'\r\n> WARNING:tensorflow:AutoGraph could not transform <bound method TopLevelFeature.decode_example of FeaturesDict({\r\n>     'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\r\n>     'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\r\n> })> and will run it as-is.\r\n> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n> Cause: 'arguments' object has no attribute 'defaults'\r\n> WARNING:tensorflow:AutoGraph could not transform <bound method TopLevelFeature.decode_example of FeaturesDict({\r\n>     'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\r\n>     'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\r\n> })> and will run it as-is.\r\n> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n> Cause: 'arguments' object has no attribute 'defaults'\r\n> WARNING:tensorflow:AutoGraph could not transform <function scale at 0x7f35a7d63040> and will run it as-is.\r\n> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n> Cause: invalid value for \"node\": expected \"ast.AST\", got \"<class 'NoneType'>\"; to visit lists of nodes, use \"visit_block\" instead\r\n> WARNING:tensorflow:AutoGraph could not transform <function scale at 0x7f35a7d63040> and will run it as-is.\r\n> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n> Cause: invalid value for \"node\": expected \"ast.AST\", got \"<class 'NoneType'>\"; to visit lists of nodes, use \"visit_block\" instead\r\n> Train for 30000.0 steps, validate for 5000.0 steps\r\n> Epoch 1/2\r\n> 2019-12-17 21:01:35.085243: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n> 2019-12-17 21:01:36.341770: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 30000/30000 [==============================] - 114s 4ms/step - loss: 0.0856 - accuracy: 0.9758 - val_loss: 0.0379 - val_accuracy: 0.9889\r\n> Epoch 2/2\r\n> 29996/30000 [============================>.] - ETA: 0s - loss: 0.0161 - accuracy: 0.99552019-12-17 21:05:09.198963: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n> 30000/30000 [==============================] - 112s 4ms/step - loss: 0.0161 - accuracy: 0.9955 - val_loss: 0.0350 - val_accuracy: 0.9898\r\n> 2019-12-17 21:05:20.499356: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n> 2019-12-17 21:05:20.516223: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n> elapsed: 226.111\r\n\r\nSeams that this is an issue of Python-3.8.0 and Tensorflow-2.1.0-rc1.", "@olk Try building tensorflow nightly version with python 3.8 and let me know if the issue still persists. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I'm too busy to build tensorflow from scratch ...\r\nAnyway, I've moved to MXNet - feel free to close this issue.", "@olk I am sorry for misleading you in the above comment. \r\n\r\nTensorflow doesn't support python 3.8 yet. May be from Tensorflow 2.2, we might support python 3.8 and this is the cause of the error. Python 3.8 support for tensorflow has already been tracked [here](https://github.com/tensorflow/tensorflow/issues/33374). \r\n\r\nI am gonna close this issue. Thanks!\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35101\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35101\">No</a>\n"]}, {"number": 35100, "title": "Error occurred when finalizing GeneratorDataset iterator", "body": "**System information**\r\n- OS Platform and Distribution: Arch Linux, 5.4.2-arch1-1-ARCH\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.1.0rc0-1\r\n- Keras version: 2.2.4-tf\r\n- Python version: 3.8\r\n- GPU model and memory: 2x GTX 1080 Ti 11GB\"`\r\n\r\n**Describe the current behavior**\r\nexecuting Tensorflow's MNIST handwriting example produces error:\r\nthe error dissapears if the code doesn't use OneDeviceStrategy or MirroredStrategy\r\n\r\n> W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n\r\n\r\n**Code to reproduce the issue**\r\n\r\n\r\n ```\r\n import tensorflow as tf\r\n  import tensorflow_datasets as tfds\r\n  import time\r\n  \r\n  from tensorflow.keras.optimizers import Adam\r\n  \r\n  def build_model():\r\n      filters = 48\r\n      units = 24\r\n      kernel_size = 7\r\n      learning_rate = 1e-4\r\n      model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(units, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n      ])\r\n      model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\r\n      return model\r\n  \r\n  datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n  mnist_train, mnist_test = datasets['train'], datasets['test']\r\n  \r\n  num_train_examples = info.splits['train'].num_examples\r\n  num_test_examples = info.splits['test'].num_examples\r\n  \r\n  strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\r\n  \r\n  BUFFER_SIZE = 10000\r\n  BATCH_SIZE = 32\r\n  \r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n  \r\n  train_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n  eval_dataset = mnist_test.map(scale).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n  \r\n  with strategy.scope():\r\n    model = build_model()\r\n  \r\n  epochs=5\r\n  start = time.perf_counter()\r\n  model.fit(\r\n          train_dataset,\r\n          validation_data=eval_dataset,\r\n          steps_per_epoch=num_train_examples/epochs,\r\n          validation_steps=num_test_examples/epochs,\r\n          epochs=epochs)\r\n  elapsed = time.perf_counter() - start\r\n  print('elapsed: {:0.3f}'.format(elapsed))\r\n```\r\n\r\n\r\n", "comments": ["@olk, I tried reproducing the reported issue but it worked as expected. Please take a look at the [gist](https://colab.sandbox.google.com/gist/gadagashwini/0f769c920b16c68d0b2c7d238256e0c9/untitled311.ipynb). Thanks!", "upgraded to TensorFlow version: 2.1.0-rc1 - still get errors\r\nplease note that I execute the example at real hardware (not colab)", "I guess this issue is related to using Tensorflow with Python-3.8.", "I've downgraded my system:\r\n- Python 3.7.4\r\n- Tensorflow-2.1.0-rc1\r\n\r\nStill facing the error:\r\n\r\n> Train for 30000.0 steps, validate for 5000.0 steps\r\n> Epoch 1/2\r\n> 2019-12-17 19:21:54.361240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n> 2019-12-17 19:21:55.824790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 2019-12-17 19:21:56.980785: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found\r\n> Relying on driver to perform ptx compilation. This message will be only logged once.\r\n> 30000/30000 [==============================] - 115s 4ms/step - loss: 0.0856 - accuracy: 0.9761 - val_loss: 0.0376 - val_accuracy: 0.9879\r\n> Epoch 2/2\r\n> 29990/30000 [============================>.] - ETA: 0s - loss: 0.0152 - accuracy: 0.99582019-12-17 19:25:28.372294: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n> 30000/30000 [==============================] - 111s 4ms/step - loss: 0.0152 - accuracy: 0.9958 - val_loss: 0.0375 - val_accuracy: 0.9889\r\n> 2019-12-17 19:25:40.010887: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n> 2019-12-17 19:25:40.031138: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n> elapsed: 226.391\r\n\r\nseams to be related to tensorflow-2.1.0-rc1", "I have the same issue.  Originally I was using:\r\n\r\ntensorflow/tensorflow:nightly-gpu-py3\r\n\r\nwhich has:\r\n2.1.0-dev20191106\r\n\r\nThen I tried upgrading tensorflow in the container with:\r\n\r\nhttps://files.pythonhosted.org/packages/a9/fa/8ac34cf1369deb4f523a80eeb86ec0be3dd44139bfb42c45dd3829d6aff5/tf_nightly_gpu-2.1.0.dev20191217-cp36-cp36m-manylinux2010_x86_64.whl\r\n\r\nI still have the same issue.  \r\n\r\n", "@guptapriya @qlzh727 this seems to be an issue related to tf.distribute + tf.keras. In particular, as far as I can tell, the user code does not use `tf.data.Dataset.from_generator` but the error indicates that GeneratorDataset is used. Could you please triage? Thanks.", "The error log suggests that the training completed fine, but something at the end caused this error. Neither the training or validation dataset are using generators, so it does seem weird that there is a generator related error. Also it seems like it's just a warning - since the user's print statement at the end \"elapsed..\" did get printed as well. \r\n\r\n@jsimsa is tf.data.Dataset.from_generator the only time generator_dataset_op is used? Or could there be something else that could trigger it? \r\n\r\n@rchao could it be something related to any of the fault tolerance callbacks? ", "I can verify this error with python 3.8 and python-tensorflow-opt-cuda 2.1.0rc1-2 on arch linux.  This error is weirdly not present if you import only the generator from tensorflow, and everything else from Keras.\r\n", "@guptapriya I realized that generator dataset is used in multi-device iterator. This seems related to newly added support for cancellation in tf.data.\r\n\r\nThe good news is that, as you pointed out, the warning is superfluous. The bad news is that, as far as I can tell, this warning will be present for all tf.distribute jobs in TF 2.1 (given how tf.data cancellation is implemented). I will look into having a fix for this cherrypicked into TF 2.1.", "Ah, great, thanks @jsimsa. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35100\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35100\">No</a>\n", "@jsimsa Any update on this? I'm getting this exact message and it looks like my model.fit is not doing it's thing on validation dataset during training.", "I'm also experiencing this on the official Google Cloud Platform tf2-gpu.2-1.m42 image with Python 3.5.3.", "This warning is spurious and should be removed by https://github.com/tensorflow/tensorflow/commit/b6edd34c5858ab0ab4380da774e7e2fd81a92da0", "I think the error is not spurious. It indicates the memory leak for each epoch. Try to train for extremely large epochs. You will use up all memory.", "> Try to train for extremely large epochs. You will use up all memory.\r\n\r\n@Mauhing Yup, it looks like! Tried to train a model with `batch_size=512` on couple millions samples on `g3s.xlarge` instance with 8GB GPU memory and 30GB RAM. It ran out of memory at one point after 2 epochs and I had to shut down the instance because it stopped responding! I hope @jsimsa, @guptapriya will look into this. ", "@spate141 , @jsimsa , I have the same error and the memory leak as well. My configuration is TF 2.1.0, Ubuntu 18.04, Python 3.6.10. I'm using fit_generator with generators for train and validation. When I try to train a model, it directly starts filling up the cache memory until it crashes. I observed that the issue might be related to the validation part because it generates way more number of batches than it supposed to. This is error is not spurious. ", "@spate141, @dogacbasaran  Yes, I have uploaded a small demo to demonstrate the problem it #36919. \r\n\r\n@dogacbasaran  I guess it depends which one is using the generator. is It the train set? or is It validation set? \r\n\r\n@spate141 Try to use a small batch_size, and extremly large epoch. The problem will still be there.", "@Mauhing when I first observed this issue, I wrote two separate generators doing exactly the same operations, one for training, one for validation. I printed the batch number in validation generator during training. At each epoch, validation generator supposed to generate 10 batches so after 280 epochs it should be around 2800 batches. But it generated more than 6600 batches and the cache memory was filled with 60GB in the process.  ", "@Mauhing Thanks for the scripts demo; I hope the team looked into it. And yes, I tried the small batch_size and it seems to be working fine as of now. Luckily, my model is reaching to a good validation accuracy point in relatively small number of epochs.. so there is that! \ud83e\udd37\ud83c\udffb\u200d\u2642\ufe0f", "It might help to point out that this error is being printed out once per active GPU.\r\n\r\n`2020-03-05 00:40:55.703069: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n`", "**Problem description**\r\n\r\nI am using TensorFlow 2.1.0 for image classification under Centos Linux. As my image training data set is growing, I have to start using a Generator as I do not have enough RAM to hold all pictures. I have coded the Generator based on this [tutorial](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly).\r\n\r\nIt seems to work fine, until my program all the sudden gets killed without an error message:\r\n\r\n```\r\nEpoch 6/30\r\n2020-03-08 13:28:11.361785: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n43/43 [==============================] - 54s 1s/step - loss: 5.6839 - accuracy: 0.4669\r\nEpoch 7/30\r\n2020-03-08 13:29:05.511813: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n 7/43 [===>..........................] - ETA: 1:04 - loss: 4.3953 - accuracy: 0.5268Killed\r\n```\r\n\r\nLooking at the growing memory consumption with linux's top, I suspect a memory leak?\r\n\r\n**What I have tried**\r\n\r\n- The above suggestion to switch to TF nightly build version. For me it did not help, also downgrading to TF2.0.1 did not help\r\n\r\n- There is a [discussion ](https://stackoverflow.com/questions/60000573/error-occurred-when-finalizing-generatordataset-iterator-cancelled-operation-w) suggesting that it is important, that 'steps_per_epoch' and 'batch size' correspond (whatever this exactly means) - I played with it without finding any improvement.\r\n\r\n- Trying to narrow down by looking at the size development of all variables in my Generator\r\n\r\n**Relevant code snippets**\r\n\r\n```\r\nclass DataGenerator(tf.keras.utils.Sequence):\r\n    'Generates data for Keras'\r\n    def __init__(self, list_IDs, labels, dir, n_classes):\r\n        'Initialization'\r\n        config = configparser.ConfigParser()\r\n        config.sections()\r\n        config.read('config.ini')\r\n\r\n        self.dim = (int(config['Basics']['PicHeight']),int(config['Basics']['PicWidth']))\r\n        self.batch_size = int(config['HyperParameter']['batchsize'])\r\n        self.labels = labels\r\n        self.list_IDs = list_IDs\r\n        self.dir = dir\r\n        self.n_channels = 3\r\n        self.n_classes = n_classes\r\n        self.on_epoch_end()        \r\n\r\n\r\n    def __len__(self):\r\n        'Denotes the number of batches per epoch'\r\n        return math.floor(len(self.list_IDs) / self.batch_size)\r\n\r\n    def __getitem__(self, index):\r\n        'Generate one batch of data'\r\n        # Generate indexes of the batch\r\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\r\n\r\n        # Find list of IDs\r\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\r\n\r\n        # Generate data\r\n        X, y = self.__data_generation(list_IDs_temp)\r\n\r\n        return X, y, [None]\r\n```\r\n\r\nbeing called by\r\n\r\n```\r\n        training_generator = datagenerator.DataGenerator(train_files, labels, dir, len(self.class_names))\r\n        self.model.fit(x=training_generator,\r\n                    use_multiprocessing=False,\r\n                    workers=6, \r\n                    epochs=self._Epochs, \r\n                    steps_per_epoch = len(training_generator),\r\n                    callbacks=[LoggingCallback(self.logger.debug)])\r\n```\r\n\r\nI have tried running the exact same code under Windows 10, which gives me the following error:\r\n\r\n```\r\nEpoch 9/30\r\n2020-03-08 20:49:37.555692: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n41/41 [==============================] - 75s 2s/step - loss: 2.0167 - accuracy: 0.3133\r\nEpoch 10/30\r\n2020-03-08 20:50:52.986306: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n 1/41 [..............................] - ETA: 2:36 - loss: 1.6237 - accuracy: 0.39062020-03-08 20:50:57.689373: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at matmul_op.cc:480 : Resource exhausted: OOM when allocating tensor with shape[1279200,322] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-03-08 20:50:57.766163: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Resource exhausted: OOM when allocating tensor with shape[1279200,322] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n         [[{{node MatMul_6}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n 2/41 [>.............................] - ETA: 2:02 - loss: 1.6237 - accuracy: 0.3906Traceback (most recent call last):\r\n  File \"run.py\", line 83, in <module>\r\n    main()\r\n  File \"run.py\", line 70, in main\r\n    accuracy, num_of_classes = train_Posture(unique_name)\r\n  File \"run.py\", line 31, in train_Posture\r\n    acc = neuro.train(picdb, train_ids, test_ids, \"Posture\")\r\n  File \"A:\\200307 3rd Try\\neuro.py\", line 161, in train\r\n    callbacks=[LoggingCallback(self.logger.debug)])\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 599, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2363, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 545, in call\r\n    ctx=ctx)\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[1279200,322] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n         [[node MatMul_6 (defined at A:\\200307 3rd Try\\neuro.py:161) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n [Op:__inference_distributed_function_764]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n\r\n2020-03-08 20:51:00.785175: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n```", "@Tuxius I seem have the same issue. Should this be reopened? Why is it closed anyway?\r\n\r\nOn my side it seems to happen when early stoppping triggers. So it does not cancel the training.\r\nAnd I get no OOM message.\r\n\r\n```\r\n156/156 [==============================] - 86s 550ms/step - loss: 0.0676 - acc: 0.9790 - val_loss: 0.7805 - val_acc: 0.8569\r\nEpoch 17/1000\r\n156/156 [==============================] - 86s 550ms/step - loss: 0.0711 - acc: 0.9748 - val_loss: 0.4852 - val_acc: 0.8875\r\nEpoch 18/1000\r\n156/156 [==============================] - 86s 550ms/step - loss: 0.0638 - acc: 0.9772 - val_loss: 1.1247 - val_acc: 0.8371\r\n2020-03-09 20:41:21.425818: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n[0.8876654] [5]\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  {'output': '...'}\r\n    to\r\n  ['...']\r\nTrain for 156 steps, validate on 11715 samples\r\nEpoch 1/1000\r\n156/156 [==============================] - 88s 566ms/step - loss: 0.4006 - acc: 0.8377 - val_loss: 1.3430 - val_acc: 0.5214\r\nEpoch 2/1000\r\n156/156 [==============================] - 86s 550ms/step - loss: 0.1554 - acc: 0.9437 - val_loss: 0.7877 - val_acc: 0.8219\r\nEpoch 3/1000\r\n```", "@Tuxius @PhilipMay @jsimsa the same problem here\r\n", "Since more and more people (@PhilipMay, @jsimsa, @drsantos89, @4doge, @Tuxius, ...) report to also still have the same issue I have reopened it [#37515](https://github.com/tensorflow/tensorflow/issues/37515)", "I have the same problem (using fit_generator)\r\nI'm using windows 10, python 3.6 and tensorflow 2.1 (cuda 10.1, cudnn 7.6.5)", "same issue here, with CPU, tf 2.1 and python 3.7", "same issue here, scales with validation_freq (occurs on those epochs for which validation is performed.) \r\nI'm using the tensorflow image tensorflow/tensorflow:2.1.0-gpu-py3 from docker hub: https://hub.docker.com/r/tensorflow/tensorflow/tags/?page=1\r\n\r\nWithin the image, the system is 18.04.3 LTS, cuda 10.1.243, python Python 3.6.9", "I'm using keras Sequence and saw this issue too. However it's weird if I only use one worker, it works well. I didn't use multiprocessing tho. 2 worker thread will leave the training hang and data show this error ", "There was a bug for Keras sequence multi-processing implementation that was fixed in https://github.com/tensorflow/tensorflow/commit/e918c6e6fab5d0005fcde83d57e92b70343d3553. This will be available in TF 2.2 and should be already available in TF nightly.", "@jsimsa thanks for the heads up! I updated to tf-nightly-gpu and the error: \r\n`W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled` went away.", "Good to know. Note that the warning and the memory leak are unrelated.", "> @jsimsa thanks for the heads up! I updated to tf-nightly-gpu and the error:\r\n> `W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled` went away.\r\n\r\nI updated today. I confirm that.", "but the training result   seems not stable.   train/val  loss/accuracy up and downs too much.", "For me the results aren't reproducible from run to run either, even with\ntf.random.set_seed(), but I suspect it has to do with multiple workers for\nmy image augmentation generator.\nOn Sat, Mar 28, 2020, 12:04 PM flydragon2018 <notifications@github.com>\nwrote:\n\n> but the training result seems not stable. train/val loss/accuracy up and\n> downs too much.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35100#issuecomment-605504881>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABI2DNKEM2P6L6YZLNMF3FLRJZC4XANCNFSM4J2WWO2A>\n> .\n>\n", "Had the same problem. Memory leak and crash after some number of epochs. Looks like the `ModelCheckpoint` callback is a culprit. Removing it solved the issue.", "> I guess this issue is related to using Tensorflow with Python-3.8.\r\n\r\nIt is not related to Python 3.8. I have the same problem with Python 3.7.4", "I found a reason for the problem on my computer - YMMV. I was using the ModelCheckpoint callback to save the best model, and if there was a model with that name already in the folder, I got the error. Removing or renaming the model with that name fixed the issue. Windows 10 system, Python 3.7.4.", "Adding this code snippet fixes this issue for me when using RTX GPUs:\r\n```\r\ndevices = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(devices[0], True)\r\n```\r\nThis is something I have to do in my training scripts as well. Might help someone :+1: ", "https://github.com/610265158/face_landmark/issues/34#issuecomment-615152235", "Ideas from stackoverflow.  I just directly copy the code from deeplearning.ai in colab. A part of it goes like this:\r\n`train_generator = train_datagen.flow_from_directory(\r\n        'horse-or-human/',  # This is the source directory for training images\r\n        target_size=(300, 300),  # All images will be resized to 300x300\r\n        batch_size=128,\r\n        # Since we use binary_crossentropy loss, we need binary labels\r\n        class_mode='binary')\r\n        \r\nhistory = model.fit(\r\n      train_generator,\r\n      steps_per_epoch=8,  \r\n      epochs=15,\r\n      verbose=1)`\r\nand there are 1027 images. 128*8=1024, less than 1027. I set steps_per_epoch to 9, the error disappear.\r\nSo, for me the **problem arises when there is wrong correspondence on the batch size and steps(iterations)**.\r\nAt least this is one of the cases for the error.\r\nHere is the original answer [https://stackoverflow.com/questions/60000573/error-occurred-when-finalizing-generatordataset-iterator-cancelled-operation-w](url)", "I have the same problem (using model.fit() ,and one numpy generator but keras.sequence)\r\nI'm using linux redhat, python 3.6 and tensorflow 2.4.1.\r\nthen I got the error \r\n  File \"/root/python_env/anaconda3/lib/python3.6/copy.py\", line 215, in _deepcopy_list\r\n    append(deepcopy(a, memo))\r\n  File \"/root/python_env/anaconda3/lib/python3.6/copy.py\", line 180, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/copy.py\", line 274, in _reconstruct\r\n    y = func(*args)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 190, in __init__\r\n    if value < 0:\r\nRecursionError: maximum recursion depth exceeded in comparison\r\n2021-03-26 14:50:45.148650: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.\r\n\t [[{{node PyFunc}}]]", "devices = tf.config.experimental.list_physical_devices('GPU')  tf.config.experimental.set_memory_growth(devices[0], True)    \r\nnot work for me", "It might be helpful ,\r\nEpoch 00001: loss improved from inf to 93.23533, saving model to model/best_model.h5\r\nTraceback (most recent call last):\r\n  File \"train_v2.py\", line 1070, in train\r\n    callbacks=callbacks_list)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1145, in fit\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 428, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 1344, in on_epoch_end\r\n    self._save_model(epoch=epoch, logs=logs)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 1396, in _save_model\r\n    self.model.save(filepath, overwrite=True, options=self._options)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 2002, in save\r\n    signatures, options, save_traces)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\", line 154, in save_model\r\n    model, filepath, overwrite, include_optimizer)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 115, in save_model_to_hdf5\r\n    model_metadata = saving_utils.model_metadata(model, include_optimizer)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/saving_utils.py\", line 155, in model_metadata\r\n    model_config['config'] = model.get_config()\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py\", line 650, in get_config\r\n    return copy.deepcopy(get_network_config(self))\r\n  File \"/root/python_env/anaconda3/lib/python3.6/copy.py\", line 150, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/copy.py\", line 240, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo) ", "> I have the same problem (using model.fit() ,and one numpy generator but keras.sequence)\r\n> I'm using linux redhat, python 3.6 and tensorflow 2.4.1.\r\n> then I got the error\r\n> File \"/root/python_env/anaconda3/lib/python3.6/copy.py\", line 215, in _deepcopy_list\r\n> append(deepcopy(a, memo))\r\n> File \"/root/python_env/anaconda3/lib/python3.6/copy.py\", line 180, in deepcopy\r\n> y = _reconstruct(x, memo, *rv)\r\n> File \"/root/python_env/anaconda3/lib/python3.6/copy.py\", line 274, in _reconstruct\r\n> y = func(*args)\r\n> File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 190, in **init**\r\n> if value < 0:\r\n> RecursionError: maximum recursion depth exceeded in comparison\r\n> 2021-03-26 14:50:45.148650: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.\r\n> [[{{node PyFunc}}]]\r\n\r\nRunning into the same error...\r\n\r\nHere is some information about my configuration:\r\nDistribution: \r\n```shell\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 18.04.5 LTS\r\nRelease:        18.04\r\nCodename:       bionic\r\n```\r\n\r\nGPU Driver:\r\n```shell\r\nFri Apr 16 12:13:45 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:07.0 Off |                    0 |\r\n| N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nCUDA version:\r\n```shell\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Sun_Feb_14_21:12:58_PST_2021\r\nCuda compilation tools, release 11.2, V11.2.152\r\nBuild cuda_11.2.r11.2/compiler.29618528_0\r\n\r\n```\r\nPython version:\r\n```shell\r\nPython 3.8.0\r\n```\r\n\r\nTensorflow version:\r\n```shell\r\n# Tensorflow-2.4.1 compiled from source with gcc and no TensorRT\r\ntensorflow @ file:///home/ubuntu/projects/tensorflow/mywhl/tensorflow-2.4.1-cp38-cp38-linux_x86_64.whl\r\ntensorflow-addons==0.12.1\r\ntensorflow-estimator==2.4.0\r\n```\r\n\r\nAny thoughts?\r\n\r\n\r\n\r\n", "The following solved the issue for me:\r\n\r\n```\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\n\r\ndef fix_gpu():\r\n    config = ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    session = InteractiveSession(config=config)\r\n\r\n\r\nfix_gpu()\r\n```\r\n\r\nCall this function at the start of your script", "Hi, \r\nI seem to be facing the same issue with Leela Chess Zero training. \r\n\r\nmessage.txt\r\nI have tried various fixes to this problem (changing the parser, changing CUDA version, changing Python version, changing TF version etc). But it continues to persist. Please help me rectify this issue.\r\n\r\nMy current PC configuration:\r\n8 GB RAM\r\nIntel i7-4790K\r\nNVIDIA RTX 2070 SUPER\r\n1TB SSD\r\n\r\nMy current requirement setup:\r\nCUDA 11.3\r\nCUDNN 8.2.1\r\nPython 3.9.5\r\nTF-Nightly GPU (2.7.0 dev)\r\n\r\nAs you can see from the attachment, the training progresses fine. But when it reaches a checkpoint an Assertion Error is thrown. I have tried this fix:\r\n\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\n\r\ndef fix_gpu():\r\n    config = ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    session = InteractiveSession(config=config)\r\n\r\nfix_gpu()\r\n\r\nBut it doesn't seem to work. Please help.\r\n", "Getting same error;  \r\n ```\r\n [[{{node PyFunc}}]]\r\n\t [[IteratorGetNext]] [Op:__inference_train_function_6832]\r\n2022-03-13 01:17:08.463663: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: FAILED_PRECONDITION: Python interpreter state is not initialized. The process may be terminated.\r\n\t [[{{node PyFunc}}]]\r\n\r\n```\r\n\r\nTried every possible way to fix it. However, could not successful. Honestly, don't know why this error occurring? I would request please reopen this issue?", "I solve this problem (in tensorflow 2.5)\r\n\r\nI suppose a file named 'train.py' to run  (this is an example)\r\n\r\nWhen run .py files in terminal\r\n```console\r\n$ CUDA_VISIBLE_DEVICES=0 python train.py  # Use GPU 0.\r\n$ CUDA_VISIBLE_DEVICES=1 python train.py  # Use GPU 1.\r\n$ CUDA_VISIBLE_DEVICES=2,3 python train.py  # Use GPUs 2 and 3.\r\n```\r\n\r\n\r\n2. Add 3 lines in train.py file\r\n~~~python\r\nimport os\r\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\r\n~~~\r\n"]}, {"number": 35099, "title": "Inconsistency with the shapes for `reduce_sum` and `reduce_logsumexp` for vectorized_map in graph mode", "body": "I don't understand the cause of different handling of shapes for two methods, here is the snippet:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\n\r\ndef _log_prob(x):\r\n    x = tf.convert_to_tensor(x, name='x')\r\n    distribution_log_probs = [x for i in range(5)]\r\n    cat_log_probs = [x for i in range(5)]\r\n    final_log_probs = [\r\n        cat_lp + d_lp\r\n        for (cat_lp, d_lp) in zip(cat_log_probs, distribution_log_probs)\r\n    ]\r\n    concat_log_probs = tf.stack(final_log_probs, 0)\r\n    log_sum = tf.reduce_logsumexp(concat_log_probs, axis=[0])\r\n    # log_sum = tf.reduce_sum(concat_log_probs, axis=[0])\r\n    return log_sum\r\n\r\n@tf.function(autograph=False)\r\ndef f():\r\n    log_prob = tf.vectorized_map(_log_prob, tf.ones((1,5)))\r\n    print(log_prob.shape) # prints (None, 5) for `tf.reduce_logsumexp` and (1, 5) for `tf.reduce_sum`\r\n```\r\n\r\nSo basically `tf.reduce_logsumexp` gives dynamic shape for the output tensor while `tf.reduce_sum` assigns static shape. Can anybody please give some clear picture on such behaviour and is it expected?\r\n\r\n```\r\ntf: 2.0.0\r\ntfp: 0.8.0\r\n```", "comments": ["#35073, tensorflow/probability#684", "I guess the problem is that v2 methods like `select_v2` are not correctly (or even not) exposed. Little fix will solve the problem with `reduce_logsumexp`. Not sure if I can open PR with more detailed review of the issues with imports.\r\n\r\n@ravikyram ", "\r\n\r\nI have tried in colab with TF 2.0, 2.1.0-rc1 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/61ab1f7a87eae6ac1049881313c5b7bd/untitled482.ipynb). Thanks!", "@ravikyram great, does the PR solve the issue?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35099\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35099\">No</a>\n", "@ravikyram @ymodak the previous code was merged again: [commit](https://github.com/tensorflow/tensorflow/commit/21882b67cbe63aaf1ee9c3d4e6f90d16cb08967d). Can you please give some info on the issue of compatibility? Do we have to wait until the next minor to update the implementation?", "@dynamicwebpaige could you give us some help here? We got a few things blocked by this.", "I also just encountered this issue, I was hoping to port my code from pymc3 to tensorflow but hit this.", "pinging a few more folks: @derifatives @brianwa84", "@agarwal-ashish \r\nIt's a difference related to `vectorized_map`. You can see by putting `assert log_prob.shape.is_fully_defined()` that both variants have a fully defined shape w/in `_log_prob`, but that they lose it when traced by vectorized_map.", "Here's my full repro against nightlies:\r\n```python\r\n%tensorflow_version 2.x\r\n!pip install --upgrade pip\r\n!python -m pip uninstall --yes tensorflow tensorflow-probability\r\n!python -m pip install -q tf-nightly-cpu tfp-nightly\r\nimport tensorflow as tf, tensorflow_probability as tfp\r\nprint(tf.__version__, tfp.__version__)\r\n\r\ndef _log_prob(fn):\r\n  def _lp(x):\r\n    x = tf.convert_to_tensor(x, name='x')\r\n    distribution_log_probs = [x for i in range(5)]\r\n    cat_log_probs = [x for i in range(5)]\r\n    final_log_probs = [\r\n        cat_lp + d_lp\r\n        for (cat_lp, d_lp) in zip(cat_log_probs, distribution_log_probs)\r\n    ]\r\n    concat_log_probs = tf.stack(final_log_probs, 0)\r\n    log_sum = fn(concat_log_probs, axis=[0])\r\n    # log_sum = tf.reduce_sum(concat_log_probs, axis=[0])\r\n    assert log_sum.shape.is_fully_defined()\r\n    return log_sum\r\n  return _lp\r\n\r\n@tf.function(autograph=False)\r\ndef f(fn):\r\n    log_prob = tf.vectorized_map(_log_prob(fn), tf.ones((1,5)))\r\n    print(fn, 'log_prob.shape = ', log_prob.shape) # prints (None, 5) for `tf.reduce_logsumexp` and (1, 5) for `tf.reduce_sum`\r\n\r\nfor fn in tf.reduce_sum, tf.reduce_logsumexp:\r\n  f(fn)\r\n```\r\n=>\r\n```\r\n2.2.0-dev20200316 0.10.0-dev20200316\r\n<function reduce_sum at 0x7f7d850b2400> log_prob.shape =  (1, 5)\r\n<function reduce_logsumexp at 0x7f7d850b6c80> log_prob.shape =  (None, 5)\r\n```", "@brianwa84 the issue was fixed before and merged: #35162 (it was built in one of the nightly builds), but then the commit was reverted due to the forward compatibility issues. I'm still not aware how the commit was breaking the forward compatibility, so that what I was asking above.", "@rrkarim \r\nI ran the code on tf-nightly(2.4.0-dev20200811) ,please find the [gist here](https://colab.research.google.com/gist/Saduf2019/893f242891da72b30927815e155518b4/untitled359.ipynb) and [here](https://colab.research.google.com/gist/Saduf2019/4e4daa12e3b9b53c47c246bec5a4f33e/untitled359.ipynb).", "@Saduf2019 yeap, recently our PR also passed the tests so yeah (related to the inconsistency with shapes), so yeah I'm closing the issue. Thank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35099\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35099\">No</a>\n"]}, {"number": 35098, "title": "add usage examples/doctests", "body": "added usage example/doctests for tf.math.is_non_decreasing and tf.math.is_strictly_increasing", "comments": ["Thanks for the review, sorry for the late reply. Pushed updates that should address the requested changes. Thanks!"]}, {"number": 35097, "title": "Update ref link for docs", "body": "Previously, this link was not clickable, fixed so that now it is :)", "comments": []}, {"number": 35096, "title": "Tensorflow lite C++ standalone projects from 'generate_projects' missing sparkfun headers?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.14.6\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.15.0 (same observation for version 2.0), from https://github.com/tensorflow/tensorflow/releases/tag/v1.15.0\r\n- Python version: python3\r\n- GCC/Compiler version (if compiling from source): arm-none-eabi-g++\r\n\r\n**Describe the problem**\r\nThe standalone projects generated with Tensorflow Lite seem to be missing some headers.\r\n\r\nFollowing this tutorial: \r\n[https://www.tensorflow.org/lite/microcontrollers/library#generate_projects_for_other_platforms\r\n](https://www.tensorflow.org/lite/microcontrollers/library#generate_projects_for_other_platforms\r\n)\r\n\r\nI run the following command (the link seems to be off here in the tutorial, as 'micro' is in another subfolder' experimental'):\r\n`gmake -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=sparkfun_edge generate_projects`\r\n\r\nAfter this command is finished, the files are generated as expected in:\r\n`tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge_cortex-m4/prj`\r\n\r\nBut when trying to build the micro_speech binary like this:\r\n```\r\ncd tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge_cortex-m4/prj/micro_speech/make\r\ngmake\r\n```\r\n\r\nI am getting the following error:\r\n```\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DPART_apollo3 -DAM_PACKAGE_BGA -DAM_PART_APOLLO3 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DNDEBUG -DTF_LITE_MCU_DEBUG_LOG -D __FPU_PRESENT=1 -DARM_MATH_CM4 -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m4 -mthumb -mfpu=fpv4-sp-d16 -mfloat-abi=hard -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -ggdb -O3 -I. -I./third_party/gemmlowp -I./third_party/flatbuffers/include -I./third_party/kissfft  -c tensorflow/lite/experimental/micro/sparkfun_edge/debug_log.cc -o tensorflow/lite/experimental/micro/sparkfun_edge/debug_log.o\r\ntensorflow/lite/experimental/micro/sparkfun_edge/debug_log.cc:22:10: fatal error: am_bsp.h: No such file or directory\r\n #include \"am_bsp.h\"   // NOLINT\r\n          ^~~~~~~~~~\r\ncompilation terminated.\r\nmake: *** [tensorflow/lite/experimental/micro/sparkfun_edge/debug_log.o] Error 1\r\n```\r\n\r\nThis is the header file of the sparkfun edge board that I want to build this for.\r\n", "comments": ["@BerndHuber,\r\nIs this still an issue? I was able to build the binaries as per the [guide](https://www.tensorflow.org/lite/microcontrollers/library#build_binaries). Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/d2a598608ca2dfd9206403095ebed6eb/35096.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35096\">No</a>\n"]}, {"number": 35095, "title": "MirroredStrategy compared to OneDeviceStrategy slower ", "body": "**System information**\r\n- OS Platform and Distribution: Arch Linux, 5.4.2-arch1-1-ARCH\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.1.0rc0-1\r\n- Keras version: 2.2.4-tf\r\n- Python version: 3.8\r\n- GPU model and memory: 2x GTX 1080 Ti 11GB\"`\r\n\r\n**Describe the current behavior**\r\nexecuting Tensorflow's MNIST handwriting example with MirroredStrategy using two GPUs is slower (286 sec) than OneDeviceStrategy (197 sec) \r\n\r\n**Describe the expected behavior**\r\nusing MirroredStrategy should be faster or at least fast as OneDeviceStrategy\r\n\r\n**Cod e to reproduce the issue**\r\nhype rparameters were tuned for MirroredStrategy using Keras Tuner\r\n\r\n```\r\n  import tensorflow as tf\r\n  import tensorflow_datasets as tfds\r\n  import time\r\n  \r\n  from tensorflow.keras.optimizers import Adam\r\n  \r\n  def build_model():\r\n      filters = 48\r\n      units = 24\r\n      kernel_size = 7\r\n      learning_rate = 1e-4\r\n      model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(units, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n      ])\r\n      model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\r\n      return model\r\n  \r\n  datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n  mnist_train, mnist_test = datasets['train'], datasets['test']\r\n  \r\n  num_train_examples = info.splits['train'].num_examples\r\n  num_test_examples = info.splits['test'].num_examples\r\n  \r\n  strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\r\n  #strategy = tf.distribute.MirroredStrategy()\r\n  print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n  \r\n  BUFFER_SIZE = 10000\r\n  BATCH_SIZE = 32\r\n  \r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n  \r\n  train_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n  eval_dataset = mnist_test.map(scale).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n  \r\n  with strategy.scope():\r\n    model = build_model()\r\n  \r\n  epochs=5\r\n  start = time.perf_counter()\r\n  model.fit(\r\n          train_dataset,\r\n          validation_data=eval_dataset,\r\n          steps_per_epoch=num_train_examples/epochs,\r\n          validation_steps=num_test_examples/epochs,\r\n          epochs=epochs)\r\n  elapsed = time.perf_counter() - start\r\n  print('elapsed: {:0.3f}'.format(elapsed))\r\n```\r\n\r\n\r\n**Other info / logs**\r\n**OneDeviceStrategy**:\r\n> ..\r\n> 12000/12000 [==============================] - 39s 3ms/step - loss: 0.0089 - accuracy: 0.9978 - val_loss: 0.0385 - val_accuracy: 0.9892\r\n> elapsed: 197.177\r\n\r\n**MirroredStrategy**:\r\n> ...\r\n> 12000/12000 [==============================] - 56s 5ms/step - loss: 0.0098 - accuracy: 0.9977 - val_loss: 0.0390 - val_accuracy: 0.9884\r\n> elapsed: 286.224", "comments": ["I have tried on colab with TF version 2.1 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/200f60befed7b2622b775b9fff6f90b0/untitled483.ipynb). Thanks!", "@ravikyram how did you run with 2 GPUs in colab? virtual GPUs? That's not a good way to debug performance issues. \r\n\r\n@olk can you please provide profiles for the run with MirroredStrategy with your 2 GPUs? Also, what was the global batch size in both cases? To get similar performance, you will likely need to increase your per replica batch size. Moreover, for a small model like MNIST, it is not unexpected that overhead of synchronization will be high compared to step time. \r\n\r\n", "@guptapriya Sorry, but what do you mean with 'profiles for the run with MirroredStrategy' exactly?\r\nThe batch size was 32 for both cases - the batch size was part of the tuned hyper parameters (using Keras Tuner).\r\n", "I too am seeing this. My rig has two RTX 2080 Tis (invested in another one of these GPUs solely for ML work) so I would be very interested in a solution.", "@neil-119 I'm switching to MXNet - it does much better utilize GPUs (including multi GPU config) + less memory consumption at the GPUs. Beside [Gluon](https://medium.com/@julsimon/gluon-building-blocks-for-your-deep-learning-universe-4bce4e56ef55) - [Keras](https://github.com/awslabs/keras-apache-mxnet) works with MXNet too.", "Closing this issue now. Thanks!"]}, {"number": 35094, "title": "Add hadoop archive support", "body": "This PR add support of hadoop archive (prefix har://) in tensorflow.\r\n\r\nHere a documentation about hadoop archive: https://hadoop.apache.org/docs/r1.2.1/hadoop_archives.html. We use it to reduce the number of files on HDFS and tensorflow is not able to read to data in these archives.\r\n\r\nIt reuses the HDFSFilesystem code as libhdfs supports hadoop archive. The difference is a hadoop archive is a new HDFS filesystem. The archive path should be injected as a name node to libhdfs filesystem builder.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35094) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35094) for more info**.\n\n<!-- ok -->", "This is causing failures of type\r\n\r\n```\r\nthird_party/tensorflow/core/platform/hadoop/hadoop_file_system.cc:186:5: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n    SplitArchiveNameAndPath(path, nn);\r\n    ^~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~\r\n1 error generated.\r\n```\r\n\r\nand has since been automatically reverted inside Google. Please fix and reopen PR."]}, {"number": 35093, "title": "Custom loss may not work when running keras model with tf.distribute.MirroredStrategy()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 in Docker\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 10.0 / 7\r\n- GPU model and memory: GTX 1080Ti / 11175MiB\r\n\r\n**Describe the current behavior**\r\n\r\nHi authors and developers,\r\n\r\nI am developing our project in tf=2.0.0 and eager_mode is disable.\r\n\r\nThe main reason is tf=1.x will not be maintained but third party libraries have not been ready for tf=2.0 yet.\r\n\r\nI got bug when I was running custom loss keras model with tf.distribute.MirroredStrategy()\r\n\r\nThis bug can be reproduced by the following minimal testcase:\r\n\r\n```python\r\n#%%\r\nfrom distutils.version import LooseVersion\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# disable eager model for tf=2.x\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nbatch_size = 100\r\nimg_h = 32\r\nimg_w = 32\r\nimg_min = 0\r\nimg_max = 1\r\nchannels = 3\r\nnum_classes = 10\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n#%%\r\ndef download_data():\r\n\r\n    # get raw data\r\n    (trainX, trainY), (testX, testY) = tf.keras.datasets.cifar10.load_data()\r\n    trainX = trainX.astype(np.float32)\r\n    testX  = testX.astype(np.float32)\r\n\r\n    # ont-hot\r\n    trainY = tf.keras.utils.to_categorical(trainY, 10)\r\n    testY  = tf.keras.utils.to_categorical(testY , 10)\r\n\r\n    # get validation sets\r\n    training_size = 45000\r\n    validX = trainX[training_size:,:]\r\n    validY = trainY[training_size:,:]\r\n\r\n    trainX = trainX[:training_size,:]\r\n    trainY = trainY[:training_size,:]\r\n\r\n    return trainX, trainY, validX, validY, testX, testY\r\n\r\n#%%\r\nclass DataGenerator:\r\n\r\n    def __init__(self, sess, dataX, dataY, total_len, batch_size):\r\n\r\n        super().__init__()\r\n\r\n        self.total_len  = total_len\r\n        self.batch_size = batch_size\r\n        self.cleanX = dataX\r\n        self.totalY = dataY\r\n        self.sess = sess\r\n        self.on_epoch_end()\r\n\r\n    def __build_pipeline(self, dataX, dataY):\r\n\r\n        # create dataset API\r\n        def preprocess_fn(dataX, dataY):\r\n            \r\n            dataX = tf.image.random_flip_left_right(dataX)\r\n\r\n            # workaround solution\r\n            if LooseVersion(tf.__version__) < LooseVersion('1.14.0'):\r\n                outputX = dataX\r\n            else:\r\n                outputX = (dataX, dataY)\r\n            return outputX, dataY\r\n\r\n        dataset = tf.data.Dataset.from_tensor_slices( (dataX, dataY) )\r\n        dataset = dataset.shuffle(batch_size * 8)\r\n        dataset = dataset.repeat()\r\n        dataset = dataset.batch(batch_size)\r\n        dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\n        self.dataset   = dataset\r\n\r\n    def  __len__(self):\r\n\r\n        return self.total_len // self.batch_size\r\n\r\n    def on_epoch_end(self):\r\n\r\n        # run permutation\r\n        rand_idx = np.random.permutation(self.total_len)\r\n        cleanX = self.cleanX[rand_idx]\r\n        totalY = self.totalY[rand_idx]\r\n\r\n        self.__build_pipeline(cleanX, totalY)\r\n\r\n#%%\r\n# ref: https://keras.io/examples/cifar10_resnet/\r\ndef build_clf():\r\n    #with strategy.scope():\r\n    with tf.compat.v1.variable_scope('optimizer'):\r\n        def resnet_layer(inputs,\r\n                        num_filters=16,\r\n                        kernel_size=3,\r\n                        strides=1,\r\n                        activation='relu',\r\n                        batch_normalization=True,\r\n                        conv_first=True):\r\n            \"\"\"2D Convolution-Batch Normalization-Activation stack builder\r\n\r\n            # Arguments\r\n                inputs (tensor): input tensor from input image or previous layer\r\n                num_filters (int): Conv2D number of filters\r\n                kernel_size (int): Conv2D square kernel dimensions\r\n                strides (int): Conv2D square stride dimensions\r\n                activation (string): activation name\r\n                batch_normalization (bool): whether to include batch normalization\r\n                conv_first (bool): conv-bn-activation (True) or\r\n                    bn-activation-conv (False)\r\n\r\n            # Returns\r\n                x (tensor): tensor as input to the next layer\r\n            \"\"\"\r\n            conv = tf.keras.layers.Conv2D(num_filters,\r\n                        kernel_size=kernel_size,\r\n                        strides=strides,\r\n                        padding='same',\r\n                        kernel_initializer='he_normal',\r\n                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))\r\n\r\n            x = inputs\r\n            if conv_first:\r\n                x = conv(x)\r\n                if batch_normalization:\r\n                    x = tf.keras.layers.BatchNormalization()(x)\r\n                if activation is not None:\r\n                    x = tf.keras.layers.Activation(activation)(x)\r\n            else:\r\n                if batch_normalization:\r\n                    x = tf.keras.layers.BatchNormalization()(x)\r\n                if activation is not None:\r\n                    x = tf.keras.layers.Activation(activation)(x)\r\n                x = conv(x)\r\n            return x\r\n\r\n        def cw_loss(y_true, y_pred):\r\n            label_mask  = label_ref\r\n            pre_softmax = x\r\n            if LooseVersion(tf.__version__) < LooseVersion('1.14.0'):\r\n                correct_logit = tf.reduce_sum(label_mask * pre_softmax, axis=1, keep_dims=True)\r\n            else:\r\n                correct_logit = tf.reduce_sum(label_mask * pre_softmax, axis=1, keepdims=True)\r\n            distance = tf.nn.relu( pre_softmax - correct_logit + (1-label_mask) * 10)\r\n            inactivate = tf.cast( tf.less_equal(distance, 1e-9), dtype=tf.float32)\r\n            weight = tf.keras.layers.Activation('softmax')(-1e9*inactivate + distance)\r\n            loss = tf.reduce_sum((1-label_mask) * distance * weight, axis=1)\r\n            loss = tf.math.reduce_mean(loss)\r\n            return loss\r\n\r\n        # set model's parameters (depth = n * 6 + 2)\r\n        n = 8\r\n        num_filters = 16\r\n\r\n        clf_input = tf.keras.layers.Input(shape=(img_h, img_w, channels), name=\"model/input\")\r\n        label_ref = tf.keras.layers.Input(shape=(num_classes,), name='label_ref')\r\n        input_list = [clf_input, label_ref]\r\n\r\n        x = resnet_layer(inputs=clf_input)\r\n        for stack in range(3):\r\n            for res_block in range(n):\r\n                strides = 1\r\n                if stack > 0 and res_block == 0:  # first layer but not first stack\r\n                    strides = 2  # downsample\r\n                y = resnet_layer(inputs=x,\r\n                                num_filters=num_filters,\r\n                                strides=strides)\r\n                y = resnet_layer(inputs=y,\r\n                                num_filters=num_filters,\r\n                                activation=None)\r\n                if stack > 0 and res_block == 0:  # first layer but not first stack\r\n                    # linear projection residual shortcut connection to match\r\n                    # changed dims\r\n                    x = resnet_layer(inputs=x,\r\n                                    num_filters=num_filters,\r\n                                    kernel_size=1,\r\n                                    strides=strides,\r\n                                    activation=None,\r\n                                    batch_normalization=False)\r\n                x = tf.keras.layers.Add()([x, y])\r\n                x = tf.keras.layers.Activation('relu')(x)\r\n            num_filters *= 2\r\n\r\n        x = tf.keras.layers.AveragePooling2D(pool_size=8)(x)\r\n        x = tf.keras.layers.Flatten()(x)\r\n        x = tf.keras.layers.Dense(num_classes , kernel_initializer='he_normal', activation=None)(x)\r\n        y = tf.keras.layers.Activation('softmax')(x)\r\n\r\n        optimizer = tf.keras.optimizers.Adam(lr=0.001)\r\n        clf_model = tf.keras.models.Model(inputs=input_list, outputs=y, name='clf_model')\r\n        clf_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', cw_loss])\r\n    clf_model.summary()\r\n\r\n    return clf_model\r\n\r\n#%%\r\nif __name__ == '__main__':\r\n\r\n    # set GPU\r\n    import os\r\n    if os.environ.get(\"CUDA_VISIBLE_DEVICES\") is None:\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\n    # reset tf session\r\n    tf.compat.v1.keras.backend.clear_session()\r\n    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\r\n    sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\r\n    tf.compat.v1.keras.backend.set_session(sess)\r\n\r\n    # Hyperparameters\r\n    batch_size = 100\r\n    epochs = 1\r\n\r\n    # prepare data\r\n    trainX, trainY, validX, validY, testX, testY = download_data()\r\n    train_gen = DataGenerator(sess, trainX, trainY, trainY.shape[0], batch_size)\r\n    valid_gen = DataGenerator(sess, validX, validY, validY.shape[0], batch_size)\r\n    test_gen  = DataGenerator(sess, testX, testY, testY.shape[0], batch_size)\r\n\r\n    # build model\r\n    model = build_clf()\r\n\r\n    # train model\r\n    model.fit(train_gen.dataset,\r\n                    epochs=epochs,\r\n                    steps_per_epoch = train_gen.__len__(),\r\n                    validation_data=valid_gen.dataset,\r\n                    validation_steps= valid_gen.__len__(),\r\n                    verbose=1)\r\n\r\n    # print result\r\n    meta_string = '[Testing]'\r\n    prefix_string = ''\r\n    output = model.evaluate(test_gen.dataset, steps = test_gen.__len__())\r\n    for ii in range( len( model.metrics_names) ):\r\n        meta_string = meta_string + '- {:s}{:s}: {:.3f} '.format(prefix_string, model.metrics_names[ii], output[ii])\r\n\r\n    print(meta_string)\r\n```\r\n\r\nFirst, this testing case looks good without enabling `tf.distribute.MirroredStrategy()`\r\n\r\nThere is the output for normal case:\r\n\r\n```\r\nTrain on 450 steps, validate on 50 steps\r\n2019-12-13 16:20:30.625379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-12-13 16:20:31.217430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-13 16:20:33.007150: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found\r\nRelying on driver to perform ptx compilation. This message will be only logged once.\r\n450/450 [==============================] - 40s 88ms/step - loss: 1.8299 - accuracy: 0.4744 - cw_loss: 9.5022 - val_loss: 1.9870 - val_accuracy: 0.4528 - val_cw_loss: 9.6570\r\n100/100 [==============================] - 3s 26ms/step - loss: 2.0089 - accuracy: 0.4511 - cw_loss: 9.6708\r\n[Testing]- loss: 2.009 - accuracy: 0.451 - cw_loss: 9.671\r\n\r\n```\r\n\r\nNext, we tried to enable `tf.distribute.MirroredStrategy()` so we modified the testcase by the following patch:\r\n\r\n```diff\r\ndef build_clf():\r\n-    #with strategy.scope():\r\n-    with tf.compat.v1.variable_scope('optimizer'):\r\ndef build_clf():\r\n+    with strategy.scope():\r\n+    #with tf.compat.v1.variable_scope('optimizer'):\r\n```\r\n\r\nAnd we got the error message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 233, in <module>\r\n    verbose=1)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 717, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 685, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 299, in model_iteration\r\n    batch_outs = f(actual_inputs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/backend.py\", line 3580, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py\", line 1472, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: You must feed a value for placeholder tensor 'model/input' with dtype float and shape [?,32,32,3]\r\n         [[{{node model/input}}]]\r\n         [[batch_normalization_9/cond/else/_325/FusedBatchNormV3/ReadVariableOp/_2529]]\r\n  (1) Invalid argument: You must feed a value for placeholder tensor 'model/input' with dtype float and shape [?,32,32,3]\r\n         [[{{node model/input}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\n\r\nThis error is very similar to the previous issue #34866.\r\n\r\nI guess that those two issues may have some strong connection.\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should work properly.\r\n\r\n**Code to reproduce the issue**\r\n\r\nPlease see the section of **Describe the current behavior**\r\n\r\n**Other info / logs**\r\n\r\nThe following message is the result generated by `tf_env_collect.sh`\r\n```\r\n== check python ===================================================\r\npython version: 3.5.2\r\npython branch:\r\npython build version: ('default', 'Oct  8 2019 13:06:37')\r\npython compiler version: GCC 5.4.0 20160609\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC 2019\r\nos release version: 5.0.0-37-generic\r\nos platform: Linux-5.0.0-37-generic-x86_64-with-Ubuntu-16.04-xenial\r\nlinux distribution: ('Ubuntu', '16.04', 'xenial')\r\nlinux os distribution: ('Ubuntu', '16.04', 'xenial')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='f7f509f1dacf', release='5.0.0-37-generic', version='#40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC 2019', machine='x86_64', processor='x86_64')\r\narchitecture: ('64bit', 'ELF')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nnumpy                  1.17.4\r\nprotobuf               3.11.1\r\ntensorflow-estimator   2.0.1\r\ntensorflow-gpu         2.0.0\r\ntensorflow-probability 0.8.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.0.0\r\ntf.version.GIT_VERSION = v2.0.0-rc2-26-g64c3d38\r\ntf.version.COMPILER_VERSION = 7.3.1 20180303\r\nSanity check: array([1], dtype=int32)\r\n       443:     find library=libpthread.so.0 [0]; searching\r\n       443:      search path=/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64          (LD_LIBRARY_PATH)\r\n       443:       trying file=/usr/local/nvidia/lib/tls/x86_64/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib/tls/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib/x86_64/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib64/tls/x86_64/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib64/tls/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib64/x86_64/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib64/libpthread.so.0\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libpthread.so.0\r\n       443:\r\n       443:     find library=libc.so.6 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libc.so.6\r\n       443:\r\n       443:     find library=libdl.so.2 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libdl.so.2\r\n       443:\r\n       443:     find library=libutil.so.1 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libutil.so.1\r\n       443:\r\n       443:     find library=libexpat.so.1 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libexpat.so.1\r\n       443:\r\n       443:     find library=libz.so.1 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libz.so.1\r\n       443:\r\n       443:     find library=libm.so.6 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libm.so.6\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libpthread.so.0\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libc.so.6\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libm.so.6\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libz.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libexpat.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libutil.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libdl.so.2\r\n       443:\r\n       443:\r\n       443:     initialize program: /usr/local/bin/python\r\n       443:\r\n       443:\r\n       443:     transferring control: /usr/local/bin/python\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_opcode.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libopenblasp-r0-34a18dc3.3.7.so [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/x86_64:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/x86_64/libopenblasp-r0-34a18dc3.3.7.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/libopenblasp-r0-34a18dc3.3.7.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/x86_64/libopenblasp-r0-34a18dc3.3.7.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so\r\n       443:\r\n       443:     find library=libgfortran-ed201abd.so.3.0.0 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs         (RPATH from file /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_tests.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libbz2.so.1.0 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libbz2.so.1.0\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libbz2.so.1.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_bz2.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=liblzma.so.5 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/liblzma.so.5\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/liblzma.so.5\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_lzma.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libmpdec.so.2 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/libmpdec.so.2\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/x86_64-linux-gnu/libmpdec.so.2\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_decimal.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/fft/_pocketfft_internal.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/common.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/bounded_integers.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/mt19937.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/bit_generator.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libcrypto.so.1.0.0 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libcrypto.so.1.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libcrypto.so.1.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_hashlib.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/philox.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/pcg64.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/sfc64.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/generator.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libtensorflow_framework.so.2 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..            (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n       443:\r\n       443:     find library=librt.so.1 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/librt.so.1\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../librt.so.1\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/librt.so.1\r\n       443:\r\n       443:     find library=libstdc++.so.6 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libstdc++.so.6\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libstdc++.so.6\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n       443:\r\n       443:     find library=libgcc_s.so.1 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libgcc_s.so.1\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libgcc_s.so.1\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libgcc_s.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libgcc_s.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/librt.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n       443:\r\n       443:     find library=libhdfs.so [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..           (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libhdfs.so\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libhdfs.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libhdfs.so\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:      search path=/lib/x86_64-linux-gnu/tls/x86_64:/lib/x86_64-linux-gnu/tls:/lib/x86_64-linux-gnu/x86_64:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/tls/x86_64:/usr/lib/x86_64-linux-gnu/tls:/usr/lib/x86_64-linux-gnu/x86_64:/usr/lib/x86_64-linux-gnu:/lib/tls/x86_64:/lib/tls:/lib/x86_64:/lib:/usr/lib/tls/x86_64:/usr/lib/tls:/usr/lib/x86_64:/usr/lib              (system search path)\r\n       443:       trying file=/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so\r\n       443:       trying file=/lib/x86_64-linux-gnu/tls/libhdfs.so\r\n       443:       trying file=/lib/x86_64-linux-gnu/x86_64/libhdfs.so\r\n       443:       trying file=/lib/x86_64-linux-gnu/libhdfs.so\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/tls/libhdfs.so\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/x86_64/libhdfs.so\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/libhdfs.so\r\n       443:       trying file=/lib/tls/x86_64/libhdfs.so\r\n       443:       trying file=/lib/tls/libhdfs.so\r\n       443:       trying file=/lib/x86_64/libhdfs.so\r\n       443:       trying file=/lib/libhdfs.so\r\n       443:       trying file=/usr/lib/tls/x86_64/libhdfs.so\r\n       443:       trying file=/usr/lib/tls/libhdfs.so\r\n       443:       trying file=/usr/lib/x86_64/libhdfs.so\r\n       443:       trying file=/usr/lib/libhdfs.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/fast_tensor_util.so\r\n       443:\r\n       443:     find library=libuuid.so.1 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libuuid.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libuuid.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/wrapt/_wrappers.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_json.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libssl.so.1.0.0 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libssl.so.1.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libssl.so.1.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_ssl.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libhdf5-49599f4e.so.103.0.0 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls:/usr/local/lib/python3.5/dist-packages/h5py/.libs/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/x86_64/libhdf5-49599f4e.so.103.0.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/libhdf5-49599f4e.so.103.0.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/x86_64/libhdf5-49599f4e.so.103.0.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0\r\n       443:\r\n       443:     find library=libhdf5_hl-db841637.so.100.1.1 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs          (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1\r\n       443:\r\n       443:     find library=libsz-1c7dd0cf.so.2.0.1 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls:/usr/local/lib/python3.5/dist-packages/h5py/.libs/./x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/x86_64/libsz-1c7dd0cf.so.2.0.1\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/libsz-1c7dd0cf.so.2.0.1\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./x86_64/libsz-1c7dd0cf.so.2.0.1\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1\r\n       443:\r\n       443:     find library=libaec-2147abcd.so.0.0.4 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4\r\n       443:\r\n       443:     find library=libz-a147dcb0.so.1.2.3 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       452:     find library=libc.so.6 [0]; searching\r\n       452:      search path=/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64          (LD_LIBRARY_PATH)\r\n       452:       trying file=/usr/local/nvidia/lib/tls/x86_64/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib/tls/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib/x86_64/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib64/tls/x86_64/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib64/tls/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib64/x86_64/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib64/libc.so.6\r\n       452:      search cache=/etc/ld.so.cache\r\n       452:       trying file=/lib/x86_64-linux-gnu/libc.so.6\r\n       452:\r\n       452:\r\n       452:     calling init: /lib/x86_64-linux-gnu/libc.so.6\r\n       452:\r\n       452:\r\n       452:     initialize program: /bin/sh\r\n       452:\r\n       452:\r\n       452:     transferring control: /bin/sh\r\n       452:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5z.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5a.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5s.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5p.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5ac.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_proxy.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5d.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5ds.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5f.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5g.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5i.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5fd.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5pl.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5o.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5l.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/_lib/_ccallback_c.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_sparsetools.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_csparsetools.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_tools.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_traversal.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_reordering.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libjpeg-3b10b538.so.9.3.0 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls:/usr/local/lib/python3.5/dist-packages/PIL/.libs/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/x86_64/libjpeg-3b10b538.so.9.3.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/libjpeg-3b10b538.so.9.3.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/x86_64/libjpeg-3b10b538.so.9.3.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0\r\n       443:\r\n       443:     find library=libopenjp2-b3d7668a.so.2.3.1 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs           (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1\r\n       443:\r\n       443:     find library=libtiff-8267adfe.so.5.4.0 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs           (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0\r\n       443:\r\n       443:     find library=liblzma-6cd627ed.so.5.2.4 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls:/usr/local/lib/python3.5/dist-packages/PIL/.libs/./x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/.            (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/x86_64/liblzma-6cd627ed.so.5.2.4\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/liblzma-6cd627ed.so.5.2.4\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./x86_64/liblzma-6cd627ed.so.5.2.4\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libopenblasp-r0-2ecf47d5.3.7.dev.so [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/x86_64:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/x86_64/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/x86_64/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flinalg.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/bin/python [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libutil.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libexpat.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libz.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_opcode.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_tests.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_bz2.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libbz2.so.1.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_lzma.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/liblzma.so.5 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_decimal.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/x86_64-linux-gnu/libmpdec.so.2 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/fft/_pocketfft_internal.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/common.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/bounded_integers.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/mt19937.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/bit_generator.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_hashlib.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/philox.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/pcg64.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/sfc64.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/generator.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/fast_tensor_util.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libuuid.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/wrapt/_wrappers.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_json.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_ssl.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libssl.so.1.0.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libcrypto.so.1.0.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5z.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5a.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5s.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5p.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5ac.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_proxy.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5d.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5ds.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5f.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5g.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5i.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5fd.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5pl.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5o.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5l.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/_lib/_ccallback_c.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_sparsetools.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_csparsetools.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_tools.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_traversal.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_reordering.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flinalg.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/x86_64-linux-gnu/libstdc++.so.6 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libgcc_s.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libm.so.6 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libpthread.so.0 [0]\r\n       443:\r\n\r\n```", "comments": ["I have tried on colab with TF version 2.0  and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/8a6018f81c690559cafa31428706bd61/untitled484.ipynb). Thanks!", "We don't support custom losses in legacy graph mode (which is what you have if you disable eager mode). Does this fail if you use proper TF 2.0 (without disabling eager mode)? ", "@guptapriya \r\n\r\nHow do I know which functions or features will not be supported or maintained in future?\r\n\r\nYes, I still got an error when I disable eager_mode.\r\nFirst, I removed `Session` and `tf.compat.v1.disable_eager_execution()` in my testcase and ran testcase without `tf.distribute.MirroredStrategy()` in tf=2.0.0.\r\n\r\nThe testcase can be seen in the following:\r\n\r\n```python\r\n#%%\r\nfrom distutils.version import LooseVersion\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n#tf.compat.v1.disable_eager_execution()\r\n\r\nbatch_size = 100\r\nimg_h = 32\r\nimg_w = 32\r\nimg_min = 0\r\nimg_max = 1\r\nchannels = 3\r\nnum_classes = 10\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n#%%\r\ndef download_data():\r\n\r\n    # get raw data\r\n    (trainX, trainY), (testX, testY) = tf.keras.datasets.cifar10.load_data()\r\n    trainX = trainX.astype(np.float32)\r\n    testX  = testX.astype(np.float32)\r\n\r\n    # ont-hot\r\n    trainY = tf.keras.utils.to_categorical(trainY, 10)\r\n    testY  = tf.keras.utils.to_categorical(testY , 10)\r\n\r\n    # get validation sets\r\n    training_size = 45000\r\n    validX = trainX[training_size:,:]\r\n    validY = trainY[training_size:,:]\r\n\r\n    trainX = trainX[:training_size,:]\r\n    trainY = trainY[:training_size,:]\r\n\r\n    return trainX, trainY, validX, validY, testX, testY\r\n\r\n#%%\r\nclass DataGenerator:\r\n\r\n    def __init__(self, dataX, dataY, total_len, batch_size):\r\n\r\n        super().__init__()\r\n\r\n        self.total_len  = total_len\r\n        self.batch_size = batch_size\r\n        self.cleanX = dataX\r\n        self.totalY = dataY\r\n        self.on_epoch_end()\r\n\r\n    def __build_pipeline(self, dataX, dataY):\r\n\r\n        # create dataset API\r\n        def preprocess_fn(dataX, dataY):\r\n            \r\n            dataX = tf.image.random_flip_left_right(dataX)\r\n\r\n            # workaround solution\r\n            if LooseVersion(tf.__version__) < LooseVersion('1.14.0'):\r\n                outputX = dataX\r\n            else:\r\n                outputX = (dataX, dataY)\r\n            return outputX, dataY\r\n\r\n        dataset = tf.data.Dataset.from_tensor_slices( (dataX, dataY) )\r\n        dataset = dataset.shuffle(batch_size * 8)\r\n        dataset = dataset.repeat()\r\n        dataset = dataset.batch(batch_size)\r\n        dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\n        self.dataset   = dataset\r\n\r\n    def  __len__(self):\r\n\r\n        return self.total_len // self.batch_size\r\n\r\n    def on_epoch_end(self):\r\n\r\n        # run permutation\r\n        rand_idx = np.random.permutation(self.total_len)\r\n        cleanX = self.cleanX[rand_idx]\r\n        totalY = self.totalY[rand_idx]\r\n\r\n        self.__build_pipeline(cleanX, totalY)\r\n\r\n#%%\r\n# ref: https://keras.io/examples/cifar10_resnet/\r\ndef build_clf():\r\n    #with strategy.scope():\r\n    if True:\r\n        def resnet_layer(inputs,\r\n                        num_filters=16,\r\n                        kernel_size=3,\r\n                        strides=1,\r\n                        activation='relu',\r\n                        batch_normalization=True,\r\n                        conv_first=True):\r\n            \"\"\"2D Convolution-Batch Normalization-Activation stack builder\r\n\r\n            # Arguments\r\n                inputs (tensor): input tensor from input image or previous layer\r\n                num_filters (int): Conv2D number of filters\r\n                kernel_size (int): Conv2D square kernel dimensions\r\n                strides (int): Conv2D square stride dimensions\r\n                activation (string): activation name\r\n                batch_normalization (bool): whether to include batch normalization\r\n                conv_first (bool): conv-bn-activation (True) or\r\n                    bn-activation-conv (False)\r\n\r\n            # Returns\r\n                x (tensor): tensor as input to the next layer\r\n            \"\"\"\r\n            conv = tf.keras.layers.Conv2D(num_filters,\r\n                        kernel_size=kernel_size,\r\n                        strides=strides,\r\n                        padding='same',\r\n                        kernel_initializer='he_normal',\r\n                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))\r\n\r\n            x = inputs\r\n            if conv_first:\r\n                x = conv(x)\r\n                if batch_normalization:\r\n                    x = tf.keras.layers.BatchNormalization()(x)\r\n                if activation is not None:\r\n                    x = tf.keras.layers.Activation(activation)(x)\r\n            else:\r\n                if batch_normalization:\r\n                    x = tf.keras.layers.BatchNormalization()(x)\r\n                if activation is not None:\r\n                    x = tf.keras.layers.Activation(activation)(x)\r\n                x = conv(x)\r\n            return x\r\n\r\n        def cw_loss(y_true, y_pred):\r\n            label_mask  = label_ref\r\n            pre_softmax = x\r\n            if LooseVersion(tf.__version__) < LooseVersion('1.14.0'):\r\n                correct_logit = tf.reduce_sum(label_mask * pre_softmax, axis=1, keep_dims=True)\r\n            else:\r\n                correct_logit = tf.reduce_sum(label_mask * pre_softmax, axis=1, keepdims=True)\r\n            distance = tf.nn.relu( pre_softmax - correct_logit + (1-label_mask) * 10)\r\n            inactivate = tf.cast( tf.less_equal(distance, 1e-9), dtype=tf.float32)\r\n            weight = tf.keras.layers.Activation('softmax')(-1e9*inactivate + distance)\r\n            loss = tf.reduce_sum((1-label_mask) * distance * weight, axis=1)\r\n            loss = tf.math.reduce_mean(loss)\r\n            return loss\r\n\r\n        # set model's parameters (depth = n * 6 + 2)\r\n        n = 8\r\n        num_filters = 16\r\n\r\n        clf_input = tf.keras.layers.Input(shape=(img_h, img_w, channels), name=\"model/input\")\r\n        label_ref = tf.keras.layers.Input(shape=(num_classes,), name='label_ref')\r\n        input_list = [clf_input, label_ref]\r\n\r\n        x = resnet_layer(inputs=clf_input)\r\n        for stack in range(3):\r\n            for res_block in range(n):\r\n                strides = 1\r\n                if stack > 0 and res_block == 0:  # first layer but not first stack\r\n                    strides = 2  # downsample\r\n                y = resnet_layer(inputs=x,\r\n                                num_filters=num_filters,\r\n                                strides=strides)\r\n                y = resnet_layer(inputs=y,\r\n                                num_filters=num_filters,\r\n                                activation=None)\r\n                if stack > 0 and res_block == 0:  # first layer but not first stack\r\n                    # linear projection residual shortcut connection to match\r\n                    # changed dims\r\n                    x = resnet_layer(inputs=x,\r\n                                    num_filters=num_filters,\r\n                                    kernel_size=1,\r\n                                    strides=strides,\r\n                                    activation=None,\r\n                                    batch_normalization=False)\r\n                x = tf.keras.layers.Add()([x, y])\r\n                x = tf.keras.layers.Activation('relu')(x)\r\n            num_filters *= 2\r\n\r\n        x = tf.keras.layers.AveragePooling2D(pool_size=8)(x)\r\n        x = tf.keras.layers.Flatten()(x)\r\n        x = tf.keras.layers.Dense(num_classes , kernel_initializer='he_normal', activation=None)(x)\r\n        y = tf.keras.layers.Activation('softmax')(x)\r\n\r\n        optimizer = tf.keras.optimizers.Adam(lr=0.001)\r\n        clf_model = tf.keras.models.Model(inputs=input_list, outputs=y, name='clf_model')\r\n        clf_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', cw_loss])\r\n    clf_model.summary()\r\n\r\n    return clf_model\r\n\r\n#%%\r\nif __name__ == '__main__':\r\n\r\n    # set GPU\r\n    import os\r\n    if os.environ.get(\"CUDA_VISIBLE_DEVICES\") is None:\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\n    # Hyperparameters\r\n    batch_size = 100\r\n    epochs = 1\r\n\r\n    # prepare data\r\n    trainX, trainY, validX, validY, testX, testY = download_data()\r\n    train_gen = DataGenerator(trainX, trainY, trainY.shape[0], batch_size)\r\n    valid_gen = DataGenerator(validX, validY, validY.shape[0], batch_size)\r\n    test_gen  = DataGenerator(testX, testY, testY.shape[0], batch_size)\r\n\r\n    # build model\r\n    model = build_clf()\r\n\r\n    # train model\r\n    model.fit(train_gen.dataset,\r\n                    epochs=epochs,\r\n                    steps_per_epoch = train_gen.__len__(),\r\n                    validation_data=valid_gen.dataset,\r\n                    validation_steps= valid_gen.__len__(),\r\n                    verbose=1)\r\n\r\n    # print result\r\n    meta_string = '[Testing]'\r\n    prefix_string = ''\r\n    output = model.evaluate(test_gen.dataset, steps = test_gen.__len__())\r\n    for ii in range( len( model.metrics_names) ):\r\n        meta_string = meta_string + '- {:s}{:s}: {:.3f} '.format(prefix_string, model.metrics_names[ii], output[ii])\r\n\r\n    print(meta_string)\r\n```\r\n\r\nI got the following error message:\r\n\r\n```\r\n__________________________________________________________________________________________________\r\nTrain for 450 steps, validate for 50 steps\r\n  1/450 [..............................] - ETA: 35:06Traceback (most recent call last):\r\n  File \"/home/ecchen/opt/miniconda/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 61, in quick_execute\r\n    num_outputs)\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: label_ref:0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 223, in <module>\r\n    verbose=1)\r\n  File \"/home/ecchen/opt/miniconda/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/ecchen/opt/miniconda/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)\r\n  File \"/home/ecchen/opt/miniconda/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/home/ecchen/opt/miniconda/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/home/ecchen/opt/miniconda/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/ecchen/opt/miniconda/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/ecchen/opt/miniconda/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/ecchen/opt/miniconda/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/ecchen/opt/miniconda/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/home/ecchen/opt/miniconda/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/home/ecchen/opt/miniconda/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 75, in quick_execute\r\n    \"tensors, but found {}\".format(keras_symbolic_tensors))\r\ntensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'label_ref:0' shape=(None, 10) dtype=float32>, <tf.Tensor 'dense/Identity:0' shape=(None, 10) dtype=float32>]\r\n```\r\n\r\nNext, I tried to custom model with `tf.distribute.MirroredStrategy()`\r\n\r\n```diff\r\n-    #with strategy.scope():\r\n-    if True:\r\n+    with strategy.scope():\r\n+    #if True:\r\n````\r\n\r\nI still got same error.\r\n\r\nIn conclusion, `tf.distribute.MirroredStrategy()` dose not support custom loss.", "The error reported in https://github.com/tensorflow/tensorflow/issues/35093#issuecomment-566334859 - is that happening both with and without MirroredStrategy? \r\nAlso you're using \"cw_loss\" as a metric? I don't see from the code how that is being used as a custom loss. \r\nThere are a few tests in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/distribute/distribute_strategy_test.py which test MirroredStrategy works with custom losses. ", "Yes, error is that happening both with and without MirroredStrategy\r\n\r\nAnd Yes, I use `cw_loss` as a metric in this testcase. \r\n\r\nBecause I will switch loss form `categorical_crossentropy` to `cw_loss` during our custom training phase. \r\n\r\nBut I want to know that which epoch is the best epoch to do the conversion. \r\n\r\nSo I have to record my custom loss as metric in the beginning.", "@guptapriya \r\n\r\nIf you want to test `cw_loss` as loss, Please apply the following patch.\r\n\r\n```diff\r\n-        clf_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', \r\n+        clf_model.compile(loss=cw_loss, optimizer=optimizer, metrics=['accuracy', 'categorical_crossentropy'])\r\n\r\n```\r\n\r\ntf still throws error.", "Hi @CNOCycle, I've run the code you provided in [this comment](https://github.com/tensorflow/tensorflow/issues/35093#issuecomment-566334859) with tf-nightly, and see a type error\r\n`TypeError: Tensors are unhashable. (KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='tf.math.reduce_sum_2/Sum:0', description=\"Symbolic value 0 from symbolic call 0 of layer 'tf.math.reduce_sum_2'\"))Instead, use tensor.ref() as the key.`\r\n\r\nI then replaced \r\n`clf_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', cw_loss]`\r\nwith\r\n`clf_model.compile(loss=cw_loss, optimizer=optimizer, metrics=['accuracy', 'categorical_crossentropy'])`\r\nAnd get\r\n` TypeError: Cannot convert a symbolic Keras input/output to a numpy array. This error may indicate that you're trying to pass a symbolic value to a NumPy call, which is not supported. Or, you may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model.`\r\n\r\nSo there seem to be two different issues that are not related to MirroredStrategy, as both errors occur when training without MirroredStrategy. You'll want to test you code without distribution and make sure it works first, before wrapping it in MirroredStrategy. \r\n\r\nIf you are still facing difficulties with your model and want to open up a new issue for either of these errors feel free to do so. As @guptapriya mentioned, MirroredStrategy does work with custom losses and it seems to me you are facing a different problem other than MirroredStrategy and custom losses.", "I guess that tensorflow has huge upgrades from `2.0` to `2.3` or above. I can't successfully run my code either in `tf-nightly`.\r\n\r\nI will try to redesign a new custom loss for `tf-nightly`.\r\n\r\nFrom my opinion, custom loss with `MirroredStrategy` is still a important feature.\r\n\r\nSome of models with custom loss are complex and it should be trained with GPUs. \r\n\r\nIt has been worked in TF1 as least.", "Agreed that it's an important feature, and as mentioned earlier custom losses do work with MirroredStrategy. I'd be happy to share an example with you if that's helpful. Also, if you look through the dist-strat component issues on Github you'll see some different examples of custom losses with MirroredStrategy.", "Closing this issue now since custom loss functions are supported with MirroredStrategy. Please open a new issue if you find an example where this is not the case.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35093\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35093\">No</a>\n", "@nikitamaia I am facing the same issue, it would be useful if you could share the links here for future references.", "Hi @uzi0espil, can you open a new issue with information on the problem you're facing and reproducible code? I can help you to debug.", "Hi, @nikitamaia, could you let me know which version supports the custom loss with MirroredStrategy?  "]}, {"number": 35092, "title": "Keyword arguments of the optimizer for learning rate schedule", "body": "This statement raises an error if the learning rate is some learning rate schedule class instance. It does not raise any error if specification is without keyword arguments. \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/c10aa0338f928dcc3da2c2660d88c80bcdd01278/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L259", "comments": ["@halidziya ,\r\nCan you please provide more information on the issue? Kindly provide share a simple and standalone code to reproduce the same ?Thanks!", "Adam(lr=CustomSchedule()) does not work since  lr is renamed to learning_rate in tf.keras.optimizers.Adam"]}, {"number": 35091, "title": "[GPU][ROCm] Fix hip-clang build", "body": "This patches fixes a bug which causes build failure for hip-clang.\r\n\r\nBasically in third_party/gpus/rocm_configure.bzl a string is checked to determine whether the compiler is hip-clang. Comparison fails due to new line not being stripped. This patch strips new line so that comparison succeeds.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35091) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it! ", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35091) for more info**.\n\n<!-- ok -->", "@chsigg / @yxsamliu Looking at test logs failures in `Windows Bazel` and `Windows Bazel GPU` don't seem to be related to changes introduced in this PR.", "@yxsamliu , please help cherry-pick this patch to the TF1.15 release branch below:\r\nhttps://github.com/tensorflow/tensorflow/tree/r1.15\r\n", "> @yxsamliu , please help cherry-pick this patch to the TF1.15 release branch below:\r\n> https://github.com/tensorflow/tensorflow/tree/r1.15\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/35231"]}, {"number": 35090, "title": "Subsequent calls to tf.data.Dataset.map() with seeded random operation give same random sequences.", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: CentOS 7\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.14.0\r\n- Python version: 2.7.17\r\n- CUDA/cuDNN version: 10.0\r\n\r\n**Describe the current behavior**\r\nIn eager mode when setting a random seed, two calls to the same mapping function in tf.data.Dataset.map() would result in two times the same generated random values.\r\n\r\n**Describe the expected behavior**\r\nAs I haven't explicitly closed or restarted a session, I would expect both calls to output different generated random values. Is tf.data.Dataset.map() creating its own session and running the mapping function subgraph in it? If so, is there a way to force the map() to take the current session as input?\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np  \r\nimport tensorflow as tf\r\ntf.compat.v1.enable_eager_execution()\r\n\r\nSEED = 88\r\ntf.compat.v1.random.set_random_seed(SEED)\r\n\r\nds_train = tf.data.Dataset.range(0, 4)\r\nds_val = tf.data.Dataset.range(0, 4)\r\nds_train = ds_train.map(\r\n    lambda x: tf.random.uniform([1], 0.2, 5.0)\r\n)\r\nds_val = ds_val.map(\r\n    lambda x: tf.random.uniform([1], 0.2, 5.0) \r\n)\r\nfor el in ds_train:\r\n    print(el.numpy())\r\n    # --> [0.44027787], [1.7892183], [2.8793733], [3.3438706]\r\nfor el in ds_val:\r\n    print(el.numpy())\r\n    # why the same here? --> [0.44027787], [1.7892183], [2.8793733], [3.3438706]\r\n```\r\n", "comments": ["I could replicate the issue on colab with Tf 1.14 and 1.15.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/7840922356bfca9f85923b8271f3aa8c/untitled305.ipynb). Thanks!", "```tf.random.uniform``` has a ```seed```argument which is same in both calls. Therefore you see same numbers when you print.\r\nIf the graph-level seed is set, but the operation seed is not: The system deterministically picks an operation seed in conjunction with the graph-level seed so that it gets a unique random sequence.\r\nSee https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/random/set_random_seed", "Hi @ymodak , thank you for your answer!\r\nI understand the double-seed system but in the case where you have the same random operation with the same seed, you should still have a sequence of different random numbers within the same session. This sequence should repeat only if you compute it in a new session. For instance\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.enable_eager_execution()\r\nSEED = 88\r\ntf.compat.v1.random.set_random_seed(SEED)\r\nrand1 = tf.random.uniform([1], 0.2, 5.0, seed=SEED) #-->[2.7339704]\r\nrand2 = tf.random.uniform([1], 0.2, 5.0, seed=SEED) #-->[1.4490409] different\r\n# Same when only graph-level seed is set\r\nrand1 = tf.random.uniform([1], 0.2, 5.0) #-->[2.9647074]\r\nrand2 = tf.random.uniform([1], 0.2, 5.0) #-->[2.7952404] different\r\n```\r\nWhich is why I would expect both random operation in the map() function to be a continuation of a sequence of different random numbers as we are still in the same session.", "Hi @johannabar, there are no longer sessions when eager mode is enabled. Every time `Dataset.map` is called, it creates a `tf.function` for the mapping function. The `random.uniform` in the mapping function gets its graph-level seed from the `tf.function`'s default graph's seed (https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/framework/random_seed.py#L62). I expected this seed to be `None`, but apparently it's not (maybe this is a side effect of `set_random_seed`), in which case the two `random.uniform`s start from the same seed and the same state (because both are in a fresh new `tf.function`). BTW you can check out the new RNG (https://www.tensorflow.org/api_docs/python/tf/random/experimental/Generator) which has much better seeding control.", "Hi @wangpengmit, thank you for the explanation!\r\nI am closing this issue as what I noticed is correct behaviour.\r\n\r\nThough I think it's a little tricky that a map call to the same mapping function will produce the same random numbers. For instance if you have a map function doing random cropping on images and you call it on both your training and validation data separately then you will get the SAME cropping positions for both training and validation data, is that right?\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35090\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35090\">No</a>\n", "Yes. And I confirm that `set_random_seed` will cause an RNG in `tf.function` to start from the same global seed (because of https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/eager/context.py#L427 and https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/framework/func_graph.py#L229). Possible solutions for your code are removing `set_random_seed` call or calling it before each `Dataset.map` with different seeds."]}, {"number": 35089, "title": "no attribute 'op' in keras.losses.SparseCategoricalCrossentropy", "body": "**System information**\r\n- OS Platform and Distribution: Jupyter on Windows 10\r\n- TensorFlow installed from: anaconda in venv\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nI'm running the example from the [tensorflow documentation for keras.losses.SparseCategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) and get the error:\r\n`'list' object has no attribute 'op'`\r\nIf we set `from_logits` to `True` it is working.\r\n\r\n**Code to reproduce the issue**\r\n```\r\ncce = tf.keras.losses.SparseCategoricalCrossentropy()\r\nloss = cce( [0, 1, 2], [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\r\n```", "comments": ["@temminks This has been fixed in the latest nightly version. Please find my gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/df198dee880d87c1813129452345ce5a/untitled260.ipynb). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35089\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35089\">No</a>\n"]}, {"number": 35088, "title": "K.learning_phase() flag does not work in Training  ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\nYes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows, Anaconda\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n\r\n- TensorFlow version (use command below):\r\n2.0 and keras 2.3.1\r\n\r\n- Python version:\r\n3.7\r\n\r\n- GPU model and memory:\r\nNo GPU\r\n\r\n**Describe the current behavior**\r\nI have written a class that will load a tf.keras model, get its config and create a new model with this configuration then using this newly init model, make predictions using dropout to create confidence intervals around the output. \r\n\r\nThe function works around the prediction element i.e. the newly initialised model makes a prediction, but the uncertainty element outputs an array of 0's, suggesting that flag is not being passed to the dropout layers during training phase.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect an np.array output containing the predictions (n_samples) and the uncertainty (n_samples)\r\n\r\n\r\n**Code to reproduce the issue**\r\n```\r\n\r\n# Create training/test datasets\r\ntrain_data = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\r\ntest_data = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\r\nforecast_data = tf.data.Dataset.from_tensor_slices((forecast_ahead_df.values))\r\n\r\n# batching\r\ntrain_batch = train_data.batch(len(X_train)) \r\ntest_batch = test_data.batch(len(X_test))\r\nforecast_batch = forecast_data.batch(len(forecast_ahead_df))\r\n\r\n\r\ndef predict_with_confidence_ints(self, \r\n                                   X: np.array,\r\n                                   dropout_rate: float, \r\n                                   n_classes: int,\r\n                                   n_iter: int):\r\n    \"\"\"Load saved .h5 keras model and generate predictions using dropout.\r\n    Args: TODO\r\n    Returns: TODO\r\n    \"\"\"\r\n\r\n    # Load saved Keras model (.h5)\r\n    model = tf.keras.models.load_model(self.saved_keras_model,\r\n                                       custom_objects = self.custom_objects) \r\n\r\n    # Load the config of the original model\r\n    conf = model.get_config()\r\n    \r\n    # Add the specified dropout to all layers\r\n    for layer in conf['layers']:\r\n      # Dropout layers\r\n      if layer[\"class_name\"]==\"Dropout\":\r\n        layer[\"config\"][\"rate\"] = dropout_rate\r\n      # Recurrent layers with Dropout\r\n      elif \"dropout\" in layer[\"config\"].keys():\r\n        layer[\"config\"][\"dropout\"] = dropout_rate\r\n\r\n    # # Create a new model with specified dropout\r\n    if type(model) == tf.keras.Sequential:\r\n      # Sequential\r\n      model_with_dropout = tf.keras.Sequential.from_config(conf)\r\n    else:\r\n      # Functional\r\n      model_with_dropout = tf.keras.models.Model.from_config(conf)\r\n \r\n    # Get config weights and init new model with same weights\r\n    model_with_dropout.set_weights(model.get_weights())  \r\n    \r\n    # Create function that applies dropout to learning phase\r\n    f = K.function([model_with_dropout.layers[0].input, K.learning_phase()],\r\n               [model_with_dropout.layers[-1].output])\r\n\r\n    result = np.zeros((n_iter,) + (x.shape[0], 1))\r\n\r\n    for i in range(n_iter):\r\n        result[i, :] = f((x, 1))[0]\r\n    \r\n    predictions_ = result.mean(axis=0)\r\n    uncertainty_ = result.std(axis=0)\r\n    # predictions = pd.Series(predictions_.reshape(-1), name = 'Predictions')\r\n    # std_devs = pd.Series(uncertainty_.reshape(-1), name = 'Uncertainty')\r\n    return predictions_, uncertainty_ \r\n\r\n# Create predictions with dropout\r\n\r\n# Get custom objects\r\ncustom_objects={'root_mean_squared_error': root_mean_squared_error,\r\n                'glorot_uniform': keras.initializers.glorot_uniform(seed=None)}\r\n\r\n# Get saved model\r\nmodel_path = 'my_saved_keras_model.h5'\r\n\r\n# Set dropout rate\r\ndropout = 0.3\r\n\r\n# Convert input data to np array\r\nX = np.array(forecast_ahead_df)\r\n\r\n# Get predictions \r\npredictions_new, uncertainties_new = predict_with_confidence_ints(X, dropout, 1, 100)\r\n\r\n```\r\n\r\n**Other info / logs**\r\nThis function was created within a class however I have omitted the __init__ etc for brevity's sake.\r\n\r\nThank you!\r\n\r\n", "comments": ["Hey hi, Ensure that all the keras used is tf.keras and not keras. One catch I got was to change: in custom, you have used keras.initializers.glorot_uniform, just make it tf.keras and see if there is any problem. Actually I have previously seen this error. Just ping me if there are more errors.", "Hey,\r\n\r\nI changed the initializer to tf.keras and whilst the class and associated funcs still ran fine, it is still returning an empty array (0's) for the uncertainty. I did notice that I import the backend as `import keras.backend as K`, however with I try to import this as `tf.keras.backend` I get \r\n\r\n`ModuleNotFoundError:` No module named 'tf'`\r\n\r\nSo I have simply replaced the backend import with  `import tensorflow.keras.backend`. Sadly I am now getting:\r\n\r\n`AttributeError:` 'int' object has no attribute 'op'`\r\n\r\nNow I am really lost! Thank you for your help here!\r\n", "@cmp1 ,\r\nWhen tried executing the given code NameError: name 'X_train' is not defined was faced, find this [gist](https://colab.sandbox.google.com/gist/oanush/07befe5f2cc641e946beecb4911d9e42/35088.ipynb) for your reference. Can you please provide complete code to reproduce the error reported ?Thanks!", "Ah yes, apologies. \r\n\r\nI have attached the dataframe and the train/test splits are defined below:\r\n\r\n```\r\n# Define train/test\r\nX_train = final_df[(final_df.index > '2017-05-01') & (final_df.index < '2019-03-01')]\r\nX_test = final_df[(final_df.index >= '2019-03-01')]\r\n\r\n# Separate label\r\ny_train = X_train.pop('platts_alumina_1-2.5%')\r\ny_test = X_test.pop('platts_alumina_1-2.5%')\r\n```\r\n\r\nAnd model def and configuration:\r\n\r\n```\r\n\r\n# Loss Function, RMSE\r\ndef root_mean_squared_error(y_true, y_pred):\r\n        return K.sqrt(K.mean(K.square(y_pred - y_true)))\r\n\r\n# Optimizer\r\nopt = Adam(learning_rate = 0.003)\r\n\r\n# Stopping callback\r\nearly_stop = EarlyStopping(monitor = 'val_loss', \r\n                           mode = 'min', \r\n                           verbose = 1, \r\n                           patience = 20)\r\n# Model\r\ndef mlp_model():\r\n  model = tf.keras.Sequential([   \r\n    tf.keras.layers.Dense(3, input_dim = 3,\r\n                          kernel_initializer = 'glorot_uniform',\r\n                          activation = 'elu'),\r\n    tf.keras.layers.Dropout(0.2),                      \r\n    tf.keras.layers.Dense(160, \r\n                          activation = 'elu', \r\n                          kernel_regularizer = regularizers.l2(0.001)),                     \r\n    tf.keras.layers.GaussianNoise(0.3),  \r\n    tf.keras.layers.Dense(160, \r\n                          activation = 'elu',\r\n                          kernel_regularizer = regularizers.l2(0.003)),\r\n    tf.keras.layers.Dropout(0.2),\r\n    tf.keras.layers.Dense(160, \r\n                          activation = 'elu',\r\n                          kernel_regularizer = regularizers.l2(0.003)),                                               \r\n    tf.keras.layers.Dense(160, \r\n                          activation = 'relu',\r\n                          kernel_regularizer = regularizers.l2(0.004)), #128 for unscaled\r\n    tf.keras.layers.Dense(1) \r\n  ])\r\n\r\n  model.compile(optimizer = opt, \r\n                loss = root_mean_squared_error,\r\n                metrics=['mean_squared_error', \r\n                         'mean_absolute_error',\r\n                         root_mean_squared_error])\r\n  return model\r\n\r\nmodel = mlp_model()\r\nmodel_history = model.fit(train_batch, \r\n                          validation_data = test_batch,\r\n                          epochs = 100,\r\n                          shuffle = False,\r\n                          callbacks = [early_stop])\r\n```\r\n\r\n\r\n[dataframe.xlsx](https://github.com/tensorflow/tensorflow/files/3967806/dataframe.xlsx)\r\n\r\n\r\nThank you for your help!\r\n", "@cmp1 The dataset that you provided and the code mentioned in the above comment is not in sink. Look at my gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/906d74be1db6d76fcfb5dfdf65f4b5f2/untitled269.ipynb). Please reproduce the issue in colab and share a github gist. Thanks!\r\n\r\nMore over I am not running into any error while importing learning_phase as given below\r\n\r\n`from tensorflow.keras.backend import learning_phase`", "@cmp1 Please let me know if the above comment helped you to resolve the issue. Thanks!", "Closing this issue as it has been inactive for more than 3 weeks. Please add additional comments and we can open this issue again. Thanks!"]}, {"number": 35087, "title": "optimize_for_inference not working for my graph?", "body": "It does not seem to make a difference if I use optimize_for_inference when I generate the pb and pbtxt graphs for my network. The size of the unoptimized graph is 177.775kb and the size for the optimized graph is 177.739kb.\r\n\r\nI'm not sure wheter the optimize_for_inference Tool is correctly working for my graph. I currently cannot deploy my model in openCV, which throws an exception [EX] because the graph does not seem to be optimized. \r\n\r\n**I want to access my tensorflow protobuf files with the following openCV function:**\r\nNet ub6net = readNetFromTensorflow (model, config) // PB and PBTXT\r\n\r\n**This throws the following Exception:**\r\nVersion 1 - optimize_for_inference disabled: **[EX]** Assertion failed in function 'addConstNodes'\r\nVersion 2 - optimize_for_inference enabled: **[EX]** Assertion failed in function 'addConstNodes'\r\n\r\n___________________________________\r\n\r\nThis exception gets thrown when the input files are not optimized for inference and it is thrown regardless of the fact that the graph has been optimized or not, indicating that no optimization was applied.\r\n\r\n**I would be glad if someone can help me to solve this issue, as it seems to be related to the optimize_for_inference Tool.**\r\n\r\n**My Code to generate the pb and pbtxt file:**\r\n\r\n```\r\n# -*- coding: utf-8 -*-\r\n# File: export.py\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import graph_util\r\nfrom tensorflow.python.platform import gfile\r\nfrom tensorflow.python.tools import optimize_for_inference_lib\r\n\r\nfrom ..compat import is_tfv2, tfv1\r\nfrom ..input_source import PlaceholderInput\r\nfrom ..tfutils.common import get_tensors_by_names, get_tf_version_tuple\r\nfrom ..tfutils.tower import PredictTowerContext\r\nfrom ..utils import logger\r\n\r\n__all__ = ['ModelExporter']\r\n\r\n\r\nclass ModelExporter(object):\r\n    \"\"\"Export models for inference.\"\"\"\r\n\r\n    def __init__(self, config):\r\n        \"\"\"Initialise the export process.\r\n        Args:\r\n            config (PredictConfig): the config to use.\r\n                The graph will be built with the tower function defined by this `PredictConfig`.\r\n                Then the input / output names will be used to export models for inference.\r\n        \"\"\"\r\n        super(ModelExporter, self).__init__()\r\n        self.config = config\r\n\r\n    def export_compact(self, filename, optimize=True, toco_compatible=False):\r\n        \"\"\"Create a self-contained inference-only graph and write final graph (in pb format) to disk.\r\n        Args:\r\n            filename (str): path to the output graph\r\n            optimize (bool): whether to use TensorFlow's `optimize_for_inference`\r\n                to prune and optimize the graph. This does not work on all types of graphs.\r\n            toco_compatible (bool): See TensorFlow's\r\n                `optimize_for_inference\r\n                <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py>`_\r\n                for details. Only available after TF 1.8.\r\n        \"\"\"\r\n        if toco_compatible:\r\n            assert optimize, \"toco_compatible is only effective when optimize=True!\"\r\n        self.graph = self.config._maybe_create_graph()\r\n        with self.graph.as_default():\r\n            input = PlaceholderInput()\r\n            input.setup(self.config.input_signature)\r\n            with PredictTowerContext(''):\r\n                self.config.tower_func(*input.get_input_tensors())\r\n\r\n            input_tensors = get_tensors_by_names(self.config.input_names)\r\n            output_tensors = get_tensors_by_names(self.config.output_names)\r\n\r\n            self.config.session_init._setup_graph()\r\n            # we cannot use \"self.config.session_creator.create_session()\" here since it finalizes the graph\r\n            sess = tfv1.Session(config=tfv1.ConfigProto(allow_soft_placement=True))\r\n            self.config.session_init._run_init(sess)\r\n\r\n            dtypes = [n.dtype for n in input_tensors]\r\n\r\n            # freeze variables to constants\r\n            frozen_graph_def = graph_util.convert_variables_to_constants(\r\n                sess,\r\n                self.graph.as_graph_def(),\r\n                [n.name[:-2] for n in output_tensors],\r\n                variable_names_whitelist=None,\r\n                variable_names_blacklist=None)\r\n\r\n            # prune unused nodes from graph\r\n            if optimize:\r\n                toco_args = () if get_tf_version_tuple() < (1, 8) else (toco_compatible, )\r\n                frozen_graph_def = optimize_for_inference_lib.optimize_for_inference(\r\n                    frozen_graph_def,\r\n                    [n.name[:-2] for n in input_tensors],\r\n                    [n.name[:-2] for n in output_tensors],\r\n                    [dtype.as_datatype_enum for dtype in dtypes],\r\n                    *toco_args)\r\n\r\n            tf.train.write_graph(frozen_graph_def, filename, 'ub6_model.pb', as_text=False)\r\n            logger.info(\"Output graph written to {}.\".format(filename))\r\n            tf.train.write_graph(frozen_graph_def, filename, 'ub6_config.pbtxt', as_text=True)\r\n            logger.info(\"Output graph written to {}.\".format(filename))\r\n```", "comments": ["My **unfrozen and unoptimized** graph (unfrozen.pb) only has a file-size of 1Mb and as soon as I freeze it (frozen.pb), it jumps to 173mb and only loses a few kb after optimization (frozenOpt.pb). The file size of the frozen.pb (173mb) and frozenOpt.pb (173mb) is the same, regardless if I use the tensorflow standalone script tools or my function call (see first comment).\r\n\r\nThe standalone script tools where: **freeze_graph.py** and **optimize_for_inference.py**.\r\n\r\n**freeze_graph shell command:**\r\n> python freeze_graph.py --input_graph=C:\\Users\\d.omran\\Desktop\\Model\\unfrozen\\ub6_unfrozen.pb \\ --input_checkpoint=C:\\Users\\d.omran\\Desktop\\Model\\unfrozen\\model-95808 \\ --input_binary=true --output_graph=C:\\Users\\d.omran\\Desktop\\Model\\unfrozen\\frozen.pb \\ --output_node_names=output/boxes,output/scores,output/labels,output/masks\r\n\r\n**optimize_for_inference shell command:**\r\n> python optimize_for_inference.py --input=C:\\Users\\d.omran\\Desktop\\Model\\unfrozen\\frozen.pb \\ --output=C:\\Users\\d.omran\\Desktop\\Model\\unfrozen\\frozenOpt.pb \\ --frozen_graph=True \\ --input_names=\"image\" \\ --output_names=\"output/boxes\",\"output/scores\",\"output/labels\",\"output/masks\"\r\n\r\n_________________________________________________\r\n\r\n**standalone script-based files:**\r\nunfrozen.pb\r\nfrozen.pb\r\nfrozenOpt.pb\r\n\r\n**funciton-based files:**\r\nfunc_unfrozen.pb\r\nfunc_frozen.pb.\r\nfunc_frozenOpt.pb\r\nfunc_config_frozen.pbtxt\r\nfunc_config_unfrozen_pbtxt", "@Razor1O9, Please provide the Tensorflow version that your using. Thanks!", "**Thanks for the response.**\r\n\r\nTensorflow 1.9\r\nOpenCV 4.1.1", "I've updated to Tensorflow 1.14 with the same result. As soon as I try to read the Model in OpenCV 4.1.1 I either get the **nodesMapIt != nodesMap.end() in function sortByExecutionOrder** exception (when I only use the pb file) or **Assertion failed in** **function 'addConstNodes'** exception (when I use the pb and pbtext file).\r\n\r\nHowever, I can predict the model without any issues when I use the training-checkpoint for prediction trough Tensorfow.\r\n\r\n**Step 1; Generate unfrozen graph (1mb in size)**\r\n\r\n**Step 2; Freeze ist with freeze_graph.py (173mb in size)**\r\nIt froze 239 variables and converted them to const ops. \r\nThe file size increased significantly \r\n\r\n**Step 3; Optimize graph with optimize_for_inference.py (173mb in size)**\r\nThe tool printed the following instructions for updating:\r\ngraph_util.extract_sub_graph\r\ngraph_util.remove_training_nodes\r\n\r\nSuprisingly, the file size is only a few hundreds kilobyte smaller between step 2 and step 3.\r\n\r\n", "**Closing this issue, as it has been solved:**\r\nhttps://github.com/opencv/opencv/issues/16155"]}, {"number": 35086, "title": "Added usage example on tf.image.rgb_to_yiq", "body": "", "comments": ["hi, i've accidentally made commit with a message \"Updated README.md\" sorry about that :)", "It looks like the Ubuntu CPU, windows bazel, and windows bazel gpu checks have failed. What does that mean?", "82a21381d5b8035735ea2010727a728541d3c47f this should resolve the Ubuntu cpu failure", "@alextp hi, I've made some changes to the example so that the ubuntu CPU test should work ", "It seems the Ubuntu CPU test failed again, I've converted the output with '...' so that it won't match with the expected output. Was something wrong with the syntax?", "@alextp hi, I've resolved a conflict where there's a usage example of tf.image.rgb_to_grayscale recently merged. So I'm only adding the tf.image.rgb_to_yiq example."]}, {"number": 35085, "title": "[DOCS] Using `>>>` in documentation is incorrectly formatted on website", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/constant_initializer\r\n\r\n## Description of issue (what needs changing):\r\nThe [example code](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/ops/init_ops_v2.py#L152) uses `>>>`, but on the website, this is incorrectly converted to `&gt;&gt;&gt;`\r\n\r\nTF docs link: https://www.tensorflow.org/api_docs/python/tf/constant_initializer\r\n\r\n### Submit a pull request?\r\n\r\nThis is more of a problem with how documentation is generated from comments in the python file. I don't mind taking a look if someone can point out where to get started. ", "comments": ["This looks fixed in nightly: https://www.tensorflow.org/api_docs/python/tf/constant_initializer?version=nightly"]}, {"number": 35084, "title": "Memory leak when using py_function inside tf.data.Dataset", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n<img width=\"368\" alt=\"\u5c4f\u5e55\u5feb\u7167 2019-12-13 \u4e0b\u53486 06 20\" src=\"https://user-images.githubusercontent.com/22017000/70792077-6c453000-1dd3-11ea-8489-5b6131950515.png\">\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nThe tf.data.Dataset instance should be freed in every step.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\nimport psutil\r\n\r\ndef _generator():\r\n    for i in range(100):\r\n        yield \"1,2,3,4,5,6,7,8\"\r\n\r\ndef _py_parse_data(record):\r\n    record = record.numpy()\r\n    record = bytes.decode(record)\r\n    rl = record.split(\",\")\r\n    rl = [str(int(r) + 1) for r in rl]\r\n    return [\",\".join(rl)]\r\n\r\ndef parse_data(record, shape=10):\r\n    sparse_data = tf.strings.split([record], sep=\",\")\r\n    sparse_data = tf.strings.to_number(sparse_data[0], tf.int64)\r\n    ids_num = tf.cast(tf.size(sparse_data), tf.int64)\r\n    indices = tf.range(0, ids_num, dtype=tf.int64)\r\n    indices = tf.reshape(indices, shape=(-1, 1))\r\n    sparse_data = tf.sparse.SparseTensor(\r\n                indices, sparse_data, dense_shape=(shape,)\r\n    )\r\n    return sparse_data\r\n\r\nprocess = psutil.Process(os.getpid())\r\n\r\nstep = 0\r\nwhile (step < 10000):\r\n    t = tf.data.Dataset.from_generator(_generator, output_types=tf.string)\r\n    t = t.map(lambda record: tf.py_function(_py_parse_data, [record], [tf.string]))\r\n    t = t.map(parse_data)\r\n    for d in t:\r\n        a = 1\r\n    if step % 10 == 0:\r\n        print(\"Memory : \", process.memory_info().rss)\r\n    step += 1\r\n```\r\n\r\n", "comments": ["@QiJune \r\n\r\nI have tried on colab with TF version 2.0 ,2.1.0-rc1 . Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/df1ca1840cabeb4463bbf1a58199d7b2/untitled485.ipynb). Is this the expected behavior?", "@ravikyram Yes, exactly, the memory is increasing quickly. \r\nI am not sure why the `tf.data.Dataset` objects are not freed.", "@kkimdev this seems related to your work on properly garbage collecting traced functions that go out of scope ... could you please take a look? thank you", "List of leaking objects per 100 iterations:\r\n```\r\n=======================================================================\r\nType                     Old_ids  Current_ids      New_ids Count_Deltas\r\n=======================================================================\r\ndict                       49562        50462         +904         +900\r\ncell                       19873        20673         +800         +800\r\ntuple                      39958        40658         +700         +700\r\nfunction                   71183        71783         +600         +600\r\nlist                       27319        27819         +502         +500\r\nKeyedRef                    3800         4200         +400         +400\r\nEagerTensor                 1900         2100         +200         +200\r\nmethod                      1413         1513         +100         +100\r\n_GeneratorState              950         1050         +100         +100\r\nTensorShape                  953         1053         +100         +100\r\nTape                         950         1050         +100         +100\r\nGradientTape                 950         1050         +100         +100\r\nEagerFunc                    950         1050         +100         +100\r\nStringIO                       3            3           +1           +0\r\nwrapper_descriptor          3782         3782           +0           +0\r\nweekday                       14           14           +0           +0\r\nweakref                    13226        13226           +0           +0\r\nweakcallableproxy              1            1           +0           +0\r\nvectorize                      4            4           +0           +0\r\nvalidate_nseq_int              1            1           +0           +0\r\nvalidate_nseq_float            5            5           +0           +0\r\nuname_result                   1            1           +0           +0\r\ntzutc                          1            1           +0           +0\r\ntzUTC                          1            1           +0           +0\r\ntype                        6277         6277           +0           +0\r\nstaticmethod                1212         1212           +0           +0\r\nslice                         72           72           +0           +0\r\nset                         6633         6633           +0           +0\r\nscputimes                    113          113           +0           +0\r\npybind11_type                 49           49           +0           +0\r\n=======================================================================\r\n```\r\nLeaking `EagerFunc` reference graph\r\n![image](https://user-images.githubusercontent.com/503414/71774612-158ccb00-2f27-11ea-85ea-292b1b1b6fc9.png)\r\n\r\nSo seems like the problem is py_function getting created every loop and `_py_funcs_used_in_graph` keeps growing.\r\n", "Thanks Kibeom. So the problem is that `_py_funcs_used_in_graph` is not garbage collected?", "Actually, there shouldn't be a global graph at the first place since this is eager mode.  With https://github.com/tensorflow/tensorflow/commit/3b74a63c0f7e1ec4618563958079f538cd9de076 , it should be correctly attached to a func graph's `_py_funcs_used_in_graph` and it will be gone when the func graph is garbage collected.\r\n\r\nThough still there are two more causes for this leak, `tape_cache` and `ag_dnc_wrapper__`.  We haven't found a good solution for these references yet.\r\n\r\n@mdanatg fyi", "@kkimdev @jsimsa hello, we are having the same problem with TF 1.14.\r\nWe use `tf.py_function` to load a wave file:\r\n\r\n```python\r\nresults = tf.py_function(\r\n                self.safe_load,\r\n                [audio_descriptor, offset, duration, sample_rate, dtype],\r\n                (tf.float32, tf.bool)),\r\n            waveform, error = results[0]\r\n```\r\n\r\nputting this into a `tf.Dataset`:\r\n\r\n```python\r\ndataset = dataset.map(\r\n        lambda sample: dict(\r\n            sample,\r\n            **audio_adapter.load_tf_waveform(\r\n                sample['audio_id'],\r\n                session=session,\r\n                sample_rate=sample_rate,\r\n                offset=sample['start'],\r\n                duration=sample['end'] - sample['start'])),\r\n        num_parallel_calls=2)\r\n```\r\n\r\nand getting a leak, where the memory leaked is the size of the wave file being loaded:\r\n\r\n```python\r\nafter prediction traced memory: 28670 KiB  peak: 28673 KiB  overhead: 29677 KiB\r\nafter load traced memory: 28801 KiB  peak: 28808 KiB  overhead: 29755 KiB\r\nafter prediction traced memory: 53988 KiB  peak: 55396 KiB  overhead: 54529 KiB\r\nafter load traced memory: 54100 KiB  peak: 55396 KiB  overhead: 54604 KiB\r\n```", "@loretoparisi do you also create the dataset in a for loop or do you instantiate it only once ?\r\n\r\nI am asking because I suspect a memory leak as well, but I am only creating one dataset object and then training on it using fit. \r\nOn my side, I use `tf.py_function` to load HDF5 files because of [this error in tfio](https://github.com/tensorflow/io/issues/841).\r\n\r\nI would also be interested in the script you used to get the last lines of your post.", "@zaccharieramzi yes sure. The source file, that you can even try yourself in the project is here: https://github.com/deezer/spleeter/blob/master/spleeter/dataset.py", "@loretoparisi I am sorry but I am not sure I understand how to use the file you linked to generate the memory leak evidence.\r\n\r\nI also don't see how it answers the question of whether you create datasets in a loop (i.e. call to `tf.data.Dataset` in a loop).", "> @loretoparisi I am sorry but I am not sure I understand how to use the file you linked to generate the memory leak evidence.\r\n> \r\n> I also don't see how it answers the question of whether you create datasets in a loop (i.e. call to `tf.data.Dataset` in a loop).\r\n\r\nThe memory leak comes out when you run the framework over more samples, but yes it's complex to test it since it's a specific tool (audio separation).", "Ok gotcha, but so you don't instantiate the dataset in a loop?", "@QiJune,\r\nIs this still an issue? On running the code with TF v2.2, I did not observe much difference between each iteration. \r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/5ec965d85a8660b18f0a3fd78860a141/35084.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 35083, "title": "Error when setting up virtual devices on system that has multiple physical GPUs", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: 1.15.0, eager mode\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0.130, 7.6.0.64\r\n- GPU model and memory: V100\r\n\r\n**Describe the current behavior**\r\n\r\n```set_virtual_device_configuration``` throws an exception with multiple physical GPUs in eager mode and ```allow growth```.\r\n\r\n**Describe the expected behavior**\r\n\r\nShould be able to create virtual devices for multiple physical GPUs.\r\n\r\n**Code to reproduce the issue**\r\n\r\n``` python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution() #required for error\r\ntf.debugging.set_log_device_placement(True)\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True #required for error\r\ntf.Session(config=config)\r\n\r\n# require multiple physical gpus\r\n\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\n\r\ntf.config.experimental.set_virtual_device_configuration(\r\n  physical_devices[0],\r\n  [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=100),\r\n   tf.config.experimental.VirtualDeviceConfiguration(memory_limit=100)])\r\n\r\ntf.config.experimental.set_virtual_device_configuration(\r\n  physical_devices[1],\r\n  [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=100),\r\n   tf.config.experimental.VirtualDeviceConfiguration(memory_limit=100)])\r\n\r\nlogicals = tf.config.experimental.list_logical_devices('GPU')\r\n\r\nprint(logicals)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAlreadyExistsError                        Traceback (most recent call last)\r\n<ipython-input-1-23cc43a56abd> in <module>\r\n     21    tf.config.experimental.VirtualDeviceConfiguration(memory_limit=100)])\r\n     22 \r\n---> 23 logicals = tf.config.experimental.list_logical_devices('GPU')\r\n     24 \r\n     25 print(logicals)\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/config.py in list_logical_devices(device_type)\r\n    345     List of LogicalDevice objects\r\n    346   \"\"\"\r\n--> 347   return context.context().list_logical_devices(device_type=device_type)\r\n    348 \r\n    349 \r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py in list_logical_devices(self, device_type)\r\n   1140   def list_logical_devices(self, device_type=None):\r\n   1141     \"\"\"Return logical devices.\"\"\"\r\n-> 1142     self.ensure_initialized()\r\n   1143 \r\n   1144     devices = []\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py in ensure_initialized(self)\r\n    490         if self._default_is_async == ASYNC:\r\n    491           pywrap_tensorflow.TFE_ContextOptionsSetAsync(opts, True)\r\n--> 492         context_handle = pywrap_tensorflow.TFE_NewContext(opts)\r\n    493       finally:\r\n    494         pywrap_tensorflow.TFE_DeleteContextOptions(opts)\r\n\r\nAlreadyExistsError: TensorFlow device (GPU:1) is being mapped to multiple CUDA devices (0 now, and 1 previously), which is not supported. This may be the result of providing different GPU configurations (ConfigProto.gpu_options, for example different visible_device_list) when creating multiple Sessions in the same process. This is not  currently supported, see https://github.com/tensorflow/tensorflow/issues/19083\r\n```\r\n", "comments": ["I just checked and this bug also exists in TF 2.0.", "@mixxen Did you take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/19083). Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@gowthamkpr Looked at that issue 19083, but unsure if it's related.\r\n\r\nIn my example, I'm trying to create 4 logical GPU devices with 2 physical GPU devices.\r\n\r\nI'm able to create N logical devices if there is only 1 physical GPU...so it would seem I should be able to create N logical devices on a system with M physical GPUs. ", "I'm thinking there is a device name conflict when using `set_virtual_device_configuration` on a system with multiple GPUs. I'm able to create virtual devices for the last listed GPU only. For instance, if a system has 4 GPUs, I am able to create virtual GPUs on `physical_devices[3]`. Creating virtual devices on GPU 0,1, and 2 will result in the exception above.", "I think I figured it out. Need to configure **all** GPUs the same before invoking `set_virtual_device_configuration`. `config.gpu_options` seems to only configure the 1st GPU.\r\n\r\nThe following code works on a 4 GPU system:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.enable_eager_execution()\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\n\r\nfor gpu in physical_devices:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\nfor gpu in physical_devices:\r\n    tf.config.experimental.set_virtual_device_configuration(\r\n      gpu,\r\n      [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=100),\r\n       tf.config.experimental.VirtualDeviceConfiguration(memory_limit=100)])\r\n\r\nlogicals = tf.config.experimental.list_logical_devices('GPU')\r\n\r\nprint(logicals)\r\n```\r\n\r\nPrints this:\r\n```\r\n[LogicalDevice(name='/job:localhost/replica:0/task:0/device:GPU:0', device_type='GPU'), LogicalDevice(name='/job:localhost/replica:0/task:0/device:GPU:1', device_type='GPU'), LogicalDevice(name='/job:localhost/replica:0/task:0/device:GPU:2', device_type='GPU'), LogicalDevice(name='/job:localhost/replica:0/task:0/device:GPU:3', device_type='GPU'), LogicalDevice(name='/job:localhost/replica:0/task:0/device:GPU:4', device_type='GPU'), LogicalDevice(name='/job:localhost/replica:0/task:0/device:GPU:5', device_type='GPU'), LogicalDevice(name='/job:localhost/replica:0/task:0/device:GPU:6', device_type='GPU'), LogicalDevice(name='/job:localhost/replica:0/task:0/device:GPU:7', device_type='GPU')]\r\n```\r\n\r\nI'll do a bit more testing and report back.\r\n", "Sure @mixxen glad that it worked. ", "I am closing this issue as it has been resolved. Please add additional comments and we can open this issue again.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35083\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35083\">No</a>\n"]}, {"number": 35082, "title": " Add survey to README.md", "body": "", "comments": []}, {"number": 35081, "title": "Tensorflow Lite Data Type Error on Android.", "body": "I have converted a super image resolution tensorflow model to tensorflow lite and have deployed it on an android application. Once I am trying to carry out Inference I am getting errors related to the data type. Please have a look at the code snippet below and the error log.\r\n\r\n```\r\n       try {\r\n            tflite = new Interpreter(loadModelFile(MainActivity.this, \"model.tflite\"));\r\n            Bitmap testImage = getBitmapFromAsset(this,\"Image.png\");\r\n            TensorBuffer test_tensor = TensorBuffer.createFixedSize(new int[]{96, 96, 3}, DataType.UINT8);\r\n            Bitmap out =Bitmap.createBitmap(384,\r\n                         384, Bitmap.Config.ARGB_8888);\r\n            TensorBuffer output_tensor = TensorBuffer.createFixedSize(new int[]{384, 384, 3}, DataType.UINT8);\r\n            tflite.run(test_tensor,output_tensor);\r\n            convertTensorBufferToBitmap(output_tensor,out);\r\n        } catch (IOException e) {\r\n            Toast.makeText(this,\"Error\",Toast.LENGTH_LONG).show();\r\n            e.printStackTrace();\r\n        }\r\n\r\n    static void convertTensorBufferToBitmap(TensorBuffer buffer, Bitmap bitmap) {\r\n        if (buffer.getDataType() != DataType.UINT8) {\r\n            // We will add support to FLOAT format conversion in the future, as it may need other configs.\r\n            throw new UnsupportedOperationException(String.format(\r\n                    \"Converting TensorBuffer of type %s to Bitmap is not supported yet.\",\r\n                    buffer.getDataType()));\r\n        }\r\n        int[] shape = buffer.getShape();\r\n        if (shape.length != 3 || shape[0] <= 0 || shape[1] <= 0 || shape[2] != 3) {\r\n            throw new IllegalArgumentException(String.format(\r\n                    \"Buffer shape %s is not valid. 3D TensorBuffer with shape [w, h, 3] is required\",\r\n                    Arrays.toString(shape)));\r\n        }\r\n        int h = shape[0];\r\n        int w = shape[1];\r\n        if (bitmap.getWidth() != w || bitmap.getHeight() != h) {\r\n            throw new IllegalArgumentException(String.format(\r\n                    \"Given bitmap has different width or height %s with the expected ones %s.\",\r\n                    Arrays.toString(new int[]{bitmap.getWidth(), bitmap.getHeight()}),\r\n                    Arrays.toString(new int[]{w, h})));\r\n        }\r\n        if (!bitmap.isMutable()) {\r\n            throw new IllegalArgumentException(\"Given bitmap is not mutable\");\r\n        }\r\n        // TODO(b/138904567): Find a way to avoid creating multiple intermediate buffers every time.\r\n        int[] intValues = new int[w * h];\r\n        int[] rgbValues = buffer.getIntArray();\r\n        for (int i = 0, j = 0; i < intValues.length; i++) {\r\n            byte r = (byte) rgbValues[j++];\r\n            byte g = (byte) rgbValues[j++];\r\n            byte b = (byte) rgbValues[j++];\r\n            intValues[i] = ((r << 16) | (g << 8) | b);\r\n        }\r\n        bitmap.setPixels(intValues, 0, w, 0, 0, w, h);\r\n    }\r\n\r\n    static void convertBitmapToTensorBuffer(Bitmap bitmap, TensorBuffer buffer) {\r\n        int w = bitmap.getWidth();\r\n        int h = bitmap.getHeight();\r\n        int[] intValues = new int[w * h];\r\n        bitmap.getPixels(intValues, 0, w, 0, 0, w, h);\r\n        // TODO(b/138904567): Find a way to avoid creating multiple intermediate buffers every time.\r\n        int[] rgbValues = new int[w * h * 3];\r\n        for (int i = 0, j = 0; i < intValues.length; i++) {\r\n            rgbValues[j++] = ((intValues[i] >> 16) & 0xFF);\r\n            rgbValues[j++] = ((intValues[i] >> 8) & 0xFF);\r\n            rgbValues[j++] = (intValues[i] & 0xFF);\r\n        }\r\n        int[] shape = new int[] {h, w, 3};\r\n        buffer.loadArray(rgbValues, shape);\r\n    }\r\n\r\n```\r\n\r\nThis is a snippet of my code and I still am getting an error:\r\n```\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.edsr, PID: 14650\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.edsr/com.example.edsr.MainActivity}: java.lang.IllegalArgumentException: DataType error: cannot resolve DataType of org.tensorflow.lite.support.tensorbuffer.TensorBufferUint8\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2817)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2892)\r\n        at android.app.ActivityThread.-wrap11(Unknown Source:0)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1593)\r\n        at android.os.Handler.dispatchMessage(Handler.java:105)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6541)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)\r\n     Caused by: java.lang.IllegalArgumentException: DataType error: cannot resolve DataType of org.tensorflow.lite.support.tensorbuffer.TensorBufferUint8\r\n        at org.tensorflow.lite.Tensor.dataTypeOf(Tensor.java:255)\r\n        at org.tensorflow.lite.Tensor.throwIfTypeIsIncompatible(Tensor.java:313)\r\n        at org.tensorflow.lite.Tensor.getInputShapeIfDifferent(Tensor.java:218)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:135)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:296)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:259)\r\n        at com.example.edsr.MainActivity.onCreate(MainActivity.java:97)\r\n        at android.app.Activity.performCreate(Activity.java:6975)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1213)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2770)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2892) \r\n        at android.app.ActivityThread.-wrap11(Unknown Source:0) \r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1593) \r\n        at android.os.Handler.dispatchMessage(Handler.java:105) \r\n        at android.os.Looper.loop(Looper.java:164) \r\n        at android.app.ActivityThread.main(ActivityThread.java:6541) \r\n        at java.lang.reflect.Method.invoke(Native Method) \r\n        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240) \r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767) \r\nProcess 14650 terminated.\r\n```", "comments": ["Over to @lu-wang-g for further work.", "@nauyan, thanks for raising up the issue. The issue is here that Interpreter does not consume TensorBuffer, but ButeBuffer and primitive arrays. See the [Java doc for Interpreter.run](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java#L227-L254). \r\n\r\nWhat you can do is to get the ByteBuffer from TensorBuffer through TensorBuffer.getBuffer(), and then pass the ByteBuffer to tflite.run(). Hope it helps.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35081\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35081\">No</a>\n", "@lu-wang-g Yeah, then you end up with:\r\n\r\n```\r\n Cannot convert between a TensorFlowLite tensor with type UINT8 and a Java object of type [[I (which is compatible with the TensorFlowLite type INT32).\r\n```"]}, {"number": 35079, "title": "Add usage examples for tf.audio APIs", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/audio\r\n\r\n## Description of issue (what needs changing):\r\nCurrently, there are no usage examples for tf.audio APIs , which makes it difficult for new users to implement the same.\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n**Audio is an area not really explored in machine learning to extent image and text has. While TensorFlow does provide a good amount of documentation for the general Args and Returns of the various functions under tf.audio, since most new users will have very little experience with audio as compared to tf.image**\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n**Yes**\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n**Yes**\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n**Yes**\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n**No**\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n**No**\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n**Formatted code blocks are present, which are satisfactory.**\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n**Yes, I think I can provide a detailed usage example.**\r\n  ", "comments": ["similar issue #28236", "@ravikyram can you please direct me to where i can add usage examples for this ? The docs page does not have a link to where the audio function is located, and I am having some trouble finding it on my own.", "Yeah, some python functions are auto-generated, and don't have python-source to link to.\r\n\r\nUsually if there's no source-link it's available in the `tensorflow/core/api_dev` directory: \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_DecodeWav.pbtxt", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]