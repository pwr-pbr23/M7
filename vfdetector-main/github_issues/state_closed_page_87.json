[{"number": 52553, "title": "Update Readme - Test PR Notification Change", "body": null, "comments": []}, {"number": 52552, "title": "Significant difference in trained model between tf2.3 and tf>2.4 ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): osx 11.2 and google colab\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3/2.4/2.5/2.6\r\n- Python version: 3.8.11\r\n\r\n**Describe the current behavior**\r\nI am working on a small deep q-network code for a course, applying it to cartpole-v0\r\nThe implementation achieves standard DQN performance level when using tf2.3.0, but learning fails (code runs but performance is extremely poor) when using tf2.4.0 and superior (tested 2.4.0, 2.5.0, 2.6.0 under osx, came accross the problem while trying to run the code on colab)\r\n\r\n**Describe the expected behavior**\r\nPerformance should be identical on all versions\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\nThis code runs fine with tf2.3 (it shows standard DQN performance level), while learning performance totally collapses for tf2.4 and superior:\r\nhttps://colab.research.google.com/drive/1xKh_Lw8gd1chPhNC8MzXmifC2zguzgU_?usp=sharing\r\n\r\nThanks in advance\r\n", "comments": ["Just noticed that by removing the `clipnorm` option in `Adam`, things seem to get back to normal in tf2.4+.\r\nCould it be related to <a href=\"https://github.com/tensorflow/tensorflow/issues/36001\">this issue</a> ? If yes, I'm not sure about the proper way of clipping gradients ?", "@sanatmpa1  ,\r\ni was able to reproduce the issue in tf v2.3, v2.6 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/7b3f96a853483b65e4b7f724cf4a9134/untitled97.ipynb).", "@jviquerat,\r\n\r\nCan you take a look at this [blog](https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48), and this [thread](https://www.reddit.com/r/MachineLearning/comments/3n8g28/gradient_clipping_what_are_good_values_to_clip_at/) which provides some insight on  selecting proper values to clip gradients. Thanks!", "> @jviquerat,\r\n> \r\n> Can you take a look at this [blog](https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48), and this [thread](https://www.reddit.com/r/MachineLearning/comments/3n8g28/gradient_clipping_what_are_good_values_to_clip_at/) which provides some insight on selecting proper values to clip gradients. Thanks!\r\n\r\nThank you for the answer, will definitely give it a look.\r\n\r\nDo you know if the difference in model performance is due to a change in the meaning of `clipnorm` ?", "There was some changes made from Tensorflow 2.4 version which states as below.\r\n`Optimizer.__init__ now accepts a gradient_aggregator to allow for customization of how gradients are aggregated across devices, as well as gradients_transformers to allow for custom gradient transformations (such as gradient clipping).`\r\nMay be this could be the reason, you can refer to [this](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md#tfkeras) 2.4 release note. Thanks.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 52551, "title": "[oneDNN] Fixing MAC build issue", "body": null, "comments": []}, {"number": 52550, "title": "Refactor label_image.py functions", "body": "Refactor `label_image.py` file.", "comments": []}, {"number": 52549, "title": "[r2.7 cherry pick] Update RELEASE.md with a note on modular file-system migration to tensorflow-io", "body": "This PR updates `RELEASE.md` with a note on the modular-filesystems migration to `tensorflow-io`. This is a follow-up of https://github.com/tensorflow/tensorflow/commit/9fcaf0838677d1095a7573e36ebb4d17511af6ed as those changes were not reflected during the 2.6.0 release.\r\n\r\ncc: @mihaimaruseac @yongtang ", "comments": []}, {"number": 52548, "title": "r2.7 cherry-pick request: Update oneDNN to version 2.4.1", "body": "Update oneDNN to version 2.4.1 to fix XByak-induced crashes on non-Intel systems. Details here:\r\n\r\nhttps://github.com/oneapi-src/oneDNN/commit/ad901e5489564d0035be0b4ec41f1cff4be96610\r\n\r\nPiperOrigin-RevId: 403166809", "comments": []}, {"number": 52547, "title": "Update RELEASE.md with a note on modular file-system migration to tensorflow-io", "body": "This PR updates `RELEASE.md` with a note on the modular-filesystems migration to `tensorflow-io`. This is a follow-up of https://github.com/tensorflow/tensorflow/commit/9fcaf0838677d1095a7573e36ebb4d17511af6ed as those changes were not reflected during the 2.6.0 release.\r\n\r\ncc: @mihaimaruseac @yongtang ", "comments": ["Please make these changes to `r2.7` branch since we merge the release notes from that branch back into master when TF 2.7 is finalized.", "thanks @mihaimaruseac . Created the PR: https://github.com/tensorflow/tensorflow/pull/52549", "Thanks. Merged"]}, {"number": 52546, "title": "Update absl-pypyi resulting in small improvment and fix on bazel test flags", "body": null, "comments": ["/cc @mihaimaruseac ", "I've updated the just released `abseil-py` so that now we support `test-filter` directly.", "We are making Copybara cry gain :robot:", "Will manually import, it's ok"]}, {"number": 52545, "title": "Improve the tf.repeat implementation", "body": "- Replaces the old implementation with a simpler algorithm that is significantly more memory and compute efficient.\r\n- Fixes https://github.com/tensorflow/tensorflow/issues/50712\r\n- **Requires https://github.com/tensorflow/tensorflow/pull/52543 to be merged first to avoid performance regression.**\r\n\r\ncc @nluehr ", "comments": ["Thanks a lot for doing this! Can you check in some benchmarks to track performance here? It could either be in this PR or another one. Would be good to quantify the benefits we expect from this. ", "I've added benchmarks and also fixed another performance issue I discovered (removed a forced cast to int32; it took me a few goes to keep all the tests passing).\r\n\r\nThese are the benchmark results I get on a Titan V GPU (this is including PR 52543):\r\n```\r\n                                                   Old Time  New Time  Speedup factor\r\nRepeatBenchmark.benchmark_repeat_few_1d\t           0.007645  0.000686  11.1\r\nRepeatBenchmark.benchmark_repeat_few_2d_axis0      0.000606  0.000277   2.2\r\nRepeatBenchmark.benchmark_repeat_few_2d_axis1      0.007869  0.000434  18.2\r\nRepeatBenchmark.benchmark_repeat_many_1d           0.005879  0.000599   9.8\r\nRepeatBenchmark.benchmark_repeat_many_2d_axis0     0.000329  0.000282   1.2\r\nRepeatBenchmark.benchmark_repeat_many_2d_axis0_big 0.001637  0.000439   3.7\r\nRepeatBenchmark.benchmark_repeat_many_2d_axis1     0.005514  0.000462  11.9\r\n```", "https://github.com/tensorflow/tensorflow/pull/52543 is merged now (after a roll-back and roll-forward), so this PR is good to go.", "We're looking into a performance regression in the benchmarks for the 1D case on CPU.", "Hi Ben,\r\n\r\nThank you for the contribution, this will help fix a number of issues around tf.repeat.\r\n\r\nI was able to observe similar improvements in performance as what you reported for GPU.\r\nHowever, I ran into a couple of performance regressions on CPU (via building without CUDA support) for the 1D case.\r\n\r\n|  | Old time | New time |\r\n|----------------------------|----------|-----------|\r\n| benchmark_repeat_few_1d | 0.029 | 0.211|\r\n|benchmark_repeat_many_1d | 0.019 | 0.086 |\r\n\r\nProfiling shows that most of the time is spent in the `upper_bound` that is invoked in `searchsorted`.\r\n\r\nCan you please take a look into this when you get a chance?\r\nThank you!", "I've added a commit to add multithreading to the searchsorted CPU kernels, but I'll probably have to revisit this in the new year.", "@bgdax Can you please review this PR ? Thanks!", "The PR is in internal review. We're also investigating the remaining internal tests that are broken."]}, {"number": 52544, "title": "TensorFlow unit test failure python/kernel_tests:self_adjoint_eig_op_test", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Aarch64 Centos-8\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): master\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.3.0\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nRunning the bazel : python/kernel_tests/self_adjoint_eig_op_test fails. \r\nBuilding the self_adjoint_eig_op_test with the following command: \r\n`bazel build --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --config=noaws --config=nogcp --config=nonccl --verbose_failures -- //tensorflow/python/kernel_tests:self_adjoint_eig_op_test`\r\n\r\ndtypes of float32 and complex64 with a [size](https://github.com/tensorflow/tensorflow/blob/5646333e1ece8170dff7b1980905e8f9c8383881/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.py#L247) of 10 all fail while the rest pass:\r\n```\r\n[  FAILED  ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_float32_3_10_10_True\r\n[  FAILED  ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_float32_3_10_10_False\r\n[  FAILED  ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_float32_10_10_True\r\n[  FAILED  ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_float32_10_10_False\r\n[  FAILED  ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_complex64_3_10_10_True\r\n[  FAILED  ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_complex64_3_10_10_False\r\n[  FAILED  ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_complex64_10_10_True\r\n[  FAILED  ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_complex64_10_10_False\r\n\r\n[       OK ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_complex128_3_10_10_False\r\n[       OK ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_complex128_3_10_10_True\r\n[       OK ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_float64_3_10_10_True\r\n```\r\n\r\nThis error is occurs on Aarch64 after [discarding a random input: `_ = RandomInput()`](https://github.com/tensorflow/tensorflow/blob/5646333e1ece8170dff7b1980905e8f9c8383881/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.py#L205) sample in the _GetSelfAdjointEigGradTest function. Without this input, or with when there is a second input discarded before running the tests, they pass as expected.\r\n\r\n**Describe the expected behavior**\r\n\r\nFor python/kernel_tests/self_adjoint_eig_op_test to pass.\r\n", "comments": ["Hi @Saduf2019  ! Could you please look at this issue!", "@cfRod @nSircombe ", "Hi all, here are the full traceback logs from running python/kernel_tests:self_adjoint_eig_op_test :\r\n\r\n```\r\nFAIL: test_SelfAdjointEigGrad_complex64_10_10_False (main.SelfAdjointEigGradTest)\r\nSelfAdjointEigGradTest.test_SelfAdjointEigGrad_complex64_10_10_False\r\nTraceback (most recent call last):\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.py\", line 241, in Test\r\nself.assertAllClose(theoretical, numerical, atol=tol, rtol=tol)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1390, in decorated\r\nreturn f(*args, **kwds)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2968, in assertAllClose\r\nself._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2906, in _assertAllCloseRecursive\r\n(path_str, path_str, msg))\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2860, in _assertArrayLikeAllClose\r\na, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\nFile \"/usr/local/lib64/python3.6/site-packages/numpy/testing/_private/utils.py\", line 1533, in assert_allclose\r\nverbose=verbose, header=header, equal_nan=equal_nan)\r\nFile \"/usr/local/lib64/python3.6/site-packages/numpy/testing/_private/utils.py\", line 846, in assert_array_compare\r\nraise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.01, atol=0.01\r\nMismatched value: a is different from b.\r\nnot close where = (array([0, 0]), array([0, 0]), array([132, 169]))\r\nnot close lhs = [ 0.05875404 -0.01270969]\r\nnot close rhs = [0.06975884 0.00339105]\r\nnot close dif = [0.0110048  0.01610075]\r\nnot close tol = [0.01069759 0.01003391]\r\ndtype = float32, shape = (1, 20, 200)\r\nMismatched elements: 2 / 4000 (0.05%)\r\nMax absolute difference: 0.01610075\r\nMax relative difference: 5.205287\r\nx: array([[[ 0.000389,  0.      ,  0.      , ..., -0.152314,  0.173274,\r\n0.      ],\r\n[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,...\r\ny: array([[[-0.000969,  0.      ,  0.      , ..., -0.154051,  0.17149 ,\r\n0.      ],\r\n[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,...\r\n\r\n======================================================================\r\nFAIL: test_SelfAdjointEigGrad_complex64_10_10_True (main.SelfAdjointEigGradTest)\r\nSelfAdjointEigGradTest.test_SelfAdjointEigGrad_complex64_10_10_True\r\nTraceback (most recent call last):\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.py\", line 241, in Test\r\nself.assertAllClose(theoretical, numerical, atol=tol, rtol=tol)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1390, in decorated\r\nreturn f(*args, **kwds)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2968, in assertAllClose\r\nself._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2906, in _assertAllCloseRecursive\r\n(path_str, path_str, msg))\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2860, in _assertArrayLikeAllClose\r\na, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\nFile \"/usr/local/lib64/python3.6/site-packages/numpy/testing/_private/utils.py\", line 1533, in assert_allclose\r\nverbose=verbose, header=header, equal_nan=equal_nan)\r\nFile \"/usr/local/lib64/python3.6/site-packages/numpy/testing/_private/utils.py\", line 846, in assert_array_compare\r\nraise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.01, atol=0.01\r\nMismatched value: a is different from b.\r\nnot close where = (array([0, 0]), array([0, 0]), array([132, 169]))\r\nnot close lhs = [ 0.05875404 -0.01270969]\r\nnot close rhs = [0.06975884 0.00339105]\r\nnot close dif = [0.0110048  0.01610075]\r\nnot close tol = [0.01069759 0.01003391]\r\ndtype = float32, shape = (1, 20, 200)\r\nMismatched elements: 2 / 4000 (0.05%)\r\nMax absolute difference: 0.01610075\r\nMax relative difference: 5.205287\r\nx: array([[[ 0.000389,  0.      ,  0.      , ..., -0.152314,  0.173274,\r\n0.      ],\r\n[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,...\r\ny: array([[[-0.000969,  0.      ,  0.      , ..., -0.154051,  0.17149 ,\r\n0.      ],\r\n[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,...\r\n\r\n======================================================================\r\nFAIL: test_SelfAdjointEigGrad_complex64_3_10_10_False (main.SelfAdjointEigGradTest)\r\nSelfAdjointEigGradTest.test_SelfAdjointEigGrad_complex64_3_10_10_False\r\nTraceback (most recent call last):\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.py\", line 241, in Test\r\nself.assertAllClose(theoretical, numerical, atol=tol, rtol=tol)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1390, in decorated\r\nreturn f(*args, **kwds)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2968, in assertAllClose\r\nself._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2906, in _assertAllCloseRecursive\r\n(path_str, path_str, msg))\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2860, in _assertArrayLikeAllClose\r\na, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\nFile \"/usr/local/lib64/python3.6/site-packages/numpy/testing/_private/utils.py\", line 1533, in assert_allclose\r\nverbose=verbose, header=header, equal_nan=equal_nan)\r\nFile \"/usr/local/lib64/python3.6/site-packages/numpy/testing/_private/utils.py\", line 846, in assert_array_compare\r\nraise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.01, atol=0.01\r\nMismatched value: a is different from b.\r\nnot close where = (array([0, 0, 0, 0, 0, 0]), array([ 0,  0, 20, 20, 40, 40]), array([132, 169, 332, 369, 532, 569]))\r\nnot close lhs = [ 0.05875404 -0.01270969  0.05875404 -0.01270969  0.05875404 -0.01270969]\r\nnot close rhs = [0.06975884 0.00339105 0.06975884 0.00339105 0.06975884 0.00339105]\r\nnot close dif = [0.0110048  0.01610075 0.0110048  0.01610075 0.0110048  0.01610075]\r\nnot close tol = [0.01069759 0.01003391 0.01069759 0.01003391 0.01069759 0.01003391]\r\ndtype = float32, shape = (1, 60, 600)\r\nMismatched elements: 6 / 36000 (0.0167%)\r\nMax absolute difference: 0.01610075\r\nMax relative difference: 5.205287\r\nx: array([[[0.000389, 0.      , 0.      , ..., 0.      , 0.      ,\r\n0.      ],\r\n[0.      , 0.      , 0.      , ..., 0.      , 0.      ,...\r\ny: array([[[-0.000969,  0.      ,  0.      , ...,  0.      ,  0.      ,\r\n0.      ],\r\n[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,...\r\n\r\n======================================================================\r\nFAIL: test_SelfAdjointEigGrad_complex64_3_10_10_True (main.SelfAdjointEigGradTest)\r\nSelfAdjointEigGradTest.test_SelfAdjointEigGrad_complex64_3_10_10_True\r\nTraceback (most recent call last):\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.py\", line 241, in Test\r\nself.assertAllClose(theoretical, numerical, atol=tol, rtol=tol)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1390, in decorated\r\nreturn f(*args, **kwds)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2968, in assertAllClose\r\nself._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2906, in _assertAllCloseRecursive\r\n(path_str, path_str, msg))\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2860, in _assertArrayLikeAllClose\r\na, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\nFile \"/usr/local/lib64/python3.6/site-packages/numpy/testing/_private/utils.py\", line 1533, in assert_allclose\r\nverbose=verbose, header=header, equal_nan=equal_nan)\r\nFile \"/usr/local/lib64/python3.6/site-packages/numpy/testing/_private/utils.py\", line 846, in assert_array_compare\r\nraise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.01, atol=0.01\r\nMismatched value: a is different from b.\r\nnot close where = (array([0, 0, 0, 0, 0, 0]), array([ 0,  0, 20, 20, 40, 40]), array([132, 169, 332, 369, 532, 569]))\r\nnot close lhs = [ 0.05875404 -0.01270969  0.05875404 -0.01270969  0.05875404 -0.01270969]\r\nnot close rhs = [0.06975884 0.00339105 0.06975884 0.00339105 0.06975884 0.00339105]\r\nnot close dif = [0.0110048  0.01610075 0.0110048  0.01610075 0.0110048  0.01610075]\r\nnot close tol = [0.01069759 0.01003391 0.01069759 0.01003391 0.01069759 0.01003391]\r\ndtype = float32, shape = (1, 60, 600)\r\nMismatched elements: 6 / 36000 (0.0167%)\r\nMax absolute difference: 0.01610075\r\nMax relative difference: 5.205287\r\nx: array([[[0.000389, 0.      , 0.      , ..., 0.      , 0.      ,\r\n0.      ],\r\n[0.      , 0.      , 0.      , ..., 0.      , 0.      ,...\r\ny: array([[[-0.000969,  0.      ,  0.      , ...,  0.      ,  0.      ,\r\n0.      ],\r\n[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,...\r\n\r\n======================================================================\r\nFAIL: test_SelfAdjointEigGrad_float32_10_10_False (main.SelfAdjointEigGradTest)\r\nSelfAdjointEigGradTest.test_SelfAdjointEigGrad_float32_10_10_False\r\nTraceback (most recent call last):\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.py\", line 241, in Test\r\nself.assertAllClose(theoretical, numerical, atol=tol, rtol=tol)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1390, in decorated\r\nreturn f(*args, **kwds)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2968, in assertAllClose\r\nself._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2906, in _assertAllCloseRecursive\r\n(path_str, path_str, msg))\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2860, in _assertArrayLikeAllClose\r\na, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\nFile \"/usr/local/lib64/python3.6/site-packages/numpy/testing/_private/utils.py\", line 1533, in assert_allclose\r\nverbose=verbose, header=header, equal_nan=equal_nan)\r\nFile \"/usr/local/lib64/python3.6/site-packages/numpy/testing/_private/utils.py\", line 846, in assert_array_compare\r\nraise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.01, atol=0.01\r\nMismatched value: a is different from b.\r\nnot close where = (array([0]), array([0]), array([98]))\r\nnot close lhs = [-0.00760599]\r\nnot close rhs = [-0.01889302]\r\nnot close dif = [0.01128702]\r\nnot close tol = [0.01018893]\r\ndtype = float32, shape = (1, 10, 100)\r\nMismatched elements: 1 / 1000 (0.1%)\r\nMax absolute difference: 0.01128702\r\nMax relative difference: 5.878891\r\nx: array([[[ 8.763281e-03,  0.000000e+00,  0.000000e+00,  0.000000e+00,\r\n0.000000e+00,  0.000000e+00,  0.000000e+00,  0.000000e+00,\r\n0.000000e+00,  0.000000e+00,  1.114071e-01,  3.540782e-01,...\r\ny: array([[[ 1.307978e-02,  0.000000e+00,  0.000000e+00,  0.000000e+00,\r\n0.000000e+00,  0.000000e+00,  0.000000e+00,  0.000000e+00,\r\n0.000000e+00,  0.000000e+00,  1.119048e-01,  3.604206e-01,...\r\n\r\n======================================================================\r\nFAIL: test_SelfAdjointEigGrad_float32_10_10_True (main.SelfAdjointEigGradTest)\r\nSelfAdjointEigGradTest.test_SelfAdjointEigGrad_float32_10_10_True\r\nTraceback (most recent call last):\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.py\", line 241, in Test\r\nself.assertAllClose(theoretical, numerical, atol=tol, rtol=tol)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1390, in decorated\r\nreturn f(*args, **kwds)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2968, in assertAllClose\r\nself._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2906, in _assertAllCloseRecursive\r\n(path_str, path_str, msg))\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2860, in _assertArrayLikeAllClose\r\na, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\nFile \"/usr/local/lib64/python3.6/site-packages/numpy/testing/_private/utils.py\", line 1533, in assert_allclose\r\nverbose=verbose, header=header, equal_nan=equal_nan)\r\nFile \"/usr/local/lib64/python3.6/site-packages/numpy/testing/_private/utils.py\", line 846, in assert_array_compare\r\nraise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.01, atol=0.01\r\nMismatched value: a is different from b.\r\nnot close where = (array([0]), array([0]), array([98]))\r\nnot close lhs = [-0.00760599]\r\nnot close rhs = [-0.01889302]\r\nnot close dif = [0.01128702]\r\nnot close tol = [0.01018893]\r\ndtype = float32, shape = (1, 10, 100)\r\nMismatched elements: 1 / 1000 (0.1%)\r\nMax absolute difference: 0.01128702\r\nMax relative difference: 5.878891\r\nx: array([[[ 8.763281e-03,  0.000000e+00,  0.000000e+00,  0.000000e+00,\r\n0.000000e+00,  0.000000e+00,  0.000000e+00,  0.000000e+00,\r\n0.000000e+00,  0.000000e+00,  1.114071e-01,  3.540782e-01,...\r\ny: array([[[ 1.307978e-02,  0.000000e+00,  0.000000e+00,  0.000000e+00,\r\n0.000000e+00,  0.000000e+00,  0.000000e+00,  0.000000e+00,\r\n0.000000e+00,  0.000000e+00,  1.119048e-01,  3.604206e-01,...\r\n\r\n======================================================================\r\nFAIL: test_SelfAdjointEigGrad_float32_3_10_10_False (main.SelfAdjointEigGradTest)\r\nSelfAdjointEigGradTest.test_SelfAdjointEigGrad_float32_3_10_10_False\r\nTraceback (most recent call last):\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.py\", line 241, in Test\r\nself.assertAllClose(theoretical, numerical, atol=tol, rtol=tol)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1390, in decorated\r\nreturn f(*args, **kwds)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2968, in assertAllClose\r\nself._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2906, in _assertAllCloseRecursive\r\n(path_str, path_str, msg))\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2860, in _assertArrayLikeAllClose\r\na, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\nFile \"/usr/local/lib64/python3.6/site-packages/numpy/testing/_private/utils.py\", line 1533, in assert_allclose\r\nverbose=verbose, header=header, equal_nan=equal_nan)\r\nFile \"/usr/local/lib64/python3.6/site-packages/numpy/testing/_private/utils.py\", line 846, in assert_array_compare\r\nraise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.01, atol=0.01\r\nMismatched value: a is different from b.\r\nnot close where = (array([0, 0, 0]), array([ 0, 10, 20]), array([ 98, 198, 298]))\r\nnot close lhs = [-0.00760599 -0.00760599 -0.00760599]\r\nnot close rhs = [-0.01889302 -0.01889302 -0.01889302]\r\nnot close dif = [0.01128702 0.01128702 0.01128702]\r\nnot close tol = [0.01018893 0.01018893 0.01018893]\r\ndtype = float32, shape = (1, 30, 300)\r\nMismatched elements: 3 / 9000 (0.0333%)\r\nMax absolute difference: 0.01128702\r\nMax relative difference: 5.878891\r\nx: array([[[ 0.008763,  0.      ,  0.      , ...,  0.      ,  0.      ,\r\n0.      ],\r\n[ 0.087142,  0.      ,  0.      , ...,  0.      ,  0.      ,...\r\ny: array([[[ 0.01308 ,  0.      ,  0.      , ...,  0.      ,  0.      ,\r\n0.      ],\r\n[ 0.091558,  0.      ,  0.      , ...,  0.      ,  0.      ,...\r\n\r\n======================================================================\r\nFAIL: test_SelfAdjointEigGrad_float32_3_10_10_True (main.SelfAdjointEigGradTest)\r\nSelfAdjointEigGradTest.test_SelfAdjointEigGrad_float32_3_10_10_True\r\nTraceback (most recent call last):\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.py\", line 241, in Test\r\nself.assertAllClose(theoretical, numerical, atol=tol, rtol=tol)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1390, in decorated\r\nreturn f(*args, **kwds)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2968, in assertAllClose\r\nself._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2906, in _assertAllCloseRecursive\r\n(path_str, path_str, msg))\r\nFile \"/home/builder/testing-ci/tst/tensorflow/bazel-bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2860, in _assertArrayLikeAllClose\r\na, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\nFile \"/usr/local/lib64/python3.6/site-packages/numpy/testing/_private/utils.py\", line 1533, in assert_allclose\r\nverbose=verbose, header=header, equal_nan=equal_nan)\r\nFile \"/usr/local/lib64/python3.6/site-packages/numpy/testing/_private/utils.py\", line 846, in assert_array_compare\r\nraise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.01, atol=0.01\r\nMismatched value: a is different from b.\r\nnot close where = (array([0, 0, 0]), array([ 0, 10, 20]), array([ 98, 198, 298]))\r\nnot close lhs = [-0.00760599 -0.00760599 -0.00760599]\r\nnot close rhs = [-0.01889302 -0.01889302 -0.01889302]\r\nnot close dif = [0.01128702 0.01128702 0.01128702]\r\nnot close tol = [0.01018893 0.01018893 0.01018893]\r\ndtype = float32, shape = (1, 30, 300)\r\nMismatched elements: 3 / 9000 (0.0333%)\r\nMax absolute difference: 0.01128702\r\nMax relative difference: 5.878891\r\nx: array([[[ 0.008763,  0.      ,  0.      , ...,  0.      ,  0.      ,\r\n0.      ],\r\n[ 0.087142,  0.      ,  0.      , ...,  0.      ,  0.      ,...\r\ny: array([[[ 0.01308 ,  0.      ,  0.      , ...,  0.      ,  0.      ,\r\n0.      ],\r\n[ 0.091558,  0.      ,  0.      , ...,  0.      ,  0.      ,...\r\n\r\nRan 89 tests in 26.818s\r\n\r\nFAILED (failures=8, skipped=1)\r\n```", "@cantonios Could you please take a look? From the logs this sounds like a tolerance issue imho.", "I checked the errors for intel.  For those failing tests, we have:\r\n```\r\ntest_SelfAdjointEigGrad_complex64_10_10_False\r\n  Max absolute difference: 0.00879528\r\n  Max relative difference: 4.538925\r\n\r\ntest_SelfAdjointEigGrad_complex64_10_10_True\r\n  Max absolute difference: 0.00879528\r\n  Max relative difference: 4.538925\r\n\r\ntest_SelfAdjointEigGrad_float32_10_10_False\r\n   Max absolute difference: 0.00921449\r\n   Max relative difference: 6.229456\r\n\r\ntest_SelfAdjointEigGrad_float32_3_10_10_True\r\n   Max absolute difference: 0.00921449\r\n   Max relative difference: 6.229456\r\n```\r\nAnd the comment above the tolerance in the test itself specifies:\r\n> ```# tolerance obtained by looking at actual differences using```\r\n>  ```# np.linalg.norm(theoretical-numerical, np.inf) on -mavx build```\r\n> ``` # after discarding one random input sample```\r\n\r\nso it's possible it's just a tolerance issue - an absolute error of 0.01 does seem a bit tight when the actual values are > 0.009.  The errors you're seeing are almost double though.\r\n\r\nMaybe try decreasing the `delta` parameter [here](https://github.com/tensorflow/tensorflow/blob/87462bfac761435a46641ff2f10ad0b6e5414a4b/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.py#L201) first to see if that lowers your errors.\r\n\r\n\r\n\r\n\r\n", "Hello @cantonios, if the delta is changed from .1 to .1001 the tests pass on aarch64. Is this an acceptable value?", "It's a step size, so I'd prefer decreasing it.  With the change from .1 to .1001, we're still likely in potential flaky territory.\r\n\r\nTry something like .05, then check what the errors are (e.g. you can set atol for the test to 0 and let it report the error to you when the test fails).  Hopefully that would bring us lower (with some buffer) than the original atol.", "Okay got it, .05 doesn't work however .09 does work pass successfully. I'll keep testing and see what the lowest possible is without it failing.", "I've tested several times at a delta of .08 and it also passes however any lower and the tests start to fail again so I think .08 would be the lowest possible. Does this seem reasonable @cantonios ?", "Andrew Goodbody has also tested this and said that the tests pass by disabling Fused Multiply Add instructions via `--copt=-ffp-contract=off --cxxopt=-ffp-contract=off`\r\n\r\n", "Are these tests built using -03? For the purpose of unit testing, I guess it is ok to disable these FMA instructions (?).\r\nTagging @penpornk  and @nSircombe here!\r\n", "By default it builds with -O2.", "Sorry for the late response! @cantonios has been on a long vacation. I think we can move forward with a 0.08 delta in the meanwhile. (We can revisit this later when @cantonios is back and if he thinks this needs more work.) \r\n\r\nWould you mind sending a PR for this and tag me? Thank you very much!", "My preference is small delta, leave FMA on.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52544\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52544\">No</a>\n"]}, {"number": 52543, "title": "Add GPU implementation for Range op", "body": "- This op currently has a GPU kernel registered, but it computes and returns its output on the host, which can cause significant slowdowns due to H2D memcopies and slow CPU execution. I don't know if there is a special reason it's like this (hopefully not).\r\n- This patch adds a proper GPU kernel that returns the output on the device.\r\n\r\ncc @nluehr ", "comments": ["Rasmus / Sanjoy - do you know why we never registered a proper GPU kernel for Range? Is it the same \"int32\" problem i.e. we expect the results to be typically used on host and so we keep the results on host always?", "cc @JXRiver @bgdax \r\n\r\nUpdate: we'll be evaluating if it's OK to register (e.g., no perf regression) Range GPU kernel.", "Thanks for your contribution. @bgdax is performing some benchmark tests for this PR. The initial result looks promising. We need some more time to thoroughly quantify its performance impact. Will get back to you once that is done.", "@benbarsdell Can you please check @rohan100jain's comments and keep us posted ? Thanks!", "> Can we add a test? You can check the tensor.backing_device for some int64 computation.\r\n\r\nI'm not sure I understand the goal of the test, could you elaborate?", "My understanding of @rohan100jain's comment is that we want to make sure int32 is forced to be placed on HostMemory and int64 could be placed on HostMemory or DeviceMemory. So future developers won't accidentally change this behavior. For example:\r\n```\r\ntf.config.set_soft_device_placement(False)\r\nwith tf.device('/GPU:0'):\r\n  # int32 is forced to be placed on HostMemory to avoid unnecessary D2H copies\r\n  t = tf.range(10, dtype=tf.int32)\r\n  print(t.backing_device)  # assert CPU\r\n\r\n  t = tf.range(10, dtype=tf.int64)\r\n  print(t.backing_device)  # assert GPU\r\n\r\nwith tf.device('/CPU:0'):\r\n  t = tf.range(10, dtype=tf.int32)\r\n  print(t.backing_device)  # assert CPU\r\n\r\n  t = tf.range(10, dtype=tf.int64)\r\n  print(t.backing_device)  # assert CPU\r\n```\r\n\r\nYou may add tests to https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/ops/math_ops_test.py or create a file in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/kernel_tests/math_ops\r\n\r\n"]}, {"number": 52542, "title": " Steps to create tensorflow lite c++ dynamic library through cmake.", "body": "\r\n\r\nI am trying to create dynamic TFLite c++ shared library.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake -\r\n\r\nWhen I run step 5 , It creates static library.\r\n\r\nCould you provide steps to create c++ dynamic library using cmake.\r\nThere is mentioned how to create only for c shared dynamic library but not c++.\r\n\r\n", "comments": ["@bhpatray In order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "\r\n**System information**\r\n\r\n- Debian 10\r\n- trying to TensorFlow Lite install from source:\r\n- TensorFlow Lite version 2.4.1:\r\n**Describe the current behavior**\r\nOnly able to generate C++ TFLite static library \r\n**Describe the expected behavior**\r\nNeed to create C++ TFLite dynamic library", "@bhpatray We see that you're using TF v2.4.1,Could you please try to use TF v2.6.0 and let us know if it is still an issue ? Thanks!", "Yes it still the issue.\r\nI need steps to build C++ TFLite dynamic library.\r\nThat is independent of TFLite version used", "Hi,\r\n\r\nI was able to create the shared library with the below changes in CMakeLists.txt present in tensorflow/lite folder.\r\nBut I was unable to use the generated shared library through CMake.\r\nI was able to run tflite models using shared library generated through bazel but not CMake.\r\n\r\n![diff1](https://user-images.githubusercontent.com/92724011/139819511-1b832fce-1068-4ba7-8dba-5f7d25d5e3ba.JPG)\r\n\r\n![diff2](https://user-images.githubusercontent.com/92724011/139819525-c08394c9-b419-45a6-9a5a-6b6eff1ced79.JPG)\r\n\r\nWhat's the issue with modified CMakeLists ?\r\nWhat's the procedure of creating a usable shared tflite library through CMake?\r\n", "Any update", "Hi @bhpatray! Could you refer these threads for building shared library using CMake.[ link1](https://stackoverflow.com/questions/17511496/how-to-create-a-shared-library-with-cmake),[link2](https://surfertas.github.io/cmake/cpp/projecteuler/2019/05/01/cmake.html).Please post this issue on Stackoverflow/[TF forum ](https://discuss.tensorflow.org/)for further assistance.Thank you!", "If you want to use shared library distribution of TFLite, you'd better use C API.\r\n\r\nhttps://www.tensorflow.org/lite/guide/build_cmake#build_tensorflow_lite_c_library\r\n\r\nThen you can use TFLite with necessary header files and the generated shared library.", "Are you planning to create a process for C++ CMake shared library in future releases?\r\nWhat's the complexity in creating C++ shared library through CMake? why it is only for C?\r\n\r\nI was able to generate a shared library through Bazel using the below command.\r\nbazel build  -c opt //tensorflow/lite:libtensorflowlite.so \r\n\r\nWhy is such a process not there for CMake?\r\nThe generated shared library through CMake was not working. Could you provide a fix for that?\r\n\r\n", "> Are you planning to create a process for C++ CMake shared library in future releases?\r\n\r\nWe don't have such a plan at the moment.\r\n\r\n> What's the complexity in creating C++ shared library through CMake? why it is only for C?\r\n\r\nThere is no easy way to share necessary header files. For C, you only need few C header files.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/README.md\r\n> \r\n> I was able to generate a shared library through Bazel using the below command.\r\n> bazel build -c opt //tensorflow/lite:libtensorflowlite.so\r\n\r\nI believe this is still possible. If this is sufficient for you, please use Bazel.\r\n> \r\n> Why is such a process not there for CMake?\r\n\r\nThe way how they handle dependent libraries are different.\r\n\r\n> The generated shared library through CMake was not working. Could you provide a fix for that?\r\n\r\nYou mean C shared library? You need to use C API instead of C++.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52542\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52542\">No</a>\n"]}, {"number": 52541, "title": "ModuleNotFoundError: No module named 'tensorflow.keras.model'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n\r\n         Edition\tWindows 10 Home Single Language\r\n        Version\t21H1\r\n        Installed on\t\u200e6/\u200e27/\u200e2020\r\n        OS build\t19043.1288\r\n        Serial number\tYN006FB0\r\n        Experience\tWindows Feature Experience Pack 120.2212.3920.0\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not Applicable\r\n- TensorFlow installed from (source or binary): Created env with Conda and then executed conda install tensorflow==2.5.0\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.8.12\r\n- Installed using virtualenv? pip? conda?:  Created env with Conda and then executed conda install tensorflow==2.5.0\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 5.3.2\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:  GeForce MX250 2048 MB\r\n\r\n\r\n\r\n**Describe the problem**\r\nUnable to use keras from tensorflow module and encoutering this error:  \r\n**ModuleNotFoundError: No module named 'tensorflow.keras.model'**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nimport numpy as np\r\nimport pandas as pd\r\n#import matplotlib.pyplot as plt\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom sklearn.metrics import mean_squared_error,r2_score\r\nfrom tensorflow.keras.model import Sequential\r\nfrom tensorflow.keras.layers import Dense,LSTM,Dropout\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@coolankit ,\r\nCan you please try to execute the code as shown in the [gist](https://colab.research.google.com/gist/tilakrayal/ea2945ac4518fbc9f610726c54c93139/untitled94.ipynb).Also please refer this [link](https://stackoverflow.com/questions/62311222/tensorflow-2-2-0-not-importing-sequential-from-tensorflow-keras-models-pyver-3) with the similar error.It helps.Thanks!", "@coolankit There is a small typo in your code. Replace the following \r\n\r\n`from tensorflow.keras.model import Sequential`\r\n\r\nwith \r\n\r\n`from tensorflow.keras.models import Sequential`", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52541\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52541\">No</a>\n"]}, {"number": 52540, "title": "TensorRT ValueError Input 1 of node StatefulPartitionedCall was passed float from unknown:0 incompatible with expected resource", "body": "**faild to use trt_convert to convert tensorflow .pb file to tensorrt**\r\n**model code:**\r\n```\r\ninputs = keras.Input(shape=(200,), dtype=\"int32\")\r\n# Embed each integer in a 128-dimensional vector\r\nx = layers.Embedding(max_features, 256)(inputs)\r\n# Add 2 bidirectional LSTMs\r\n# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\r\n# x = layers.Bidirectional(layers.LSTM(64))(x)\r\nx = layers.LSTM(64)(x)\r\n# Add a classifier\r\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\r\nmodel = keras.Model(inputs, outputs)\r\n```\r\n**convter pb to trt code**\r\n```\r\nmodel_dir = '/data/danlu/tensorrt/resnet/lstm'\r\nopt_model_dir = '/data/danlu/tensorrt/resnet/lstm_trt'\r\n \r\nprecision = \"FP32\"\r\nmax_workspace_size_bytes = 8000000000\r\nconversion_params = tf_trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=precision,\r\n                                                                  max_workspace_size_bytes=max_workspace_size_bytes,\r\n                                                                  maximum_cached_engines=100)\r\nconverter = tf_trt.TrtGraphConverterV2(input_saved_model_dir=model_dir, conversion_params=conversion_params)\r\nconverter.convert()\r\n \r\n \r\ndef build_fn():\r\n    Inp1 = np.random.randint(1, 200, (1, 200), dtype=np.int32)\r\n    yield Inp1\r\n \r\n \r\nconverter.build(input_fn=build_fn)\r\nconverter.save(opt_model_dir)\r\n```\r\n**pb file**\r\n[https://drive.google.com/file/d/1M4TEB3t1SDpBFj-RS7WL8AuWybZaP1YZ/view?usp=sharing](url)\r\n\r\n**env:**\r\nos: ubutun 16.04\r\npython: 3.8.3\r\ntensorflow: 2.6.0\r\nTensorRT: 7.2.3.4\r\ncuda: 10.2\r\ngpu: v100:32g\r\n\r\n**error log**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/importer.py\", line 496, in _import_graph_def_internal\r\n    results = c_api.TF_GraphImportGraphDefWithResults(\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input 1 of node StatefulPartitionedCall was passed float from unknown:0 incompatible with expected resource.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"trt_transfer.py\", line 21, in <module>\r\n    converter.convert()\r\n  File \"/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1087, in convert\r\n    frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)\r\n  File \"/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 1074, in convert_variables_to_constants_v2\r\n    return _construct_concrete_function(func, output_graph_def,\r\n  File \"/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 999, in _construct_concrete_function\r\n    new_func = wrap_function.function_from_graph_def(output_graph_def,\r\n  File \"/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 650, in function_from_graph_def\r\n    wrapped_import = wrap_function(_imports_graph_def, [])\r\n  File \"/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 621, in wrap_function\r\n    func_graph.func_graph_from_py_func(\r\n  File \"/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 87, in __call__\r\n    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n  File \"/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 93, in wrapped\r\n    return fn(*args, **kwargs)\r\n  File \"/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 648, in _imports_graph_def\r\n    importer.import_graph_def(graph_def, name=\"\")\r\n  File \"/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/importer.py\", line 400, in import_graph_def\r\n    return _import_graph_def_internal(\r\n  File \"/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/importer.py\", line 501, in _import_graph_def_internal\r\n    raise ValueError(str(e))\r\nValueError: Input 1 of node StatefulPartitionedCall was passed float from unknown:0 incompatible with expected resource.\r\n```\r\n", "comments": ["Hi @hotpeppeper ! Could you look at these threads for answer ? [Link1](https://forums.developer.nvidia.com/t/valueerror-input-1-of-node-statefulpartitionedcall-was-passed-float-from-conv2d-kernel-0-incompatible-with-expected-resource/112608),[Link2](https://forums.developer.nvidia.com/t/incompatible-with-expected-resource/72935#5356209),[Link3](https://github.com/onnx/tensorflow-onnx/issues/1152) . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "> Hi @hotpeppeper ! Could you look at these threads for answer ? [Link1](https://forums.developer.nvidia.com/t/valueerror-input-1-of-node-statefulpartitionedcall-was-passed-float-from-conv2d-kernel-0-incompatible-with-expected-resource/112608),[Link2](https://forums.developer.nvidia.com/t/incompatible-with-expected-resource/72935#5356209),[Link3](https://github.com/onnx/tensorflow-onnx/issues/1152) . Thanks!\r\n\r\nThis may not have any help, I still don't know how to resolve my problem", "Hi @GuoGuiRong ! Could you please check this [gist ](https://colab.sandbox.google.com/gist/mohantym/1812947bb3bad58b48eeddd9f43d79f1/github_52540.ipynb#scrollTo=H3yfLjsFKM6c) and let us know after configuring TensorRT?"]}, {"number": 52539, "title": "Clear global state cached by backend module for moinitored session", "body": "Clear global state cached by backend module for moinitored session to avoid memory leak.\r\n\r\nIn TF 2.x, the graph created by monitored_session will be cached by backend.py even after monitored_session exit.\r\n\r\nIf not do clear, it would cause memeory leak, especially when using TF estimator.", "comments": ["It looks like your PR relates to the Keras component. Please submit it to the github.com/keras-team/keras repository instead. Thankyou.\r\n@fchollet, @qlzh727", "> It looks like your PR relates to the Keras component. Please submit it to the github.com/keras-team/keras repository instead. Thankyou. @fchollet, @qlzh727\r\n\r\nThanks for reply. \r\nAfter I located root cause, I opened a new PR on keras. https://github.com/keras-team/keras/pull/15520"]}, {"number": 52538, "title": "AttributeError: module 'keras.engine.base_layer' has no attribute 'BaseRandomLayer'", "body": "Hello. \r\n\r\nContext: I'm using Colab and most of the time I'm using the  GPU runtime and High-RAM. I trained EfficientNetb7 and saved the model. When I ran tf.keras.load_model(), I got a bunch of error messages like:\r\n```\r\nWARNING:absl:Importing a function (__inference_block2f_activation_layer_call_and_return_conditional_losses_230237) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_sequential_6_layer_call_and_return_conditional_losses_294716) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\n```\r\nSo I did some Googling and I think someone recommended installing tf-nightly. That seems to have been a mistake! I couldn't get the model to re-run after that. I uninstalled tf-nightly, and restarted my runtime. I reinstalled tf version 2.6. Now when I go to run my imports, I get the error message in the subject line. Here are some more details of the error message: \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-25-5d853ee59f92> in <module>()\r\n     11 import tensorflow as tf\r\n     12 from tensorflow import keras\r\n---> 13 from tensorflow.keras import Model\r\n     14 from tensorflow.keras.models import Sequential\r\n     15 from tensorflow.keras.utils import to_categorical\r\n\r\n11 frames\r\n/usr/local/lib/python3.7/dist-packages/keras/layers/core/dropout.py in <module>()\r\n     24 \r\n     25 @keras_export('keras.layers.Dropout')\r\n---> 26 class Dropout(base_layer.BaseRandomLayer):\r\n     27   \"\"\"Applies Dropout to the input.\r\n     28 \r\n\r\nAttributeError: module 'keras.engine.base_layer' has no attribute 'BaseRandomLayer'\r\n```\r\nAny ideas on what is going on and how to fix it??", "comments": ["I think there are a couple of issues in my post. The first has something to do with keras and how to import it or what version to use. I noticed that even though I import keras like ```from tensorflow import keras``` I still have to type out ```tensorflow.keras.<whatever package>``` to import a package rathre than from ```keras import <whatever package>```\r\n\r\nThe second thing that I've noticed is that I get a lot of errors that I do not understand when I use ```.load_model```\r\n\r\nHere's is my code:\r\n```\r\nen_model.save('/content/drive/MyDrive/Fourthbrain/Midterm/EfficientNet_model_2')\r\nEfficientNet_model_2 = tf.keras.models.load_model('/content/drive/MyDrive/Fourthbrain/Midterm/EfficientNet_model_2')\r\n```\r\nAnd here are the errors:\r\n```\r\nWARNING:absl:Importing a function (__inference_block2d_expand_activation_layer_call_and_return_conditional_losses_107677) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_108631) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block1c_activation_layer_call_and_return_conditional_losses_199950) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_110133) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2f_activation_layer_call_and_return_conditional_losses_204021) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4j_activation_layer_call_and_return_conditional_losses_214897) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_117545) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4j_se_reduce_layer_call_and_return_conditional_losses_214959) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_118375) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_112625) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5j_activation_layer_call_and_return_conditional_losses_220920) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4g_expand_activation_layer_call_and_return_conditional_losses_212905) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6f_activation_layer_call_and_return_conditional_losses_115753) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4g_activation_layer_call_and_return_conditional_losses_213076) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5e_expand_activation_layer_call_and_return_conditional_losses_113246) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2f_activation_layer_call_and_return_conditional_losses_108176) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5g_activation_layer_call_and_return_conditional_losses_219099) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_210103) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6h_activation_layer_call_and_return_conditional_losses_116201) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5j_se_reduce_layer_call_and_return_conditional_losses_114458) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4g_se_reduce_layer_call_and_return_conditional_losses_213138) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_115081) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4i_se_reduce_layer_call_and_return_conditional_losses_112009) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7d_expand_activation_layer_call_and_return_conditional_losses_118151) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference__wrapped_model_82355) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4j_expand_activation_layer_call_and_return_conditional_losses_214726) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4g_expand_activation_layer_call_and_return_conditional_losses_111469) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3g_expand_activation_layer_call_and_return_conditional_losses_109909) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3f_expand_activation_layer_call_and_return_conditional_losses_208074) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5j_se_reduce_layer_call_and_return_conditional_losses_220982) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_114648) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2e_expand_activation_layer_call_and_return_conditional_losses_107901) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5h_expand_activation_layer_call_and_return_conditional_losses_113918) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_106282) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4j_expand_activation_layer_call_and_return_conditional_losses_112141) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_201422) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4h_se_reduce_layer_call_and_return_conditional_losses_213745) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5f_se_reduce_layer_call_and_return_conditional_losses_218554) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6l_activation_layer_call_and_return_conditional_losses_117097) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4d_activation_layer_call_and_return_conditional_losses_211255) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_221611) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5d_se_reduce_layer_call_and_return_conditional_losses_113114) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_201095) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5i_expand_activation_layer_call_and_return_conditional_losses_114142) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block1d_se_reduce_layer_call_and_return_conditional_losses_200466) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6g_activation_layer_call_and_return_conditional_losses_115977) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4h_expand_activation_layer_call_and_return_conditional_losses_111693) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2d_activation_layer_call_and_return_conditional_losses_107728) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6k_se_reduce_layer_call_and_return_conditional_losses_227634) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2c_activation_layer_call_and_return_conditional_losses_107504) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3g_activation_layer_call_and_return_conditional_losses_109960) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_205257) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5d_activation_layer_call_and_return_conditional_losses_217278) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3c_activation_layer_call_and_return_conditional_losses_109064) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3g_se_reduce_layer_call_and_return_conditional_losses_208914) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_201655) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6l_activation_layer_call_and_return_conditional_losses_228179) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4e_expand_activation_layer_call_and_return_conditional_losses_211691) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_216671) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block1d_se_reduce_layer_call_and_return_conditional_losses_106881) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3f_activation_layer_call_and_return_conditional_losses_109736) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6j_activation_layer_call_and_return_conditional_losses_116649) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_112416) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7d_activation_layer_call_and_return_conditional_losses_231167) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3d_activation_layer_call_and_return_conditional_losses_207031) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5f_activation_layer_call_and_return_conditional_losses_113521) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2g_expand_activation_layer_call_and_return_conditional_losses_108349) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5f_expand_activation_layer_call_and_return_conditional_losses_113470) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_199089) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6e_expand_activation_layer_call_and_return_conditional_losses_223759) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3c_se_reduce_layer_call_and_return_conditional_losses_206486) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_115305) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_110191) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5g_activation_layer_call_and_return_conditional_losses_113745) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_222778) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5h_activation_layer_call_and_return_conditional_losses_113969) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4f_activation_layer_call_and_return_conditional_losses_212469) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_110573) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_108881) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7c_expand_activation_layer_call_and_return_conditional_losses_117927) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7b_expand_activation_layer_call_and_return_conditional_losses_229782) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6f_activation_layer_call_and_return_conditional_losses_224537) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block1d_activation_layer_call_and_return_conditional_losses_106840) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_229393) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_216733) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5f_activation_layer_call_and_return_conditional_losses_218492) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6j_se_reduce_layer_call_and_return_conditional_losses_116690) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_112890) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6g_expand_activation_layer_call_and_return_conditional_losses_224973) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6f_se_reduce_layer_call_and_return_conditional_losses_115794) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_114857) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_110441) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2g_se_reduce_layer_call_and_return_conditional_losses_108441) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6k_se_reduce_layer_call_and_return_conditional_losses_116914) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_221938) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5i_activation_layer_call_and_return_conditional_losses_220313) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_229222) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3e_se_reduce_layer_call_and_return_conditional_losses_207700) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6e_expand_activation_layer_call_and_return_conditional_losses_115478) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2f_expand_activation_layer_call_and_return_conditional_losses_203850) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6f_expand_activation_layer_call_and_return_conditional_losses_115702) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4f_se_reduce_layer_call_and_return_conditional_losses_111337) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4f_expand_activation_layer_call_and_return_conditional_losses_212298) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3e_expand_activation_layer_call_and_return_conditional_losses_207467) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6j_activation_layer_call_and_return_conditional_losses_226965) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4f_se_reduce_layer_call_and_return_conditional_losses_212531) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4d_expand_activation_layer_call_and_return_conditional_losses_211084) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3c_expand_activation_layer_call_and_return_conditional_losses_109013) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6f_expand_activation_layer_call_and_return_conditional_losses_224366) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3g_expand_activation_layer_call_and_return_conditional_losses_208681) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6k_expand_activation_layer_call_and_return_conditional_losses_116822) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3d_se_reduce_layer_call_and_return_conditional_losses_207093) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6l_se_reduce_layer_call_and_return_conditional_losses_117138) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4e_se_reduce_layer_call_and_return_conditional_losses_111113) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6h_expand_activation_layer_call_and_return_conditional_losses_225580) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block1b_activation_layer_call_and_return_conditional_losses_106492) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_106333) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7d_expand_activation_layer_call_and_return_conditional_losses_230996) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_112849) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_108840) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2d_activation_layer_call_and_return_conditional_losses_202807) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_199151) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block1b_se_reduce_layer_call_and_return_conditional_losses_199558) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6k_expand_activation_layer_call_and_return_conditional_losses_227401) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_205319) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5d_expand_activation_layer_call_and_return_conditional_losses_217107) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7b_expand_activation_layer_call_and_return_conditional_losses_117703) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4h_activation_layer_call_and_return_conditional_losses_213683) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_210477) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_200840) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_216064) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6m_activation_layer_call_and_return_conditional_losses_228786) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5g_expand_activation_layer_call_and_return_conditional_losses_113694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5h_se_reduce_layer_call_and_return_conditional_losses_114010) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5d_expand_activation_layer_call_and_return_conditional_losses_113022) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4f_expand_activation_layer_call_and_return_conditional_losses_111245) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_110665) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3g_se_reduce_layer_call_and_return_conditional_losses_110001) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6j_expand_activation_layer_call_and_return_conditional_losses_226794) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6i_se_reduce_layer_call_and_return_conditional_losses_116466) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3d_expand_activation_layer_call_and_return_conditional_losses_109237) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6h_se_reduce_layer_call_and_return_conditional_losses_116242) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_231603) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4g_se_reduce_layer_call_and_return_conditional_losses_111561) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4e_activation_layer_call_and_return_conditional_losses_111072) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6g_se_reduce_layer_call_and_return_conditional_losses_116018) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6i_activation_layer_call_and_return_conditional_losses_116425) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_205646) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3d_expand_activation_layer_call_and_return_conditional_losses_206860) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_112365) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2e_activation_layer_call_and_return_conditional_losses_203414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block1d_activation_layer_call_and_return_conditional_losses_200404) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_107321) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4i_expand_activation_layer_call_and_return_conditional_losses_111917) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3e_activation_layer_call_and_return_conditional_losses_207638) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6m_activation_layer_call_and_return_conditional_losses_117321) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5i_se_reduce_layer_call_and_return_conditional_losses_114234) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block1c_se_reduce_layer_call_and_return_conditional_losses_106707) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4i_activation_layer_call_and_return_conditional_losses_111968) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_209543) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_117494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6g_activation_layer_call_and_return_conditional_losses_225144) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_108789) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block1b_activation_layer_call_and_return_conditional_losses_199496) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_205879) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4j_se_reduce_layer_call_and_return_conditional_losses_112233) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6m_expand_activation_layer_call_and_return_conditional_losses_117270) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_205064) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_110349) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2c_expand_activation_layer_call_and_return_conditional_losses_107453) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5j_activation_layer_call_and_return_conditional_losses_114417) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4e_expand_activation_layer_call_and_return_conditional_losses_111021) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5e_expand_activation_layer_call_and_return_conditional_losses_217714) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7c_se_reduce_layer_call_and_return_conditional_losses_230622) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_210710) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5e_se_reduce_layer_call_and_return_conditional_losses_113338) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5i_se_reduce_layer_call_and_return_conditional_losses_220375) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7b_activation_layer_call_and_return_conditional_losses_229953) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_198918) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6m_se_reduce_layer_call_and_return_conditional_losses_117362) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6j_se_reduce_layer_call_and_return_conditional_losses_227027) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6l_se_reduce_layer_call_and_return_conditional_losses_228241) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_216126) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3e_se_reduce_layer_call_and_return_conditional_losses_109553) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3c_activation_layer_call_and_return_conditional_losses_206424) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4j_activation_layer_call_and_return_conditional_losses_112192) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_209481) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7c_activation_layer_call_and_return_conditional_losses_230560) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3c_expand_activation_layer_call_and_return_conditional_losses_206253) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4h_expand_activation_layer_call_and_return_conditional_losses_213512) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_sequential_1_layer_call_and_return_conditional_losses_178802) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_112457) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_223323) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5j_expand_activation_layer_call_and_return_conditional_losses_114366) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_216500) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5h_activation_layer_call_and_return_conditional_losses_219706) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4i_expand_activation_layer_call_and_return_conditional_losses_214119) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_114590) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4e_se_reduce_layer_call_and_return_conditional_losses_211924) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7b_activation_layer_call_and_return_conditional_losses_117754) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5e_activation_layer_call_and_return_conditional_losses_113297) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5g_se_reduce_layer_call_and_return_conditional_losses_219161) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6i_activation_layer_call_and_return_conditional_losses_226358) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7b_se_reduce_layer_call_and_return_conditional_losses_230015) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6l_expand_activation_layer_call_and_return_conditional_losses_117046) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2c_expand_activation_layer_call_and_return_conditional_losses_202029) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2g_activation_layer_call_and_return_conditional_losses_204628) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_114806) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6i_se_reduce_layer_call_and_return_conditional_losses_226420) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_209870) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4i_se_reduce_layer_call_and_return_conditional_losses_214352) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4d_activation_layer_call_and_return_conditional_losses_110848) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_112798) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6e_se_reduce_layer_call_and_return_conditional_losses_115570) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5i_activation_layer_call_and_return_conditional_losses_114193) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_107280) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5h_expand_activation_layer_call_and_return_conditional_losses_219535) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4f_activation_layer_call_and_return_conditional_losses_111296) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_215893) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3d_activation_layer_call_and_return_conditional_losses_109288) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2e_expand_activation_layer_call_and_return_conditional_losses_203243) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_223385) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_107112) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6e_activation_layer_call_and_return_conditional_losses_115529) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_112666) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4e_activation_layer_call_and_return_conditional_losses_211862) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2c_se_reduce_layer_call_and_return_conditional_losses_202262) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block1c_se_reduce_layer_call_and_return_conditional_losses_200012) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4d_se_reduce_layer_call_and_return_conditional_losses_211317) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2f_se_reduce_layer_call_and_return_conditional_losses_204083) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7c_activation_layer_call_and_return_conditional_losses_117978) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_222545) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2c_activation_layer_call_and_return_conditional_losses_202200) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3d_se_reduce_layer_call_and_return_conditional_losses_109329) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4h_se_reduce_layer_call_and_return_conditional_losses_111785) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_215333) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_210648) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_107013) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_114689) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2d_expand_activation_layer_call_and_return_conditional_losses_202636) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_223152) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7c_se_reduce_layer_call_and_return_conditional_losses_118019) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6g_expand_activation_layer_call_and_return_conditional_losses_115926) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_efficientnetb7_layer_call_and_return_conditional_losses_194508) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_215566) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_106374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block1b_se_reduce_layer_call_and_return_conditional_losses_106533) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7d_activation_layer_call_and_return_conditional_losses_118202) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_209288) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2d_se_reduce_layer_call_and_return_conditional_losses_107769) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_108573) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6i_expand_activation_layer_call_and_return_conditional_losses_226187) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5j_expand_activation_layer_call_and_return_conditional_losses_220749) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6h_se_reduce_layer_call_and_return_conditional_losses_225813) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3f_se_reduce_layer_call_and_return_conditional_losses_109777) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6i_expand_activation_layer_call_and_return_conditional_losses_116374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5h_se_reduce_layer_call_and_return_conditional_losses_219768) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6h_activation_layer_call_and_return_conditional_losses_225751) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6k_activation_layer_call_and_return_conditional_losses_227572) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_110624) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2c_se_reduce_layer_call_and_return_conditional_losses_107545) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2g_activation_layer_call_and_return_conditional_losses_108400) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_210041) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_108672) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2f_expand_activation_layer_call_and_return_conditional_losses_108125) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5e_activation_layer_call_and_return_conditional_losses_217885) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_115346) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2f_se_reduce_layer_call_and_return_conditional_losses_108217) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6k_activation_layer_call_and_return_conditional_losses_116873) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_221549) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6g_se_reduce_layer_call_and_return_conditional_losses_225206) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6l_expand_activation_layer_call_and_return_conditional_losses_228008) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_115030) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4d_expand_activation_layer_call_and_return_conditional_losses_110797) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_205817) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5d_se_reduce_layer_call_and_return_conditional_losses_217340) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2e_se_reduce_layer_call_and_return_conditional_losses_203476) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7d_se_reduce_layer_call_and_return_conditional_losses_118243) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6m_expand_activation_layer_call_and_return_conditional_losses_228615) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4d_se_reduce_layer_call_and_return_conditional_losses_110889) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7d_se_reduce_layer_call_and_return_conditional_losses_231229) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_107071) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5g_expand_activation_layer_call_and_return_conditional_losses_218928) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_115122) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_sequential_1_layer_call_and_return_conditional_losses_172655) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5f_se_reduce_layer_call_and_return_conditional_losses_113562) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7c_expand_activation_layer_call_and_return_conditional_losses_230389) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3g_activation_layer_call_and_return_conditional_losses_208852) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_107229) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_221356) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5e_se_reduce_layer_call_and_return_conditional_losses_217947) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6m_se_reduce_layer_call_and_return_conditional_losses_228848) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3c_se_reduce_layer_call_and_return_conditional_losses_109105) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2e_se_reduce_layer_call_and_return_conditional_losses_107993) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4g_activation_layer_call_and_return_conditional_losses_111520) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_222109) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4i_activation_layer_call_and_return_conditional_losses_214290) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5i_expand_activation_layer_call_and_return_conditional_losses_220142) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6e_se_reduce_layer_call_and_return_conditional_losses_223992) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6e_activation_layer_call_and_return_conditional_losses_223930) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3e_expand_activation_layer_call_and_return_conditional_losses_109461) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2e_activation_layer_call_and_return_conditional_losses_107952) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_117586) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_efficientnetb7_layer_call_and_return_conditional_losses_188378) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block1c_activation_layer_call_and_return_conditional_losses_106666) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3f_activation_layer_call_and_return_conditional_losses_208245) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5f_expand_activation_layer_call_and_return_conditional_losses_218321) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6h_expand_activation_layer_call_and_return_conditional_losses_116150) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2g_se_reduce_layer_call_and_return_conditional_losses_204690) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_114898) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_115254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3f_se_reduce_layer_call_and_return_conditional_losses_208307) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5g_se_reduce_layer_call_and_return_conditional_losses_113786) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3f_expand_activation_layer_call_and_return_conditional_losses_109685) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6j_expand_activation_layer_call_and_return_conditional_losses_116598) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block3e_activation_layer_call_and_return_conditional_losses_109512) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2d_se_reduce_layer_call_and_return_conditional_losses_202869) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_229455) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2g_expand_activation_layer_call_and_return_conditional_losses_204457) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4h_activation_layer_call_and_return_conditional_losses_111744) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6f_se_reduce_layer_call_and_return_conditional_losses_224599) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_201593) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_222716) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_222171) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_110400) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block7b_se_reduce_layer_call_and_return_conditional_losses_117795) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_201033) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_112574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5d_activation_layer_call_and_return_conditional_losses_113073) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_215504) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_110232) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\n```", "@yogeshriyat \r\nCould you please share a colab gist with the error reported.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52537, "title": "Ref() does not return the same ref for the element.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 34\r\n- TensorFlow installed from (source or binary): 2.6.0\r\n- Python version: 3.9.7\r\n\r\n**Describe the current behavior**\r\n\r\nWhen loading MINST and requesting the ref() of the first element, it is not equal to itself.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe element should have the same ref at all time.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no, I don't know why it's doing this.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow_datasets as tfds\r\n\r\n# %% Train dataset\r\n(ds_train_original, ds_test_original), ds_info = tfds.load(\r\n    \"mnist\",\r\n    split=[\"train\", \"test\"],\r\n    shuffle_files=True,\r\n    as_supervised=True,\r\n    with_info=True,\r\n)\r\n\r\niterator = iter(ds_train_original)\r\nel = iterator.get_next()[0]\r\nel[0].ref() == el[0].ref()   # <- this should be True\r\n```\r\n", "comments": ["@MalcolmMielle ,\r\nI tried to reproduce the issue and haven't found any issue in execution.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/420e1417beb10b77183748861756b074/untitled92.ipynb).Can you please provide the error you are facing or more details about the issue mentioned.It helps to analyse the issue.Thanks", "@tilakrayal \r\nThanks for looking it up. I have updated the gist [here](https://colab.research.google.com/gist/MalcolmMielle/ca49d1687fdc608f6243ed73d6e507fd/untitled92.ipynb). I have simply added the last comparison into an assert to show the difference in the refs of `el[0]` even thought it is the same element.\r\n\r\nThe last line of the gist IMO should return True. However, this is False.\r\n\r\nAccording to the ref documentation:\r\n_Returns a hashable reference object to this Tensor. The primary use case for this API is to put tensors in a set/dictionary._\r\n\r\nMy understanding is that you should be able to use the ref() to check for equality between Tensor. Here the problem doesn't happen anymore once I have extracted the ref. For example, this is True:\r\n\r\n```\r\na_ref = el[0].ref()\r\na_deref = a_ref.deref()\r\nanother_ref = a_deref.ref()\r\na_ref == another_ref\r\n```\r\n\r\nSo the \"problem\" seems confined to extracting the ref() from iterator.", "@sanatmpa1 ,\r\nI was able to reproduce the issue in tf v2.5, v2.6 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/f45190e0037d925286cab18427ef0486/untitled98.ipynb).", "I think it is not the problem of Tensorflow, It has to do with the concept of `weakref` , please refer the [attached](https://colab.research.google.com/gist/sachinprasadhs/49e8cf7b29b5dff44664e4f7702c6c51/52537.ipynb#scrollTo=2qOVBS7dCS4K) Gist which is also returning False for `numpy` example.\r\nAlso, refer [this](https://docs.python.org/3/library/weakref.html) documentation for more details.", "Ok that make sense, I wasn't aware of that. Is there another way to get a consistent if per training sample from a dataset then?", "You can use [tf.data.experimental.sample_from_datasets](https://www.tensorflow.org/api_docs/python/tf/data/experimental/sample_from_datasets) and set seed in the parameter to get the consistent behavior.", "This [gist](https://colab.research.google.com/gist/MalcolmMielle/ca49d1687fdc608f6243ed73d6e507fd/untitled92.ipynb) still crashes on the assert. Is that what you meant?", "Sorry my bad, above reference was for multiple dataset, for checking if both the iter items are same, you can convert the tensor to numpy and test with .all(). \r\nHere is the [gist](https://colab.research.google.com/gist/sachinprasadhs/8655f7bcd83c9591eff0d73b5a9122be/untitled92.ipynb) for reference.", "This seems like the perfect solution for datasets like MNIST where each input is distinct. Thanks for the gist!\r\nHowever, for other dataset where multiple samples can be identical, that method can return `True` for two similar samples, right?\r\n\r\nThanks for taking the time to answer on this thread. I really appreciate the help!", "Yes, if the two samples are identical it should return True.\r\nCloud you please close the issue, if you don't have any further question.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52537\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52537\">No</a>\n"]}, {"number": 52536, "title": "Tensorflow serving in docker causes \"Invalid reduction dimension (1 for input with 1 dimension(s)...", "body": "I am using tensorflow-serving which runs with the build-in examples well and without issues:\r\ncd c:\\tmp\\tfserving\r\nPS C:\\tmp\\tfserving> C:\\programme\\git\\bin\\git clone https://github.com/tensorflow/serving\r\nset-variable -Name \"TESTDATA\" -Value \"$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata\"\r\ndocker run -t --rm -p 8501:8501 -v \"$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two\" -e MODEL_NAME=half_plus_two tensorflow/serving\r\ncurl -d \"{\\\"instances\\\": [1.0, 2.0, 5.0]}\" -X POST http://localhost:8501/v1/models/half_plus_two:predict\r\n\r\nHowever, by trying to run the following model in a recent docker container it causes:\r\n\"Invalid reduction dimension (1 for input with 1 dimension(s)\\\r\n\r\nSteps to verify:\r\nI used the example code ...\r\nhttps://keras.io/examples/structured_data/structured_data_classification_from_scratch/\r\nand at the end saved the model \r\nmodel.save('my-model.tf')\r\n\r\nand put it into the serving directory where it was recognized:\r\nC:\\tmp\\tfserving\\serving\\tensorflow_serving\\servables\\tensorflow\\testdata\r\ncd \\tmp\\tfserving\r\nset-variable -Name \"TESTDATA\" -Value \"$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata\"\r\ndocker run -t --rm -p 8501:8501 -v \"$TESTDATA/my_model:/models/my_model\" -e MODEL_NAME=my_model tensorflow/serving\r\n\r\ncurl http://localhost:8501/v1/models/my_model\r\n{  \"model_version_status\": [  { \"version\": \"1\", \"state\": \"AVAILABLE\", \"status\": { \"error_code\": \"OK\", [...]\r\n\r\nHowever, trying to run a prediction \r\n\r\ncurl -d \"{ \\\"instances\\\": [ {\\\"age\\\": 50,\\\"sex\\\": 1,\\\"cp\\\": 1,\\\"trestbps\\\": 145,\\\"chol\\\": 133,\\\"fbs\\\": 1,\\\"restecg\\\": 2,\\\"thalach\\\": 150,\\\"exang\\\": 0,\\\"oldpeak\\\": 2.3,\\\"slope\\\": 3,\\\"ca\\\": 0,\\\"thal\\\": \\\"fixed\\\" } ]}\" -X POST http://localhost:8501/v1/models/my_model:predict\r\n\r\n... returns:\r\n>>\"error\": \"Invalid reduction dimension (1 for input with 1 dimension(s)\\n\\t [[{{node model/integer_lookup_5/bincount/Max}}]]\"\r\n\r\n\r\n**System information**\r\n- Running on Windows with no GPU-Support. No further changes were made on the example code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n.a.\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):  v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: 3.8.11 (default, Aug  6 2021, 09:57:55) [MSC v.1916 64 bit (AMD64)]\r\n- Bazel version (if compiling from source): have also tried to compile on linux with bazel but with the same result\r\n- GCC/Compiler version (if compiling from source): n.a.\r\n- CUDA/cuDNN version: n.a.\r\n- GPU model and memory: n.a.\r\n\r\n\r\n", "comments": ["Hi @mbkgh! Could you look at these similar issues ? [Link1](https://stackoverflow.com/questions/43130365/valueerror-invalid-reduction-dimension-1-for-input-with-1-dimensions),[Link2](https://github.com/tensorflow/serving/issues/564) .Thanks!", "Thank you mohantym for the hint, however I was not able to fix the issue finally!\r\nThis is how the model is being defined:\r\n\r\n# Categorical features encoded as integers\r\nsex = keras.Input(shape=(1,), name=\"sex\", dtype=\"int64\")\r\ncp = keras.Input(shape=(1,), name=\"cp\", dtype=\"int64\")\r\nfbs = keras.Input(shape=(1,), name=\"fbs\", dtype=\"int64\")\r\nrestecg = keras.Input(shape=(1,), name=\"restecg\", dtype=\"int64\")\r\nexang = keras.Input(shape=(1,), name=\"exang\", dtype=\"int64\")\r\nca = keras.Input(shape=(1,), name=\"ca\", dtype=\"int64\")\r\n\r\n# Categorical feature encoded as string\r\nthal = keras.Input(shape=(1,), name=\"thal\", dtype=\"string\")\r\n\r\n# Numerical features\r\nage = keras.Input(shape=(1,), name=\"age\")\r\ntrestbps = keras.Input(shape=(1,), name=\"trestbps\")\r\nchol = keras.Input(shape=(1,), name=\"chol\")\r\nthalach = keras.Input(shape=(1,), name=\"thalach\")\r\noldpeak = keras.Input(shape=(1,), name=\"oldpeak\")\r\nslope = keras.Input(shape=(1,), name=\"slope\")\r\n\r\nall_inputs = [\r\n    sex,\r\n    cp,\r\n    fbs,\r\n    restecg,\r\n    exang,\r\n    ca,\r\n    thal,\r\n    age,\r\n    trestbps,\r\n    chol,\r\n    thalach,\r\n    oldpeak,\r\n    slope,\r\n]\r\n\r\n# Integer categorical features\r\nsex_encoded = encode_categorical_feature(sex, \"sex\", train_ds, False)\r\ncp_encoded = encode_categorical_feature(cp, \"cp\", train_ds, False)\r\nfbs_encoded = encode_categorical_feature(fbs, \"fbs\", train_ds, False)\r\nrestecg_encoded = encode_categorical_feature(restecg, \"restecg\", train_ds, False)\r\nexang_encoded = encode_categorical_feature(exang, \"exang\", train_ds, False)\r\nca_encoded = encode_categorical_feature(ca, \"ca\", train_ds, False)\r\n\r\n# String categorical features\r\nthal_encoded = encode_categorical_feature(thal, \"thal\", train_ds, True)\r\n\r\n# Numerical features\r\nage_encoded = encode_numerical_feature(age, \"age\", train_ds)\r\ntrestbps_encoded = encode_numerical_feature(trestbps, \"trestbps\", train_ds)\r\nchol_encoded = encode_numerical_feature(chol, \"chol\", train_ds)\r\nthalach_encoded = encode_numerical_feature(thalach, \"thalach\", train_ds)\r\noldpeak_encoded = encode_numerical_feature(oldpeak, \"oldpeak\", train_ds)\r\nslope_encoded = encode_numerical_feature(slope, \"slope\", train_ds)\r\n\r\nall_features = layers.concatenate(\r\n    [\r\n        sex_encoded,\r\n        cp_encoded,\r\n        fbs_encoded,\r\n        restecg_encoded,\r\n        exang_encoded,\r\n        slope_encoded,\r\n        ca_encoded,\r\n        thal_encoded,\r\n        age_encoded,\r\n        trestbps_encoded,\r\n        chol_encoded,\r\n        thalach_encoded,\r\n        oldpeak_encoded,\r\n    ]\r\n)\r\nx = layers.Dense(32, activation=\"relu\")(all_features)\r\nx = layers.Dropout(0.5)(x)\r\noutput = layers.Dense(1, activation=\"sigmoid\")(x)\r\nmodel = keras.Model(all_inputs, output)\r\nmodel.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\r\n\r\nI assume the model creation needs  to be modified to reshape the labels as 2d-tensors - or am I wrong?\r\nAny further help is appreciated.\r\nThis is how the model looks like:\r\n\r\nModel: \"model_6\"\r\n\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\nsex (InputLayer)                [(None, 1)]          0                                            \r\ncp (InputLayer)                 [(None, 1)]          0                                            \r\nfbs (InputLayer)                [(None, 1)]          0                                            \r\nrestecg (InputLayer)            [(None, 1)]          0                                            \r\nexang (InputLayer)              [(None, 1)]          0                                            \r\nslope (InputLayer)              [(None, 1)]          0                                            \r\nca (InputLayer)                 [(None, 1)]          0                                            \r\nthal (InputLayer)               [(None, 1)]          0                                            \r\nage (InputLayer)                [(None, 1)]          0                                            \r\ntrestbps (InputLayer)           [(None, 1)]          0                                            \r\nchol (InputLayer)               [(None, 1)]          0                                            \r\nthalach (InputLayer)            [(None, 1)]          0                                            \r\noldpeak (InputLayer)            [(None, 1)]          0                                            \r\ninteger_lookup_36 (IntegerLooku (None, 3)            0           sex[0][0]                        \r\ninteger_lookup_37 (IntegerLooku (None, 6)            0           cp[0][0]                         \r\ninteger_lookup_38 (IntegerLooku (None, 3)            0           fbs[0][0]                        \r\ninteger_lookup_39 (IntegerLooku (None, 4)            0           restecg[0][0]                    \r\ninteger_lookup_40 (IntegerLooku (None, 3)            0           exang[0][0]                      \r\nnormalization_41 (Normalization (None, 1)            3           slope[0][0]                      \r\ninteger_lookup_41 (IntegerLooku (None, 5)            0           ca[0][0]                         \r\nstring_lookup_6 (StringLookup)  (None, 6)            0           thal[0][0]                       \r\nnormalization_36 (Normalization (None, 1)            3           age[0][0]                        \r\nnormalization_37 (Normalization (None, 1)            3           trestbps[0][0]                   \r\nnormalization_38 (Normalization (None, 1)            3           chol[0][0]                       \r\nnormalization_39 (Normalization (None, 1)            3           thalach[0][0]                    \r\nnormalization_40 (Normalization (None, 1)            3           oldpeak[0][0]                    \r\nconcatenate_6 (Concatenate)     (None, 36)           0           integer_lookup_36[0][0]          \r\n                                                                 integer_lookup_37[0][0]          \r\n                                                                 integer_lookup_38[0][0]          \r\n                                                                 integer_lookup_39[0][0]          \r\n                                                                 integer_lookup_40[0][0]          \r\n                                                                 normalization_41[0][0]           \r\n                                                                 integer_lookup_41[0][0]          \r\n                                                                 string_lookup_6[0][0]            \r\n                                                                 normalization_36[0][0]           \r\n                                                                 normalization_37[0][0]           \r\n                                                                 normalization_38[0][0]           \r\n                                                                 normalization_39[0][0]           \r\n                                                                 normalization_40[0][0]           \r\ndense_25 (Dense)                (None, 32)           1184        concatenate_6[0][0]              \r\ndropout_15 (Dropout)            (None, 32)           0           dense_25[0][0]                   \r\ndense_26 (Dense)                (None, 1)            33          dropout_15[0][0]                 \r\n\r\nTotal params: 1,235\r\nTrainable params: 1,217\r\nNon-trainable params: 18", "I could not find any error in the model. \r\nIt seems to be either an error in tensorflow/serving or in tensorflow model.save(\"filename.tf\") method.\r\nThis I aussume otherwise the model would not work in tensorflow either.", "Ok @mbkgh!  Could you try again just saving the model without arguments or with .h5 extension .\r\n\r\n[Reference ](https://www.tensorflow.org/tutorials/keras/save_and_load#savedmodel_format)", "Thanks mohantym!\r\nmodel.save('my-model')\r\nINFO:tensorflow:Assets written to: my-model\\assets\r\n---\r\nmodel.save('my-model.h5') causes ...\r\n--------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n~\\AppData\\Local\\Temp/ipykernel_13596/716006849.py in <module>\r\n----> 1 model.save('my-model.h5')\r\n\r\nC:\\workspaces\\anaconda\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\r\n   2143     \"\"\"\r\n   2144     # pylint: enable=line-too-long\r\n-> 2145     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n   2146                     signatures, options, save_traces)\r\n   2147 \r\n\r\nC:\\workspaces\\anaconda\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\saving\\save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\r\n    143           'to the Tensorflow SavedModel format (by setting save_format=\"tf\") '\r\n    144           'or using `save_weights`.')\r\n--> 145     hdf5_format.save_model_to_hdf5(\r\n    146         model, filepath, overwrite, include_optimizer)\r\n    147   else:\r\n\r\nC:\\workspaces\\anaconda\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\saving\\hdf5_format.py in save_model_to_hdf5(model, filepath, overwrite, include_optimizer)\r\n    118     model_weights_group = f.create_group('model_weights')\r\n    119     model_layers = model.layers\r\n--> 120     save_weights_to_hdf5_group(model_weights_group, model_layers)\r\n    121 \r\n    122     # TODO(b/128683857): Add integration tests between tf.keras and external\r\n\r\nC:\\workspaces\\anaconda\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\saving\\hdf5_format.py in save_weights_to_hdf5_group(f, layers)\r\n    634   for layer in sorted(layers, key=lambda x: x.name):\r\n    635     g = f.create_group(layer.name)\r\n--> 636     weights = _legacy_weights(layer)\r\n    637     weight_values = backend.batch_get_value(weights)\r\n    638     weight_names = [w.name.encode('utf8') for w in weights]\r\n\r\nC:\\workspaces\\anaconda\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\saving\\hdf5_format.py in _legacy_weights(layer)\r\n    893   weights = layer.trainable_weights + layer.non_trainable_weights\r\n    894   if any(not isinstance(w, tf.Variable) for w in weights):\r\n--> 895     raise NotImplementedError(\r\n    896         'Save or restore weights that is not an instance of `tf.Variable` is '\r\n    897         'not supported in h5, use `save_format=\\'tf\\'` instead. Got a model '\r\n\r\nNotImplementedError: Save or restore weights that is not an instance of `tf.Variable` is not supported in h5, use `save_format='tf'` instead. Got a model or layer IntegerLookup with weights [<keras.layers.preprocessing.index_lookup.VocabWeightHandler object at 0x000001A3851A8A60>]", "In addition I tried with the one model created: \r\nC:\\Users\\User>curl http://localhost:8501/v1/models/my_model\r\n{ \"model_version_status\": [   {    \"version\": \"1\",   \"state\": \"AVAILABLE\",   \"status\": {     \"error_code\": \"OK\", ...\r\n\r\nand it leads to the same issue...\r\n\r\nC:\\Users\\User>curl -d \"{ \\\"instances\\\": [ {\\\"age\\\": 50,\\\"sex\\\": 1,\\\"cp\\\": 1,\\\"trestbps\\\": 145,\\\"chol\\\": 133,\\\"fbs\\\": 1,\\\"restecg\\\": 2,\\\"thalach\\\": 150,\\\"exang\\\": 0,\\\"oldpeak\\\": 2.3,\\\"slope\\\": 3,\\\"ca\\\": 0,\\\"thal\\\": \\\"fixed\\\" } ]}\" -X POST http://localhost:8501/v1/models/my_model:predict\r\n{\r\n    \"error\": \"Invalid reduction dimension (1 for input with 1 dimension(s)\\n\\t [[{{node model/string_lookup/bincount/Max}}]]\"\r\n}\r\n\r\nMaybe the error is on my side by passing the parameters in curl in a wrong way\r\nThis is how it works fine in Python - from the example:\r\nsample = {\r\n    \"age\": 50,\r\n    \"sex\": 1,\r\n    \"cp\": 1,\r\n    \"trestbps\": 145,\r\n    \"chol\": 133,\r\n    \"fbs\": 1,\r\n    \"restecg\": 2,\r\n    \"thalach\": 150,\r\n    \"exang\": 0,\r\n    \"oldpeak\": 2.3,\r\n    \"slope\": 3,\r\n    \"ca\": 0,\r\n    \"thal\": \"fixed\",\r\n}\r\nfor n in sample.items():\r\n    print (n)\r\n\r\ninput_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\r\npredictions = model.predict(input_dict)\r\n\r\n('age', 50)\r\n('sex', 1)\r\n('cp', 1)\r\n('trestbps', 145)\r\n('chol', 133)\r\n('fbs', 1)\r\n('restecg', 2)\r\n('thalach', 150)\r\n('exang', 0)\r\n('oldpeak', 2.3)\r\n('slope', 3)\r\n('ca', 0)\r\n('thal', 'fixed')\r\n{'age': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([50])>, 'sex': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>, 'cp': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>, 'trestbps': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([145])>, 'chol': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([133])>, 'fbs': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>, 'restecg': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>, 'thalach': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([150])>, 'exang': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>, 'oldpeak': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.3], dtype=float32)>, 'slope': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([3])>, 'ca': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>, 'thal': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'fixed'], dtype=object)>}\r\nThis particular patient had a 10.1 percent probability of having a heart disease, as evaluated by our model.", "Ok! @mbkgh! Please post this issue in [TF serving ](https://github.com/tensorflow/serving) too.Thank you !", "Issue posted as requested: [https://github.com/tensorflow/serving/issues/1926](https://github.com/tensorflow/serving/issues/1926)", "Ok @mbkgh! Could you close this issue here as It will be tracked there. Thanks!", "Thank you mohantym!\r\nClosing the issue, it will be tracked in [https://github.com/tensorflow/serving/issues/1926](https://github.com/tensorflow/serving/issues/1926)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52536\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52536\">No</a>\n"]}, {"number": 52535, "title": "Cannot update variable with shape [2] using a Tensor with shape [512,1], shapes must be equal. [Op:AssignAddVariableOp]", "body": "I think this issue is more relevant to tensorflow than keras hence I'm asking it here and not in the keras repository. \r\nI've also gone through different versions of this problem on stack overflow but I can't seem to figure out what is going wrong.\r\nBefore starting , I'd also like to add that I've asked this question on the Tensorflow Forum  & SO, but didn't get any response.\r\n\r\nMy model takes one input and gives two outputs , I'm passing it in a dictionary:\r\n```Python\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\r\n    (\r\n        {\"input_1\": atr},\r\n        {\"ed\": wtr, \"sd\": wbtr},\r\n    )\r\n)\r\ntrain_dataset = train_dataset.batch(100).repeat(3)\r\n```\r\nShape of all the three arrays `atr` , `wtr` and `wbtr` is `(7838, 512, 1)`.\r\nThis is my model:\r\n```Python\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\nfrom tensorflow.keras import Input, Model\r\ninput1 = tf.keras.layers.Input(shape=(None,1),name=\"input_1\")\r\nx = tf.keras.layers.Conv1D(filters=16, kernel_size=3, strides=1, padding=\"causal\", activation=\"relu\",input_shape=[None,1])(input1)\r\nx = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, activation=\"tanh\", return_sequences=True))(x)\r\nx = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(256, activation=\"tanh\", return_sequences=True))(x)\r\nx = tf.keras.layers.Dense(128, activation=\"tanh\")(x)\r\no1 = tf.keras.layers.Dense(1, activation=\"linear\",name=\"ed\")(x)\r\no2 = tf.keras.layers.Dense(1, activation=\"sigmoid\",name=\"sd\")(x)\r\n\r\nmodel = Model(inputs=[input1], outputs=[o1, o2])\r\n\r\nmodel.compile(loss={'ed': 'mean_squared_error', \r\n                    'sd': 'binary_crossentropy'},\r\n              loss_weights={'ed':0.4,\r\n                            'sd':0.6},\r\n              optimizer='adam',\r\n              metrics={'ed': tf.keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error\", dtype=None),\r\n                       'sd': tfa.metrics.F1Score(name=\"f1_score\",num_classes=2, threshold=0.5)})\r\n```\r\nHere is the model summary:\r\n```PythonModel: \"model_14\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, None, 1)]    0                                            \r\n__________________________________________________________________________________________________\r\nconv1d_18 (Conv1D)              (None, None, 16)     64          input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\nbidirectional_33 (Bidirectional (None, None, 256)    112128      conv1d_18[0][0]                  \r\n__________________________________________________________________________________________________\r\nbidirectional_34 (Bidirectional (None, None, 512)    789504      bidirectional_33[0][0]           \r\n__________________________________________________________________________________________________\r\ndense_17 (Dense)                (None, None, 128)    65664       bidirectional_34[0][0]           \r\n__________________________________________________________________________________________________\r\ned(Dense)                      (None, None, 1)      129         dense_17[0][0]                   \r\n__________________________________________________________________________________________________\r\nsd (Dense)                      (None, None, 1)      129         dense_17[0][0]                   \r\n==================================================================================================\r\nTotal params: 967,618\r\nTrainable params: 967,618\r\nNon-trainable params: 0\r\n__________________________\r\n```\r\nFinally my `model.fit()` method:\r\n```Python\r\nhistory = model.fit(train_dataset,epochs=3,verbose=1,steps_per_epoch= 78)\r\n```\r\nHere is the error:\r\n```Python\r\n    InvalidArgumentError: Cannot update variable with shape [2] using a Tensor with shape [512,1], shapes must be equal. [Op:AssignAddVariableOp]\r\n```\r\nI don't where I'm going wrong , pls help.\r\n", "comments": ["@akshit0201  In order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "**System information**.\r\n- Have I written custom code (as opposed to using a stock example script provided in Keras): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- TensorFlow version (use command below): 2.x\r\n- TensorFlow add-on version:  0.14.0\r\n- Python version: 3.6.9\r\n- GPU model and memory: None\r\n\r\n**Describe the problem**.\r\n\r\nMentioned clearly above \r\n\r\n**Describe the current behavior**.\r\nError:\r\n```Python\r\n    InvalidArgumentError: Cannot update variable with shape [2] using a Tensor with shape [512,1], shapes must be equal. [Op:AssignAddVariableOp]\r\n```\r\n\r\n**Describe the expected behavior**.\r\n`model.fit()` should run successfully \r\n\r\n**Standalone code to reproduce the issue**.\r\n\r\n```Python\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\nfrom tensorflow.keras import Input, Model\r\nimport numpy as np\r\n\r\n!pip install -U tensorflow-addons\r\n\r\ntf.config.run_functions_eagerly(True)\r\n\r\natr = np.random.rand(7838, 512, 1)\r\nwtr = np.random.rand(7838, 512, 1)\r\nwbtr = np.random.rand(7838, 512, 1)\r\n\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\r\n    (\r\n        {\"input_1\": atr},\r\n        {\"ed\": wtr, \"sd\": wbtr},\r\n    )\r\n)\r\ntrain_dataset = train_dataset.batch(100).repeat(3)\r\n\r\ninput1 = tf.keras.layers.Input(shape=(None,1),name=\"input_1\")\r\nx = tf.keras.layers.Conv1D(filters=16, kernel_size=3, strides=1, padding=\"causal\", activation=\"relu\",input_shape=(None,1))(input1)\r\nx = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, activation=\"tanh\", return_sequences=True))(x)\r\nx = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(256, activation=\"tanh\", return_sequences=True))(x)\r\nx = tf.keras.layers.Dense(128, activation=\"tanh\")(x)\r\no1 = tf.keras.layers.Dense(1, activation=\"linear\",name=\"ed\")(x)\r\no2 = tf.keras.layers.Dense(1, activation=\"sigmoid\",name=\"sd\")(x)\r\n\r\nmodel = Model(inputs=[input1], outputs=[o1, o2])\r\n\r\nmodel.compile(loss={'ed': 'mean_squared_error', \r\n                    'sd': 'binary_crossentropy'},\r\n              loss_weights={'ed':0.4,\r\n                            'sd':0.6},\r\n              optimizer='adam',\r\n              metrics={'ed': tf.keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error\", dtype=None),\r\n                       'sd': tfa.metrics.F1Score(name=\"f1_score\",num_classes=2, threshold=0.5)})\r\n\r\nhistory = model.fit(train_dataset,epochs=3,verbose=1,steps_per_epoch= 78)\r\n```\r\n\r\n**Source code / logs**.\r\n\r\n```Python\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-16-a35494538429> in <module>()\r\n     13 \r\n     14 \r\n---> 15 history = model.fit(train_dataset,epochs=3,callbacks=[cp_callback],verbose=1,steps_per_epoch= 78)\r\n\r\n12 frames\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1182                 _r=1):\r\n   1183               callbacks.on_train_batch_begin(step)\r\n-> 1184               tmp_logs = self.train_function(iterator)\r\n   1185               if data_handler.should_sync:\r\n   1186                 context.async_wait()\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in train_function(iterator)\r\n    851       def train_function(iterator):\r\n    852         \"\"\"Runs a training execution with one step.\"\"\"\r\n--> 853         return step_function(self, iterator)\r\n    854 \r\n    855     else:\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in step_function(model, iterator)\r\n    840 \r\n    841       data = next(iterator)\r\n--> 842       outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    843       outputs = reduce_per_replica(\r\n    844           outputs, self.distribute_strategy, reduction='first')\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py in run(***failed resolving arguments***)\r\n   1284       fn = autograph.tf_convert(\r\n   1285           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\r\n-> 1286       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n   1287 \r\n   1288   def reduce(self, reduce_op, value, axis):\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)\r\n   2847       kwargs = {}\r\n   2848     with self._container_strategy().scope():\r\n-> 2849       return self._call_for_each_replica(fn, args, kwargs)\r\n   2850 \r\n   2851   def _call_for_each_replica(self, fn, args, kwargs):\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)\r\n   3630   def _call_for_each_replica(self, fn, args, kwargs):\r\n   3631     with ReplicaContext(self._container_strategy(), replica_id_in_sync_group=0):\r\n-> 3632       return fn(*args, **kwargs)\r\n   3633 \r\n   3634   def _reduce_to(self, reduce_op, value, destinations, options):\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    595   def wrapper(*args, **kwargs):\r\n    596     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.UNSPECIFIED):\r\n--> 597       return func(*args, **kwargs)\r\n    598 \r\n    599   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in run_step(data)\r\n    833 \r\n    834       def run_step(data):\r\n--> 835         outputs = model.train_step(data)\r\n    836         # Ensure counter is updated only if `train_step` succeeds.\r\n    837         with tf.control_dependencies(_minimum_control_deps(outputs)):\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in train_step(self, data)\r\n    790     # Run backwards pass.\r\n    791     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\r\n--> 792     self.compiled_metrics.update_state(y, y_pred, sample_weight)\r\n    793     # Collect metrics to return\r\n    794     return_metrics = {}\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py in update_state(self, y_true, y_pred, sample_weight)\r\n    455         if metric_obj is None:\r\n    456           continue\r\n--> 457         metric_obj.update_state(y_t, y_p, sample_weight=mask)\r\n    458 \r\n    459       for weighted_metric_obj in weighted_metric_objs:\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/utils/metrics_utils.py in decorated(metric_obj, *args, **kwargs)\r\n     71 \r\n     72     with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):\r\n---> 73       update_op = update_state_fn(*args, **kwargs)\r\n     74     if update_op is not None:  # update_op will be None in eager execution.\r\n     75       metric_obj.add_update(update_op)\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/metrics.py in update_state_fn(*args, **kwargs)\r\n    175         control_status = tf.__internal__.autograph.control_status_ctx()\r\n    176         ag_update_state = tf.__internal__.autograph.tf_convert(obj_update_state, control_status)\r\n--> 177         return ag_update_state(*args, **kwargs)\r\n    178     else:\r\n    179       if isinstance(obj.update_state, tf.__internal__.function.Function):\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    693       except Exception as e:  # pylint:disable=broad-except\r\n    694         if hasattr(e, 'ag_error_metadata'):\r\n--> 695           raise e.ag_error_metadata.to_exception(e)\r\n    696         else:\r\n    697           raise\r\n\r\nInvalidArgumentError: in user code:\r\n\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow_addons/metrics/f_scores.py:160 update_state  *\r\n        self.true_positives.assign_add(_weighted_sum(y_pred * y_true, sample_weight))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:858 assign_add  **\r\n        name=name)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py:47 assign_add_variable_op\r\n        _ops.raise_from_not_ok_status(e, name)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:6941 raise_from_not_ok_status\r\n        six.raise_from(core._status_to_exception(e.code, message), None)\r\n    <string>:3 raise_from\r\n        \r\n\r\n    InvalidArgumentError: Cannot update variable with shape [2] using a Tensor with shape [512,1], shapes must be equal. [Op:AssignAddVariableOp]\r\n```\r\n", "@akshit0201 Thanks for the update!\r\nCould you please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52535\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52535\">No</a>\n"]}, {"number": 52534, "title": "TypeError: Protocols cannot be instantiated - when iterating over a dataset", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 21.04\r\n- TensorFlow installed from binary: pip install tf-nightly\r\n- TensorFlow version: v1.12.1-65564-gea2d617f792 2.8.0-dev20211016\r\n- Python version: 3.9\r\n\r\n**Describe the current behavior**\r\nI can not iterate over a tf.data.Dataset . My guess is, that I might have a version mismatch in a dependency, but I can not figure it out.\r\n\r\n    Traceback (most recent call last):\r\n      File \"/home/----------/example.py\", line 6, in <module>\r\n        for element in ds:\r\n      File \"/home/----------/miniconda3/envs/tf-gpu-07/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 481, in __iter__\r\n        return iterator_ops.OwnedIterator(self)\r\n      File \"/home/----------/miniconda3/envs/tf-gpu-07/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 727, in __init__\r\n        super(OwnedIterator, self).__init__()\r\n      File \"/home/----------/miniconda3/envs/tf-gpu-07/lib/python3.9/typing.py\", line 1083, in _no_init\r\n        raise TypeError('Protocols cannot be instantiated')\r\n    TypeError: Protocols cannot be instantiated\r\n\r\n**Describe the expected behavior**\r\ntf.Tensor(1, shape=(), dtype=int32)\r\ntf.Tensor(2, shape=(), dtype=int32)\r\ntf.Tensor(3, shape=(), dtype=int32)\r\ntf.Tensor(4, shape=(), dtype=int32)\r\ntf.Tensor(5, shape=(), dtype=int32)\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n    import tensorflow as tf\r\n    \r\n    ds = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5])\r\n    \r\n    for element in ds:\r\n        print(element)\r\n", "comments": ["@BAAZKonqi ,\r\nWe see that you are using tf version 1.12, 1.x is not actively supported, please update to latest stable v2.5 or v2.6 and let us know if you are using same issue.", "Actually I think I am using version 2.8 . The versions I put in the description is the output of print(tf.version.GIT_VERSION, tf.version.VERSION) . I'm not exactly sure, what GIT_VERSION means in this context.\r\npip list gives:\r\n    tf-nightly                   2.8.0.dev20211016", "@BAAZKonqi ,\r\nI tried to execute the mentioned code in tf v2.5,v2.6 and nightly.I haven't found any issue while executing the code.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/179b7dfe50a9ba402a1548487dfdecc5/untitled95.ipynb).Can you please try to test the code again in new environment.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52534\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52534\">No</a>\n"]}, {"number": 52533, "title": "Failed to build python3 package", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 21.10\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:2.6.0\r\n- Python version:3.9.7\r\n- Bazel version (if compiling from source):3.7.2\r\n- GCC/Compiler version (if compiling from source):GCC9\r\n- CUDA/cuDNN version:11.0\r\n- GPU model and memory:RTX2060 6GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n\u276f bazel build //tensorflow/tools/pip_package:build_pip_package --config=opt --config=cuda --config=mkl\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=86\r\nINFO: Reading rc options for 'build' from /home/zwq/workspace/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/zwq/workspace/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from /home/zwq/workspace/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 --config=cuda\r\nINFO: Found applicable config definition build:short_logs in file /home/zwq/workspace/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/zwq/workspace/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda in file /home/zwq/workspace/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:opt in file /home/zwq/workspace/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare\r\nINFO: Found applicable config definition build:cuda in file /home/zwq/workspace/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:mkl in file /home/zwq/workspace/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt\r\nINFO: Found applicable config definition build:linux in file /home/zwq/workspace/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/zwq/workspace/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: /home/zwq/.cache/bazel/_bazel_zwq/004cf1a57827e926c0c6bde2a3de4f2d/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (444 packages loaded, 29466 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 24.994s, Critical Path: 8.73s\r\nINFO: 1 process: 1 internal.\r\nINFO: Build completed successfully, 1 total action\r\n\u276f \r\n\u276f  bazel-bin/tensorflow/tools/pip_package/build_pip_package pkg\r\n2021\u5e74 10\u6708 17\u65e5 \u661f\u671f\u65e5 12:22:28 CST : === Preparing sources in dir: /tmp/tmp.FwVpMUz8nN\r\n~/workspace/tensorflow ~/workspace/tensorflow\r\n~/workspace/tensorflow\r\n~/workspace/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow ~/workspace/tensorflow\r\n~/workspace/tensorflow\r\n/tmp/tmp.FwVpMUz8nN/tensorflow/include ~/workspace/tensorflow\r\n~/workspace/tensorflow\r\n2021\u5e74 10\u6708 17\u65e5 \u661f\u671f\u65e5 12:22:58 CST : === Building wheel\r\nusage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\r\n   or: setup.py --help [cmd1 cmd2 ...]\r\n   or: setup.py --help-commands\r\n   or: setup.py cmd --help\r\n\r\nerror: invalid command 'bdist_wheel'\r\n```\r\nC++ Library has been compiled successfully.", "comments": ["See tested GPU configuration for Linux [here](https://www.tensorflow.org/install/source#gpu)\r\nFor TF 2.6.0 the lowest CUDA version - doesn't above  11.2\r\n", "Hi @StarOnTheSky! Inline with above comment, Could you also look at these issues with similar error stack trace ?[ link1](https://github.com/tensorflow/tensorflow/issues/348),[link2](https://stackoverflow.com/questions/34819221/why-is-python-setup-py-saying-invalid-command-bdist-wheel-on-travis-ci).", "> Hi @StarOnTheSky! Inline with above comment, Could you also look at these issues with similar error stack trace ?[ link1](https://github.com/tensorflow/tensorflow/issues/348),[link2](https://stackoverflow.com/questions/34819221/why-is-python-setup-py-saying-invalid-command-bdist-wheel-on-travis-ci).\n\nOkay, problem solved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52533\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52533\">No</a>\n"]}, {"number": 52532, "title": "[tensorflow/compiler/xla/**/*.cc] Add calls to `reserve()` before populating vectors", "body": "https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-945027209 told me to merge into one PR per 'large module/namespace'", "comments": ["As this PR is failing again in the CI do you have followed my advise in https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940458368 to locally build and execute tests on the `xla` target before submitting this new PR?", "@SamuelMarks Can you please resolve conflicts? Thanks!", "@gbaned It looks like all my smaller\u20141 file\u2014commits are starting to be merged. So should this aggregate one be closed?", "Yes"]}, {"number": 52531, "title": "tf.data.dataset function that only creates dataset of windows, containing tensors", "body": "It is necessary to have tf.data.dataset function that creates dataset of windows of tensor type (instead of datasets of timestamps).\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.6\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, there are 2 functions that do smth similar, but not exactly:\r\n- `tf.keras.utils.timeseries_dataset_from_array`  that also does unnecessary batching\r\n-  `dataset.window` that creates nested Dataset. And in the case when all timestamps need to be joined together in a single Tensor - it is a problem, as Dataset API doesn't allow to join multiple dataset elements in a Tensor (or am I missing smth?)\r\n(BTW, those 2 functions confusingly use opposite names for the arguments with a similar behavior)\r\n\r\nLack of a single function doing what requested above - leads to hacks [like this one, described at StackOverflow](https://stackoverflow.com/questions/64497977/tensorflow-convert-tf-dataset-to-tf-tensor)\r\n\r\nDetailed use case:\r\nRaw data comes has 1 sec frequency. For prediction history up to several hours is relevant. But old history is far less important (only general level), while the last minute data must be taken with the highest resolution (i.e. every second).\r\nIt is necessary to create 5 hour windows (i.e. windows size = 3600*5=18000 ) and select from them only a hundred of timestamps, that likely contain all the relevant information for the modl.\r\n\r\n**Who will benefit with this feature?**\r\nThose who need to do further transformations over the timeseries windows.\r\nAs a more specific example: those who need to have an uneven sampling of timesteps (.e.g  gradually increasing frequency)", "comments": ["I have realized that  what I wanted can be achieved via:\r\n```python\r\ndef get_tensor(timestep_ds):\r\n  timestep_ds.batch(WINDOW_SIZE).get_single_element()\r\n\r\ndf.window(SIZE).map(get_tensor)\r\n```\r\nSomehow it was not evident for me from the documentation.\r\n\r\n"]}, {"number": 52530, "title": "Failed to initialize NVML: Driver/library version mismatch", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\nCan someone help me please with this issue?\r\nThank you\r\n\r\nSystem information\r\n\r\nOS Platform and Distribution: Ubuntu 20.04\r\nTensorFlow installed from (source or binary): pip install --upgrade tensorflow-gpu\r\nTensorFlow version:\r\ntensorflow==2.6.0\r\ntensorflow-estimator==2.6.0\r\ntensorflow-gpu==2.6.0\r\ntensorflow-io-gcs-filesystem==0.21.0\r\ntf-estimator-nightly==2.7.0.dev2021092408\r\ntf-nightly==2.7.0.dev20210922\r\nPython version: Python 3.7.3\r\nInstalled using virtualenv? pip? conda?: (condo env) pip install --upgrade tensorflow-gpu\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from source): gcc version 9.3.0 (Ubuntu 9.3.0-17ubuntu1~20.04)\r\nCUDA/cuDNN version: kernel version 460.91.3 does not match DSO version 470.57.2 -- cannot find working devices in this configuration\r\n<img width=\"804\" alt=\"image\" src=\"https://user-images.githubusercontent.com/74671619/137600054-1be66398-2738-421d-9cc3-72c150a066fb.png\">\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nAfter following these steps $ nvidia-smi is not working anymore\r\n\r\nFailed to initialize NVML: Driver/library version mismatch\r\n<img width=\"1355\" alt=\"image\" src=\"https://user-images.githubusercontent.com/74671619/137599966-a0d39a5a-f80c-429e-b156-76447489a48e.png\">\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n$ nvidia-smi\r\nFailed to initialize NVML: Driver/library version mismatch\r\n\r\n2021-10-16 14:56:09.420644: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\r\n2021-10-16 14:56:09.420676: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: Primer\r\n2021-10-16 14:56:09.420700: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: Primer\r\n2021-10-16 14:56:09.420758: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.57.2\r\n2021-10-16 14:56:09.420782: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\r\n2021-10-16 14:56:09.420791: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 460.91.3 does not match DSO version 470.57.2 -- cannot find working devices in this configuration\r\n\r\n$ uname -a\r\nLinux Primer 5.4.0-88-generic #99-Ubuntu SMP Thu Sep 23 17:29:00 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n$ nvidia-settings --help\r\n\r\nnvidia-settings:  version 470.57.02\r\n  The NVIDIA X Server Settings tool.\r\n\r\n$ nvidia-smi\r\nFailed to initialize NVML: Driver/library version mismatch\r\n\r\n$ sudo prime-select query\r\nnvidia\r\n\r\n$ cat /proc/driver/nvidia/version\r\nNVRM version: NVIDIA UNIX x86_64 Kernel Module  460.91.03  Fri Jul  2 06:04:10 UTC 2021\r\nGCC version:  gcc version 9.3.0 (Ubuntu 9.3.0-17ubuntu1~20.04)\r\n\r\n$ sudo lspci -v | grep VGA\r\n02:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1) (prog-if 00 [VGA controller])\r\n03:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1) (prog-if 00 [VGA controller])\r\n04:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1) (prog-if 00 [VGA controller])\r\n\r\n$ lspci | grep -i nvidia\r\n02:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1)\r\n02:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)\r\n03:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1)\r\n03:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)\r\n04:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1)\r\n04:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)\r\n\r\n$ dpkg -l | grep -i nvidia\r\nii  cuda-nsight-compute-11-2                        11.2.0-1   amd64        NVIDIA Nsight Compute\r\nii  cuda-nsight-compute-11-4                        11.4.2-1   amd64        NVIDIA Nsight Compute\r\nii  cuda-nsight-systems-11-2                        11.2.0-1   amd64        NVIDIA Nsight Systems\r\nii  cuda-nsight-systems-11-4                        11.4.2-1   amd64        NVIDIA Nsight Systems\r\nii  cuda-nvtx-11-2                                  11.2.67-1   amd64        NVIDIA Tools Extension\r\nii  cuda-nvtx-11-4                                  11.4.120-1   amd64        NVIDIA Tools Extension\r\nii  libaccinj64-10.1:amd64                          10.1.243-3   amd64        NVIDIA ACCINJ Library (64-bit)\r\nii  libcublas10:amd64                               10.1.243-3   amd64        NVIDIA cuBLAS Library\r\nii  libcublaslt10:amd64                             10.1.243-3   amd64        NVIDIA cuBLASLt Library\r\nii  libcudart10.1:amd64                             10.1.243-3   amd64        NVIDIA CUDA Runtime Library\r\nii  libcufft10:amd64                                10.1.243-3   amd64        NVIDIA cuFFT Library\r\nii  libcufftw10:amd64                               10.1.243-3   amd64        NVIDIA cuFFTW Library\r\nii  libcuinj64-10.1:amd64                           10.1.243-3   amd64        NVIDIA CUINJ Library (64-bit)\r\nii  libcupti-dev:amd64                              10.1.243-3   amd64        NVIDIA CUDA Profiler Tools Interface development files\r\nii  libcupti-doc                                    10.1.243-3   all          NVIDIA CUDA Profiler Tools Interface documentation\r\nii  libcupti10.1:amd64                              10.1.243-3   amd64        NVIDIA CUDA Profiler Tools Interface runtime library\r\nii  libcurand10:amd64                               10.1.243-3   amd64        NVIDIA cuRAND Library\r\nii  libcusolver10:amd64                             10.1.243-3   amd64        NVIDIA cuSOLVER Library\r\nii  libcusolvermg10:amd64                           10.1.243-3   amd64        NVIDIA cuSOLVERmg Library\r\nii  libcusparse10:amd64                             10.1.243-3   amd64        NVIDIA cuSPARSE Library\r\nii  libnppc10:amd64                                 10.1.243-3   amd64        NVIDIA Performance Primitives core runtime library\r\nii  libnppial10:amd64                               10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Arithmetic and Logic\r\nii  libnppicc10:amd64                               10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Color Conversion\r\nii  libnppicom10:amd64                              10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Compression\r\nii  libnppidei10:amd64                              10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Data Exchange and Initialization\r\nii  libnppif10:amd64                                10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Filters\r\nii  libnppig10:amd64                                10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Geometry transforms\r\nii  libnppim10:amd64                                10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Morphological operations\r\nii  libnppist10:amd64                               10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Statistics\r\nii  libnppisu10:amd64                               10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Support\r\nii  libnppitc10:amd64                               10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Threshold and Compare\r\nii  libnpps10:amd64                                 10.1.243-3   amd64        NVIDIA Performance Primitives for signal processing runtime library\r\nii  libnvgraph10:amd64                              10.1.243-3   amd64        NVIDIA Graph Analytics library (nvGRAPH)\r\nii  libnvidia-cfg1-470:amd64                        470.57.02-0ubuntu1   amd64        NVIDIA binary OpenGL/GLX configuration library\r\nii  libnvidia-common-460                            460.27.04-0ubuntu1   all          Shared files used by the NVIDIA libraries\r\nii  libnvidia-common-470                            470.57.02-0ubuntu1   all          Shared files used by the NVIDIA libraries\r\nii  libnvidia-compute-418:amd64                     430.50-0ubuntu3   amd64        Transitional package for libnvidia-compute-430\r\nii  libnvidia-compute-430:amd64                     470.57.02-0ubuntu1   amd64        Transitional package for libnvidia-compute-470\r\nrc  libnvidia-compute-450:amd64                     450.102.04-0ubuntu0.20.04.1   amd64        NVIDIA libcompute package\r\nrc  libnvidia-compute-460:amd64                     460.91.03-0ubuntu0.20.04.1   amd64        NVIDIA libcompute package\r\nii  libnvidia-compute-470:amd64                     470.57.02-0ubuntu1   amd64        NVIDIA libcompute package\r\nii  libnvidia-decode-470:amd64                      470.57.02-0ubuntu1   amd64        NVIDIA Video Decoding runtime libraries\r\nii  libnvidia-encode-470:amd64                      470.57.02-0ubuntu1   amd64        NVENC Video Encoding runtime library\r\nii  libnvidia-extra-470:amd64                       470.57.02-0ubuntu1   amd64        Extra libraries for the NVIDIA driver\r\nii  libnvidia-fbc1-470:amd64                        470.57.02-0ubuntu1   amd64        NVIDIA OpenGL-based Framebuffer Capture runtime library\r\nii  libnvidia-gl-470:amd64                          470.57.02-0ubuntu1   amd64        NVIDIA OpenGL/GLX/EGL/GLES GLVND libraries and Vulkan ICD\r\nii  libnvidia-ifr1-470:amd64                        470.57.02-0ubuntu1   amd64        NVIDIA OpenGL-based Inband Frame Readback runtime library\r\nii  libnvidia-ml-dev                                10.1.243-3   amd64        NVIDIA Management Library (NVML) development files\r\nii  libnvjpeg10:amd64                               10.1.243-3   amd64        NVIDIA JPEG library (nvJPEG)\r\nii  libnvrtc10.1:amd64                              10.1.243-3   amd64        CUDA Runtime Compilation (NVIDIA NVRTC Library)\r\nii  libnvtoolsext1:amd64                            10.1.243-3   amd64        NVIDIA Tools Extension Library\r\nii  libnvvm3:amd64                                  10.1.243-3   amd64        NVIDIA NVVM Library\r\nii  nsight-compute                                  10.1.243-3   amd64        NVIDIA Nsight Compute\r\nii  nsight-compute-2020.3.0                         2020.3.0.18-1   amd64        NVIDIA Nsight Compute\r\nii  nsight-compute-2021.2.2                         2021.2.2.1-1   amd64        NVIDIA Nsight Compute\r\nrc  nvidia-compute-utils-450                        450.102.04-0ubuntu0.20.04.1   amd64        NVIDIA compute utilities\r\nrc  nvidia-compute-utils-460                        460.91.03-0ubuntu0.20.04.1   amd64        NVIDIA compute utilities\r\nii  nvidia-compute-utils-470                        470.57.02-0ubuntu1   amd64        NVIDIA compute utilities\r\nii  nvidia-cuda-dev                                 10.1.243-3   amd64        NVIDIA CUDA development files\r\nii  nvidia-cuda-doc                                 10.1.243-3   all          NVIDIA CUDA and OpenCL documentation\r\nii  nvidia-cuda-gdb                                 10.1.243-3   amd64        NVIDIA CUDA Debugger (GDB)\r\nii  nvidia-cuda-toolkit                             10.1.243-3   amd64        NVIDIA CUDA development toolkit\r\nrc  nvidia-dkms-450                                 450.102.04-0ubuntu0.20.04.1   amd64        NVIDIA DKMS package\r\nrc  nvidia-dkms-460                                 460.91.03-0ubuntu0.20.04.1   amd64        NVIDIA DKMS package\r\nii  nvidia-dkms-470                                 470.57.02-0ubuntu1   amd64        NVIDIA DKMS package\r\nii  nvidia-driver-470                               470.57.02-0ubuntu1   amd64        NVIDIA driver metapackage\r\nrc  nvidia-kernel-common-450                        450.102.04-0ubuntu0.20.04.1   amd64        Shared files used with the kernel module\r\nrc  nvidia-kernel-common-460                        460.91.03-0ubuntu0.20.04.1   amd64        Shared files used with the kernel module\r\nii  nvidia-kernel-common-470                        470.57.02-0ubuntu1   amd64        Shared files used with the kernel module\r\nii  nvidia-kernel-source-470                        470.57.02-0ubuntu1   amd64        NVIDIA kernel source package\r\nii  nvidia-modprobe                                 470.57.02-0ubuntu1   amd64        Load the NVIDIA kernel driver and create device files\r\nii  nvidia-prime                                    0.8.14   all          Tools to enable NVIDIA's Prime\r\nii  nvidia-profiler                                 10.1.243-3   amd64        NVIDIA Profiler for CUDA and OpenCL\r\nii  nvidia-settings                                 470.57.02-0ubuntu1   amd64        Tool for configuring the NVIDIA graphics driver\r\nii  nvidia-utils-470                                470.57.02-0ubuntu1   amd64        NVIDIA driver support binaries\r\nii  nvidia-visual-profiler                          10.1.243-3   amd64        NVIDIA Visual Profiler for CUDA and OpenCL\r\nii  nvtop                                           1.0.0-1ubuntu2   amd64        Interactive NVIDIA GPU process monitor\r\nii  screen-resolution-extra                         0.18build1   all          Extension for the nvidia-settings control panel\r\nii  xserver-xorg-video-nvidia-470                   470.57.02-0ubuntu1   amd64        NVIDIA binary Xorg driver\r\n\r\n\r\n", "comments": ["@maryamxasghari ,\r\nCan you please refer this [link](https://stackoverflow.com/questions/43022843/nvidia-nvml-driver-library-version-mismatch) and [issue](https://github.com/tensorflow/tensorflow/issues/40278) with the similar error.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "the problem solved after reboot. I will close this issue. Thank you ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52530\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52530\">No</a>\n", "Glad the suggestion worked to resolved the issue.Thanks!"]}, {"number": 52529, "title": "How to decrease the learning rate every 10 epochs by a factor of 0.9?", "body": "I want to set the learning rate at 10^-3 with a decay every 10 epochs by a factor of 0.9. I am using the Adam optimizer in Tensorflow Keras. I have found this code in the official documentation:\r\n\r\ninitial_learning_rate = 0.1\r\n\r\n```\r\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\r\n    initial_learning_rate,\r\n    decay_steps=100000,\r\n    decay_rate=0.96,\r\n    staircase=True\r\n)\r\n```\r\nI do not know what is this decay_steps=100000. Actually, I want to decrease my learning rate after 10 epochs. How can I do it?", "comments": ["Hi @Nafees-060!\r\nThis is not a bug or feature request, please open this issue in Tf discussion [forum ](https://discuss.tensorflow.org/)as there is a larger community there.Thank you!", "In tensorflow it uses mini batches to train the model. For your problem you need to understand two things \r\n1) when we uses generators on our datasets it keeps on giving batches infinitely so we need to tell keras that after these many batches we consider as one epoch in\r\n model.fit( steps_per_epoch) argument. so generally we uses **steps_per_epoch = train_dataset.shape[0] // batch_size** \r\n(total training example / batchsize)\r\n\r\nfor more info  [check this](https://stackoverflow.com/questions/49922252/choosing-number-of-steps-per-epoch)\r\n\r\n2) here you want that after 10 epochs your lr should be lr = lr * 0.9  so in exponential decay uses this equation \r\n`def decayed_learning_rate(step):\r\n  return initial_learning_rate * decay_rate ^ (step / decay_steps)`\r\n\r\nwhere step/decay_steps is integer hence it will 1 when step == decay_steps other wise if step<decay_step it will be zero\r\nso in decay_steps you pass\r\n ### decay_steps = 10 * steps_per_epoch\r\n\r\nanother approach would be you can create your own optimzer for your requirement\r\n\r\n<pre>class MyLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\r\n\r\n  def __init__(self, initial_learning_rate,decay=10000):\r\n    self.initial_learning_rate = initial_learning_rate\r\n\r\n  def __call__(self, step):\r\n     return self.initial_learning_rate / (step + 1)\r\n\r\n '''here in call you can check conditions and customize the things you want to do like here you can add if condition to reduce your learning rate do\r\n    if (step % decay ==0)\r\n   {\r\n       self.initial_learning_rate = self.initial_learning_rate * 0.9\r\n   }\r\nreturn self.initial_learning_rate '''\r\noptimizer = tf.keras.optimizers.SGD(learning_rate=MyLRSchedule(0.1,10 * steps_per_epoch))</pre>\r\n\r\nlinks:\r\n[example link](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule)\r\n[other schedules](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules)", "Hi @Nafees-060! Could you check with above suggestions?", "> Hi @Nafees-060! Could you check with the above suggestions?\r\n\r\nI am working on it, please give me some time", "> Hi @Nafees-060! Could you check with above suggestions?\r\n\r\nThank you for the post. But I have got a solution from [here](https://stackoverflow.com/questions/69595923/how-to-decrease-the-learning-rate-every-10-epochs-by-a-factor-of-0-9/69603109) and somehow it works. If you varify this solution, would it be grateful? Thanks ", "Hi @Nafees-060! Both approach looks valid to me . Feel free to close this issue if it helped. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52529\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52529\">No</a>\n", "@mohantym I have seen `tf.keras.callbacks.ReduceLROnPlateau` here on this [Link](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau)  Its look like it will work the same? Can we replace this instead of the custom callback or your solution? Please confirm. ", "@Nafees-060  \r\nThis call back is for when loss is not improving ,like if you provide patience = 10 then it will wait for 10 epochs and if loss does not decrease (in general we use min functions) then and then it will factor the lr while your question was for regardless of loss you want to decrease lr for every 10 epochs.I hope you got the difference..."]}, {"number": 52528, "title": "Convolution with custom operation", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.4\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently `tf.nn.convolution` seems to support only a convolution of the sum-product of the kernel applied to the input. Could this function (or another one with proper naming) support the declaration of a custom application function instead?\r\n\r\nExample: I'd like to convolve a pairwise distance function between the kernel and the input vector. If the part of the input vector currently convolved on is (example with a 1-d input vector):\r\n\r\n```\r\n[10, 20, 30]\r\n```\r\n\r\nAnd the kernel is:\r\n\r\n```\r\n[9, 21, 30]\r\n```\r\n\r\nThen the result of this step of the convolution would be the average of the element-wise distances between input and kernel:\r\n\r\n```\r\n(abs(10-9) + abs(20-21) + abs(30-30)) / 3 = 0.6666\r\n```\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt would likely need adding a new function like `tf.nn.convolution_custom`, or adding an optional argument to the existing `tf.nn.convolution`.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nI can see a use when the network needs to learn about specific sequences or arrangements, when the element-wise values are not ordered. Example: letters. It doesn't make sense to convolve a sumproduct over an input vector if the letters are represented by numbers. However, with this feature, using the operation mentioned above as an example, the kernel could represent a sequence of letters e.g. `[a, m, s]` and it would return `0` if the current input window is also representing `[a, m, s]`, or a positive float number otherwise.\r\n\r\nThis would allow the network to spot sequences of letters such as, if again using the `ams` kernel (conceptually, and assuming a further `1 - distance` operation applied to make an exact match a `1` and a non-match a `0`):\r\n\r\n```\r\nt    h    e      p    o    r    t       o    f       a    m    s    t    e    r    d    a    m\r\n     0    0      0    0    0    0       0    0       0    1    0    0    0    0    0    0\r\n```\r\n\r\nAfter this, normal convolutions could be applied in order to learn patterns.\r\n\r\nI'm using letters as example here but I can imagine many other different applications where the sequence and its ordering matter, but the items are not ordered among themselves (e.g. categorical).\r\n\r\n**Any Other info.**\r\n", "comments": ["Hi JivanRoquet, \r\n\r\ntf.nn.conv* Ops are rather narrower than the general mathematical form of convolution -- the primary reason, to my understanding, is that generally we expect tf.nn shall be lowered efficiently to the corresponding neural network libraries in Cuda and/or XLA that provides accelerated version of the ops.\r\n\r\nFor the 1d case, perhaps you can build on top of `tf.signal`? (Refer to `https://www.tensorflow.org/api_docs/python/tf/signal/frame`)\r\n\r\nHere is a quick doodling I came up with:\r\n```\r\ninput = tf.constant([1, 2, 3, 4, 5, 6, 7], tf.float32)\r\nkernel = tf.constant([3, 4, 5], tf.float32)\r\n\r\nframes = tf.signal.frame(input, 3, 1)\r\ntf.reduce_sum(tf.abs(frames - tf.reshape(kernel, (1, 3))), axis=-1)\r\n```\r\n\r\n```\r\n<tf.Tensor: shape=(5,), dtype=float32, numpy=array([6., 3., 0., 3., 6.], dtype=float32)>\r\n```\r\n", "This is very interesting. Exactly the behaviour I was describing.", "@JivanRoquet If your issue is resolved, can you please close this issue, Thank you."]}, {"number": 52527, "title": "whether there is a timeout value can be set for bazel building download action?", "body": "When I use bazel build tensorflow, which will download a lot of rep, so often timeout and rebuild and timeout agian...\r\nwhether thesre is a timeout value can be set for download action for bazel?", "comments": ["@flowold ,\r\nCan you please refer this [link](https://docs.bazel.build/versions/main/user-manual.html#bazelrc) which provided the more information.It helps.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52527\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52527\">No</a>\n"]}, {"number": 52526, "title": "logistic error in tf.split function", "body": "**tf nightly**\r\n\r\n```\r\n@tf.function\r\ndef test():\r\n    nearest_color_neighbor = tf.constant([\r\n     [0, 3],[0, 4],[0, 6],[0, 7],\r\n     [1, 0],[1, 1],[1, 2],[1, 3],[1, 4],[1, 6],\r\n     [2, 0],[2, 1],[2, 2],[2, 3],[2, 4],[2, 6],\r\n     [3, 3],[3, 4]])\r\n\r\n    unique, _, counts = tf.unique_with_counts(nearest_color_neighbor[:, 0])\r\n\r\n    # ValueError: Cannot infer num from shape Tensor(\"UniqueWithCounts:2\", shape=(None,), dtype=int32)\r\n    splits = tf.split(nearest_color_neighbor[:, -1], counts)\r\n\r\n    # TypeError: Expected int for argument 'num_split' not <tf.Tensor 'Shape:0' shape=(1,) dtype=int32>.\r\n    splits = tf.split(nearest_color_neighbor[:, -1], counts, num=tf.shape(counts)) # error\r\n\r\n    # output:\r\n    # [<tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 4, 6, 7], dtype=int32)>,\r\n    #  <tf.Tensor: shape=(6,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 6], dtype=int32)>,\r\n    #  <tf.Tensor: shape=(6,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 6], dtype=int32)>,\r\n    #  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4], dtype=int32)>]\r\n    splits = tf.split(nearest_color_neighbor[:, -1], counts, num=4)\r\n```\r\n\r\n1. What's the type of `num` parameter for tf.split in https://tensorflow.google.cn/api_docs/python/tf/split?hl=en#args. Can it be of the type 'tensor' or just the 'int' type?\r\n2. If the number of  split output can only be inferred from the context, I mean the **tf.shape(counts)** in the above example, then how to revise the code?", "comments": ["Please do not tell me to remove the **@tf.function** because I have to use it in the dynamic **graph** . Why the **num** parameter for **tf.split**  could only be **int** instead of **tensor**?", "Hi @Saduf2019! Could you look at this issue! This Issue is replicating in [2.5 ](https://colab.research.google.com/gist/mohantym/35245f182df3acf813eafd1e6dbe41d6/github_52526_2-6.ipynb#scrollTo=pqnNAIaR7Dyb),[2.6 ](https://colab.research.google.com/gist/mohantym/6a096cb4cd89459b26f333deb86f30b8/github_52526_2-6.ipynb#scrollTo=pqnNAIaR7Dyb)and [nightly](https://colab.research.google.com/gist/mohantym/6fe766ee098da775b57eafda5d7ec458/github_52526_2-6.ipynb#scrollTo=pqnNAIaR7Dyb). ", "@Saduf2019 Hi, this problem has been posted for about 10 days,yet without any reply. I want to make sure whether you just forgot this issue, or this issue is not meaningful at all", "Hi, I have the same issue. Have you solved this? @sjtusmartboy ", "Since you are running it in graph mode, try to provide the specific values or the values generated before graph mode.\r\nYou can try changing the below `tf.split` parameters to the allowed formats mentioned here.\r\n`num_or_size_splits: Either an integer indicating the number of splits along axis or a 1-D integer Tensor or Python list containing the sizes of each output tensor along axis. If a scalar, then it must evenly divide value.shape[axis]; otherwise the sum of sizes along the split axis must match that of the value.`\r\n\r\n\r\n`\r\nnum : Optional, used to specify the number of outputs when it cannot be inferred from the shape of\u00a0size_splits.`\r\n\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52526\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52526\">No</a>\n", "Having the same issue. Can we reopen this issue?"]}, {"number": 52524, "title": "[tensorflow/compiler/xla/service/convolution_group_converter.cc] Add calls to `reserve()` before populating vectors", "body": "https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)", "comments": ["I think that you could group your change with 1 PR x folder. Eg 1 PR for all the changes for the `xla` folder. \r\nIf you still have too much PRs I think that you could open 1 PR for the whole parent folder `compiler`"]}, {"number": 52523, "title": "[tensorflow/compiler/xla/client/lib/comparators_test.cc] Add calls to `reserve()` before populating vectors", "body": "https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)", "comments": ["I think that you could group your change with 1 PR x folder. Eg 1 PR for all the changes for the `xla` folder. \r\nIf you still have too much PRs I think that you could open 1 PR for the whole parent folder `compiler`"]}]