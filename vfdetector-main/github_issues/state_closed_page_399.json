[{"number": 41976, "title": "TF 2.4.0 build from source gets InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid.", "body": "Nvidia-SMI command issued from inside the container \r\nNVIDIA-SMI 450.57       Driver Version: 450.57       CUDA Version: ERR!     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 107...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   33C    P8     6W / 180W |    193MiB /  8117MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|============================================================================\r\n\r\nI am running Ubuntu 20.04. I followed the instructions to Build from source:\r\n\r\nAfter I compiled TF inside the container, I committed and saved it. \r\n\r\nI run the following commands to load the image and execute jupyter notebook:\r\ndocker run --gpus all --ipc=\"host\" -it -w /tensorflow -v $PWD:/mnt -p 8888:8888 -e HOST_PERMS=\"$(id -u):$(id -g)\" tensorflow/tensorflow:from-src2 bash \r\nexport LD_LIBRARY_PATH=\u201c/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/include/x64_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\u201d\r\npip install jupyter\r\npip install jupyter_http_over_ws\r\njupyter serverextension enable --py jupyter_http_over_ws\r\njupyter notebook --no-browser --notebook-dir=/mnt/notebooks --ip=0.0.0.0  --debug --NotebookApp.allow_origin='https://www.example.com' --NotebookApp.allow_remote_access=True --allow-root\r\n\r\nThis gets me a running notebook server.\r\nI try to run the tensorflow-tutorials/text_classification.ipynb file\r\n\r\nWhen I ran the: \r\n\r\nraw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\r\n    'aclImdb/train', \r\n    batch_size=batch_size, \r\n    validation_split=0.2, \r\n    subset='training', \r\n    seed=seed)\r\n\r\nIn the jupyter notebook, I get: \r\n**TypeError: Could not build a TypeSpec for ['aclImdb/train/neg/4932_4.txt', \r\n [there follows many pages of text similar to the above]...**\r\n\r\nThen I get the following \r\n\r\n**with type list\r\n\r\nDuring handling of the above exception, another exception occurred:**\r\n\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-10-09c13e5c92d7> in <module>\r\n      7     validation_split=0.2,\r\n      8     subset='training',\r\n----> 9     seed=seed)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/text_dataset.py in text_dataset_from_directory(directory, labels, label_mode, class_names, batch_size, max_length, shuffle, seed, validation_split, subset, follow_links)\r\n    159       label_mode=label_mode,\r\n    160       num_classes=len(class_names),\r\n--> 161       max_length=max_length)\r\n    162   if shuffle:\r\n\r\n163     # Shuffle locally at each iteration\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/text_dataset.py in paths_and_labels_to_dataset(file_paths, labels, label_mode, num_classes, max_length)\r\n    175                                 max_length):\r\n    176   \"\"\"Constructs a dataset of text strings and labels.\"\"\"\r\n--> 177   path_ds = dataset_ops.Dataset.from_tensor_slices(file_paths)\r\n    178   string_ds = path_ds.map(\r\n    179       lambda x: path_to_string_content(x, max_length))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in from_tensor_slices(tensors)\r\n    680       Dataset: A `Dataset`.\r\n    681     \"\"\"\r\n--> 682     return TensorSliceDataset(tensors)\r\n    683 \r\n    684   class _GeneratorState(object):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, element)\r\n   2999   def __init__(self, element):\r\n   3000     \"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\r\n-> 3001     element = structure.normalize_element(element)\r\n   3002     batched_spec = structure.type_spec_from_value(element)\r\n   3003     self._tensors = structure.to_batched_tensor_list(batched_spec, element)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py in normalize_element(element)\r\n     96         # the value. As a fallback try converting the value to a tensor.\r\n     97         normalized_components.append(\r\n---> 98             ops.convert_to_tensor(t, name=\"component_%d\" % i))\r\n     99       else:\r\n    100         if isinstance(spec, sparse_tensor.SparseTensorSpec):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n   1524 \r\n   1525     if ret is None:\r\n-> 1526       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1527 \r\n   1528     if ret is NotImplemented:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    337                                          as_ref=False):\r\n    338   _ = as_ref\r\n--> 339   return constant(v, dtype=dtype, name=name)\r\n    340 \r\n    341 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n    263   \"\"\"\r\n    264   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 265                         allow_broadcast=True)\r\n    266 \r\n    267 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    274       with trace.Trace(\"tf.constant\"):\r\n    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n--> 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n    277 \r\n    278   g = ops.get_default_graph()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):\r\n    300   \"\"\"Implementation of eager constant.\"\"\"\r\n--> 301   t = convert_to_eager_tensor(value, ctx, dtype)\r\n    302   if shape is None:\r\n    303     return t\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n     95     except AttributeError:\r\n     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum\r\n---> 97   ctx.ensure_initialized()\r\n     98   return ops.EagerTensor(value, ctx.device_name, dtype)\r\n     99 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in ensure_initialized(self)\r\n    547         if self._use_tfrt is not None:\r\n    548           pywrap_tfe.TFE_ContextOptionsSetTfrt(opts, self._use_tfrt)\r\n--> 549         context_handle = pywrap_tfe.TFE_NewContext(opts)\r\n    550       finally:\r\n    551         pywrap_tfe.TFE_DeleteContextOptions(opts)\r\n\r\n**InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid.**\r\n\r\nThis from /tensorflow_src/.bazelrc : release_gpu_common --action_env=TF_CUDA_COMPUTE_CAPABILITIES=\"sm_35,sm_37,sm_52,sm_60,sm_61,compute_70\r\n\r\nI believe the GeForce 1070 is sm_61 compute level.\r\n\r\nSome software versions\r\ngcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nPython 3.6.9\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\r\nCuda compilation tools, release 10.1, V10.1.243\r\n\r\n\r\n\r\n<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nI will attach the full .bazelrc file and a piped output of the build from source when I can figure out how to do that. I\u2019m on an iPad now and can copy and paste but can\u2019t seem to figure out how to copy a file to the ipad and then upload to github issue...", "comments": ["\r\n[BazelRC.txt](https://github.com/tensorflow/tensorflow/files/5011355/BazelRC.txt)\r\nI thought I had kept the build pipe around but apparently didn't so don't have that file. If you need any more information please let me know. \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41976\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41976\">No</a>\n", "Sorry closed the issue when I just wanted to add my comment. I re-opened ", "I\u2019m using Nvidia-MPS so this is the nvidia-smi from outside the container. \r\n NVIDIA-SMI 450.57       Driver Version: 450.57       CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 107...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   32C    P8     6W / 180W |    193MiB /  8117MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 1070    Off  | 00000000:05:00.0 Off |                  N/A |\r\n|  0%   29C    P8     5W / 166W |      7MiB /  8119MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 1070    Off  | 00000000:06:00.0 Off |                  N/A |\r\n|  0%   29C    P8     4W / 166W |      7MiB /  8119MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      1056      G   /usr/lib/xorg/Xorg                 36MiB |\r\n|    0   N/A  N/A      1501      G   /usr/bin/gnome-shell               15MiB |\r\n|    0   N/A  N/A      2205      C   /usr/bin/python3                  137MiB |\r\n|    1   N/A  N/A      1056      G   /usr/lib/xorg/Xorg                  4MiB |\r\n|    2   N/A  N/A      1056      G   /usr/lib/xorg/Xorg                  4MiB \r\n\r\n I only let 1 GPU into the container.", "Here\u2019s the trace back from the 1st error (type error)\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py in normalize_element(element)\r\n     92       try:\r\n---> 93         spec = type_spec_from_value(t, use_fallback=False)\r\n     94       except TypeError:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py in type_spec_from_value(element, use_fallback)\r\n    465   raise TypeError(\"Could not build a TypeSpec for %r with type %s\" %\r\n--> 466                   (element, type(element).__name__))\r\n    467 ", "This from inside container:\r\nBazel label: 3.1.0\r\nUbuntu version: 18.04\r\nc++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0 \r\nnumpy                  1.18.5                                                                                                                                 \r\nprotobuf               3.12.2                                                                                                                                 \r\ntensorflow             2.4.0                                                                                                                                  \r\ntensorflow-estimator   2.2.0 \r\ntf.version.VERSION = 2.4.0\r\ntf.version.GIT_VERSION = v1.12.1-37438-g6711d96f6c\r\ntf.version.COMPILER_VERSION = 7.5.0\r\npython version: 3.6.9\r\npython compiler version: GCC 8.4.0\r\n\r\nIn Docker: Yes\r\nCheck for Virtual Environment: False\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart.so.10.1.243\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart_static.a\r\n\r\nIf you need anything else out of tf_env.txt let me know.\r\n", "could this be some sort of timeout. I'm running \r\nJupyter Notebook 6.1.0\r\n\r\nWhen I tried persistence mode on the GPU it seemed to transfer more data before it generated the error. it went from 135MiB of data assigned to python3 at time of error (with persistence mode OFF) to 189MiB of data assigned to Python3 at time of error (with persistence mode ON).  ", "Very strange, when I reran the jupyter server and tried to run the same notebook (a 2nd time) within the same container (but adding a --MappingKernelManager.cull_interval=1200 to the server command. The notebook ran just fine and much faster. I do have persistence mode on... I don\u2019t think the cull-interval had anything to do with it. \r\n\r\nIs it possible that the first load of the Nvidia GPU takes a long time because we are loading in some standard NVidia and Tensorflow routines and modules but once they are present in the GPU they no longer have to be loaded (as long as persistence mode is on). That could explain why the first time I ran the text classification notebook it took so long (and ultimately errored off) but the 2nd time it flew right threw that statement.", "Here is some other information from the debug run of the jupyter notebook:\r\n2020-08-04 04:45:30.731142: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-04 04:45:30.731178: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-08-04 04:45:30.731193: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-08-04 04:45:30.731244: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-08-04 04:45:30.731263: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-04 04:45:30.731281: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-04 04:45:30.731295: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n\r\nSeems to be loading some files for cuda 10.1 (libcudart) and others from cuda 10.0 (libcublas, libcufft, libcurand, libcusolver, & libcusparse)\r\n\r\nI tried to recreate the sequence that worked last night but can\u2019t seem to get it to work tonight.  Any time I Ctrl-c the jupyter notebook server it seems to clear GPU memory so I get a fresh start. It seems last night it left it alone and I was able to start another jupyter notebook server session and it worked ok the second time I ran it. Sigh, that\u2019s not working at all today.", "sorry i updated the wrong incident. Here's the \"lshw\" command output:\r\nubuntu-ai                   \r\n    description: Desktop Computer\r\n    product: MS-7A62 (Default string)\r\n    vendor: MSI\r\n    version: 1.0\r\n    serial: Default string\r\n    width: 64 bits\r\n    capabilities: smbios-3.0.0 dmi-3.0.0 smp vsyscall32\r\n    configuration: boot=normal chassis=desktop family=Default string sku=Default string uuid=00000000-0000-0000-0000-4CCC6AF7447F\r\n  *-core\r\n       description: Motherboard\r\n       product: Z270 GAMING M3 (MS-7A62)\r\n       vendor: MSI\r\n       physical id: 0\r\n       version: 1.0\r\n       serial: H316061861\r\n       slot: Default string\r\n     *-firmware\r\n          description: BIOS\r\n          vendor: American Megatrends Inc.\r\n          physical id: 0\r\n          version: 1.10\r\n          date: 02/07/2017\r\n          size: 64KiB\r\n          capacity: 16MiB\r\n          capabilities: pci upgrade shadowing cdboot bootselect socketedrom edd int13floppy1200 int13floppy720 int13floppy2880 int5printscreen int9keyboard int14serial int17printer acpi usb biosbootspecification uefi\r\n     *-memory\r\n          description: System Memory\r\n          physical id: 3c\r\n          slot: System board or motherboard\r\n          size: 8GiB\r\n        *-bank:0\r\n             description: [empty]\r\n             physical id: 0\r\n             slot: ChannelA-DIMM0\r\n        *-bank:1\r\n             description: DIMM DDR4 Synchronous 2400 MHz (0.4 ns)\r\n             product: BLS4G4D240FSB.8FBD2\r\n             vendor: 859B\r\n             physical id: 1\r\n             serial: A624BDFD\r\n             slot: ChannelA-DIMM1\r\n             size: 4GiB\r\n             width: 64 bits\r\n             clock: 2400MHz (0.4ns)\r\n        *-bank:2\r\n             description: [empty]\r\n             physical id: 2\r\n             slot: ChannelB-DIMM0\r\n        *-bank:3\r\n             description: DIMM DDR4 Synchronous 2400 MHz (0.4 ns)\r\n             product: BLS4G4D240FSB.8FBD2\r\n             vendor: 859B\r\n             physical id: 3\r\n             serial: A624BDFC\r\n             slot: ChannelB-DIMM1\r\n             size: 4GiB\r\n             width: 64 bits\r\n             clock: 2400MHz (0.4ns)\r\n     *-cache:0\r\n          description: L1 cache\r\n          physical id: 42\r\n          slot: L1 Cache\r\n          size: 128KiB\r\n          capacity: 128KiB\r\n          capabilities: synchronous internal write-back unified\r\n          configuration: level=1\r\n     *-cache:1\r\n          description: L2 cache\r\n          physical id: 43\r\n          slot: L2 Cache\r\n          size: 512KiB\r\n          capacity: 512KiB\r\n          capabilities: synchronous internal write-back unified\r\n          configuration: level=2\r\n     *-cache:2\r\n          description: L3 cache\r\n          physical id: 44\r\n          slot: L3 Cache\r\n          size: 3MiB\r\n          capacity: 3MiB\r\n          capabilities: synchronous internal write-back unified\r\n          configuration: level=3\r\n     *-cpu\r\n          description: CPU\r\n          product: Intel(R) Pentium(R) CPU G4400 @ 3.30GHz\r\n          vendor: Intel Corp.\r\n          physical id: 45\r\n          bus info: cpu@0\r\n          version: Intel(R) Pentium(R) CPU G4400 @ 3.30GHz\r\n          serial: To Be Filled By O.E.M.\r\n          slot: U3E1\r\n          size: 3206MHz\r\n          capacity: 4005MHz\r\n          width: 64 bits\r\n          clock: 100MHz\r\n          capabilities: lm fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp x86-64 constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave rdrand lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust erms invpcid rdseed smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d cpufreq\r\n          configuration: cores=2 enabledcores=2 threads=2\r\n     *-pci\r\n          description: Host bridge\r\n          product: Xeon E3-1200 v5/E3-1500 v5/6th Gen Core Processor Host Bridge/DRAM Registers\r\n          vendor: Intel Corporation\r\n          physical id: 100\r\n          bus info: pci@0000:00:00.0\r\n          version: 07\r\n          width: 32 bits\r\n          clock: 33MHz\r\n          configuration: driver=skl_uncore\r\n          resources: irq:0\r\n        *-pci:0\r\n             description: PCI bridge\r\n             product: Xeon E3-1200 v5/E3-1500 v5/6th Gen Core Processor PCIe Controller (x16)\r\n             vendor: Intel Corporation\r\n             physical id: 1\r\n             bus info: pci@0000:00:01.0\r\n             version: 07\r\n             width: 32 bits\r\n             clock: 33MHz\r\n             capabilities: pci pm msi pciexpress normal_decode bus_master cap_list\r\n             configuration: driver=pcieport\r\n             resources: irq:120 ioport:e000(size=4096) memory:de000000-df0fffff ioport:c0000000(size=301989888)\r\n           *-display\r\n                description: VGA compatible controller\r\n                product: GP104 [GeForce GTX 1070 Ti]\r\n                vendor: NVIDIA Corporation\r\n                physical id: 0\r\n                bus info: pci@0000:01:00.0\r\n                version: a1\r\n                width: 64 bits\r\n                clock: 33MHz\r\n                capabilities: pm msi pciexpress vga_controller bus_master cap_list rom\r\n                configuration: driver=nvidia latency=0\r\n                resources: irq:137 memory:de000000-deffffff memory:c0000000-cfffffff memory:d0000000-d1ffffff ioport:e000(size=128) memory:c0000-dffff\r\n           *-multimedia\r\n                description: Audio device\r\n                product: GP104 High Definition Audio Controller\r\n                vendor: NVIDIA Corporation\r\n                physical id: 0.1\r\n                bus info: pci@0000:01:00.1\r\n                version: a1\r\n                width: 32 bits\r\n                clock: 33MHz\r\n                capabilities: pm msi pciexpress bus_master cap_list\r\n                configuration: driver=snd_hda_intel latency=0\r\n                resources: irq:17 memory:df080000-df083fff\r\n        *-generic:0 UNCLAIMED\r\n             description: System peripheral\r\n             product: Xeon E3-1200 v5/v6 / E3-1500 v5 / 6th/7th/8th Gen Core Processor Gaussian Mixture Model\r\n             vendor: Intel Corporation\r\n             physical id: 8\r\n             bus info: pci@0000:00:08.0\r\n             version: 00\r\n             width: 64 bits\r\n             clock: 33MHz\r\n             capabilities: msi pm cap_list\r\n             configuration: latency=0\r\n             resources: memory:dd12f000-dd12ffff\r\n        *-usb\r\n             description: USB controller\r\n             product: 200 Series/Z370 Chipset Family USB 3.0 xHCI Controller\r\n             vendor: Intel Corporation\r\n             physical id: 14\r\n             bus info: pci@0000:00:14.0\r\n             version: 00\r\n             width: 64 bits\r\n             clock: 33MHz\r\n             capabilities: pm msi xhci bus_master cap_list\r\n             configuration: driver=xhci_hcd latency=0\r\n             resources: irq:127 memory:dd110000-dd11ffff\r\n           *-usbhost:0\r\n                product: xHCI Host Controller\r\n                vendor: Linux 5.4.0-42-generic xhci-hcd\r\n                physical id: 0\r\n                bus info: usb@1\r\n                logical name: usb1\r\n                version: 5.04\r\n                capabilities: usb-2.00\r\n                configuration: driver=hub slots=16 speed=480Mbit/s\r\n              *-usb:0\r\n                   description: Mouse\r\n                   product: USB Optical Mouse\r\n                   vendor: Logitech\r\n                   physical id: 7\r\n                   bus info: usb@1:7\r\n                   version: 72.00\r\n                   capabilities: usb-2.00\r\n                   configuration: driver=usbhid maxpower=100mA speed=2Mbit/s\r\n              *-usb:1\r\n                   description: Keyboard\r\n                   product: USB Keyboard\r\n                   vendor: Logitech\r\n                   physical id: 8\r\n                   bus info: usb@1:8\r\n                   version: 64.00\r\n                   capabilities: usb-1.10\r\n                   configuration: driver=usbhid maxpower=90mA speed=2Mbit/s\r\n           *-usbhost:1\r\n                product: xHCI Host Controller\r\n                vendor: Linux 5.4.0-42-generic xhci-hcd\r\n                physical id: 1\r\n                bus info: usb@2\r\n                logical name: usb2\r\n                version: 5.04\r\n                capabilities: usb-3.00\r\n                configuration: driver=hub slots=10 speed=5000Mbit/s\r\n        *-generic:1 UNCLAIMED\r\n             description: Signal processing controller\r\n             product: 200 Series PCH Thermal Subsystem\r\n             vendor: Intel Corporation\r\n             physical id: 14.2\r\n             bus info: pci@0000:00:14.2\r\n             version: 00\r\n             width: 64 bits\r\n             clock: 33MHz\r\n             capabilities: pm msi cap_list\r\n             configuration: latency=0\r\n             resources: memory:dd12e000-dd12efff\r\n        *-communication\r\n             description: Communication controller\r\n             product: 200 Series PCH CSME HECI #1\r\n             vendor: Intel Corporation\r\n             physical id: 16\r\n             bus info: pci@0000:00:16.0\r\n             version: 00\r\n             width: 64 bits\r\n             clock: 33MHz\r\n             capabilities: pm msi bus_master cap_list\r\n             configuration: driver=mei_me latency=0\r\n             resources: irq:132 memory:dd12d000-dd12dfff\r\n        *-sata\r\n             description: SATA controller\r\n             product: 200 Series PCH SATA controller [AHCI mode]\r\n             vendor: Intel Corporation\r\n             physical id: 17\r\n             bus info: pci@0000:00:17.0\r\n             logical name: scsi0\r\n             version: 00\r\n             width: 32 bits\r\n             clock: 66MHz\r\n             capabilities: sata msi pm ahci_1.0 bus_master cap_list emulated\r\n             configuration: driver=ahci latency=0\r\n             resources: irq:131 memory:dd128000-dd129fff memory:dd12c000-dd12c0ff ioport:f050(size=8) ioport:f040(size=4) ioport:f020(size=32) memory:dd12b000-dd12b7ff\r\n           *-disk\r\n                description: ATA Disk\r\n                product: SanDisk SDSSDA12\r\n                physical id: 0.0.0\r\n                bus info: scsi@0:0.0.0\r\n                logical name: /dev/sda\r\n                version: 80RL\r\n                serial: 171833459715\r\n                size: 111GiB (120GB)\r\n                capabilities: gpt-1.00 partitioned partitioned:gpt\r\n                configuration: ansiversion=5 guid=725cc7ba-faa9-4fe5-9e62-6b7b8a0b7aa7 logicalsectorsize=512 sectorsize=512\r\n              *-volume:0\r\n                   description: Windows FAT volume\r\n                   vendor: mkfs.fat\r\n                   physical id: 1\r\n                   bus info: scsi@0:0.0.0,1\r\n                   logical name: /dev/sda1\r\n                   logical name: /boot/efi\r\n                   version: FAT32\r\n                   serial: 1773-3e94\r\n                   size: 510MiB\r\n                   capacity: 511MiB\r\n                   capabilities: boot fat initialized\r\n                   configuration: FATs=2 filesystem=fat mount.fstype=vfat mount.options=rw,relatime,fmask=0022,dmask=0022,codepage=437,iocharset=iso8859-1,shortname=mixed,errors=remount-ro state=mounted\r\n              *-volume:1\r\n                   description: EXT4 volume\r\n                   vendor: Linux\r\n                   physical id: 2\r\n                   bus info: scsi@0:0.0.0,2\r\n                   logical name: /dev/sda2\r\n                   logical name: /boot\r\n                   version: 1.0\r\n                   serial: af28e231-0e6d-41f6-a632-b96cc017ffba\r\n                   size: 1GiB\r\n                   capabilities: journaled extended_attributes large_files huge_files dir_nlink recover 64bit extents ext4 ext2 initialized\r\n                   configuration: created=2020-07-22 19:01:51 filesystem=ext4 lastmountpoint=/boot modified=2020-08-03 17:23:18 mount.fstype=ext4 mount.options=rw,relatime mounted=2020-08-03 17:23:18 state=mounted\r\n              *-volume:2\r\n                   description: EFI partition\r\n                   physical id: 3\r\n                   bus info: scsi@0:0.0.0,3\r\n                   logical name: /dev/sda3\r\n                   serial: PYSAzy-ypr8-pyaN-oJcD-y4NN-YdMJ-hPwFpv\r\n                   size: 110GiB\r\n                   capabilities: lvm2\r\n        *-pci:1\r\n             description: PCI bridge\r\n             product: 200 Series PCH PCI Express Root Port #17\r\n             vendor: Intel Corporation\r\n             physical id: 1b\r\n             bus info: pci@0000:00:1b.0\r\n             version: f0\r\n             width: 32 bits\r\n             clock: 33MHz\r\n             capabilities: pci pciexpress msi pm normal_decode bus_master cap_list\r\n             configuration: driver=pcieport\r\n             resources: irq:121\r\n        *-pci:2\r\n             description: PCI bridge\r\n             product: 200 Series PCH PCI Express Root Port #19\r\n             vendor: Intel Corporation\r\n             physical id: 1b.2\r\n             bus info: pci@0000:00:1b.2\r\n             version: f0\r\n             width: 32 bits\r\n             clock: 33MHz\r\n             capabilities: pci pciexpress msi pm normal_decode bus_master cap_list\r\n             configuration: driver=pcieport\r\n             resources: irq:122 memory:df300000-df3fffff\r\n           *-usb\r\n                description: USB controller\r\n                product: ASM2142 USB 3.1 Host Controller\r\n                vendor: ASMedia Technology Inc.\r\n                physical id: 0\r\n                bus info: pci@0000:03:00.0\r\n                version: 00\r\n                width: 64 bits\r\n                clock: 33MHz\r\n                capabilities: msi msix pm pciexpress xhci bus_master cap_list\r\n                configuration: driver=xhci_hcd latency=0\r\n                resources: irq:18 memory:df300000-df307fff\r\n              *-usbhost:0\r\n                   product: xHCI Host Controller\r\n                   vendor: Linux 5.4.0-42-generic xhci-hcd\r\n                   physical id: 0\r\n                   bus info: usb@3\r\n                   logical name: usb3\r\n                   version: 5.04\r\n                   capabilities: usb-2.00\r\n                   configuration: driver=hub slots=2 speed=480Mbit/s\r\n              *-usbhost:1\r\n                   product: xHCI Host Controller\r\n                   vendor: Linux 5.4.0-42-generic xhci-hcd\r\n                   physical id: 1\r\n                   bus info: usb@4\r\n                   logical name: usb4\r\n                   version: 5.04\r\n                   capabilities: usb-3.10\r\n                   configuration: driver=hub slots=2 speed=10000Mbit/s\r\n        *-pci:3\r\n             description: PCI bridge\r\n             product: 200 Series PCH PCI Express Root Port #1\r\n             vendor: Intel Corporation\r\n             physical id: 1c\r\n             bus info: pci@0000:00:1c.0\r\n             version: f0\r\n             width: 32 bits\r\n             clock: 33MHz\r\n             capabilities: pci pciexpress msi pm normal_decode bus_master cap_list\r\n             configuration: driver=pcieport\r\n             resources: irq:123\r\n        *-pci:4\r\n             description: PCI bridge\r\n             product: 200 Series PCH PCI Express Root Port #6\r\n             vendor: Intel Corporation\r\n             physical id: 1c.5\r\n             bus info: pci@0000:00:1c.5\r\n             version: f0\r\n             width: 32 bits\r\n             clock: 33MHz\r\n             capabilities: pci pciexpress msi pm normal_decode bus_master cap_list\r\n             configuration: driver=pcieport\r\n             resources: irq:124 ioport:d000(size=4096) memory:dc000000-dd0fffff ioport:a0000000(size=301989888)\r\n           *-display\r\n                description: VGA compatible controller\r\n                product: GP104 [GeForce GTX 1070]\r\n                vendor: NVIDIA Corporation\r\n                physical id: 0\r\n                bus info: pci@0000:05:00.0\r\n                version: a1\r\n                width: 64 bits\r\n                clock: 33MHz\r\n                capabilities: pm msi pciexpress vga_controller bus_master cap_list rom\r\n                configuration: driver=nvidia latency=0\r\n                resources: irq:138 memory:dc000000-dcffffff memory:a0000000-afffffff memory:b0000000-b1ffffff ioport:d000(size=128) memory:dd000000-dd07ffff\r\n           *-multimedia\r\n                description: Audio device\r\n                product: GP104 High Definition Audio Controller\r\n                vendor: NVIDIA Corporation\r\n                physical id: 0.1\r\n                bus info: pci@0000:05:00.1\r\n                version: a1\r\n                width: 32 bits\r\n                clock: 33MHz\r\n                capabilities: pm msi pciexpress bus_master cap_list\r\n                configuration: driver=snd_hda_intel latency=0\r\n                resources: irq:18 memory:dd080000-dd083fff\r\n        *-pci:5\r\n             description: PCI bridge\r\n             product: 200 Series PCH PCI Express Root Port #7\r\n             vendor: Intel Corporation\r\n             physical id: 1c.6\r\n             bus info: pci@0000:00:1c.6\r\n             version: f0\r\n             width: 32 bits\r\n             clock: 33MHz\r\n             capabilities: pci pciexpress msi pm normal_decode bus_master cap_list\r\n             configuration: driver=pcieport\r\n             resources: irq:125 ioport:c000(size=4096) memory:da000000-db0fffff ioport:80000000(size=301989888)\r\n           *-display\r\n                description: VGA compatible controller\r\n                product: GP104 [GeForce GTX 1070]\r\n                vendor: NVIDIA Corporation\r\n                physical id: 0\r\n                bus info: pci@0000:06:00.0\r\n                version: a1\r\n                width: 64 bits\r\n                clock: 33MHz\r\n                capabilities: pm msi pciexpress vga_controller bus_master cap_list rom\r\n                configuration: driver=nvidia latency=0\r\n                resources: irq:139 memory:da000000-daffffff memory:80000000-8fffffff memory:90000000-91ffffff ioport:c000(size=128) memory:db000000-db07ffff\r\n           *-multimedia\r\n                description: Audio device\r\n                product: GP104 High Definition Audio Controller\r\n                vendor: NVIDIA Corporation\r\n                physical id: 0.1\r\n                bus info: pci@0000:06:00.1\r\n                version: a1\r\n                width: 32 bits\r\n                clock: 33MHz\r\n                capabilities: pm msi pciexpress bus_master cap_list\r\n                configuration: driver=snd_hda_intel latency=0\r\n                resources: irq:19 memory:db080000-db083fff\r\n        *-pci:6\r\n             description: PCI bridge\r\n             product: 200 Series PCH PCI Express Root Port #8\r\n             vendor: Intel Corporation\r\n             physical id: 1c.7\r\n             bus info: pci@0000:00:1c.7\r\n             version: f0\r\n             width: 32 bits\r\n             clock: 33MHz\r\n             capabilities: pci pciexpress msi pm normal_decode bus_master cap_list\r\n             configuration: driver=pcieport\r\n             resources: irq:126 ioport:b000(size=4096) memory:df200000-df2fffff\r\n           *-network\r\n                description: Ethernet interface\r\n                product: Killer E2500 Gigabit Ethernet Controller\r\n                vendor: Qualcomm Atheros\r\n                physical id: 0\r\n                bus info: pci@0000:07:00.0\r\n                logical name: enp7s0\r\n                version: 10\r\n                serial: 4c:cc:6a:f7:44:7f\r\n                size: 1Gbit/s\r\n                capacity: 1Gbit/s\r\n                width: 64 bits\r\n                clock: 33MHz\r\n                capabilities: pm pciexpress msi msix bus_master cap_list ethernet physical tp 10bt 10bt-fd 100bt 100bt-fd 1000bt-fd autonegotiation\r\n                configuration: autonegotiation=on broadcast=yes driver=alx duplex=full ip=10.0.0.184 latency=0 link=yes multicast=yes port=twisted pair speed=1Gbit/s\r\n                resources: irq:19 memory:df200000-df23ffff ioport:b000(size=128)\r\n        *-isa\r\n             description: ISA bridge\r\n             product: 200 Series PCH LPC Controller (Z270)\r\n             vendor: Intel Corporation\r\n             physical id: 1f\r\n             bus info: pci@0000:00:1f.0\r\n             version: 00\r\n             width: 32 bits\r\n             clock: 33MHz\r\n             capabilities: isa bus_master\r\n             configuration: latency=0\r\n        *-memory UNCLAIMED\r\n             description: Memory controller\r\n             product: 200 Series/Z370 Chipset Family Power Management Controller\r\n             vendor: Intel Corporation\r\n             physical id: 1f.2\r\n             bus info: pci@0000:00:1f.2\r\n             version: 00\r\n             width: 32 bits\r\n             clock: 33MHz (30.3ns)\r\n             configuration: latency=0\r\n             resources: memory:dd124000-dd127fff\r\n        *-multimedia\r\n             description: Audio device\r\n             product: 200 Series PCH HD Audio\r\n             vendor: Intel Corporation\r\n             physical id: 1f.3\r\n             bus info: pci@0000:00:1f.3\r\n             version: 00\r\n             width: 64 bits\r\n             clock: 33MHz\r\n             capabilities: pm msi bus_master cap_list\r\n             configuration: driver=snd_hda_intel latency=32\r\n             resources: irq:133 memory:dd120000-dd123fff memory:dd100000-dd10ffff\r\n        *-serial\r\n             description: SMBus\r\n             product: 200 Series/Z370 Chipset Family SMBus Controller\r\n             vendor: Intel Corporation\r\n             physical id: 1f.4\r\n             bus info: pci@0000:00:1f.4\r\n             version: 00\r\n             width: 64 bits\r\n             clock: 33MHz\r\n             configuration: driver=i801_smbus latency=0\r\n             resources: irq:16 memory:dd12a000-dd12a0ff ioport:f000(size=32)\r\n     *-pnp00:00\r\n          product: PnP device PNP0c02\r\n          physical id: 1\r\n          capabilities: pnp\r\n          configuration: driver=system\r\n     *-pnp00:01\r\n          product: PnP device PNP0303\r\n          physical id: 2\r\n          capabilities: pnp\r\n          configuration: driver=i8042 kbd\r\n     *-pnp00:02\r\n          product: PnP device PNP0f03\r\n          physical id: 3\r\n          capabilities: pnp\r\n          configuration: driver=i8042 aux\r\n     *-pnp00:03\r\n          product: PnP device PNP0501\r\n          physical id: 4\r\n          capabilities: pnp\r\n          configuration: driver=serial\r\n     *-pnp00:04\r\n          product: PnP device PNP0c02\r\n          physical id: 5\r\n          capabilities: pnp\r\n          configuration: driver=system\r\n     *-pnp00:05\r\n          product: PnP device PNP0c02\r\n          physical id: 6\r\n          capabilities: pnp\r\n          configuration: driver=system\r\n     *-pnp00:06\r\n          product: PnP device PNP0b00\r\n          physical id: 7\r\n          capabilities: pnp\r\n          configuration: driver=rtc_cmos\r\n     *-pnp00:07\r\n          product: PnP device INT3f0d\r\n          physical id: 8\r\n          capabilities: pnp\r\n          configuration: driver=system\r\n     *-pnp00:08\r\n          product: PnP device PNP0c02\r\n          physical id: 9\r\n          capabilities: pnp\r\n          configuration: driver=system\r\n     *-pnp00:09\r\n          product: PnP device PNP0c02\r\n          physical id: a\r\n          capabilities: pnp\r\n          configuration: driver=system\r\n     *-pnp00:0a\r\n          product: PnP device PNP0c02\r\n          physical id: b\r\n          capabilities: pnp\r\n          configuration: driver=system\r\n     *-pnp00:0b\r\n          product: PnP device PNP0c02\r\n          physical id: c\r\n          capabilities: pnp\r\n          configuration: driver=system\r\n  *-power UNCLAIMED\r\n       description: To Be Filled By O.E.M.\r\n       product: To Be Filled By O.E.M.\r\n       vendor: To Be Filled By O.E.M.\r\n       physical id: 1\r\n       version: To Be Filled By O.E.M.\r\n       serial: To Be Filled By O.E.M.\r\n       capacity: 32768mWh\r\n  *-network:0\r\n       description: Ethernet interface\r\n       physical id: 2\r\n       logical name: vetha11e727\r\n       serial: 7e:40:7f:0a:04:8c\r\n       size: 10Gbit/s\r\n       capabilities: ethernet physical\r\n       configuration: autonegotiation=off broadcast=yes driver=veth driverversion=1.0 duplex=full link=yes multicast=yes port=twisted pair speed=10Gbit/s\r\n  *-network:1\r\n       description: Ethernet interface\r\n       physical id: 3\r\n       logical name: docker0\r\n       serial: 02:42:69:a5:23:2a\r\n       capabilities: ethernet physical\r\n       configuration: broadcast=yes driver=bridge driverversion=2.3 firmware=N/A ip=172.17.0.1 link=yes multicast=yes\r\n", "here's the \"lspci\" command output:\r\nlspci\r\n00:00.0 Host bridge: Intel Corporation Xeon E3-1200 v5/E3-1500 v5/6th Gen Core Processor Host Bridge/DRAM Registers (rev 07)\r\n00:01.0 PCI bridge: Intel Corporation Xeon E3-1200 v5/E3-1500 v5/6th Gen Core Processor PCIe Controller (x16) (rev 07)\r\n00:08.0 System peripheral: Intel Corporation Xeon E3-1200 v5/v6 / E3-1500 v5 / 6th/7th/8th Gen Core Processor Gaussian Mixture Model\r\n00:14.0 USB controller: Intel Corporation 200 Series/Z370 Chipset Family USB 3.0 xHCI Controller\r\n00:14.2 Signal processing controller: Intel Corporation 200 Series PCH Thermal Subsystem\r\n00:16.0 Communication controller: Intel Corporation 200 Series PCH CSME HECI #1\r\n00:17.0 SATA controller: Intel Corporation 200 Series PCH SATA controller [AHCI mode]\r\n00:1b.0 PCI bridge: Intel Corporation 200 Series PCH PCI Express Root Port #17 (rev f0)\r\n00:1b.2 PCI bridge: Intel Corporation 200 Series PCH PCI Express Root Port #19 (rev f0)\r\n00:1c.0 PCI bridge: Intel Corporation 200 Series PCH PCI Express Root Port #1 (rev f0)\r\n00:1c.5 PCI bridge: Intel Corporation 200 Series PCH PCI Express Root Port #6 (rev f0)\r\n00:1c.6 PCI bridge: Intel Corporation 200 Series PCH PCI Express Root Port #7 (rev f0)\r\n00:1c.7 PCI bridge: Intel Corporation 200 Series PCH PCI Express Root Port #8 (rev f0)\r\n00:1f.0 ISA bridge: Intel Corporation 200 Series PCH LPC Controller (Z270)\r\n00:1f.2 Memory controller: Intel Corporation 200 Series/Z370 Chipset Family Power Management Controller\r\n00:1f.3 Audio device: Intel Corporation 200 Series PCH HD Audio\r\n00:1f.4 SMBus: Intel Corporation 200 Series/Z370 Chipset Family SMBus Controller\r\n01:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070 Ti] (rev a1)\r\n01:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)\r\n03:00.0 USB controller: ASMedia Technology Inc. ASM2142 USB 3.1 Host Controller\r\n05:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1)\r\n05:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)\r\n06:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1)\r\n06:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)\r\n07:00.0 Ethernet controller: Qualcomm Atheros Killer E2500 Gigabit Ethernet Controller (rev 10)", "If you need anything else let me know. I\u2019m only passing one gpu into Tensorflow and thats the 1070Ti", "The exact steps I took to generate the problem:\r\n\r\nFirst I built from source for GPU following the instructions at the bottom of https://www.tensorflow.org/install/source \r\n\r\nsudo nvidia-smi -pm 1\r\ndocker run --gpus 1 --ipc=\"host\" -it -w /tensorflow -v $PWD:/mnt -p 8888:8888 -e HOST_PERMS=\"$(id -u):$(id -g)\" tensorflow/tensorflow:from-src2 bash \r\nexport LD_LIBRARY_PATH=\u201c/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/include/x64_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\u201d\r\npip install jupyter\r\npip install jupyter_http_over_ws\r\njupyter serverextension enable --py jupyter_http_over_ws\r\njupyter notebook --no-browser --notebook-dir=/mnt/notebooks --ip=0.0.0.0 --MappingKernelManager.cull_interval=1200 --debug --NotebookApp.allow_origin='https://www.example.com' --NotebookApp.allow_remote_access=True --allow-root\r\n", "@RayLucchesi TF 2.4 version is a nightly version which is unstable. Perhaps you may want to try with latest stable TF 2.3 version for building with source?", "@ymodak I was just following instructions on the website I indicated above. I'd like to try to rebuild from source from TF2.2 as TF 2.3 seems to have a similar problem.\r\n\r\n How would one build with a different level than what's current?   I guess use \"git fetch\" with either \"git merge v2.2\" or \"git merge v2.3\" instead of \"git pull\" as per the instructions in https://www.tensorflow.org/install/source?", "maybe use \"git fetch\" with \"git merge v2.2.0-rc4\"?\r\n\r\nor can I just do \"git pull refs/tag-/v2.2.0-rc4\"?", "@RayLucchesi \"git checkout v2.3.0\", for example - see [https://www.tensorflow.org/install/source#download_the_tensorflow_source_code](https://www.tensorflow.org/install/source#download_the_tensorflow_source_code).", "Did the git checkout \u201cgit checkout v2.3.0\u201d in the tensorflow_src directory and it came back with \u201c error: pathspec 'v2.3.0' did not match any file(s) known to git.\u201d\r\n\r\nIt seems I had to do a git fetch first", "Can you try building for 2.2?\r\n`$ git checkout v2.2.0`", "i tried to build from source using  v2.3.0 and the build failed but I think it ran out of space. \r\n\r\nI cleaned up some files and tried to build from source using git checkout v2.2.0 and the ./configure command failed with \r\n\r\n./configure\r\nTraceback (most recent call last):\r\n  File \"./configure.py\", line 1551, in <module>\r\n    main()\r\n  File \"./configure.py\", line 1368, in main\r\n    _TF_MAX_BAZEL_VERSION)\r\n  File \"./configure.py\", line 483, in check_bazel_version\r\n    ['bazel', '--batch', '--bazelrc=/dev/null', 'version'])\r\n  File \"./configure.py\", line 159, in run_shell\r\n    output = subprocess.check_output(cmd, stderr=stderr)\r\n  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\r\n    **kwargs).stdout\r\n  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\r\n    output=stdout, stderr=stderr)\r\nsubprocess.CalledProcessError: Command '['bazel', '--batch', '--bazelrc=/dev/null', 'version']' returned non-zero exit status 1.\r\n\r\nSo I'm going to retry build from source using v2.3.0 again...\r\nRealize that every time I build from source it's a 11+ hour operation.\r\n\r\nAlso not sure what to make of \"stu1130 added a commit to awslabs/djl that referenced this issue 5 hours ago\" comment above is this a fix to the problem in v2.3?\r\n", "So after another evening of building from source this time with V2.3.0. I get through that ok (after clearing out some used and unused container images) and then I do the build whl and that works and then I pip uninstall tensorflow and then pip install tensorflow-2.3.0-cp36-cp36m-linux_x86_64.whl and that seems to work. \r\n\r\nFollowing the instructions in the build from source website I try a simple python program to print out the #GPUs. \r\npython -c \"import tensorflow as tf; print(\\\"Num GPUs Available: \\\", len(tf.config.experimental.list_physical_devices('GPU')))\"\r\nwhich fails as follows:\r\n\r\npython -c \"import tensorflow as tf; print(\\\"Num GPUs Available: \\\", len(tf.config.experimental.list_physical_devices('GPU')))\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/tensorflow_src/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/tensorflow_src/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/tensorflow_src/tensorflow/python/eager/context.py\", line 32, in <module>\r\n    from tensorflow.core.framework import function_pb2\r\nImportError: cannot import name 'function_pb2'\r\n\r\nI have copies of the terminal output from the bazel build, the build wheel, the pip uninstall and pip install if you want them.", "when I cd $HOME and try the same program\r\ncd $HOME\r\nroot@17638e5f9f58:~# python -c \"import tensorflow as tf; print(' Num GPUs Available: ', len(tf.config.experimental.list_physical_devices('GPU')))\"\r\n2020-08-08 20:19:24.128138: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-08 20:19:25.347871: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-08-08 20:19:25.347901: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: UNKNOWN ERROR (-1)\r\n2020-08-08 20:19:25.347918: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 17638e5f9f58\r\n2020-08-08 20:19:25.348048: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 17638e5f9f58\r\n2020-08-08 20:19:25.348182: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\n2020-08-08 20:19:25.348559: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 450.57.0\r\n Num GPUs Available:  0\r\n\r\n\r\nI get my favorite CUINIT unknown error (-1)", "Of course that\u2019s because the stubs folder is in $LD_LIBRARY_PATH. When I delete that with\r\nexport LD_LIBRARY_PATH=\u201c/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/include/x64_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\u201d\r\nIt now comes up ok \r\n\r\nBut now after i commit the container and startup a jupyter notebook server in the container it seems to work. thanks for your help. \r\n\r\nMind if I ask what the revert update to tf2_3 did?", "It\u2019s taken me a bit but it appears as if tf is built for GPUs, the docker container has GPUS ALL and I can see the GPUS in the container using NVIDIA-SMI but for some reason TF is not using any gpus.\r\n\r\nprint(device_lib.list_local_devices())\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 7047076578536459057\r\n, name: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\n\r\ntf.test.is_built_with_gpu_support()\r\n\u200b\r\nOut[5]:\r\nTrue\r\n\r\n\r\n\r\ntf.test.is_gpu_available(\r\n    cuda_only=False, min_cuda_compute_capability=None\r\n)\r\nWARNING:tensorflow:From <ipython-input-3-97ecbf874269>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.config.list_physical_devices('GPU')` instead.\r\nOut[3]:\r\nFalse\r\n\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n\u200b\r\nNum GPUs Available:  0\r\n\r\ndocker container exec b48df43026b0 nvidia-smi\r\nMon Aug 24 06:08:41 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.57       Driver Version: 450.57       CUDA Version: ERR!     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 107...  On   | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   32C    P8     6W / 180W |     54MiB /  8116MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 1070    On   | 00000000:05:00.0 Off |                  N/A |\r\n|  0%   28C    P8     4W / 166W |      7MiB /  8119MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 1070    On   | 00000000:06:00.0 Off |                  N/A |\r\n|  0%   27C    P8     4W / 166W |      7MiB /  8119MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n\r\nSo the docker container has GPUs (3 of them). TF was built with GPU support but for some reason TF is not discovering the GPUs nor using them. \r\n\r\nI\u2019ve tried this by just using one GPU \u201cGPUS 1\u201d on docker command and it doesn\u2019t seem to make a difference except in the Nvidia-SMi command.\r\n\r\nI used to have debug on for the jupyter notebook server and I saw that it accessed CUDA libraries...", "Oh and I tried this:\r\ntf.config.list_physical_devices('GPU')\r\n\u200b\r\nOut[4]:\r\n[] \r\nWhich also shows no GPUs...\r\n", "Sorry my mistake forgot to extract \"/usr/local/cuda/lib64/stubs\" from $LD_LIBRARY_PATH. When I did that GPUs were available again.", "After all that I'm back to my favorite CUDA intialization failed error:\r\n\r\nCUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid.\r\n\r\nAfter the failure here is what NVIDIA-SMI command shows in the container:\r\nMon Aug 24 22:41:54 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.57       Driver Version: 450.57       CUDA Version: ERR!     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 107...  On   | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   40C    P8     7W / 180W |    165MiB /  8116MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\nand this is what the NVIDIA-SMI command shows at the ubuntu system (outside the container)\r\nnvidia-smi\r\nMon Aug 24 16:42:45 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.57       Driver Version: 450.57       CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 107...  On   | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   39C    P8     7W / 180W |    165MiB /  8116MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 1070    On   | 00000000:05:00.0 Off |                  N/A |\r\n|  0%   30C    P8     5W / 166W |      7MiB /  8119MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 1070    On   | 00000000:06:00.0 Off |                  N/A |\r\n|  0%   28C    P8     4W / 166W |      7MiB /  8119MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      1010      G   /usr/lib/xorg/Xorg                 36MiB |\r\n|    0   N/A  N/A      1684      G   /usr/bin/gnome-shell               15MiB |\r\n|    0   N/A  N/A      5623      C   /usr/bin/python3                  109MiB |\r\n|    1   N/A  N/A      1010      G   /usr/lib/xorg/Xorg                  4MiB |\r\n|    2   N/A  N/A      1010      G   /usr/lib/xorg/Xorg                  4MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\nIt seems I can use TF services to see the GPU but I can't seem to get tf.keras to build a model that uses them. Here's the model I was attempting to define at the time of the error:\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(layers.Conv2D(32,(3,3), activation='relu', input_shape=(150,150,3)))\r\nmodel.add(layers.MaxPooling2D((2,2)))\r\nmodel.add(layers.Conv2D(64,(3,3), activation='relu'))\r\nmodel.add(layers.MaxPooling2D((2,2)))\r\nmodel.add(layers.Conv2D(128,(3,3), activation='relu'))\r\nmodel.add(layers.MaxPooling2D((2,2)))\r\nmodel.add(layers.Conv2D(128,(3,3), activation='relu'))\r\nmodel.add(layers.MaxPooling2D((2,2)))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(512, activation='relu'))\r\nmodel.add(layers.Dense(1, activation='sigmoid'))\r\nmodel.summary()\r\n\r\nThe error seems to have been generated on the 1st statement above.  I uploaded the traceback for the error. \r\n[cuda_init_error_traceback.log](https://github.com/tensorflow/tensorflow/files/5120556/cuda_init_error_traceback.log)\r\n\r\nanything else you need please let me know. At this point we are back to ground 0, getting the cuda initialization error.\r\n\r\nIt seems every time I thought it was working we were not working with GPUs.\r\n\r\n\r\n", "Turns out I need to rebuild from source with compute capability 6.1 to support the GeForce 1070/1070 Titan GPUs... doing that now. I'll let you know tomorrow after the build finishes whether it works or not.", "After rebuild from source  (TF 2.4) and using the correct CUDA GPU compute capability, 6.1, it now seems to work once I delete stubs from LD_LIBRARY_PATH. So as far as I\u2019m concerned you can close this issue. \r\n\r\nHowever, the CUINIT error message is somewhat cryptic, I would suggest rewording it to say something about TF build doesn\u2019t support the GPU that\u2019s available and that it needs to be rebuilt from source with the correct CUDA GPU compute capability.\r\n\r\nThanks for all your help in this. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41976\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41976\">No</a>\n"]}, {"number": 41975, "title": "add tf.mask_fill()", "body": "## Added new api  `tf.mask_fill()` according to issue #41617 \r\n\r\n> This api will work similar to [Pytorch's masked_fill_](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.masked_fill_)\r\n\r\n### Example:\r\n`>>>tensor = tf.constant([1, 2, 3])`\r\n`>>>mask = tf.constant([True, False, True])`\r\n`>>>value = 5`\r\n`>>>tf.mask_fill(tensor, mask, value)`\r\n`<tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 2, 5], dtype=int32)>`", "comments": ["Isn't tf.mask_fill(tensor, mask, value) equivalent tf.where(mask, value, tensor)? (and the implementation of tf.where is better than the multiply-by-mask one."]}, {"number": 41974, "title": "Hi, I am trying to load a saved model using load_model('path') but i am getting the following error :", "body": "  File \"F:\\anaconda\\envs\\facerecog\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\", line 189, in load_model\r\n    loader_impl.parse_saved_model(filepath)\r\n  File \"F:\\anaconda\\envs\\facerecog\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 113, in parse_saved_model\r\n    constants.SAVED_MODEL_FILENAME_PB))\r\nOSError: SavedModel file does not exist at: ferlatest.h5/{saved_model.pbtxt|saved_model.pb}\r\n", "comments": ["@rishabhs-s \r\n\r\nCan you please, fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.\r\n\r\nYou can refer the [tutorial](https://www.tensorflow.org/tutorials/keras/save_and_load) which will help you in loading the saved model.Thanks!\r\n", "https://github.com/rishabhs-s/Face-Recog-update\r\nIn this web app when I run I get the error.\r\nPlease see\r\nIn this utils.py when I load model I get error", "@rishabhs-s \r\n\r\nCan you try by loading with the below command and see if you are able to load model or not.\r\n`model= tf.keras.models.load_model('ferlatest.h5')`\r\nPlease, refer this [documentation](https://www.tensorflow.org/tutorials/keras/save_and_load).Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41974\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41974\">No</a>\n"]}, {"number": 41973, "title": "Memory Leak with latest Tensorflow", "body": "\r\n**System information**\r\n- Have I written custom code: Yes, but minimal\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: conda\r\n- TensorFlow version: `2.3.0`\r\n- Python version: 3.7.4\r\n\r\n**Current behavior**\r\nMemory leaks.\r\n**Expected behavior**\r\nMemory does not leak.\r\n**Code to reproduce the issue**\r\n```Python\r\nimport numpy as np\r\nimport tensorflow\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport gc\r\nimport tracemalloc    \r\nif __name__ == \"__main__\":\r\n    tracemalloc.start()\r\n    while True:\r\n        inputs = keras.Input(shape=(10,))\r\n        out = layers.Dense(1)(inputs)\r\n        model = keras.Model(inputs=inputs, outputs=out)\r\n        model.compile(optimizer=\"adam\", loss=\"mse\")\r\n        train = np.random.rand(1000,10)\r\n        label = np.random.rand(1000)\r\n        model.fit(train, label)\r\n        gc.collect()\r\n        current, peak = tracemalloc.get_traced_memory()\r\n        print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\r\n```\r\n", "comments": ["Was able to reproduce the issue with TF v2.3. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1e765ba9a8323af3d0ecfbdcf81a7243/41973.ipynb). Thanks!", "Hi @fabianvdW, in TF 2.3 and earlier Keras puts all model construction in the same global background graph workspace, which leads to a memory leak unless you explicitly call keras.backend.clear_session.\r\n\r\nA few days ago though we enabled a refactoring of the Functional API implementation internals in the nightlies which gets rid of this background global graph, so you should no longer see this memory leak. (In the nightlies at least). Please do re-open this issue if you're still seeing one in the nightlies though or if you spot memory leaks elsewhere!\r\n\r\nBest,\r\nTomer", "This issue still exists in the nightlies. It affects` model.fit() `and `model.predict()`. I have also tried passing a generator to `model.fit()` but the leak persists.\r\nI have tried several versions of tensorflow, built from sources with different versions of cuda and cudnn - the problem exists in all versions I tested.\r\nTensorflow  2.3.0/2.4.0/nightly\r\nCuda 10.1/10.2\r\nCudnn 7.6/8.0\r\nnvidia driver 440.33\r\n\r\nI am using psutil to check memory consumption - `pip install psutil`\r\n\r\nCode to reproduce:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport psutil\r\nimport numpy as np\r\nprint(tf.__version__)\r\n\r\n\r\ndef generateInps():\r\n    for i in range(200000):\r\n        states = [ [ 1 ] * rows * columns for i in range(2000) ]\r\n        y = np.zeros([2000,512])\r\n        states = tf.one_hot(states, dtype='float32', depth=3)\r\n        print('iteration', i, \"memory usage %s\" %psutil.virtual_memory().percent)\r\n        yield states, y\r\n \r\nuse_generator = True \r\nfit = True\r\n    \r\nrows = 6\r\ncolumns = 7\r\n\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=[rows * columns, 3]),\r\n  tf.keras.layers.Dense(512, input_shape=[rows * columns * 3]),\r\n])\r\n\r\nmodel.compile(\r\n  optimizer=tf.keras.optimizers.SGD(lr=0.01),\r\n  loss='mean_squared_error',\r\n  metrics=['accuracy']\r\n)\r\n\r\n\r\nif fit:\r\n    if use_generator:\r\n        model.fit(generateInps(), verbose=0)\r\n    else:\r\n        states = [ [ 1 ] * rows * columns for i in range(2000) ]\r\n        y = np.zeros([2000,512])\r\n        states = tf.one_hot(states, dtype='float32', depth=3)\r\n        for iteration in range(1000000):\r\n            print('iteration', iteration, \"memory usage %s\" %psutil.virtual_memory().percent)\r\n            model.fit(states, y, verbose=0)\r\n            tf.keras.backend.clear_session()\r\nelse:\r\n    states = [ [ 1 ] * rows * columns for i in range(2000) ]\r\n    y = np.zeros([2000,512])\r\n    states = tf.one_hot(states, dtype='float32', depth=3)        \r\n    for iteration in range(1000000):\r\n        print('iteration', iteration, \"memory usage %s\" %psutil.virtual_memory().percent)\r\n        model.predict(states)        \r\n\r\n```\r\n\r\nExplicitly calling `tf.keras.backend.clear_session()` does not work and is therefore not a work around.\r\n\r\nMy current work around is to save a model after each batch of training and keep track of where in the training data I am and continue from there, this isn't ideal, but until this bug is fixed, I don't see a better option.", "@ems7,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 41972, "title": "tensorflow.python.framework.errors_impl.NotFoundError: 'DummySeedGenerator'", "body": "On Google Colab using TPU\r\n\r\n2020-08-01 18:34:58.551171: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nUsing TensorFlow backend.\r\nTENSORFLOW version: 2.3.0\r\nTensorflow device List:\r\n2020-08-01 18:35:01.566174: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-01 18:35:01.575940: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\r\n2020-08-01 18:35:01.576357: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2fc4a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-01 18:35:01.576430: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-01 18:35:01.581369: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-08-01 18:35:01.585626: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2020-08-01 18:35:01.585692: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cbed57a08e06): /proc/driver/nvidia/version does not exist\r\nRunning on TPU  ['10.102.83.42:8470']\r\n2020-08-01 18:35:01.604749: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.102.83.42:8470}\r\n2020-08-01 18:35:01.604991: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:34406}\r\n2020-08-01 18:35:01.624392: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.102.83.42:8470}\r\n2020-08-01 18:35:01.624476: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:34406}\r\n2020-08-01 18:35:01.625695: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://localhost:34406\r\nREPLICAS:  8\r\ntrain dataset:  665 , validation dataset:  36\r\n2020-08-01 18:35:40.190966: E tensorflow/core/common_runtime/eager/context.cc:678] Failed to register function remotely due to 'DummySeedGenerator' is neither a type of a primitive operation nor a name of a function registered in binary running on n-a43c3c58-w-0. Make sure the operation or function is registered in the binary running in this process.\r\n_**This shouldn't happen, please file a bug to tensorflow team.**_\r\nTraceback (most recent call last):\r\n  File \"scripts/tf_imgseg_train.py\", line 196, in <module>\r\n    callbacks=callbacks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1063, in fit\r\n    steps_per_execution=self._steps_per_execution)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1102, in __init__\r\n    self._steps_per_execution_value = steps_per_execution.numpy().item()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/values.py\", line 662, in numpy\r\n    return self.read_value().numpy()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1063, in numpy\r\n    maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1031, in _numpy\r\n    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: 'DummySeedGenerator' is neither a type of a primitive operation nor a name of a function registered in binary running on n-a43c3c58-w-0. Make sure the operation or function is registered in the binary running in this process.\r\n2020-08-01 18:35:40.194364: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:76] Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: 'DummySeedGenerator' is neither a type of a primitive operation nor a name of a function registered in binary running on n-a43c3c58-w-0. Make sure the operation or function is registered in the binary running in this process.\r\n2020-08-01 18:35:40.195337: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 6815, Output num: 0\r\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\r\n:{\"created\":\"@1596306940.195137527\",\"description\":\"Error received from peer ipv4:10.102.83.42:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 6815, Output num: 0\",\"grpc_status\":3}\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py\", line 540, in async_wait\r\n    context.async_wait()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\", line 2319, in async_wait\r\n    context().sync_executors()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\", line 658, in sync_executors\r\n    pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)\r\ntensorflow.python.framework.errors_impl.NotFoundError: 'DummySeedGenerator' is neither a type of a primitive operation nor a name of a function registered in binary running on n-a43c3c58-w-0. Make sure the operation or function is registered in the binary running in this process.", "comments": ["@IOTpreneur \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41971, "title": "Error in nce_loss using intermediate layer output", "body": "**Standalone code to reproduce the issue**\r\n[colab notebook](https://colab.research.google.com/drive/1JTQzXF_Gu4WHzwCg_EWbjAU2gc-8VAOx?usp=sharing)\r\n\r\n**System information**\r\ntensorflow version on my machine v2.2.0-rc4-8-g2b96f3662b 2.2.\r\n\r\nI am trying to use the nce_loss by passing the second to last layer's ouput to this loss function, then I got error\r\n\r\n>OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n\r\n", "comments": ["I have tried in colab with TF version 2.2 and was able to reproduce the issue.However with TF nightly version i am seeing the different errror message.\r\n(`TypeError: Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly.`).\r\nPlease, find the gist [here](https://colab.research.google.com/gist/ravikyram/2e33f9829e2eca5558d1b5a798c63e9e/untitled76.ipynb).Thanks!", "@chchannn `model.inputs` and `model.layer.output` are symbolic Tensors that cannot be used in losses, metrics, etc. Please check [here](https://github.com/tensorflow/tensorflow/issues/39702#issuecomment-631750377) more details about the root-cause. Thanks!\r\n\r\nFor further support questions, please post them in stackoverflow as there is a big community to support this kind of support questions. Either community or us will answer those support questions. GitHub is mainly for Bug and performance related issues. Thanks!", "I see. I did post on stackoverflow first but the [question](https://stackoverflow.com/questions/63201831/negative-sampling-for-classification-task-with-tensorflow-nce) is never answered. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41971\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41971\">No</a>\n"]}, {"number": 41970, "title": "[DOC] Fix tf.math.conj doc", "body": "Fix broken doc rendering of https://www.tensorflow.org/api_docs/python/tf/math/conj, and apply doctest on it.", "comments": []}, {"number": 41969, "title": "[TF:MLIR] Verify TransposeOp", "body": "Verify `tf.Transpose`.", "comments": []}, {"number": 41967, "title": "UserWarning: Tensorflow optimizers do not mkae it possible to access optimizer attributes or optimizer state after instantiation.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  \r\n- TensorFlow version: 2.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Quadro GV100, 32GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nHello, I just make some model from keras, and got some problems like below:\r\n```UserWarning: Tensorflow optimizers do not mkae it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers)```\r\n\r\nIn my codes,\r\n```\r\nmodel = Sequential()\r\nmodel.add(Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), data_format=\"channels_last\", activation='relu', input_shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3])))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(len(label), activation='softmax'))\r\nmodel.summary()\r\n\r\ninitial_learning_rate = args.lr\r\n    \r\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\r\ninitial_learning_rate,\r\ndecay_steps=4000,\r\ndecay_rate=0.96,\r\nstaircase=True)\r\n\r\noptimizer = tf.keras.optimizers.Adam(lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\r\n\r\nmodel.compile(loss=tf.keras.losses.KLDivergence(), optimizer=optimizer, metrics=['accuracy'])\r\n\r\nmodel_path='./'\r\nfile_path = os.path.join(model_path, 'saved-model-{epoch:02d}-{val_loss:.2f}.hdf5') # \r\ncheckpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=False, mode='max') # \r\ncallback_list = [checkpoint]\r\n\r\nhist = model.fit(train_datagen.flow(x_train, y_train, batch_size=args.batch_size), epochs=args.epochs, steps_per_epoch=train_steps, validation_data = (valid_datagen.flow(x_valid, y_valid, batch_size=args.batch_size)), callbacks = callback_list, shuffle=True)\r\n```\r\nAs shown in my code, I used the optimizers from ```tf.keras.optimizers``` but I got some errors.\r\n\r\nWhen I do evaluate, I got this warning message:\r\n```UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually```\r\n\r\nHere the summary of my ```eval.py``` code:\r\n```\r\nmodel = keras.models.load_model(model_dir)\r\nmodel.summary()\r\n\r\ny_pred = model.predict(x_test)\r\n```\r\n\r\nIs it possible to use that saved model directly? or how should I use this model? May I ignore that Warning message?\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\r\n", "comments": ["@kaen2891 \r\nThe code shared is not sufficient to replicate the issue faced , please provide stand alone code[with all dependencies] such that we can replicate the issue faced or if possible share a colab gist with the error\r\n\r\nmeanwhile for your first error can you refer to [this link](https://datascience.stackexchange.com/questions/45893/tf-tells-me-to-use-keras-optimizer-tells-me-the-opposite-when-i-change-it) and make changes a s suggested.\r\n\r\nfor the warning please refer to [this link](https://stackoverflow.com/questions/53295570/userwarning-no-training-configuration-found-in-save-file-the-model-was-not-c). and let us know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41967\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41967\">No</a>\n"]}, {"number": 41966, "title": "Unable to use libtensorflowlite.so ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.8.3\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: 10.1, cuDNN  \r\n- GPU model and memory: Nvidia Geforce 940MX 2GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n I want to use tensorflowlite C++ API to run inference. I used bazel build and installed it using \" bazel build -c opt --config=android_x86_64 //tensorflow/lite:libtensorflowlite.so\" this command. I copied the header files of flatbuffer and abseil and created a new folder \r\n\r\nI used the code provided in Android Quickstart:\r\n\r\n```c++\r\n#include \"tensorflow/lite/model.h\"\r\n#include \"tensorflow/lite/kernels/register.h\"\r\n#include <iostream>\r\n#include <memory>\r\n\r\nusing namespace std;\r\n\r\nint main()\r\n{\r\n    const char* filename = \"models/inception_v4.tflite\";\r\n    // Load the model\r\n    std::unique_ptr<tflite::FlatBufferModel> model =\r\n        tflite::FlatBufferModel::BuildFromFile(filename);\r\n\r\n    // Build the interpreter\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    std::unique_ptr<tflite::Interpreter> interpreter;\r\n    tflite::InterpreterBuilder(*model.get(), resolver)(&interpreter);\r\n\r\n    // Resize input tensors, if desired.\r\n    interpreter->AllocateTensors();\r\n\r\n    float* input = interpreter->typed_input_tensor<float>(0);\r\n    // Fill `input`.\r\n\r\n    interpreter->Invoke();\r\n\r\n    float* output = interpreter->typed_output_tensor<float>(0);\r\n\r\n}\r\n```\r\n\r\nI ran this command to compile\r\n\r\n```g++ hello.cpp  -I/<working_directory>/include -ltensorflowlite```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n```\r\nusr/bin/ld: warning: libm.so, needed by //usr/local/lib/libtensorflowlite.so, not found (try using -rpath or -rpath-link)\r\n/usr/bin/ld: warning: liblog.so, needed by //usr/local/lib/libtensorflowlite.so, not found (try using -rpath or -rpath-link)\r\n/usr/bin/ld: warning: libc.so, needed by //usr/local/lib/libtensorflowlite.so, not found (try using -rpath or -rpath-link)\r\n/tmp/ccWPry2X.o: In function `main':\r\nmodel.cpp:(.text+0xa4): undefined reference to `tflite::impl::InterpreterBuilder::operator()(std::unique_ptr<tflite::impl::Interpreter, std::default_delete<tflite::impl::Interpreter> >*)'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strtod@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `towlower@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_self@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_key_create@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `iswcntrl@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `fseeko@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wcsnrtombs@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `vsnprintf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `roundf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strtoll_l@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `atan@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_mutexattr_destroy@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_detach@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `munmap@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_mutex_destroy@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `mbsnrtowcs@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wmemmove@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strerror_r@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `iswdigit@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `__strlen_chk@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `sysconf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `dlopen@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `vsscanf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strlen@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `fwrite@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `sched_yield@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `ldexp@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `puts@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strtol@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `localeconv@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strtold@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `btowc@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `openlog@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `fclose@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `ftello@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_mutex_trylock@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `realloc@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `log@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `vfprintf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `close@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `qsort@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strxfrm@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `fopen@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `sinf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `mmap@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strtoull@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `abort@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `isupper@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strtoll@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `exit@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `putchar@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `memset@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_cond_signal@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `calloc@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `syscall@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_cond_destroy@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `vasprintf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `acos@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_getspecific@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `getenv@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `clock_gettime@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `sin@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `isspace@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wcrtomb@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `bsearch@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `sincos@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `memcmp@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `__system_property_get@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `iswpunct@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `__cxa_finalize@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_create@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strcoll@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `__android_log_vprint'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `tan@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `__cxa_atexit@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `asin@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `mbrtowc@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wcstof@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `fputs@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `realpath@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strtoul@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strpbrk@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `snprintf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wmemchr@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `round@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `sscanf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_cond_timedwait@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pow@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strcmp@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `fseek@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `mbtowc@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wcstoull@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `setlocale@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `tanhf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `iswupper@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `__vsnprintf_chk@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `towupper@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wctob@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `fflush@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wcsxfrm@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `exp@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `fprintf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strncpy@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `closelog@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `expf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `open@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `fmodf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `cos@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `__ctype_get_mb_cur_max@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `freelocale@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `mbsrtowcs@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wmemcmp@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `ldexpf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `tolower@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `iswblank@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wcstod@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `powf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `uselocale@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `atoi@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `frexp@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_mutexattr_settype@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_cond_wait@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `mkdir@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_setspecific@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_cond_init@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `__stack_chk_fail@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `nanosleep@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `dlsym@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strftime@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wcstoll@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `memmove@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_mutex_init@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `toupper@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `mbrlen@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `__memmove_chk@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wcstold@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `__errno@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `syslog@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `iswlower@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `newlocale@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `android_set_abort_message@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wcstol@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_join@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `swprintf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `memchr@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `__sF@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `isxdigit@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `memcpy@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `dl_iterate_phdr@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `iswspace@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `read@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `malloc@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strncmp@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_mutexattr_init@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strtoull_l@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_equal@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `posix_memalign@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `logf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_mutex_unlock@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strtold_l@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `iswalpha@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `stat@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strtof@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_once@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `fread@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_mutex_lock@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `iswprint@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `fputc@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wcscoll@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `fileno@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `pthread_cond_broadcast@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `strerror@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `free@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `fstat@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wcslen@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `cosf@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `log1p@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wmemcpy@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wcstoul@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `iswxdigit@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `wmemset@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `islower@LIBC'\r\n//usr/local/lib/libtensorflowlite.so: undefined reference to `printf@LIBC'\r\n```\r\n", "comments": ["Very supportive, thanks. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41966\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41966\">No</a>\n", "hi @jonpsy  how you solved this?"]}, {"number": 41965, "title": "Tensor into numpy", "body": "How do I turn a tensor such as `Tensor(\"lstm_42/Identity:0\", shape=(1, 100, 13), dtype=float32)` into a numpy array in keras? Tried .numpy() and .eval() but nothing seems to work.", "comments": ["What's the current tensorflow version you're using? besides that, have you read the template before publlishing? \r\n\r\n```For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow). ```\r\n", "I thought I read it, but somehow I missed that.\r\n\r\nMy bad, I'll close the issue."]}, {"number": 41964, "title": "ImportError: DLL load failed: The specified module could not be found.  During handling of the above exception, another exception occurred:", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\nI have python 3.7.4 and tensorflow 2.2 installed along with keras 2.2.4\r\n\r\n\r\n\r\n**Describe the problem**\r\nHere's the traceback for the error I'm getting: \r\n\r\nC:\\Users\\Edan\\AppData\\Local\\Programs\\Python\\Python37\\python.exe C:/Users/Edan/Desktop/Studies/AI/Project/final_project_ai/engine/game_engine.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Edan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Edan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Edan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Edan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Edan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Edan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from tensorflow.keras.layers.experimental.preprocessing import RandomRotation\r\n  File \"C:\\Users\\Edan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Edan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Edan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Edan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Edan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Edan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Edan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Edan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/Edan/Desktop/Studies/AI/Project/final_project_ai/engine/game_engine.py\", line 12, in <module>\r\n    from agents.simpleAgents import GreedyAgent, RandomAgent\r\n  File \"C:\\Users\\Edan\\Desktop\\Studies\\AI\\Project\\final_project_ai\\agents\\simpleAgents.py\", line 1, in <module>\r\n    from agents.Agents import Agent\r\n  File \"C:\\Users\\Edan\\Desktop\\Studies\\AI\\Project\\final_project_ai\\agents\\Agents.py\", line 5, in <module>\r\n    from keras.models import Sequential, Model\r\n  File \"C:\\Users\\Edan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\__init__.py\", line 6, in <module>\r\n    'Keras requires TensorFlow 2.2 or higher. '\r\nImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`\r\n\r\nProcess finished with exit code 1\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nJust trying to import tensorflow into some python files:\r\n\r\nimport random\r\nimport numpy as np\r\n# import tensorflow as tf\r\nfrom keras.models import Sequential, Model\r\nfrom keras.layers import Dense, Dropout, Input, InputLayer, Flatten\r\nfrom keras.layers.merge import Add, Multiply\r\nfrom keras.optimizers import Adam\r\nfrom random import choices\r\nfrom collections import deque\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@punims \r\nPlease refer to [this link](https://github.com/tensorflow/tensorflow/issues/36683#issuecomment-585097726) and let us know.", "@Saduf2019 \r\nProcessor\tIntel(R) Core(TM) i7-1065G7 CPU @ 1.30GHz, 1498 Mhz, 4 Core(s), 8 Logical Processor(s)\r\nThis is my processor, as far as I can tell \r\nhttps://ark.intel.com/content/www/us/en/ark/products/196597/intel-core-i7-1065g7-processor-8m-cache-up-to-3-90-ghz.html\r\nas far as I can tell I have a processor that does support Avx ", "@punims \r\nPlease verify you have downloaded the latest [microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\nMake sure you update environment path for cuda(please refer this https://www.tensorflow.org/install/gpu#windows_setup ).\r\n Make sure if there is a library that is in a different location/not installed on your system that cannot be loaded.Also, please follow the instructions from [Tensorflow website](https://www.tensorflow.org/install/gpu#windows_setup).\r\nPlease, check Your CPU/Python is on 32 bits?\r\n\r\nPlease, refer #36167, it will surely help, as this is a resolved common issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41964\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41964\">No</a>\n"]}, {"number": 41963, "title": "TFLite flex delegate build failed on Windows: fatal error C1001: An internal error has occurred in the compiler.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro Version 1903\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: `r2.3` branch\r\n- Python version: 3.6.0\r\n- Installed using virtualenv? pip? conda?: None, but have pip.\r\n- Bazel version (if compiling from source): 3.1.0 (as it recommends [here](https://www.tensorflow.org/install/source))\r\n- GCC/Compiler version (if compiling from source): MSVC 2017 (v141-14.16.27023) x64\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: RX 470 4GB\r\n\r\n**Describe the problem**\r\n`tensorflow\\compiler\\xla\\literal.cc(1166) : fatal error C1001: An internal error has occurred in the compiler.`\r\n\r\nI previously built the Tensorflow Lite C++ DLL successfully via the command `bazel build -c opt //tensorflow/lite:tensorflowlite`, but when trying to build with the Flex delegate (either by adding to BUILD dependencies and doing monolithic like it says [here](https://www.tensorflow.org/lite/guide/ops_select?hl=es-419#c)) or trying to straight out build the delegate itself I get this error after some time. I'm building without CUDA nor ROCm support\r\n\r\nI tried, according to other issues I found: (none worked)\r\n1. Turning off strong inline.\r\n2. Running cmd as administrator.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nAttempt 1: `bazel build --config monolithic //tensorflow/lite:tensorflowlite` after adding flex delegate to BUILD dependencies for lite.\r\nAttempt 2: `bazel build -c opt //tensorflow/lite/delegates/flex:delegate`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nERROR: C:/users/zdisket/documents/edit/tflitecompile/tensorflow/tensorflow/compiler/xla/BUILD:432:1: C++ compilation of rule '//tensorflow/compiler/xla:literal' failed (Exit 3): cl.exe failed: error executing command\r\n  cd C:/users/zdisket/_bazel_zdisket/jc7gu5by/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\cppwinrt\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\3.1;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\HTML Help Workshop;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/ZDisket/AppData/Local/Programs/Python/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/ZDisket/AppData/Local/Programs/Python/Python36/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\ZDisket\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_ENABLE_XLA=1\r\n    SET TMP=C:\\Users\\ZDisket\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /DTF_USE_SNAPPY /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=\"redacted\" -D__TIMESTAMP__=\"redacted\" -D__TIME__=\"redacted\" /Gy /Gw /w /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /std:c++14 /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/_objs/literal/literal.obj /c tensorflow/compiler/xla/literal.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\nc:\\users\\zdisket\\_bazel_zdisket\\jc7gu5by\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1166) : fatal error C1001: An internal error has occurred in the compiler.\r\n(compiler file 'd:\\agent\\_work\\3\\s\\src\\vctools\\compiler\\utc\\src\\p2\\main.c', line 187)\r\n To work around this problem, try simplifying or changing the program near the locations listed above.\r\nPlease choose the Technical Support command on the Visual C++\r\n Help menu, or open the Technical Support help file for more information\r\n  cl!CloseTypeServerPDB()+0x132a74\r\n  cl!CloseTypeServerPDB()+0x1499b\r\n  cl!CloseTypeServerPDB()+0x14b47\r\n  cl!CloseTypeServerPDB()+0x14df0\r\n  cl!CloseTypeServerPDB()+0x79992\r\n  cl!CloseTypeServerPDB()+0xb4aff\r\n  cl!CloseTypeServerPDB()+0xb415e\r\n  cl!InvokeCompilerPassW()+0x67d7b\r\n  cl!beginthreadex()+0x142\r\n  cl!BaseThreadInitThunk()+0x14\r\n  cl!RtlUserThreadStart()+0x21\r\n\r\nc:\\users\\zdisket\\_bazel_zdisket\\jc7gu5by\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1166) : fatal error C1001: An internal error has occurred in the compiler.\r\n(compiler file 'd:\\agent\\_work\\3\\s\\src\\vctools\\compiler\\utc\\src\\common\\error.c', line 835)\r\n To work around this problem, try simplifying or changing the program near the locations listed above.\r\nPlease choose the Technical Support command on the Visual C++\r\n Help menu, or open the Technical Support help file for more information\r\nTarget //tensorflow/lite/delegates/flex:delegate failed to build\r\nINFO: Elapsed time: 14977.319s, Critical Path: 371.66s\r\nINFO: 1335 processes: 1335 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["Can you please test MSVC 2019 since its tested for TF 2.3?\r\nThanks!", "Since then I decided to use the regular Tensorflow C API, closing issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41963\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41963\">No</a>\n"]}, {"number": 41962, "title": "Pycharm:TensorFlow-Attribute error issue ", "body": "Hi,\r\n\r\nWhile trying to run the below code I encountered an Attribute error, description for which is given below.\r\n\r\nCode\r\n\r\nimport tensorflow as tf\r\n\r\nnode1 = tf.constant (3.0,tf.float32)\r\n\r\nnode2 = tf.constant (4.0)\r\n\r\nprint(node1,node2)\r\n\r\nError Message encountered-AttributeError: module 'os' has no attribute 'add_dll_directory'\r\n\r\nSystem Configurations are as mentioned below\r\n\r\nOS-Windows 7\r\nPython version-3.8.0\r\nPycharm version-Community 2019.1\r\nTensorflow-2.3.0\r\n\r\nPFA the screenshots of Pycharm packages\r\n![pycharm_packages_1](https://user-images.githubusercontent.com/64397079/89103662-09bba580-d431-11ea-86ec-29bbd93c066a.jpg)\r\n![pycharm_packages_2](https://user-images.githubusercontent.com/64397079/89103664-0b856900-d431-11ea-8003-395027d06fef.jpg)\r\n\r\n", "comments": ["@vigneshenoy-SDET,\r\nCould you please check if you are running 64-bit Python and take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) to install TensorFlow.\r\n\r\nAlso, please run the command `pip list` and share the output with us. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41962\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41962\">No</a>\n"]}, {"number": 41961, "title": "Add c interface for `core/platform:logging`", "body": "@mihaimaruseac \r\nThis PR adds C API for `core/platform:logging`. \r\n\r\nSome observations:\r\n- We may want to have a `macro.h` for some common macros. I have to include `:c_api` as a dependency for only one macro `TF_CAPI_EXPORT`.\r\n- We could convert old code in `//tensorflow/c` that is using `LOG()`, `VLOG()` to C API Logging", "comments": ["For the first part, see https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/c_api_macros.h;drc=a92488b55c599280bc84dcdef4fb52c0dff90499 and https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/BUILD;l=133-138;drc=24f835217fd27f21141ff0254a2b93ea3cfd7b6c\r\n\r\nFor the second part, I agree, but we need to make sure we don't create circular deps. If we replace `:c_api` with the above then I think it should work"]}, {"number": 41960, "title": "cross compilation for tflite delegate testing tools", "body": "Hi,\r\n\r\nI'm implementing my custom delegates for tflite to use RiscV-based accelerators.\r\nWhile following the guides in **https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/utils/dummy_delegate**\r\nI found that kernel tests are not convenient for cross-compilation.\r\n\r\nIs there a recommended way to test my delegate functions for other architectures?\r\n\r\nThanks!\r\n\r\n", "comments": ["CCing @multiverse-tf who recently worked on this.", "Hi Junhyeok,\r\n\r\nHow do you cross-compile the TFLite main library and TFLite tests at the moment? Could you elaborate on the inconvenience?\r\n\r\nIf it's about plugging in a new cross-compiling toolchain to Bazel, it might be useful to following this [commit](https://github.com/tensorflow/tensorflow/commit/4961f18733ca3967198393abf419e14476b4a85c) as an example.", "Thanks for reply!\r\n\r\nI was doing cross-compilation by adding `riscv64_makefile.inc` in `tensorflow/lite/tool/make/targets` and setting environment variable when invoking `make`. I share my `riscv64_makefile.inc` below.\r\n```\r\n# Settings for RiscV platforms.\r\nifeq ($(TARGET), riscv64)\r\n  TARGET_ARCH := riscv64\r\n  TARGET_TOOLCHAIN_PREFIX := riscv64-unknown-linux-gnu-\r\n\r\n  CXXFLAGS += -march=rv64gc\r\n\tLIBS += -ldl -latomic\r\nendif\r\n```\r\n\r\nBut I believe the Makefile does not provide a compilation for test codes in `tensorflow/lite/kernels`\r\nIf I can plug riscv toolchains in Bazel, it would definitely help. :)\r\n\r\nI'm following the commit you suggested but I faced another error below.\r\nHow can I solve it?\r\n```\r\n$ bazel build --config=riscv64 //tensorflow/lite/kernels:fully_connected_test\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=187\r\nINFO: Reading rc options for 'build' from /home/jhjang/ml-accelerator/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/jhjang/ml-accelerator/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Found applicable config definition build:short_logs in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:riscv64 in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --crosstool_top=@local_config_riscv64//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=riscv64\r\nINFO: Found applicable config definition build:linux in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nERROR: /home/jhjang/.cache/bazel/_bazel_jhjang/8a72e618241ede75353c494f725f5447/external/XNNPACK/BUILD.bazel:3031:1: Configurable attribute \"deps\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n @XNNPACK//:linux_k8\r\n @XNNPACK//:linux_arm\r\n @XNNPACK//:linux_armeabi\r\n @XNNPACK//:linux_armhf\r\n @XNNPACK//:linux_armv7a\r\n @XNNPACK//:linux_aarch64\r\n @XNNPACK//:macos_x86_64\r\n @XNNPACK//:windows_x86_64_clang\r\n @XNNPACK//:windows_x86_64_mingw\r\n @XNNPACK//:windows_x86_64_msys\r\n @XNNPACK//:windows_x86_64\r\n @XNNPACK//:android_armv7\r\n @XNNPACK//:android_arm64\r\n @XNNPACK//:android_x86\r\n @XNNPACK//:android_x86_64\r\n @XNNPACK//:ios_armv7\r\n @XNNPACK//:ios_arm64\r\n @XNNPACK//:ios_arm64e\r\n @XNNPACK//:ios_x86\r\n @XNNPACK//:ios_x86_64\r\n @XNNPACK//:watchos_armv7k\r\n @XNNPACK//:watchos_arm64_32\r\n @XNNPACK//:watchos_x86\r\n @XNNPACK//:watchos_x86_64\r\n @XNNPACK//:tvos_arm64\r\n @XNNPACK//:tvos_x86_64\r\n @XNNPACK//:emscripten_wasm\r\n @XNNPACK//:emscripten_wasmsimd\r\n @XNNPACK//:emscripten_asmjs\r\nERROR: Analysis of target '//tensorflow/lite/kernels:fully_connected_test' failed; build aborted:\r\n\r\n/home/jhjang/.cache/bazel/_bazel_jhjang/8a72e618241ede75353c494f725f5447/external/XNNPACK/BUILD.bazel:3031:1: Configurable attribute \"deps\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n @XNNPACK//:linux_k8\r\n @XNNPACK//:linux_arm\r\n @XNNPACK//:linux_armeabi\r\n @XNNPACK//:linux_armhf\r\n @XNNPACK//:linux_armv7a\r\n @XNNPACK//:linux_aarch64\r\n @XNNPACK//:macos_x86_64\r\n @XNNPACK//:windows_x86_64_clang\r\n @XNNPACK//:windows_x86_64_mingw\r\n @XNNPACK//:windows_x86_64_msys\r\n @XNNPACK//:windows_x86_64\r\n @XNNPACK//:android_armv7\r\n @XNNPACK//:android_arm64\r\n @XNNPACK//:android_x86\r\n @XNNPACK//:android_x86_64\r\n @XNNPACK//:ios_armv7\r\n @XNNPACK//:ios_arm64\r\n @XNNPACK//:ios_arm64e\r\n @XNNPACK//:ios_x86\r\n @XNNPACK//:ios_x86_64\r\n @XNNPACK//:watchos_armv7k\r\n @XNNPACK//:watchos_arm64_32\r\n @XNNPACK//:watchos_x86\r\n @XNNPACK//:watchos_x86_64\r\n @XNNPACK//:tvos_arm64\r\n @XNNPACK//:tvos_x86_64\r\n @XNNPACK//:emscripten_wasm\r\n @XNNPACK//:emscripten_wasmsimd\r\n @XNNPACK//:emscripten_asmjs\r\nINFO: Elapsed time: 0.871s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (101 packages loaded, 2309 targets configured)\r\n```\r\n", "> Thanks for reply!\r\n> \r\n> I was doing cross-compilation by adding `riscv64_makefile.inc` in `tensorflow/lite/tool/make/targets` and setting environment variable when invoking `make`. \r\n\r\nI see. As we primarily use Bazel, the support for Makefile is indeed generally behind Bazel BUILD rules.\r\n\r\nI share my `riscv64_makefile.inc` below.\r\n> \r\n> ```\r\n> # Settings for RiscV platforms.\r\n> ifeq ($(TARGET), riscv64)\r\n>   TARGET_ARCH := riscv64\r\n>   TARGET_TOOLCHAIN_PREFIX := riscv64-unknown-linux-gnu-\r\n> \r\n>   CXXFLAGS += -march=rv64gc\r\n> \tLIBS += -ldl -latomic\r\n> endif\r\n> ```\r\n> \r\n> But I believe the Makefile does not provide a compilation for test codes in `tensorflow/lite/kernels`\r\n> If I can plug riscv toolchains in Bazel, it would definitely help. :)\r\n> \r\n> I'm following the commit you suggested but I faced another error below.\r\n> How can I solve it?\r\n> \r\n> ```\r\n> $ bazel build --config=riscv64 //tensorflow/lite/kernels:fully_connected_test\r\nIt looks like it's because XNNPACK isn't configured to support this option as suggested by \"external/XNNPACK/BUILD.bazel:3031:1: Configurable attribute \"deps\" doesn't match this configuration (would a default condition help?).\" \r\n\r\nOne way to bypass XNNPACK in this case, I think, is to comment out  the xnnpack delegate provider in //tensorflow/lite/kernels:test_util_delegate_providers BUILD rule.\r\n\r\n> INFO: Options provided by the client:\r\n>   Inherited 'common' options: --isatty=1 --terminal_columns=187\r\n> INFO: Reading rc options for 'build' from /home/jhjang/ml-accelerator/tensorflow/.bazelrc:\r\n>   Inherited 'common' options: --experimental_repo_remote_exec\r\n> INFO: Reading rc options for 'build' from /home/jhjang/ml-accelerator/tensorflow/.bazelrc:\r\n>   'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\n> INFO: Found applicable config definition build:short_logs in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\n> INFO: Found applicable config definition build:v2 in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\n> INFO: Found applicable config definition build:riscv64 in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --crosstool_top=@local_config_riscv64//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=riscv64\r\n> INFO: Found applicable config definition build:linux in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\n> INFO: Found applicable config definition build:dynamic_kernels in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\n> ERROR: /home/jhjang/.cache/bazel/_bazel_jhjang/8a72e618241ede75353c494f725f5447/external/XNNPACK/BUILD.bazel:3031:1: Configurable attribute \"deps\" doesn't match this configuration (would a default condition help?).\r\n> Conditions checked:\r\n>  @XNNPACK//:linux_k8\r\n>  @XNNPACK//:linux_arm\r\n>  @XNNPACK//:linux_armeabi\r\n>  @XNNPACK//:linux_armhf\r\n>  @XNNPACK//:linux_armv7a\r\n>  @XNNPACK//:linux_aarch64\r\n>  @XNNPACK//:macos_x86_64\r\n>  @XNNPACK//:windows_x86_64_clang\r\n>  @XNNPACK//:windows_x86_64_mingw\r\n>  @XNNPACK//:windows_x86_64_msys\r\n>  @XNNPACK//:windows_x86_64\r\n>  @XNNPACK//:android_armv7\r\n>  @XNNPACK//:android_arm64\r\n>  @XNNPACK//:android_x86\r\n>  @XNNPACK//:android_x86_64\r\n>  @XNNPACK//:ios_armv7\r\n>  @XNNPACK//:ios_arm64\r\n>  @XNNPACK//:ios_arm64e\r\n>  @XNNPACK//:ios_x86\r\n>  @XNNPACK//:ios_x86_64\r\n>  @XNNPACK//:watchos_armv7k\r\n>  @XNNPACK//:watchos_arm64_32\r\n>  @XNNPACK//:watchos_x86\r\n>  @XNNPACK//:watchos_x86_64\r\n>  @XNNPACK//:tvos_arm64\r\n>  @XNNPACK//:tvos_x86_64\r\n>  @XNNPACK//:emscripten_wasm\r\n>  @XNNPACK//:emscripten_wasmsimd\r\n>  @XNNPACK//:emscripten_asmjs\r\n> ERROR: Analysis of target '//tensorflow/lite/kernels:fully_connected_test' failed; build aborted:\r\n> \r\n> /home/jhjang/.cache/bazel/_bazel_jhjang/8a72e618241ede75353c494f725f5447/external/XNNPACK/BUILD.bazel:3031:1: Configurable attribute \"deps\" doesn't match this configuration (would a default condition help?).\r\n> Conditions checked:\r\n>  @XNNPACK//:linux_k8\r\n>  @XNNPACK//:linux_arm\r\n>  @XNNPACK//:linux_armeabi\r\n>  @XNNPACK//:linux_armhf\r\n>  @XNNPACK//:linux_armv7a\r\n>  @XNNPACK//:linux_aarch64\r\n>  @XNNPACK//:macos_x86_64\r\n>  @XNNPACK//:windows_x86_64_clang\r\n>  @XNNPACK//:windows_x86_64_mingw\r\n>  @XNNPACK//:windows_x86_64_msys\r\n>  @XNNPACK//:windows_x86_64\r\n>  @XNNPACK//:android_armv7\r\n>  @XNNPACK//:android_arm64\r\n>  @XNNPACK//:android_x86\r\n>  @XNNPACK//:android_x86_64\r\n>  @XNNPACK//:ios_armv7\r\n>  @XNNPACK//:ios_arm64\r\n>  @XNNPACK//:ios_arm64e\r\n>  @XNNPACK//:ios_x86\r\n>  @XNNPACK//:ios_x86_64\r\n>  @XNNPACK//:watchos_armv7k\r\n>  @XNNPACK//:watchos_arm64_32\r\n>  @XNNPACK//:watchos_x86\r\n>  @XNNPACK//:watchos_x86_64\r\n>  @XNNPACK//:tvos_arm64\r\n>  @XNNPACK//:tvos_x86_64\r\n>  @XNNPACK//:emscripten_wasm\r\n>  @XNNPACK//:emscripten_wasmsimd\r\n>  @XNNPACK//:emscripten_asmjs\r\n> INFO: Elapsed time: 0.871s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully (101 packages loaded, 2309 targets configured)\r\n> ```\r\n\r\n", "It worked! Thanks\r\nThough I also had to remove some other delegates as well, like hexagon.\r\n\r\nBut I found that bazel can only generate shared libraries, which is quite hard to deploy in my machine.\r\nIs there a workaround to statically build the tests using bazel?\r\n\r\nIf not, what should I include to build some executables with my delegate using cmake?\r\nlibtensorflow-lite.a of course, and then?", "> It worked! Thanks\r\n> Though I also had to remove some other delegates as well, like hexagon.\r\n\r\nGreat to hear this! Might consider contribute your RISC-V toolchain configuration back to TF :-)?\r\n> \r\n> But I found that bazel can only generate shared libraries, which is quite hard to deploy in my machine.\r\n> Is there a workaround to statically build the tests using bazel?\r\n\r\nHave you tried adding \"linkstatic=True\" to your rule? By default, I think bazel generates shared libraries.\r\n> \r\n> If not, what should I include to build some executables with my delegate using cmake?\r\n> libtensorflow-lite.a of course, and then?\r\n\r\nWe have plans to support CMake in TFLite in the near future.\r\n\r\n", "> Great to hear this! Might consider contribute your RISC-V toolchain configuration back to TF :-)?\r\n\r\nSure, I'll make a pull request after some minor cleanups\r\n\r\n>Have you tried adding \"linkstatic=True\" to your rule? By default, I think bazel generates shared libraries.\r\n\r\nCould you be more specific about how to add linkstatic to a rule? To be honest, I'm not quite familiar with bazel, yet.\r\nWhat I want to compile is `tensorflow/lite/kernels/fully_connected_test`", "I figured it out by adding `linkstatic=True` in the cc_binary rule and `-static` in `default_link_flags_feature` in my `cc_config.bzl.tpl` :)", "I share [my repository](https://github.com/junhyk/tensorflow) which contains how I figured out to build TFLite using RISC-V toolchains with bazel.\r\nMy solution is a bit hacky way since I commented out lots of line related to XNNPACK . I don't think I can directly contribute to TF. Hopefully, someone would find a better way.\r\n", "Hi all,\r\nI am also trying to cross compile label_image for riscv. Following is a command used.\r\n$bazel build --config=riscv //tensorflow/lite/examples/label_image:label_image\r\n\r\nI too get the same error as previously occured:\r\nbazel build --config=riscv //tensorflow/lite/examples/label_image:label_image\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=204\r\nINFO: Reading rc options for 'build' from /home/vaibhavvj/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/vaibhavvj/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Found applicable config definition build:short_logs in file /home/vaibhavvj/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/vaibhavvj/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:riscv in file /home/vaibhavvj/tensorflow/.bazelrc: --crosstool_top=//toolchain:riscv64 --cpu=riscv --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nINFO: Found applicable config definition build:linux in file /home/vaibhavvj/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/vaibhavvj/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nERROR: /home/vaibhavvj/.cache/bazel/_bazel_vaibhavvj/91ae5ce1d1ec4fdc8dfbac046528275c/external/XNNPACK/BUILD.bazel:3692:1: Configurable attribute \"deps\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n @XNNPACK//:linux_k8\r\n @XNNPACK//:linux_arm\r\n @XNNPACK//:linux_armeabi\r\n @XNNPACK//:linux_armhf\r\n @XNNPACK//:linux_armv7a\r\n @XNNPACK//:linux_aarch64\r\n @XNNPACK//:macos_x86_64\r\n @XNNPACK//:windows_x86_64_clang\r\n @XNNPACK//:windows_x86_64_mingw\r\n @XNNPACK//:windows_x86_64_msys\r\n @XNNPACK//:windows_x86_64\r\n @XNNPACK//:android_armv7\r\n @XNNPACK//:android_arm64\r\n @XNNPACK//:android_x86\r\n @XNNPACK//:android_x86_64\r\n @XNNPACK//:ios_armv7\r\n @XNNPACK//:ios_arm64\r\n @XNNPACK//:ios_arm64e\r\n @XNNPACK//:ios_x86\r\n @XNNPACK//:ios_x86_64\r\n @XNNPACK//:watchos_armv7k\r\n @XNNPACK//:watchos_arm64_32\r\n @XNNPACK//:watchos_x86\r\n @XNNPACK//:watchos_x86_64\r\n @XNNPACK//:tvos_arm64\r\n @XNNPACK//:tvos_x86_64\r\n @XNNPACK//:emscripten_wasm\r\n @XNNPACK//:emscripten_wasmsimd\r\nERROR: Analysis of target '//tensorflow/lite/examples/label_image:label_image' failed; build aborted: \r\n\r\n/home/vaibhavvj/.cache/bazel/_bazel_vaibhavvj/91ae5ce1d1ec4fdc8dfbac046528275c/external/XNNPACK/BUILD.bazel:3692:1: Configurable attribute \"deps\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n @XNNPACK//:linux_k8\r\n @XNNPACK//:linux_arm\r\n @XNNPACK//:linux_armeabi\r\n @XNNPACK//:linux_armhf\r\n @XNNPACK//:linux_armv7a\r\n @XNNPACK//:linux_aarch64\r\n @XNNPACK//:macos_x86_64\r\n @XNNPACK//:windows_x86_64_clang\r\n @XNNPACK//:windows_x86_64_mingw\r\n @XNNPACK//:windows_x86_64_msys\r\n @XNNPACK//:windows_x86_64\r\n @XNNPACK//:android_armv7\r\n @XNNPACK//:android_arm64\r\n @XNNPACK//:android_x86\r\n @XNNPACK//:android_x86_64\r\n @XNNPACK//:ios_armv7\r\n @XNNPACK//:ios_arm64\r\n @XNNPACK//:ios_arm64e\r\n @XNNPACK//:ios_x86\r\n @XNNPACK//:ios_x86_64\r\n @XNNPACK//:watchos_armv7k\r\n @XNNPACK//:watchos_arm64_32\r\n @XNNPACK//:watchos_x86\r\n @XNNPACK//:watchos_x86_64\r\n @XNNPACK//:tvos_arm64\r\n @XNNPACK//:tvos_x86_64\r\n @XNNPACK//:emscripten_wasm\r\n @XNNPACK//:emscripten_wasmsimd\r\nINFO: Elapsed time: 6.659s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (56 packages loaded, 1600 targets configured)\r\n\r\nSo how can I reject xnnpack dependency, during label_image build.\r\n"]}, {"number": 41959, "title": "tf.math.l2_normalize - gradients seem to be wrong", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.16.16, Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): docker container, versions `tensorflow/tensorflow:2.2.0-gpu` and `tensorflow/tensorflow:2.3.0`\r\n- TensorFlow version (use command below): `v2.2.0-rc4-8-g2b96f3662b 2.2.0` and `v2.3.0-rc2-23-gb36436b087 2.3.0`\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0.130 on Linux, no CUDA on OSX\r\n- GPU model and memory: GTX 1080 Ti 12GB on Linux, no GPU on OSX\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nGradients of `tf.math.l2_normalize` seem to be wrong. I confirmed both analytically and with numerical computations that values computed by tensorflow are off. There is always a chance my math is wrong, but I'm quite confident in it + numerical computations indicate same results as my workings.\r\n\r\nHere's a unit test that compares gradients computed by tensorflow to expected values.\r\nSince they differ, test fails:\r\n```python\r\ndef test_l2_gradients_with_tensorflow():\r\n\r\n    x = tf.constant([3, 4], tf.float32)\r\n\r\n    with tf.GradientTape() as gradient_tape:\r\n\r\n        gradient_tape.watch(x)\r\n        y = tf.math.l2_normalize(x)\r\n\r\n    expected_y = np.array([0.6, 0.8])\r\n\r\n    assert np.all(np.isclose(expected_y, y))\r\n\r\n    gradients = gradient_tape.gradient(y, x)\r\n\r\n    expected_gradients = np.array([0.128, 0.072])\r\n\r\n    # This line fails, tensorflow computes gradients to be [0.032 -0.024]\r\n    assert np.all(np.isclose(expected_gradients, gradients))\r\n```\r\n\r\n**Describe the expected behavior**\r\nBelow are my workings for what the correct `dy/dx1` derivative should be for values in unit test above. `dy/dx2` follows the same logic.\r\n\r\n![IMG_20200801_205810](https://user-images.githubusercontent.com/13689310/89102189-b0a83d80-d441-11ea-8b52-0e9a05e7d2e3.jpg)\r\n\r\nI also double checked with numerical computations in numpy, and they agree with my numbers:\r\n\r\n```python\r\ndef test_l2_gradients_with_numpy():\r\n\r\n    # Specify x1 only\r\n    x1 = np.arange(2.998, 3.003, 0.001)\r\n\r\n    # index for x1 = 3\r\n    test_index = 2\r\n\r\n    # x2 is set to constant 4\r\n    y = x1 / np.sqrt(x1*x1 + 16)\r\n    expected_y = 0.6\r\n\r\n    assert np.isclose(expected_y, y[test_index])\r\n\r\n    gradients = np.gradient(y, x1)\r\n    expected_gradient = 0.128\r\n\r\n    assert np.isclose(expected_gradient, gradients[test_index])\r\n```\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nPlease refer to unit tests above\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["And 10 minutes after posting the issue I understood I was wrong.\r\nWhat I computed was `d(x1') / dx1`, what I needed to compute was `d(x1')/d(x1) + d(x2')/dx1 + dx1` where x1' and x2' are normalized values of x1 and x2, since x1 contributes to both of them. I forgot about this and only computed contributions from x1'. Adding `d(x2')/dx1 + dx1` yielded same results as computed by tensorflow.\r\n\r\nMy apologies for the mistake.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41959\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41959\">No</a>\n"]}, {"number": 41958, "title": "Keras evaluate method returns wrong value after loading a model", "body": "**System information**\r\n- OS Platform : Windows10\r\n- TensorFlow installed from : source(pip in anaconda)\r\n- TensorFlow version : 2.3.0\r\n- Python version : 3.8.3\r\n\r\n**Describe the current behavior**\r\nKeras Sequential method:`evaluate()` returns wrong value after loading a model. value of loss is equal but metrics differs before and after the model is saved and loaded.\r\nI calculated the accuracy manually using sklearn.metrics and got the same value. Therefore, the problem seems to be in this method.\r\nIn the below code, the `acc1`, `acc2` are the same. However `acc1=0.863` and `acc2=0.086` in my environment.\r\n\r\n~~When I ran the code in colab, this issue was not occurred.~~\r\n\r\n**Describe the expected behavior**\r\nKeras Sequential method:`evaluate()` returns same value before and after the model is saved and loaded.\r\n\r\n**Standalone code to reproduce the issue**\r\nThis code is a modified version of this [tutorial](https://www.tensorflow.org/tutorials/keras/save_and_load) code to reproduce the issue.\r\nIf you run the code in the above tutorial on Windows10, I think you are able to reproduce the issue.\r\nFor your information, I comment on the results in my environment.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom sklearn.metrics import accuracy_score\r\n\r\n(train_images, train_labels),(test_images, test_labels) = tf.keras.datasets.mnist.load_data()\r\ntrain_labels = train_labels[:1000]\r\ntest_labels = test_labels[:1000]\r\ntrain_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\r\ntest_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0\r\n\r\ndef create_model():\r\n    model = tf.keras.models.Sequential([\r\n        keras.layers.Dense(512, activation='relu', input_shape=(784,)),\r\n        keras.layers.Dropout(0.2),\r\n        keras.layers.Dense(10)\r\n    ])\r\n    model.compile(optimizer='adam',\r\n                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                  metrics=['accuracy'])\r\n    return model\r\n\r\n# Create first model\r\nmodel = create_model()\r\nmodel.fit(train_images, train_labels, epochs=5)\r\n\r\nmodel.save('saved_model/my_model')\r\n\r\n# Create second model (load the saved model)\r\nnew_model = tf.keras.models.load_model('saved_model/my_model')\r\n\r\n# First model's results\r\nloss1, acc1 = model.evaluate(test_images, test_labels)\r\naccuracy_score1 = accuracy_score(test_labels, np.argmax(model.predict(test_images), axis=1))\r\n# loss1, acc1 = [0.4392167329788208, 0.8629999756813049]\r\n# accuracy_score1 = 0.863\r\n\r\n# Second model's results\r\nloss2, acc2 = new_model.evaluate(test_images, test_labels)                           \r\naccuracy_score2 = accuracy_score(test_labels, np.argmax(new_model.predict(test_images), axis=1))\r\n# loss2, acc2 = [0.4392167329788208, 0.0860000029206276] <- THIS!!!\r\n# accuracy_score2 = 0.863\r\n```\r\n\r\nWhy is this happening?\r\nCan you help me?\r\n", "comments": ["Was able to reproduce the issue with TF v2.2 and TF v2.3. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/bcbb82c4dc189486a4bd7594bfbe92f2/41958.ipynb). Thanks!", "@HKatoo This is a known issue that team is working on. Please check the the comment by @k-w-w \r\n\r\n> This is a bug with using the sparse categorical accuracy. For now, please compile the model with metrics='sparse_categorical_accuracy' instead of just 'accuracy'.\r\n\r\nWith the above modification, `acc1` and `acc2` are the same. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/c50083ed3bdeea12ef14ef83724e6cbb/41958.ipynb).\r\n\r\nLet's monitor the progress [there](https://github.com/tensorflow/tensorflow/issues/42045#issuecomment-674232499) in that issue. Thanks!", "@jvishnuvardhan Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41958\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41958\">No</a>\n", "This is resolved in recent tf-nightly. Please feel free to reopen if you notice the issue.\r\nThis is available in stable TF2.4 in the near future. Thanks!"]}, {"number": 41956, "title": "not sure but should it not be \"ZerosLike\" rather than \"OnesLike\" over here?", "body": "https://github.com/tensorflow/tensorflow/blob/05632ed9bad5bf9eee3edd57ade3d8250d580019/tensorflow/c/eager/gradients.cc#L104", "comments": ["@smj007 \r\n\r\nThis is duplicate of #41912. Can we track with that issue and close this one. It will help us to follow easily. Please let us know. Thanks!", "@ravikyram \r\n\r\nOh I did not see that! Thanks for pointing it out, I shall close this issue!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41956\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41956\">No</a>\n"]}, {"number": 41955, "title": "add random_resized_crop", "body": "", "comments": []}, {"number": 41954, "title": "tensorflow-io 0.14.0 requires tensorflow<2.3.0,>=2.2.0, but you'll have tensorflow 2.3.0 which is incompatible", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n - TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0\r\n- Python version:3.8.2\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 1650 4GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nCant use tensorflow-io , if I try to install tensorflow-io with pip  tensorflow downgrades  please fix this dependency issue.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npip install tensorflow \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\npip install tensorflow \r\nDefaulting to user installation because normal site-packages is not writeable\r\nCollecting tensorflow\r\n\r\nUsing cached tensorflow-2.3.0-cp38-cp38-manylinux2010_x86_64.whl (320.5 MB)\r\nRequirement already satisfied: wrapt>=1.11.1 in ./.local/lib/python3.8/site-packages (from tensorflow) (1.12.1)\r\nRequirement already satisfied: gast==0.3.3 in ./.local/lib/python3.8/site-packages (from tensorflow) (0.3.3)\r\nRequirement already satisfied: numpy<1.19.0,>=1.16.0 in ./.local/lib/python3.8/site-packages (from tensorflow) (1.18.4)\r\nRequirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in ./.local/lib/python3.8/site-packages (from tensorflow) (2.3.0)\r\nRequirement already satisfied: termcolor>=1.1.0 in ./.local/lib/python3.8/site-packages (from tensorflow) (1.1.0)\r\nRequirement already satisfied: astunparse==1.6.3 in ./.local/lib/python3.8/site-packages (from tensorflow) (1.6.3)\r\nRequirement already satisfied: google-pasta>=0.1.8 in ./.local/lib/python3.8/site-packages (from tensorflow) (0.2.0)\r\nRequirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in ./.local/lib/python3.8/site-packages (from tensorflow) (1.1.2)\r\nRequirement already satisfied: protobuf>=3.9.2 in ./.local/lib/python3.8/site-packages (from tensorflow) (3.12.2)\r\nRequirement already satisfied: h5py<2.11.0,>=2.10.0 in ./.local/lib/python3.8/site-packages (from tensorflow) (2.10.0)\r\nRequirement already satisfied: tensorboard<3,>=2.3.0 in ./.local/lib/python3.8/site-packages (from tensorflow) (2.3.0)\r\nRequirement already satisfied: scipy==1.4.1 in ./.local/lib/python3.8/site-packages (from tensorflow) (1.4.1)\r\nRequirement already satisfied: grpcio>=1.8.6 in ./.local/lib/python3.8/site-packages (from tensorflow) (1.29.0)\r\nRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\r\nRequirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow) (0.34.2)\r\nRequirement already satisfied: opt-einsum>=2.3.2 in ./.local/lib/python3.8/site-packages (from tensorflow) (3.2.1)\r\nRequirement already satisfied: absl-py>=0.7.0 in ./.local/lib/python3.8/site-packages (from tensorflow) (0.9.0)\r\nRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from protobuf>=3.9.2->tensorflow) (45.2.0)\r\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\r\nRequirement already satisfied: google-auth<2,>=1.6.3 in ./.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.16.0)\r\nRequirement already satisfied: markdown>=2.6.8 in ./.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\r\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.22.0)\r\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.6.0.post3)\r\nRequirement already satisfied: werkzeug>=0.11.15 in ./.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\r\nRequirement already satisfied: requests-oauthlib>=0.7.0 in ./.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in ./.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.0)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in ./.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\r\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in ./.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.0)\r\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\r\nRequirement already satisfied: pyasn1>=0.1.3 in ./.local/lib/python3.8/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\r\nInstalling collected packages: tensorflow\r\nERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\r\n\r\nWe recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\r\n\r\ntensorflow-io 0.14.0 requires tensorflow<2.3.0,>=2.2.0, but you'll have tensorflow 2.3.0 which is incompatible", "comments": ["@tkrsh For API compatible reasons, tensorflow-io 0.14.0 only works with `tensorflow<2.3.0,>=2.2.0`. We are, however, in the process of releasing a new version tensorflow-io 0.15.0 which works with `tensorflow<2.4.0,>=2.3.0`. (tracked in https://github.com/tensorflow/io/issues/1060). The new version 0.15.0 is planned to be released on Monday (barring any surprises). Once 0.15.0 is released you can update to have a compatible version and the issue should be resolved.", "@tkrsh The 0.15.0 release of tensorflow-io is available now: https://pypi.org/project/tensorflow-io/0.15.0/\r\n\r\nI think the issue could be closed. Please feel free to re-open if the issue persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41954\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41954\">No</a>\n", "During handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\PC\\Desktop\\kodak.py\", line 3, in <module>\r\n    from keras.datasets import imdb\r\n\r\n  File \"C:\\ass\\lib\\site-packages\\keras\\__init__.py\", line 5, in <module>\r\n    raise ImportError(\r\n\r\nImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`\r\n\r\n\r\nI have this problem.\r\n\r\n\r\nPython 3.8, windows 64 \r\nIn anaconda powersheel prompt I write \r\n\r\npip install tensorflow==2.2\r\nRequirement already satisfied: tensorflow==2.2 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (2.2.0)\r\nRequirement already satisfied: protobuf>=3.8.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (3.13.0)\r\nRequirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (2.10.0)\r\nRequirement already satisfied: absl-py>=0.7.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (0.11.0)\r\nRequirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (2.2.0)\r\nRequirement already satisfied: gast==0.3.3 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (0.3.3)\r\nRequirement already satisfied: keras-preprocessing>=1.1.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (1.1.2)\r\nRequirement already satisfied: six>=1.12.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (1.15.0)\r\nRequirement already satisfied: wrapt>=1.11.1 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (1.12.1)\r\nRequirement already satisfied: google-pasta>=0.1.8 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (0.2.0)\r\nRequirement already satisfied: numpy<2.0,>=1.16.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (1.19.2)\r\nRequirement already satisfied: scipy==1.4.1 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (1.4.1)\r\nRequirement already satisfied: astunparse==1.6.3 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (1.6.3)\r\nRequirement already satisfied: grpcio>=1.8.6 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (1.32.0)\r\nRequirement already satisfied: wheel>=0.26 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (0.36.2)\r\nRequirement already satisfied: termcolor>=1.1.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (1.1.0)\r\nRequirement already satisfied: opt-einsum>=2.3.2 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (3.3.0)\r\nRequirement already satisfied: tensorboard<2.3.0,>=2.2.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==2.2) (2.2.2)\r\nRequirement already satisfied: setuptools in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from protobuf>=3.8.0->tensorflow==2.2) (51.0.0.post20201207)\r\nRequirement already satisfied: werkzeug>=0.11.15 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.0.1)\r\nRequirement already satisfied: requests<3,>=2.21.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.25.1)\r\nRequirement already satisfied: google-auth<2,>=1.6.3 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.24.0)\r\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.6.0)\r\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.2)\r\nRequirement already satisfied: markdown>=2.6.8 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.3.3)\r\nRequirement already satisfied: rsa<5,>=3.1.4 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.6)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.2.8)\r\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.2.0)\r\nRequirement already satisfied: requests-oauthlib>=0.7.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.3.0)\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.8)\r\nRequirement already satisfied: idna<3,>=2.5 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.10)\r\nRequirement already satisfied: certifi>=2017.4.17 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2020.12.5)\r\nRequirement already satisfied: chardet<5,>=3.0.2 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.0.4)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.26.2)\r\nRequirement already satisfied: oauthlib>=3.0.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.1.0)\r\n(tf-gpu) PS C:\\Users\\PC> pip install tensorflow-gpu==2.2\r\nRequirement already satisfied: tensorflow-gpu==2.2 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (2.2.0)\r\nRequirement already satisfied: keras-preprocessing>=1.1.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (1.1.2)\r\nRequirement already satisfied: grpcio>=1.8.6 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (1.32.0)\r\nRequirement already satisfied: numpy<2.0,>=1.16.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (1.19.2)\r\nRequirement already satisfied: absl-py>=0.7.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (0.11.0)\r\nRequirement already satisfied: protobuf>=3.8.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (3.13.0)\r\nRequirement already satisfied: wrapt>=1.11.1 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (1.12.1)\r\nRequirement already satisfied: wheel>=0.26 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (0.36.2)\r\nRequirement already satisfied: astunparse==1.6.3 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (1.6.3)\r\nRequirement already satisfied: termcolor>=1.1.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (1.1.0)\r\nRequirement already satisfied: gast==0.3.3 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (0.3.3)\r\nRequirement already satisfied: scipy==1.4.1 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (1.4.1)\r\nRequirement already satisfied: opt-einsum>=2.3.2 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (3.3.0)\r\nRequirement already satisfied: tensorboard<2.3.0,>=2.2.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (2.2.2)\r\nRequirement already satisfied: six>=1.12.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (1.15.0)\r\nRequirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (2.10.0)\r\nRequirement already satisfied: google-pasta>=0.1.8 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (0.2.0)\r\nRequirement already satisfied: tensorflow-gpu-estimator<2.3.0,>=2.2.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorflow-gpu==2.2) (2.2.0)\r\nRequirement already satisfied: setuptools in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from protobuf>=3.8.0->tensorflow-gpu==2.2) (51.0.0.post20201207)\r\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (0.4.2)\r\nRequirement already satisfied: werkzeug>=0.11.15 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (1.0.1)\r\nRequirement already satisfied: requests<3,>=2.21.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (2.25.1)\r\nRequirement already satisfied: google-auth<2,>=1.6.3 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (1.24.0)\r\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (1.6.0)\r\nRequirement already satisfied: markdown>=2.6.8 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (3.3.3)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (0.2.8)\r\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (4.2.0)\r\nRequirement already satisfied: rsa<5,>=3.1.4 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (4.6)\r\nRequirement already satisfied: requests-oauthlib>=0.7.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (1.3.0)\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (0.4.8)\r\nRequirement already satisfied: chardet<5,>=3.0.2 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (3.0.4)\r\nRequirement already satisfied: idna<3,>=2.5 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (2.10)\r\nRequirement already satisfied: certifi>=2017.4.17 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (2020.12.5)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (1.26.2)\r\nRequirement already satisfied: oauthlib>=3.0.0 in c:\\ass\\envs\\tf-gpu\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (3.1.0)\r\n\r\n\r\nBut again problem ImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`", "@Angelafe123,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 41953, "title": "tensorflow 2.3.0: target 'grpc++_public_hdrs' not declared in package when build from source", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Gentoo linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: -\r\n- Bazel version (if compiling from source): 3.2.0\r\n- GCC/Compiler version (if compiling from source): 8.4.0\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: -\r\n\r\n\r\n**Describe the problem**\r\ntf 2.3.0 introduces experimental tf.data service and uses undefined `grpc++_public_hdrs` dependency. Thus building tf 2.3.0 from source on Gentoo linux gives the following error:\r\n`tensorflow/python/data/experimental/service/BUILD:11:27: no such target '@com_github_grpc_grpc//:grpc++_public_hdrs': target 'grpc++_public_hdrs' not declared in package '' defined by /var/tmp/portage/sci-libs/tensorflow-2.3.0/work/tensorflow-2.3.0-python3_7-bazel-base/external/com_github_grpc_grpc/BUILD.bazel and referenced by '//tensorflow/python/data/experimental/service:_pywrap_server_lib.so'`\r\n\r\nThe installation was done by the portage system (ebuild file) of Gentoo linux, please see [here](https://github.com/naturomics/gentoo-overlays) for the commands I used.\r\n\r\nnot sure if this problem affects other system, but [here](https://github.com/naturomics/gentoo-overlays/blob/master/sci-libs/tensorflow/files/tensorflow-2.3.0-0002-systemlibs-grpc-Fix-deps.patch) is a patch I wrote to fix it for gentoo linux.\r\n", "comments": ["@naturomics,\r\n\r\nCan you try installing the latest stable version of tensorflow i.e 2.6.0 and lets us know if the issue still persists. You can follow this guide to build from [source](https://www.tensorflow.org/install/source#tested_build_configurations). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41953\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41953\">No</a>\n"]}, {"number": 41952, "title": "Model predicts only one class In Binary Classification", "body": "Model : \r\n\r\nbase_model=tf.keras.applications.EfficientNetB7(input_shape=input_shape,include_top=False, weights='imagenet')\r\n\r\nglobal_average_layer = tf.keras.layers.GlobalAveragePooling2D()\r\nprediction_layer = tf.keras.layers.Dense(1,activation='sigmoid')\r\n\r\nmodel = tf.keras.Sequential([base_model,\r\n  global_average_layer,\r\n  prediction_layer\r\n])\r\n\r\nmodel.compile(optimizer=tf.optimizers.Adam(lr=0.00001),loss='BinaryCrossentropy',metrics=[tf.keras.metrics.AUC()])\r\n\r\nhistory=model.fit(train_data_gen,epochs=1,validation_data=validation_data_gen,callbacks=[lr_scheduler])\r\n\r\nsub['Test_Classes']=model.predict_classes(test_data_gen)\r\n\r\nExpected Output : 0 1 0 0 0 1 0 1 \r\n\r\nActual Output      : 0 0 0 0 0 0 0 0 \r\n", "comments": ["@tkrsh  Your code seems all right.\nCheck the train-data & test-data again?", "train_image_generator=ImageDataGenerator(rescale=1./255)\r\ntrain_data_gen=train_image_generator.flow_from_dataframe(train,directory=dirctory_train,x_col='image_name',y_col='benign_malignant',class_mode='binary',batch_size=32,target_size=IMG_SHAPE)\r\nLGTM ...", "> train_image_generator=ImageDataGenerator(rescale=1./255)\n> train_data_gen=train_image_generator.flow_from_dataframe(train,directory=dirctory_train,x_col='image_name',y_col='benign_malignant',class_mode='binary',batch_size=32,target_size=IMG_SHAPE)\n> LGTM ...\n\nI see. Your data source is panda-dataframe, so maybe the problem is in column \"benign_malignant\". \nI guess-----is there something wrong with the data type?", "I checked train_data_gen it was labelled correctly with 0, 1 ", "@tkrsh \r\n\r\nRequest you to fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose)\r\n\r\nCan you please share your colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!"]}, {"number": 41951, "title": "use tf.image.ssim to calculate ssim is differen from paper(matlab?)", "body": "I use the following code to calculate the ssim of set5 dataset, but the results is differen from paper. When use 'Bicubic' method and set scale to 4, papers are usually PSNR(28.42)/ SSIM(0.810) .\r\n```python\r\nimport tensorflow as tf\r\nssims = 0\r\npsnrs = 0\r\n\r\nfor i in range(5):\r\n    path1 = 'Set5\\\\Set5\\\\image_SRF_4\\\\img_00'+str(i+1)+'_SRF_4_bicubic.png'#bicubic\r\n    path2 = 'Set5\\\\Set5\\\\image_SRF_4\\\\img_00'+str(i+1)+'_SRF_4_HR.png'\r\n    img1 = tf.io.read_file(path1)\r\n    img1 = tf.io.decode_image(img1,3,expand_animations=False)\r\n\r\n    img2 = tf.io.read_file(path2)\r\n    img2 = tf.io.decode_image(img2,3,expand_animations=False)\r\n\r\n    ssims = ssims + tf.image.ssim(img1, img2, max_val=255)\r\n    psnrs = psnrs + tf.image.psnr(img1, img2, max_val=255)\r\n    \r\nssims/5,psnrs/5\r\n```\r\n```\r\n(<tf.Tensor: shape=(), dtype=float32, numpy=0.7731458>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=26.69437>)\r\n```\r\n**Why is there a big gap between my results and the paper? Is my calculation wrong?**", "comments": ["@Stealers,\r\nCould you please provide the link to the paper you are comparing the results with?\r\n\r\nAlso, share the sample images you are using along with the TensorFlow version. Thanks!", "@amahendrakar I made a mistake. The paper is testing y channel not RGB."]}, {"number": 41950, "title": "Keras 2.4.3 test missing cases", "body": "**System information**\r\n- OS Platform and Distribution :Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.2.0 \r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version:  Cuda10.0, cudnn 7.6.5.32\r\n- GPU model and memory: GTX1070 , 8G\r\n\r\nKeras version 2.4.3\r\n\r\nI have a mnist script to test my tensorflow installation\r\n\r\n```\r\n'''Trains a simple convnet on the MNIST dataset.\r\nGets to 99.25% test accuracy after 12 epochs\r\n(there is still a lot of margin for parameter tuning).\r\n16 seconds per epoch on a GRID K520 GPU.\r\n'''\r\n\r\nimport keras\r\nfrom keras.datasets import mnist\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Flatten\r\nfrom keras.layers import Conv2D, MaxPooling2D\r\nfrom keras import backend as K\r\n\r\nbatch_size = 128\r\nnum_classes = 10\r\nepochs = 12\r\n\r\n# input image dimensions\r\nimg_rows, img_cols = 28, 28\r\n\r\n# the data, split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\nif K.image_data_format() == 'channels_first':\r\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n    input_shape = (1, img_rows, img_cols)\r\nelse:\r\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n    input_shape = (img_rows, img_cols, 1)\r\n\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\n\r\n# convert class vectors to binary class matrices\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, kernel_size=(3, 3),\r\n                 activation='relu',\r\n                 input_shape=input_shape))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(128, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(num_classes, activation='softmax'))\r\n\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,\r\n              optimizer=keras.optimizers.Adadelta(),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          verbose=1,\r\n          validation_data=(x_test, y_test))\r\nscore = model.evaluate(x_test, y_test, verbose=0)\r\nprint('Test loss:', score[0])\r\nprint('Test accuracy:', score[1])\r\n\r\n```\r\n**Describe the current behavior**\r\n\r\nA lot of samples are lost, here is the output of running the script with keras 2.4.3 with tensoflow backend [in my setup python=python3] Comparing to expected behaviour below (with kereas 2.3.1), there are only 496 samples in an epoch instead of 6000\r\n\r\n```\r\npython /home/bernard/python-dev/test/mnist_cnn_demo.py\r\n2020-07-31 21:34:53.674307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\nx_train shape: (60000, 28, 28, 1)\r\n60000 train samples\r\n10000 test samples\r\n2020-07-31 21:34:55.075415: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-07-31 21:34:55.104800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:34:55.105872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.695GHz coreCount: 16 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-31 21:34:55.105894: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-07-31 21:34:55.107057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-07-31 21:34:55.107952: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-07-31 21:34:55.108219: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-07-31 21:34:55.109467: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-07-31 21:34:55.110373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-07-31 21:34:55.113179: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-31 21:34:55.113334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:34:55.113952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:34:55.114509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-07-31 21:34:55.121289: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2904000000 Hz\r\n2020-07-31 21:34:55.121631: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x42e2280 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-31 21:34:55.121650: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-31 21:34:55.182191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:34:55.182711: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x436c5e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-07-31 21:34:55.182743: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1070, Compute Capability 6.1\r\n2020-07-31 21:34:55.182917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:34:55.183371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.695GHz coreCount: 16 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-31 21:34:55.183411: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-07-31 21:34:55.183441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-07-31 21:34:55.183470: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-07-31 21:34:55.183482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-07-31 21:34:55.183493: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-07-31 21:34:55.183516: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-07-31 21:34:55.183545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-31 21:34:55.183616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:34:55.184210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:34:55.184688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-07-31 21:34:55.184751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-07-31 21:34:55.665812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-31 21:34:55.665860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-07-31 21:34:55.665888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-07-31 21:34:55.666241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:34:55.666841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:34:55.667435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6988 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nEpoch 1/12\r\n2020-07-31 21:34:56.279338: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-07-31 21:34:56.477766: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n469/469 [==============================] - 4s 8ms/step - loss: 2.2858 - accuracy: 0.1543 - val_loss: 2.2619 - val_accuracy: 0.2190\r\nEpoch 2/12\r\n469/469 [==============================] - 3s 7ms/step - loss: 2.2462 - accuracy: 0.2411 - val_loss: 2.2135 - val_accuracy: 0.3888\r\nEpoch 3/12\r\n469/469 [==============================] - 3s 7ms/step - loss: 2.1951 - accuracy: 0.3359 - val_loss: 2.1482 - val_accuracy: 0.5546\r\nEpoch 4/12\r\n469/469 [==============================] - 3s 7ms/step - loss: 2.1254 - accuracy: 0.4195 - val_loss: 2.0573 - val_accuracy: 0.6437\r\nEpoch 5/12\r\n469/469 [==============================] - 3s 7ms/step - loss: 2.0294 - accuracy: 0.4904 - val_loss: 1.9342 - val_accuracy: 0.6910\r\nEpoch 6/12\r\n469/469 [==============================] - 3s 7ms/step - loss: 1.9057 - accuracy: 0.5410 - val_loss: 1.7749 - val_accuracy: 0.7224\r\nEpoch 7/12\r\n469/469 [==============================] - 3s 7ms/step - loss: 1.7500 - accuracy: 0.5860 - val_loss: 1.5826 - val_accuracy: 0.7497\r\nEpoch 8/12\r\n469/469 [==============================] - 3s 7ms/step - loss: 1.5807 - accuracy: 0.6156 - val_loss: 1.3772 - val_accuracy: 0.7735\r\nEpoch 9/12\r\n469/469 [==============================] - 3s 7ms/step - loss: 1.4153 - accuracy: 0.6406 - val_loss: 1.1840 - val_accuracy: 0.7954\r\nEpoch 10/12\r\n469/469 [==============================] - 3s 7ms/step - loss: 1.2699 - accuracy: 0.6618 - val_loss: 1.0213 - val_accuracy: 0.8093\r\nEpoch 11/12\r\n469/469 [==============================] - 3s 7ms/step - loss: 1.1508 - accuracy: 0.6834 - val_loss: 0.8939 - val_accuracy: 0.8220\r\nEpoch 12/12\r\n469/469 [==============================] - 3s 7ms/step - loss: 1.0579 - accuracy: 0.7003 - val_loss: 0.7974 - val_accuracy: 0.8316\r\nTest loss: 0.7974222898483276\r\nTest accuracy: 0.83160001039505\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nDowngraded keras to version 2.3.1 and run the script again, the outcome is consistent with older versions of keras before\r\n\r\nNote that **Train on 60000 samples, validate on 10000 samples** is missing from the output above using keras 2.4.3.\r\n\r\n```\r\npython /home/bernard/python-dev/test/mnist_cnn_demo.py\r\nUsing TensorFlow backend.\r\nx_train shape: (60000, 28, 28, 1)\r\n60000 train samples\r\n10000 test samples\r\n2020-07-31 21:17:44.616490: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-07-31 21:17:44.641796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:17:44.643804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.695GHz coreCount: 16 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-31 21:17:44.644032: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-07-31 21:17:44.645151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-07-31 21:17:44.646009: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-07-31 21:17:44.646247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-07-31 21:17:44.647392: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-07-31 21:17:44.648247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-07-31 21:17:44.650968: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-31 21:17:44.651097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:17:44.651925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:17:44.652674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-07-31 21:17:44.658691: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2904000000 Hz\r\n2020-07-31 21:17:44.659154: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5cb4a20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-31 21:17:44.659167: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-31 21:17:44.718817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:17:44.719399: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5d43f30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-07-31 21:17:44.719434: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1070, Compute Capability 6.1\r\n2020-07-31 21:17:44.719622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:17:44.720238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.695GHz coreCount: 16 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-31 21:17:44.720286: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-07-31 21:17:44.720299: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-07-31 21:17:44.720330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-07-31 21:17:44.720343: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-07-31 21:17:44.720354: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-07-31 21:17:44.720379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-07-31 21:17:44.720390: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-31 21:17:44.720463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:17:44.721061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:17:44.721531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-07-31 21:17:44.721574: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-07-31 21:17:44.722447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-31 21:17:44.722457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-07-31 21:17:44.722479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-07-31 21:17:44.722716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:17:44.723177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-31 21:17:44.724393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7088 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n**Train on 60000 samples, validate on 10000 samples**\r\nEpoch 1/12\r\n2020-07-31 21:17:45.945316: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-07-31 21:17:46.131405: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\nEpoch 1/12\r\n2020-07-31 21:24:25.101205: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-07-31 21:24:25.295790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n60000/60000 [==============================] - 5s 91us/step - loss: 0.2614 - accuracy: 0.9188 - val_loss: 0.0578 - val_accuracy: 0.9811\r\nEpoch 2/12\r\n60000/60000 [==============================] - 4s 67us/step - loss: 0.0910 - accuracy: 0.9730 - val_loss: 0.0411 - val_accuracy: 0.9857\r\nEpoch 3/12\r\n60000/60000 [==============================] - 4s 67us/step - loss: 0.0656 - accuracy: 0.9805 - val_loss: 0.0407 - val_accuracy: 0.9862\r\nEpoch 4/12\r\n60000/60000 [==============================] - 4s 67us/step - loss: 0.0538 - accuracy: 0.9837 - val_loss: 0.0311 - val_accuracy: 0.9898\r\nEpoch 5/12\r\n60000/60000 [==============================] - 4s 67us/step - loss: 0.0483 - accuracy: 0.9850 - val_loss: 0.0288 - val_accuracy: 0.9904\r\nEpoch 6/12\r\n60000/60000 [==============================] - 4s 67us/step - loss: 0.0412 - accuracy: 0.9878 - val_loss: 0.0291 - val_accuracy: 0.9905\r\nEpoch 7/12\r\n60000/60000 [==============================] - 4s 67us/step - loss: 0.0381 - accuracy: 0.9888 - val_loss: 0.0283 - val_accuracy: 0.9902\r\nEpoch 8/12\r\n60000/60000 [==============================] - 4s 67us/step - loss: 0.0354 - accuracy: 0.9892 - val_loss: 0.0286 - val_accuracy: 0.9914\r\nEpoch 9/12\r\n60000/60000 [==============================] - 4s 68us/step - loss: 0.0330 - accuracy: 0.9900 - val_loss: 0.0275 - val_accuracy: 0.9916\r\nEpoch 10/12\r\n60000/60000 [==============================] - 4s 74us/step - loss: 0.0314 - accuracy: 0.9909 - val_loss: 0.0276 - val_accuracy: 0.9916\r\nEpoch 11/12\r\n60000/60000 [==============================] - 5s 77us/step - loss: 0.0300 - accuracy: 0.9907 - val_loss: 0.0248 - val_accuracy: 0.9917\r\nEpoch 12/12\r\n60000/60000 [==============================] - 5s 77us/step - loss: 0.0264 - accuracy: 0.9920 - val_loss: 0.0292 - val_accuracy: 0.9907\r\nTest loss: 0.02923813719809523\r\nTest accuracy: 0.9907000064849854\r\n```\r\n\r\n", "comments": ["@beew \r\nCould you please try \"tf.keras\" instead of \"keras\" and let us know.", "@Saduf2019 \r\n\r\nNo change. But may be I get you wrong, replace keras by tf.keras where? Everywhere?", "@beew \r\nYes beew everywhere in the code.", "I have updated the script but there is no change, still the same wrong output.\r\n\r\nBut the interesting thing is with this script both keras2.4.3 and  keras2.3.1 gave the same wrong result in exactly the same way so maybe the problem is tf.keras. It seems that keras-2.4.3 somehow switches to tf.keras automatically but keras-2.3.1 doesn't unless it is explicit like in the modified script, so 2.3.1 works for the unmodified script but 2.4.3 fails in the same way.\r\n\r\nHere is the updated script\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.datasets import mnist\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\r\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\r\nfrom tensorflow.keras import backend as K\r\n\r\nbatch_size = 128\r\nnum_classes = 10\r\nepochs = 12\r\n\r\n# input image dimensions\r\nimg_rows, img_cols = 28, 28\r\n\r\n# the data, split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\nif K.image_data_format() == 'channels_first':\r\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n    input_shape = (1, img_rows, img_cols)\r\nelse:\r\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n    input_shape = (img_rows, img_cols, 1)\r\n\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\n\r\n# convert class vectors to binary class matrices\r\ny_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\ny_test = tf.keras.utils.to_categorical(y_test, num_classes)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, kernel_size=(3, 3),\r\n                 activation='relu',\r\n                 input_shape=input_shape))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(128, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(num_classes, activation='softmax'))\r\n\r\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy,\r\n              optimizer=tf.keras.optimizers.Adadelta(),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          verbose=1,\r\n          validation_data=(x_test, y_test))\r\nscore = model.evaluate(x_test, y_test, verbose=0)\r\nprint('Test loss:', score[0])\r\nprint('Test accuracy:', score[1])\r\n\r\n```", "@ymodak \r\nI am able to replicate this, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/171fc7a89e6d68f5a7bd669f5f4e28a2/untitled337.ipynb)", "I think this is correct since we have `steps_per_epoch` as 469;\r\n`\r\nsteps_per_epoch (469) = #total no of samples (60000) / batch_size (128)\r\n`\r\nIn earlier versions only the number of samples are shown (60000).", "@ymodak \r\n\r\nBut what about the test accuracy?\r\n\r\nBefore it was 99% with keras 2.4.3 it is only 83%, the loss was 3% with Keras2.3.1 vs 79% with Keras2.4.3 Something doesn't look right.", "I tested with keras 2.3.1 and get similar accuracy of 83%.\r\nSee the [gist](https://colab.research.google.com/gist/ymodak/1bd8ba79b38dad7798ed9e197fa9fcb7/untitled337.ipynb) for your reference.\r\nClosing this issue since the original topic is resolved. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41950\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41950\">No</a>\n", "> I tested with keras 2.3.1 and get similar accuracy of 83%.\r\n> See the [gist](https://colab.research.google.com/gist/ymodak/1bd8ba79b38dad7798ed9e197fa9fcb7/untitled337.ipynb) for your reference.\r\n> Closing this issue since the original topic is resolved. Thanks!\r\n\r\n\r\n\r\nWell I don't think you really resolve the issue, it probably just means there is something wrong in using tf.keras in place of keras and it still seems like something wrong considering that the error and loss cases are much bigger than before.", "@beew Can you please share a gist showing the discrepancy in accuracy across various versions?\r\nI have tested with Keras 2.3.1 and got accuracy of 83%, whereas with TF 2.3 and Keras - 2.4.3 it is 86% . Thus, I see better accuracy with later version.\r\nAlso which error did you encounter?\r\nMay be I am missing something here. Thanks!", "> I think this is correct since we have `steps_per_epoch` as 469;\r\n> `steps_per_epoch (469) = #total no of samples (60000) / batch_size (128)`\r\n> In earlier versions only the number of samples are shown (60000).\r\n\r\nDoes that mean it is trained 60000 time or do we have to change batch size to 1 to train 60000 time ? Can you please explain im new to cnn ? why is batch size so important ?"]}, {"number": 41949, "title": "E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] layout failed: Invalid argument: Size of values 0 does not match size of permutation 4 @ fanin shape ingradient_tape /replica_1/SelectV2_4-2-TransposeNHWCToNCHW-LayoutOptimizer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0 and tf-nightly\r\n- Python version: 2.7\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: v100\r\n\r\n**Describe the current behavior**\r\nRunning a tf.keras fit method triggers this error in the logs. I am working with a really large codebase, hence providing a standlone code example is not feasible. Also finding what block of code is causing this isn't straightforwad for the same reason. Interestingly the fit method runs successfully even with this error popping up in the logs.\r\n\r\n\r\n**Other info / logs** \r\n```\r\n2020-08-01 11:02:39.503101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1, 2, 3\r\n2020-08-01 11:02:39.503161: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-01 11:02:43.832731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-01 11:02:43.832785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 2 3\r\n2020-08-01 11:02:43.832795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N Y Y Y\r\n2020-08-01 11:02:43.832800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   Y N Y Y\r\n2020-08-01 11:02:43.832805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 2:   Y Y N Y\r\n2020-08-01 11:02:43.832810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 3:   Y Y Y N\r\n2020-08-01 11:02:43.848539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14755 MB memory) -> p\r\nhysical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:0a:00.0, compute capability: 7.0)\r\n2020-08-01 11:02:43.854140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 14755 MB memory) -> p\r\nhysical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:0b:00.0, compute capability: 7.0)\r\n2020-08-01 11:02:43.858886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 14755 MB memory) -> p\r\nhysical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:0f:00.0, compute capability: 7.0)\r\n2020-08-01 11:02:44.130409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 14755 MB memory) -> p\r\nhysical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:10:00.0, compute capability: 7.0)\r\n2020-08-01 11:03:44.659136: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] layout failed: Invalid argument: Size of values 0 does not match size of permutation 4 @ fanin\r\n shape ingradient_tape/replica_1/SelectV2_4-2-TransposeNHWCToNCHW-LayoutOptimizer\r\n2020-08-01 11:03:57.040696: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-01 11:04:01.069013: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library \r\n```", "comments": ["@srihari-humbarwadi \r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@srihari-humbarwadi \r\n\r\nYou can refer similar issue #34499 and see if it helps you.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41949\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41949\">No</a>\n"]}, {"number": 41948, "title": "Undefined symbols for architecture arm64 when loading TensorFlowLiteSelectTfOps on iOS device", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS 13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 11\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: TensorFlowLiteSwift (same issue with both 2.2.0 and 0.0.1-nightly), TensorFlowLiteSelectTfOps (0.0.1-nightly)\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?: CocoaPods\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the problem**\r\nI'm trying to integrate a tflite model that requires select TF ops with an iOS project, following the steps given here: https://www.tensorflow.org/lite/guide/ops_select#ios \r\nI created an empty Xcode project just to test out the setup, I installed the dependencies using CocoaPods, and then linked the TF ops, but when I build the project on a device I encountered the following error: Undefined symbol: _uprv_getICUData_conversion (see screenshot below):\r\n![error-device](https://user-images.githubusercontent.com/23728605/89082882-4fa73980-d35d-11ea-8bba-b57de28c97c3.png)\r\n\r\nAnd just to confirm, does select TF ops currently support simulator for iOS? I get a warning during build and the app crashes when the interpreter is invoked (see screenshot below):\r\n![error-simulator](https://user-images.githubusercontent.com/23728605/89083018-b88eb180-d35d-11ea-9563-a36bb3a2d6ba.png)\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Create a new project\r\n2. Create a Podfile with TensorFlowLiteSwift and TensorFlowLiteSelectTfOps dependencies\r\n3. Install dependencies and add linker flag\r\n4. Build project\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Can confirm I've encountered the same issue - I was able to hack around this by manually defining the symbol as `NULL` in my project, which I assume may crash at runtime:\r\n\r\n```objc\r\n// To work around a nasty linker issue in TensorFlowLiteSelectTFOps\r\nconst void * uprv_getICUData_conversion = NULL;\r\n```\r\n\r\n@yyoon could this be related to the linker issues I flagged in https://github.com/tensorflow/tensorflow/issues/41876?", "I have the same issue.\r\n\r\n` - TensorFlowLiteC (2.3.0):\r\n- TensorFlowLiteC/Core (= 2.3.0)\r\n\r\nTensorFlowLiteC/Core (2.3.0)\r\nTensorFlowLiteSelectTfOps (0.0.1-nightly.20200819)\r\nTensorFlowLiteSwift (2.3.0):\r\nTensorFlowLiteSwift/Core (= 2.3.0)\r\nTensorFlowLiteSwift/Core (2.3.0):\r\nTensorFlowLiteC (= 2.3.0)\r\nTensorFlowLiteSelectTfOps (~> 0.0.1-nightly)\r\nTensorFlowLiteSwift (= 2.3.0)\r\nTensorFlowLiteC\r\nTensorFlowLiteSelectTfOps\r\nTensorFlowLiteSwift\r\nTensorFlowLiteC: 51f50caf5777f740a70e2c1a5dbdc149e7aeb50b\r\nTensorFlowLiteSelectTfOps: 003d038ccf014b39b65b7d02627e1b2856c0a340\r\nTensorFlowLiteSwift: fb152cc1eec36b25b03a23c07f5d58113170af58`\r\n\r\n`Undefined symbols for architecture arm64: \"_uprv_getICUData_conversion\", referenced from: openCommonData(char const*, int, UErrorCode*) in TensorFlowLiteSelectTfOps(udata_2985644a25bfa5ed30c4cd35b64b13bb.o) ld: symbol(s) not found for architecture arm64 clang: error: linker command failed with exit code 1 (use -v to see invocation)`", "@psobot in which file you have defined that symbol as `NULL`?", "@yyoon @abattery @teijeong @wangtz if anyone of you guys can suggest any workaround to temporarily fix this problem, that will be very helpful. Thanks", "@yyoon could you take a look at this issue?", "Same issue here. Neither cocoapods nor the Bazel build would solve.", "Same issue here, are you working on a fix or workaround?", "I was able to reproduce this issue. Investigating now.", "+1, reproducible from [lite examples too](https://github.com/tensorflow/examples/tree/master/lite/examples), via both cocoa pod & bazel ", "I'm getting the error still when my pod spec looks like this:\r\n\r\nPODS:\r\n  - TensorFlowLiteC (0.0.1-nightly.20200818):\r\n    - TensorFlowLiteC/Core (= 0.0.1-nightly.20200818)\r\n  - TensorFlowLiteC/Core (0.0.1-nightly.20200818)\r\n  - TensorFlowLiteSelectTfOps (0.0.1-nightly.20200819)\r\n  - TensorFlowLiteSwift (0.0.1-nightly.20200818):\r\n    - TensorFlowLiteSwift/Core (= 0.0.1-nightly.20200818)\r\n  - TensorFlowLiteSwift/Core (0.0.1-nightly.20200818):\r\n    - TensorFlowLiteC (= 0.0.1-nightly.20200818)\r\n\r\nDEPENDENCIES:\r\n  - TensorFlowLiteSelectTfOps (~> 0.0.1-nightly)\r\n  - TensorFlowLiteSwift (~> 0.0.1-nightly)", "Please fix this...It has not been resolved..yet..", "Please fix this...", "I am working on a fix. If you don't need recent feature then v2.2.0 will work.", "A fix is under review internally. Please wait a bit, I'll notify when it is merged to the master.", "Hi all,\r\nCommit https://github.com/tensorflow/tensorflow/commit/4b6c15218b690c5c75b28c423991096e4f0ea51b contains the fix for this error.\r\nYou can try to build the master branch or waiting for it to be included in the nightly build tomorrow.", "Thanks @thaink!   That worked for me.   \r\n", "> Thanks @thaink! That worked for me.\r\n\r\nGlad to hear that. Thanks for the confirmation.", "When I use this nightly build version, my demo app crashed like below.\r\n\r\nThread 2: EXC_BAD_ACCESS (code=1, address=0x0)\r\n\r\nThread 2 Queue : org.tensorflow.examples.lite.style_transfer (serial)\r\n#0\t0x00000001f0bce618 in _platform_memmove ()\r\n#1\t0x00000001026b406c in ___lldb_unnamed_symbol7040$$TFL Style Transfer ()\r\n#2\t0x00000001026a82e4 in ___lldb_unnamed_symbol6806$$TFL Style Transfer ()\r\n#3\t0x00000001026b0f90 in ___lldb_unnamed_symbol6968$$TFL Style Transfer ()\r\n#4\t0x00000001026a3744 in ___lldb_unnamed_symbol6746$$TFL Style Transfer ()\r\n#5\t0x00000001026a4304 in ___lldb_unnamed_symbol6748$$TFL Style Transfer ()\r\n#6\t0x00000001026994bc in ___lldb_unnamed_symbol6574$$TFL Style Transfer ()\r\n#7\t0x0000000102309cfc in ___lldb_unnamed_symbol129$$TFL Style Transfer ()\r\n#8\t0x0000000102309924 in ___lldb_unnamed_symbol128$$TFL Style Transfer ()\r\n#9\t0x0000000102309678 in ___lldb_unnamed_symbol127$$TFL Style Transfer ()\r\n#10\t0x0000000102699420 in ___lldb_unnamed_symbol6573$$TFL Style Transfer ()\r\n#11\t0x000000010230cc80 in ___lldb_unnamed_symbol169$$TFL Style Transfer ()\r\n#12\t0x00000001024438fc in ___lldb_unnamed_symbol2108$$TFL Style Transfer ()\r\n#13\t0x0000000102404554 in ___lldb_unnamed_symbol1773$$TFL Style Transfer ()\r\n#14\t0x000000010240431c in TfLiteInterpreterCreate ()\r\n#15\t0x00000001022ee854 in Interpreter.init(modelPath:options:delegates:) at /Users/user/git/examples-master/lite/examples/style_transfer/ios/Pods/TensorFlowLiteSwift/tensorflow/lite/experimental/swift/Sources/Interpreter.swift:98\r\n#16\t0x00000001022ee144 in Interpreter.__allocating_init(modelPath:options:delegates:) ()\r\n#17\t0x00000001022de3a0 in closure #1 in static Inpainting.newInstance(inpaintingModel:useMetalDelegate:completion:) at /Users/user/git/examples-master/lite/examples/style_transfer/ios/StyleTransfer/Inpainting.swift:97\r\n#18\t0x00000001022dee34 in thunk for @escaping @callee_guaranteed () -> () ()\r\n#19\t0x0000000102e53b68 in _dispatch_call_block_and_release ()\r\n#20\t0x0000000102e555f0 in _dispatch_client_callout ()\r\n#21\t0x0000000102e5cfa8 in _dispatch_lane_serial_drain ()\r\n#22\t0x0000000102e5dcb4 in _dispatch_lane_invoke ()\r\n#23\t0x0000000102e69e38 in _dispatch_workloop_worker_thread ()\r\n#24\t0x00000001f0bd4908 in _pthread_wqthread ()\r\n", "@jucysoft Can you create a new bug for this.\r\nThe error is not related to this issue. It seems to be related to MeTalDelegate instead."]}, {"number": 41947, "title": "TFLite: reduced redundant calculation in integer_ops conv.h", "body": "This PR reduces the repeatedly redundant calculations of `in_x_origin`, `in_y_origin`, `in_x`, `in_y`, and `is_point_inside_image` in `tensorflow/lite/kernels/internal/reference/integer_ops/conv.h` and skips the busy loop in the innermost of the nested loop when `is_point_inside_image` is `false` for both versions of `ConvPerChannel`.\r\n\r\nThe change results in a 14.48% of speedup during inference when running [person_detection_experimental](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection_experimental) (MobileNetV1) on Arty A7 (using the soft CPU provided by Antmicro, see [antmicro/litex-vexriscv-tensorflow-lite-demo](https://github.com/antmicro/litex-vexriscv-tensorflow-lite-demo) for more info.)\r\n\r\n[Updated] August 11, 2020: Update speedup, previous calculation (8.55%) is wrong, it's 85.5% runtime compared to original implementation.", "comments": []}, {"number": 41946, "title": "Moving some filesystems to Transactional API part 1 ", "body": "This PR uncomments previously introduced changes and replaces the argument order to match inputs first and outputs last order for some Filesystems. Follow up PRs will move more filesystems to new API.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41946) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41946) for more info**.\n\n<!-- ok -->", "Retriggering builds since it seems Ubuntu CPU fialed", "This passed as flaky in GPU build but still failing in CPU build. Not sure if the issue is related with this PR or the state of the time of the branch. ", "Build should be green on master. Let's try resyncing?", "If we can have #42013 merged soon, I can make use of it in this PR after rebase. If you don't want that I will rebase this and make another PR for using the macro. What do you think?", "#42013 should land soon", "I rebased this one to todays master. Lets see if this pass.", "@mihaimaruseac it looks like it passed after rebase, though it requires re-approval.", "@mihaimaruseac current failure is not related with this PR. It seems something else changed on the master again.", "There seem to be internal failures too on the internal testing filesystems. I'll have to import manually.", "Rebasing and adding the new macro should fix all these errors. It is\npossible that name hiding causing issues in internal places that I couldn't\nsee.\n\nOn Tue, Aug 4, 2020, 9:12 PM Mihai Maruseac <notifications@github.com>\nwrote:\n\n> There seem to be internal failures too on the internal testing\n> filesystems. I'll have to import manually.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/41946#issuecomment-668969037>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACQNEFDQQXQLBP4MLGHMRITR7DL3FANCNFSM4PQ6A3LA>\n> .\n>\n", "Let's attempt that first then (rebasing and converting to use the macro)", "@mihaimaruseac, done. Would require re-approval though\r\n"]}, {"number": 41945, "title": "Can't use tensorflow", "body": "I'm trying to use keras, but the output says that keras requires tensorflow 2.2 or higher, which I already have installed. And when I try this command \r\n`import tensorflow as tf\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\nprint(sess.run(hello))`\r\nReturns: \r\n`Traceback (most recent call last):\r\n\r\n  File \"C:\\Users\\luiz_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\n\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: N\u00e3o foi poss\u00edvel encontrar o m\u00f3dulo especificado.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-4-9d5f9c01b035>\", line 1, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\Users\\luiz_\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n\r\n  File \"C:\\Users\\luiz_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n\r\n  File \"C:\\Users\\luiz_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n\r\n  File \"C:\\Users\\luiz_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\luiz_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\luiz_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: N\u00e3o foi poss\u00edvel encontrar o m\u00f3dulo especificado.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.`\r\n\r\nPlease help! I installed both tensorflow and keras with pip install", "comments": ["@Parrass \r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the latest [microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\n\r\n#41314 #40459 [link](https://github.com/tensorflow/tensorflow/issues/38916#issuecomment-619798621)\r\n\r\n\r\n\r\n", "I have a i7 8th gen. I dont think that it is the problem. In the course I did they recommended to use Spyder, from Anaconda. The installing process is the same?", "@Parrass \r\nPlease refer to [this link](https://docs.anaconda.com/anaconda/install/) for anaconda, we do not handle anaconda installation related issues.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41945\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41945\">No</a>\n"]}]