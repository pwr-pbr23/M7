[{"number": 39237, "title": "tf.keras.preprocessing.image.random_rotation makes a translation instead of rotation", "body": "**System information**\r\nstandard tesorflow docker, tensorflow/tensorflow:2.1.0-py3-jupyter\r\nv2.1.0-rc2-17-ge5bf8de 2.1.0\r\n\r\n**Describe the current behavior**\r\n![elephant](https://user-images.githubusercontent.com/26264000/81228345-f167f600-8fbb-11ea-8722-d25dcda78a0c.jpg)\r\n\r\n![descarga](https://user-images.githubusercontent.com/26264000/81228466-1a888680-8fbc-11ea-8801-5e9061f489b5.png)\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nit should rotate the image 90 degrees\r\n\r\n**Standalone code to reproduce the issue**\r\n![elephant](https://user-images.githubusercontent.com/26264000/81228345-f167f600-8fbb-11ea-8722-d25dcda78a0c.jpg)\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\ndef show(im,im2): \r\n    plt.subplot(1,2,1) \r\n    plt.imshow(im) \r\n    plt.subplot(1,2,2) \r\n    plt.imshow(im2) \r\n    plt.show() \r\n    \r\nimg = tf.io.read_file('elephant.jpg')\r\nimg = tf.image.decode_jpeg(img, channels=0)/255\r\nimg = tf.image.resize(img, (320,320))\r\nshow(img,tf.keras.preprocessing.image.random_rotation(img,90))\r\n```\r\nif I try \r\n```python\r\nshow(img,tf.keras.preprocessing.image.random_rotation(img, rg=30,row_axis=0,col_axis=1,channel_axis=2))\r\n```\r\nI got the following error\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-21-42509f257892> in <module>()\r\n     11 img = tf.image.resize(img, (180,320))\r\n     12 #np.rollaxis(img, 2, 0)\r\n---> 13 show(img,tf.keras.preprocessing.image.random_rotation(img, rg=30,row_axis=1,col_axis=0,channel_axis=2))\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py in random_rotation(x, rg, row_axis, col_axis, channel_axis, fill_mode, cval, interpolation_order)\r\n     56     x = apply_affine_transform(x, theta=theta, channel_axis=channel_axis,\r\n     57                                fill_mode=fill_mode, cval=cval,\r\n---> 58                                order=interpolation_order)\r\n     59     return x\r\n     60 \r\n\r\n/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py in apply_affine_transform(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)\r\n    321         transform_matrix = transform_matrix_offset_center(\r\n    322             transform_matrix, h, w)\r\n--> 323         x = np.rollaxis(x, channel_axis, 0)\r\n    324         final_affine_matrix = transform_matrix[:2, :2]\r\n    325         final_offset = transform_matrix[:2, 2]\r\n\r\n<__array_function__ internals> in rollaxis(*args, **kwargs)\r\n\r\n/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py in rollaxis(a, axis, start)\r\n   1272     axes.remove(axis)\r\n   1273     axes.insert(start, axis)\r\n-> 1274     return a.transpose(axes)\r\n   1275 \r\n   1276 \r\n\r\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'transpose'\r\n\r\n```\r\n\r\n[](https://colab.research.google.com/drive/1x8ySj1tt5zA8woEU6NUIH0AJqmYDgIop#scrollTo=sqxsXx6kxde2)\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n", "comments": ["I want to work on this issue **[#39237](https://github.com/tensorflow/tensorflow/issues/39237)**", "Was able to reproduce the issue with TF v2.1, [TF v2.2.0rc4](https://colab.research.google.com/gist/amahendrakar/786180db34adfd921ca7e629e3b6786d/39237.ipynb#scrollTo=3bJ-n9TCcWnX) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/c2f9fafb5d9795db6b49192a5cb1489a/39237-tf-nightly.ipynb). Please find the attached gist. Thanks!", "use the layers.preprocessing.RandomRotation layer", "@Dr-Gandalf As suggested by @tanzhenyu please use `layers.experimental.preprocessing.RandomRotation` layer as shown below. Please take a look at the gist [here](https://colab.research.google.com/gist/jvishnuvardhan/4698ae014a3960ce271a700de419200a/39237-tf-nightly.ipynb).\r\n\r\n```\r\ndata_augmentation = keras.Sequential([\r\n  layers.experimental.preprocessing.RandomRotation(0.25),\r\n])\r\n\r\naugmented_image = data_augmentation(tf.expand_dims(img, 0), training=True)\r\nshow(img,augmented_image[0].numpy())\r\n\r\n```\r\n\r\nPlease verify it once and close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39237\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39237\">No</a>\n", "# SOLUTION\r\nThe default arguments work on 4D images. If you want it to word in 3D images, use the argument `channel_axis=2`"]}, {"number": 39236, "title": "TF Hub BERT model output names mismatch with the singature.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\n# model is from: https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1\r\n\r\nsaved_model_dir = \"path/to/bert_en_cased_L-12_H-768_A-12/1\"\r\nloaded_model = tf.saved_model.load(saved_model_dir)\r\nconcrete_func = loaded_model.signatures[\"serving_default\"]\r\nprediction = concrete_func(**inputs)\r\n```\r\n\r\nExpected output names:\r\n```\r\nconcrete_fn.outputs\r\nOut[4]: \r\n[<tf.Tensor 'Identity:0' shape=(None, 768) dtype=float32>,\r\n <tf.Tensor 'Identity_1:0' shape=(None, None, 768) dtype=float32>]\r\n```\r\n\r\n**However, actual prediction returns a dict with different output names.**\r\n\r\n```\r\nprediction.keys()\r\nOut[5]: [u'bert_model_1', u'bert_model']\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nExpecting the prediction output has same name as `concrete_func.outputs`.", "comments": ["@1duo \r\n Please post TF_Hub related issues in the [Hub Repo.](https://github.com/tensorflow/hub/issues) Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39236\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39236\">No</a>\n"]}, {"number": 39235, "title": "Attension Layer Tensorflow TypeError: Cannot iterate over a tensor with unknown first dimension", "body": "I am trying to apply attention Layer to CRNN Econder and Decoder\r\nbut there is an error cant understand what it means?\r\n\r\n\r\n\r\nUse distribution to create a linear combination of value with shape batch_size, Tq, dim]: return tf.matmul(distribution, value).\r\n    cnn = MaxPooling2D(pool_size=(1,2), strides=(1,2), padding=\"valid\")(cnn)\r\n    shape = cnn.get_shape()\r\n    nb_units = shape[2] * shape[3]\r\n\r\n    gru = Reshape((shape[1], nb_units))(cnn)\r\n    gru, forward_h, forward_c, backward_h, backward_c = Bidirectional \\\r\n    (LSTM\r\n     (nb_units,\r\n      dropout=0.2,\r\n      return_sequences=True,\r\n      return_state=True,\r\n      recurrent_activation='relu',\r\n      recurrent_initializer='glorot_uniform'))(gru)\r\n\r\n    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\r\n    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\r\n    gru, attention_weights = Attention(32)([gru, state_h])\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-10-1b228a635105> in <module>()\r\n      4 # note: `learning_rate=None` will get architecture default value\r\n      5 model = HTRModel(architecture=arch, input_size=input_size, vocab_size=dtgen.tokenizer.vocab_size)\r\n----> 6 model.compile(learning_rate=0.001)\r\n      7 \r\n      8 # save network summary\r\n\r\nC:\\Users\\a.abdallah\\Desktop\\handwritten-text-recognition\\src\\network\\model.py in compile(self, learning_rate)\r\n    134 \r\n    135         # define inputs, outputs and optimizer of the chosen architecture\r\n--> 136         outs = self.architecture(self.input_size, self.vocab_size + 1, learning_rate)\r\n    137         inputs, outputs, optimizer = outs\r\n    138 \r\n\r\nC:\\Users\\a.abdallah\\Desktop\\handwritten-text-recognition\\src\\network\\model.py in abdo(input_size, d_model, learning_rate)\r\n    571 \r\n    572     \r\n--> 573     gru, attention_weights = Attention(32)([gru, state_h])\r\n    574 \r\n    575     \r\n\r\nc:\\users\\a.abdallah\\anaconda3\\envs\\handwritten\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py in __iter__(self)\r\n    546     if shape[0] is None:\r\n    547       raise TypeError(\r\n--> 548           \"Cannot iterate over a tensor with unknown first dimension.\")\r\n    549     for i in xrange(shape[0]):\r\n    550       yield self[i]\r\n\r\nTypeError: Cannot iterate over a tensor with unknown first dimension.\r\n", "comments": ["@abdoelsayed2016 \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version?\r\nRequest you to provide colab link or simple standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "**System information**\r\n- Windows 10 x64 \r\n- TensorFlow 2.1.0 GPU\r\n- Python version: 3.6.10:: Anaconda, Inc.\r\n- CUDA/cuDNN version:\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Fri_Feb__8_19:08:26_Pacific_Standard_Time_2019\r\nCuda compilation tools, release 10.1, V10.1.105\r\n\r\n\r\nYou can also obtain the TensorFlow version with:\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n2020-05-07 22:47:55.783521: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nv2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n", "@abdoelsayed2016 \r\n\r\nRequest you to provide colab link or simple standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Hello @ravikyram, Did you solved the problem. if yes, please help me with the same. \r\nI am trying to add attention layer in seq2seq model"]}, {"number": 39234, "title": "Symbol not found during bazel build", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX mojave 10.14.6 18.7.0 Darwin Kernel Version 18.7.0 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source commit 70087ab4f46a4bebaacce1023cd12bd9c655e159 (HEAD -> rc2.2, tag: v2.2.0-rc4, origin/r2.2) \r\n- TensorFlow version: 2.2\r\n- Python version:Python 3.6.10 :: Anaconda, Inc.\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): bazel 2.0.0 \r\n- GCC/Compiler version (if compiling from source):Target: x86_64-apple-darwin18.7.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory:NA\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen trying to bazel build --config=opt --copt=-g //tensorflow/tools/pip_package:build_pip_package , I get this error. \r\n```\r\nImportError: dlopen(/private/var/tmp/_bazel_vikumar/03ac0190026ff3e87deaafab7f769284/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: __ZN10tensorflow4data12experimental14SnapshotReader33kSnappyReaderInputBufferSizeBytesE\r\n  Referenced from: /private/var/tmp/_bazel_vikumar/03ac0190026ff3e87deaafab7f769284/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Expected in: flat namespace\r\n in /private/var/tmp/_bazel_vikumar/03ac0190026ff3e87deaafab7f769284/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n```\r\n\r\nI explicitly tried changing snapshot_util.h(where kSnappyReaderInputBufferSizeByte is defined) to force recompile, but still fails with same above error. \r\n```\r\nindex a2df3ccec1..0488136b2b 100644\r\n--- a/tensorflow/core/kernels/data/experimental/snapshot_util.h\r\n+++ b/tensorflow/core/kernels/data/experimental/snapshot_util.h\r\n@@ -70,6 +70,7 @@ class SnapshotReader {\r\n   // The reader input buffer size is deliberately large because the input reader\r\n   // will throw an error if the compressed block length cannot fit in the input\r\n   // buffer.\r\n+   static constexpr const int v =1; \r\n```\r\n\r\n\r\n\r\nMy tf.configure.bazelrc file contents\r\n```\r\n(tb2-2) 38f9d370d4a3:tensorflow vikumar$ cat .tf_configure.bazelrc \r\nbuild --action_env PYTHON_BIN_PATH=\"/Users/vikumar/anaconda3/envs/tb2-2/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/Users/vikumar/anaconda3/envs/tb2-2/lib/python3.6/site-packages\"\r\nbuild --python_path=\"/Users/vikumar/anaconda3/envs/tb2-2/bin/python\"\r\nbuild --config=xla\r\nbuild:opt --copt=-g\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel clean --expunge \r\n./configure\r\nbazel build --config=opt --copt=-g //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I think this should be fixed at HEAD. Please let us know if not", "This seems to have stalled. I'll close it but if it still reproduces at head please reopen.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39234\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39234\">No</a>\n"]}, {"number": 39233, "title": "[tf.image.convert_image_dtype] - Add extra warning if scaling is skipped", "body": "We have noticed recently a number of people facing input data scaling using `tf.image.convert_image_dtype`. \r\n\r\nThe problem is the following: we see users converting a tensor dtype unwantedly and having the normalization step skipped silently. See example below:\r\n\r\n```python\r\n# Bad order of operations\r\nimg_file = tf.read_file(\"my_filename_here.png\")   # Binary dtype (string)\r\nimg = tf.image.decode_png(img_file, channels=3, dtype=tf.uint8)   # UINT8 dtype\r\nimg = tf.image.resize_images(img, shape, shape)   # Silently converts to FP32\r\nimg = tf.convert_image_dtype(img , tf.float32)   # this operation is skipped silently\r\n\r\n# Correct order of operations\r\nimg_file = tf.read_file(\"my_filename_here.png\")   # Binary dtype (string)\r\nimg = tf.image.decode_png(img_file, channels=3, dtype=tf.uint8)   # UINT8 dtype\r\nimg = tf.convert_image_dtype(img , tf.float32)   # Converted to FP32 + Normalized\r\nimg = tf.image.resize_images(img, shape, shape)   # Rescaling\r\n```\r\n\r\nThe objective here is really simple, making sure the user is aware of this behavior and realize that the expected behavior might not be respected.\r\nIn addition the documentation should IMO highlight that the normalization is not systematic and only takes place when going from INT format to FP format.\r\n\r\nCC: @reedwm ", "comments": ["I actually think this is a bug (borderline) in resize_images. I would prefer if resize_images warned about automatically casting (but not properly converting) an image. I.e., if it warned about integer inputs. \r\n\r\nI do have a performance concern about a warning in these functions, they are often in input pipelines, and if we add a warning, it should be in C++ (inside the resize kernels which do the actual conversion), and it should be smart about not warning for every image, since most likely this affects either every image or none.", "@martinwicke \r\n\r\n> I do have a performance concern about a warning in these functions, they are often in input pipelines, and if we add a warning, it should be in C++ (inside the resize kernels which do the actual conversion), and it should be smart about not warning for every image, since most likely this affects either every image or none.\r\n\r\nI don't really agree here. The warning wouldn't be apply at each iterations. Only when you are designing the dataloading graph. The cost of one warning is virtually none.\r\n\r\n> I actually think this is a bug (borderline) in resize_images\r\n\r\nEven if we fix it, that does not fix the fact that if you input a floating point tensor (fp16, or fp32). I feel like the documentation is unclear and it leads a lot of users to crash into issues that are expensive to debug (finding that the rescale you expected is not here is no trivial task).\r\n\r\nJust in a one minute google search, we can find example of people bumping into this issue:\r\nhttps://stackoverflow.com/questions/54972167/bug-with-tf-image-convert-image-dtype-values-not-scaling\r\n\r\nI don't really mind, how we implement a fix on this. I feel like the OP shouldn't silently execute or not execute a normalization that may have adverse effects on lower precision compute (FP16, INT8).", "If you're strictly talking about the compat.v1 version I can see what you\nmean. But remember that TensorFlow supports (preferentially) eager\nexecution, so the Python function will be executed for each batch.\n\nI know that lots of users are confused. And if you look at other packages\n(e.g. OpenCV), you'll see that they have taken pretty similar approaches.\nFor better or worse, image representations are underspecified, and in order\nto write processing functions, a standard is necessary. The one chosen in\nTF is: float images are assumed to be [0,1) normalized (unless HDR), and\ninteger images are [0,max] representing fix point [0,1).\n\nThe bug in resize I am referring to is that it accepts non-float images,\nbut does not renornalize them. It therefore can return float images which\nare not in a native format.\n\nI don't believe we can change this behavior, but we can warn about integer\nimages passed directly to resize.\n\nIndependently, I agree this isn't well-understood, and If you have a\nsuggestion on how to improve the documentation I am definitely interested.\n\n>\n", "@DEKHTIARJonathan Can you please check @martinwicke's comments and keep us posted. Thanks!", "@martinwicke sorry for the delay in answer.\r\n\r\nImplementing a warning in `tf.image.resize_image` or more precisely this function: https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/ops/image_ops_impl.py#L1180-L1255 sounds a good idea. However, it can't be added in C++ without a massive refactoring. Most of the code is Python logic relying on basic/common TF C++ Operations.\r\n\r\nWhat I can do is to add a warning and make sure it's only printed once (probably using global var, I will see if I can avoid it).\r\n\r\n-------\r\n\r\nFor the second point, `tf.convert_image_dtype` should still IMHO issue a warning the user requests same kind of inputs/outputs dtype (integers/floating). Resize is just one scenario triggering this issue, it could happen with any TF operation that changes the dtype without the user realizing. Unfortunately same issue here again, no C++ code, just python logic and basic/common C++ TF Operations (identity and cast for our usecase). We can nonetheless apply the same trick as for `tf.image.resize_image` to warn only once the user.", "Fair point. Looking at the size of the Python logic in that function, it looks to me like we can add a simple `if input_dtype == output_dtype: WARN` without fear of terrible performance implications. \r\n\r\nWe have code to limit the warnings to the first n in platform/tf_logging, but we have deprecated that code since it's too close to generic python logging. Fundamentally, any such limit would involve a global (and that's a fine use of a global in my opinion).\r\n\r\nHowever, that is a question that @tensorflow/api-owners should maybe be involved in.", "I believe there are two potentially dangerous paths for this API:\r\n- identical dtypes (you already pointed out this one).\r\n- floating to floating: with more and more integration of FP16/BFLOAT. It would be wise to also protect that route. For some reason an OP could convert to FP16/BFLOAT16 without informing the user.\r\n\r\n@tensorflow/api-owners Please let me know how you would like this bug fixed. Or feel free to directly correct it ;) ", "Ah. Sorry, I mistyped this: I meant to say, we should add the following check to `tf.image.resize_image` (and other endpoints which share the \"feature\" of potentially silently changing dtypes):\r\n\r\n```\r\nif output_dtype != input_dtype: WARN\r\n```", "Np. Just left a review requesting changes so status is clear.", "@DEKHTIARJonathan Any update on this PR? Please. Thanks!", "@gbaned @martinwicke given that i didn't get any guidance on how to implement \"warn only once\". I prototyped something using a decorator and a python static (aka class) variable. This has two advantages:\r\n\r\n1. I don't touch the core logic of the function by itself.\r\n2. If we find a second function with the same issue, we can reuse the decorator as-is.\r\n\r\nThe decorator prints only on the first \"dangerous\" call:\r\n\r\n```python\r\nimport os\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    print(\"Processing with the correct order of operations ... \"\r\n          \"No warning shall be printed ...\")\r\n    for step in range(100):\r\n        # Bad order of operations\r\n        img_file = tf.compat.v1.read_file(\"my_image.png\")   # Binary dtype (string)\r\n        img = tf.image.decode_png(img_file, channels=3, dtype=tf.uint8)   # UINT8 dtype\r\n        img = tf.image.convert_image_dtype(img, tf.float32)   # Converts explicitly to FP32\r\n        img = tf.image.resize(img, (255, 255))   # No silent  conversion happens as the input is already in FP32\r\n\r\n    print(\"\\nProcessing with the dangerous order of operations ... \"\r\n          \"Only one warning shall be printed ...\")\r\n    for step in range(100):\r\n        # Bad order of operations\r\n        img_file = tf.compat.v1.read_file(\"my_image.png\")   # Binary dtype (string)\r\n        img = tf.image.decode_png(img_file, channels=3, dtype=tf.uint8)   # UINT8 dtype\r\n        img = tf.image.resize(img, (255, 255))   # Silently converts to FP32\r\n        img = tf.image.convert_image_dtype(img, tf.float32)   # this operation is skipped silently\r\n```\r\n\r\n```bash\r\n$ python main.py\r\nProcessing with the correct order of operations ... No warning shall be printed ...\r\n\r\nProcessing with the dangerous order of operations ... Only one warning shall be printed ...\r\nWARNING:tensorflow:The operation `_resize_images_common` has silently converted the data type from `<dtype: 'uint8'>` to `<dtype: 'float32'>`. This might have an adverse effect on data numerical range.\r\n```\r\n\r\n@martinwicke please let me know if that works with you.", "I think the comments in your code snippet are wrong?", "@martinwicke oh yes my bad. Fixing immediately the post above", "Cool. Some stylistic comments, I think in principle this will work.", "@DEKHTIARJonathan Can you please check @martinwicke's comments and keep us posted. Thanks!", "@DEKHTIARJonathan Any update on this PR? Please. Thanks!", "@martinwicke Can you please take a look on above comments from @DEKHTIARJonathan. Thanks!", "@martinwicke Can you please assist on above comments from @DEKHTIARJonathan. Thanks!", "@DEKHTIARJonathan  Can you please check build failures. Thanks!", "@DEKHTIARJonathan, Any update on this PR? Please. Thanks!", "I need a bit more time", "@DEKHTIARJonathan, Any update on this PR? Please. Thanks!", "@DEKHTIARJonathan, Any update on this PR? Please. Thanks!", "@gbaned sorry I need more time. I can't focus on this atm.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@DEKHTIARJonathan, Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 39232, "title": "Upgrade/Provide Docker Images based on Ubuntu 20.04 LTS", "body": "Upgrade/Provide Docker Images based on Ubuntu 20.04 LTS", "comments": ["@tarmath Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "I am running the official docker image with gpu support on kubernetes\r\n(upgraded to 2.2.0-gpu as of yesterday)\r\n\r\nCurrently, those docker images are based on the 2 years old Ubuntu 18.04 LTS (Long Term Support) Release. A new Ubuntu LTS Release (20.04, for April 2020) came out a couple weeks ago, and as such, it hopefully follows that tensorflow would naturally start migrating towards it.\r\n\r\nAmongst other things, this latest official release comes with python 3.8, which itself provides a ton of improvements, as announced here:\r\n\r\nhttps://docs.python.org/3.7/whatsnew/3.7.html\r\nhttps://docs.python.org/3/whatsnew/3.8.html\r\n\r\nIt also may be pertinent to mention that the nvidia cuda drivers work perfectly under this new release.\r\n\r\nLet me know if you would like anymore background on this issue!", "/cc @angerson @perfinion ", "@tarmath \r\nIs this still an issue", "Yes, this work is as close as can be to being inevitable. \r\n\r\nAs pointed out in previous comments, there seems to be no blocker for this to occur. \r\n\r\nAs I understand there may be competing priorities, it would be great to minimally have a sense on when this could potentially happen, if nothing else.\r\n\r\nAppreciate you taking time for this! \ud83d\ude4f", "the [install docs](https://www.tensorflow.org/install) say \"Ubuntu 16.04 or later\".  please update the docker images to 20.04!  thanks.", "This isn't likely to be prioritized any time soon -- all our Dockerfiles are defined [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/dockerfiles) and you're welcome to submit a PR.", "@angerson @perfinion Any updates on this? I think many people would like to upgrade to the new LTS, including myself. Currently, this is the only issue holding me back...", "why does this issue have the tf2.2 label?  would be nice to see ubuntu 20.04 docker support in TF2.4!", "I think there are some dockerfiles based on 20.04 already [here](https://github.com/tensorflow/tensorflow/tree/6618bbc2d145db7e9da898dff68be9d1faa5c59a/tensorflow/tools/dockerfiles/dockerfiles/onednn).\r\nHowever, I guess the ci release step doesn't docker push such dockerfiles on docker hub yet.\r\n\r\nAlso, I think something wrong with using this [file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/partials/onednn/ubuntu/version.partial.Dockerfile). It makes all dockerfiles with Ubuntu 20.04 unintentionally as [this file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/onednn/ubuntu-16.04-devel-jupyter.Dockerfile#L22)\r\n\r\nIf you guys have a little time, Could you explain the file or docs which explains \"how pushing dockerfiles process works in CI step\"? @angerson \r\nI'd like to handle this issue for my first contribution to this awesome project, but I couldn't get such process in this project by just searching \"docker push\" in the source code.", "@angerson you have a volunteer to resolve this issue in @anencore94 .  could you please help him per his request ^", "@anencore94 thanks for the links.  as a temporary workaround i've simply pre-pended my dockerfile that used to `FROM tensorflow/tensorflow:latest-gpu` with\r\n\r\n```\r\nARG UBUNTU_VERSION=20.04\r\n\r\nFROM ubuntu:${UBUNTU_VERSION} as base\r\n\r\nRUN apt-get update && apt-get install -y curl\r\n\r\n# See http://bugs.python.org/issue19846\r\nENV LANG C.UTF-8\r\n\r\nRUN apt-get update && apt-get install -y \\\r\n    python3 \\\r\n    python3-pip\r\n\r\nRUN python3 -m pip --no-cache-dir install --upgrade \\\r\n    pip \\\r\n    setuptools\r\n\r\nRUN ln -s $(which python3) /usr/local/bin/python\r\n\r\nARG TF_PACKAGE=tensorflow\r\nARG TF_PACKAGE_VERSION=\r\nRUN python3 -m pip install --no-cache-dir ${TF_PACKAGE}${TF_PACKAGE_VERSION:+==${TF_PACKAGE_VERSION}}\r\n```\r\n\r\nthat is, i replaced `FROM tensorflow...` with the above, which is straight from one of your links.", "@bjarthur just want to point out that since you're not installing the cuda drivers, you will not get any gpu acceleration with this solution.", "Still no 20.04 images as of 2.4.1, quite sad. I hope once they are released (if ever), it will be communicated and tracked somewhere as having to literally run a container to find out what ubuntu version it has is an overhead.", "Is there any traction on this? We're mostly deciding whether to wait for an official Tensorflow release or attempt it ourselves. If it's in the works then we're happy to wait.", "Any news on this?", "There's been some progress, but nothing new available yet: see\nhttps://github.com/tensorflow/tensorflow/pull/48371 and\nhttps://github.com/tensorflow/build/pull/21.\n\nOn Tue, Apr 13, 2021 at 6:02 AM Alexander Kurakin ***@***.***>\nwrote:\n\n> Any news on this?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/39232#issuecomment-818717940>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AHXWEQBTCANCE3EJTRJYSS3TIQ6IDANCNFSM4M2U3Z4A>\n> .\n>\n", "@tarmath,\r\n\r\nThe [PR](https://github.com/tensorflow/tensorflow/pull/51914/files) to update docker files to Ubuntu 20.04 is merged already and can you confirm if that helps resolving your issue? Thanks!", "Hi @sanatmpa1! \r\n\r\nThanks for the nudge! \r\n\r\nI have just tried downloading the latest `tensorflow/tensorflow:latest-gpu` image, and it is still on 18.04:\r\n\r\n```shell\r\nroot@d3bdbdcc7fa2:/# cat /etc/lsb-release \r\nDISTRIB_ID=Ubuntu\r\nDISTRIB_RELEASE=18.04\r\nDISTRIB_CODENAME=bionic\r\nDISTRIB_DESCRIPTION=\"Ubuntu 18.04.5 LTS\"\r\n```\r\n\r\nMy guess is that we'll be seeing images based on the newer stack (besides the nightly ones) next time tensorflow is released...\r\n\r\nI'll be looking forward to that!", "@tarmath Can you also try out `tensorflow/tensorflow:nightly-gpu`? Those have picked up the changes, while the other main ones will get updated once the next TF release happens.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39232\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39232\">No</a>\n"]}, {"number": 39231, "title": "Add mlir graph optimization to the build", "body": "This PR is part of the process to resolve #39135. \r\n\r\nAs was mentioned in #39135, 248bc00 enables MLIR graph optimizations. However, it is not part of the build for pip wheel. The reason was that MLIR graph optimizations is done through a registration (mlir_graph_optimization_pass_registration.cc) but this file is not included in `libtensorflow_framerowk.so` so pip wheel package is not enabled.\r\n\r\nThis PR add the `mlir_graph_optimization_pass_registration` to be part of the `libtensorflow_framework.so`. Due to the `bazel` dependency reasons, a direct inclusion will not work as ops in core and xla will be pulled in multiple times by bazel. There will be 2 copies of xla ops, 2 copies of lite and core ops in `libtensorflow_framework.so`\r\n\r\nSo the following change has been made.  In general `import_model.[h|cc]` has been split into 3 parts:\r\n- new `import_base.[h|cc] consists of ImporterBase class and common shared functions (by `import_model.[h|cc]` and `import_graphdef.[h|cc]`\r\n- new `import_graphdef.[h|cc]` consists of graph only conversion to mlir\r\n- the `import_model.[h|cc]` has also been updated to consist model only conversion to mlir\r\n\r\nThere are also some small bazel changes that removes unnecessary dependencies.\r\n\r\nNote with the change, MLIR graph optimizations only need to depends on `import_graphdef.cc` and `export_graphdef.cc`. It will not depend on `import_model.cc` which pulled in xla and core ops multiple time by bazel.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks for the fix! Seems like the CI is not yet happy with it right now.", "Thanks @joker-eph for the review. The tests error was due to tests normally depends on LLVM so there are two copies again (one in libtensorflow_io.so and one in tests binary itself). While it is possible to updates the tests, there are so many and dependencies are just too interleaved.\r\n\r\nI take another look and now the PR takes a different approach: it adds `graph_optimization_pass_registration` to  `tensorflow/python/_pywrap_mlir.so`.\r\n\r\n`_pywrap_mlir.so` is the python binding for mlir related API. At the moment only `tf.mlir.experimental.convert_graph_def` is exposed.\r\n\r\nSince `tensorflow/python/_pywrap_mlir.so` is a python biding, it will be loaded once `import tensorflow` is executed. _pywrap_mlir.so already depends on LLVM anyway so it fits as well.\r\n\r\nIn the long term it still might makes sense to build `graph_optimization_pass`  into `libtensorflow_io.so`, as it is the natural place for core components. Though for now, I think it will be a viable approach to build `graph_optimization_pass` to `_pywrap_mlir.so`.\r\n\r\n", "@joker-eph The PR has been updated. The `import_module.[cc|h]` does not need to be split now. Please take a look and let me know if there are any issues."]}, {"number": 39230, "title": "tf.keras.losses.categorical_crossentropy and binary_crossentropy (and other losses) only works for channels_last layout networks", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Pip3\r\n- TensorFlow version (use command below): v2.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nLooks like in a bizarre api design the constructor of neither `tf.keras.losses.categorical_crossentropy` nor `tf.keras.losses.binary_crossentropy` take an axis parameter as input. An axis specifies along which dimensions one wants to calculate these losses. Current implementation simply assumes last axis. Without a way to specify a different axis in the constructor, these can only be used for networks where `image_data_layout` is set to `channels_last`. `channels_first` network are out of luck because they require different axis. \r\n\r\nAlso inconsistent api design is evident in `tf.keras.backend.categorical_crossentropy`. This backend version of categorical_crossentropy does that an axis as an input in its constructor. Though one can use this as a loss function as a workaround, it still does not solve the binary cross entropy problem. And also results in an overall inconsistent api.\r\n\r\nThe `tf.keras.losses.categorical_crossentropy` is actually calling the backend version but since axis was an optional parameter, it gets set by default to -1 all the time. See [this line in the losses.py code](https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/losses.py#L971)\r\n\r\n**Describe the expected behavior**\r\n\r\n `tf.keras.losses.categorical_crossentropy` and `tf.keras.losses.binary_crossentropy` (and may be all other loss functions) should take an axis parameter as input. So that these losses can be used in channels_first setups.\r\n\r\n**Standalone code to reproduce the issue**\r\nNone needed. See the documentation of these loss functions and see the source code link above.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@dd1923 \r\n\r\nWill it be possible to share the colab link or simple standalone code to reproduce the issue reported here.It helps us in localizing the issue faster.Thanks!", "@ravikyram Did you read the post above? What do you want to reproduce? The documentation and the code link I mentioned clearly shows it. This is a case of bad api design.", "@pavithrasv ", "@jhseu @pavithrasv when do you think this would be assigned to someone?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39230\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39230\">No</a>\n"]}, {"number": 39229, "title": "map_fn doesn't work with empty lists", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.2.0-rc4-0-g70087ab4f4\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\nmap_fn doesn't support empty lists\r\n\r\n**Describe the expected behavior**\r\nIt should return an empty list\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfn = lambda x: x\r\ntf.map_fn(fn, [])\r\n\r\n# additionally, this works:\r\ntf.map_fn(fn, np.array([1.]))\r\n# but not this eventhough [1.] is not a scalar.\r\ntf.map_fn(fn, [1.])\r\n```", "comments": ["The second and the third examples are expected. In the second example,\r\n```\r\ntf.map_fn(fn, np.array([1.]))\r\n```\r\n`np.array([1.])` is a tensor (converted to tensor by np.array), so it is a **single element of shape `[1]`.**\r\n\r\nThe third example:\r\n```\r\ntf.map_fn(fn, [1.])\r\n```\r\nis a list of one tensor `1.` as map_fn treat it as **a list of one (scalar) element `1.`**, thus you see an error.\r\n\r\nThe first example `tf.map_fn(fn, [])` is not something that has been captured in the implementation of `tf.map_fn`. I think it makes sense to return an explicit error here.\r\n\r\nCreated a PR #39241 for the fix of  `tf.map_fn(fn, [])`", "Hi,\r\n\r\nI really don't think you should raise when the list is empty. Every framework out there just returns an empty sequence when using map on an empty sequence.", "Was able to reproduce the issue with TF v2.1, [TF v2.2.0-rc4](https://colab.research.google.com/gist/amahendrakar/c164706bf7bd7f67bf13fe6719e0a489/39229.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/81517ce8a62dd8cc9f121c1956630636/39229-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@AdrienCorenflos If you want to map over a sequence, then you must pass that sequence to `map_fn` as a Tensor.  \r\n\r\nIf you pass a non-Tensor sequence to `map_fn`, then it does *not* map over that sequence.  Instead, it unstacks each tensor in that sequence, and calls the function with a list constructed from those unstacked slices.\r\n\r\nI.e., `tf.map_fn(func, [a, b, c])` is equivalent to Python's `map(func, a, b, c)`, and *not* to `map(func, [a, b, c])`.  So calling `tf.map_fn(func, [])` is equivalent to calling `map(func)` in Python, which does indeed give an error (`\"map() requires at least two args\"`).\r\n\r\nA more complex example might help illustrate what's going on here -- we can pass any nested structure in to `elems`, including e.g. nested dictionaries.  So if I call:\r\n\r\n```\r\ntf.map_fn(func, {'a': t1, 'b': [t2, t3]})\r\n```\r\n\r\nThen `func` will be called with:\r\n```\r\nfunc({'a': t1[0], 'b': [t2[0], t3[0]})\r\nfunc({'a': t1[1], 'b': [t2[1], t3[1]})\r\nfunc({'a': t1[2], 'b': [t2[2], t3[2]})\r\n...\r\nfunc({'a': t1[N], 'b': [t2[N], t3[N]})\r\n```\r\n\r\nWhere `N==t1.shape[0]==t2.shape[0]==t3.shape[0]`.  By default, each function needs to return a value with the same structure that was passed in.  E.g., in the example above, `func` must return a dictionary with keys `a` and `b`, where `a` is a tensor and `b` is a list of two tensors.  If you want `func` to return a different structure, then you need to specify that structure with the `fn_output_signature` argument.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39229\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39229\">No</a>\n"]}, {"number": 39228, "title": "Cannot build TFLite with OpenCL SYCL support", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow version: master (May 6th, 2020)\r\n- Bazel version (if compiling from source): 2.0.0\r\n\r\n**Describe the problem**\r\nCannot build TFLite with OpenCL SYCL support. Not with GCC (version 7.5.0). Not with Clang (version 10.0.0).\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nUsing GCC (version 7.5.0)\r\n```\r\nterminal: python3 configure.py\r\nExtracting Bazel installation...\r\nYou have bazel 2.0.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.6/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.6/dist-packages]\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: y\r\nOpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nPlease specify which C++ compiler should be used as the host C++ compiler. [Default is /usr/bin/g++]:\r\n\r\n\r\nPlease specify which C compiler should be used as the host C compiler. [Default is /usr/bin/gcc]:\r\n\r\n\r\nDo you wish to build TensorFlow with ComputeCPP support? [Y/n]: y\r\nComputeCPP support will be enabled for TensorFlow.\r\n\r\nPlease specify the location where ComputeCpp for SYCL 1.2 is installed. [Default is /usr/local/computecpp]: /opt/ComputeCpp-CE-2.0.0-x86_64-linux-gnu\r\n\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nterminal: bazel build -c opt --config=sycl //tensorflow/lite:libtensorflowlite.so\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [sycl]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=275\r\nINFO: Reading rc options for 'build' from /media/ssd512/tensorflow-master/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /media/ssd512/tensorflow-master/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /media/ssd512/tensorflow-master/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages --python_path=/usr/bin/python3 --config=xla --config=sycl --action_env HOST_CXX_COMPILER=/usr/bin/g++ --action_env HOST_C_COMPILER=/usr/bin/gcc --action_env TF_NEED_COMPUTECPP=1 --action_env COMPUTECPP_TOOLKIT_PATH=/opt/ComputeCpp-CE-2.0.0-x86_64-linux-gnu --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /media/ssd512/tensorflow-master/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /media/ssd512/tensorflow-master/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:sycl in file /media/ssd512/tensorflow-master/.bazelrc: --crosstool_top=@local_config_sycl//crosstool:toolchain --define=using_sycl=true --action_env TF_NEED_OPENCL_SYCL=1\r\nINFO: Found applicable config definition build:sycl in file /media/ssd512/tensorflow-master/.bazelrc: --crosstool_top=@local_config_sycl//crosstool:toolchain --define=using_sycl=true --action_env TF_NEED_OPENCL_SYCL=1\r\nINFO: Found applicable config definition build:linux in file /media/ssd512/tensorflow-master/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /media/ssd512/tensorflow-master/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - /media/ssd512/tensorflow-master/WORKSPACE:37:1\r\nERROR: /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/local_config_sycl/crosstool/BUILD:12:1: @local_config_sycl//crosstool:cc-compiler-local: missing value for mandatory attribute 'toolchain_config' in 'cc_toolchain' rule\r\nERROR: /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/local_config_sycl/crosstool/BUILD:12:1: Target '@local_config_sycl//crosstool:empty' contains an error and its package is in error and referenced by '@local_config_sycl//crosstool:cc-compiler-local'\r\nERROR: /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/local_config_sycl/crosstool/BUILD:5:1: Target '@local_config_sycl//crosstool:cc-compiler-local' contains an error and its package is in error and referenced by '@local_config_sycl//crosstool:toolchain'\r\nERROR: /media/ssd512/tensorflow-master/tensorflow/lite/BUILD:574:1: every rule of type cc_binary implicitly depends upon the target '@local_config_sycl//crosstool:toolchain', but this target could not be found because of: Target '@local_config_sycl//crosstool:toolchain' contains an error and its package is in error\r\nERROR: Analysis of target '//tensorflow/lite:libtensorflowlite.so' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 11.888s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (24 packages loaded, 53 targets configured)\r\n```\r\nUsing Clang (version 10.0.0)\r\n```\r\nterminal: python3 configure.py\r\nExtracting Bazel installation...\r\nYou have bazel 2.0.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.6/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: y\r\nOpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nPlease specify which C++ compiler should be used as the host C++ compiler. [Default is /usr/bin/g++]: /opt/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04/bin/clang++\r\n\r\n\r\nPlease specify which C compiler should be used as the host C compiler. [Default is /usr/bin/gcc]: /opt/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04/bin/clang\r\n\r\n\r\nDo you wish to build TensorFlow with ComputeCPP support? [Y/n]: y\r\nComputeCPP support will be enabled for TensorFlow.\r\n\r\nPlease specify the location where ComputeCpp for SYCL 1.2 is installed. [Default is /usr/local/computecpp]: /opt/ComputeCpp-CE-2.0.0-x86_64-linux-gnu\r\n\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nterminal: bazel build -c opt --config=sycl //tensorflow/lite:libtensorflowlite.so\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [sycl]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=275\r\nINFO: Reading rc options for 'build' from /media/ssd512/tensorflow-master/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /media/ssd512/tensorflow-master/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /media/ssd512/tensorflow-master/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=xla --config=sycl --action_env HOST_CXX_COMPILER=/opt/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04/bin/clang++ --action_env HOST_C_COMPILER=/opt/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04/bin/clang --action_env TF_NEED_COMPUTECPP=1 --action_env COMPUTECPP_TOOLKIT_PATH=/opt/ComputeCpp-CE-2.0.0-x86_64-linux-gnu --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /media/ssd512/tensorflow-master/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /media/ssd512/tensorflow-master/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:sycl in file /media/ssd512/tensorflow-master/.bazelrc: --crosstool_top=@local_config_sycl//crosstool:toolchain --define=using_sycl=true --action_env TF_NEED_OPENCL_SYCL=1\r\nINFO: Found applicable config definition build:sycl in file /media/ssd512/tensorflow-master/.bazelrc: --crosstool_top=@local_config_sycl//crosstool:toolchain --define=using_sycl=true --action_env TF_NEED_OPENCL_SYCL=1\r\nINFO: Found applicable config definition build:linux in file /media/ssd512/tensorflow-master/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /media/ssd512/tensorflow-master/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - /media/ssd512/tensorflow-master/WORKSPACE:37:1\r\nERROR: /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/local_config_sycl/crosstool/BUILD:12:1: @local_config_sycl//crosstool:cc-compiler-local: missing value for mandatory attribute 'toolchain_config' in 'cc_toolchain' rule\r\nERROR: /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/local_config_sycl/crosstool/BUILD:12:1: Target '@local_config_sycl//crosstool:empty' contains an error and its package is in error and referenced by '@local_config_sycl//crosstool:cc-compiler-local'\r\nERROR: /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/local_config_sycl/crosstool/BUILD:5:1: Target '@local_config_sycl//crosstool:cc-compiler-local' contains an error and its package is in error and referenced by '@local_config_sycl//crosstool:toolchain'\r\nERROR: /media/ssd512/tensorflow-master/tensorflow/lite/BUILD:574:1: every rule of type cc_binary implicitly depends upon the target '@local_config_sycl//crosstool:toolchain', but this target could not be found because of: Target '@local_config_sycl//crosstool:toolchain' contains an error and its package is in error\r\nERROR: Analysis of target '//tensorflow/lite:libtensorflowlite.so' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 12.664s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (23 packages loaded, 55 targets configured)\r\n```", "comments": ["same issue :( would love to get opencl working", "@Lotte1990 \r\n@Titaniumtown \r\nCould you please refer to this [similar issue](https://github.com/tensorflow/tensorflow/issues/33733#issuecomment-547498674) and let us know if it helps.\r\n[link1](https://www.codeplay.com/portal/03-30-17-setting-up-tensorflow-with-opencl-using-sycl)\r\n[link2](https://missinglink.ai/guides/tensorflow/tensorflow-support-opencl/)", "No, this doesn't fix the problem for me.", "Bump. Anyone figure out the issue? I still can't get it to build. I've installed computecpp and everything (I'm on arch linux)", "I'm from Codeplay, we implemented the OpenCL SYCL support for TensorFlow. TFLite has never been supported with OpenCL SYCL I'm afraid. It would be a big effort to do that and at the moment there are no plans (by us or others) I am aware of to do the work.", "Well, I get the issue when I compile TFLite and regular tensorflow.\r\n@rodburns Isn't your fork of tensorflow only 1.x atm? What's the plan for 2.x support?", "TFLite has OpenCL delegate but it's enabled only for mobile devices.\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/gpu/cl\r\n\r\nCan I assume this is a feature request of OpenCL support for normal Linux machines?", "@rodburns regarding comment https://github.com/tensorflow/tensorflow/issues/39228#issuecomment-630113166\r\n\r\nTFLite from v1.15 has support for OpenCL through delegates and can be used on mobile and desktop, but if someone wants to add custom ops as far as I understand he/she will have to write OpenCL code.\r\n\r\nIs there a version of SYCL that supports TFlite so that you don't have to write OpenCL code for new custom ops?", "@terryheo regarding https://github.com/tensorflow/tensorflow/issues/39228#issuecomment-630608530\r\n\r\nthe delegates workout of the box on the desktop as well, but I've found out all kernels are written using OpenCL and if you want to add new custom ops, you have to write them also using OpenCL.\r\n\r\nAny help on using SYCL?", "Currently we don't support SYCL for TFLite. @hamlatzis  Could you share what's your target devices and why do you want to use SYCL? Can OpenCL solve your issue?", "> \r\n> \r\n> Well, I get the issue when I compile TFLite and regular tensorflow.\r\n> @rodburns Isn't your fork of tensorflow only 1.x atm? What's the plan for 2.x support?\r\n\r\nAt the moment the support is only for TF v1.9 using ComputeCpp, we currently don't have plans for 2.x but there are discussions about how that could happen.", "> \r\n> \r\n> Currently we don't support SYCL for TFLite. @hamlatzis Could you share what's your target devices and why do you want to use SYCL? Can OpenCL solve your issue?\r\n\r\nThis is correct. Apologies if there is some confusion. The SYCL implementation we have done relies on OpenCL drivers to ingest the instructions, and this has not been implemented for TFLite. I'm not fully familiar with the OpenCL work done for mobile devices.", "@Titaniumtown Can you be clearer on your problem please? If you are using TF v1.9 with the SYCL part turned on and are having issues please raise a new issue with details of the problems you are having and tag me, we will try to help you. Please be clear about your environment set up as well.", "@rodburns (https://github.com/tensorflow/tensorflow/issues/39228#issuecomment-630712615)\r\n\r\nI'm targeting at the moment Android devices, maybe later move on to iOS. I could do my job writing OpenCL and that's what I'm currently doing, but as I'm not that familiar with OpenCL the work progresses very slowly (_I think a tortoise moves faster_)\r\n\r\nI would love to have one codebase in c++ and have the compiler do its magic behind the scenes, like in TF", "Hi! @Lotte1990 ,\r\nCould you please try on latest stable version of TF 2.6 or nightly  and let us know if this is still an issue.Thanks!", "FYI, `TensorFlow with OpenCL SYCL` option is only applied for TensorFlow build. It's not a TensorFlow Lite build option.\r\n\r\nFor the TFLite OpenCL, you should check https://www.tensorflow.org/lite/guide/build_cmake#opencl_gpu_delegate", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39228\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39228\">No</a>\n"]}, {"number": 39227, "title": "FailedPreconditionError:  Error while reading resource variable _AnonymousVar555 from Container: localhost.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow version (use command below):2.2.0-rc-4\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n> FailedPreconditionError:  Error while reading resource variable _AnonymousVar555 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar555/N10tensorflow3VarE does not exist.\r\n> \t [[node MatMul_388/ReadVariableOp (defined at <ipython-input-86-300e1b0d20e1>:30) ]] [Op:__inference_keras_scratch_graph_21594]\r\n> \r\n> Function call stack:\r\n> keras_scratch_graph\r\n\r\nAt building Model\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1IYvi-iIM2gcTVBPhE2K2qjTVUURyZvIi\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have tried in colab with TF 2.1.0 , 2.2-rc4 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/9222785fa6154ec7fae7b3bbd78e12e3/untitled863.ipynb).Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39227\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39227\">No</a>\n"]}, {"number": 39226, "title": "Inputs mismatch in model.predict does not raise Error", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.4\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.5\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nI wrote a simple model that takes 1 input array and outputs another. I produce then 2 input arrays, one with the right dimension and the other with wrong dimensions. If you feed to model.predict the 2 input arrays, the correct one first, the model simply takes the first array and the run goes fine. The second input is completely ignored.\r\n\r\n**Describe the expected behavior**\r\nI would expect an error like \"You are giving two input arrays to a model that expects one\"\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Dense\r\nimport numpy as np\r\n\r\n\r\ndef generator(input_dim, output_dim):\r\n        # Input\r\n        g_input = Input(input_dim, name='noise_input')\r\n\r\n        # Dense block\r\n        x = Dense(32)(g_input)\r\n\r\n        # Final layer\r\n        g_output = Dense(output_dim)(x)\r\n\r\n        G = Model(g_input, g_output, name='Generator')\r\n        return G\r\n\r\n\r\ninput_dim = 6\r\noutput_dim = 2\r\n\r\ngen = generator(input_dim, output_dim)\r\n\r\nsample_size = 5\r\n\r\nnoise_data_1 = np.random.normal(0, 1, size=(sample_size, input_dim))\r\nnoise_data_2 = np.random.normal(0, 1, size=(sample_size, 1))\r\n\r\n\r\nout = gen.predict([noise_data_1, noise_data_2])\r\n\r\nprint(out)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue with TF v2.2.0rc4. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/c2833385469c38ee05ad804f217f9159/39226.ipynb). Thanks!", "@LucaMantani This is intended behavior of the `model.predict(x)` which is defined [here](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#predict). \r\n\r\nIf you look at the input `x`, it should be an array or or a list of arrays (in case the model has multiple inputs). \r\n\r\n```\r\nArguments:\r\nx: Input samples. It could be:\r\nA Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs).\r\nA TensorFlow tensor, or a list of tensors (in case the model has multiple inputs).\r\nA tf.data dataset.\r\nA generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior for iterator-like inputs section of Model.fit.\r\n```\r\nIn your case, you have a model that takes single input so when you pass a list of arrays ([noise_data_1, noise_data_2]), it is taking  array[0] only as it was designed for single input. \r\n\r\nIf you try the following line, it will throw mismatch error that you are expecting.\r\n```\r\n# It will throw size mismatch error\r\nout2 = gen.predict([noise_data_2])\r\nprint(out2)  # throw InvalidArgumentError:  Matrix size-incompatible: In[0]: [5,1], In[1]: [6,32]\r\n```\r\n\r\nIn another case where given input is longer that what is expected, it will throw an error as shown below\r\n\r\n```\r\ngen2 = generator(input_dim, output_dim)\r\nnoise_data_3 = np.random.normal(0, 1, size=(sample_size, input_dim+5))\r\nnoise_data_4 = np.random.normal(0, 1, size=(sample_size, 1))\r\nout2 = gen2.predict([noise_data_3, noise_data_4]) # this will throw an error as follows\r\n#     ValueError: Input 0 of layer dense_4 is incompatible with the layer: expected axis -1 of input shape to have value 6 but received input with shape [None, 11]\r\n```\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/5bc15e5deb21f7654b274eb57b7ca28a/39226.ipynb). \r\n\r\nPlease verify once and close the issue. Thanks!", "Yes, I understand what is happening. But in particular\r\n\r\n> you have a model that takes single input so when you pass a list of arrays ([noise_data_1, noise_data_2]), it is taking array[0] only as it was designed for single input.\r\n\r\nshouldn't be better to raise an error? This \"feature\" that if you feed multiple inputs to a single input NN it just takes the first in the list could very well make it difficult to spot bugs in codes.", "@LucaMantani This is well documented and only related to `model.predict`. Changing this behavior may not be as simple as it looks and could affect performance (checking input signature for each element of array list). Thanks!", "Just to jump on that. Even if you don't use model.predict but just use this generator as model within the functional API in the following way\r\n\r\n\r\n    from tensorflow.keras.models import Model\r\n    from tensorflow.keras.layers import Input, Dense\r\n    import numpy as np\r\n\r\n\r\n    def generator(input_dim, output_dim):\r\n        # Input\r\n        g_input = Input(input_dim, name='noise_input')\r\n\r\n        # Dense block\r\n        x = Dense(32)(g_input)\r\n\r\n        # Final layer\r\n        g_output = Dense(output_dim)(x)\r\n\r\n        G = Model(g_input, g_output, name='Generator')\r\n        return G\r\n\r\n\r\n    input_dim = 6\r\n    output_dim = 2\r\n\r\n    gen = generator(input_dim, output_dim)\r\n\r\n\r\n    noise_data_1 = Input((input_dim))\r\n    noise_data_2 = Input(1)\r\n\r\n\r\n    out_tensor = gen([noise_data_1, noise_data_2])\r\n\r\n    print(out_tensor)\r\n\r\nEven then it doesn't give an error. This is potentially dangerous isn't it?", "@LucaMantani Looks like this was resolved. I checked your code with `tf-nightly` and it throws an shape mismatch error as shown below. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/3449cb4058fcc994da8164c3722d23ea/untitled.ipynb).\r\n\r\nHere is the error\r\n\r\n`ValueError: Layer Generator expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 6) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 1) dtype=float32>]`\r\n\r\nPlease verify once and close the issue if this was resolved for you. thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39226\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39226\">No</a>\n"]}, {"number": 39225, "title": "make `bazel build` work on aarch64/arm linux", "body": "make\r\n```\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\n````\r\nwork again on aarch64 (e.g., EdgeTPU Dev board and Jetson boards)\r\nand arm (e.g, Raspberry Pi 3/4 boards) Linux platforms\r\n\r\nwithout this patch, I saw\r\n\r\n```\r\n...\r\nERROR: While resolving toolchains for target //tensorflow:libtensorflow_framework.so.2.1.0: No matching toolchains found for types @bazel_tools//tools/cpp:toolchain_type. Maybe --incompatible_use_cc_configure_from_rules_cc has been flipped and there is no default C++ toolchain added in the WORKSPACE file? See https://github.com/bazelbuild/bazel/issues/10134 for details and migration instructions.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: No matching toolchains found for types @bazel_tools//tools/cpp:toolchain_type. Maybe --incompatible_use_cc_configure_from_rules_cc has been flipped and there is no default C++ toolchain added in the WORKSPACE file? See https://github.com/bazelbuild/bazel/issues/10134 for details and migration instructions.\r\n```", "comments": ["Please add this, I want to compile tensorflow on my raspberry pi 4."]}, {"number": 39224, "title": "Use tf.data and tf.keras in multitasking (each task uses a different custom loss)", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.0\r\n- Python version:3.6\r\n\r\n\r\n**description**\r\n**step 1)** I used tf.keras to build a single-input dual-output multi-task model, each output loss is customized (by inheriting tf.keras.losses.Loss).\r\n<img width=\"174\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15835439/81172546-069b4f80-8fd1-11ea-881a-93fecf39992f.png\">\r\n**step 2)** Construct a data set through tf.data-(image, (label1, label2)).\r\n**step 3)** \r\n<img width=\"359\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15835439/81172981-d7d1a900-8fd1-11ea-97c9-85933f1deaa4.png\">\r\n<img width=\"176\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15835439/81172995-df914d80-8fd1-11ea-9852-aff980257594.png\">\r\n**bug**\r\n\r\nI found that the two outputs of the network were sent to the corresponding loss functions, but the corresponding label1 and label2 in tf.data were not sent to the two loss functions\r\n \r\n\r\n", "comments": ["@liupengkd \r\nCan you please share simple stand alone code for us to replicate the issue or is possible share a colab gist for us to analyse the issue faced.", " @Saduf2019 \r\ncode like this\uff1a\r\n```\r\n\r\ndef test(input_shape=(None,None,3), is_training=True):                                                                                                      \r\n     inputs = tf.keras.layers.Input(shape=input_shape) #one input                                                                                             \r\n                                                                                                                                                          \r\n     #Various network layers                                                                                                                                  \r\n                                                                                                                                                         \r\n     #two outputs                                                                                                                                            \r\n     predict1 = ...                                                                                                                                          \r\n     predict2 = ...                                                                                                                                          \r\n                                                                                                                                                             \r\n     model = tf.keras.models.Model(inputs=inputs, outputs=[predict1, preidct2])                                                                              \r\n     model.summary()                                                                                                                                         \r\n                                                                                                                                                             \r\n     return model                                                                                                                                            \r\n                                                                                                                                                             \r\n datas = [...]                                                                                                                                               \r\n def process(l):                                                                                                                                             \r\n                                                                                                                                                       \r\n     #Process each piece of data to get training data, label1 and label2                                                                                      \r\n                                                                                                                                                          \r\n                                                                                                                                                             \r\n     return data, (label1, label2)                                                                                                                           \r\n                                                                                                                                                             \r\n class loss1(tf.keras.losses.Loss):\r\n     def __init__(self):                                                                                                                                     \r\n         super().__init__()                                                                                                                                  \r\n                                                                                                                                                             \r\n     def call(self, label1, predict1):                                                                                                                       \r\n                                                                                                                                                         \r\n         #Processing to get loss1                                                                                                                             \r\n         '                                                                                                                                                 \r\n                                                                                                                                                             \r\n         return loss1                                                                                                                                        \r\n                                                                                                                                                             \r\n class loss2(tf.keras.losses.Loss):                                                                                                                          \r\n                                                                                                                                                             \r\n     def __init__(self):                                                                                                                                     \r\n         super().__init__()                                                                                                                                  \r\n                                                                                                                                                             \r\n     def call(self, label2, predicts2):                                                                                                                      \r\n                                                                                                                                                     \r\n         #Processing to get loss2                                                                                                                             \r\n                                                                                                                                                          \r\n                                                                                                                                                             \r\n         return loss2\r\n\r\n                                                                                                                                                             \r\n train_ds = tf.data.Dataset.from_tensor_slices(datas).map(process, 8)                                                                                        \r\n                  \r\n model = test()                                                                                                                                           \r\n model.compile(                                                                                                                                              \r\n     optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True),                                                                    \r\n     loss=[loss1(), loss2()],                                                                                                                                \r\n     loss_weights=[0.5, 1.])                                                                                                                                 \r\n                                                                                                                                                             \r\n                                                                                                                                                             \r\n model.fit(train_ds,epochs=100) \r\n```\r\n\r\nI found that the two outputs of the network\uff08predict1 and predict2\uff09 were sent to the corresponding loss functions, but the corresponding label1 and label2 in tf.data were not sent to the two loss functions\r\n", "@liupengkd \r\nCould you please provide with indented stand alone code such that we could replicate the issue faced, i ran the code shared and [here is the gist](https://colab.sandbox.google.com/gist/Saduf2019/a5b0d3d40d416dd5dd03ce43a7a7182f/untitled166.ipynb) ", "@liupengkd\r\nPlease update as per above comment", "@Saduf2019 I have changed another training method\uff0cnot tf.keras.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39224\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39224\">No</a>\n"]}, {"number": 39223, "title": "Binary classification using Keras always give wrong predictions: The acc is always 0.5", "body": "Hi~ I am using Keras to make a simple binary classification. And I am using TF as backend.\r\n\r\nI checked:\r\n\r\n- data shuffle: I set the param in model.fit() shuffle = True\r\n- network structure: The NN take a vector with 1024 elements and makes a prediction 0 or 1.\r\n\r\nENV: \r\n\r\n- tensorflow 1.13.2 \r\n- Keras 2.2.4\r\n- Keras-applications 1.0.8\r\n- Keras-Processing 1.1.0\r\n- Tensorflow-estimator 1.13.0\r\n- Ubuntu 16.04 \r\n- python3\r\n\r\n\r\nBut the output is still wrong. The acc is always 0.5.\r\n\r\nSome people ran my code and get the acc 0.9857 after 1 epoch.\r\nSee here:\r\n![image](https://user-images.githubusercontent.com/22954901/81170557-79a2c700-8fcd-11ea-978e-72e2bc192f95.png)\r\n\r\n[https://stackoverflow.com/questions/61633602/binary-classification-using-keras-always-give-wrong-predictions-the-acc-is-alwa?noredirect=1#comment109021786_61633602](https://stackoverflow.com/questions/61633602/binary-classification-using-keras-always-give-wrong-predictions-the-acc-is-alwa?noredirect=1#comment109021786_61633602)\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Flatten, Dense, Lambda, Conv2D, Reshape, MaxPool2D, Average, Dropout, Concatenate, \\\r\n    Add, Maximum, Layer, Activation, Conv1D, TimeDistributed, GlobalAvgPool2D\r\nimport numpy as np\r\n\r\n\r\nclass Test(tf.keras.Model):\r\n    def __init__(self,attention_sz,dropout_rt, name=None):\r\n        super(Test, self).__init__(name=name)\r\n        # here we define the layer:\r\n        self.fc = Dense(attention_sz,input_dim = attention_sz ,activation='relu')\r\n        self.fc2 = Dense(attention_sz, activation='relu')\r\n        self.fc3 = Dense(1, activation='sigmoid')\r\n\r\n        self.dp = Dropout(dropout_rt,input_shape=(attention_sz,))\r\n        self.dp2 = Dropout(dropout_rt,input_shape=(attention_sz,))\r\n\r\n\r\n    def call(self, inp):\r\n        # here we get the segmentation and pose\r\n        with tf.device('/gpu:0'):\r\n            print(\"~~~~~~~~~~~\")\r\n            x = self.fc(inp)\r\n            print(x.shape)\r\n            z = self.dp(x)\r\n            print(z.shape)\r\n            x = self.fc2(z)\r\n            print(x.shape)\r\n            z = self.dp2(x)\r\n            print(z.shape)\r\n            y = self.fc3(z)\r\n            print(y.shape)\r\n        return y \r\n\r\nif __name__ == '__main__':\r\n    model  = Test(1024, 0.05)\r\n    model.compile(optimizer='rmsprop',\r\n                  loss='binary_crossentropy',\r\n                  metrics=['accuracy'])\r\n    x = np.round(np.random.normal(1.75, 0.2, size=(10000, 1024)), 2)\r\n    x2 = np.round(np.random.normal(100.75, 0.2, size=(10000, 1024)), 2)\r\n    labels = np.zeros((10000, 1))\r\n    labels2 = np.ones((10000, 1))\r\n\r\n    x_t = np.row_stack((x, x2))\r\n    labels = np.row_stack((labels,labels2))\r\n    print(x_t.shape)\r\n    print(labels.shape)\r\n    model.fit(x_t, labels, shuffle=True, epochs=10, batch_size=32)\r\n    x = np.round(np.random.normal(1.75, 0.2, size=(1, 1024)), 2)\r\n    y = np.round(np.random.normal(100.75, 0.2, size=(1, 1024)), 2)\r\n    res = model.predict(x)\r\n    print(res)\r\n    print(res.shape)\r\n    res = model.predict(y)\r\n    print(res)\r\n    print(res.shape)\r\n\r\n```\r\noutput:\r\n```\r\nWARNING:tensorflow:From /home/frank/Desktop/mesh-py3/my_venv/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\n2020-05-06 19:00:58.440615: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-05-06 19:00:58.616327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-06 19:00:58.617158: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55201b0 executing computations on platform CUDA. Devices:\r\n2020-05-06 19:00:58.617175: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2080, Compute Capability 7.5\r\n2020-05-06 19:00:58.636996: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz\r\n2020-05-06 19:00:58.637508: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x558add0 executing computations on platform Host. Devices:\r\n2020-05-06 19:00:58.637523: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2020-05-06 19:00:58.637876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.095\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.77GiB freeMemory: 7.06GiB\r\n2020-05-06 19:00:58.637892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2020-05-06 19:00:58.639694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-05-06 19:00:58.639708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2020-05-06 19:00:58.639713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2020-05-06 19:00:58.639923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6868 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nEpoch 1/10\r\n2020-05-06 19:00:59.495123: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\n20000/20000 [==============================] - 3s 148us/sample - loss: 8.0497 - acc: 0.4997\r\nEpoch 2/10\r\n20000/20000 [==============================] - 2s 98us/sample - loss: 8.0590 - acc: 0.5000\r\nEpoch 3/10\r\n20000/20000 [==============================] - 2s 99us/sample - loss: 8.0590 - acc: 0.5000\r\nEpoch 4/10\r\n20000/20000 [==============================] - 2s 80us/sample - loss: 8.0590 - acc: 0.5000\r\nEpoch 5/10\r\n20000/20000 [==============================] - 2s 81us/sample - loss: 8.0590 - acc: 0.5000\r\nEpoch 6/10\r\n20000/20000 [==============================] - 2s 80us/sample - loss: 8.0590 - acc: 0.5000\r\nEpoch 7/10\r\n20000/20000 [==============================] - 2s 89us/sample - loss: 8.0590 - acc: 0.5000\r\nEpoch 8/10\r\n20000/20000 [==============================] - 2s 83us/sample - loss: 8.0590 - acc: 0.5000\r\nEpoch 9/10\r\n20000/20000 [==============================] - 2s 78us/sample - loss: 8.0590 - acc: 0.5000\r\nEpoch 10/10\r\n20000/20000 [==============================] - 2s 79us/sample - loss: 8.0590 - acc: 0.5000\r\n[[0.]]\r\n(1, 1)\r\n[[0.]]\r\n(1, 1)\r\n\r\nProcess finished with exit code 0\r\n\r\n```\r\nThanks in advance!", "comments": ["Plus, I change the version of TF to ```tensorflow-gpu==1.15.0``` and I get:\r\n\r\n![image](https://user-images.githubusercontent.com/22954901/81171383-e074b000-8fce-11ea-8a70-f6531dfdc317.png)\r\n\r\n\r\nI want to figure it out. Why I met this problem? \r\nThanks! \r\n", "@OOF-dura \r\n\r\nCan you please try with latest TF versions 1.15 and 2.2-rc4 and you can see more than 99 percent accuracy.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/002a14545c23349d5dfb08314867c3db/untitled862.ipynb).There were lots of performance improvements in the latest versions. Thanks!", "> @OOF-dura\r\n> \r\n> Can you please try with latest TF versions 1.15 and 2.2-rc4 and you can see more than 99 percent accuracy.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/002a14545c23349d5dfb08314867c3db/untitled862.ipynb).There were lots of performance improvements in the latest versions. Thanks!\r\n\r\nHi~ Yep. I am using TF 1.15.0. But my project is build on tensorflow 1.13\r\nWhen I switch to 15 I get:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/frank/PycharmProjects/MultiGarmentNetwork/test_network.py\", line 14, in <module>\r\n    from network.base_network import PoseShapeOffsetModel\r\n  File \"/home/frank/PycharmProjects/MultiGarmentNetwork/network/base_network.py\", line 16, in <module>\r\n    from render.generic_renderer import render_colored_batch, render_shaded_batch, perspective_projection\r\n  File \"/home/frank/PycharmProjects/MultiGarmentNetwork/render/generic_renderer.py\", line 8, in <module>\r\n    import dirt\r\n  File \"/home/frank/Desktop/mesh-py3/my_venv/lib/python3.5/site-packages/dirt/__init__.py\", line 2, in <module>\r\n    from .rasterise_ops import rasterise, rasterise_batch, rasterise_deferred, rasterise_batch_deferred\r\n  File \"/home/frank/Desktop/mesh-py3/my_venv/lib/python3.5/site-packages/dirt/rasterise_ops.py\", line 6, in <module>\r\n    _rasterise_module = tf.load_op_library(_lib_path + '/librasterise.so')\r\n  File \"/home/frank/Desktop/mesh-py3/my_venv/lib/python3.5/site-packages/tensorflow/python/framework/load_library.py\", line 61, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: libtensorflow_framework.so: cannot open shared object file: No such file or directory\r\n\r\n```\r\nI learn it is a problem with the env. But the answer on github it quite unclear.", "I am worried about that if I switch to tf15. Is there something can never be fixed?\r\n\r\nThanks! ", "The question is why it's giving better results when switching it to the newest version what changes have been made in tf?\r\n\r\nThanks and Regards,\r\nPygirl\r\n\r\n", "@OOF-dura @ninjakx Root-cause of the issue is related to numerical instabilities of `sigmoid` activation in the final layer of model when used with `tensorflow-cpu` version. I changed two lines in your code as follows and got the similar as you get with `TF1.15`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/8921e4737e1061bc3ad24b9d1285535a/39223_cpu.ipynb).\r\n\r\n```\r\nself.fc3 = Dense(1) #, activation='sigmoid'\r\n\r\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\nmodel.compile(optimizer='rmsprop',\r\n                  loss=loss, #'binary_crossentropy'\r\n                  metrics=['accuracy'])\r\n```\r\n\r\nWhen I used your code as it is with `tensorflow-gpu` version of `TF1.13.2`, then I noticed similar results as you have seen with `TF1.15`. Please note that the `cpu` and `gpu` versions uses different libraries for optimum computational time. [Here](https://colab.research.google.com/gist/jvishnuvardhan/dfbb1b550478e5b171e407aec7b32af5/39223_with_gpu.ipynb) is a gist with `TF1.13.2-gpu` version. Hope it is clear. \r\n\r\nPlease verify once and close the issue. Thanks!", "> @OOF-dura @ninjakx Root-cause of the issue is related to numerical instabilities of `sigmoid` activation in the final layer of model when used with `tensorflow-cpu` version. I changed two lines in your code as follows and got the similar as you get with `TF1.15`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/8921e4737e1061bc3ad24b9d1285535a/39223_cpu.ipynb).\r\n> \r\n> ```\r\n> self.fc3 = Dense(1) #, activation='sigmoid'\r\n> \r\n> loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n> model.compile(optimizer='rmsprop',\r\n>                   loss=loss, #'binary_crossentropy'\r\n>                   metrics=['accuracy'])\r\n> ```\r\n> \r\n> When I used your code as it is with `tensorflow-gpu` version of `TF1.13.2`, then I noticed similar results as you have seen with `TF1.15`. Please note that the `cpu` and `gpu` versions uses different libraries for optimum computational time. [Here](https://colab.research.google.com/gist/jvishnuvardhan/dfbb1b550478e5b171e407aec7b32af5/39223_with_gpu.ipynb) is a gist with `TF1.13.2-gpu` version. Hope it is clear.\r\n> \r\n> Please verify once and close the issue. Thanks!\r\n\r\nThanks!\r\nClosed!"]}, {"number": 39222, "title": "Typos in source code docs", "body": "[Here](https://github.com/tensorflow/tensorflow/blob/fedc6d951faa73936a1154d6507d54240614d416/tensorflow/python/eager/backprop.py#L532) a minor typo in the source code: **rturns** ==> **returns**\r\n", "comments": ["Thanks for the issue. I have sent a fix for this. ", "Closing as fixed by fe97200"]}, {"number": 39221, "title": "Error in load/save models with FeatureLayer when refitting or reloading", "body": "Hello,\r\n\r\nMy model has FeatureLayer and i want do flow like this:\r\n\r\n1. Create, fit model  on basic data and save.\r\n2. Load model and fit on new data and save updated model.\r\n3. Do 2 step periodically e.g. 1 time in week.\r\n\r\nWhen I do model  first time, it's no problem, also I can use it in tensorflow-serving and it works.\r\nThen I can load this model, fit on new data and save updated model. But when I try to load updated model i have an error:\r\n\r\n` raise ValueError('Passing a dictionary input to a Sequential Model '\r\nValueError: Passing a dictionary input to a Sequential Model which doesn't have FeatureLayer as the first layer is an error.`\r\n\r\nIn tf v 2.0.0 problem is when load model:\r\n```\r\nmodel = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\nFile \"/usr/lib64/python3.6/site-packages/tensorflow_core/python/saved_model/load.py\", line 541, in load_internal\r\nexport_dir)\r\nFile \"/usr/lib64/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 103, in __init__\r\nself._finalize()\r\nFile \"/usr/lib64/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 132, in _finalize\r\nnode._set_inputs(inputs)\r\nFile \"/usr/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2697, in _set_inputs\r\ninputs = self._set_input_attrs(inputs)\r\nFile \"/usr/lib64/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\nresult = method(self, *args, **kwargs)\r\nFile \"/usr/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2731, in _set_input_attrs\r\nraise ValueError('Passing a dictionary input to a Sequential Model '\r\nValueError: Passing a dictionary input to a Sequential Model which doesn't have FeatureLayer as the first layer is an error.\r\n```\r\nI tested it and it's not problem with fitting model but with save or load, becouse when I only load first model and save as new model (without any interference in model) the error occurs when i try load updated model.\r\n\r\nIn version 2.1.0 its not error when load updated model, but when I try to fit. Final error is the same in both cases\r\n```\r\n File \"C:\\Users\\kglapiak\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\kglapiak\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 235, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\kglapiak\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 593, in _process_training_inputs\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\kglapiak\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 706, in _process_inputs\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\kglapiak\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\", line 702, in __init__\r\n    x = standardize_function(x)\r\n  File \"C:\\Users\\kglapiak\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 660, in standardize_function\r\n    standardize(dataset, extract_tensors_from_dataset=False)\r\n  File \"C:\\Users\\kglapiak\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2346, in _standardize_user_data\r\n    all_inputs, y_input, dict_inputs = self._build_model_with_inputs(x, y)\r\n  File \"C:\\Users\\kglapiak\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2572, in _build_model_with_inputs\r\n    self._set_inputs(cast_inputs)\r\n  File \"C:\\Users\\kglapiak\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2647, in _set_inputs\r\n    inputs = self._set_input_attrs(inputs)\r\n  File \"C:\\Users\\kglapiak\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test2\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\Users\\kglapiak\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2681, in _set_input_attrs\r\n    raise ValueError('Passing a dictionary input to a Sequential Model '\r\nValueError: Passing a dictionary input to a Sequential Model which doesn't have FeatureLayer as the first layer is an error.\r\n```\r\n\r\n\r\nI save model by mehod:\r\n`model.save('original_model',save_format='tf')`\r\n\r\nExample of model:\r\n```\r\nfeature_columns = []\r\n\r\ncategorical = tf.feature_column.categorical_column_with_vocabulary_list(\r\n        key='f1', vocabulary_list=['x1','x2','x3'],\r\n        num_oov_buckets=0)\r\none_hot = feature_column.indicator_column(categorical)\r\nfeature_columns.append(one_hot)\r\n\r\ncategorical = tf.feature_column.categorical_column_with_vocabulary_list(\r\n        key='f2', vocabulary_list=['x1','x2','x3','x4','x5','x6'],\r\n        num_oov_buckets=5)\r\nembedding = feature_column.embedding_column(categorical, dimension=10)\r\nfeature_columns.append(embedding)\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n\r\nmodel = tf.keras.Sequential([\r\n  feature_layer,\r\n  layers.Dense(20, activation='relu'),\r\n  layers.Dense(1,activation='softsign')\r\n])\r\n    \r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\n\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n    dataframe = dataframe.copy()\r\n    labels = dataframe.pop('target')\r\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n    if shuffle:\r\n        ds = ds.shuffle(buffer_size=len(dataframe))\r\n    ds = ds.batch(batch_size)\r\n    return ds\r\n\r\ntrain = df_to_dataset(train)\r\nval = df_to_dataset(val)\r\n\r\nmodel.fit(train,\r\n          validation_data=val,\r\n          epochs=2)\r\n```\r\n\r\nI tested it on conda and pip distributions, on tf versions 2.0.0 and 2.1.0 and on Windows and Centos and several versions of Python and all time is the same problem.\r\n\r\n\r\n- OS Platform and Distribution (Windows, Centos):\r\n- TensorFlow version (2.0.0,2.1.0 also pip and conda distridutions ):\r\n- Python version: several versions from  3.6.5 to 3.7.1\r\n", "comments": ["@kglapiak \r\nCode shared is incomplete we will not be able to replicate the issue, please share simple stand alone code or if possible a colab gist for us to analyse the issue faced", "All code:\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.models import load_model,Model\r\nfrom tensorflow import feature_column\r\nfrom tensorflow.keras import layers\r\n\r\n\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  labels = dataframe.pop('target')\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  return ds\r\n\r\n###--------- sample dataset ---------\r\nn = 10000\r\ndata = pd.DataFrame({'f1':np.random.choice(['x1','x2','x3'], n, replace=True),\r\n                     'f2':np.random.choice(['x1','x2','x3','x4','x5','x6'], n, replace=True),\r\n                     'target':np.random.choice([0,1], n, replace=True)})\r\n\r\ntrain,val = train_test_split(data,test_size=0.2)\r\ntrain,new_train = train_test_split(train,test_size=0.2)\r\n\r\n\r\n###--------- define features columns ---------\r\nfeature_columns = []\r\n\r\ncategorical = tf.feature_column.categorical_column_with_vocabulary_list(\r\n        key='f1', vocabulary_list=['x1','x2','x3'],\r\n        num_oov_buckets=0)\r\none_hot = feature_column.indicator_column(categorical)\r\nfeature_columns.append(one_hot)\r\n\r\ncategorical = tf.feature_column.categorical_column_with_vocabulary_list(\r\n        key='f2', vocabulary_list=['x1','x2','x3','x4','x5','x6'],\r\n        num_oov_buckets=5)\r\nembedding = feature_column.embedding_column(categorical, dimension=10)\r\nfeature_columns.append(embedding)\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n\r\n###--------- define and compile model ---------\r\nmodel = tf.keras.Sequential([\r\n  feature_layer,\r\n  layers.Dense(20, activation='relu'),\r\n  layers.Dense(1,activation='softsign')\r\n])\r\n    \r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\n\r\ntrain = df_to_dataset(train)\r\nval = df_to_dataset(val)\r\nnew_train = df_to_dataset(new_train)\r\n\r\n#----------- train model first time - all its ok --------\r\nmodel.fit(train,\r\n          validation_data=val,\r\n          epochs=2)\r\n\r\nmodel.save('basic_model',save_format='tf')\r\n\r\n###------------- first update, no errors -------------------------\r\nmodel_update1 = tf.keras.models.load_model('basic_model')\r\nmodel_update1.fit(new_train,\r\n          validation_data=val,\r\n          epochs=2)\r\n\r\nmodel_update1.save('update1',save_format='tf')\r\n\r\n###------------- 2nd update is problem and generate errors -------------------------\r\nmodel_update2 = tf.keras.models.load_model('update1')\r\nmodel_update2.fit(new_train,\r\n          validation_data=val,\r\n          epochs=2)\r\n\r\n```\r\n\r\n\r\n\r\n\r\n", "@kglapiak \r\nI ran the code shared above on tf 2.1 but do not face any error as reported above, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/2b40a2625e180590b2ff6ec028dac7ac/2.ipynb)", "For me this colab returns error:\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-4-b1ad4c6f08c2> in <module>()\r\n     82 model_update2.fit(new_train,\r\n     83           validation_data=val,\r\n---> 84           epochs=2)\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _set_input_attrs(self, inputs)\r\n   2679         # We assert that the first layer is a FeatureLayer.\r\n   2680         if not training_utils.is_feature_layer(self.layers[0]):\r\n-> 2681           raise ValueError('Passing a dictionary input to a Sequential Model '\r\n   2682                            'which doesn\\'t have FeatureLayer as the first layer'\r\n   2683                            ' is an error.')\r\n\r\nValueError: Passing a dictionary input to a Sequential Model which doesn't have FeatureLayer as the first layer is an error.\r\n```", "Correct @kglapiak.\r\nI've just tried taking a look at this issue, and yes -- it's breaking up exactly where you've pointed out.\r\n@Saduf2019  - have you tested out the linked gist?", "@kglapiak \r\nI ran the code shared by you and do not face any error, in case you are still facing an error can you please share a colab gist so we could analyse the error faced. faced", "Here colab [gist](https://colab.research.google.com/drive/17h5_Tceuh8Pt7mjRdaJ6xOPZY6XovaIr?usp=sharing) exacly same like yours. \r\nAnd the same error occurs.\r\nI show this for some people and they all had the same.\r\nIf you are sure, its works for you, only explanation is it's work in US and dot't worki in EU.\r\nIs it even possible?", "@kglapiak\r\nCould you please give me permission to view the gist.", "@Saduf2019 yea - now should be ok", "I am able to replicate the reported issue on tf 2.1,please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/3099abb57ef7134f37949a6b36d99f74/untitled177.ipynb). Thanks!", "Hi, @ymodak - something happends with that issue? ", "This is fixed with TensorFlow 2.2\r\nSee [gist](https://colab.research.google.com/gist/ymodak/450ecd9a1e30e717794492cf2ea775a5/untitled5.ipynb)\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39221\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39221\">No</a>\n", "Yea, its works. Thanks !"]}, {"number": 39220, "title": "fix ops.Tensor initializer in keras.initializers.get", "body": "This PR allows keras.initializers.get method deal with ops.Tensor initializer correctly, and fixes the following issue.\r\n\r\nIn partitioned variable cases, like distributed training, `constant_initializer` do not parse partitioned_info in call stage, where an ops.Tensor initializer will lead expected results (see line 746 in tensorflow/tensorflow/python/ops/variable_scope.py for more details). But the following code leads a ValueError.\r\n\r\n```python\r\n>>> import tensorflow as tf\r\n>>> x = tf.constant([[1], [2]])\r\n>>> layer = tf.keras.layers.Dense(3, kernel_initializer=tf.constant([[1]]))\r\n>>> y = layer(x)\r\nValueError: Could not interpret initializer identifier: tf.Tensor([[1]], shape=(1, 1), dtype=int32)\r\n```", "comments": ["I don't think we should allow tensor to be initializer, which is too board to be initializer, only some specific tensor can be initializer (eg constants).\r\n\r\nI think the PR is just walking around the issue. In tf2, I don't think partitioned variable is a supported use case any more, nor does variable_scope.", "Allowing tensor to be initializer will support many applications. For example, a widely used method to transfer small model into a big model is Net2Net, where we need the interface to specify the initial weight of the big model. This demand always occurs in recommender systems or computation advertisement to add some new features into the model. Another application is the deep model quantization, where we need to initialize a new model by setting some of weights to zero in the old one. I think support this will be more friendly to users for industrial usage and model quantization.\r\n\r\n", "> Allowing tensor to be initializer will support many applications. For example, a widely used method to transfer small model into a big model is Net2Net, where we need the interface to specify the initial weight of the big model. \r\n\r\nIf you need to set the weights for the model, you should use model.set_weights() with the numpy data. Also the value to set here is not a tensor (graph tensor will not work, and eager tensor can always be converted to numpy data).\r\n\r\n> This demand always occurs in recommender systems or computation advertisement to add some new features into the model. Another application is the deep model quantization, where we need to initialize a new model by setting some of weights to zero in the old one. I think support this will be more friendly to users for industrial usage and model quantization.\r\n\r\nSame here, I think this should be achieved by model.set_weights(). I still don't see any use case here that strictly need this change.\r\n\r\nAlso if the tensor is allowed here, I don't think we can properly serialize it.", "Thanks for your kind reply. I close this PR right now."]}, {"number": 39219, "title": "How to release CPU memory after session.close() java api", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (linux centOS 7):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (1.14.0):\r\n- javaversion:java8\r\n\r\nI use java to load tensorflow model\uff0cWhen I update the model\uff0cI close session and graph like that\uff1asession.close() & graph.close(), \r\nBut the memory is not released\uff0cHow could I release CPU memory timely to avoid OOM error please?\r\nThanks!", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 39218, "title": "TF 2 libraries for Arduino IDE", "body": "@tensorflow/micro\r\n\r\nThe arduino library managers only provide tf 1.14 and 1.15 build-in libraries and hope tensorflow team could provide the tf 2 libraries for Arduino IDE.", "comments": []}, {"number": 39217, "title": "DLL load failed", "body": "(comp1) E:\\Computer Vision\\Projects\\face-mask-detector>python train_mask_detector.py --dataset dataset\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_mask_detector.py\", line 5, in <module>\r\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@surabhijain123\r\nWhat is the version of the tensorflow.\r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See[ hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n.Also, please follow the instructions from to install from[ Tensorflow website.\r\n](https://www.tensorflow.org/install/source_windows)\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.\r\nPlease, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Duplicate. Closing", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39217\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39217\">No</a>\n"]}, {"number": 39216, "title": "GCS plugins filesystem", "body": "@mihaimaruseac \r\nThis is an initial implementation for `gcs` plugins. Unlike the `s3` implementation, I wrote this implementation with the minimum requests made to server ( one request / one function, no request to pre-check if path is invaild, etc ). I have also included a log to the files changed when I run the test `test.log` here so we can review and discuss easier. ( it will be dropped later )", "comments": ["@mihaimaruseac \r\nIn additional, I think we should have some dicussions about my schedule for GSoC. Writing plugins seems does not take much time as expect. But I haven't thought about what will do after finishing writing plugins yet.", "I'm thinking that each plugin would take around a week to fully land, so that would cover 2-3 weeks of GSoC (we're doing S3 and GCS). There will probably be a need for some API changes in the plugin interface (to solve issues discovered in these tests), so that could take 1-2 more weeks. Then, we'll have to move them to SIG IO and ensure that TF can still use them, on Linux, windows and mac.\r\n\r\nMy ideal plan is to have Windows and POSIX be the only plugins built by default in the pip package on the 2.3 release (~ midterm eval) and allow all other plugins to be loaded by the 2.4 release (~end of GSoC)", ">My ideal plan is to have Windows and POSIX be the only plugins built by default in the pip package on the 2.3 release (~ midterm eval) and allow all other plugins to be loaded by the 2.4 release (~end of GSoC)\r\n\r\nIt sounds good. Actually, I have some small ideas in mind related to filesystem ( I will try it when we finish the plugins ). They are implementing save model to cloud for `keras.model` ( not so hard ) and trying to fix some linking issues with the build on windows ( and static build in general )\r\n", "I would like to ask an off-topic questions. Regarding the build on windows, it seems likely that we will face the issue about filesystem on windows again. After this PR #39124, [tf.io](https://www.tensorflow.org/api_docs/python/tf/io) works with `s3://` but [tf.io.gfile.GFile](https://www.tensorflow.org/api_docs/python/tf/io/gfile/GFile) throws unimplemented. I think since there are multiple instances of `Env::Default()` instead of one shared instance on Windows, `s3://` is registered into the `Env::Default()` of `pywrap_tensorflow_internal` but not `pywrap_file_io` ( this is the internal implementation of `GFile` ).\r\n\r\nIf we loaded the plugin into tensorflow on Windows, I think that it will be registered into the `Env::Default()` of `pywrap_tensorflow_internal` but not `pywrap_file_io`and the issue will happen again. I am trying to build a `gcs_filesystem.dll` on windows and load it with [tf.load_library()](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/framework/load_library.py;l=123;drc=a02fe6c24a5a38c24d0330120c8426aeea28db9f). I will tell you what happen later.\r\n\r\nUpdate: `gcs_filesystem.dll` was built successfully on Windows. The test result was the same as on Linux. `tf.load_library()` loaded successfully but both `tf.io` and `tf.python.lib.io.file_io` threw unimplemented. Maybe we need a different way to load the plugins ?\r\n\r\nMy question is why we have 2 different parts for I/O, namely, [tf.io](https://github.com/tensorflow/tensorflow/blob/2ebc291f6a94163c49fc835d3afc93892e645e45/tensorflow/tools/api/golden/v2/tensorflow.io.pbtxt) and [tf.python.lib.io.file_io](https://github.com/tensorflow/tensorflow/tree/56f4edbb5a78ae9d29d797f39e107a28c9bcf6e5/tensorflow/python/lib/io/file_io.py). Is it better that we merge 2 parts into 1, let's say into `tf.python.lib.io` ? Merging into 1 part maybe a solution for the issue we have on Windows ( As there will be only one instance of `Env::Default()` left)\r\n\r\nAnother solution is build tensorflow on windows dynamically but there is a issue of protobuf on windows\r\nhttps://github.com/protocolbuffers/protobuf/blob/master/cmake/README.md#dlls-vs-static-linking", "> I would like to ask an off-topic questions. Regarding the build on windows, it seems likely that we will face the issue about filesystem on windows again. After this PR #39124, [tf.io](https://www.tensorflow.org/api_docs/python/tf/io) works with `s3://` but [tf.io.gfile.GFile](https://www.tensorflow.org/api_docs/python/tf/io/gfile/GFile) throws unimplemented. I think since there are multiple instances of `Env::Default()` instead of one shared instance on Windows, `s3://` is registered into the `Env::Default()` of `pywrap_tensorflow_internal` but not `pywrap_file_io` ( this is the internal implementation of `GFile` ).\r\n\r\nThis looks like a bug for us that we will have to fix. I'll try looking into it. There should be only one filesystem registry, especially after the filesystem support is all converted to modular.\r\n\r\n> If we loaded the plugin into tensorflow on Windows, I think that it will be registered into the `Env::Default()` of `pywrap_tensorflow_internal` but not `pywrap_file_io`and the issue will happen again. I am trying to build a `gcs_filesystem.dll` on windows and load it with [tf.load_library()](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/framework/load_library.py;l=123;drc=a02fe6c24a5a38c24d0330120c8426aeea28db9f). I will tell you what happen later.\r\n\r\nThe ideal plan is to have this working in the modular filesystem world. If it doesn't, we need to fix. Let me know what happens.\r\n\r\n> Update: `gcs_filesystem.dll` was built successfully on Windows. The test result was the same as on Linux. `tf.load_library()` loaded successfully but both `tf.io` and `tf.python.lib.io.file_io` threw unimplemented. Maybe we need a different way to load the plugins ?\r\n\r\nOh, so there is a bug that we'll have to check. I'll be looking on this next week, after I unblock you with the `TranslateName` change.\r\n\r\n> My question is why we have 2 different parts for I/O, namely, [tf.io](https://github.com/tensorflow/tensorflow/blob/2ebc291f6a94163c49fc835d3afc93892e645e45/tensorflow/tools/api/golden/v2/tensorflow.io.pbtxt) and [tf.python.lib.io.file_io](https://github.com/tensorflow/tensorflow/tree/56f4edbb5a78ae9d29d797f39e107a28c9bcf6e5/tensorflow/python/lib/io/file_io.py). Is it better that we merge 2 parts into 1, let's say into `tf.python.lib.io` ? Merging into 1 part maybe a solution for the issue we have on Windows ( As there will be only one instance of `Env::Default()` left)\r\n\r\nI think this is due to how the interface evolved, but will have to check. If we can reduce API that's also a great gain. I'll be back on this.\r\n\r\n> Another solution is build tensorflow on windows dynamically but there is a issue of protobuf on windows\r\n> https://github.com/protocolbuffers/protobuf/blob/master/cmake/README.md#dlls-vs-static-linking\r\n\r\nThe goal is to have all filesystem plugins be built dynamically. Due to the issues reported there on Windows, we have to have the memory allocation and deallocation routines included in the plugin.", "> > My ideal plan is to have Windows and POSIX be the only plugins built by default in the pip package on the 2.3 release (~ midterm eval) and allow all other plugins to be loaded by the 2.4 release (~end of GSoC)\r\n> \r\n> It sounds good. Actually, I have some small ideas in mind related to filesystem ( I will try it when we finish the plugins ). They are implementing save model to cloud for `keras.model` ( not so hard ) and trying to fix some linking issues with the build on windows ( and static build in general )\r\n\r\nThis is great and sure, we can do this as part of GSoC. Thank you.", "> > Update: `gcs_filesystem.dll` was built successfully on Windows. The test result was the same as on Linux. `tf.load_library()` loaded successfully but both `tf.io` and `tf.python.lib.io.file_io` threw unimplemented. Maybe we need a different way to load the plugins ?\r\n> \r\n> Oh, so there is a bug that we'll have to check. I'll be looking on this next week, after I unblock you with the `TranslateName` change.\r\n\r\nIs `tf.load_library()` the right way to load the plugin into filesystem or there is a specific way to load a plugin ?", "`tf.load_library()` should be the only way to load the plugins into TF.", "> `tf.load_library()` should be the only way to load the plugins into TF.\r\n\r\nthere is a problem with `tf.load_library()`, it threw unimplemented in `macos, windows, linux` even after I load the shared object `libgcs_filesystem.so/dylib/dll`. I have tried to build a shared object for `modular_filesystem` as well and load it into tensorflow. but after loading successfully 2 shared object, the problem still happened", "This is interesting. I'll try to replicate. Most likely we have an ODR violation which I'll have to fix.", "@mihaimaruseac \nIt seems that this RFC is related to our project. I have just skimmed through it but I think if this RFC is accepted, we will have to rewrite our cloud filesystem plugins ?\nhttps://github.com/tensorflow/community/blob/8ae88c1f8ac844a75dc901c017d696e1f7fd7c16/rfcs/20200505-transactional-fs.md", "Hi. Yes, they are related though I am thinking that the implementation won't change too much and that there can be collaboration between these two approaches.\r\n\r\nThat API is still in flux and might change. But if you want to give a review there that should be good.", "@mihaimaruseac \r\nI have rewrited the `s3` plugins as well. So I will wait for you to review codes for both plugins `gcs` and `s3`. In the mean time, I will learn more about `hdfs` and Transactional File Systems Support", "Awesome. I shall look over these two PRs promptly.", "because we make use of google-cloud-cpp \u00edntead of write our own client. So we have to rewrite everything. And the new implementation is based on current implementation of `s3`. So I assume it won't be a problem", "About c++17, if we want to use `std::string_view`, we have to use it. However, this is plugins-wide only and does not affect anything in Tensorflow. ", "* `std::string_view` and C++17, I think that should be ok. Otherwise we could try using `absl::string_view` which works in C++11/14. Since it's easy to change, let's not worry about it for now.\r\n\r\n* I will have to get some info on how gcs filesystem is used internally and determine if all internal users would be ok with a complete rewrite instead of only an API change with minimal non-API changes. Should get an answer by mid f next week, sorry for the new delay.", "Ok. Don't worry about the delay", "And could you help me with something ? While getting information about the API change, would you like to help me get some information about how Tensorflow use `CreateDir` internally, especially TensorBoard. As `CreateDir` is used for creating directory to store file (because local filesystem does not allow store file when parent directory does not exist). I am thinking of a `CreateDir` which does nothing, just sets the status to Ok. \n\nIn general, instead of providing users the same feeling as local filesystem, I think we should keep the characteristics of the cloud filesystem since it is highly optimized for big data. \n\nI think `DeleteDir` should just set the status to Ok as well", "And there is a gcs smoke test. maybe it could help. if you need the compiled plugins, please tell me. I have gcs plugins on linux, macos and windows too\nhttps://github.com/tensorflow/tensorflow/tree/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/tools/gcs_test", "One issue I noticed is that the implementation tries to download the files locally. There a couple of issues with that approach:\r\n* Write throughput on local disk is limited. \r\n* We will need to wait for the file to be fully downloaded before we can return the first batch. This introduces a lot of latency to the user. \r\n\r\nI'm sure there are other details in the implementation that a rewrite would miss. I suggest updating the previous implementation to use the pluggable interface first, and then updating it step by step to switch to the google cpp client instead to clean up the code.\r\n", "@sshrdp \r\nI have thought about the issue you said with the first batch download lately. I will check the current implementation as you suggest. Thank you", "@sshrdp @mihaimaruseac \r\nI find it is very hard to port the current implementation to Filesystem C API. Because it is heavily dependent on other part of Tensorflow (`core/platform/mutex` and `core/platform/env`).\r\nDo you have any idea about this problem ?\r\n\r\n`core/platform/env` seems to have an alternative `c/env` but since we will move the plugin into SIG IO, could we use it ?", "Afaik, we should be able to unentangle the current implementation so it only depends on stuff exposed on the stable C API. If there's something missing, we can expand the API with new PRs.", "@mihaimaruseac \r\nI have just skimmed throught the current implementation. As I understand, It used a LRU cache filesystem underlying. When the users want something on the server, we will fetch it to a cache file named \"filename@offset\". Nextime, we will check if the file that the users want exists in cache file or not, if the range is enough or we need to fetch more range, ... \r\n\r\nI think I can implement this without use the stuff in Tensorflow Core. However, problems come when we use multithread.\r\n\r\nI think I will make a complete rapport about what the current implementation do and how I deal with it tomorrow or the day after. I will upload it to the filechange so we could discuss easily", "That sounds great. Thank you", "@mihaimaruseac @sshrdp \r\nI have added a `design-proposal.md` to file changes. Please take a look at it and tell me if I am missing something. Thank you", "@mihaimaruseac \r\nBy the way, I have finished porting `file_block_cache`, `ram_file_block_cache` and `expiring_lru_cache` to Modular Filesystem. I rename `file_block_cache` to `block_cache` in case we want to add `file_block_cache` ( save cached file to local file rather than to memory ).\r\n\r\nIn fact, I put those files into a seperate folder `plugins/cache` since cloud filesystems may need it. Should I make a new PR for those files or just push those files to here so you can take a look at ?", "Let's split them into another PR as this is already large.", "@mihaimaruseac \r\nSorry I have misunderstood your review.\r\n> Alternatively, we can add one API entry to the filesystem ops set_filesystem_option(name, value) and use it in GCS to control throttling this way, since caller code might know better about what throttling levels are acceptable.\r\n\r\nThe problem with GCS Throttle isn't how we set the throttling level. It is we have no way to pass it to `google-cloud-cpp`. The same thing holds for timeout related parameters. I think the best solution is forking `google-cloud-cpp` and add more control", "@sshrdp how important is to have control over GCS throttle?"]}, {"number": 39215, "title": "TF: Model SubCassing | AttributeError: 'NoneType' object has no attribute 'compile'", "body": "**System information**\r\n- System: Kaggle Kernel\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.6.6\r\n- TPU model and memory: TPU V3-8\r\n\r\n\r\nI am trying to implement sub-classing for model definition but face a following error:\r\n\r\n```python\r\ndef my_model(Model):\r\n    def __init__(self, dim):\r\n        super(my_model, self).__init__(**kwargs)\r\n        self.efnet  = efn.EfficientNetB0(input_shape=(dim, 3), include_top = False, weights = 'imagenet')\r\n        self.gap    = L.GlobalAveragePooling2D()\r\n        self.bn     = L.BatchNormalization()\r\n        self.denseA = L.Dense(784, activation='relu', name = 'dense_A')\r\n        self.out    = L.Dense(1, activation='sigmoid')\r\n    \r\n    def call(self, inputs):\r\n        x     = self.efnet(inputs)\r\n        x_gap = self.gap(x)\r\n        bn    = self.bn(x_gap)\r\n        den_A = self.denseA(bn)\r\n        drop  = self.drop(den_A)\r\n        return self.out(drop)\r\n\r\ndim = (124,124)\r\nmodel = my_model((dim)\r\n```\r\n\r\nI understand I don't load the model properly, but I think it should load in this way. I am trying on this on Kaggle tpu with a proper strategy scheme (though I didn't mention that syntax in the above snippet). Any catch?", "comments": ["@Lincoln93 \r\n\r\nPlease note syntax is not correct in the attached code.`model = my_model((dim)`.Two brackets given before dim where it has to be one.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/b5346042bd4847e56f3250711859c0ac/untitled857.ipynb).Thanks!", "@ravikyram, please [check](https://colab.research.google.com/drive/1d1oUDznS0zzO4aZrsX0BYyJl5oMw8M27?usp=sharing) this.", "@Lincoln93 \r\n\r\ni am not seeing any issue in the colab link you have provided. Can you please share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram sorry, would you please check again.", "@Lincoln93 \r\n\r\nI am not able to access the colab link you have provided. Please, grant me access so it helps me to reproduce the issue.Thanks!\r\n\r\n", "sorry for the inconvenience. here [it is](https://colab.research.google.com/drive/1d1oUDznS0zzO4aZrsX0BYyJl5oMw8M27?usp=sharing ).\r\n", "I have tried in colab with TF version 2.1.0, 2.2-rc4 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/f8f3165fa91902706788d3a040f3bf98/untitled869.ipynb).Thanks!", "@ravikyram is it bug or there's something wrong in the code?", "Your `my_model` function returns `None` which is passed to `compile` method.\r\nThis looks like more of an error model building instance. Thanks!", "It should be class not function. \r\n\r\nInstead of `def my_model(Model):` , it should be `class my_model(Model):` . And also we need to build the model further. Here is the full code:\r\n\r\n```python\r\n\r\nclass my_model(Model):\r\n    def __init__(self, dim):\r\n        super(my_model, self).__init__(**kwargs)\r\n        self.efnet  = efn.EfficientNetB0(input_shape=(dim, 3), include_top = False, weights = 'imagenet')\r\n        self.gap    = L.GlobalAveragePooling2D()\r\n        self.bn     = L.BatchNormalization()\r\n        self.denseA = L.Dense(784, activation='relu', name = 'dense_A')\r\n        self.out    = L.Dense(1, activation='sigmoid')\r\n    \r\n    def call(self, inputs):\r\n        x     = self.efnet(inputs)\r\n        x_gap = self.gap(x)\r\n        bn    = self.bn(x_gap)\r\n        den_A = self.denseA(bn)\r\n        drop  = self.drop(den_A)\r\n        return self.out(drop)\r\n\r\ndim = (124,124)\r\nmodel = my_model((dim)\r\nmodel.build((None, *dim))\r\nmodel.compile(...)\r\nmodel.summary()\r\n````"]}, {"number": 39214, "title": "Node name obfuscation in TF 2.X", "body": "**System information**\r\n- TensorFlow version (you are using): **2.2.0-rc3**\r\n- Are you willing to contribute it (Yes/No): **Yes**\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIt seems like the [Graph Transform Tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms) is now deprecated (https://github.com/tensorflow/tensorflow/issues/33352) in TF 2.X and Grappler will take care of graph optimization. Unfortunately, [obfuscate_names](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#obfuscate_names) is missing and we could not find an alternative way for node name obfuscation in TF 2.X.\r\n\r\n**Will this change the current api? How?**\r\nNot sure. Basically there are 3 possible approaches:\r\n\r\n1. Grappler: AFAIK, saving a `saved_model` is in fact saving a snapshot of a Grappler-optimized concrete function. So it might be possible to add a new option to control the behavior?\r\n\r\n2. Graph Transform Tool: Reviving Graph Transform Tool and support TF 2.X. However, I don't think this is a good idea since most graph optimizations are now handled by Grappler, and `saved_model` is now the de facto standard format. There is a PR https://github.com/tensorflow/tensorflow/pull/28099 working on `saved_model` support for Graph Transform Tool, but I'm not sure if it is compatible with TF 2.X.\r\n\r\n3. Workarounds: Manually obfuscate names *after* training/debugging and *before* saving a model. This approach will add a lot of boilerplate code and global vars, and is way less flexible comparing to 1 & 2. Most importantly, changing `name`s of ops cannot strip the `/` namespaces added by Keras layers, which will expose the hierarchical structure of the model.\r\n\r\n**Who will benefit with this feature?**\r\nDevelopers who build proprietary on-premise softwares, or embed models in mobile apps.\r\n\r\nAOT compilation might not work for every situation, node name obfuscation could at least provide a certain degree of protection against reverse engineering.", "comments": []}, {"number": 39213, "title": "TFLu: Add broadcast support", "body": "A bug or limitation has been found for models with broadcast, like e.g.\r\nssd mobilenet v3. When using broadcast a tensor may appear without any\r\ndimension since it should not be needed for a constant, which result in\r\nundefined behavior. The fix therefore allocates a minimal dimension\r\nwith size 1 in those cases.\r\n\r\nBy allocating a dimension a larger refactoring is avoided.", "comments": ["@mansnils  Can you please resolve conflicts? Thanks!", "@gbaned conflicts are resolved.", "@mansnils Can you please resolve conflicts? Thanks!"]}, {"number": 39212, "title": "Full screen camera needed for pose estimation androie", "body": "Provide code for full screen camera (now only half screen camera ) shown on pose estimation android .Please provide the code for change to full screen ", "comments": ["@kittusster \r\nHave you referred to any of these links:\r\n[link](https://blog.tensorflow.org/2019/08/track-human-poses-in-real-time-on-android-tensorflow-lite.html)\r\n[link1](https://www.tensorflow.org/lite/models/pose_estimation/overview)\r\n[link2](https://www.reddit.com/r/computervision/comments/bohklz/using_pose_estimation_in_android_project/)\r\n[link3](https://github.com/ildoonet/tf-pose-estimation)\r\n[link4](https://github.com/google-coral/project-posenet)\r\n[link](https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5)", "Yes ,but not found solution ..where do we change the code for full screen\ncamera for pose estimation\n\nOn Wed, 6 May 2020, 13:24 Saduf2019, <notifications@github.com> wrote:\n\n> @kittusster <https://github.com/kittusster>\n> Have you referred to any of these links:\n> link\n> <https://blog.tensorflow.org/2019/08/track-human-poses-in-real-time-on-android-tensorflow-lite.html>\n> link1 <https://www.tensorflow.org/lite/models/pose_estimation/overview>\n> link2\n> <https://www.reddit.com/r/computervision/comments/bohklz/using_pose_estimation_in_android_project/>\n> link3 <https://github.com/ildoonet/tf-pose-estimation>\n> link4 <https://github.com/google-coral/project-posenet>\n> link\n> <https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/39212#issuecomment-624498050>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ADFMZCTMW5AVL27HRRKTKN3RQEJUZANCNFSM4M2EC3QQ>\n> .\n>\n", "@kittusster Can you please mention little more details on your question? \r\n1. Are you seeing only half screen? (I can see full screen in pixel)\r\n2. can you show me an image of half screen?\r\n3. did you change any settings?\r\n\r\nProvide more details so that it is easy for us to resolve faster. Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39211, "title": "Full screen camera needed for tensor flow pose estimation android", "body": "Provide full screen camera for pose estimation android", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the [Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Android ,tensor flow latest version .need full screen camera for pose\nestimation is issue\n\nOn Wed, 6 May 2020, 14:48 ravikyram, <notifications@github.com> wrote:\n\n> Please provide details about what platform you are using (operating\n> system, architecture). Also include your TensorFlow version. Also, did you\n> compile from source or install a binary?\n>\n> Make sure you also include the code snippet to reproduce the issue. If you\n> are unclear what to include see the issue template displayed in the Github\n> new issue template\n> <https://github.com/tensorflow/tensorflow/issues/new/choose>.\n>\n> We ask for this in the issue submission template, because it is really\n> difficult to help without that information. Thanks!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/39211#issuecomment-624535577>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ADFMZCU5WW4MWCRH2AL4W73RQETPXANCNFSM4M2EA66A>\n> .\n>\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 39210, "title": "[tf-dbg] source_utils_test fails in Python 3.8", "body": "TF version hash: 47ee0d08b22e79b84eb442ccb75ef8e4ccd5ec07\r\n\r\nSee the following test log:\r\n```\r\nTest output for //tensorflow/python/debug:source_utils_test:\r\n/bazel-buildfarm/default/operations/cf843b16-8c97-4552-8d55-fd16fe09d4f4/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/ops/random_ops.py:287: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\r\n  minval_is_zero = minval is 0  # pylint: disable=literal-comparison\r\n/bazel-buildfarm/default/operations/cf843b16-8c97-4552-8d55-fd16fe09d4f4/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/ops/random_ops.py:288: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\r\n  maxval_is_one = maxval is 1  # pylint: disable=literal-comparison\r\n/bazel-buildfarm/default/operations/cf843b16-8c97-4552-8d55-fd16fe09d4f4/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/ops/ragged/ragged_batch_gather_with_default_op.py:84: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\r\n  if (default_value.shape.ndims is not 0\r\n/bazel-buildfarm/default/operations/cf843b16-8c97-4552-8d55-fd16fe09d4f4/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/ops/ragged/ragged_batch_gather_with_default_op.py:85: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\r\n  and default_value.shape.ndims is not 1):\r\nRunning tests under Python 3.8.2: /usr/bin/python3.8\r\n[ RUN      ] GuessIsTensorFlowLibraryTest.testDebuggerExampleFilePathReturnsFalse\r\n[       OK ] GuessIsTensorFlowLibraryTest.testDebuggerExampleFilePathReturnsFalse\r\n[ RUN      ] GuessIsTensorFlowLibraryTest.testFileInPythonKernelsPathReturnsTrue\r\n[       OK ] GuessIsTensorFlowLibraryTest.testFileInPythonKernelsPathReturnsTrue\r\n[ RUN      ] GuessIsTensorFlowLibraryTest.testGuessedBaseDirIsProbablyCorrect\r\n[       OK ] GuessIsTensorFlowLibraryTest.testGuessedBaseDirIsProbablyCorrect\r\n[ RUN      ] GuessIsTensorFlowLibraryTest.testNonPythonFileRaisesException\r\n[       OK ] GuessIsTensorFlowLibraryTest.testNonPythonFileRaisesException\r\n[ RUN      ] GuessIsTensorFlowLibraryTest.testSourceUtilModuleReturnsTrue\r\n[       OK ] GuessIsTensorFlowLibraryTest.testSourceUtilModuleReturnsTrue\r\n[ RUN      ] GuessIsTensorFlowLibraryTest.testUnitTestFileReturnsFalse\r\n[       OK ] GuessIsTensorFlowLibraryTest.testUnitTestFileReturnsFalse\r\n[ RUN      ] GuessIsTensorFlowLibraryTest.test_session\r\nWARNING:tensorflow:From /usr/lib/python3.8/contextlib.py:83: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `self.session()` or `self.cached_session()` instead.\r\nW0506 00:11:11.384973 140211243161344 deprecation.py:317] From /usr/lib/python3.8/contextlib.py:83: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `self.session()` or `self.cached_session()` instead.\r\n[       OK ] GuessIsTensorFlowLibraryTest.test_session\r\n[ RUN      ] ListSourceAgainstDumpTest.testGenerateSourceList\r\n[  SKIPPED ] ListSourceAgainstDumpTest.testGenerateSourceList\r\n[ RUN      ] ListSourceAgainstDumpTest.testGenerateSourceListWithNodeNameFilter\r\n[  SKIPPED ] ListSourceAgainstDumpTest.testGenerateSourceListWithNodeNameFilter\r\n[ RUN      ] ListSourceAgainstDumpTest.testGenerateSourceListWithPathRegexFilter\r\n[  SKIPPED ] ListSourceAgainstDumpTest.testGenerateSourceListWithPathRegexFilter\r\n[ RUN      ] ListSourceAgainstDumpTest.test_session\r\n[  SKIPPED ] ListSourceAgainstDumpTest.test_session\r\n[ RUN      ] SourceHelperTest.testAnnotateDumpedTensorsGivesCorrectResult\r\n2020-05-06 00:11:11.473613: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-05-06 00:11:11.489470: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2500005000 Hz\r\n2020-05-06 00:11:11.497351: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28121b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-06 00:11:11.497393: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n/bazel-buildfarm/default/operations/cf843b16-8c97-4552-8d55-fd16fe09d4f4/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/autograph/utils/testing.py:21: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import imp\r\n[       OK ] SourceHelperTest.testAnnotateDumpedTensorsGivesCorrectResult\r\n[ RUN      ] SourceHelperTest.testAnnotateSubsetOfLinesGivesCorrectResult\r\n[       OK ] SourceHelperTest.testAnnotateSubsetOfLinesGivesCorrectResult\r\n[ RUN      ] SourceHelperTest.testAnnotateWholeValidSourceFileGivesCorrectResult\r\n[  FAILED  ] SourceHelperTest.testAnnotateWholeValidSourceFileGivesCorrectResult\r\n[ RUN      ] SourceHelperTest.testAnnotateWithStackTopGivesCorrectResult\r\n[  FAILED  ] SourceHelperTest.testAnnotateWithStackTopGivesCorrectResult\r\n[ RUN      ] SourceHelperTest.testCallingAnnotateSourceOnUnrelatedSourceFileDoesNotError\r\n[       OK ] SourceHelperTest.testCallingAnnotateSourceOnUnrelatedSourceFileDoesNotError\r\n[ RUN      ] SourceHelperTest.testCallingAnnotateSourceWithoutPythonGraphRaisesException\r\n[       OK ] SourceHelperTest.testCallingAnnotateSourceWithoutPythonGraphRaisesException\r\n[ RUN      ] SourceHelperTest.testLoadNonexistentNonParPathFailsWithIOError\r\n[       OK ] SourceHelperTest.testLoadNonexistentNonParPathFailsWithIOError\r\n[ RUN      ] SourceHelperTest.testLoadingPythonSourceFileInParFileFailsRaisingIOError\r\n[       OK ] SourceHelperTest.testLoadingPythonSourceFileInParFileFailsRaisingIOError\r\n[ RUN      ] SourceHelperTest.testLoadingPythonSourceFileInParFileSucceeds\r\n[       OK ] SourceHelperTest.testLoadingPythonSourceFileInParFileSucceeds\r\n[ RUN      ] SourceHelperTest.testLoadingPythonSourceFileWithNonAsciiChars\r\n[       OK ] SourceHelperTest.testLoadingPythonSourceFileWithNonAsciiChars\r\n[ RUN      ] SourceHelperTest.test_session\r\n[       OK ] SourceHelperTest.test_session\r\n======================================================================\r\nERROR: testAnnotateWholeValidSourceFileGivesCorrectResult (__main__.SourceHelperTest)\r\ntestAnnotateWholeValidSourceFileGivesCorrectResult (__main__.SourceHelperTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/bazel-buildfarm/default/operations/cf843b16-8c97-4552-8d55-fd16fe09d4f4/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/debug/lib/source_utils_test.py\", line 159, in testAnnotateWholeValidSourceFileGivesCorrectResult\r\n    source_annotation[self.u_init_line_number])\r\nKeyError: 116\r\n\r\n======================================================================\r\nERROR: testAnnotateWithStackTopGivesCorrectResult (__main__.SourceHelperTest)\r\ntestAnnotateWithStackTopGivesCorrectResult (__main__.SourceHelperTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/bazel-buildfarm/default/operations/cf843b16-8c97-4552-8d55-fd16fe09d4f4/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/debug/lib/source_utils_test.py\", line 181, in testAnnotateWithStackTopGivesCorrectResult\r\n    source_annotation[self.u_init_line_number])\r\nKeyError: 116\r\n\r\n----------------------------------------------------------------------\r\nRan 22 tests in 1.288s\r\n\r\nFAILED (errors=2, skipped=4)\r\n```\r\n\r\ncc @caisq ", "comments": ["Local test results:\r\n\r\n```\r\nExecuting tests from //tensorflow/python/debug:source_utils_test\r\n-----------------------------------------------------------------------------\r\n/home/byronyi/.cache/bazel/_bazel_byronyi/53d6946ea85fa3b31de84f256c98e548/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/ops/random_ops.py:287: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\r\n  minval_is_zero = minval is 0  # pylint: disable=literal-comparison\r\n/home/byronyi/.cache/bazel/_bazel_byronyi/53d6946ea85fa3b31de84f256c98e548/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/ops/random_ops.py:288: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\r\n  maxval_is_one = maxval is 1  # pylint: disable=literal-comparison\r\n/home/byronyi/.cache/bazel/_bazel_byronyi/53d6946ea85fa3b31de84f256c98e548/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/ops/ragged/ragged_batch_gather_with_default_op.py:84: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\r\n  if (default_value.shape.ndims is not 0\r\n/home/byronyi/.cache/bazel/_bazel_byronyi/53d6946ea85fa3b31de84f256c98e548/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/ops/ragged/ragged_batch_gather_with_default_op.py:85: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\r\n  and default_value.shape.ndims is not 1):\r\nRunning tests under Python 3.8.2: /usr/bin/python3.8\r\n[ RUN      ] GuessIsTensorFlowLibraryTest.testDebuggerExampleFilePathReturnsFalse\r\n[       OK ] GuessIsTensorFlowLibraryTest.testDebuggerExampleFilePathReturnsFalse\r\n[ RUN      ] GuessIsTensorFlowLibraryTest.testFileInPythonKernelsPathReturnsTrue\r\n[       OK ] GuessIsTensorFlowLibraryTest.testFileInPythonKernelsPathReturnsTrue\r\n[ RUN      ] GuessIsTensorFlowLibraryTest.testGuessedBaseDirIsProbablyCorrect\r\n[       OK ] GuessIsTensorFlowLibraryTest.testGuessedBaseDirIsProbablyCorrect\r\n[ RUN      ] GuessIsTensorFlowLibraryTest.testNonPythonFileRaisesException\r\n[       OK ] GuessIsTensorFlowLibraryTest.testNonPythonFileRaisesException\r\n[ RUN      ] GuessIsTensorFlowLibraryTest.testSourceUtilModuleReturnsTrue\r\n[       OK ] GuessIsTensorFlowLibraryTest.testSourceUtilModuleReturnsTrue\r\n[ RUN      ] GuessIsTensorFlowLibraryTest.testUnitTestFileReturnsFalse\r\n[       OK ] GuessIsTensorFlowLibraryTest.testUnitTestFileReturnsFalse\r\n[ RUN      ] GuessIsTensorFlowLibraryTest.test_session\r\nWARNING:tensorflow:From /usr/lib/python3.8/contextlib.py:83: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `self.session()` or `self.cached_session()` instead.\r\nW0506 05:14:47.790310 139630758512448 deprecation.py:317] From /usr/lib/python3.8/contextlib.py:83: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `self.session()` or `self.cached_session()` instead.\r\n[       OK ] GuessIsTensorFlowLibraryTest.test_session\r\n[ RUN      ] ListSourceAgainstDumpTest.testGenerateSourceList\r\n[  SKIPPED ] ListSourceAgainstDumpTest.testGenerateSourceList\r\n[ RUN      ] ListSourceAgainstDumpTest.testGenerateSourceListWithNodeNameFilter\r\n[  SKIPPED ] ListSourceAgainstDumpTest.testGenerateSourceListWithNodeNameFilter\r\n[ RUN      ] ListSourceAgainstDumpTest.testGenerateSourceListWithPathRegexFilter\r\n[  SKIPPED ] ListSourceAgainstDumpTest.testGenerateSourceListWithPathRegexFilter\r\n[ RUN      ] ListSourceAgainstDumpTest.test_session\r\n[  SKIPPED ] ListSourceAgainstDumpTest.test_session\r\n[ RUN      ] SourceHelperTest.testAnnotateDumpedTensorsGivesCorrectResult\r\n2020-05-06 05:14:47.850294: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n/home/byronyi/.cache/bazel/_bazel_byronyi/53d6946ea85fa3b31de84f256c98e548/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/autograph/utils/testing.py:21: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import imp\r\n[       OK ] SourceHelperTest.testAnnotateDumpedTensorsGivesCorrectResult\r\n[ RUN      ] SourceHelperTest.testAnnotateSubsetOfLinesGivesCorrectResult\r\n[       OK ] SourceHelperTest.testAnnotateSubsetOfLinesGivesCorrectResult\r\n[ RUN      ] SourceHelperTest.testAnnotateWholeValidSourceFileGivesCorrectResult\r\n[  FAILED  ] SourceHelperTest.testAnnotateWholeValidSourceFileGivesCorrectResult\r\n[ RUN      ] SourceHelperTest.testAnnotateWithStackTopGivesCorrectResult\r\n[  FAILED  ] SourceHelperTest.testAnnotateWithStackTopGivesCorrectResult\r\n[ RUN      ] SourceHelperTest.testCallingAnnotateSourceOnUnrelatedSourceFileDoesNotError\r\n[       OK ] SourceHelperTest.testCallingAnnotateSourceOnUnrelatedSourceFileDoesNotError\r\n[ RUN      ] SourceHelperTest.testCallingAnnotateSourceWithoutPythonGraphRaisesException\r\n[       OK ] SourceHelperTest.testCallingAnnotateSourceWithoutPythonGraphRaisesException\r\n[ RUN      ] SourceHelperTest.testLoadNonexistentNonParPathFailsWithIOError\r\n[       OK ] SourceHelperTest.testLoadNonexistentNonParPathFailsWithIOError\r\n[ RUN      ] SourceHelperTest.testLoadingPythonSourceFileInParFileFailsRaisingIOError\r\n[       OK ] SourceHelperTest.testLoadingPythonSourceFileInParFileFailsRaisingIOError\r\n[ RUN      ] SourceHelperTest.testLoadingPythonSourceFileInParFileSucceeds\r\n[       OK ] SourceHelperTest.testLoadingPythonSourceFileInParFileSucceeds\r\n[ RUN      ] SourceHelperTest.testLoadingPythonSourceFileWithNonAsciiChars\r\n[       OK ] SourceHelperTest.testLoadingPythonSourceFileWithNonAsciiChars\r\n[ RUN      ] SourceHelperTest.test_session\r\n[       OK ] SourceHelperTest.test_session\r\n======================================================================\r\nERROR: testAnnotateWholeValidSourceFileGivesCorrectResult (__main__.SourceHelperTest)\r\ntestAnnotateWholeValidSourceFileGivesCorrectResult (__main__.SourceHelperTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/byronyi/.cache/bazel/_bazel_byronyi/53d6946ea85fa3b31de84f256c98e548/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/debug/lib/source_utils_test.py\", line 159, in testAnnotateWholeValidSourceFileGivesCorrectResult\r\n    source_annotation[self.u_init_line_number])\r\nKeyError: 116\r\n\r\n======================================================================\r\nERROR: testAnnotateWithStackTopGivesCorrectResult (__main__.SourceHelperTest)\r\ntestAnnotateWithStackTopGivesCorrectResult (__main__.SourceHelperTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/byronyi/.cache/bazel/_bazel_byronyi/53d6946ea85fa3b31de84f256c98e548/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/debug/lib/source_utils_test.py\", line 181, in testAnnotateWithStackTopGivesCorrectResult\r\n    source_annotation[self.u_init_line_number])\r\nKeyError: 116\r\n\r\n----------------------------------------------------------------------\r\nRan 22 tests in 0.833s\r\n\r\nFAILED (errors=2, skipped=4)\r\n```", "Just found out that this is a known issue. I added test tag filters to workaround this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39210\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39210\">No</a>\n", "This should have been taken care of by the recently merged https://github.com/tensorflow/tensorflow/commit/fb416f16e2b01252326816bb311c3e6165d13bcf"]}, {"number": 39209, "title": "download_dependencies.sh failure: Usage: download_and_extract URL DIR", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nDescription:\tUbuntu 18.04.4 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version:\r\ncloned today\r\n- Python version:\r\n\r\nii  python         2.7.15~rc1-1 amd64        interactive high-level object-ori\r\n\r\n- Installed using virtualenv? pip? conda?:\r\nno\r\n\r\n- Bazel version (if compiling from source):\r\nnone\r\n- GCC/Compiler version (if compiling from source):\r\nii  python         2.7.15~rc1-1 amd64        interactive high-level object-ori\r\n\r\n- CUDA/cuDNN version:\r\nnone\r\n- GPU model and memory:\r\ninxi -G\r\nGraphics:  Card: Advanced Micro Devices [AMD/ATI] Cayman PRO [Radeon HD 6950]\r\n           Display Server: x11 (X.Org 1.20.5 ) driver: radeon\r\n           Resolution: 1920x1080@60.00hz\r\n           OpenGL: renderer: AMD CAYMAN (DRM 2.50.0 / 5.3.0-51-generic, LLVM 9.0.0)\r\n           version: 4.3 Mesa 19.2.8\r\n\r\n\r\n\r\n**Describe the problem**\r\ntrying to build deepbacksub which depends on tensorflow lite.\r\n\r\nbuild fails with:\r\n./tensorflow/tensorflow/lite/tools/make/download_dependencies.sh: line 59: 1: Usage: download_and_extract URL DIR\r\n\r\nwith \"set -x -v\" added to 2nd line of script, we see it is the eigen package, which is the first one.\r\ndownload_and_extract \"${EIGEN_URL}\" \"${DOWNLOADS_DIR}/eigen\"\r\n+ download_and_extract '' tensorflow/lite/tools/make/downloads/eigen\r\n+ local 'usage=Usage: download_and_extract URL DIR'\r\n./download_dependencies.sh: line 60: 1: Usage: download_and_extract URL DIR\r\n\r\n\r\ncd /usr/local/src/deepbacksub/tensorflow\r\ngrep -o 'http.*bitbucket.org/eigen/eigen/get/.*tar\\.gz' \"tensorflow/workspace.bzl\" | grep -v mirror.tensorflow | head -n1\r\n[nada]\r\n cat tensorflow/workspace.bzl | fgrep -i eigen\r\n        name = \"eigen_archive\",\r\n        build_file = clean_dep(\"//third_party:eigen.BUILD\"),\r\n        patch_file = clean_dep(\"//third_party/eigen3:gpu_packet_math.patch\"),\r\n        strip_prefix = \"eigen-4e696901f873a2347f76d931cf2f701e31e15d05\",\r\n            \"https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen/-/archive/4e696901f873a2347f76d931cf2f701e31e15d05/eigen-4e696901f873a2347f76d931cf2f701e31e15d05.tar.gz\",\r\n            \"https://gitlab.com/libeigen/eigen/-/archive/4e696901f873a2347f76d931cf2f701e31e15d05/eigen-4e696901f873a2347f76d931cf2f701e31e15d05.tar.gz\",\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n\r\nsudo apt-get install libopencv-dev opencv-doc\r\ncd /usr/local/srcsudo mkdir deepbacksubcd deepbacksubsudo git clone -b v2.1.0 https://github.com/tensorflow/tensorflow\r\n# NEXT LINE FAILS:\r\nsudo ./tensorflow/tensorflow/lite/tools/make/download_dependencies.sh\r\n./tensorflow/tensorflow/lite/tools/make/download_dependencies.sh: line 59: 1: Usage: download_and_extract URL DIR\r\nsudo \u00a0./tensorflow/tensorflow/lite/tools/make/build_lib.sh\r\nsudo git clone https://github.com/floe/deepbacksub.git\r\nsudo make all\r\nsudo make install\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nsudo git clone -b v2.1.0  https://github.com/tensorflow/tensorflow/\r\nCloning into 'tensorflow'...\r\nremote: Enumerating objects: 278, done.\r\nremote: Counting objects: 100% (278/278), done.\r\nremote: Compressing objects: 100% (244/244), done.\r\nremote: Total 891232 (delta 104), reused 133 (delta 32), pack-reused 890954\r\nReceiving objects: 100% (891232/891232), 523.35 MiB | 3.49 MiB/s, done.\r\nResolving deltas: 100% (723322/723322), done.\r\nNote: checking out 'e5bf8de410005de06a7ff5393fafdf832ef1d4ad'.\r\n\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\n\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n\r\n  git checkout -b <new-branch-name>\r\n\r\nChecking out files: 100% (19161/19161), done.\r\nsudo ./tensorflow/tensorflow/lite/tools/make/download_dependencies.sh\r\n./tensorflow/tensorflow/lite/tools/make/download_dependencies.sh: line 59: 1: Usage: download_and_extract URL DIR\r\n\r\n\r\n", "comments": ["@whitis \r\nplease refer to these links and let us know if it helps.\r\n\r\n[link1](https://github.com/tensorflow/tensorflow/issues/35747)\r\n[link2](https://github.com/tensorflow/tensorflow/issues/37846)\r\n[link3](https://github.com/tensorflow/tensorflow/issues/31799#issuecomment-525244201)\r\n[link4](https://gitmemory.com/issue/tensorflow/tensorflow/35747/573960517)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39209\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39209\">No</a>\n"]}, {"number": 39208, "title": "Inability to use tf.Session()", "body": "Hi, I am new to tensorflow and I am still learning on what to do for printing. I've seen that I should be using tf.Session() but whenever I do that(like Sess = tf.Session() or just tf.Session()) i get the error >>\r\n\r\nmodule 'tensorflow' has no attribute 'Session'\r\n\r\nI've learned that I should also use tf.compat.v1 but it doesn't provide the same for my lessons as i cannot use .run or any of the things that session uses. I also use tf.print() or print() which is standard but doesn't run the same as .Session(). Are there any alternatives to run the same thing in Tensorflow 2.1.0 or if there was a way, is there a page that can help me do so?", "comments": ["@BattleTaco \r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "You should install TF 1.15 if you want to use Session. In 2.0 world, the entire API is eager, there is no more `Session`.\r\n\r\nThis is a question better suited for StackOverflow as it is not about a bug in the implementation. Hence, closing."]}]