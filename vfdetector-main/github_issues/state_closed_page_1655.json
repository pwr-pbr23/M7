[{"number": 3256, "title": "[feature request]copy lstm parameters", "body": "I'm using tensorflow to implement DQN network and use LSTM at the bottom layer. \n\nBut in DQN, a target network is used(coping the main network and keep it for a while, and at the mean time update the original network), when use the `tf.Variable` as parameter A, I can use `tf.Variable.assign` to copy A to A', but when I use LSTM, it seems like there is no way to do this copy operation.\n\nOr maybe there is a way to copy the whole graph include its variable weights?\n", "comments": ["@ebrevdo Could you take a look at this please?\n", "You can access the underlying variables using tf.get_ variable() or, alternatively, by looking through tf.trainable_variables().  E.g.\n\nprint [v.name for v in tf.trainable_variables))]\n", "@ebrevdo It can serve my needs, though I need to see what variables have been created by LSTMCell or other interface and their names. Thank you very much!\ncc to @jmchen-g \n", "If i have those ema variables, how can i load them into another lstm cell?\n"]}, {"number": 3255, "title": "Added post link for building TF estimators", "body": "", "comments": ["Thanks!\n"]}, {"number": 3254, "title": "Cannot see summaries after running \"mnist_with_summaries\" exmaple", "body": "Hi, I'm learning something about tensorboard and summaries. After I run the code from /tensorflow/examples/tutorials/mnist/mnist_with_summaries.py, open the tensorboard, I can't see any histograms or scalar summaries. All are blank. It's wired.\n![](http://ww3.sinaimg.cn/large/901f9a6fjw1f5on9oq9tnj20a90m5wft.jpg)\n![](http://ww1.sinaimg.cn/large/901f9a6fjw1f5on9xq4npj20iy0gt0uw.jpg)\n\noperation system: ubuntu14.04\ngpu: gtx1080\ncuda 7.5\ncudnn 4.0.7\ninstall tensorflow from sources\n\nAnother thing I should mention is that I cannot run the mnist_with_summaries.py directly in Pycharm. Its log like that\n\n```\n/usr/bin/python2.7 /home/lan/Packages/tensorflow/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\nTraceback (most recent call last):\n  File \"/home/lan/Packages/tensorflow/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\", line 27, in <module>\n    import tensorflow as tf\n  File \"/home/lan/Packages/tensorflow/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/home/lan/Packages/tensorflow/tensorflow/python/__init__.py\", line 48, in <module>\n    from tensorflow.python import pywrap_tensorflow\nImportError: cannot import name pywrap_tensorflow\n\nProcess finished with exit code 1\n```\n\nSo I run the code in terminal by `python mnist_with_summaries.py`. It do run successfully, but there are warnings (or sth?): Unhandled API Callback\n![](http://ww3.sinaimg.cn/large/901f9a6fjw1f5onjmkjoaj20lx07y77z.jpg)\nI'm not sure if this warning influences summaries.\n", "comments": ["This was a bug in the tutorial code. It's fixed in a recent commit (but hasn't made it into a release branch yet) - see:\nhttps://github.com/tensorflow/tensorflow/commit/cf7f7de701874a091156cd76286c0ead6dc98be1\n"]}, {"number": 3253, "title": "Fix for protobuf library order building problems", "body": "This should fix #3191. The problem was that we'd added internally-built protoc, but the include and library orders in the makefile would pull in headers and libraries in /usr/local/ before the locally-generated ones.\n", "comments": []}, {"number": 3252, "title": "Update README.md", "body": "Fix links to iOS examples\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "Merged. Thank you.\n"]}, {"number": 3251, "title": "tensorflow mobile : pi_examples - unable to locate graph.pb.h", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\npi@raspberrypi:~ $ cat /proc/version\nLinux version 4.4.13-v7+ (dc4@dc4-XPS13-9333) (gcc version 4.9.3 (crosstool-NG crosstool-ng-1.22.0-88-g8460611) ) #894 SMP Mon Jun 13 13:13:27 BST 2016\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. Follow https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/pi_examples/\n2. When you get to the final step in \"Building the Examples\", the following error occurs.\n   pi@raspberrypi:~/tf/tensorflow $ make -f tensorflow/contrib/pi_examples/label_image/Makefile \n   gcc --std=c++11 -O0 -I/usr/local/include -I. -I/home/pi/tf/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/downloads -I/home/pi/tf/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/downloads/eigen-latest/ -I/home/pi/tf/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/gen/proto/ -I/home/pi/tf/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/gen/proto_text/ -c tensorflow/contrib/pi_examples/label_image/label_image.cc -o /home/pi/tf/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o\n   tensorflow/contrib/pi_examples/label_image/label_image.cc:30:48: fatal error: tensorflow/core/framework/graph.pb.h: No such file or directory\n   #include \"tensorflow/core/framework/graph.pb.h\"\n                                                 ^\n   compilation terminated.\n   tensorflow/contrib/pi_examples/label_image/Makefile:78: recipe for target '/home/pi/tf/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o' failed\n   make: **\\* [/home/pi/tf/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o] Error 1\n### What have you tried?\n1. Searching for similar issues.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["@petewarden could you take a look at this please?\n", "Sorry for the long delay on this one. I've made some changes to the Pi building process and this seems to be working generally now. Can you let me know if you're still hitting problems?\n", "hi p @petewarden  ,\nI have the same issue #4680 . when i fixed just like magicwifi said. I got the error\n\n```\npi@raspberrypi:~/tensorflow/tensorflow $ make -f tensorflow/contrib/pi_examples/label_image/Makefile\ngcc --std=c++11 -O0 -I/usr/local/include -I. -I/home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/downloads  -c tensorflow/contrib/pi_examples/label_image/label_image.cc -o /home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o\ntensorflow/contrib/pi_examples/label_image/label_image.cc:31:48: fatal error: tensorflow/core/framework/graph.pb.h: No such file or directory\n #include \"tensorflow/core/framework/graph.pb.h\"\n                                                ^\ncompilation terminated.\ntensorflow/contrib/pi_examples/label_image/Makefile:80: recipe for target '/home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o' failed\nmake: *** [/home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o] Error 1\n```\n\nHow i can fix this error for my pi zero\n", "Can you attach the generated makefile?\n", "Hi @drpngx \nhttps://github.com/shaolinkhoa/tensorflow.git\n\njust to make sure, Can I install tensorflow on pi zero ? or it's only could installed on pi 2 and pi 3\nthank you\n\np/s: i fixed the link\n", "1. Could you check the link?\n", "Do you just want to install it?\n\nThere are some [instructions](https://github.com/samjabrahams/tensorflow-on-raspberry-pi) from one of our users @samjabrahams \n", "It looks like there's a missing dependency, so I'd have to take a look at the graph. if you're building with `-j` just use one core first. Try `make -k` then `make` again\n", "Can you remove the line with the comment? It's technically wrong, and IIRC in some versions of make the strict representation is honored.\n", "I tried \n`make -kf tensorflow/contrib/pi_examples/label_image/Makefile`\nbut the error is still the same\n\n*_```\npi@raspberrypi:~/tensorflow/tensorflow $ make -kf tensorflow/contrib/pi_examples/label_image/Makefile\ngcc --std=c++11 -O0 -I/usr/local/include -I. -I/home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/downloads  -c tensorflow/contrib/pi_examples/label_image/label_image.cc -o /home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o\ntensorflow/contrib/pi_examples/label_image/label_image.cc:31:48: fatal error: tensorflow/core/framework/graph.pb.h: No such file or directory\n #include \"tensorflow/core/framework/graph.pb.h\"\n                                                ^\ncompilation terminated.\ntensorflow/contrib/pi_examples/label_image/Makefile:80: recipe for target '/home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o' failed\nmake: *_\\* [/home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o] Error 1\nmake: Target 'all' not remade because of errors.\n\n``` **\n\n```\n", "I don't see anything in that directory that creates the `graph.pb.h`, you have to call `protoc` yourself.\n", "I'm sorry. I really don't know what is `protoc` or how to create graph.pb.h ?\n\nWould you mind giving more clues\n", "Look for protoc in `tensorflow/contrib/makefile/Makefile`\n", "Sir, please give me a solution to pass this problem so that I could install tensorflow on Pi zero.\ni'm really not familiar with make.\n\nI'm not the only one stucking with it. johnnymakk in post #4680 is having the same problem too\n", "Please copy the lines regarding protoc. `protoc` is the protobuf compiler that converts `.proto` files into `*.pb.{cc,h}`. It should be called to generate the `graph.pb.h`. The Makefile in `tensorflow/contrib/makefile/Makefile` contains this. It's not clear why it's missing from your contrib makefile, perhaps the aforementioned makefile was run first.\n", "Hi @drpngx thanks a lot for your help.\nI think I'm following the same instructions as @shaolinkhoa .\n\nCreate a tensorflow static library using the makefile as in:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#raspberry-pi\nmake -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI \\\n OPTFLAGS=\"-Os -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize\" CXX=g++-4.8\n\nYes, you are right, I ran the tensorflow/contrib/makefile/Makefile first to build tensorflow.\nSeemed to build OK. No errors.\n\nThen tried to compile the label_image example from makefile and received the graph.pb.h error.\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/pi_examples\nmake -f tensorflow/contrib/pi_examples/label_image/Makefile\n\nThanks a lot for any suggestions!\n", "@petewarden I'm not sure why the graph.pb.h is not generated.\n", "I managed to get the label image example compiled.\nI cloned the most recent tensorflow repo and was able to make the project OK.\ngraph.pb.h is generated in tensorflow/contrib/makefile/gen/proto/tensorflow/core/framework\nI noticed the fix for eigen is done.\nThanks @drpngx and @petewarden\n", "Could you summarize the steps that you used, so that other people can understand, in a nutshell, what the resolution is?\n\nThanks!\n", "I pretty much followed the instructions in the following posts.\nFirst git clone the tensorflow repo. (did a fresh clone with the latest code)\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#raspberry-pi\nI used the second option which should optimize for Pi 2&3.\n\n```\nmake -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI \\\nOPTFLAGS=\"-Os -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize\" CXX=g++-4.8\n```\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/pi_examples\n`make -f tensorflow/contrib/pi_examples/label_image/Makefile`\n\nI'm not sure what caused the problem with graph.pb.h but downloading the latest repo and going through the procedure again resolved the problem.\n\nThanks!\n", "Thanks!\n", "It seems Pi zero is hopeless.\nI followed all the steps again and it works just like johnnymakk said.\n\nbut when I run the line\n`make -f tensorflow/contrib/pi_examples/label_image/Makefile`\n\nI got error \n\n```\npi@raspberrypi:~/tensorflow $ tensorflow/contrib/pi_examples/label_image/gen/bin/label_image\nIllegal instruction\n```\n\nTensorflow seems cannot install on ARM V6 \n\npls sir @drpngx ,Would you mind testing tensorflow with pi zero\n"]}, {"number": 3250, "title": "Remove support for CuDNN v2 runtime/compile from TensorFlow.", "body": "Major changes in the APIs between CuDNN versions make it\nhard to update to newer versions of the library as they\ncome out every few months (as can be seen by the lines\nremoved by this commit).  As CuDNN APIs become more stable\nwe can likely leave support for more versions, but V2 was\nparticularly problematic, and versions V3+ are a large\nenough improvement over V2 that we don't need to support V2\nanymore.\n\nWhen V6 comes out, we'll re-evaluate the list of supported\nversions and see whether to drop V3 at that time.\n", "comments": ["LGTM\n"]}, {"number": 3249, "title": "Change cudnn version checking to check Major version.", "body": "CuDNN versions with the same major version should be ABI\ncompatible, which means that if we release a binary\nwith cudnn version 5.1, it should be compatible with\na system whose runtime loads version 5.0.  This check\nwould trigger if the cudnn version loaded at runtime is\n5 but the compile-time version used was 4.\n\nThis change is based off of recommendations from\nhttps://github.com/tensorflow/tensorflow/issues/2525#issuecomment-223523008.\n\nFixes #3225.\n", "comments": ["Just FYI to @3XX0 who suggested this and @zer0n who ran into this.\n\n(my GPU machine is off so this is untested at the moment, using our CI to 'test' ;)\n", "LGTM\n"]}, {"number": 3248, "title": "NUM_CHANNELS instead of '1' for consistency", "body": "Even if the MNIST dataset is made by one-channels images, NUM_CHANNELS variable is used for creating the model. To be consistent and for generality, shall we use NUM_CHANNELS also for extracting data from images into the 4D tensor.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3247, "title": "Different padding boundaries for different directions. ", "body": "Hello, \n\nI recently started to use TensorFlow to implement and idea using 3D convolutions. It requires me to use different padding boundary conditions along spatial and temporal directions (dimensions) of the tensor (I need `padding='SAME'` for spatial and `padding='VALID'` for temporal directions).  `tf.nn.conv3d` and `tf.nn.max_pool3d` specify padding for the operation as a whole. \n\nIs there any way to specify padding type for each dimension individually, like how the stride is specified? I don't mind contributing for this if necessary, if someone could direct me on how to add this feature or create a new op for it. \n", "comments": ["Unfortunately at the moment there is no way to specify different padding algorithms for different dimensions.\n\nThe ideal way to handle this (and other future feature requests like this) would be to support explicit padding -- the user passes the amount of padding on the 'top' and 'bottom' of each dimension, so they have full control of the algorithm.\n\nTo do this, code that calls into eigen_spatial_convolution.h and cuboid_convolution.h (etc.) would need to call a version of the interfaces that passes the explicit dimensions.  Right now we pass 'padding algorithms.'   For example: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/eigen_spatial_convolutions.h#L932 would need to be changed to take explicit padding values instead of 'padding_type'.  We could then change the API that has padding_type to call into the one that provides the explicit padding dimensions, to maximize code reuse.\n\nThis would be a major/difficult change for someone not familiar with the codebase, and it's something we can track internally. \n", "Tracked this internally so I will close this one here.\n", "Hi Vijay and Jianmin, thanks for taking the time to look into this. \n\nI agree that controlling padding amounts of each dimension is probably the most general scenario, but  I also like the existing abstraction of padding types in place. \n\nUntil this feature is brought in, would it be possible for me to define an alternate operation to conv3d, \nwhere the amount of padding for planes, rows and columns is decided case-wise based on the padding type? \nIt seems that the conv_ops_3d.cc seems to be doing something similar (If I'm right, the VALID padding is some sort of default case, and SAME padding is built over that?), except that it clubs rows, columns and planes within those if statements checking padding types. \n", "Has this ever been looked in to?", "Here's the solution I've used: Set padding to be 'VALID' (= no padding), but pad manually in the spatial dimension using `tf.pad`:\r\n\r\n```py\r\n# pad spatially\r\npaddings = tf.constant([[0, 0], [0, 0], [1, 1], [1, 1], [0, 0]])\r\npadded = tf.pad(inputs, paddings)\r\n\r\n# convolution without automatic padding\r\ntf.nn.conv3d(inputs, filters, strides=[1, 1, 1, 1, 1], padding='VALID)\r\n```"]}, {"number": 3246, "title": "tf.matmul() fails when matrices are sparse, even if a_is_sparse is set.  Cryptic error.", "body": "tf.matmul(A, B, a_is_sparse=True) fails on a = ops.convert_to_tensor(a, name=\"a\") with cryptic error.\n\nExample code:\n\n```\nimport tensorflow as tf\nimport numpy as np\n\nx = tf.sparse_placeholder(tf.float32)\ny = tf.Variable(tf.random_uniform([9, 9], minval=0.0, maxval=1.0, dtype=tf.float32))\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    indices = np.array([[3, 2], [4, 5]], dtype=np.int64)\n    values = np.array([1.0, 2.0], dtype=np.float32)\n    shape = np.array([9, 9], dtype=np.int64)\n    _ = sess.run(tf.matmul(x, y), feed_dict={\n        x: tf.SparseTensorValue(indices, values, shape)})\n```\n\nError:\n line 12, in <module>\n    _ = sess.run(tf.matmul(x, y, a_is_sparse=True), feed_dict={\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1189, in matmul\n    a = ops.convert_to_tensor(a, name=\"a\")\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 620, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.py\", line 179, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.py\", line 162, in constant\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 421, in make_tensor_proto\n    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/util/compat.py\", line 44, in as_bytes\n    raise TypeError('Expected binary or unicode string, got %r' % bytes_or_text)\nTypeError: Expected binary or unicode string, got <tensorflow.python.framework.ops.SparseTensor object at 0x11d952d90>\n\nIs fixed by explicitly converting from sparse to dense tensor before multiplication:\n\n```\n\nimport tensorflow as tf\nimport numpy as np\n\nx = tf.sparse_placeholder(tf.float32)\nz = tf.sparse_tensor_to_dense(x)\ny = tf.Variable(tf.random_uniform([9, 9], minval=0.0, maxval=1.0, dtype=tf.float32))\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    indices = np.array([[3, 2], [4, 5]], dtype=np.int64)\n    values = np.array([1.0, 2.0], dtype=np.float32)\n    shape = np.array([9, 9], dtype=np.int64)\n    _ = sess.run(tf.matmul(z, y), feed_dict={\n        x: tf.SparseTensorValue(indices, values, shape)})\n```\n", "comments": ["@concretevitamin, Is this expected to work, or is this a known limitation of sparse tensors.\n", "Flags `a_is_sparse` and `b_is_sparse` do not indicate the operands being `SparseTensor`s.  Instead, they are algorithmic hints to invoke a more efficient algorithm on the two dense `Tensor` operands. \n\nTo matmul a SparseTensor and a dense Tensor, you could use `tf.sparse_tensor_dense_matmul()` instead.\n", "> Flags a_is_sparse and b_is_sparse do not indicate the operands being SparseTensors. Instead, they are algorithmic hints to invoke a more efficient algorithm on the two dense Tensor operands.\r\n\r\nThis should be part of the [documentation](https://www.tensorflow.org/api_docs/python/tf/matmul) of matmut. It is very unclear what the flag `a_is_sparse` does. "]}, {"number": 3245, "title": "Training learn.DNNClassifier iteratively does not yield similar result", "body": "### Environment info\n\nOperating System: OS X 10.11.5\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n\n```\nhttps://storage.googleapis.com/tensorflow/mac/tensorflow-0.9.0-py3-none-any.whl\n```\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\n0.9.0\n```\n### Steps to reproduce\n1. I used the code in [tf.contrib.learn.QuickStart](https://www.tensorflow.org/versions/r0.9/tutorials/tflearn/index.html#tf-contrib-learn-quickstart)\n2. change\n   \n   ```\n   # Fit model.\n   classifier.fit(x=x_train, y=y_train, steps=200)\n   ```\n   \n   to\n   \n   ```\n   # Fit model.\n   for i in range(40):\n     classifier.fit(x=x_train, y=y_train, steps=5)\n   ```\n3. The result accuracy is around 0.33, which is similar to call `fit` with `steps=5` only once.\n", "comments": ["Could you print out the result accuracy for each iteration and report that here?\n", "Here is the output:\n\n```\nAccuracy for iteration  0  is 0.300000\nAccuracy for iteration  1  is 0.300000\nAccuracy for iteration  2  is 0.300000\nAccuracy for iteration  3  is 0.300000\nAccuracy for iteration  4  is 0.300000\nAccuracy for iteration  5  is 0.300000\nAccuracy for iteration  6  is 0.300000\nAccuracy for iteration  7  is 0.300000\nAccuracy for iteration  8  is 0.300000\nAccuracy for iteration  9  is 0.300000\nAccuracy for iteration  10  is 0.300000\nAccuracy for iteration  11  is 0.300000\nAccuracy for iteration  12  is 0.300000\nAccuracy for iteration  13  is 0.300000\nAccuracy for iteration  14  is 0.300000\nAccuracy for iteration  15  is 0.300000\nAccuracy for iteration  16  is 0.300000\nAccuracy for iteration  17  is 0.300000\nAccuracy for iteration  18  is 0.300000\nAccuracy for iteration  19  is 0.300000\nAccuracy for iteration  20  is 0.300000\nAccuracy for iteration  21  is 0.300000\nAccuracy for iteration  22  is 0.300000\nAccuracy for iteration  23  is 0.300000\nAccuracy for iteration  24  is 0.300000\nAccuracy for iteration  25  is 0.300000\nAccuracy for iteration  26  is 0.300000\nAccuracy for iteration  27  is 0.300000\nAccuracy for iteration  28  is 0.300000\nAccuracy for iteration  29  is 0.300000\nAccuracy for iteration  30  is 0.300000\nAccuracy for iteration  31  is 0.300000\nAccuracy for iteration  32  is 0.300000\nAccuracy for iteration  33  is 0.300000\nAccuracy for iteration  34  is 0.300000\nAccuracy for iteration  35  is 0.300000\nAccuracy for iteration  36  is 0.300000\nAccuracy for iteration  37  is 0.300000\nAccuracy for iteration  38  is 0.300000\nAccuracy for iteration  39  is 0.300000\n```\n", "Yeah that looks pretty much like it is starting from scratch each iteration. Looking...\n", "Also see #3193 (not exactly same issue), but it indicates that this part of tf.contrib.learn is in flux, but will hopefully settle soon.\n", "tf.learn in 0.9 is woefully incomplete. The issue is likely fixed at head.\n", "Could you check if the issue is fixed at head now?\n", "I just installed from the source in master, and verified that the problem has been fixed on master.\n"]}, {"number": 3244, "title": "TensorForestEstimator - removed unused continue_training and changed to use config in constructor", "body": "- Remove unused `continue_training`\n- Remove unused `verbose`\n- Switch to use config - `Estimator` handles case when `config=None` already \n", "comments": []}, {"number": 3243, "title": "Push changes from internal: 126965578", "body": "", "comments": ["Something's not right about this push PR. Closing it. Do not merge.\n"]}, {"number": 3242, "title": "fix nightly download links in readme", "body": "I grabbed these links from [build history](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/).  Fix #2939 \n", "comments": ["Can one of the admins verify this patch?\n", "This is already an internal change set that fixes the links. The change set has not been synced to public yet. I will close this PR. But thank you for spotting and trying to fix them :)\n"]}, {"number": 3241, "title": "Branch 126943819", "body": "Merging internal Google changes.\n", "comments": ["@tensorflow-jenkins test this please\n"]}, {"number": 3240, "title": "Update Scikit flow link in resources", "body": "", "comments": []}, {"number": 3239, "title": "FFT gradient over variables fails", "body": "The following code fails with an error `ValueError: Shapes (3, 32, 28, 28) and () are not compatible`\n\n```\nimport tensorflow as tf\n\nfilters = 32\nwidth = 28\nheight = 28\nchannels = 3\n\nx = tf.placeholder(tf.float32, shape=[None, height, width, channels])\nw = tf.Variable(tf.truncated_normal([channels, filters, height, width], stddev=0.1))\nb = tf.Variable(tf.constant(0.1, shape=[32]))\n\nw_ifft = tf.real(tf.batch_ifft2d(tf.complex(w, 0.0)))\nw_ifft_transpose = tf.transpose(w_ifft, [2, 3, 0, 1])\n\nconv = tf.nn.conv2d(x, w_ifft_transpose, strides=[1, 1, 1, 1], padding='SAME')\noutput = tf.nn.bias_add(conv, b)\n\nprint tf.gradients(output, w)\n```\n\nPerforming backprop of an FFT over a variable is important for reproducing the work found [https://hips.seas.harvard.edu/files/rippel-spectral-nips-2015.pdf](https://hips.seas.harvard.edu/files/rippel-spectral-nips-2015.pdf)\n", "comments": ["Correct me if I'm wrong @vrv, but batch_ifft2d does not have a gradient on the cpu. I assume you are using a cpu build. If you want to do gradients of fft's you need to use a gpu. (assigning to @petewarden  as internal bug 29447493 is assigned to him)\n", "Perhaps I am not doing it right -- but calling `tf.device(\":gpu0\"):` resulted in the same error\n", "Please see https://www.tensorflow.org/versions/r0.9/how_tos/using_gpu/index.html.\n\nSpecify your gpu with `tf.device(\"/gpu:0\")`, and let us know if that works.\n\nAdditionally, ensure that you are running a gpu-enabled installation of tensorflow. See https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html.\n", "It did not work -- however, after diving into the code a bit, I realized the issue is in the tf.complex() call, changing the code from\n\n`w_ifft = tf.real(tf.batch_ifft2d(tf.complex(w, 0.0)))`\n\nto \n\n`w_ifft = tf.real(tf.batch_ifft2d(tf.complex(w, w * 0.0)))`\n\nResults in a gradient and no error. I'm not sure if this is a bug - but it's logical at least.\n", "@rryan is this intended behavior?\n", "Nope, I'll take a look.\n", "Hm, I think I misread this bug initially -- it's intended behavior that for `tf.complex(x, y)`, x and y must have the same shape. Instead of `w * 0.0` you could do `tf.zeros_like(w)` to get a zero tensor of the same shape/dtype as w.\n"]}, {"number": 3238, "title": "Android example fails to build with AndroidResourceProcessingAction: No such file or directory", "body": "### Environment info\n\nOperating System:\nUbuntu 16.04 LTS\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n-rw-r--r-- 1 root root 189170 May 18 19:42 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 May 18 19:42 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 May 18 19:42 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 May 18 19:42 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 May 18 19:42 /usr/local/cuda/lib/libcudart_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.:\n   Compiled python wheel from source and installed it. But I don't think that is relevant for building the Android demo. I maybe wrong.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.8.0\n\nIf installed from sources, provide the commit hash:\nDon't have it\n### Steps to reproduce\n\n```\n$ git clone https://github.com/tensorflow/tensorflow.git\n$ cd tensorflow\n$ # Edit WORKSPACE file\n$ head -n 20 WORKSPACE \nworkspace(name = \"org_tensorflow\")\n\n\nandroid_sdk_repository(\n    name = \"androidsdk\",\n    api_level = 23,\n    build_tools_version = \"23.0.1\",\n    # Replace with path to Android SDK on your system\n    path = \"/home/username/Android/Sdk\",\n)\n\nandroid_ndk_repository(\n    name=\"androidndk\",\n    path=\"/home/username/software/android-ndk-r10e\",\n    api_level=21)\n\n# Please add all new TensorFlow dependencies in workspace.bzl.\nload(\"//tensorflow:workspace.bzl\", \"tf_workspace\")\ntf_workspace()\n\n$ ls ~/Android/Sdk/\nadd-ons  build-tools  extras  licenses  ndk-bundle  platforms  platform-tools  SDK Readme.txt  sources  system-images  temp  tools\n$ ls ~/software/android-ndk-r10e/\nbuild  find-win-host.cmd  ndk-build      ndk-depends  ndk-gdb-py  ndk-gdb-py.cmd  ndk-which  prebuilt    RELEASE.TXT                samples  tests\ndocs   GNUmakefile        ndk-build.cmd  ndk-gdb      ndk-gdb.py  ndk-stack       platforms  README.TXT  remove-windows-symlink.sh  sources  toolchains\n\n$ bazel build //tensorflow/examples/android:tensorflow_demo\n```\n### What have you tried?\n\nNothing yet.\n### Logs or other output that would be helpful\n\n```\nERROR: /home/username/tmp/tensorflow/tensorflow/examples/android/BUILD:47:1: Processing resources failed: namespace-sandbox failed: error executing command /home/username/.cache/bazel/_bazel_username/2b0ff57a0352a92ef031f8d7910813fd/tensorflow/_bin/namespace-sandbox ... (remaining 26 argument(s) skipped).\nbazel-out/host/bin/external/bazel_tools/tools/android/resources_processor: line 21: /home/username/.cache/bazel/_bazel_username/2b0ff57a0352a92ef031f8d7910813fd/tensorflow/bazel-out/host/bin/external/bazel_tools/tools/android/resources_processor.runfiles/external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/AndroidResourceProcessingAction: No such file or directory\n```\n\nI notice that although the file bazel is looking for does not exist, this file seems to be there\n\n```\n~/.cache/bazel/_bazel_username/2b0ff57a0352a92ef031f8d7910813fd/tensorflow/bazel-out/host/bin/external/bazel_tools/tools/android/resources_processor.runfiles/org_tensorflow/external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/AndroidResourceProcessingAction\n```\n\nIt looks to me that some component is not attaching \"org_tensorflow\" to some path\n\nI did a fresh clone and tried compiling the Android example without compiling the rest of tensorflow. My assumption is that bazel would build whatever was necessary. I will attach the full log if it's required. \n", "comments": ["The steps you've tried should be sufficient. Did you clone recently? The current version of TF is 0.9.0, but you are seeing 0.8.0\n", "Yes I cloned today. The tensorflow wheel I installed was compiled from source a month back. That should not matter, right ?\n", "Which version of Bazel are you using?\n\nCan you please bazel clean, then run:\n`bazel build //tensorflow/examples/android:tensorflow_demo --verbose_failures -s`\nand paste the full log?\n", "Bazel details are \n\n```\nBuild label: 0.2.2-jdk7\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Apr 21 12:57:12 2016 (1461243432)\nBuild timestamp: 1461243432\nBuild timestamp as int: 1461243432\n```\n\nI will paste the log shortly. Should I run `./configure` before building ?\n", "It shouldn't be necessary to run configure for the Android demo.\n\nI'd try upgrading to the most recent bazel (0.3.0) first to see if that fixes your problem.\n", "Thanks @andrewharp That fixed the issue. I remember specifically downgrading bazel because tensorflow (0.8.0) would not build with newest bazel. Closing this\n"]}, {"number": 3237, "title": "Fix issue 3186: buffer alignment needs to be 8-byte for uint64 vals ", "body": "This Closes #3186 \n\nAdd uint64_t to the u_ union to ensure 8-byte alignment, pointers are not 8-byte aligned on 32 bit architectures.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "(we can only take a look if the CLA is signed).\n", "Working on CLA.\n", "any updates on CLA?\n", "@eplondke any update?\n", "Closing due to inactivity.\n"]}, {"number": 3236, "title": "fix import error for graph_util (#3203)", "body": "Fixes #3203 - ImportError for graph_util from tensorflow.python.framework\n\nRunning the retraining at https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/?utm_campaign=chrome_series_machinelearning_063016&utm_source=gdev&utm_medium=yt-desc#4 causes an import error without this patch\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "it looks like graph_util in master is moved from client to framework in a future release (see a558c6e3b38846727873b5afbbc3ba309ae5dff5) and therefore this request is redundant.\n"]}, {"number": 3235, "title": "Accompany TF_LoadLibrary with TF_DeleteLibrary", "body": "The pull request addresses issue #3213. Please let me know\n- if I should add a comment to the new function, and\n- if I should also change [tf_session.i](https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/python/client/tf_session.i#L239) or [load_library.py](https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/python/framework/load_library.py#L60).\n\nThanks!\n\nRegards,\nIvan\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "@IvanUkhov Thanks a lot for the PR. Although your `TF_DeleteLibrary` frees up the memory owned by a `TF_Library` handle, I am not sure how useful that is. The reason that we didn't bother adding this function in the first place is that we don't know of a reliable way to unload the library from the process' address space. If you know of an implementation for unloading the library on linux and mac, that would make this a much stronger PR.\n", "@keveman, please have a look at issue #3213 and at the comment there. This function is not for unloading libraries; it\u2019s for freeing memory. Freeing memory is the caller\u2019s responsibility, but the caller doesn\u2019t know how it was allocated and, hence, cannot do it adequately. For unloading libraries, one might add something like `TF_UnloadLibrary` in the future.\n", "@IvanUkhov, sure, I read your comment. What I'm wondering about is the utility of just deleting the handle without unloading the library.\n", "@keveman, I see. Thanks for the clarification! I think it depends on what `TF_DeleteLibrary` is supposed to do. If we agree that its sole purpose is to dispose `TF_Library` then the current implementation of the function satiates the purpose. It eliminates a memory leakage, and it makes one confident about the usage of the API as one knows exactly what to do with the pointer given by `TF_LoadLibrary`. Imagine, for instance, what people calling the function from other languages think when they are given a pointer and told that they own it. If, however, `TF_DeleteLibrary` is also responsible for unloading the library then, I agree, it\u2019s better to do not have any function at all than a half-working one.\n\nIt might be interesting to note that there is a certain analogy between `TF_DeleteSession` and `TF_DeleteLibrary` and between `TF_CloseSession` and `TF_UnloadLibrary` (potential).\n", "Is dlclose() not sufficient for a TF_UnloadLibrary implementation?\n", "@dauba-dauba Reading the documentation [here](http://pubs.opengroup.org/onlinepubs/009695399/functions/dlclose.html), it doesn't seem like there are any guarantees about the shared library being unmapped from the memory space. Also, with a multithreaded application like TensorFlow, I don't know how we can reason about `dlclose` when there is another thread executing a kernel loaded from the shared library. @IvanUkhov, while I share your sentiment about avoiding memory leaks, I can't imagine people loading hundreds of shared libraries via `TF_LoadLibrary` and leaking the handles. If we get into a situation like that, we can definitely revisit this, but until I see some compelling reason, I would rather have libraries loaded and remain in memory.\n", "@keveman, I phrased it poorly, but I was actually referring to confidence. Leakage is an unpleasant but indeed minor issue. In any case, I appreciate your time and respect your opinion. Please feel free to close both the issue and the pull request. Thank you.\n", "IIRC, the spec reads that way because some implementations on now-rarely-used flavors of Unix (e.g. AIX, or maybe HP-UX) never removed libraries from the process address space. Both Linux and OS X man pages for dlclose() state that it decrements a ref count and, if zero, unloads the library.\n\nThat said, if semantically there's no good way of tracking symbol references into the loaded library, then I agree it's easier to just declare that such libraries are \"load-only\".\n", "@dauba-dauba, right, even if the OS removed it from the address space, it would be hard to track currently running threads that have ops and kernels from the library.\n@IvanUkhov Thanks a lot for your time. Expecting more PRs in the future :)\n"]}, {"number": 3234, "title": "distributed tensorflow does not use GPU 1,2,3 on server B ", "body": "### Environment info\n\nOperating System: Ubuntu 14.04 desktop\nInstalled version of CUDA and cuDNN: 7.5,  5.0.5\n tensorflow 0.9.0.rc0 is installed from source.\n\n---\n\nI'm using distributed tensorflow with 8 Titan-x GPUs in two servers.\n4 GPUs are in one server.\n\nGU=gpu utilization\n\nThese are what I tested, changing ps and workers.\nCifar-10, ResNet. batchsize=32.\n\n1) serverA (1 ps, 1 worker),  serverB (None) ==> Mem allocated on (serverA: GPU 0: GU>30%)\n\n2) serverA (1 ps, 4 workers),  serverB (None)  ==> Mem allocated on (serverA: GPU 0,1,2,3: GU>30%)\n\n3) serverA (1 ps, 1 workers),  serverB (1 ps, 1 worker)  ==> Mem allocated on (serverA: GPU 0,1: GU>30%), (serverB: GPU 0: GU ~8%). \n\n4) serverA (1 ps, 1 workers),  serverB (1 ps, 2 workers)  ==> Mem allocated on (serverA: GPU 0,1,2: GU>30%), (serverB: GPU 0: GU ~8%).\n\n5) serverA (1 ps, 1 workers),  serverB (1 ps, 3 workers)  ==> Mem allocated on (serverA: GPU 0,1,2,3: GU>30%), (serverB: GPU 0: GU ~8%).\n\n6) serverA (1 ps, 4 workers),  serverB (1 ps, 4 workers)  ==> Mem allocated on (serverA: GPU 0,1,2,3: GU>50%), (serverB: GPU 0: GU ~8%).\n\nIt looks dist tensorflow does not use GPU on serverB.\n\nnext is \"nvidia-smi\" in case of 6)\n\n((serverA)) Mem allocated on GPU 0,1,2,3\n![image](https://cloud.githubusercontent.com/assets/9377459/16659014/75379c00-44a3-11e6-957c-845bed33f492.png)\n\n((serverB)) Mem allocated on GPU 0 (last one is 0)\n![image](https://cloud.githubusercontent.com/assets/9377459/16658977/5748e794-44a3-11e6-94d1-f2eac24639cf.png)\n\nI use replica_device_setter to allocate workers to GPU.\n\n```\nif FLAGS.job == \"ps\":\n    server.join()\nelif FLAGS.job == \"worker\":\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(worker_device=\"/gpu:%d\" % (FLAGS.task_id%4), cluster=cluster)):\n```\n\nplease help me.\n", "comments": ["Could you please take a look @mrry.\n", " [@aselle](http://github.com/aselle)\nI could not find anything related with this issue in  [@mrry](https://github.com/mrry)\nCould you tell me which file or directory should I look at?\n", "@hellf, mrry is a person.  That link goes to his personal github page.\naselle was asking mrry to to take a look at the issue.\n", "Hi there! I'm on vacation right now, but if this is still a problem in a fortnight, feel free to reassign to me.\n", "Thanks all :)\nI'm still in this problem.\n", "It looks like the problem is in your `worker_device=\"/gpu:%d\" % (FLAGS.task_id%4)` argument to `tf.train.replica_device_setter()`. There are two parts to the problem:\n1. The device string doesn't specify a task ID (i.e. `\"/task:%d\" % (FLAGS.task_id)`). Unless you have specified `device_filters` in your session configuration, this will result in all ops being placed in task 0, which runs in server A.\n2. Each process on a particular server is creating devices `\"/gpu:0\"`, ... ,`\"/gpu:3\"`, because by default a server (or a single-process `tf.Session`) will create one TensorFlow device per physical device on the system. This will lead to inefficient memory allocation between the processes. You should use the `CUDA_VISIBLE_DEVICES` environment variable to limit each server to being able to see only a single device, which will be available as `\"/gpu:0\"` in that process. \n\nAfter setting `CUDA_VISIBLE_DEVICES` appropriately, you can use `worker_device=\"/job:worker/task:%d/gpu:0\" % (FLAGS.task_id)` as the argument to `tf.train.replica_device_setter()`, and the utilization should be balanced across the GPUs (assuming that you build the same graph in each of your worker processes, and use something like the `tf.train.Supervisor` to manage the distributed execution).\n", "Closing for now due to lack of response.  @hellf I'm happy to reopen if more details emerge.\n"]}, {"number": 3233, "title": "partial_run can't do incremental feeds in InteractiveSession", "body": "Here is a section of code copied from the docstring of `partial_run`, with required import statements added. However, note that this constructs an `InteractiveSession` instead of a `Session`.\n\n``` python\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.ops import array_ops, math_ops\nfrom tensorflow.python import dtypes\nsess = tf.InteractiveSession()\n\na = array_ops.placeholder(dtypes.float32, shape=[])\nb = array_ops.placeholder(dtypes.float32, shape=[])\nc = array_ops.placeholder(dtypes.float32, shape=[])\nr1 = math_ops.add(a, b)\nr2 = math_ops.mul(r1, c)\n\nh = sess.partial_run_setup([r1, r2], [a, b, c])\nres = sess.partial_run(h, r1, feed_dict={a: 1, b: 2})\nres = sess.partial_run(h, r2, feed_dict={c: res})\n```\n\nThe code fails with a `NotFoundError`. When the `InteractiveSession` is replaced by a normal `Session`, the code runs correctly. The difference between the two is that the interactive session sets the `place_pruned_graph` option to True (starting a plain `Session` with `place_pruned_graph=True` reproduces the issue.)\n\nI do all of my work in an interactive terminal, and I would find it useful to have `partial_run` work correctly in this setting.\n\n**System info**: Ubuntu 14.04, tensorflow 11834fb02bfa9296f4aa48ee1eaa2a002fecbf1f; python3; cudnn 4.0.4, cuda 7.0.28\n", "comments": ["I am able to reproduce this. @yuanbyu could you take a look, as you have looked at other partial_run issues?\n", "Any updates on this issue?\n", "Thank you for reporting this. Since the code has changed substantially since the issue was open, it's not clear whether it's still valid. Please open a new issue if the problem persists with new versions of tensorflow.", "@drpngx Please don't close this.\r\n\r\nNothing has \"changed substantially\" that I am aware of -- it took me less than a minute to re-run the above sample code in tensorflow 0.12 (I just had to fix some trivial renames). The same error persists.\r\n\r\nCode to reproduce is:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nsess = tf.InteractiveSession()\r\n\r\na = tf.placeholder(tf.float32, shape=[])\r\nb = tf.placeholder(tf.float32, shape=[])\r\nc = tf.placeholder(tf.float32, shape=[])\r\nr1 = tf.add(a, b)\r\nr2 = tf.mul(r1, c)\r\n\r\nh = sess.partial_run_setup([r1, r2], [a, b, c])\r\nres = sess.partial_run(h, r1, feed_dict={a: 1, b: 2})\r\nres = sess.partial_run(h, r2, feed_dict={c: res})\r\n```", "@yuanbyu this looks like a bug in `partial_run`.", "Here's the stack trace:\r\n\r\n```\r\n    res = sess.partial_run(h, r1, feed_dict={a: 1, b: 2})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 821, in partial_run\r\n    return self._run(handle, fetches, feed_dict, None, None)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1017, in _do_run\r\n    fetch_list)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Feed Placeholder_2:0: not found\r\n```\r\n\r\n`_do_run` calls `_do_call` with this (both working and non-working paths print the same):\r\n\r\n```\r\n<function _prun_fn at 0x7f7c98f80488>\r\n<Swig Object of type 'TF_DeprecatedSession *' at 0x7f7c98fa8db0>\r\nPlaceholder:0,Placeholder_1:0,Placeholder_2:0->Add:0,Mul:0//1/;0\r\n{'Placeholder:0': array(1.0, dtype=float32), 'Placeholder_1:0': array(2.0, dtype=float32)}\r\n```\r\n", "OK, I need to call it a day for now but we're failing to find it in `name_to_node` here:\r\nhttps://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/direct_session.cc#L822\r\nand when using `InteractiveSession` we're not inserting it in the [values](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/direct_session.cc#L932) (only `add` and `mul` get inserted, not any of the placeholders).", "@drpngx looks like this is still a problem in the latest version.\r\nAny updates on the problem?", "@nikitakit did you try a regular Session instead of an InteractiveSession?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 3232, "title": "ImportError: cannot import name pywrap_tensorflow", "body": "Operating System: MAC ISO 10.11\n", "comments": ["See https://github.com/tensorflow/tensorflow/issues/3217\n", "Thanks for the reply @Waffleboy. @wwfeng2 please close the issue if this suggestion resolves your problem. \n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n", "thanks,\n"]}, {"number": 3231, "title": "Fix doc typo from `Tensor.run` to `Tensor.eval`", "body": "Quick documentation in session.py to fix to point to correct function name and permalink id.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "Looks good to me.\n"]}, {"number": 3230, "title": "Reserve Udacity docker container after the session finished", "body": "Fix issue #2596 as a beginner.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "Looks good to me. Thanks.\n"]}, {"number": 3229, "title": "maximize function added to Optimizer", "body": "I have added a Maximize() function to the Optimizer.  This makes efforts where maximizing a function straight forward to implement.\n\nI have used the \"minimize()\" function as a model, created \"maximize()\" and simply inverted the sign to the cost of the \"compute_gradients()\" function.\n\nThis is my first attempt at an update on Github, please let me know if there is an error in the request.\n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "I'm not sure this is worth adding.  Can't you just pass `-value` to `minimize`?  Having a separate function is a nontrivial cognitive burden since `Optimizer` is part of a class hierarchy.  For example, your current implementation is wrong, since if someone overrides `minimize` to do something new `maximize` will no longer do the right thing for that derived class.\n", "I am sure I can now use (-COST), but I am just trying to make it easier for people in the future and have code more readable.  When maximizing, it really isn't a COST that is being maximized so that isn't intuitive and certainly -COST was not clear to me and not likely clear to many readers of future code so it seems like Maximize() would simply round out Optimization.  You can then minimize a cost or maximize a value function.  It seems like this will help make the python wrapper easier to use.\n\nI did think about changes to \"minimize()\" in the future, so in my implementation, I didn't call \"minimize()\" from the \"maximize()\" function but use it as a reference.  \"maximize\" only depends upon the \"compute_gradients\" & \"apply_gradients\" functions.  I think this is a simple addition that will have benefit in the long run.\n\nI can write a test based upon \"testBasic\" in \"optimizer_test.py\" if that is required for a core change.  I have done a simple bench test to test maximize() within my local repository.  I can call it \"testBasicMaximize\" but I am unsure how to get it to run during the build test phase.\n", "It is natural to think it would help, but I do think this would cause more confusion for users down the road rather than less, in particular for users who write their own optimizers.  As I said above, your current implementation of `maximize` is wrong in the presence of derived classes.  If you fix it to call `minimize` the situation improves, but not enough: the result is still fragile if someone writes an optimizer and decides to override both minimize and maximize, then a third person overrides just minimize, etc.  These fears are based on experience: it's already surprisingly difficult to robustly inherit from optimizers in nonstandard ways, so I'm leery of making it worse in order to save 1 character.\n", "I would prefer not to add another API to this class too.  Users should be able to easily understand that maximizing a value is the same as minimizing the negative value (or even the reciprocal of the value).  Keeping our core classes simple is going to be a better benefit to everyone in the long term.  You are free to develop a higher-level library with this functionality and API though!\n", "I have no problem moving it to a contribute section, but I would still like to keep the same context:\n        sgd_op = tf.train.GradientDescentOptimizer(3.0).maximize(value, global_step, [var0, var1])\n\nI will try to inherit Optimizer and add the maximize() function.  Is that acceptable?\n", "You're free to do that in your own code, but I would recommend against it.  If you do that, your code will break if you want to switch optimizers.  If you really do want to replace one character with a whole function, I'd recommend:\n\n```\ndef maximize(optimizer, value, ...):\n  return optimizer.minimize(-value, ...)\n```\n"]}, {"number": 3228, "title": "Dockerhub image tags naming inconsistency", "body": "Hi,\n\nI see on dockerhub that the image for version `r0.8` is the tag `0.8.0` but for version `r0.9` is `r0.9`. It would be nice if they were consistent so you don't have to look up every time you want to update.\n", "comments": ["Yes. r0.9 should be 0.9.0. We should update our scripts.\n", "I have fixed the process generating our docker images, and rerunning our release script for 0.9.0.\nSorry for taking so long to fix this simple issue. I will Close the issue once I verify the images are uploaded with the correct tags.\n", "Thanks!\n"]}, {"number": 3227, "title": "Some additional pointers to resources.", "body": "", "comments": []}]