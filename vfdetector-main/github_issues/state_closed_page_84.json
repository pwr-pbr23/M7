[{"number": 52653, "title": "TFLite: XNNPACK delegate failed to delegate FULLY_CONNECTED node", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): Source for inference, Binary for model generation\r\n- TensorFlow version (use command below): r2.6.0 (from git tag) for the inference code.  2.4.1 for model generation.\r\n- Python version: 3.8.5 (model generation side only)\r\n- Bazel version (if compiling from source): N/A, I'm building TFLite via CMake\r\n- GCC/Compiler version (if compiling from source): MSVC 19.29.30136.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen loading the attached MobileNet model via the TFLite C API, the XNNPACK delegate fails with the following output:\r\n\r\n```\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nERROR: failed to delegate FULLY_CONNECTED node #67\r\nERROR: Node number 83 (TfLiteXNNPackDelegate) failed to prepare.\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe model loads correctly.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): -\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nModel file: [model.fp32.tflite.zip](https://github.com/tensorflow/tensorflow/files/7409553/model.fp32.tflite.zip)\r\n\r\n### Model generation (if not using the attached tflite model):\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom pathlib import Path\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n  base_model = tf.keras.applications.mobilenet_v2.MobileNetV2(\r\n        input_shape=(224,224,3), alpha=1.0, weights='imagenet',\r\n        classes=6, include_top=False, pooling='avg')\r\n  output = tf.keras.layers.Dense(6, name='logits', dtype='float32')(base_model.output)\r\n  model = tf.keras.Model(base_model.inputs, output)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\nPath('model.fp32.tflite').write_bytes(tflite_model)\r\n```\r\n\r\n### Model loading in Cpp\r\n\r\n```cpp\r\n#include <tensorflow/lite/c/c_api.h>\r\n#include <tensorflow/lite/delegates/xnnpack/xnnpack_delegate.h>\r\n\r\nint main()\r\n{\r\n    auto m_model = TfLiteModelCreateFromFile(\"model.fp32.tflite\");\r\n    auto m_options = TfLiteInterpreterOptionsCreate();\r\n    TfLiteXNNPackDelegateOptions opt = TfLiteXNNPackDelegateOptionsDefault();\r\n    auto m_xnnpack_delegate = TfLiteXNNPackDelegateCreate(&opt);\r\n    TfLiteInterpreterOptionsAddDelegate(m_options, m_xnnpack_delegate);\r\n    auto m_interpreter = TfLiteInterpreterCreate(m_model, m_options); // This returns nullptr and prints error messages in the console\r\n}\r\n```\r\n", "comments": ["Working on this, I found the source of the issue: `converter.optimizations = [tf.lite.Optimize.DEFAULT]` does something to the graph that breaks the model for XNNPACK. Exporting the very same model without that line generates a model that is loaded correctly in the delegate.", "Hi, refer [this](https://www.tensorflow.org/lite/performance/post_training_quantization#dynamic_range_quantization) and let me know if this helps, Thanks!", "@sachinprasadhs It does not, as I already came to the conclusion that this is the source of the problem I show above. There is no mention on the linked page about XNNPACK (or it not supporting models with dynamic range quantization enabled), nor were the error messages produced during execution mentioning anything connected to quantization being the problem.", "If your issue is resolved by removing` converter.optimizations = [tf.lite.Optimize.DEFAULT] `, could you please close this issue. Thanks", "@GPhilo: this was fixed in 29d64ea02d00ac97a05d4aa6cd373e78fde3bc74", "@sachinprasadhs no, the issue *is* the undocumented side effects of applying the optimisation to a model.\r\nI found out myself that \"don't apply the default optimisations\" prevents the error from happening. Unfortunately, it also increases the model size massively (around 4x, since the weights are not quantises anymore).\r\n\r\nSo, again, the issue that needs addressing is either an undocumented incompatibility between the default optimisations and the xnnpack delegate, in which case please document it, or a bug in the dequantisation of the weights when loading a model with the xnnpack delegate, in which case that should probably be fixed by someone who knows how that code works internally.\r\n\r\n@Maratyszcza could you elaborate on the fix? The linked commit only adds a check and an error, which more than a fix seems a \"we don't support this\". If that's the case, could you clarify exactly what is that is not supported? Quoting the documentation for the default optimisation options found here: https://www.tensorflow.org/lite/performance/post_training_quantization\r\n\r\n\"At inference, weights are converted from 8-bits of precision to floating point and computed using floating-point kernels.\"\r\n\r\nSo, as far as I understand it, the optimisation should only help towards reducing the model size (since the same kernels are used after de-quantization at inference time). So, assuming the model is dequantised correctly, shouldn't it work the same way as a \"pure\" floating point model works, with respect to delegates?", "`FULLY_CONNECTED` operator with floating-point input, output, and bias, but `INT8` weights is not supported. The referenced commit prevents trying to offload such operators to XNNPACK.", "@Maratyszcza Thanks for the clarification, but then I still don't understand the descripton of the optimisation options I quoted above (putting it here again with the whole context and adding with proper formatting):\r\n\r\n> The simplest form of post-training quantization statically quantizes only the weights from floating point to integer, which has 8-bits of precision. At inference, weights are converted from 8-bits of precision to floating point and computed using floating-point kernels.\r\n\r\nThis line coms from [here](https://www.tensorflow.org/lite/performance/post_training_quantization#dynamic_range_quantization). The way I read it, this is just a (lossy) operation to compress down the weights, which is then undone at runtime, because \"weights are converted from 8-bits of precision to floating point **and computed using floating-point kernels**\". This to me is extremely useful, because it leads in a big reduction in model weight, while still being executed as a fp32 model (which on our platform is more performant than the full int8 variant).\r\n\r\nThat said, the operator is not a fully-connected with floating-point I/O, bias and INT8 weights (or, at least, it was never meant to be): it is a fully fp32 fully-connected operator whose weights have not yet been converted back to fp32.", "TFLite has convolution kernels which directly consume `INT8` weights and do conversion to floating-point just before doing the computations. XNNPACK doesn't include such kernels, so it requires that weights are unpacked into FP32 in advance. You can achieve it by inserting `DEQUANTIZE` operator to unpack `INT8` weights of the `FULLY_CONNECTED` operator into floating-point.", "Thank you for the additional information.\r\n\r\nTaking a deeper look at the xnnpack_delegate.c you linked in your first comment, I noticed that between r2.6 and the current master [the handling of kTfLiteInt8 tensors has been geatly extended](https://github.com/tensorflow/tensorflow/blob/8a2a4c3ca06a622989823c7dfa58e01044244935/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc#L233). Building from the current master branch, I can load the quantized model correctly and it seems to work as expected, so I'll close the issue because it seems to be solved in the upcoming r2.7.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52653\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52653\">No</a>\n"]}, {"number": 52652, "title": "Input tensors to a Functional must come from `tf.keras.Input`. Received: tf.Tensor( [[[ 0.48200893]   [ 2.3927188 ]   [ 2.27573   ]   ...   [ 0.02205751]   [ 0.49903008]   [ 0.27605903]]   [[ 5.696963  ]   [ 2.674962  ]   [-0.40183762]   ...   [-0.9503441 ]   [-0.2546521 ]   [-0.09128924]]   [[ 6.234084  ]   [ 3.4273057 ]   [ 0.97633094]   ...   [-0.7193552 ]   [-0.17173009]   [-2.2666237 ]]   ...   [[ 5.675222  ]   [ 3.1183105 ]   [ 1.2060118 ]   ...   [ 0.40663907]   [-1.2273816 ]   [ 0.5917349 ]]   [[ 5.627962  ]   [ 2.7021565 ]   [ 0.14773515]   ...   [ 0.16272612]   [ 0.263398  ]   [-1.0668343 ]]   [[ 4.9977493 ]   [ 2.6142614 ]   [ 2.8206005 ]   ...   [-0.35571998]   [-0.05585755]   [-1.796899  ]]], shape=(341, 101, 1), dtype=float32) (missing previous layer metadata).", "body": "<em>Please make sure that this is an issue related to keras.\r\ntag:keras_template</em>\r\n\r\n**Important Notice**\r\n\r\nPlease note that `tf.keras` code was moved entirely to\r\n[keras-team/keras](https://github.com/keras-team/keras) repository\r\n\r\nYou can open any code/doc bugs, performance issues, and feature requests\r\n in [keras-team/keras](https://github.com/keras-team/keras/issues) repository\r\n\r\n`tf.keras` related issues opened in\r\n[tensorflow/tensorflow](https://github.com/tensorflow/tensorflow) repository may\r\nnot get attention as [keras-team/keras](https://github.com/keras-team/keras)\r\nrepository is dedicated for the development of `keras` code\r\n", "comments": ["Hi @arshikasoni! \r\nCould you please the template too as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52652\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52652\">No</a>\n"]}, {"number": 52651, "title": "GPU DynamicPartition support int32&int64", "body": "Currently, DynamicPartitionOp on GPU doesn't support int32&int64. They were implemented but not registered. I tested them in my project and they worked fine.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52651) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52651) for more info**.\n\n<!-- need_author_cla -->", "@sanjoy Any update on this PR? Please. Thanks!", "> @sanjoy Any update on this PR? Please. Thanks!\r\n\r\n@gbaned  Anything I can do for this, such as adding some tests ?  ", "@googlebot I fixed it.", "@gbaned @sanjoy @reedwm I fixed some failing tests. Please help merge this PR."]}, {"number": 52650, "title": "error: `'Py_hash_t'` does not name a type", "body": "**System information**\r\n- OS Platform: `Ubuntu 20.04.3 LTS`\r\n- TensorFlow installed from (source or binary): `source`\r\n- TensorFlow version: 487ce666eba608e3ac909db94c4211c2661f2828\r\n- Python version: `Python 3.8.10`\r\n- Bazel version: `bazel 3.7.2`\r\n- GCC/Compiler version (if compiling from source): `gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0`\r\n- CUDA/cuDNN version: `NaN`\r\n- GPU model and memory: `NaN`\r\n\r\n**Problem**\r\n\r\nWhen I try installing `tensorflow` from source the build fails with a message saying `'error: 'npy_hash_t' does not name a type; did you mean 'npy_half'?'` which is the result of the error `'error: 'Py_hash_t' does not name a type'`\r\n\r\n![Screenshot from 2021-10-25 11-27-41](https://user-images.githubusercontent.com/70365318/138641957-3e555ff7-291d-4b3f-a9a2-b59387814a3f.png)\r\n\r\n**Command that I used to build tensorflow from source**\r\n\r\n```shell\r\n$ bazel --host_jvm_args=-Xms512m build --local_ram_resources 2048 --jobs=2 //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Other info**\r\nThere's a strange behaviour that I would like to inform you, when I examine the messages from the kernel ring buffer I found the `Out of memory` issue which is strange for me because I'm limiting both the `memory` and the `number of jobs` while building `tensorflow` from source.\r\n\r\n![Screenshot from 2021-10-25 11-21-12](https://user-images.githubusercontent.com/70365318/138641409-da4b242f-af19-49f9-be44-bf489882fda1.png)\r\n", "comments": ["@joshiayush Could you please try to use **TF v2.6.0** & have a look at the [build from source](https://www.tensorflow.org/install/source)  ? Please refer to the [tested build configurations](https://www.tensorflow.org/install/source#linux)  and  let us know if it helps? Thank you!", "@sushreebarsa This time I tried building __TF v2.6.0__ and it failed with the same error.\r\n\r\n![Screenshot from 2021-10-25 17-04-01](https://user-images.githubusercontent.com/70365318/138687991-0d676591-8071-4c6a-bb49-4bf1983bb8c4.png)\r\n\r\nAny solution? I'm following the similar instructions given [here](https://www.tensorflow.org/install/source). I'm not enabling the GPU support as it is optional. Am I doing something wrong?", "@sushreebarsa When I looked in the `bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_common.h` file I found that the indentifier `Py_hash_t` is undefined.\r\n\r\n![Screenshot from 2021-10-26 08-00-57](https://user-images.githubusercontent.com/70365318/138798844-3f9abe30-46ec-471c-bbf5-129c05dc75e2.png)\r\n\r\n[This](https://github.com/numpy/numpy/blob/548bc6826b597ab79b9c1451b79ec8d23db9d444/numpy/core/include/numpy/npy_common.h#L388) is the file I'm having issue with at line 388.", "> `--host_jvm_args=-Xms4g --host_jvm_args=-Xms512m`\r\n\r\nThese are self-contradictory! why are you setting two different values for the same thing, just to then add another RAM restriction in the next command line argument? This seems to indicate copy/paste without understanding what you're doing (not a bad thing, that's how you start understanding things), so are you *sure* you want to build TF from source?\r\n\r\n> file I found that the indentifier Py_hash_t is undefined.\r\n\r\nyep. That's the problem here. However, `Py_hash_t` has been part of python/pyport.h since Python v3.2 and it's imported through `Python.h`, which TF most likely includes. \r\n\r\nSo, customer support experience makes me take a **wild guess** (not a diagnosis!): You've got some manually installed Python2 headers somewhere and quite possibly, for some reason, bazel leads your compiler to use these instead of the correct ones.", "> --host_jvm_args=-Xms4g --host_jvm_args=-Xms512m\r\n\r\n@marcusmueller I knew that there's something weird in these arguments because they're kind of self explanatory. And yes this was a copy paste, but I didn't think much on it as my problem was the `Py_hash_t` indentifier, but thanks.\r\n\r\n> So, customer support experience makes me take a wild guess (not a diagnosis!): You've got some manually installed Python2 headers somewhere and quite possibly, for some reason, bazel leads your compiler to use these instead of the correct ones.\r\n\r\nWell I remember I installed Python2 headers manually using `apt`.\r\n\r\n> so are you sure you want to build TF from source?\r\n\r\nWell I successfully installed it using `pip`, but I would love to learn how to do the same with `bazel` particularly on __Ubuntu 20.04__", "@joshiayush,\r\n\r\nCan you confirm if the above [comment](https://github.com/tensorflow/tensorflow/issues/52650#issuecomment-951720449) from @marcusmueller helped in resolving your issue? Also to build TF on Ubuntu 20.04 with bazel you can refer the official [TF guide](https://www.tensorflow.org/install/source) and you can also refer this [link](https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03).Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52650\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52650\">No</a>\n"]}, {"number": 52649, "title": "TFlite support for FakeQuantWithMinMaxVarsPerChannel", "body": "Is there support for conversion of QAT model trained with per-channel quantization to TFlite models?\r\nThanks!", "comments": ["@mmanoharahere ,\r\nCan you please take a look at this [issue](https://github.com/tensorflow/model-optimization/issues/840#issuecomment-927590851) and [link1](https://www.tensorflow.org/jvm/api_docs/java/org/tensorflow/op/quantization/FakeQuantWithMinMaxVarsPerChannel), [link](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/fake-quant-with-min-max-vars-per-channel) which provides more information on quantization to TFlite models.It helps.Thanks", " Thank you for your response. But I am specifically asking for per channel quantization.\r\nI have a QAT model trained with per-channel quantization, and I would like to convert it into TFLite. The model has FakeQuantWithMinMaxVarsPerChannel ops. I would like to see minimum degradation of accuracy.\r\nThis is the error on the console:\r\nArray */*/*/FakeQuantWithMinMaxVarsPerChannel, which is an input to the Transpose operator producing the output array */*/*/FakeQuantWithMinMaxVarsPerChannel/transpose, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\n\r\nIs there a way to support conversion to TFlite?", "@mmanoharahere ,\r\nPlease refer this [document](https://www.tensorflow.org/lite/performance/post_training_quantization) for more information.It helps.Also Kindly open a tensorflow discussion [forum](https://discuss.tensorflow.org/) issue for this as it is not a bug or feature request.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52649\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52649\">No</a>\n"]}, {"number": 52648, "title": "ValueError: Tensor-typed variable initializers must either be wrapped in an init_scope or callable (e.g., `tf.Variable(lambda : tf.truncated_normal([10, 40]))`) when building functions. Please file a feature request if this restriction inconveniences you.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["Hi @zhengweiqin969! \r\nCould you please share a standalone code to reproduce this issue? Feel free to look at similar issues.[ Link1,](https://github.com/tensorflow/tensorflow/issues/41306),[Link2](https://stackoverflow.com/questions/61579678/valueerror-tensor-typed-variable-initializers-must-either-be-wrapped-in-an-init) . Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52648\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52648\">No</a>\n"]}, {"number": 52647, "title": "TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [None, None, None]. Consider casting elements to a supported type.", "body": "I have 4 params: out2, cur_x_input, cur_y_input, cur_z_input.Their shape is:\r\n\r\nout2: [None, 1]\r\ncur_x_input: [None, 4, 1]\r\ncur_y_input: [None, 4, 1]\r\ncur_z_input: [None, 4, 1]\r\n\r\nBut when I execute the following code\uff1a\r\n\r\npressure_grad_x = tf.keras.backend.gradients(out2, cur_x_input)[0]\r\npressure_grad_y = tf.keras.backend.gradients(out2, cur_y_input)[0]\r\npressure_grad_z = tf.keras.backend.gradients(out2, cur_z_input)[0]\r\npressure_grad = tf.convert_to_tensor([pressure_grad_x, pressure_grad_y, pressure_grad_z])\r\n\r\nIt will report an error\uff1a\r\n\r\nTypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [None, None, None]. Consider casting elements to a supported type.\r\n\r\nI find that the value of pressure_grad_x (or pressure_grad_y, or pressure_grad_z) is None. It means that the result of tf.keras.backend.gradients(out2, cur_x_input)[0] \r\n(or tf.keras.backend.gradients(out2, cur_y_input)[0], or tf.keras.backend.gradients(out2, cur_z_input)[0]) is None\r\nThe version of Tensorflow is 2.6.0\r\n\r\nHow should I solve this problem\uff1f\r\nThank you very much! \r\n", "comments": ["@Liozizy \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),Thanks!"]}, {"number": 52646, "title": "Improve overall readblity of code", "body": "Hi,\r\nI made some grammar and spelling fixups to improve overall readablity.\r\n\r\nthanks", "comments": ["Hi @gbaned , is this PR ready to pull?", "> Hi @gbaned , is this PR ready to pull?\r\n\r\nHi @shubham-shahh Yes, but it is showing some internal checks failures. We are working on these.  Thank you!.", "> > Hi @gbaned , is this PR ready to pull?\n> \n> Hi @shubham-shahh Yes, but it is showing some internal checks failures. We are working on these.  Thank you!.\n\nThanks for the update : )", "Hi, @gbaned, shall I revert the commits that are causing the issue?\r\n\r\nThanks"]}, {"number": 52645, "title": "xla compile op cache many tensors to out of memory .", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (use command below): master\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.1 (clang-1001.0.46.4)\r\n- CUDA/cuDNN version: - \r\n- GPU model and memory: - \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nzsh: command not found: v1.12.1-54214-gb51e7cff1aa\r\n\r\n**Describe the current behavior**\r\nxla OOM\r\n\r\n\r\n**Describe the expected behavior**\r\nxla not OOM\r\n\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing):\r\n    flat_hash_map -> LRUCache(with large size) .\r\n\r\n1. OOM\r\n    XlaCompilationCache in XlaCompileOp maybe cache many tensors in ```absl::flat_hash_map<Signature, std::unique_ptr<Entry>, Signature::Hash> cache_```.\r\n    My XlaCompileOp has an input named TruncatedNormalV2, and TruncatedNormalV2 will output a dynamic shape, so my XlaCompileOp will compile many exe file and cache lots of input tensors. \r\n\r\n2. LOG\r\n  I spend 3 days to find tensor OOM, so I think  If there is too much content in cache_, at least print a warning to remind it.\r\n\r\nI want to confirm whether there is such a problem in TF ? or I have a mistake usage ?\r\nIf you think it's a bug, I am happy to fix this problem.\r\n\r\n\r\n\r\n", "comments": ["@zhaozheng09,\r\n\r\nCan you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) to expedite the troubleshooting process?Thanks!", "There's a known limitation of XLA that for dyamic size, it may need to do multiple recompilations and so that may be the reason for the cache to get filled and cause OOM. It can also add unexpected latency for the dynamic shapes. You can take a look at this section of the [video](https://youtu.be/cPAD9vLKE0c?t=822) which explains about the same.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> There's a known limitation of XLA that for dyamic size, it may need to do multiple recompilations and so that may be the reason for the cache to get filled and cause OOM. It can also add unexpected latency for the dynamic shapes. You can take a look at this section of the [video](https://youtu.be/cPAD9vLKE0c?t=822) which explains about the same.\r\n\r\n1. If user misuse leads to OOM, is there a log to remind them of this in time?\r\n2. Should we replace flat_map with LRUCache ?", "Displaying warning message on memory usage will not be a good option since it will be specific to system hardware specific level.\r\nInstead you can mange your GPU memory using options like `set_memory_growth`, more details [here](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth). \r\nAdditionally, you can investigate further on memory consumption using HLO graph, you can find the reference and similar discussion [here](https://discuss.tensorflow.org/t/xla-jit-compile-flag-and-gpu-memory-usage/5127/10). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52645\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52645\">No</a>\n"]}, {"number": 52644, "title": "' TypeError: '<=' not supported between instances of 'list' and 'Rectangle'  ' from pose_classification.ipynb", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nGithub : https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb\r\n\r\nGoogle colab : https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb \r\n\r\n## Description of issue (what needs changing):\r\n\r\nI'm following steps from that URL (Movenet yoga pose classification) by google colab and Jupyter notebook. \r\nWhen executing 'Preprocess the TRAIN dataset' code, an error occurs.\r\n\r\n-----------------------------------------------------------------------------------\r\n\r\nCode \r\n-------------\r\n\r\nif not is_skip_step_1:\r\n  images_in_train_folder = os.path.join(IMAGES_ROOT, 'train')\r\n  images_out_train_folder = 'poses_images_out_train'\r\n  csvs_out_train_path = 'train_data.csv'\r\n\r\n  preprocessor = MoveNetPreprocessor(\r\n      images_in_folder=images_in_train_folder,\r\n      images_out_folder=images_out_train_folder,\r\n      csvs_out_path=csvs_out_train_path,\r\n  )\r\n\r\n  preprocessor.process(per_pose_class_limit=None)\r\n\r\n---------------------------------------------------------------------------\r\n\r\nError message\r\n-------------\r\n\r\nPreprocessing chair\r\n  0%|          | 0/200 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\r\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n  0%|          | 0/200 [00:00<?, ?it/s]\r\n'---------------------------------------------------------------------------'\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-11-87cdca3ca58e> in <module>()\r\n     10   )\r\n     11 \r\n---> 12   preprocessor.process(per_pose_class_limit=None)\r\n\r\n2 frames\r\n<__array_function__ internals> in amin(*args, **kwargs)\r\n\r\n/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)\r\n     85                 return reduction(axis=axis, out=out, **passkwargs)\r\n     86 \r\n---> 87     return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n     88 \r\n     89 \r\n\r\nTypeError: '<=' not supported between instances of 'list' and 'Rectangle'\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/43366200/138588872-3d9f22a0-28f8-4807-a02d-e69fd99cb801.png)\r\n\r\nHow can I solve this issue?\r\n", "comments": ["@donghyun128 \r\nCan you please share the error in text format so its easy for other users, also the code in a colab gist with the error reported.\r\nYou may also refer to this [link](https://stackoverflow.com/questions/42739880/typeerror-not-supported-between-instances-of-block-and-block)", "Sure! Thank you for response.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52643, "title": "[tflite] make op profiling work when CoreML Delegate is used", "body": "- issue:\r\nWhen running CoreML delegate + op profiling, I met bus error most of the time.\r\n- how to reproduce: on MacOS, \r\n```\r\nbenchmark_mode --graph=.. --use_coreml=1 --enable_op_profiling=1 ...\r\n```\r\non iOS devices, add\r\n\r\n```\r\nuse_coreml : \"true\", \r\nenable_op_profiling : \"true\", \r\n``` \r\n\r\nto  `benchmark_params.json`  of  https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark/ios.\r\n\r\nThe model I used to test this problem is [MobilenetEdgeTPU](https://storage.cloud.google.com/mobilenet_edgetpu/checkpoints/mobilenet_edgetpu_224_1.0.tgz).\r\n\r\n- when it went wrong:\r\n```\r\n% bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model --graph=/tmp/mobilenet_edgetpu_224_1.0_float.tflite --enable_op_profiling=1 --use_coreml=1\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [/tmp/mobilenet_edgetpu_224_1.0_float.tflite]\r\nEnable op profiling: [1]\r\nUse CoreML: [1]\r\nLoaded model /tmp/mobilenet_edgetpu_224_1.0_float.tflite\r\n2021-10-24 20:34:08.503 benchmark_model[42608:216982] coreml_version must be 2 or 3. Setting to 3.\r\nCOREML delegate created.\r\nINFO: CoreML delegate: 75 nodes delegated out of 77 nodes, with 2 partitions.\r\n\r\nExplicitly applied COREML delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.\r\nThe input model file size (MB): 16.3221\r\nInitialized session in 568.672ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=262 first=3849 curr=1887 min=956 max=11424 avg=1888.11 std=1682\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\nzsh: bus error  bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model   --use_coreml=1\r\n\r\n```\r\nAfter adding `profiling_string` initialization\r\n```\r\n% bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model --graph=/tmp/mobilenet_edgetpu_224_1.0_float.tflite --enable_op_profiling=1 --use_coreml=1\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [/tmp/mobilenet_edgetpu_224_1.0_float.tflite]\r\nEnable op profiling: [1]\r\nUse CoreML: [1]\r\nLoaded model /tmp/mobilenet_edgetpu_224_1.0_float.tflite\r\n2021-10-24 20:38:10.823 benchmark_model[45038:226402] coreml_version must be 2 or 3. Setting to 3.\r\nCOREML delegate created.\r\nINFO: CoreML delegate: 75 nodes delegated out of 77 nodes, with 2 partitions.\r\n\r\nExplicitly applied COREML delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.\r\nThe input model file size (MB): 16.3221\r\nInitialized session in 590.552ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=261 first=2610 curr=8604 min=973 max=10389 avg=1908.45 std=1752\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=407 first=2088 curr=5167 min=955 max=11235 avg=2388.13 std=2227\r\n\r\nInference timings in us: Init: 590552, First inference: 2610, Warmup (avg): 1908.45, Inference (avg): 2388.13\r\nProfiling Info for Benchmark Initialization:\r\n============================== Run Order ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t ModifyGraphWithDelegate\t            0.000\t  576.714\t  576.714\t 99.988%\t 99.988%\t     0.000\t        1\tModifyGraphWithDelegate/0\r\n\t         AllocateTensors\t          576.697\t    0.066\t    0.034\t  0.012%\t100.000%\t     0.000\t        2\tAllocateTensors/0\r\n\r\n============================== Top by Computation Time ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t ModifyGraphWithDelegate\t            0.000\t  576.714\t  576.714\t 99.988%\t 99.988%\t     0.000\t        1\tModifyGraphWithDelegate/0\r\n\t         AllocateTensors\t          576.697\t    0.066\t    0.034\t  0.012%\t100.000%\t     0.000\t        2\tAllocateTensors/0\r\n\r\nNumber of nodes executed: 2\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t ModifyGraphWithDelegate\t        1\t   576.714\t    99.988%\t    99.988%\t     0.000\t        1\r\n\t         AllocateTensors\t        1\t     0.067\t     0.012%\t   100.000%\t     0.000\t        2\r\n\r\nTimings (microseconds): count=1 curr=576781\r\nMemory (bytes): count=0\r\n2 nodes observed\r\n\r\n\r\n\r\nOperator-wise Profiling Info for Regular Benchmark Runs:\r\n============================== Run Order ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t    TfLiteCoreMlDelegate\t            0.000\t    2.074\t    2.379\t 99.710%\t 99.710%\t     0.000\t        1\t[MobilenetEdgeTPU/Logits/Conv2d_1c_1x1/BiasAdd]:77\r\n\t                 RESHAPE\t            2.380\t    0.002\t    0.001\t  0.053%\t 99.763%\t     0.000\t        1\t[MobilenetEdgeTPU/Logits/Squeeze]:75\r\n\t                 SOFTMAX\t            2.381\t    0.008\t    0.006\t  0.237%\t100.000%\t     0.000\t        1\t[Softmax]:76\r\n\r\n============================== Top by Computation Time ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t    TfLiteCoreMlDelegate\t            0.000\t    2.074\t    2.379\t 99.710%\t 99.710%\t     0.000\t        1\t[MobilenetEdgeTPU/Logits/Conv2d_1c_1x1/BiasAdd]:77\r\n\t                 SOFTMAX\t            2.381\t    0.008\t    0.006\t  0.237%\t 99.947%\t     0.000\t        1\t[Softmax]:76\r\n\t                 RESHAPE\t            2.380\t    0.002\t    0.001\t  0.053%\t100.000%\t     0.000\t        1\t[MobilenetEdgeTPU/Logits/Squeeze]:75\r\n\r\nNumber of nodes executed: 3\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t    TfLiteCoreMlDelegate\t        1\t     2.379\t    99.748%\t    99.748%\t     0.000\t        1\r\n\t                 SOFTMAX\t        1\t     0.005\t     0.210%\t    99.958%\t     0.000\t        1\r\n\t                 RESHAPE\t        1\t     0.001\t     0.042%\t   100.000%\t     0.000\t        1\r\n\r\nTimings (microseconds): count=407 first=2084 curr=5164 min=955 max=11231 avg=2386.31 std=2227\r\nMemory (bytes): count=0\r\n3 nodes observed\r\n```\r\n\r\n- why it went wrong: in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/interpreter.h#L637-L642\r\nThe interpreter wanted to access non-null string, it profiling_string is not properly set :-(", "comments": []}, {"number": 52642, "title": "Frozen graph converted by freeze_graph.py is not usable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution: Ubuntu 20.04.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.9.5\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\nI tried to convert tf2 object detection into a frozen graph.  I downloaded the model from [here](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz).  \r\nAt first, i tried to use tf tool freeze_graph.py, it worked. But the converted pb is not usable, it cannot run normally as tf1 pb  downloaded [here](http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz)   \r\nThen i tried to convert tf2 ckpt to tf1 ckpt, using the script [here](https://www.tensorflow.org/guide/migrate/migrating_checkpoints#convert_tf2_checkpoint_to_tf1), but the converted ckpt was not usable either.   \r\nCould you please tell me is there any way to convert  or maybe how to run this different pb?\r\n\r\n**Standalone code to reproduce the issue**\r\n- code to convert saved model to frozen graph\r\n```shell\r\npath=/path/to/saved_model\r\npython tensorflow/python/tools/freeze_graph.py \\\r\n--input_saved_model_dir=$path/saved_model \\\r\n--output_node_names=StatefulPartitionedCall \\\r\n--output_graph=$path/tf2.pb\r\n```\r\n- error when running this converted pb\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Failed to find function \"__inference_signature_wrapper_frozen_1938\" in function library:\r\n```", "comments": ["@Wanzizhu \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "hi, @sushreebarsa, below is the script.\r\n- bash script\r\n```shell\r\nwget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\r\ntar -xvf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz \r\n \r\npath=./ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/\r\npython3 -m tensorflow.python.tools.freeze_graph \\\r\n--input_saved_model_dir=$path/saved_model \\\r\n--output_node_names=StatefulPartitionedCall \\\r\n--output_graph=$path/frozen.pb\r\n\r\npython infer_detections.py -g ./ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/frozen.pb\r\n```\r\n- infer_detection.py\r\n```python\r\nfrom __future__ import division\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\r\nfrom tensorflow.python.framework import dtypes\r\nimport os\r\nimport sys\r\nfrom argparse import ArgumentParser\r\n\r\nBATCH_SIZE = 1\r\nIMAGE_SIZE = 640\r\n\r\n\r\nclass model_infer:\r\n\r\n    def __init__(self):\r\n        arg_parser = ArgumentParser(description='Parse args')\r\n\r\n        arg_parser.add_argument('-g', \"--input-graph\",\r\n                                help='Specify the input graph.',\r\n                                dest='input_graph')\r\n\r\n        # parse the arguments\r\n        self.args = arg_parser.parse_args()\r\n\r\n        self.input_layer = 'serving_default_input_tensor'\r\n        self.output_layers = 'StatefulPartitionedCall'\r\n        self.output_node_index = ['0', '1', '2', '3', '4']\r\n        self.load_graph()\r\n\r\n        self.input_tensor = self.infer_graph.get_tensor_by_name(\r\n            self.input_layer + \":0\")\r\n        self.output_tensors = [self.infer_graph.get_tensor_by_name(self.output_layers + \":\" + index) for index in self.output_node_index]\r\n\r\n\r\n    def load_graph(self):\r\n        print('load graph from: ' + self.args.input_graph)\r\n\r\n        self.infer_graph = tf.Graph()\r\n        with self.infer_graph.as_default():\r\n            graph_def = tf.compat.v1.GraphDef()\r\n            with tf.compat.v1.gfile.FastGFile(self.args.input_graph, 'rb') as input_file:\r\n                input_graph_content = input_file.read()\r\n                graph_def.ParseFromString(input_graph_content)\r\n            output_graph = optimize_for_inference(graph_def, [self.input_layer],\r\n                                                  [self.output_layers], dtypes.uint8.as_datatype_enum, False)\r\n            tf.import_graph_def(output_graph, name='')\r\n        print('----------------------load graph: success------------------------', flush=True)\r\n\r\n    def run_benchmark(self):\r\n        with tf.compat.v1.Session(graph=self.infer_graph) as sess:\r\n            input_images = np.random.normal(size=[BATCH_SIZE,\r\n                    IMAGE_SIZE, IMAGE_SIZE, 3])\r\n            _ = sess.run(self.output_tensors, {\r\n                                 self.input_tensor: input_images})\r\n\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    infer = model_infer()\r\n    infer.run_benchmark()\r\n\r\n```", "@Wanzizhu I tried to reproduce the issue using TF v2.6.0 & tf-nightly on Colab , could you please have a look at the [gist](https://colab.research.google.com/gist/sushreebarsa/41e2f7a60236e5b8ba2581c000e2e90e/52642.ipynb#scrollTo=mwnnnNHQkg_D) and confirm the same ? Please have a look at the similar[ issue1](https://github.com/tensorflow/tensorflow/issues/27614), [issue2 ](https://github.com/tensorflow/tensorflow/issues/20532), [official ](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/tools/freeze_graph.py)freeze.graph.py and let us know if it helps? Thank you! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52642\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52642\">No</a>\n"]}, {"number": 52641, "title": "When using make to run the test, the error \"there should be no - m at this time. Find: the parameter format is incorrect\" is reported", "body": "F:\\\u684c\u9762\\TyniML\\tensorflow-2.4.3>make -f tensorflow/lite/micro/tools/make/Makefile 'test_hello_world_test'\r\nprocess_begin: CreateProcess(NULL, uname -m, ...) failed.\r\n\u6b64\u65f6\u4e0d\u5e94\u6709 -m\u3002\r\nFIND: \u53c2\u6570\u683c\u5f0f\u4e0d\u6b63\u786e\r\nFIND: \u53c2\u6570\u683c\u5f0f\u4e0d\u6b63\u786e\r\nmake: *** No rule to make target ''test_hello_world_test''\u3002 \u505c\u6b62\u3002\r\n", "comments": ["@Big141 ,\r\nCan you please check this [issue1](https://github.com/tensorflow/tensorflow/issues/42765) and [issue2](https://github.com/tensorflow/tensorflow/issues/42476) with the similar error.It helps.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52641\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52641\">No</a>\n"]}, {"number": 52640, "title": "Typo in the model.compile code examples: tf.keras.optimizers not tf.keras.optimizer", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#compile\r\n\r\n## Description of issue (what needs changing):\r\nError in the model.compile code examples: tf.keras.optimizers not tf.keras.optimizer\r\nThe code is misspelt. See link\r\n<img width=\"705\" alt=\"Screen Shot 2021-10-23 at 10 58 08 PM\" src=\"https://user-images.githubusercontent.com/93064122/138578511-289edad0-d236-4b5d-8ef8-a689b66ca2c4.png\">\r\n.\r\n", "comments": ["@rand42studios - Great observation.  Its a typo in doc string - \r\nhttps://github.com/keras-team/keras/blob/v2.6.0/keras/engine/training.py#L463\r\n\r\nMaster Branch & r2.7:\r\nhttps://github.com/keras-team/keras/blob/master/keras/engine/training.py#L493\r\nhttps://github.com/keras-team/keras/blob/r2.7/keras/engine/training.py#L493\r\nIt has been fixed in master & r2.7 branch and will be fixed in next release of TF (2.7).  You can also post about this in github.com/keras-team/keras repo.", "You can also submit a Pull Request to r2.6.1 in keras repo, so its updated when TF 2.6.1 is released:\r\nhttps://github.com/keras-team/keras/blob/r2.6.1/keras/engine/training.py#L468\r\n\r\nIt will be a great way to contribute to Keras Repo.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "This has been fixed in r2.6.1 , Thank you all"]}, {"number": 52639, "title": "AttributeError: 'FlatMapDataset' object has no attribute 'bucket_by_sequence_length'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur Version 11.3.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): i don't know, just did pip3 install tensorflow a few month ago\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nSee attached file.\r\n<img width=\"920\" alt=\"Screen Shot 2021-10-23 at 10 51 44 PM\" src=\"https://user-images.githubusercontent.com/93064122/138578322-c5a2f5a1-b84b-4592-a787-226fdd197a31.png\">\r\n\r\n**Describe the expected behavior**\r\nSee below. It should function without error as described in the official documentation.\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/Dataset#bucket_by_sequence_length\r\n<img width=\"562\" alt=\"Screen Shot 2021-10-23 at 10 52 35 PM\" src=\"https://user-images.githubusercontent.com/93064122/138578345-5aac8010-96b3-43fb-a84c-76c523373d9b.png\">", "comments": ["I've tried using this function in my own projects, all throw the same errors.", "@rand42studios \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "> @rand42studios In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\nimport matplotlib.pyplot as plt\r\n\r\nnp.set_printoptions(precision=4)\r\nprint(tf.__version__)\r\n\r\nelements = [\r\n  [0], [1, 2, 3, 4], [5, 6, 7],\r\n  [7, 8, 9, 10, 11], [13, 14, 15, 16, 19, 20], [21, 22]]\r\ndataset = tf.data.Dataset.from_generator(\r\n    lambda: elements, tf.int64, output_shapes=[None])\r\ndataset = dataset.bucket_by_sequence_length(\r\n        element_length_func=lambda elem: tf.shape(elem)[0],\r\n        bucket_boundaries=[3, 5],\r\n        bucket_batch_sizes=[2, 2, 2])\r\nfor elem in dataset.as_numpy_iterator():\r\n    print(elem)", "@rand42studios Thank you for the update!\r\nI was able to run your code successfully on **colab** using `TF v2.6.0 ` without any error .Could you please find the [gist ](https://colab.research.google.com/gist/sushreebarsa/83a969ea9830e717cbf7cc04fd7233e0/52639.ipynb)and confirm the same ? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52639\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52639\">No</a>\n", "Thanks! Issue resolved after update"]}, {"number": 52638, "title": "ValueError: Converting a list of object of custom class (ExtensionType) to tensor error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):linux ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.8.0-dev20211023\r\n- Python version:3.8.10\r\n- Bazel version (if compiling from source):no\r\n- GCC/Compiler version (if compiling from source):no\r\n- CUDA/cuDNN version: nil\r\n- GPU model and memory: nil\r\n\r\ni have implemented a simple class in python extending from tf.exerimental.ExtensionType which stores three tensors into it. My goal is to create a list of these objects which then can be returned from a function after converting this list into tensor. \r\nI have tried creating a tf.Variable object , tf.TensorArray object , tf.constant obect , passed numpy object of this list into these function too but getting an error that i cannot convert to tensor or \"classname\" is not identified as proper tf.Dtype. \r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1nea-BQ6nRsbiAAKFMnmeKSeqAeHiq563?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n```\r\nValueError: Attempt to convert a value (Array(values=<tf.Tensor: shape=(5,), dtype=int32, numpy=array([1, 2, 3, 4, 5], dtype=int32)>, matched=<tf.Tensor: shape=(), dtype=bool, numpy=True>, iou=<tf.Tensor: shape=(), dtype=float32, numpy=53.46>)) with an unsupported type (<class '__main__.Array'>) to a Tensor.\r\n```\r\n", "comments": ["We don't support converting ExtensionTypes to tensors. you should be able to return them from tf.function directly (see other bug). Closing this one for now - lets continue discussion on the other bug.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52638\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52638\">No</a>\n"]}, {"number": 52637, "title": "Periodic delays of TF inference time in batch process ", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: --\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.2.0, 2.6.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2, 11.2\r\n- GPU model and memory: GTX 960M\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nmytime = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S_%f\")\r\nlogFile.write((\"Prediction Time Start          :{ptime}\\n\").format(ptime=mytime))\r\n\r\nprediction_cam1 = model.predict(my_image_arr_norm_expand)\r\n\r\nmytime = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S_%f\")\r\nlogFile.write((\"Prediction Time End          :{ptime}\\n\").format(ptime=mytime))\r\n```\r\n\r\nThe graph below shows batch inference time of a group of images. X-Axis is image no and Y-axis is inference time in milliseconds. Average inference time 60-70 ms is OK for my project but these periodic delays which have inference time over 150 ms are not OK. At first I thought this may be related to program priority. So I changed its priority to real time priority in Windows 10(TF:2.2.0, CUDA:10.2) but it did not changed anything. There were still these periodic delays.\r\n\r\nThen I changed OS to Ubuntu where I can modify kernel. I repeated same steps as I did in Windows 10 first. My steps were :\r\n\r\n- Take log of prediction code.\r\n- Take log of prediction code with niceness value -20.\r\n- Changed linux kernel to low latency version\r\n- Take log of prediction code with priority 40.\r\n- Take log of prediction code with priority 80(change multiple pid values which had same name like 'python inference.py')\r\n- Force garbage collector at the end of each cycle with gc.collect().\r\n\r\nAfter all these steps periodic delays still exist. \r\n\r\n![image](https://user-images.githubusercontent.com/12881237/138571880-9dcb6ce8-f242-46de-a815-b12fbbd74afa.png)\r\n\r\nThanks to any help in advance.", "comments": ["Hi @mayantd!\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "Thanks. I will post it there."]}, {"number": 52636, "title": "supporting \"object\" type in tensorflow dtype", "body": "**System information**\r\n- TensorFlow version (you are using): 2.6.0\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\nCurrently one can create tf. Variable , tf.TensorArray or tf.ragged.constant with dtype related to numbers repr (float, int) , well string too. But will it not be beneficial to allow the user to create these variables containing objects of custom class implemented in python. In NumPy, we can do this by stating the dtype = \"object\" but TensorFlow isn't compatible with dtype of NumPy. Given that the user handles this variable with proper caution, it gives more flexibility.\r\n\r\n**Will this change the current api? How?**\r\nNot very sure\r\n\r\n**Who will benefit with this feature?**\r\nTensorflow is mainly used to create models to deal with problems involving complex mathematical calculations/approximation, but given the huge popularity of tf , while implementing some other components of the model (which can be done under numpy too) there might be a case where, some sub-components can be forced to be implemented in tf , involving custom python class objects, thus making it impossible to run, unless one removes the dependency of using those object altogether.  \r\nOne use case might be, as described in the below colab notebook , while calculating mAP metric, the postprocessing function is called via test_step function of tf.Keras.model.Model class. for abstraction purposes, I am returning a list of objects of the custom class (Bounding Box) which will be used later in the mAP class. Currently, I am able to return NumPy array of this list where dtype = \"object\" and while running in eager mode this is running, but while running in graph mode , defun function wants me to return only tensor objects and I cant do it as the list contains objects of a type not supported by tf.\r\nIf someone is wondering why would you run post-processing in tf instead of NumPy, it becomes a necessity when this function is required to run metric calc in the validation step per epoch.\r\n\r\n**Any Other info.**\r\nplease refer to this notebook, for a possible use case of the custom class object. This might seem totally unnecessary but yet on the other side provides really good abstraction and flexibility.\r\nhttps://colab.research.google.com/drive/1Ei2t9coPNEVrfmejzaRUDIs-tm05EZFD?usp=sharing", "comments": ["Have you seen https://www.tensorflow.org/guide/extension_type? ExtensionTypes are user defined custom objects that have first class support in TF. ", "I did not know about this feature. Thank for this direction @rohan100jain . will take a look.\r\n"]}, {"number": 52635, "title": "Merge pull request #51715 from yongtang:46909-tf.summary.create_file_\u2026", "body": "\u2026writer\r\n\r\nPiperOrigin-RevId: 394109960\r\nChange-Id: I444e59a7f90acd6487832d74b677093403c8a083", "comments": []}, {"number": 52634, "title": "Merge pull request #51715 from yongtang:46909-tf.summary.create_file_\u2026", "body": "\u2026writer\r\n\r\nPiperOrigin-RevId: 394109960\r\nChange-Id: I444e59a7f90acd6487832d74b677093403c8a083", "comments": []}, {"number": 52633, "title": "Merge pull request #51715 from yongtang:46909-tf.summary.create_file_\u2026", "body": "\u2026writer\r\n\r\nPiperOrigin-RevId: 394109960\r\nChange-Id: I444e59a7f90acd6487832d74b677093403c8a083", "comments": []}, {"number": 52632, "title": "Merge pull request #51359 from yongtang:46913-range-overflow", "body": "PiperOrigin-RevId: 391529518\r\nChange-Id: Ie3db4ae6d3c0f3dc88404e1dbdc22f7d03cbeb3b", "comments": ["Moving to #52667"]}, {"number": 52631, "title": "Simple improvements to GetTensorListDynamicDims", "body": "To be honest, all this code looks very dubious to me.\r\n\r\nI saw it by accident and perhaps it has some kind of secret meaning.\r\nIn general, both the signature and the implementation of the function look strange. \r\n\r\nThese are simple changes that should do a little better(?).\r\nBut I would pay attention to all the uses of this function.\r\n\r\nI'm sorry if I was wrong, correct me if so.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52631) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 52630, "title": "quantization aware training issue", "body": "Hi, I am training a U2Net model and want to use quantization aware training to reduce the size of the model. According to the official Docs regarding this, We need to import the library of tensorflow_model_optimization and use the quantize model function.\r\n\r\nIssue - When i am using this, I am getting AttributeError: 'list' object has no attribute 'dtype'\r\nThis is my model declaration code-\r\n\r\nquantize_model = tfmot.quantization.keras.quantize_model\r\n\r\nnet_input = Input(shape=(256,256,3)) \r\n\r\nmodel_output = U2NET(net_input)\r\n\r\nmodel = Model(inputs = net_input, outputs = model_output)\r\n\r\nqa_model = quantize_model(model)\r\nlr = 1e-3\r\n\r\nopt = tf.keras.optimizers.Adam(learning_rate = lr)\r\n\r\nbce = BinaryCrossentropy()\r\n\r\nqa_model.compile(optimizer = opt, loss = loss, metrics = None)\r\n\r\nU2net function is the model declaration part.\r\nI am able to get the normal model.summary()\r\n\r\nI am trying to train the model of google colab gpu\r\n\r\nWithout QAT the model is training fine.\r\n\r\nPlease help\r\n", "comments": ["can somebody help me please? its a bit urgent.", "@Sgsouham \r\nCan you please refr to these links and let us know:[link](https://stackoverflow.com/questions/46759801/how-to-solve-the-attributeerrorlist-object-has-no-attribute-astype),[link1](https://newbedev.com/python-list-object-has-no-attribute-dtype-code-example), [link2](https://www.codegrepper.com/code-examples/python/frameworks/file-path-in-python/AttributeError%3A+%27list%27+object+has+no+attribute+%27dtype%27),[link2](https://www.py4u.net/discuss/1386834).\r\nthis is not a bug, please check the code and in case error still persist please create this issue in tf discussion forum as there is a larger community there to support.", "I think i was not able to explain the issue properly.\r\n\r\n```\r\nquantize_model = tfmot.quantization.keras.quantize_model\r\n\r\nnet_input = Input(shape=(256,256,3))\r\n\r\nmodel_output = U2NET(net_input)\r\n\r\nmodel = Model(inputs = net_input, outputs = model_output)\r\n\r\nqa_model = quantize_model(model)\r\nlr = 1e-3\r\n\r\nopt = tf.keras.optimizers.Adam(learning_rate = lr)\r\n\r\nbce = BinaryCrossentropy()\r\n\r\nqa_model.compile(optimizer = opt, loss = loss, metrics = None)\r\n```\r\n\r\nqa_model should be the quantized keras model named \"model\" right? but i am getting the dtype attribute error in that specific statement.", "Now when I am removing the quantization parts from the code, i am able to train the model without any issue.\r\n\r\n```\r\n\r\n#quantize_model = tfmot.quantization.keras.quantize_model\r\n\r\nnet_input = Input(shape=(256,256,3))\r\n\r\nmodel_output = U2NET(net_input)\r\n\r\nmodel = Model(inputs = net_input, outputs = model_output)\r\n\r\n#qa_model = quantize_model(model)\r\nlr = 1e-3\r\n\r\nopt = tf.keras.optimizers.Adam(learning_rate = lr)\r\n\r\nbce = BinaryCrossentropy()\r\n\r\n#qa_model.compile(optimizer = opt, loss = loss, metrics = None)\r\n\r\nmodel.compile(optimizer = opt, loss = loss, metrics = None)\r\n```\r\n\r\nthis works fine. But i want the Quantization aware model.", "Could you find a solution? I'm having the same problem, can't use the _quantize_model_ function. This is my code:\r\n\r\n```number_of_classes = 10\r\n\r\ninputs = tf.keras.Input(shape=(51))\r\nembedding = landmarks_to_embedding(inputs)\r\n\r\nlayer = keras.layers.Dense(128, activation=\"relu\")(embedding)\r\nlayer = keras.layers.Dropout(0.5)(layer)\r\nlayer = keras.layers.Dense(64, activation=\"relu\")(layer)\r\noutputs = keras.layers.Dense(number_of_classes, activation=\"softmax\")(layer)\r\n\r\nmodel = keras.Model(inputs, outputs)\r\nquant_aware_model = tfmot.quantization.keras.quantize_model(model)\r\n\r\nmodel.summary()```", "from what i can gather @rsuriano , tihs api works only with sequential or functional api, and for custom class, layers need to be annotated manually. Though i am still waiting for an official answer. In your case, you can move the entire layers into a sequential api and it might work.\r\n", "There will be some limitations and complications while using `quantization aware training`, instead you can also try with `post_training quantization `to reduce the model size with less complications. Thanks!", "Thanks for the comment @Sgsouham! At the end of the day I solved my problem without quantization, as the issue was somewhere else. ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52630\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52630\">No</a>\n"]}, {"number": 52629, "title": "in tf2.6, predict_classes has disappeared broke the backward compatibility", "body": "y_proba(shape) (3, 30)\r\nTraceback (most recent call last):\r\n  File \"p297.py\", line 79, in <module>\r\n    y_pred = model.predict_classes(X_new)\r\nAttributeError: 'Sequential' object has no attribute 'predict_classes'\r\n\r\n\r\nSince I transitioned from tensorflow 1.x to 2.6, the above method no longer work. It appears a poor job of maintaining backward compatibility. \r\n\r\nmodel=keras.models.Sequential()\r\n...\r\ny_proba = model.predict(X_new) # ok\r\ny_pred = model.predict_classes(X_new) # broken\r\n\r\n", "comments": ["Hi @gggh000 ! please use model.predict() method , mode.predict_classes() has been removed from TF 2.6 .Please provide a simple standalone to proceed further. Reference: [Link1](https://stackoverflow.com/questions/68836551/keras-attributeerror-sequential-object-has-no-attribute-predict-classes),[Link2](https://github.com/keras-team/keras/issues/3938) .Thank you!", "no you got it wrong, predict_classes is not predict. ", "Ok! @gggh000 ! Could you please share a stand alone code to replicate this issue?", "```\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport matplotlib as plt\r\nimport sys\r\nimport time\r\nimport re\r\nimport numpy as np\r\nimport helper\r\nfrom tensorflow import keras\r\nprint(tf.__version__)\r\nprint(keras.__version__)\r\nDEBUG=0\r\nCONFIG_ENABLE_PLOT=0\r\nCONFIG_EPOCHS=30\r\nCONFIG_BATCH_SIZE=32\r\n\r\nCONFIG_EPOCHS, CONFIG_BATCH_SIZE = helper.process_params(sys.argv, [\"epochs\", \"batch_size\"])\r\n\r\nfashion_mnist = keras.datasets.fashion_mnist\r\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\r\nprint(\"X_train_full.shape: \", X_train_full.shape)\r\nprint(\"X_train_full.dtype: \", X_train_full.dtype)\r\n\r\nX_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:]/255.0\r\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\r\nX_test = X_test / 255.0\r\nclass_names = [\"T-shirt/top\",\"Trouser\", \"Pullover\", \"Dress\", \"Coat\" , \"Sandal\", \"Shirt\", \"Sneaker\",\"Bad\",\"Ankle boot\"]\r\n\r\nmodel=keras.models.Sequential()\r\nmodel.add(keras.layers.Flatten(input_shape = [28, 28]))\r\nmodel.add(keras.layers.Dense(300, activation=\"relu\"))\r\nmodel.add(keras.layers.Dense(100, activation=\"relu\"))\r\nmodel.add(keras.layers.Dense(30, activation=\"softmax\"))\r\n\r\nprint(\"model summary: \", model.summary())\r\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\r\nhistory=model.fit(X_train, y_train, epochs=CONFIG_EPOCHS, batch_size=CONFIG_BATCH_SIZE, validation_data=(X_valid, y_valid))\r\n\r\nif CONFIG_ENABLE_PLOT:\r\n    pd.DataFrame(history.history).plot(figsize=(8, 5))\r\n    plt.pyplot.grid(True)\r\n    plt.pyplot.gca().set_ylim(0, 1)\r\n    plt.pyplot.show()\r\n\r\nmodel.evaluate(X_test, y_test)\r\n\r\nif DEBUG:\r\n    print(\"model layers: \", model.layers)\r\n\r\nweights, biases  = model.layers[1].get_weights()\r\n\r\nif DEBUG:\r\n    print(\"weights, biases (shapes): \", weights, biases, weights.shape, biases.shape)\r\n\r\nmodel.save(\"p297.h5\")\r\nX_new = X_test[:3]\r\nprint(\"X_new shape: \", X_new.shape)\r\ny_proba = model.predict(X_new)\r\nprint(\"y_proba (predict)(value): \", y_proba.round(2), \"\\ny_proba(shape)\", np.array(y_proba).shape)\r\n\r\ny_pred = model.predict_classes(X_new)\r\nprint(\"y_pred (predict_classes): \", y_pred)\r\n\r\n\r\n\r\n\r\n```", "Hi @gggh000! I have resolved this issue in TF 2.6 , Attaching [Gist](https://colab.research.google.com/gist/mohantym/a5246b54e00063d6e065d908fef076fc/github_52629.ipynb) for reference . [Reference](https://exerror.com/keras-attributeerror-sequential-object-has-no-attribute-predict_classes/). Thanks!", "this may work for end result however code does not look clean. For everyone who want to predict classes, tapping into numpy library etc. I would be interested in why predict_classes has deprecated and any substitute API is available. \r\ny_pred=np.argmax(predict_x,axis=1)\r\n#y_pred = model.predict_classes(X_new)", "Ok @gggh000 ! Could you please post  this query on Stackoverflow/[TF forum](https://discuss.tensorflow.org/)  as there will be a larger community to  answer your query . Thank you!"]}, {"number": 52628, "title": "[r2.7] Remove C++ content from tensorflow/c/c_api_macros.h", "body": "**Note: This PR is a cherry-pick of #52313 to R2.7**\r\n\r\nThis PR tries to remove C++ conent from tensorflow/c/c_api_macros.h\r\nso that it is possible to include this file from an external plugin\r\nwithout depending on tensorflow's C++ headers (tensorflow/core/platform/status.h).\r\n\r\nThe reason for this move is that for external plugins (e.g., file systems plugins)\r\nthat utilize C API, it cannot include status.h as otherwise the whole tensorflow's\r\nC++ library will be a dependency. This defeat the purpose of C APi modular plugin.\r\n\r\nThis PR place C++ content to tensorflow/core/platform/status.h instead so that\r\neverything will still be combined both for C external plugins and C++ plugins.\r\n\r\n(Note the C++ inclusion was added in PR 51647.)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\nUpdate to create c_api_macros_internal.h based on review feedback\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 52627, "title": "TypeError: To be compatible with tf.eager.defun, Python functions must return zero or more Tensors;", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): no\r\n- CUDA/cuDNN version: cuda_11.2\r\n- GPU model and memory: nvidia geforce rtx 2060 / 6gb\r\n\r\nI am implementing a function which is calld by tf.keras model.fit function while running the validation dataset after every epoch, hence the fucntion will be runnig in graph mode. The problem is when i am reurning parameters from this function . I am getting a typeerror stating that 'i should return 0 or more tensors' , whereas currently I am returning a list. \r\n*Since the list contains numpy array and each np array is collection of custom class in python. I have tried converting this numpy array in tensor variable , tensorarray (dtype is a problem ) to no avail . Hence i am unable to figure out how should i return the Box objects in form of tensor.\r\n*As this function is running in eager mode  **prefectly fine** , i am wondering is it a strict signature constraint to return tensors and not even numpy array from a function decorated under tf.fucntion decorator.\r\n\r\n**Standalone code to reproduce the issue**  \r\nhttps://colab.research.google.com/drive/1Ei2t9coPNEVrfmejzaRUDIs-tm05EZFD?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in convert(x)\r\n    962         try:\r\n--> 963           x = ops.convert_to_tensor_or_composite(x)\r\n    964         except (ValueError, TypeError):\r\n\r\n20 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor_or_composite(value, dtype, name)\r\n   1688   return internal_convert_to_tensor_or_composite(\r\n-> 1689       value=value, dtype=dtype, name=name, as_ref=False)\r\n   1690 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor_or_composite(value, dtype, name, as_ref)\r\n   1727         as_ref=as_ref,\r\n-> 1728         accepted_result_types=(Tensor, composite_tensor.CompositeTensor))\r\n   1729 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py in wrapped(*args, **kwargs)\r\n    162           return func(*args, **kwargs)\r\n--> 163       return func(*args, **kwargs)\r\n    164 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n   1565     if ret is None:\r\n-> 1566       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1567 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)\r\n     51   del as_ref  # Unused.\r\n---> 52   return constant_op.constant(value, dtype, name=name)\r\n     53 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n    271   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 272                         allow_broadcast=True)\r\n    273 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    289           value, dtype=dtype, shape=shape, verify_shape=verify_shape,\r\n--> 290           allow_broadcast=allow_broadcast))\r\n    291   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n    563         \"Element type not supported in TensorProto: %s\" % numpy_dtype.name)\r\n--> 564   append_fn(tensor_proto, proto_values)\r\n    565 \r\n\r\ntensorflow/python/framework/fast_tensor_util.pyx in tensorflow.python.framework.fast_tensor_util.AppendObjectArrayToTensorProto()\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/compat.py in as_bytes(bytes_or_text, encoding)\r\n     86     raise TypeError('Expected binary or unicode string, got %r' %\r\n---> 87                     (bytes_or_text,))\r\n     88 \r\n\r\nTypeError: Expected binary or unicode string, got <__main__.BoundingBox object at 0x7f3f3b798a50>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-119-e9ab93aab472> in <module>()\r\n     29 \r\n     30 if __name__ == \"__main__\":\r\n---> 31     test_postprocess()\r\n\r\n<ipython-input-119-e9ab93aab472> in test_postprocess()\r\n     24         batch_labels = [np.random.uniform(0,1 , size = [1,13,13,3,7]).astype(np.float32) , np.random.uniform(0,1 , size = [1,26,26,3,7]).astype(np.float32) , np.random.uniform(0,1 , size = [1,52,52,3,7]).astype(np.float32)]\r\n     25         # box_objects = post_process(batch_labels , anchors)\r\n---> 26         box_objects = post_process(batch_labels , anchors)\r\n     27         # print(box_objects)\r\n     28 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    883 \r\n    884       with OptionalXlaContext(self._jit_compile):\r\n--> 885         result = self._call(*args, **kwds)\r\n    886 \r\n    887       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    931       # This is the first call of __call__, so we have to initialize.\r\n    932       initializers = []\r\n--> 933       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    934     finally:\r\n    935       # At this point we know that the initialization is complete (or less\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    758     self._concrete_stateful_fn = (\r\n    759         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 760             *args, **kwds))\r\n    761 \r\n    762     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   3064       args, kwargs = None, None\r\n   3065     with self._lock:\r\n-> 3066       graph_function, _ = self._maybe_define_function(args, kwargs)\r\n   3067     return graph_function\r\n   3068 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3461 \r\n   3462           self._function_cache.missed.add(call_context_key)\r\n-> 3463           graph_function = self._create_graph_function(args, kwargs)\r\n   3464           self._function_cache.primary[cache_key] = graph_function\r\n   3465 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3306             arg_names=arg_names,\r\n   3307             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 3308             capture_by_value=self._capture_by_value),\r\n   3309         self._function_attributes,\r\n   3310         function_spec=self.function_spec,\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\r\n   1010       # TensorArrays and `None`s.\r\n   1011       func_outputs = nest.map_structure(convert, func_outputs,\r\n-> 1012                                         expand_composites=True)\r\n   1013 \r\n   1014       check_mutation(func_args_before, func_args, original_func)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)\r\n    867 \r\n    868   return pack_sequence_as(\r\n--> 869       structure[0], [func(*x) for x in entries],\r\n    870       expand_composites=expand_composites)\r\n    871 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py in <listcomp>(.0)\r\n    867 \r\n    868   return pack_sequence_as(\r\n--> 869       structure[0], [func(*x) for x in entries],\r\n    870       expand_composites=expand_composites)\r\n    871 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in convert(x)\r\n    967               \"must return zero or more Tensors; in compilation of %s, found \"\r\n    968               \"return value of type %s, which is not a Tensor.\" %\r\n--> 969               (str(python_func), type(x)))\r\n    970       if add_control_dependencies:\r\n    971         x = deps_ctx.mark_as_return(x)\r\n\r\nTypeError: To be compatible with tf.eager.defun, Python functions must return zero or more Tensors; in compilation of <function post_process at 0x7f3f368f9b00>, found return value of type <class 'numpy.ndarray'>, which is not a Tensor.\r\n```\r\n", "comments": ["From the looks of it you're trying to return a list of BoundingBox objects? I think if you made BoundingBox a tf.experimental.ExtensionType it should work\r\n\r\n@edloper ", "@rohan100jain  i have tried creating objects of class inhering from tf.experimental.ExtensionType , but still unable to create a list of  these objects to a tensor , as i would need to return a tensor from this above function . Please refer to this issue for detailed description : #52638 ", "@sushreebarsa could you please look into it?", "tf.constant(..) of an ExtensionType would not work. The idea is that you can use an ExtensionType like a Tensor e.g. if you wanted to return it (or a list or dict containing ExtensionsTypes) from a tf.function that would work - you don't need to wrap it in a Tensor. \r\n\r\nWe'll update the error message in func_graph.py to reflect it (it mistakenly says that return type can only be Tensor)", "@rohan100jain \r\nthanks for the clarification, after experimenting a bit , i was able to return a nested python list of ExtensionType objects from a ```function```. (@tf.function)\r\nSince it said i could only return tensors , and i wanted to return collection of these objects i was trying to create a tensorarray object or so .  Returning a list in nested form  compiled without any erros.\r\nBut i am still wondering that doesn't autograph comverts this pythonic list of these objects into some kind of tensor collection? and if so what would be the datatype of that collective tensor?\r\n", "> We'll update the error message in func_graph.py to reflect it (it mistakenly says that return type can only be Tensor)\r\n\r\nthat would be great, thanks", "Glad to hear that returning a nest of ExtensionTypes works. \r\n\r\nIn this case, the tf.function infrastructure (of which autograph is one part of), flattens the collection of objects into a list of Tensors since at the core of it, a tf.function is tensors in and tensors out\r\n\r\nBut we can only do this flattening that for objects we recognize - for an arbitrary python object, we don't know how to flatten that to a list of tensors. This is where the ExtensionType specification helps us - now we know how what fields to pull out of this custom python object to flatten it and then everything works. \r\n\r\nHope that makes sense!", "yes that makes sense now. again thanks for the direction . closing it now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52627\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52627\">No</a>\n"]}, {"number": 52626, "title": "Cannot load back model with no-op Concatenate layer", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: 3.9.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I create a simple model with a dummy Concatenate layer (i.e. the concatenation receives one single element), I am able to save it successfully, but the subsequent model loading fails.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model loading should finish without errors.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): No\r\n- Briefly describe your candidate solution(if contributing): N/A\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nif __name__ == \"__main__\":\r\n    input_layer = tf.keras.Input(shape=[100])\r\n    dense_layer = tf.keras.layers.Dense(1)(input_layer)\r\n    concatenate_layer = tf.keras.layers.Concatenate()([dense_layer])\r\n    model = tf.keras.Model([input_layer], [concatenate_layer])\r\n    model.compile(optimizer=\"adam\", loss=\"mean_absolute_error\")\r\n    model.save(\"model.h5\")\r\n    loaded_model = tf.keras.models.load_model(\"model.h5\")\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nFull traceback:\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/Users/stefan/workspace/tierra/bug.py\", line 10, in <module>\r\n    loaded_model = tf.keras.models.load_model(\"model.h5\")\r\n  File \"/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/saving/save.py\", line 200, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\r\n  File \"/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/saving/hdf5_format.py\", line 180, in load_model_from_hdf5\r\n    model = model_config_lib.model_from_config(model_config,\r\n  File \"/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/saving/model_config.py\", line 52, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/layers/serialization.py\", line 208, in deserialize\r\n    return generic_utils.deserialize_keras_object(\r\n  File \"/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/utils/generic_utils.py\", line 674, in deserialize_keras_object\r\n    deserialized_obj = cls.from_config(\r\n  File \"/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/functional.py\", line 662, in from_config\r\n    input_tensors, output_tensors, created_layers = reconstruct_from_config(\r\n  File \"/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/functional.py\", line 1283, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/functional.py\", line 1231, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 976, in __call__\r\n    return self._functional_construction_call(inputs, args, kwargs,\r\n  File \"/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1114, in _functional_construction_call\r\n    outputs = self._keras_tensor_symbolic_call(\r\n  File \"/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 848, in _keras_tensor_symbolic_call\r\n    return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n  File \"/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 886, in _infer_output_signature\r\n    self._maybe_build(inputs)\r\n  File \"/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 2659, in _maybe_build\r\n    self.build(input_shapes)  # pylint:disable=not-callable\r\n  File \"/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/utils/tf_utils.py\", line 259, in wrapper\r\n    output_shape = fn(instance, input_shape)\r\n  File \"/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/layers/merge.py\", line 489, in build\r\n    raise ValueError('A `Concatenate` layer should be called '\r\nValueError: A `Concatenate` layer should be called on a list of at least 1 input.\r\n```", "comments": ["@stefanistrate ,\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\n", "Also, please take a look at this [link](https://fixexception.com/keras/a-concatenate-layer-should-be-called-on-a-list-of-at-least-1-input/) with the similar error.It helps.Thanks!", "Cross-posted at https://github.com/keras-team/keras/issues/15547.", "@stefanistrate ,\r\nPlease feel free to close this issue here,since it is already being tracked there? Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52626\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52626\">No</a>\n"]}, {"number": 52625, "title": "[TFTRT] Debug Feature - Converter summary added", "body": "This PR adds a capability to the converter to print a conversion summary, inspired by the `keras.Model.summary()` API\r\n\r\n@bixia1 @tfeher for review\r\n\r\nDepends on: #53327\r\n\r\n```python\r\nconverter.convert()\r\nconverter.summary(detailed=True)\r\n\r\nTRTEngineOP Name                              # Nodes         Input DType                    Output Dtype                   Input Shape                              Output Shape                             \r\n==============================================================================================================================================================================================================\r\nTRTEngineOp_0_0                               798             ['float32']                    ['float32']                    [[-1, 299, 299, 3]]                      [[-1, 1001]]                             \r\n\r\n        - AvgPool: 10x\r\n        - BiasAdd: 1x\r\n        - ConcatV2: 11x\r\n        - Const: 485x\r\n        - Conv2D: 95x\r\n        - FusedBatchNormV3: 94x\r\n        - Identity: 1x\r\n        - MaxPool: 4x\r\n        - Relu: 94x\r\n        - Squeeze: 1x\r\n        - Transpose: 2x\r\n\r\n==============================================================================================================================================================================================================\r\n[*] Total number of TensorRT engines: 1\r\n[*] % of OPs Converted: 99.75% [798/800]\r\n```", "comments": ["@bixia1 are you sure the use of F-Strings is accepted in TensorFlow codebase ? There are less than 10 examples in the whole TF codebase. Could you confirm that ?", "Yes, I can confirm that you can use f\"..\" format string in TF-TRT code.  We use it in python/compiler/tensorrt/model_tests/result_analyzer.py\r\nCan you share with me on how you search that? I couldn't figure out how to search on the github tree, but within the google tensorflow tree I  found numerous use cases, far more than what you said.", "Grepping the whole TF Codebase for the string `print(f\"` and `print(f'`\r\n\r\nTotal of 6 hits in 3 files\r\n\r\n![image](https://user-images.githubusercontent.com/10923599/138937200-f5c4bfab-544e-4e60-8064-06700859826b.png)\r\n\r\n![image](https://user-images.githubusercontent.com/10923599/138937250-e7910428-55bf-4f79-b2a0-f42ee6d623a9.png)\r\n\r\n----------\r\n\r\nEDIT: I found a lot more using just grep for `f'` and `f\"`, forget about it ;) I'm editing my PR", "@bixia1 all comments answered, please review answers and new changes", "I still see 8 comments that have no response. Would you please check?", "@bixia1 all comments fixed, I added the `print_fn` argument we talked about this morning, that way we perfectly match Keras API and allow people to provide a custom function that would do smthg with the string if they want instead of print\r\n\r\nPlease review", "> @bixia1 all comments fixed, I added the `print_fn` argument we talked about this morning, that way we perfectly match Keras API and allow people to provide a custom function that would do smthg with the string if they want instead of print\r\n> \r\n> Please review\r\n\r\nI don't understand what you see from your screen, but there are still many comments unaddressed. In particular, there are a few places where we shouldn't use nested functions and you seem to ignore the comments.\r\n\r\nI also add more review for your new change.", "I still see those comments mentioned [here](https://github.com/tensorflow/tensorflow/pull/52625#issuecomment-953318107) that don't have a response yet. Please let me know when the PR is ready for review again.", "@bixia1 I fixed the nested functions, but I really don't agree with this. They really don't make any sense outside of the scope of the function, moving them outside the scope is really making the code more difficult to read and understand.\r\n\r\nWe have a number of nested functions when the function have no chance to be re-used by any other part of the code:\r\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/compiler/tensorrt/trt_convert.py#L585-L587\r\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/compiler/tensorrt/trt_convert.py#L765-L808\r\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/compiler/tensorrt/trt_convert.py#L1258-L1284\r\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/compiler/tensorrt/trt_convert.py#L1301-L1302\r\n- etc.\r\n\r\nI really don't see the logic aside of making the code much more difficult to read.", "For nested functions, I assume you see these in the style guide that I provided:\r\nNested functions and classes cannot be directly tested. Nesting can make the outer function longer and less readable.\r\n Avoid nested functions or classes except when closing over a local value other than self or cls. Do not nest a function just to hide it from users of a module. Instead, prefix its name with an _ at the module level so that it can still be accessed by tests.\r\n\r\nWhy do you believe un-nesting the functions make the code more difficult to read?\r\n\r\n", "We are good. @bixia1 all comments have been addressed", "@bixia1 please re-review", "@DEKHTIARJonathan  Can you please check @bixia1's comments and keep us posted ? Thanks!", "@DEKHTIARJonathan Any update on this PR? Please. Thanks!", "Will come back to this PR after https://github.com/tensorflow/tensorflow/pull/53327", "Depends on #53327\r\n@bixia1 please review\r\n\r\n- RELEASE.md and the test golden file have been updated.\r\n- `print_row` has been renamed `_print_row` as requested", "@bixia1 can you reapprove ? For some reason I had to fix a PyLint issue on a line I didn't change ...\r\nThanks\r\n\r\nPyLint fix: https://github.com/tensorflow/tensorflow/pull/52625/files#diff-8e470ff4b5b82fda4fb3fe008b694323d63484f3cbf490d4e3ba104894f914e6R309-R313", "@bixia1 can you re-approve please ?", "@DEKHTIARJonathan can you please resolve conflicts ?", "@rthadur conflicts have been solved. Can you re-approve ? Thanks", "I haved started a manual process to fix issue in the PR and merge it last week. Still in the process of getting the needed approvals inside google. Please stop updating this PR unless I request for changes. "]}, {"number": 52624, "title": "Removed NV GPU NUMA check for ARM64", "body": "NUMA support message on ARM64 system\r\n\r\nWith this patch, on ARM64 system, tested 2 GPUs connected on different sockets.\r\nBoth are functional.\r\n\r\nHardware:\r\nDual socket ARM64 N1 system.\r\n2 GPUs connected on different sockets\r\n\r\nSoftware:\r\nUbuntu 20.04.3 LTS\r\nKernel : 5.4.0-89-generic (Linux devBox 5.4.0-89-generic #100-Ubuntu SMP Fri Sep 24 14:29:20 UTC 2021 aarch64 aarch64 aarch64 GNU/Linux)\r\nCUDA SDK : 11.4 (cuda_11.4.2_470.57.02_linux_sbsa.run)\r\ncuDNN : v8.2 (cudnn-11.4-linux-aarch64sbsa-v8.2.2.26.tgz)\r\nTensorRT: v8.0.1.6 (TensorRT-8.0.1.6.Ubuntu-20.04.aarch64-gnu.cuda-11.3.cudnn8.2.tar.gz)\r\n\r\nGPU: Tesla T4\r\nPCI slot info:\r\n\r\n$lspci | grep -i NVIDIA\r\n0005:01:00.0 3D controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)\r\n0009:03:00.0 3D controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)\r\n\r\nNUMA Info :\r\n$ cat /sys/devices/pci0005:00/0005:00:01.0/0005:01:00.0/numa_node\r\n0\r\n$ cat /sys/devices/pci0009:00/0009:00:05.0/0009:03:00.0/numa_node\r\n1\r\n\r\nTensorFlow GPU access info:\r\n```\r\nname: \"/device:GPU:0\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 15558377472\r\nlocality {\r\n  bus_id: 1\r\n  links {\r\n  }\r\n}\r\nincarnation: 11914144743508156396\r\nphysical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0005:01:00.0, compute capability: 7.5\"\r\nxla_global_id: 416903419\r\n\r\nname: \"/device:GPU:1\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 15558377472\r\nlocality {\r\n  bus_id: 2\r\n  numa_node: 1\r\n  links {\r\n  }\r\n}\r\nincarnation: 14202384483451516137\r\nphysical_device_desc: \"device: 1, name: Tesla T4, pci bus id: 0009:03:00.0, compute capability: 7.5\"\r\nxla_global_id: 2144165316\r\n```\r\n\r\nScript to get info:\r\n\r\n```\r\nfrom tensorflow.python.client import device_lib\r\n\r\nlocal_device = device_lib.list_local_devices()\r\n\r\nfor x in local_device:\r\n  if x.device_type == 'GPU':\r\n    print(x)\r\n\r\n```", "comments": []}]