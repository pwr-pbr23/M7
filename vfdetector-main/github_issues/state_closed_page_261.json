[{"number": 46603, "title": "Broken link in doc", "body": "**Link to Doc:**\r\n\r\nhttps://www.tensorflow.org/tutorials/audio/simple_audio\r\n\r\n\r\n![-2021-jan-22-008](https://user-images.githubusercontent.com/20347013/105484563-87ba8400-5cd1-11eb-8daf-370b3601ff0f.jpg)\r\n\r\n\r\n**Dead Link(Name of Link: contributing five minutes of your own voice):**\r\n\r\nhttps://aiyprojects.withgoogle.com/open_speech_recording\r\n\r\n\r\n![-2021-jan-22-010](https://user-images.githubusercontent.com/20347013/105484575-8b4e0b00-5cd1-11eb-91aa-283abea5e457.jpg)\r\n", "comments": ["AYI Projects is no longer accepting contributions."]}, {"number": 46601, "title": "Update broken GitHub policy link in issue templates", "body": "The GitHub policy page has been moved from \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/ISSUES.md\r\n\r\nto \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/opensource_only/ISSUES.md\r\n\r\nUpdated the links in the issue templates. Fixes [#46592](https://github.com/tensorflow/tensorflow/issues/46592).", "comments": ["Let's move it back instead.\r\n\r\nThat directory should not exist in OSS (we're removing it iternally)", "> Let's move it back instead.\r\n\r\n@mihaimaruseac,\r\nIn this case, do we just close this PR so that the change is never merged?  \r\n\r\nCould you please suggest the best course of action? Thanks!", "Yes, we should close this Pr and instead make sure the files are at the old location.", "@mihaimaruseac,\r\nThank you for the update. Closing this PR."]}, {"number": 46600, "title": "TensorflowLite 1.15.2  aar build failure in Win10", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution :**win10**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**no**\r\n- TensorFlow installed from (source or binary):**source**\r\n- TensorFlow version:**1.15.2**\r\n- Python version:**3.7.9**\r\n- Installed using virtualenv? pip? conda?: **no,using bazel and the default python in system**\r\n- Bazel version (if compiling from source):**0.26.1**\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: **no**\r\n- GPU model and memory:**no**\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI was trying to build the tensorflow-lite-1.15.2.aar from source on my local WIN10 PC,\r\nI downloaded the source code and installed bazel following instructions online.\r\nThe version of  ANDROID_NDK is 18.1.5063045  and ANDOIRD_SDK is 23\r\nAfter configuring the bazelrc, I executed the build command and got the following error:\r\n\r\n```\r\n[18 / 56] Processing Android resources for //tensorflow/lite/java:tensorflow-lite_dummy_app_for_so; 1s local ... (12 actions, 11 running)\r\nERROR: D:/code/tensorflow-1.15.2/tensorflow-1.15.2/tensorflow/lite/java/BUILD:22:1: Processing Android resources for //tensorflow/lite/java:tensorflow-lite_dummy_app_for_so failed (Exit 1): ResourceProcessorBusyBox.exe failed: error executing command\r\n  cd C:/users/c00/_bazel_c00/7oqu2vjd/execroot/org_tensorflow\r\n  SET ANDROID_BUILD_TOOLS_VERSION=30.0.0\r\n    SET ANDROID_NDK_API_LEVEL=18\r\n    SET ANDROID_NDK_HOME=D:/softwares/AndroidSDK/ndk/18.1.5063045\r\n    SET ANDROID_SDK_API_LEVEL=23\r\n    SET ANDROID_SDK_HOME=D:/softwares/AndroidSDK\r\n    SET PATH=C:\\Program Files\\Git\\usr\\bin;C:\\Program Files\\Git\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\Users\\c00\\bin;C:\\Program Files\\Git\\mingw64\\bin;C:\\Program Files\\Git\\usr\\local\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Program Files\\Git\\mingw64\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Users\\c00\\bin;%PyCharm%;%GoLand%;%IntelliJ IDEA%;C:\\WINDOWS\\system32\\config\\systemprofile\\go\\bin;C:\\Program Files\\Python37;C:\\Program Files\\Python37\\Scripts;D:\\softwares\\apache-maven-3.6.3\\bin;D:\\softwares\\AndroidSDK\\platform-tools\\adb.exe;C:\\Program Files (x86)\\GnuWin32\\bin;C:\\Windows\\System32;C:\\Program Files\\Git\\usr\\bin\\git.exe;C:\\Program Files\\Git LFS;C:\\Program Files\\Java\\jdk1.8.0_202\\bin;C:\\Program Files\\Java\\jdk1.8.0_202\\jre\\bin;D:\\softwares\\PyCharm 2020.1\\bin;D:\\softwares\\GoLand 2020.1.3\\bin;D:\\softwares\\IntelliJ IDEA 2020.2\\bin;C:\\Users\\c00\\go\\bin;C:\\Program Files\\Git\\usr\\bin\\vendor_perl;C:\\Program Files\\Git\\usr\\bin\\core_perl\r\n    SET PYTHON_BIN_PATH=C:/Program Files/Python37/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Program Files/Python37/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=0\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_ENABLE_XLA=1\r\n  bazel-out/x64_windows-opt/bin/external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/ResourceProcessorBusyBox.exe @bazel-out/x64_windows-opt/bin/tensorflow/lite/java/tensorflow-lite_dummy_app_for_so_symbols/R.txt-0.params\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n\u4e00\u6708 22, 2021 5:18:13 \u4e0b\u5348 com.google.devtools.build.android.ResourceProcessorBusyBox processRequest\r\n\u4e25\u91cd: Error during processing\r\njava.nio.file.InvalidPathException: Illegal character [:] in path at index 4: ///C:/Users/C00520~1/AppData/Local/Temp/android_resources_tmp5724987117968661034/linked/bin.-pb.apk\r\n        at sun.nio.fs.WindowsPathParser.nextSlash(WindowsPathParser.java:212)\r\n        at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:111)\r\n        at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77)\r\n        at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94)\r\n        at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255)\r\n        at java.nio.file.Paths.get(Paths.java:84)\r\n        at com.google.devtools.build.android.aapt2.ProtoApk.asApkPath(ProtoApk.java:203)\r\n        at com.google.devtools.build.android.aapt2.ResourceLinker.link(ResourceLinker.java:555)\r\n        at com.google.devtools.build.android.aapt2.ResourceLinker.link(ResourceLinker.java:536)\r\n        at com.google.devtools.build.android.Aapt2ResourcePackagingAction.main(Aapt2ResourcePackagingAction.java:185)\r\n        at com.google.devtools.build.android.ResourceProcessorBusyBox$Tool$14.call(ResourceProcessorBusyBox.java:144)\r\n        at com.google.devtools.build.android.ResourceProcessorBusyBox.processRequest(ResourceProcessorBusyBox.java:240)\r\n        at com.google.devtools.build.android.ResourceProcessorBusyBox.main(ResourceProcessorBusyBox.java:203)\r\n\r\nException in thread \"main\" java.nio.file.InvalidPathException: Illegal character [:] in path at index 4: ///C:/Users/C00520~1/AppData/Local/Temp/android_resources_tmp5724987117968661034/linked/bin.-pb.apk\r\n        at sun.nio.fs.WindowsPathParser.nextSlash(WindowsPathParser.java:212)\r\n        at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:111)\r\n        at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77)\r\n        at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94)\r\n        at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255)\r\n        at java.nio.file.Paths.get(Paths.java:84)\r\n        at com.google.devtools.build.android.aapt2.ProtoApk.asApkPath(ProtoApk.java:203)\r\n        at com.google.devtools.build.android.aapt2.ResourceLinker.link(ResourceLinker.java:555)\r\n        at com.google.devtools.build.android.aapt2.ResourceLinker.link(ResourceLinker.java:536)\r\n        at com.google.devtools.build.android.Aapt2ResourcePackagingAction.main(Aapt2ResourcePackagingAction.java:185)\r\n        at com.google.devtools.build.android.ResourceProcessorBusyBox$Tool$14.call(ResourceProcessorBusyBox.java:144)\r\n        at com.google.devtools.build.android.ResourceProcessorBusyBox.processRequest(ResourceProcessorBusyBox.java:240)\r\n        at com.google.devtools.build.android.ResourceProcessorBusyBox.main(ResourceProcessorBusyBox.java:203)\r\nTarget //tensorflow/lite/java:tensorflow-lite failed to build\r\nINFO: Elapsed time: 2.462s, Critical Path: 1.37s\r\nINFO: 14 processes: 14 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n_bazel build --cxxopt='--std=c++11' -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a //tensorflow/lite/java:tensorflow-lite\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nHere is the tf_configure.bazelrc content I got ,\r\n- I have JDK1.8 installed before ,so I added the \"java_toolchain\" setting\r\n- I used the ./configure to interactively configure it,but I couldn't get these \"action_env\" adout android, so I add those action_env manually.\r\n- I tried to remove some of the \"build:opt\" \uff0cit didn't work\uff0ccause I don't really understand all of them\r\n- I tried to change the SDK and NDK higher ,to like 29/30, it didn't solve the problem either.\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"C:/Program Files/Python37/python.exe\"\r\nbuild --action_env PYTHON_LIB_PATH=\"C:/Program Files/Python37/lib/site-packages\"\r\nbuild --python_path=\"C:/Program Files/Python37/python.exe\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --config=xla\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --config monolithic\r\nbuild --copt=-w --host_copt=-w\r\nbuild --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI\r\nbuild --verbose_failures\r\nbuild --distinct_host_configuration=false\r\nbuild --define=override_eigen_strong_inline=true\r\nbuild:v2 --define=tf_api_version=2\r\nbuild --java_toolchain=@bazel_tools//tools/jdk:toolchain_hostjdk8\r\nbuild --action_env ANDROID_NDK_HOME=\"D:/softwares/AndroidSDK/ndk/18.1.5063045\"\r\nbuild --action_env ANDROID_NDK_API_LEVEL=\"18\"\r\nbuild --action_env ANDROID_BUILD_TOOLS_VERSION=\"30.0.0\"\r\nbuild --action_env ANDROID_SDK_API_LEVEL=\"23\"\r\nbuild --action_env ANDROID_SDK_HOME=\"D:/softwares/AndroidSDK\"\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-no_windows,-gpu\r\ntest --build_tag_filters=-no_windows,-gpu\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n\r\nI am new to bazel and tensorflow , I feel there must be something wrong with my bazelrc, but I can't find similar problem online, so I'm trying to seek for help here. Thanks.", "comments": ["I ran the build command in git bash at first,now I tried cmder/cmd-bash, I still got the same error,\r\nAnd I got another win10 laptop ,configured the environment from the very first step of python installation....\r\nAnd I still got the same error message after I configured the same environment(Python3.7/ndk 18/android sdk 23/buildtool 30/bazel 0.26/jdk9)\r\nI really hope somebody can give me some advice ,I'm totally stuck here\r\n", "@thaink Sorry to ask,but may you offer me some pro advice if possible,\r\nI think it may be a very rookie mistake,but I just have no clue what to do with it now\r\nany help would be appreciated... thanks", "@nickkchenn In my experience, this error is related to bazel version. Maybe yours is too old.\r\nI recomend to build it using docker https://www.tensorflow.org/lite/guide/build_android#set_up_build_environment_using_docker\r\n\r\nAnd is there any reason that you are building 1.15.2 instead of our latest release?", "@thaink  Thank you for your reply,\r\n1. I'm building 1.15.2 for a old existing program which depends on this version of tflite, change of version may produce more unwanted workloads.\r\n2. My goal was to  put the building process on our own building-pipeline system, so I was trying to implement the process locally on my own machine first , now it seems not going to work though... \r\n3. I read in  the configure.py that   _TF_MAX_BAZEL_VERSION = '0.26.1' ,so I chosed 0.26.1 , is it because that the bazel version is too old for my system?", "Looking at the error message:\r\n\r\n```\r\njava.nio.file.InvalidPathException: Illegal character [:] in path at index 4: ///C:/Users/C00520~1/AppData/Local/Temp/android_resources_tmp5724987117968661034/linked/bin.-pb.apk\r\n```\r\nLook like bazel at that version is not supporting Windows' path really well. If you make no modification to the code it might be better to just use our prebuilt aar file or build it in Linux.\r\n\r\nAnyway, TFLite is designed to be backward-compatibility so I would still recommend to try our latest version as the first option.", "ok,I understand it. I'll try to build it in linux instead or try with latest version\r\nthank you very much", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46600\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46600\">No</a>\n"]}, {"number": 46599, "title": "Conversion img_to_array and array_to_img does not work in TF 2.4", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: 3.6.1\r\n- CUDA/cuDNN version: 11\r\n- GPU model and memory: RTX2060 6GB\r\n\r\nI try to convert numpy array ones to img and img to array with preprocessing.image functions.\r\n\r\nCode to repoduce issue:\r\n\r\n```\r\nimport numpy as np\r\nfrom tensorflow.compat.v1.keras.preprocessing.image import array_to_img, img_to_array\r\n\r\ninput_array_ones = np.ones((4, 8, 1), dtype=np.uint8)\r\nimg_ones = array_to_img(input_array_ones)\r\noutput_array_ones = img_to_array(img_ones, dtype=np.uint8)\r\n```\r\n\r\n```\r\n>>> (input_array_ones) = [[[1],  [1],  [1],  [1],  [1],  [1],  [1],  [1]],, [[1],  [1],  [1],  [1],  [1],  [1],  [1],  [1]],, [[1],  [1],  [1],  [1],  [1],  [1],  [1],  [1]],, [[1],  [1],  [1],  [1],  [1],  [1],  [1],  [1]]]\r\n>>> (img_ones) = <PIL.Image.Image image mode=L size=8x4 at 0x20990EBADA0>\r\n>>> (output_array_ones) = [[[0],  [0],  [0],  [0],  [0],  [0],  [0],  [0]],, [[0],  [0],  [0],  [0],  [0],  [0],  [0],  [0]],, [[0],  [0],  [0],  [0],  [0],  [0],  [0],  [0]],, [[0],  [0],  [0],  [0],  [0],  [0],  [0],  [0]]]\r\n```\r\n\r\nWorks fine with the previous version of TensorFlow (1.x).\r\n", "comments": ["To fix this add scale=False to array_to_img.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46599\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46599\">No</a>\n"]}, {"number": 46597, "title": "Loading model from GCS takes longer than copying it to local and then loading the model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): x86_64 GNU/Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n```\r\nnew_model = tf.keras.models.load_model(\r\n 'gs://benchmarking_test/bert_en_uncased_L-12_H-768_A-12_2' \r\n)\r\n```\r\ntakes about 30min or so.\r\nwhereas, following works in less than a minute\r\n```\r\n!gsutil cp -r gs://benchmarking_test/bert_en_uncased_L-12_H-768_A-12_2 local_path\r\nnew_model = tf.keras.models.load_model(\r\n 'local_path' \r\n)\r\n```\r\n\r\n**Describe the expected behavior**\r\nBoth should take a similar time.\r\n", "comments": ["@sumitbinnani \r\n\r\nCan you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.\r\nAlso, can you try with latest stable version TF 2.4 otr Nightly version and see if you are facing the same issue. There are lot of performance improvement in latest versions of TF.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46596, "title": "Fix build errors on OSX", "body": "Without explicitly specifying C++ 11 as a required minimal version, on OSX Catalina I was getting these errors:\r\n\r\n```\r\n\u279c  hello_tflite pwd\r\n/tmp/hello_tflite\r\n\u279c  hello_tflite cmake --build . -j\r\n[100%] Building CXX object tensorflow-lite/CMakeFiles/tensorflow-lite.dir/tools/optimize/sparsity/format_converter.cc.o\r\n[100%] Linking CXX static library libtensorflow-lite.a\r\n[100%] Built target tensorflow-lite\r\nScanning dependencies of target minimal\r\n[100%] Building CXX object CMakeFiles/minimal.dir/minimal.cc.o\r\nIn file included from /Users/tleyden/Development/tensorflow_src/tensorflow/lite/examples/minimal/minimal.cc:16:\r\nIn file included from /Users/tleyden/Development/tensorflow_src/tensorflow/lite/interpreter.h:34:\r\n/Users/tleyden/Development/tensorflow_src/tensorflow/lite/allocation.h:38:8: warning: scoped enumerations are a\r\n      C++11 extension [-Wc++11-extensions]\r\n  enum class Type {\r\n       ^\r\n/Users/tleyden/Development/tensorflow_src/tensorflow/lite/allocation.h:66:28: warning: 'override' keyword is a C++11\r\n      extension [-Wc++11-extensions]\r\n  const void* base() const override;\r\n                           ^\r\n/Users/tleyden/Development/tensorflow_src/tensorflow/lite/allocation.h:67:24: warning: 'override' keyword is a C++11\r\n      extension [-Wc++11-extensions]\r\n  size_t bytes() const override;\r\netc..\r\n```\r\n\r\nAdding the proposed change fixed the issues.", "comments": []}, {"number": 46594, "title": "How to get the walltime of tensorflow trace?", "body": "I'm using `tensorflow.keras.callbacks.TensorBoard` API to collect gpu kernels events. \r\n\r\nI need to know the Jiffies time or the walltime of Linux, but not the time relative to the training task as shown in the following picture. \r\n\r\nSo how could I get the system time?\r\n\r\n![9sjgS](https://user-images.githubusercontent.com/20624289/105435140-c3d0f300-5c97-11eb-8247-aff2a505e519.png)\r\n\r\nMy Code:\r\n```python\r\nimport os\r\nimport sys\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nprint(tf.__version__)\r\nprint(sys.version_info)\r\n\r\ngpus = tf.config.experimental.list_physical_devices(\"GPU\")\r\ntf.config.experimental.set_memory_growth(gpus[0], True)\r\ntf.config.experimental.set_visible_devices(gpus[0], \"GPU\")\r\n\r\nfashion_mnist = keras.datasets.fashion_mnist\r\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\r\nx_valid = x_train[:5000]\r\ny_valid = y_train[:5000]\r\nx_train = x_train[5000:]\r\ny_train = y_train[5000:]\r\n\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\nscaler = StandardScaler()\r\n\r\nx_train_scaled = scaler.fit_transform(x_train.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28, 1)\r\nx_valid_scaled = scaler.transform(x_valid.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28, 1)\r\nx_test_scaled = scaler.transform(x_test.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28, 1)\r\n\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='selu', input_shape=(28, 28, 1)))\r\nmodel.add(keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='selu'))\r\nmodel.add(keras.layers.MaxPool2D(pool_size=2))\r\nmodel.add(keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='selu'))\r\nmodel.add(keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='selu'))\r\nmodel.add(keras.layers.MaxPool2D(pool_size=2))\r\nmodel.add(keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='selu'))\r\nmodel.add(keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='selu'))\r\nmodel.add(keras.layers.MaxPool2D(pool_size=2))\r\n\r\nmodel.add(keras.layers.Flatten())\r\nmodel.add(keras.layers.Dense(128, activation='relu'))\r\nmodel.add(keras.layers.Dense(10, activation='softmax'))\r\n\r\nmodel.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\r\n              optimizer=keras.optimizers.SGD(learning_rate=1e-2), metrics=['accuracy'])\r\n\r\nlogdir = os.path.join('.', 'cnn-callbacks')\r\nif os.path.exists(logdir) == False:\r\n    os.mkdir(logdir)\r\noutput_model_file = os.path.join(logdir, \"fashion_mnist_model.h5\")\r\nprint(output_model_file)\r\ncallbacks = [\r\n    keras.callbacks.TensorBoard(logdir, profile_batch=tuple(range(3, 13))),\r\n    keras.callbacks.ModelCheckpoint(output_model_file, save_best_only=True),\r\n    keras.callbacks.EarlyStopping(patience=5, min_delta=1e-3),\r\n]\r\n\r\nhistory = model.fit(x_train_scaled, y_train, epochs=10, validation_data=(x_valid_scaled, y_valid), callbacks=callbacks,\r\n                    batch_size=1024)\r\n```", "comments": ["@1013801464\r\nI ran the code shared and face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/438a3a06313e8a32f57eafd5b586fd73/untitled505.ipynb).", "> @1013801464\r\n> I ran the code shared and face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/438a3a06313e8a32f57eafd5b586fd73/untitled505.ipynb).\r\n\r\nThis code should be ran on cuda device, and there is indeed an error. The first element of `callbacks` array should change to `keras.callbacks.TensorBoard(logdir, profile_batch=(3, 13)),`", "Is this still an issue? Did you try using [TensorBoard Profiler](https://www.tensorflow.org/guide/profiler#profiler_tools) ?\r\n", "Yes, it is still an issue. I have read the _TensorBoard Profiler_ before, but that page is not helpful. Maybe it is an issue of CUPTI but not TensorFlow. I have already given up."]}, {"number": 46593, "title": "TFLite from file descriptor on Android 11", "body": "**System information**\r\n- TensorFlow version (you are using): Likely https://github.com/mozilla/tensorflow/tree/23ad988fcde60fb01f9533e95004bbc4877a9143\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThis issue is a continuation of mozilla/DeepSpeech#3507. TFLite is used in the app, but the model is provided by the user and usually in `/sdcard/Download`.\r\n\r\nCurrently, TFLite accepts arrays or file paths. With Android's scoped storage and SAF, it's hard to use file paths to the external storage without either compatibility edge cases or overly invasive permissions. On Android 10, I could use the workaround of converting the file descriptor I received from Android to a path like `\"/proc/self/fd/\" + fd`. On Android 11, that stopped working. It seems to be no longer possible to open a file under `/proc/self/fd/` that points to external storage without requesting the read permission I was hoping to avoid.\r\n\r\nI would like TFLite to be compatible with Android 11's external storage. The following solutions come to mind:\r\n\r\n- Duplicate `tflite::FlatBufferModel::BuildFromFile` into `tflite::FlatBufferModel::BuildFromFileDescriptor`\r\n- Add https://stackoverflow.com/a/59004193/10477326\r\n- On Android builds, manually add a conditional to detect the `/proc/self/fd/` prefix and use a `dup` instead of a `fopen`\r\n\r\n**Will this change the current api? How?**\r\n\r\nNot unless we choose the `BuildFromFileDescriptor` solution.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nDevelopers and users of apps that:\r\n\r\n- Explore, educate, or demonstrate machine learning, and\r\n- Run on up-to-date Android versions, and\r\n- Load user-supplied models, and\r\n- Usually take large (~1 GiB) models that reside on external storage, which we cannot afford to read into memory or duplicate on disk, and\r\n- Would like to respect privacy by obeying scoped storage and SAF by not requesting file permissions to everything but rather allow the user to pick only the files necessary\r\n", "comments": ["@jdduke is better to justify if the benefit is big enough to support this feature.\r\n\r\nBut if it is blocking you, there are definitely some workaround:\r\n- If the path can be fix, you can try to put the model to /data/local/tmp\r\n- If you select the model using UI, you can copy the model to the app's internal storage and get it path", "> justify if the benefit is big enough to support this feature\r\n\r\nIf you don't want to increase the API surface, we can just go with the solution with the tiny check to make `/proc/self/fd` use `dup`.\r\n\r\n> if it is blocking you\r\n\r\nIt was never blocking me. It was just that whatever workaround I applied would be inconvenient to the end-user.\r\n\r\n> /data/local/tmp\r\n\r\nThat path requires ADB to access. However, I am trying to remove the requirement for ADB.\r\n\r\n> If you select the model using UI\r\n\r\nYes, that I what I was doing on Android 10, and plan to continue doing\r\n\r\n> can copy the model to the app's internal storage\r\n\r\nDuplicating a gigabyte of data isn't nice to users whose 16 GB phones are already running out of storage", "As I see, this will be mainly used for experimental and users are mostly engineers. Above concerns might not really problematic in those cases.", "An API addition seems like a reasonable request, though I'd also be OK with the temporary workaround in the provided StackOverflow link (though that might be somewhat annoying to test?). Do you have a strong preference? If folks had already been using the /proc/self/fd/ workaround, I could see some value in avoiding the breakage, though I suspect that approach will always be somewhat brittle, and better to just make FD usage a first-class part of the API.", "> As I see [...]\r\n\r\nAlright. I think I'm going to go for the approach where I copy to internal storage. A fix in TFLite will take a month or two to propagate through my dependencies and reach me anyway.\r\n\r\nI can't think of any myself, but other users might have different use cases for this. A lot of stuff on Linux is moving towards file descriptors, and file descriptors are more general than file paths. Adding this feature would let users enjoy the generality.\r\n\r\n> though that might be somewhat annoying to test?\r\n\r\nGetting a file descriptor is the easy part. In `bash`, we can just `exec 3<>/path/to/file`. In C, we can similarly open the file without passing `O_CLOEXEC`.\r\n\r\nThe hard part is reproducing what Android does with `/proc/self/fd`. I have no clue how Android does that (especially since it wasn't the case on Android 10), and I couldn't simulate something on my desktop Linux box. I would like to find out.\r\n\r\n>  /proc/self/fd/ workaround, I could see some value in avoiding the breakage\r\n\r\nThis would indeed avoid having to request changes in the libraries that wrap TFLite. However, ...\r\n\r\n> suspect that approach will always be somewhat brittle\r\n\r\nThis is a problem. It feels a bit too magic. I only suggested the `/proc/self/fd` approach in case we didn't want to increase the API surface, but it seems like this is no longer the case.\r\n", "Thanks for the reply. I'm happy to review a pull request for the proposed API addition (we might need to also include an optional offset/length for completeness?), otherwise I can try to get something in within the next week or so. ", "Another question @danielzgtg, if we added some helper Java APIs for feeding in the model via a `FileDescriptor`  (e.g., from a `ContentResolver`, which allows mmap'ing), would that also be useful? I noticed your example is using the Java API, and I think we can find a cleaner way of doing this from Java as well.", "> happy to review\r\n\r\nI am quite busy and I work more in Java than C++, so you might get to this sooner.\r\n\r\n> would that also be useful\r\n\r\nThat would be helpful if the implementation would work. However, I see `FileDescriptor` (as opposed to `ParcelFileDescriptor`) doesn't have a public method to get the integer. Neither is the particular native API said to be stable. This would increase the coupling with either Android or Oracle's implementation. I'm not sure having this increased coupling and specific dependency is worth it.\r\n\r\n> noticed your example is using the Java API\r\n\r\nNo, I myself am not using the Java bindings, even though my code is in Java/Kotlin. I am indirectly using TFLite through the DeepSpeech project, whose Android app I'm working on. https://github.com/mozilla/DeepSpeech/blob/f27908e7e3781b4ebed228a27439d9988b13a5c7/native_client/java/libdeepspeech/src/main/java/org/deepspeech/libdeepspeech/DeepSpeechModel.java#L33 https://github.com/mozilla/DeepSpeech/blob/f27908e7e3781b4ebed228a27439d9988b13a5c7/native_client/deepspeech.cc#L266\r\n\r\nHowever, I think the majority of people are going to use the Java API. It would indeed be nice if there was a vendor-independent improvement to TFLite's Java API."]}, {"number": 46592, "title": "ISSUES.md link broken", "body": "## URL(s) with the issue:\r\n\r\nMost files under https://github.com/tensorflow/tensorflow/tree/f2d8cfe09234329e14c98798540f146bd94558e0/.github/ISSUE_TEMPLATE\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe links to the [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md) are broken.\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\nI don't know why there is a separate GitHub policy aside from the CONTRIBUTING.md, but it seems important and people are supposed to read it\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? No\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? N/A\r\n\r\n### Returns defined\r\n\r\nAre return values defined? N/A\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? N/A\r\n\r\n### Usage example\r\n\r\nIs there a usage example? N/A\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content? N/A\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? No\r\n", "comments": ["@danielzgtg,\r\nThank you for reporting the issue. The change has been requested in [#46601](https://github.com/tensorflow/tensorflow/pull/46601) and the links will be updated upon merging. ", "This bug is still in the repo. #46601 was closed without merging. It mentioned something about moving the files instead, but there is no newly linked PR or commit", "@danielzgtg,\r\nAs mentioned in PR #46601, the directory where the GitHub policy page currently exists is being removed internally.\r\n\r\nOnce it is reverted, the broken links should work as intended. Thanks!", "This should be done, afaik. If not, please list the broken link and the URL of the page where the link is broken."]}, {"number": 46590, "title": "Fix the source specialization for the examples (and minor fixes to the documentation).", "body": "With https://github.com/tensorflow/tensorflow/pull/46473, we removed support for TAGS on the makefile command line. An unintended consequence was that we were no longer specializing the sources in the examples.\r\n\r\nThis change specializes the sources using the TARGET, which appears to be the only command line option that is needed.\r\n\r\nManually confirmed that the generated arduino projects have the correct sources (e.g. micro_speech/arduino/audio_provider.cc is used in the output directory).\r\n\r\nTest sequence:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=arduino OPTIMIZED_KERNEL_DIR=cmsis_nn generate_arduino_zip\r\ncd tensorflow/lite/micro/tools/make/gen/arduino_x86_64_default\r\nunzip prj/tensorflow_lite.zip\r\n```\r\n\r\nAnd then confirmed that the code in\r\n```\r\ntensorflow/lite/micro/tools/make/gen/arduino_x86_64_default/tensorflow_lite/examples/micro_speech/arduino_audio_provider.cc matches\r\n```\r\n\r\nmatches the code in:\r\n```\r\ntensorflow/lite/micro/examples/micro_speech/arduino/audio_provider.cc\r\n```\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46589, "title": "Mixed Precision - Conv3D - error: No algorithm worked!", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes, I wrote a custom code.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux (CentOS 7)\r\n- TensorFlow installed from (source or binary):\r\nInstalled using PIP\r\n- TensorFlow version (use command below):\r\nv2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version:\r\n3.8.0\r\n- CUDA/cuDNN version:\r\nCUDA/11.1.1\r\ncuDNN/8.0.4.30-CUDA-11.1.1\r\n- GPU model and memory:\r\n2 NVIDIA A100-PCIE-40GB\r\n2 GPUs on the same computer\r\n\r\n**Describe the current behavior**\r\nWhen running the code with double precision, the code does not generate errors. When running with mixed precision, launches an error:\r\n\r\n**Describe the expected behavior**\r\nNot having errors when running with mixed precision.\r\n\r\n**Standalone code to reproduce the issue**\r\nI tried to create a google colab for this code. There is no error there, but I could not get a google colab with 2 GPUs, like in my enviroment.\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\r\nfrom tensorflow.keras import layers, Model, optimizers\r\nimport numpy as np\r\n\r\n# If you comment the following 2 lines, the code runs\r\npolicy = mixed_precision.Policy('mixed_float16')\r\nmixed_precision.set_policy(policy)\r\n\r\n\r\nmirrored_strategy = tf.distribute.MirroredStrategy()\r\nwith mirrored_strategy.scope():\r\n    x_input = layers.Input(shape=(60, 60, 2, 32))\r\n\r\n    x = layers.Conv3D(filters=32, kernel_size=(3, 3, 1), strides=(1, 1, 1), padding = 'same')(x_input)\r\n    x = layers.BatchNormalization()(x)\r\n    x_out = layers.ReLU()(x)\r\n\r\n    test_model = Model(inputs=x_input, outputs=x_out, name='test_model')\r\n    adam = optimizers.Adam(learning_rate=1E-4)\r\n\r\ntest_model.compile(optimizer=adam, loss=\"mse\", metrics=[\"mae\"])\r\n\r\ninput_data = np.random.random((1000, 60, 60, 2, 32))\r\ntarget_data = np.random.random((1000, 60, 60, 2, 32))\r\n\r\nds_tuple = tf.data.Dataset.from_tensor_slices((input_data,target_data))\r\nds_tuple = ds_tuple.shuffle(1000).batch(100)\r\n\r\nhistory = test_model.fit(ds_tuple, epochs=10, verbose=1)\r\n```\r\n\r\n**Other info / logs** \r\nThis is the log of the run when I try running with mixed precision:\r\n```\r\n2021-01-21 15:37:22.318957: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-01-21 15:37:36.119468: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-21 15:37:36.123358: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-01-21 15:37:36.201391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:3b:00.0 name: A100-PCIE-40GB computeCapability: 8.0\r\ncoreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\r\n2021-01-21 15:37:36.204229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:\r\npciBusID: 0000:d8:00.0 name: A100-PCIE-40GB computeCapability: 8.0\r\ncoreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\r\n2021-01-21 15:37:36.204423: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-01-21 15:37:37.945043: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-01-21 15:37:37.945235: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-01-21 15:37:37.998423: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-01-21 15:37:38.072705: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-01-21 15:37:38.207114: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-01-21 15:37:38.574293: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-01-21 15:37:38.612582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-01-21 15:37:38.624584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\r\n2021-01-21 15:37:38.624772: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-21 15:37:38.626778: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nWARNING:tensorflow:From /scratch/user/eee/conda/myEnvs/tf_2.4/lib/python3.8/site-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\r\n2021-01-21 15:37:38.638220: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-01-21 15:37:38.638847: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-21 15:37:38.840057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:3b:00.0 name: A100-PCIE-40GB computeCapability: 8.0\r\ncoreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\r\n2021-01-21 15:37:38.842228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:\r\npciBusID: 0000:d8:00.0 name: A100-PCIE-40GB computeCapability: 8.0\r\ncoreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\r\n2021-01-21 15:37:38.842375: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-01-21 15:37:38.842436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-01-21 15:37:38.842486: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-01-21 15:37:38.842536: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-01-21 15:37:38.842585: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-01-21 15:37:38.842635: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-01-21 15:37:38.842683: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-01-21 15:37:38.842733: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-01-21 15:37:38.850268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\r\n2021-01-21 15:37:38.850360: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-01-21 15:37:40.396681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-21 15:37:40.396829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1\r\n2021-01-21 15:37:40.396881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y\r\n2021-01-21 15:37:40.396925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N\r\n2021-01-21 15:37:40.407031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 37570 MB memory) -> physical GPU (device: 0, name: A100-PCIE-40GB, pci bus id: 0000:3b:00.0, compute capability: 8.0)\r\n2021-01-21 15:37:40.414936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 37570 MB memory) -> physical GPU (device: 1, name: A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0)\r\n     --->     Running Tensorflow in Mixed Precision...\r\nWARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\r\n  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\r\n\r\n2021-01-21 15:37:47.583748: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\r\nop: \"TensorSliceDataset\"\r\ninput: \"Placeholder/_0\"\r\ninput: \"Placeholder/_1\"\r\nattr {\r\n  key: \"Toutput_types\"\r\n  value {\r\n    list {\r\n      type: DT_DOUBLE\r\n      type: DT_DOUBLE\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: 60\r\n        }\r\n        dim {\r\n          size: 60\r\n        }\r\n        dim {\r\n          size: 2\r\n        }\r\n        dim {\r\n          size: 32\r\n        }\r\n      }\r\n      shape {\r\n        dim {\r\n          size: 60\r\n        }\r\n        dim {\r\n          size: 60\r\n        }\r\n        dim {\r\n          size: 2\r\n        }\r\n        dim {\r\n          size: 32\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n2021-01-21 15:37:48.602974: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-01-21 15:37:48.606716: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3000000000 Hz\r\nEpoch 1/10\r\n2021-01-21 15:37:56.331798: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-01-21 15:39:18.436971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-01-21 15:39:28.779314: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-01-21 15:39:31.933180: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_grad_ops_3d.cc:1994 : Not found: No algorithm worked!\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 31, in <module>\r\n    history = test_model.fit(ds_tuple, epochs=10, verbose=1)\r\n  File \"/scratch/user/eee/conda/myEnvs/tf_2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"/scratch/user/eee/conda/myEnvs/tf_2.4/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/scratch/user/eee/conda/myEnvs/tf_2.4/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 888, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/scratch/user/eee/conda/myEnvs/tf_2.4/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2942, in __call__\r\n    return graph_function._call_flat(\r\n  File \"/scratch/user/eee/conda/myEnvs/tf_2.4/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1918, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/scratch/user/eee/conda/myEnvs/tf_2.4/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 555, in call\r\n    outputs = execute.execute(\r\n  File \"/scratch/user/eee/conda/myEnvs/tf_2.4/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.NotFoundError: 3 root error(s) found.\r\n  (0) Not found:  No algorithm worked!\r\n         [[node gradient_tape/test_model/conv3d/Conv3D/Conv3DBackpropFilterV2 (defined at /threading.py:932) ]]\r\n  (1) Not found:  No algorithm worked!\r\n         [[node gradient_tape/test_model/conv3d/Conv3D/Conv3DBackpropFilterV2 (defined at /threading.py:932) ]]\r\n         [[cond_4/then/_40/cond_4/cond/pivot_t/_256/_187]]\r\n  (2) Not found:  No algorithm worked!\r\n         [[node gradient_tape/test_model/conv3d/Conv3D/Conv3DBackpropFilterV2 (defined at /threading.py:932) ]]\r\n         [[div_no_nan/ReadVariableOp_3/_108]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_2870]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function\r\n\r\n```\r\n", "comments": ["Can you please try with cuda 11.0 and TF 2.4 final version?", "Met the same problem, have you solved it?", "> Can you please try with cuda 11.0 and TF 2.4 final version?\r\n\r\n@ymodak , I installed TF 2.4.1 from PIP and using CUDA/11.0.2 and cuDNN/8.0.4.30, and got the same error with Mixed Precision.", "Same error, any solution??\r\n", "Same error.", "Upgrading to cuDNN 8.0.5 fixes this. 8.0.5 is compatible with TF 2.4 and tf-nightly, so please upgrade.\r\n\r\nI can reproduce with cuDNN 8.0.3 and 8.0.4, but not with cuDNN 8.0.2 and 8.0.5, so this has been fixed by 8.0.5. If it's feasible to check the cuDNN version at runtime, I'll improve the error message to state 8.0.5 should be used. @nluehr do you know if there are any workarounds for 8.0.3 and 8.0.4?\r\n\r\n@sanjoy, can we update the [GPU support](https://www.tensorflow.org/install/gpu) guide to recommend 8.0.5? Currently it recommends 8.0.4. \r\n\r\nFor reference, with 8.0.5, the only `cudnnConvolutionBwdFilterAlgo_t` which works is `CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1`. This fails on 8.0.3 and 8.0.4 though.", "> Upgrading to cuDNN 8.0.5 fixes this. 8.0.5 is compatible with TF 2.4 and tf-nightly, so please upgrade.\r\n> \r\n> I can reproduce with cuDNN 8.0.3 and 8.0.4, but not with cuDNN 8.0.2 and 8.0.5, so this has been fixed by 8.0.5. If it's feasible to check the cuDNN version at runtime, I'll improve the error message to state 8.0.5 should be used. @nluehr do you know if there are any workarounds for 8.0.3 and 8.0.4?\r\n> \r\n> @sanjoy, can we update the [GPU support](https://www.tensorflow.org/install/gpu) guide to recommend 8.0.5? Currently it recommends 8.0.4.\r\n> \r\n> For reference, with 8.0.5, the only `cudnnConvolutionBwdFilterAlgo_t` which works is `CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1`. This fails on 8.0.3 and 8.0.4 though.\r\n*********\r\nReinstall cuDNN 8.0.5 worked for me, thanks!\r\n", "Same error. Solved by installing cuDNN 8.0.5.\r\n``` bash\r\nsudo apt-get install --no-install-recommends \\\r\n    cuda-11-0 \\\r\n    libcudnn8=8.0.5.39-1+cuda11.0  \\\r\n    libcudnn8-dev=8.0.5.39-1+cuda11.0\r\n```\r\nThis [GPU installation guide](https://www.tensorflow.org/install/gpu) still requirers some improvements.", "After updating to cuDNN 8.0.5 the problem was solved. Thanks, @reedwm.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46589\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46589\">No</a>\n"]}, {"number": 46588, "title": "Handle `set_weights()` for dtype string", "body": "For further details see issue: https://github.com/tensorflow/tensorflow/issues/46553", "comments": ["@mihaimaruseac Should I change anything due to the failed check: `import/copybara`?", "@MansBermellLeo no, copybara is the service that imports the PR and then merges once the internal change is approved and lands.", "Was it reverted?", "No, it wasn't reverted. Just needs tests added in a separate PR."]}, {"number": 46587, "title": "error: cannot initialize a member subobject of type 'ternaryfunc' (aka '_object *(*)(_object *, _object *, _object *)') with an lvalue of type 'PyObject *(PyObject *)' (aka '_object *(_object *)'): different number of parameters (3 vs 1)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- OS X 10.15.7 - Catalina\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- source\r\n- TensorFlow version:\r\n- Master\r\n- Python version:\r\n- Anaconda 3.6\r\n- Installed using virtualenv? pip? conda?:\r\n- Conda\r\n- Bazel version (if compiling from source):\r\n- bazelisk\r\n- GCC/Compiler version (if compiling from source):\r\n- Apple clang version 12.0.0 (clang-1200.0.32.28)\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n- CPU\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nERROR: /Users/davidlaxer/tensorflow/tensorflow/python/lib/core/BUILD:49:11: C++ compilation of rule '//tensorflow/python/lib/core:bfloat16_lib' failed (Exit 1): wrapped_clang failed: error executing command external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 63 argument(s) skipped)\r\ntensorflow/python/lib/core/bfloat16.cc:219:5: error: cannot initialize a member subobject of type 'ternaryfunc' (aka '_object *(*)(_object *, _object *, _object *)') with an lvalue of type 'PyObject *(PyObject *)' (aka '_object *(_object *)'): different number of parameters (3 vs 1)\r\n    PyBfloat16_Negative,  // nb_negative\r\n    ^~~~~~~~~~~~~~~~~~~\r\ntensorflow/python/lib/core/bfloat16.cc:229:5: error: cannot initialize a member subobject of type 'binaryfunc' (aka '_object *(*)(_object *, _object *)') with an lvalue of type 'PyObject *(PyObject *)' (aka '_object *(_object *)'): different number of parameters (2 vs 1)\r\n    PyBfloat16_Int,       // nb_int\r\n    ^~~~~~~~~~~~~~\r\ntensorflow/python/lib/core/bfloat16.cc:333:1: error: unknown type name 'Py_hash_t'; did you mean 'npy_hash_t'?\r\nPy_hash_t PyBfloat16_Hash(PyObject* self) {\r\n^~~~~~~~~\r\nnpy_hash_t\r\nbazel-out/darwin-opt/bin/external/local_config_python/numpy_include/numpy/npy_common.h:357:14: note: 'npy_hash_t' declared here\r\ntypedef long npy_hash_t;\r\n             ^\r\n3 errors generated.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build --config=opt  //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@dbl001,\r\nPlease make sure you have followed all the instructions as per the [build and install guide](https://www.tensorflow.org/install/source#macos_1). \r\n\r\nAlso, check if you have all the required dependencies listed in the [tested build configurations](https://www.tensorflow.org/install/source#cpu_2). \r\n\r\nVersion | Python version | Compiler | Build tools\r\n-- | -- | -- | --\r\ntensorflow-2.4.0 | 3.6-3.8 | Clang from xcode 10.3 | Bazel 3.1.0\r\ntensorflow-2.3.0 | 3.5-3.8 | Clang from xcode 10.1 | Bazel 3.1.0\r\ntensorflow-2.2.0 | 3.5-3.8 | Clang from xcode 10.1 | Bazel 2.0.0\r\n\r\nThanks! ", "1. I am using basilisk, which should chose the appropriate version of bazel.\n2. Xcode 12.3 (12c33)\n3. $ clang --version\nApple clang version 12.0.0 (clang-1200.0.32.28)\nTarget: x86_64-apple-darwin19.6.0\nThread model: posix\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\n\nSo, is it likely that 'clang 12.0.0\u2019  has unresolved issues and is not supported?\n\n> On Jan 22, 2021, at 2:43 AM, Abhilash Mahendrakar <notifications@github.com> wrote:\n> \n> \n> @dbl001 <https://github.com/dbl001>,\n> Please make sure you have followed all the instructions as per the build and install guide <https://www.tensorflow.org/install/source#macos_1>.\n> \n> Also, check if you have all the required dependencies listed in the tested build configurations <https://www.tensorflow.org/install/source#cpu_2>.\n> \n> Version\tPython version\tCompiler\tBuild tools\n> tensorflow-2.4.0\t3.6-3.8\tClang from xcode 10.3\tBazel 3.1.0\n> tensorflow-2.3.0\t3.5-3.8\tClang from xcode 10.1\tBazel 3.1.0\n> tensorflow-2.2.0\t3.5-3.8\tClang from xcode 10.1\tBazel 2.0.0\n> Thanks!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/46587#issuecomment-765315230>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAXWFW4GJONS6P65XLFYQQDS3FJDXANCNFSM4WNMSBDA>.\n> \n\n", "ERROR: /Users/davidlaxer/tensorflow/tensorflow/python/lib/core/BUILD:49:11: C++ compilation of rule '//tensorflow/python/lib/core:bfloat16_lib' failed (Exit 1): wrapped_clang failed: error executing command external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 63 argument(s) skipped)\ntensorflow/python/lib/core/bfloat16.cc:219:5: error: cannot initialize a member subobject of type 'ternaryfunc' (aka '_object *(*)(_object *, _object *, _object *)') with an lvalue of type 'PyObject *(PyObject *)' (aka '_object *(_object *)'): different number of parameters (3 vs 1)\n    PyBfloat16_Negative,  // nb_negative\n    ^~~~~~~~~~~~~~~~~~~\ntensorflow/python/lib/core/bfloat16.cc:229:5: error: cannot initialize a member subobject of type 'binaryfunc' (aka '_object *(*)(_object *, _object *)') with an lvalue of type 'PyObject *(PyObject *)' (aka '_object *(_object *)'): different number of parameters (2 vs 1)\n    PyBfloat16_Int,       // nb_int\n    ^~~~~~~~~~~~~~\ntensorflow/python/lib/core/bfloat16.cc:333:1: error: unknown type name 'Py_hash_t'; did you mean 'npy_hash_t'?\nPy_hash_t PyBfloat16_Hash(PyObject* self) {\n^~~~~~~~~\nnpy_hash_t\nbazel-out/darwin-opt/bin/external/local_config_python/numpy_include/numpy/npy_common.h:357:14: note: 'npy_hash_t' declared here\ntypedef long npy_hash_t;\n             ^\n3 errors generated.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 50465.258s, Critical Path: 822.13s\nINFO: 5640 processes: 9 internal, 5631 local.\nFAILED: Build did NOT complete successfully\n(ai) MacBook-Pro:tensorflow davidlaxer$ \n(ai) MacBook-Pro:tensorflow davidlaxer$ clang --version\nApple LLVM version 10.0.1 (clang-1001.0.46.4)\nTarget: x86_64-apple-darwin19.6.0\nThread model: posix\nInstalledDir: /Applications/Xcode 10.3.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\n(ai) MacBook-Pro:tensorflow davidlaxer$ bazel --version\nbazel 3.7.2\n(ai) MacBook-Pro:tensorflow davidlaxer$ xcode-select -p\n/Applications/Xcode 10.3.app/Contents/Developer\n(ai) MacBook-Pro:tensorflow davidlaxer$ \n\n\n> On Jan 22, 2021, at 4:03 AM, David Laxer <davidl@softintel.com> wrote:\n> \n> 1. I am using basilisk, which should chose the appropriate version of bazel.\n> 2. Xcode 12.3 (12c33)\n> 3. $ clang --version\n> Apple clang version 12.0.0 (clang-1200.0.32.28)\n> Target: x86_64-apple-darwin19.6.0\n> Thread model: posix\n> InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\n> \n> So, is it likely that 'clang 12.0.0\u2019  has unresolved issues and is not supported?\n> \n>> On Jan 22, 2021, at 2:43 AM, Abhilash Mahendrakar <notifications@github.com <mailto:notifications@github.com>> wrote:\n>> \n>> \n>> @dbl001 <https://github.com/dbl001>,\n>> Please make sure you have followed all the instructions as per the build and install guide <https://www.tensorflow.org/install/source#macos_1>.\n>> \n>> Also, check if you have all the required dependencies listed in the tested build configurations <https://www.tensorflow.org/install/source#cpu_2>.\n>> \n>> Version\tPython version\tCompiler\tBuild tools\n>> tensorflow-2.4.0\t3.6-3.8\tClang from xcode 10.3\tBazel 3.1.0\n>> tensorflow-2.3.0\t3.5-3.8\tClang from xcode 10.1\tBazel 3.1.0\n>> tensorflow-2.2.0\t3.5-3.8\tClang from xcode 10.1\tBazel 2.0.0\n>> Thanks!\n>> \n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/46587#issuecomment-765315230>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAXWFW4GJONS6P65XLFYQQDS3FJDXANCNFSM4WNMSBDA>.\n>> \n> \n\n", "Tensorflow 2.4.1 built successfully with Xcode 10.1 NOT 10.3.\r\n\r\nIn [1]: import tensorflow\r\nIn [2]: print(tensorflow.__version__)\r\n2.4.1\r\n$ xcode-select -p\r\n/Applications/Xcode 10.1.app/Contents/Developer\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46587\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46587\">No</a>\n"]}, {"number": 46586, "title": "What is mac osx tensorflow=2.0.0 equivalent to in windows 64bit?", "body": "I have a mac book pro and I've been using tensorflow 2.0.0, but my lab computer is a windows (likely 64bit). I want to make sure that this lab computer has the equivalent package as my mac. Can anyone tell me which tensorflow version for windows is equivalent to mac osx 2.0.0?", "comments": ["If I understand correctly, the versions should be the same regardless of which operating system you use.", "TF is tested and supports various major 64-bit operating systems.\r\nSee https://www.tensorflow.org/install\r\nSo you can install TF on windows in the same manner as you did for macOS using `pip`.\r\n`pip install tensorflow==2.0.0`\r\nOn a side note you can also setup gpu support on your windows if have following [system configuration](https://www.tensorflow.org/install/gpu#hardware_requirements)."]}, {"number": 46585, "title": "Added tf_doctest_lib.py to COMMON_PIP_DEPS", "body": "Fixes #41279\r\n\r\n@mihaimaruseac could you kindly review this?", "comments": []}, {"number": 46584, "title": "Rename directories to have underscores instead of dashes (cmsis-nn -> cmsis_nn and ethos-u -> ethos_u) ", "body": "\r\nFixes http://b/168824958\r\n\r\nSome additional background discussion is in https://github.com/tensorflow/tensorflow/pull/46352 (which has the same PR title).", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46583, "title": "Jupyter kernel restarts/crashes when importing Tensorflow, 'ModuleNotFoundError: No module named 'tensorflow' in Python CLI", "body": "**System information**\r\nUsing an M1 MacBook\r\n\r\nI cannot use environment capture because the package is not being detected. I have tried the following: restarting computer, reinstalling Conda, reinstalling individual packages, installing using different package managers, setting up new environments, using different versions of python in different environments. It was working before and then suddenly stopped. I have seen reports of it being a memory issue, but it never occurred before, and I have 16GB of RAM, it should be more than enough. \r\n\r\n**Describe the current behavior**\r\n\r\nJupyter kernel restarts/crashes when importing Tensorflow, 'ModuleNotFoundError: No module named 'tensorflow' in Python CLI\r\n\r\n**Describe the expected behavior**\r\n\r\nFor it to import normally\r\n\r\nBelow is a video of the behaviour. \r\n\r\nhttps://drive.google.com/file/d/11hCwbneKVdx0srZN9YxCyyIwDE8BKKQN/view?usp=sharing\r\n\r\n<img width=\"724\" alt=\"Screen Shot 2021-01-21 at 10 02 34 AM\" src=\"https://user-images.githubusercontent.com/51058259/105384701-c9630600-5bcf-11eb-9f2c-53cb693ddf88.png\">\r\n<img width=\"723\" alt=\"Screen Shot 2021-01-21 at 10 02 57 AM\" src=\"https://user-images.githubusercontent.com/51058259/105384741-d7b12200-5bcf-11eb-850a-9a47cfb3c8ea.png\">\r\n", "comments": ["@oresttokovenko \r\n\r\nDid you install Tensorflow before importing by (`pip install tensorflow`). Also refer this [SO link](https://stackoverflow.com/questions/46568913/tensorflow-import-error-no-module-named-tensorflow) and see if it helps you.\r\nAlso, check whether your cpu supports AVX instructions.Please, see hardware requirements from [here](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements).Thanks!", "@ravikyram I solved the problem by going at it through a different route. I installed miniforge3 instead of Miniconda and used the native apple silicon Tensorflow. I'm still not sure what caused regular Tensorflow to break but my new one works now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46583\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46583\">No</a>\n"]}, {"number": 46582, "title": "Hi, I actually received this issue when I am running a code which is : WARNING:tensorflow:AutoGraph could not transform <function read_image at 0x0000026A371D8700> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index' To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Could you please see this issue and let me know the solution for it. In addition, what is the faster way to let the pycharm run the code more quickly? \r\nThank you in advance.\r\nBest Regards,\r\nMohamed Abdul-Al", "@mabdulal,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. \r\n\r\nAlso, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/45956#issuecomment-750906198) from issue #45956 with a similar error log and check if it helps.\r\n\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46582\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46582\">No</a>\n"]}, {"number": 46581, "title": "Tensorboard not loading scalars with reload arrow button", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version (use command below): N/A\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.1.106\r\n- GPU model and memory: NVIDIA GeFroce RTX 2070, 32GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nUsing Jupyter Notebook, Tensorboard will not load scalars from logs.  Will not work at all.  I click the reload button on the orange header of Tensorboard and nothing happens.  Nothing... just updates the last reload date and time.  I have confirmed that the temporary files are created and that there are event files within the test folder within the temporary files.  I do not have tensorflow installed.  I have python 3.7.9, numpy, pandas, matplotlib, tb-nightly, pillow, torch.\r\n\r\n**Describe the expected behavior**\r\nI expected to see graphs of scalars and data that is saved in the loggers.  It does not anything.\r\n\r\n**Standalone code to reproduce the issue**\r\n%pylab inline\r\nimport torch\r\n%load_ext tensorboard\r\nimport torch.utils.tensorboard as tb\r\nimport tempfile\r\nlog_dir = tempfile.mkdtemp()\r\n%tensorboard --logdir {log_dir} --reload_interval 1\r\nlogger = tb.SummaryWriter(log_dir+'/test', flush_secs=1)\r\nlogger.add_scalar('first/number', 0, global_step=0)\r\nlogger.add_scalar('first/number', 1, global_step=1)\r\n\r\n\r\n\r\n\r\n\r\n![Capture](https://user-images.githubusercontent.com/72282737/105374384-45a81a00-5bcd-11eb-99dc-359d80ea0b4b.PNG)\r\n\r\n\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@DJJD17 I ran your code in a colab and can see the graph. Can you please check this [gist](https://colab.research.google.com/gist/jvishnuvardhan/7abe95afbce80f947380a8cdff4a45a1/untitled78.ipynb) and let us know if I am missing anything here. Thanks!", "Thank you\n\nI was able to get it to work in colab. But I simply cannot get it to run on my laptop. It\u2019s a Windows 10 Alienware R15. I have tried it with tb-nightly  by itself and tensorflow by itself. i can\u2019t get it to work and I am highly confused. The laptop is brand new. \n\nYour thoughts?\nJeff\n\n> On Jan 26, 2021, at 6:37 PM, Vishnuvardhan Janapati <notifications@github.com> wrote:\n> \n> \ufeff\n> @DJJD17 I ran your code in a colab and can see the graph. Can you please check this gist and let us know if I am missing anything here. Thanks!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "@DJJD17 Sorry for the late response. This is more related to TensorBoard, so can you please open this issue in [`TB repo`](https://github.com/tensorflow/tensorboard/issues). That repository has TB experts who can resolve your issue faster. In this repo, we deal `TensorBoard` callback under `tf.keras`. Thanks!\r\n\r\nPlease close the issue here and open a new issue (with the same content) in TB repo. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46581\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46581\">No</a>\n"]}, {"number": 46579, "title": "tf.config.run_functions_eagerly(run_eagerly=True) cashes on model load", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3 and 2.4 verified\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Not required\r\n- GPU model and memory: Not required\r\n\r\n\r\n**Describe the current behavior**\r\nException when loading saved model when a custom layer having `training=None` as argument is implemented.\r\n```\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (1 total):\r\n    * Tensor(\"Placeholder:0\", shape=(None, 28, 28), dtype=float32)\r\n  Keyword arguments: {'training': False}\r\n\r\nExpected these arguments to match one of the following 2 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (2 total):\r\n    * TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='inputs')\r\n    * False\r\n  Keyword arguments: {}\r\n\r\nOption 2:\r\n  Positional arguments (2 total):\r\n    * TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='inputs')\r\n    * True\r\n  Keyword arguments: {}\r\n```\r\n\r\n**Describe the expected behavior**\r\nNo exception\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n[minimal example](https://colab.research.google.com/drive/1NBb6neAWOLhtBfORWviZB-Cy2BRrScz3?usp=sharing)\r\n\r\n**Other info / logs**\r\nSee Colab error.\r\nRemoving the `tf.config.run_functions_eagerly` does not result in an exception and training the model works flawlessly.\r\n", "comments": ["@ChWick,\r\nPlease check these comments from issue [#45602](https://github.com/tensorflow/tensorflow/issues/45602#issuecomment-744065797), [#43605](https://github.com/tensorflow/tensorflow/issues/43605#issuecomment-739076002) and [#38620](https://github.com/tensorflow/tensorflow/issues/38620#issuecomment-651338446) with similar error log and let us know if it helps. Thanks!", "@amahendrakar Thank you so much! I obviously I was unable to perform a solid search...", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46579\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46579\">No</a>\n"]}, {"number": 46578, "title": "remove duplicated lines in documentation", "body": "Remove duplicated lines in documentation. See: https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomRotation\r\n\r\n![Captura de tela de 2021-01-21 09-44-28](https://user-images.githubusercontent.com/12160840/105353478-0c23de80-5bce-11eb-9589-890722f33d31.png)\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46578) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 46577, "title": "Request to add tf.Cholesky and tf.MatrixTriangularSolve to tensorflow lite operations ", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04.5\r\n- Python version: 3.7.4\r\n- Tensorflow : 2.4.0\r\n\r\nI'm trying to use the function tf.linalg.lstsq inside my module and I'm getting this error for the converter: \r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n    ```\r\n    Traceback (most recent call last):\r\n      File \"/home/aia/anaconda3/envs/notebooks/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 213, in toco_convert_protos\r\n        enable_mlir_converter)\r\n      File \"/home/aia/anaconda3/envs/notebooks/lib/python3.7/site-packages/tensorflow/lite/python/wrap_toco.py\", line 38, in wrapped_toco_convert\r\n        enable_mlir_converter)\r\n    Exception: <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/Cholesky@matrix_solve_ls_1_cond_false_28664\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.Cholesky' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/cholesky_solve/MatrixTriangularSolve@matrix_solve_ls_1_cond_false_28664\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/cholesky_solve/MatrixTriangularSolve_1@matrix_solve_ls_1_cond_false_28664\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/Cholesky@matrix_solve_ls_1_cond_true_28663\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.Cholesky' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/cholesky_solve/MatrixTriangularSolve@matrix_solve_ls_1_cond_true_28663\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/cholesky_solve/MatrixTriangularSolve_1@matrix_solve_ls_1_cond_true_28663\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/Cholesky@matrix_solve_ls_cond_false_25991\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.Cholesky' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/cholesky_solve/MatrixTriangularSolve@matrix_solve_ls_cond_false_25991\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/cholesky_solve/MatrixTriangularSolve_1@matrix_solve_ls_cond_false_25991\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/Cholesky@matrix_solve_ls_cond_true_25990\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.Cholesky' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/cholesky_solve/MatrixTriangularSolve@matrix_solve_ls_cond_true_25990\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/cholesky_solve/MatrixTriangularSolve_1@matrix_solve_ls_cond_true_25990\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(\"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\" at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): failed while converting: 'tf.IfRegion306_else': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n\t    tf.Cholesky {device = \"\"}\r\n\t    tf.MatrixTriangularSolve {adjoint = false, device = \"\", lower = true}\r\n\t    tf.MatrixTriangularSolve {adjoint = true, device = \"\", lower = true}\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    \r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    Traceback (most recent call last):\r\n      File \"/home/aia/repos/algo-pipeline/TF_HBR.py\", line 144, in <module>\r\n        tflite_model = converter.convert()\r\n      File \"/home/aia/anaconda3/envs/notebooks/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 739, in convert\r\n        result = _convert_saved_model(**converter_kwargs)\r\n      File \"/home/aia/anaconda3/envs/notebooks/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 637, in convert_saved_model\r\n        enable_mlir_converter=True)\r\n      File \"/home/aia/anaconda3/envs/notebooks/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 216, in toco_convert_protos\r\n        raise ConverterError(str(e))\r\n    tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/Cholesky@matrix_solve_ls_1_cond_false_28664\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.Cholesky' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/cholesky_solve/MatrixTriangularSolve@matrix_solve_ls_1_cond_false_28664\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/cholesky_solve/MatrixTriangularSolve_1@matrix_solve_ls_1_cond_false_28664\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/Cholesky@matrix_solve_ls_1_cond_true_28663\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.Cholesky' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/cholesky_solve/MatrixTriangularSolve@matrix_solve_ls_1_cond_true_28663\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/cholesky_solve/MatrixTriangularSolve_1@matrix_solve_ls_1_cond_true_28663\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/Cholesky@matrix_solve_ls_cond_false_25991\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.Cholesky' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/cholesky_solve/MatrixTriangularSolve@matrix_solve_ls_cond_false_25991\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/cholesky_solve/MatrixTriangularSolve_1@matrix_solve_ls_cond_false_25991\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/Cholesky@matrix_solve_ls_cond_true_25990\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.Cholesky' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/cholesky_solve/MatrixTriangularSolve@matrix_solve_ls_cond_true_25990\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/cholesky_solve/MatrixTriangularSolve_1@matrix_solve_ls_cond_true_25990\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(\"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\" at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): failed while converting: 'tf.IfRegion306_else': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n\t    tf.Cholesky {device = \"\"}\r\n\t    tf.MatrixTriangularSolve {adjoint = false, device = \"\", lower = true}\r\n\t    tf.MatrixTriangularSolve {adjoint = true, device = \"\", lower = true}\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from```\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nhttps://colab.research.google.com/gist/AiaHaruv/396c2473b4601e9d35332884dbd02fa9/tflie_gist.ipynb\r\n\r\n**Any other info / logs**\r\nIn the colab, the traceback is: \r\n          4 frames\r\n          Exception: <unknown>:0: error: \r\n  loc(callsite(callsite(\"matrix_solve_ls/cholesky_solve/MatrixTriangularSolve@__inference_calculate_403\" at \r\n  \"PartitionedCall@__inference_signature_wrapper_411\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom \r\n  op nor a flex op\r\n          <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n          <unknown>:0: error: \r\n  loc(callsite(callsite(\"matrix_solve_ls/cholesky_solve/MatrixTriangularSolve_1@__inference_calculate_403\" at \r\n  \"PartitionedCall@__inference_signature_wrapper_411\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom \r\n  op nor a flex op\r\n          <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n          <unknown>:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit- \r\n  custom-ops flag):\r\n\t          tf.MatrixTriangularSolve {adjoint = false, device = \"\", lower = true}\r\n\t          tf.MatrixTriangularSolve {adjoint = true, device = \"\", lower = true}\r\n          \r\n        \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    ConverterError                            Traceback (most recent call last)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n        214       return model_str\r\n        215     except Exception as e:\r\n    --> 216       raise ConverterError(str(e))\r\n        217 \r\n        218   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n    \r\n    ConverterError: <unknown>:0: error: loc(callsite(callsite(\"matrix_solve_ls/cholesky_solve/MatrixTriangularSolve@__inference_calculate_403\" at \"PartitionedCall@__inference_signature_wrapper_411\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: loc(callsite(callsite(\"matrix_solve_ls/cholesky_solve/MatrixTriangularSolve_1@__inference_calculate_403\" at \"PartitionedCall@__inference_signature_wrapper_411\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n    <unknown>:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n\t    tf.MatrixTriangularSolve {adjoint = false, device = \"\", lower = true}\r\n\t    tf.MatrixTriangularSolve {adjoint = true, device = \"\", lower = true}\r\n\r\nThanks you \r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/f5edf08a8b21c7137bfffa5b5adf1349/46577-2-3.ipynb) , [TF v2.4](https://colab.research.google.com/gist/amahendrakar/d2646e8c29a2c10a357d7cd313848bfa/46577.ipynb#scrollTo=dV8_F-td9OZP) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/2755c27d44a9f27746f351750b270a33/46577-tf-nightly.ipynb). Please check the linked gist for reference. Thanks!", "First, we will support them via Select TF option. https://www.tensorflow.org/lite/guide/ops_select Will add those ops to the Select TF option.", "Please use the tomorrow's tf-nightly version and convert your model with the Select TF option.", "@abattery - Thank you, I installed the latest nightly version and indeed the conversion with these operations now work.\r\nHowever, I have a new issue with the tflite converter which is happening only when using nightly (it isn't happening when using tensorflow main branch). I tried as much as I can to create the minimal code to reproduce: \r\n\r\n    !pip install tf-nightly\r\n    import tensorflow as tf\r\n    import os\r\n    import numpy as np\r\n    \r\n    class EXAMPLE(tf.Module):\r\n    \r\n        @tf.function(input_signature=[tf.TensorSpec(shape=[100], dtype=tf.float32)])\r\n        def calculate(self, x):\r\n        \r\n          maxima_ind = tf.where(x > 0.8)\r\n          maxima_ind = tf.gather(maxima_ind,0,axis=1)\r\n    \r\n          maxima_ind = tf.cast(maxima_ind, dtype=tf.float32)\r\n    \r\n          if len(maxima_ind) > 10:\r\n              maxima_ind = tf.cast(maxima_ind, dtype=tf.float32)\r\n    \r\n          return maxima_ind\r\n          \r\n    to_export = EXAMPLE()\r\n    np.random.seed(54)\r\n    buffer_size = 100\r\n    x1  = tf.convert_to_tensor(np.random.rand(buffer_size).astype('float32'))\r\n    \r\n    \r\n    solution = to_export.calculate(x1)\r\n    print(solution)\r\n    models_dir = '/content/model_example/'\r\n    tf.saved_model.save(to_export, models_dir)\r\n    imported = tf.saved_model.load(models_dir)\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(models_dir) # path to the SavedModel directory\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                            tf.lite.OpsSet.SELECT_TF_OPS]\r\n    \r\n    tflite_model = converter.convert()\r\n    with open(models_dir + 'model_example.tflite', 'wb') as f:\r\n      f.write(tflite_model)\r\n\r\nWhen running it, I'm getting this error: \r\n\r\n    Exception                                 Traceback (most recent call last)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n        216                                                  debug_info_str,\r\n    --> 217                                                  enable_mlir_converter)\r\n        218       return model_str\r\n    \r\n    4 frames\r\n    Exception: <unknown>:0: error: loc(\"cond@__inference_calculate_3155\"): 'tf.If' op 'then_branch' input type tensor<?xf32> is incompatible with input type tensor<?xi64> at index 0\r\n    \r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n        ConverterError                            Traceback (most recent call last)\r\n        /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n            218       return model_str\r\n            219     except Exception as e:\r\n        --> 220       raise ConverterError(str(e))\r\n            221 \r\n            222   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n        \r\n        ConverterError: <unknown>:0: error: loc(\"cond@__inference_calculate_3155\"): 'tf.If' op 'then_branch' input type tensor<?xf32> is incompatible with input type tensor<?xi64> at index 0\r\n\r\nYou can also use this gist (which include the above code): https://colab.research.google.com/gist/AiaHaruv/6cac62614ef85f76db8bcfcfa0b8baae/conversion_bug.ipynb\r\nI understand there is a thing with the types but I'm using casting before the if statement and inside of it I'm actually doing nothing, and it isn't happening in the main branch..  \r\nAny help will be appreciated. Thank you \r\n", "@AiaHaruv thank you for filing a bug to us :-) Could you file a separate issue for the if op behavior in order to track the issue without the unrelated stuffs?", "@abattery - sure, [opened here](https://github.com/tensorflow/tensorflow/issues/46656) \r\nThank you"]}, {"number": 46576, "title": "Illegal instruction (core dumped) while importing TensorFlow", "body": "System:\r\n - HP EliteBook 8470p\r\n - Freshly installed Ubuntu 20.04 (5.8.0-38-generic)\r\n - Python 3.8.5\r\n - TensorFlow 2.4\r\n\r\nI installed TensorFlow with \r\n``` \r\npip install --upgrade pip\r\npip install tensorflow\r\n```\r\nas recommended on [https://www.tensorflow.org/install](https://www.tensorflow.org/install).\r\n\r\nWhen I try to import TensorFlow : \r\n```\r\n>>> import tensorflow as tf\r\n```\r\nI've got the following error:\r\n```\r\nIllegal instruction (core dumped)\r\n```\r\n\r\nI already read the following issues or SO questions:\r\n- [https://github.com/tensorflow/tensorflow/issues/17411](https://github.com/tensorflow/tensorflow/issues/17411)\r\n- [https://github.com/tensorflow/tensorflow/issues/45744](https://github.com/tensorflow/tensorflow/issues/45744)\r\n- [https://tech.amikelive.com/node-887/how-to-resolve-error-illegal-instruction-core-dumped-when-running-import-tensorflow-in-a-python-program/](https://tech.amikelive.com/node-887/how-to-resolve-error-illegal-instruction-core-dumped-when-running-import-tensorflow-in-a-python-program/)\r\n- [https://stackoverflow.com/questions/49092527/illegal-instructioncore-dumped-tensorflow](https://stackoverflow.com/questions/49092527/illegal-instructioncore-dumped-tensorflow)\r\n\r\nMost are referring to old TensorFlow versions (1.5) or related to CPU that does not support AVX. \r\n\r\nMy CPU does support AVX:\r\n```\r\n$ more /proc/cpuinfo | grep flags\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti ssbd ibrs ibpb stibp fsgsbase smep erms xsaveopt dtherm ida arat pln pts md_clear flush_l1d\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti ssbd ibrs ibpb stibp fsgsbase smep erms xsaveopt dtherm ida arat pln pts md_clear flush_l1d\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti ssbd ibrs ibpb stibp fsgsbase smep erms xsaveopt dtherm ida arat pln pts md_clear flush_l1d\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti ssbd ibrs ibpb stibp fsgsbase smep erms xsaveopt dtherm ida arat pln pts md_clear flush_l1d\r\n```\r\n\r\nSince my laptop is quite recent and my OS is a fresh insall, I think something sounds wrong with the TensorFlow installer. ", "comments": ["Same bug as https://github.com/tensorflow/tensorflow/issues/45744"]}, {"number": 46575, "title": "I got this error while trying to run the webcam_demo.py example in Posenet library from tensorflow. how to resolve this?", "body": "I got this error/warning while trying to run the webcam_demo.py example in Posenet library from Tensorflow. how to resolve this?\r\n\r\nThis is the **Git Repo** from where I forked this code : [posenet-python](https://github.com/rwightman/posenet-python)\r\n\r\nand This is my **Output Screen** :\r\n```\r\n>>> \r\n RESTART: A:\\Python\\Scripts\\Posenet-Forked -- OGCode\\posenet-python-master\\webcam_demo.py \r\nCannot find model file ./_models\\model-mobilenet_v1_101.pb, converting from tfjs...\r\nWARNING:tensorflow:From A:\\Python\\lib\\site-packages\\tensorflow\\python\\tools\\freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nTraceback (most recent call last):\r\n  File \"A:\\Python\\Scripts\\Posenet-Forked -- OGCode\\posenet-python-master\\webcam_demo.py\", line 66, in <module>\r\n    main()\r\n  File \"A:\\Python\\Scripts\\Posenet-Forked -- OGCode\\posenet-python-master\\webcam_demo.py\", line 20, in main\r\n    model_cfg, model_outputs = posenet.load_model(args.model, sess)\r\n  File \"A:\\Python\\Scripts\\Posenet-Forked -- OGCode\\posenet-python-master\\posenet\\model.py\", line 42, in load_model\r\n    convert(model_ord, model_dir, check=False)\r\n  File \"A:\\Python\\Scripts\\Posenet-Forked -- OGCode\\posenet-python-master\\posenet\\converter\\tfjs2python.py\", line 198, in convert\r\n    initializer_nodes=\"\")\r\n  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\tools\\freeze_graph.py\", line 361, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\tools\\freeze_graph.py\", line 190, in freeze_graph_with_def_protos\r\n    var_list=var_list, write_version=checkpoint_version)\r\n  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 835, in __init__\r\n    self.build()\r\n  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 847, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 885, in _build\r\n    build_restore=build_restore)\r\n  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 489, in _build_internal\r\n    names_to_saveables)\r\n  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\", line 362, in validate_and_slice_inputs\r\n    for converted_saveable_object in saveable_objects_for_op(op, name):\r\n  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\", line 223, in saveable_objects_for_op\r\n    yield ResourceVariableSaveable(variable, \"\", name)\r\n  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\", line 95, in __init__\r\n    self.handle_op = var.op.inputs[0]\r\nIndexError: tuple index out of range\r\n>>> \r\n```", "comments": ["@ArimaValanImmanuel,\r\nThe linked tutorial uses TensorFlow 1.12, which is not actively supported. \r\n\r\nCould you please migrate the code to the latest stable version TensorFlow 2.4 and check if you are facing the same issue. Thanks!", "I migrated after reading the code fully, now I got this error:\n\nHow to resolve this?\n\n `google.protobuf.message.DecodeError: Error parsing message\n\n`\nCode snippet is:\n\n```\ngraph_def.ParseFromString(f.read())\n```\n\n@amahendrakar ", "@ArimaValanImmanuel,\r\nIn order to expedite the trouble-shooting process, could you please remove all the external dependencies and provide a minimal code snippet so that we can reproduce the issue on our end. That will also allow us to determine the source of the issue easily. Thanks!", "> I got this error/warning while trying to run the webcam_demo.py example in Posenet library from Tensorflow. how to resolve this?\r\n> \r\n> This is the **Git Repo** from where I forked this code : [posenet-python](https://github.com/rwightman/posenet-python)\r\n> \r\n> and This is my **Output Screen** :\r\n> \r\n> ```\r\n> >>> \r\n>  RESTART: A:\\Python\\Scripts\\Posenet-Forked -- OGCode\\posenet-python-master\\webcam_demo.py \r\n> Cannot find model file ./_models\\model-mobilenet_v1_101.pb, converting from tfjs...\r\n> WARNING:tensorflow:From A:\\Python\\lib\\site-packages\\tensorflow\\python\\tools\\freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Use standard file APIs to check for files with this prefix.\r\n> Traceback (most recent call last):\r\n>   File \"A:\\Python\\Scripts\\Posenet-Forked -- OGCode\\posenet-python-master\\webcam_demo.py\", line 66, in <module>\r\n>     main()\r\n>   File \"A:\\Python\\Scripts\\Posenet-Forked -- OGCode\\posenet-python-master\\webcam_demo.py\", line 20, in main\r\n>     model_cfg, model_outputs = posenet.load_model(args.model, sess)\r\n>   File \"A:\\Python\\Scripts\\Posenet-Forked -- OGCode\\posenet-python-master\\posenet\\model.py\", line 42, in load_model\r\n>     convert(model_ord, model_dir, check=False)\r\n>   File \"A:\\Python\\Scripts\\Posenet-Forked -- OGCode\\posenet-python-master\\posenet\\converter\\tfjs2python.py\", line 198, in convert\r\n>     initializer_nodes=\"\")\r\n>   File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\tools\\freeze_graph.py\", line 361, in freeze_graph\r\n>     checkpoint_version=checkpoint_version)\r\n>   File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\tools\\freeze_graph.py\", line 190, in freeze_graph_with_def_protos\r\n>     var_list=var_list, write_version=checkpoint_version)\r\n>   File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 835, in __init__\r\n>     self.build()\r\n>   File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 847, in build\r\n>     self._build(self._filename, build_save=True, build_restore=True)\r\n>   File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 885, in _build\r\n>     build_restore=build_restore)\r\n>   File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 489, in _build_internal\r\n>     names_to_saveables)\r\n>   File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\", line 362, in validate_and_slice_inputs\r\n>     for converted_saveable_object in saveable_objects_for_op(op, name):\r\n>   File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\", line 223, in saveable_objects_for_op\r\n>     yield ResourceVariableSaveable(variable, \"\", name)\r\n>   File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\", line 95, in __init__\r\n>     self.handle_op = var.op.inputs[0]\r\n> IndexError: tuple index out of range\r\n> >>> \r\n> ```\r\n\r\ngetting the same error even after migrating to tf 2 , did anyone find any solution?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46575\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46575\">No</a>\n", "Still getting the same error even after migrating to tf 2 , did anyone find any solution? Please help !!!!\r\ntensorflow==2.4.1\r\ntensorflow-gpu==2.4.1\r\n![2021-05-21 (1)](https://user-images.githubusercontent.com/48823353/119087558-f19b2e00-ba24-11eb-9f8e-179bc25508d9.png)\r\n\r\n"]}, {"number": 46574, "title": "How to avoid downloading when using many times of the tff.simulation.datasets.stackoverflow.load_data()", "body": "As I see in this function, there is a parameter called cache_dir, which can be used to define the ```__pycache__```. However when I set this each time, every time when I run the code, it still tries to download the data. Is there any way that can avoid this? Or should I change the type of HDF5ClientData file to other format then I can save the data?\r\n\r\nThanks a lot for your help!", "comments": ["I see there is another [thread](https://github.com/tensorflow/federated/issues/896) pointing the same problem in TF Federated repository.\r\nTF Federated repository is the right place to surface this issue. I have a tagged the owner as per the `blame` in the above thread. Thus I will close the issue here. Thanks!"]}, {"number": 46573, "title": "cannot convert tf savedmodel to onnx", "body": "**System information**\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04\r\nTensorFlow installed from (source or binary):\r\nbinary\r\nTensorFlow version (use command below):\r\ntf-nightly-gpu         2.5.0.dev20210119\r\nPython version:\r\n3.6 (Anaconda)\r\nTensorflow-onnx version:\r\n1.8.0. build from source\r\n\r\nmy command line :\r\n```shell\r\npython -m tf2onnx.convert --saved-model ./model.savedmodel --output fea.onnx --custom-ops Bucketize,AsString,StringToHashBucketFast --signature_def serving_default --tag serve --opset 12 \r\n```\r\nBut I got the following error\uff1a\r\n```shell\r\n......\r\n2021-01-21 11:29:41,413 - ERROR - Could not find table resource to replace placeholder unknown_172\r\n2021-01-21 11:29:41,415 - ERROR - Could not find table resource to replace placeholder unknown_174\r\n2021-01-21 11:29:41,416 - ERROR - Could not find table resource to replace placeholder unknown_176\r\n2021-01-21 11:29:41,417 - ERROR - Could not find table resource to replace placeholder unknown_178\r\n2021-01-21 11:29:41,418 - ERROR - Could not find table resource to replace placeholder unknown_180\r\n2021-01-21 11:29:41,418 - ERROR - Could not find table resource to replace placeholder unknown_183\r\n2021-01-21 11:29:41,418 - ERROR - Could not find table resource to replace placeholder unknown_185\r\n2021-01-21 11:29:41,418 - ERROR - Could not find table resource to replace placeholder unknown_187\r\n2021-01-21 11:29:41,418 - ERROR - Could not find table resource to replace placeholder unknown_189\r\n2021-01-21 11:29:41,418 - ERROR - Could not find table resource to replace placeholder unknown_193\r\n2021-01-21 11:29:41,418 - ERROR - Could not find table resource to replace placeholder unknown_195\r\n2021-01-21 11:29:41,419 - ERROR - Could not find table resource to replace placeholder unknown_197\r\n......\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\r\nException ignored in: <bound method CapturableResourceDeleter.__del__ of <tensorflow.python.training.tracking.tracking.CapturableResourceDeleter object at 0x7f70486cbcf8>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py\", line 208, in __del__\r\n    self._destroy_resource()\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 797, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 841, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 695, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2981, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 3373, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 3218, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 998, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 603, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/saved_model/function_deserialization.py\", line 257, in restored_function_body\r\n    return _call_concrete_function(function, inputs)\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/saved_model/function_deserialization.py\", line 75, in _call_concrete_function\r\n    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 116, in _call_flat\r\n    cancellation_manager)\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1944, in _call_flat\r\n    flat_outputs = forward_function.call(ctx, args_with_tangents)\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 590, in call\r\n    executor_type=executor_type)\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\", line 1206, in partitioned_call\r\n    f.add_to_graph(graph)\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 506, in add_to_graph\r\n    g._add_function(self)\r\n  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3403, in _add_function\r\n    gradient)\r\n\r\n```\r\nI want to get the ONNX model, desperate for some advice\uff01\r\n\r\nthank you very much\r\n", "comments": ["@zhaohb \r\n\r\nPlease, share colab link or complete code snippet along with supporting files t reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "@ravikyram I can send the model file to you by email. Is that OK? Then you can repeat the question.", "[https://colab.research.google.com/drive/1wxu8piPR9qyAC8EjtDd6-STZqek77BO7?usp=sharing](url)\r\ncolab link", "@zhaohb Sorry for the late response. I notice you have opened same issue in the onnx repo https://github.com/onnx/tensorflow-onnx/issues/1287 and also getting the responses to resolve your issue. That is the correct repo for your issue.\r\n\r\nI am closing this issue from this repo. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46573\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46573\">No</a>\n"]}, {"number": 46572, "title": "Virtual Batch Normalization using Tensorflow", "body": "What is the recommended way to use [VBN](https://arxiv.org/pdf/1606.03498v1.pdf) using Tensorflow? \r\nWould it be an idea to add this to the documentation?", "comments": ["You can use the `virtual_batch_size` argument in `BatchNormalization` to do this. It's undocumented bc when it was exposed, we didn't see many users of it"]}, {"number": 46571, "title": "constant learning rate logging added to TensorBoard callback", "body": "Allowing TensorBoard callback to log leraning rate (lr) even if there is no scheduler used. Tensorboard will log every lr even when it is a constant. This is especially important when using ReduceLROnPlateau callback as the lr is treated as constatnt in every iteration and changed by assigning a new constant. ", "comments": ["### `optimizer.lr` values:\r\nAccording to the docs, learning rate should be one of \"A Tensor, floating point value, or a schedule that is a tf.keras.optimizers.schedules.LearningRateSchedule, or a callable\". Only problem for `tf.summary.scalar` could be a callable, but in case of a callable being passed as learning rate, [`__getattribute__`](https://github.com/tensorflow/tensorflow/blob/582c8d236cb079023657287c318ff26adb239002/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L785) of `keras.optimizers.Optimizer` (actually of OptimizerV2) is calling `self._get_hyper` and that returns the value after call if `self._hyper[name]` is callable. So if user is consistent with how optimizer should be used according to docs, it should work fine. \r\n\r\nIn fact I think a better elif condition could be more future proof (although the proposed one should work), but I am not sure how to check if value is tf.summary.scalar consistent. Maybe you have an idea?\r\n\r\n### Tests:\r\nI ran local test using code below and I also performed test by running `keras.callbacks_test.py` (both in tensorflow:devel Docker image) and I put gist for output below. The only test fails are due to the fact, that 'epoch_learning_rate' is logged all the time (which is expected) so there is an extra element in summary output leading to *AssertionError: Items in the first set but not the second*\r\n\r\nPlease let me know if further tests are needed.\r\n\r\n<details>\r\n<summary>Local test code</summary>\r\n\r\n``````\r\nimport tensorflow as tf\r\nimport datetime\r\nimport os\r\n\r\nclass SimpleModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(SimpleModel, self).__init__()\r\n        self.Dense_in = tf.keras.layers.Dense(units = 50, \r\n                                activation=tf.nn.relu, \r\n                                use_bias=True)\r\n        self.Dense_hidden = tf.keras.layers.Dense(units = 20, \r\n                                activation=tf.nn.relu, \r\n                                use_bias=True)\r\n        self.Dense_out = tf.keras.layers.Dense(units = 1, \r\n                                activation=None, \r\n                                use_bias=True)\r\n\r\n    def call(self, inputs, training = True):\r\n        X = self.Dense_in(inputs)\r\n        X = self.Dense_hidden(X)\r\n        X = self.Dense_out(X)\r\n        return X\r\n\r\ndummy_data = tf.random.normal((10000, 50))\r\ndummy_y = tf.random.uniform((10000,), minval=0, maxval=2, dtype=tf.dtypes.int32)\r\n\r\n\r\ndef test_tf_lr_logging(lr):\r\n    model = SimpleModel()\r\n    loss = tf.keras.losses.BinaryCrossentropy(name='binary_crossentropy', from_logits=True)\r\n\r\n    opt = tf.keras.optimizers.SGD(learning_rate = lr)\r\n    model.compile(optimizer = opt, loss = loss, metrics=[\"acc\"])\r\n\r\n    _datetime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n    log_dir = os.path.join('dummy_logs', _datetime)\r\n    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\r\n\r\n    model.fit(x = dummy_data, y = dummy_y, epochs = 2,\r\n        callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='acc', \r\n                        factor=0.5, patience=10, verbose=0, mode='auto', \r\n                        min_delta=0.0001, cooldown=0, min_lr=0),\r\n                    tb_callback])\r\n\r\n\r\nif __name__ == '__main__':\r\n    ### schedule\r\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\r\n        initial_learning_rate = 0.001,\r\n        decay_steps=100000,\r\n        decay_rate=0.96)\r\n    test_tf_lr_logging(lr_schedule)\r\n\r\n    ### callable\r\n    def lr_callable():\r\n      return .1\r\n    test_tf_lr_logging(lr_callable)\r\n\r\n    ### float\r\n    le_constant = .05\r\n    test_tf_lr_logging(le_constant)\r\n\r\n``````\r\n\r\n</details>\r\n\r\n[gist](https://gist.github.com/MatejHl/c1f696791fa3bdcbccbb4ee69a6c56e8) for callbacks_test.", "Hi @MatejHl ,\r\n\r\nOkay that makes sense. Re: the cases, can you add a comment to your else branch noting these cases?\r\n\r\nAs for the tests: Can you make sure to include updates to the tensorboard tests in the callbacks_Test.py in your PR covering all of these cases? (Callable & constant). Local testing is insufficient because it won't get included in our automated test infrastructure, and it could break or regress really easily."]}, {"number": 46570, "title": "Update merge.py", "body": "Updated typo as per issue #46568", "comments": []}, {"number": 46569, "title": " A function to get number of TFLite model parameters", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3 \r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.** A function to get number of TFLite model parameters\r\n\r\n**Will this change the current api? How?** It will add a new function to Interpreter\r\n\r\n**Who will benefit with this feature?** People who need to optimize the model. They will be able to see the change in parameter number and estimate benefits \r\n\r\n**Any Other info.** If there is a way to do it in a current version, please, let me know.\r\n", "comments": ["We do not have such API yet since the parameters are frozen into the constant tensors. However, the number of TFLite tensors will be approximately similar with the number of the variables of the original TF model but they are not exactly same.\r\n\r\nhttps://www.tensorflow.org/lite/api_docs/cc/class/tflite/interpreter#classtflite_1_1_interpreter_1a578af9beb1d3281fc185e742553096ab", "Thank you for your answer! But as I understand, It is only avaylable in C++ API. Is there a way to get this value through python API?", "Flatbuffer object can be obtained to retrieve the information of the model directly.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/flatbuffer_utils.py#L40 "]}]