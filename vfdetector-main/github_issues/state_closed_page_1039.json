[{"number": 22145, "title": "Depthwise convolution inside Dataset API throws data_format error", "body": "### System information\r\n-  Linux Ubuntu 16.04\r\n- TensorFlow installed using: pip\r\n- TensorFlow version: 1.10\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.0/7.2\r\n- GPU model and memory: GeForce GTX 1080 ti\r\n\r\nI tried to reproduce the error in a simpler fashion but couldn't manage it. Basically I use dataset API to load some patches from tfrecords and apply a synthethic blur on it (loaded from another tfrrecords) similar to this:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef apply_blur(img):\r\n    blur = np.random.rand(3,3,1,1)\r\n    img = tf.nn.depthwise_conv2d(img[None], blur, [1,1,1,1], 'VALID')\r\n    return img\r\n\r\ntf.reset_default_graph()\r\ndataset = tf.data.Dataset.from_tensor_slices(np.ones((10, 128, 128, 1)))\r\ndataset = dataset.map(apply_blur, 2)\r\n\r\niterator = dataset.make_one_shot_iterator()\r\nbatch = iterator.get_next()\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nwith tf.Session(config=config) as sess:\r\n    out = sess.run(batch)\r\n```\r\n\r\nMy code is still working with tf v1.8, but not in higher versions. It says the data_format should be \"NCHW\" for \r\nDepthwise convolution on CPU, while in fact the data_format is \"NCHW\".\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"cli_deblurring.py\", line 95, in <module>\r\n    cli()\r\n  File \"/home/user/miniconda3/lib/python3.6/site-packages/click/core.py\", line 722, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/user/miniconda3/lib/python3.6/site-packages/click/core.py\", line 697, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/user/miniconda3/lib/python3.6/site-packages/click/core.py\", line 895, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/user/miniconda3/lib/python3.6/site-packages/click/core.py\", line 535, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"cli_deblurring.py\", line 90, in cli\r\n    model.train()\r\n  File \"/home/user/Project/model/__init__.py\", line 159, in train\r\n    self._train(sess)\r\n  File \"/home/user/Project/model/deblurring.py\", line 318, in _train\r\n    epoch, global_step = self._train_epoch(sess)\r\n  File \"/home/user/Project/model/deblurring.py\", line 291, in _train_epoch\r\n    sess.run(light_fetches, feed_dict=self.train_feed_dict)\r\n  File \"/home/user/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/home/user/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/user/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/home/user/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnimplementedError: Depthwise convolution on CPU is only supported for NHWC format\r\n         [[Node: depthwise_3 = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1]](depthwise-0-TransposeNHWCToNCHW-LayoutOptimizer, strided_slice_4)]]\r\n         [[Node: load_data/IteratorGetNext = IteratorGetNext[output_shapes=[[?,8,128,128,1], [?,128,128,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](load_data/Iterator)]]\r\n         [[Node: load_data/IteratorGetNext/_671 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_64_load_data/IteratorGetNext\"\r\n, tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n```\r\nAny idea why this error could happen in higher versions?\r\n", "comments": ["I tried with the latest tf-nightly and the code seems to be fine. Can you try newer version to see if the issue has been fixed?", "I didn't explain well. The code I posted works for me as well. The code that produced the error is a bit more complex (depends on tfrecords, etc.), but similar. Unfortunately I couldn't reproduce the error with simplified version of the code. It seems there is a problem that TransposeNHWCToNCHW is applied before DepthwiseConv2dNative, where instead it should leave NHWC. I have tried with tf-nightly and got the same error.", "Could you provide a means of replicating the issue if not the above? Otherwise, there's not much we can do to help here.", "@karmel You can replicate the issue by adding any operation after the `tf.nn.depthwise_conv2d`. As an example, you can add `[0]`:\r\n\r\n``` python\r\nimg = tf.nn.depthwise_conv2d(img[None], blur, [1,1,1,1], 'VALID')[0]\r\n```\r\n\r\nAs a workaround, you can use another map to unwrap the output, e.g.:\r\n\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef apply_blur(img):\r\n    blur = np.random.rand(3,3,1,1)\r\n    img = tf.nn.depthwise_conv2d(img[None], blur, [1,1,1,1], 'VALID')\r\n    return img\r\n\r\ndef workaround(img):\r\n    return img[0]\r\n\r\ntf.reset_default_graph()\r\ndataset = tf.data.Dataset.from_tensor_slices(np.ones((10, 128, 128, 1)))\r\ndataset = dataset.map(apply_blur, 2).map(workaround, 2)\r\n\r\niterator = dataset.make_one_shot_iterator()\r\nbatch = iterator.get_next()\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nwith tf.Session(config=config) as sess:\r\n    out = sess.run(batch)\r\n```", "Yes. Tested on TensorFlow `v1.11.0-0-gc19e29306c 1.11.0` and the error persists, i.e., the same error is thrown.", "I am running ok with tf-nightly, wondering if the issue still exist?\r\n```\r\nroot@ubuntu:/v# python -c 'import tensorflow as tf; print(tf.VERSION)'\r\n1.13.0-dev20181016\r\nroot@ubuntu:/v# python 22145.py\r\n2018-10-16 21:22:48.542684: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nroot@ubuntu:/v# cat 22145.py \r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef apply_blur(img):\r\n    blur = np.random.rand(3,3,1,1)\r\n    img = tf.nn.depthwise_conv2d(img[None], blur, [1,1,1,1], 'VALID')\r\n    return img\r\n\r\ndef workaround(img):\r\n    return img[0]\r\n\r\ntf.reset_default_graph()\r\ndataset = tf.data.Dataset.from_tensor_slices(np.ones((10, 128, 128, 1)))\r\ndataset = dataset.map(apply_blur, 2).map(workaround, 2)\r\n\r\niterator = dataset.make_one_shot_iterator()\r\nbatch = iterator.get_next()\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nwith tf.Session(config=config) as sess:\r\n    out = sess.run(batch)\r\nroot@ubuntu:/v# \r\n```", "@yongtang you ran the code with the workaround. The one that throws the error is the one in the OP's message with (AFAIK) any operation after the `tf.nn.depthwise_conv2d`. Anyway, I just noticed you said `tf-nightly` instead of `tf-nightly-gpu`, i.e., if it is not a typo, you're running TensorFlow for CPU-only. **This problem only occurs when using TensorFlow for GPU** (`tensorflow-gpu` and `tf-nightly-gpu`). I just tested on `tf-nightly-gpu` (version `b'v1.12.0-rc0-963-gbcfcb4d765' 1.13.0-dev20181016`) and the error persists. Full code, just in case:\r\n\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef apply_blur(img):\r\n    blur = np.random.rand(3,3,1,1)\r\n    img = tf.nn.depthwise_conv2d(img[None], blur, [1,1,1,1], 'VALID')[0]\r\n    return img\r\n\r\ntf.reset_default_graph()\r\ndataset = tf.data.Dataset.from_tensor_slices(np.ones((10, 128, 128, 1)))\r\ndataset = dataset.map(apply_blur, 2)\r\n\r\niterator = dataset.make_one_shot_iterator()\r\nbatch = iterator.get_next()\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nwith tf.Session(config=config) as sess:\r\n    out = sess.run(batch)\r\n```", "Can you try disabling the layout optimizer? I suspect it might be changing the layout without regard for the fact that your `apply_blur` function is running on the CPU. The following session configuration should disable it:\r\n\r\n```python\r\nconfig = tf.ConfigProto()\r\nfrom tensorflow.core.protobuf import rewriter_config_pb2\r\nconfig.graph_options.rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.OFF\r\n```", "It looks like the layout optimizer is disabled when no GPUs are present, which might explain why @yongtang didn't see the bug when running on a CPU build:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/822337e2b3377b89dad3b88cb5c4df7b369aa521/tensorflow/core/grappler/optimizers/layout_optimizer.cc#L2176-L2180", "@mrry You're correct. Disabling the layout optimizer did the trick. Based on your second reply, I also tried using `with tf.device('/cpu:0'):` in the `apply_blur(...)` and it worked! Full code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef apply_blur(img):\r\n    with tf.device('/cpu:0'):\r\n        blur = tf.random_uniform((3,3,1,1))\r\n        img = tf.nn.depthwise_conv2d(img[None], blur, [1,1,1,1], 'VALID')[0]\r\n        return img\r\n\r\ntf.reset_default_graph()\r\ndataset = tf.data.Dataset.from_tensor_slices(tf.ones((10, 128, 128, 1)))\r\ndataset = dataset.map(apply_blur, 2)\r\n\r\niterator = dataset.make_one_shot_iterator()\r\nbatch = iterator.get_next()\r\n\r\nconfig = tf.ConfigProto()\r\n# this also works\r\n# from tensorflow.core.protobuf import rewriter_config_pb2\r\n# config.graph_options.rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.OFF\r\nconfig.gpu_options.allow_growth = True\r\nwith tf.Session(config=config) as sess:\r\n    out = sess.run(batch)\r\n```\r\n\r\nIf you use the `with tf.device('/cpu:0'):` only on the `tf.nn.depthwise_conv2d(...)` it works too:\r\n\r\n```python\r\ndef apply_blur(img):\r\n    blur = tf.random_uniform((3,3,1,1))\r\n    with tf.device('/cpu:0'):\r\n        img = tf.nn.depthwise_conv2d(img[None], blur, [1,1,1,1], 'VALID')\r\n    return img[0]\r\n```", "Thanks for confirming that! I'm glad you have a workarounds, but it seems like it should be possible to avoid optimizing the function when it runs on the CPU (or perhaps generate different optimized versions of a function for CPU and GPU execution).\r\n\r\n@rmlarsen Can you triage this within Grappler? Thanks!", "I've also just run across this - it's a bit frustrating as this is a shared function and sometimes it does indeed run on the GPU. So guarding it with the `tf.device('/cpu:0')` isn't very scalable given that sometimes I do want it to run on the GPU (in other function calls - large shared code base) (Tensorflow 1.14.0)", "Same here, with CUDA 10.0 / CuDNN 7.5 / Tensorflow, after switching to 1.13.1 to 1.14.0 I started getting this issue.\r\n\r\nSame setting as above: I implement gaussian blur transform using conv2d as part of tf.Data pipeline which is placed on cpu, without layout optimizer knowing that.", "+1, got same error on TensorFlow 1.13.1\r\nCodes:\r\n```python\r\nimage = tf.cast(image, tf.float32)\r\nimage = tf.expand_dims(image, 0)\r\nkernel = tf.constant(\r\n    [[1, 1, 1], [1, 5, 1], [1, 1, 1]], dtype=tf.float32,\r\n    shape=[3, 3, 1, 1]) / 13.\r\nkernel = tf.tile(kernel, [1, 1, 3, 1])\r\nstrides = [1, 1, 1, 1]\r\ndegenerate = tf.nn.depthwise_conv2d(\r\n    image, kernel, strides, padding='VALID', rate=[1, 1]) # Error on this line\r\n```", "Hey folks, any progress here? It is 2020, tf.2.1, and still cannot build blur function using depthwise_conv2d. As reported above, works with CPU build, works even in eager execution mode on GPU build, but when it comes to training, crashes inside tf.Data.dataset pipeline.", "@jsimsa Should 3ec28a7ee327244fe96d864c15dfa10e8753436c (in the nightly build, but not in any release, yet) fix this issue, or is there anything else that @chayka needs to do?\r\n", "tf-nightly should indeed not have this issue. @chayka could you give it a try? thanks", "@jsimsa will try tomorrow, thanks!", "Hi @Talmaj ! I did not get any issue in [TF 2.8 version](https://colab.sandbox.google.com/gist/mohantym/c47a1c67c4497aac7f9b2b9fc3062eb1/github_22145.ipynb) using compatibility mode. Can we move this to closed status now?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 22144, "title": "Fix a bug in TF_LITE_ENSURE_OK.", "body": "For example,\r\n\r\nTF_LITE_ENSURE_OK(context, foo());\r\n\r\nfoo() should not be invoked twice when its return value is not kTfLiteOk.", "comments": []}, {"number": 22143, "title": "ValueError: Op type not registered 'NcclAllReduce' in binary running on 35712d892b7a.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: based on nvidia-docker tensorflow/tensorflow:1.10.0-gpu-py3 image.\r\n- **GPU model and memory**: Titan Xp 11GB 2 pcs.\r\n- **Exact command to reproduce**: gen_nccl_ops.nccl_all_reduce( ... )\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nHi, I'm trying to implement multi-gpu tensorflow code.\r\nI am trying to use **gen_nccl_ops.nccl_all_reduce** to sum values from each GPU, however, the code raises ValueError written below.\r\n`ValueError: Op type not registered 'NcclAllReduce' in binary running on 35712d892b7a. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. while building NodeDef 'Tower_0/Network/NcclAllReduce'\r\n`\r\nAs suggested by the error message, I tried to access **tf.contrib.nccl** at the beginning of the whole code files.\r\n```\r\nimport tensorflow as tf\r\ntf.contrib.nccl\r\n\r\n# ... Code continues ... #\r\n```\r\nBut it doesn't work! \r\nI checked NCCL is installed using \r\n`export NCCL_DEBUG=VERSION`\r\n, and got **NCCL version 2.2.13+cuda9.2**.\r\n\r\nI'm using nvidia-docker image tensorflow/tensorflow:1.10.0-gpu-py3.\r\nI confirmed **nccl.all_sum()** is working.\r\n\r\nIs there anyway to import NCCL library before graph establishment?\r\n\r\nThanks.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\n# Gradient Penalty on real distribution\r\nself.gradient = tf.gradients(tf.reduce_sum(self.D[real][logits]), [self.x])[0]\r\nself.slope_square = tf.reduce_sum(tf.square(self.gradient), reduction_indices=[1, 2, 3])\r\n\r\nif self.tower_config is None:\r\n    penalty = tf.reduce_mean(self.slope_square)\r\n    self.gradient_penalty = self.C_gradient * penalty\r\nelse:\r\n    # Utilize NCCL\r\n    shared_name = tf.get_variable_scope().name.\\\r\n        replace(self.tower_config.name, self.tower_config.prefix.format(\"NCCL\"))\r\n    penalty = gen_nccl_ops.nccl_all_reduce(\r\n        input=self.slope_square,\r\n        reduction=\"sum\",\r\n        num_devices=self.tower_config.num_devices,\r\n        shared_name=shared_name\r\n    ) / (1.0 / self.tower_config.num_devices)\r\n    self.gradient_penalty = self.C_gradient * penalty\r\n```\r\n", "comments": ["Try:\r\n\r\n` sudo apt install libnccl2=2.2.13-1+cuda9.0  libnccl-dev=2.2.13-1+cuda9.0 `\r\n\r\nTensorflow is built with CUDA 9.0 and you have the NCCL for CUDA 9.2 installed", "> Try:\r\n> \r\n> `sudo apt install libnccl2=2.2.13-1+cuda9.0 libnccl-dev=2.2.13-1+cuda9.0`\r\n> \r\n> Tensorflow is built with CUDA 9.0 and you have the NCCL for CUDA 9.2 installed\r\n\r\nHi,\r\nThanks for the solution.\r\nI tried to apply your solution, it doesn't work.\r\nI removed current container and I reinstalled **tensorflow/tensorflow:1.10.0-gpu-py3** image.\r\nRight after installation, I only can find libnccl2 as shown below and code was not working.\r\n```\r\nroot@28838711efd4:/notebooks# apt list | grep nccl\r\n\r\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\r\n\r\nlibnccl2/now 2.2.13-1+cuda9.0 amd64 [installed,local]\r\n```\r\n\r\nSo, I tried to install libnccl-dev as follows.\r\n```\r\nroot@28838711efd4:/notebooks# apt update\r\nroot@28838711efd4:/notebooks# apt install libnccl-dev=2.2.13-1+cuda9.0\r\nroot@28838711efd4:/notebooks# apt list | grep nccl\r\n\r\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\r\n\r\nlibhttpasyncclient-java/xenial 4.1-1 all\r\nlibnccl-dev/unknown 2.2.13-1+cuda9.2 amd64 [upgradable from: 2.2.13-1+cuda9.0]\r\nlibnccl1/unknown 1.2.3-1+cuda8.0 amd64\r\nlibnccl2/unknown 2.2.13-1+cuda9.2 amd64 [upgradable from: 2.2.13-1+cuda9.0]\r\nlibvncclient1/xenial-updates,xenial-security 0.9.10+dfsg-3ubuntu0.16.04.2 amd64\r\nlibvncclient1-dbg/xenial-updates,xenial-security 0.9.10+dfsg-3ubuntu0.16.04.2 amd64\r\npython-ncclient/xenial 0.4.7-1 all\r\n```\r\nHowever, same problem.\r\nDid I installed libnccl2 and libnccl-dev properly?", "Have you solved this problem? I have the same problem\u3002", "Not yet.. Still waiting for response.", "If you solve it, I hope you can make the solution public. If I solve it, I will tell you. 0.0", "> If you solve it, I hope you can make the solution public. If I solve it, I will tell you. 0.0\r\n\r\nSure. I'll post the solution as soon as I get the answer.", "hey bro:\r\nMaybe you can try this code\uff1a\r\n\r\nfrom tensorflow.contrib.nccl.python.ops import nccl_ops\r\nnccl_ops._maybe_load_nccl_ops_so()", "> hey bro:\r\n> Maybe you can try this code\uff1a\r\n> \r\n> from tensorflow.contrib.nccl.python.ops import nccl_ops\r\n> nccl_ops._maybe_load_nccl_ops_so()\r\n\r\nOkay, your solution works for importing NCCL library. Thanks!\r\n\r\nHowever, after building network, sess.run() doesn't give any response and just stay in that line. I think some of codes are in infinite loop or some of GPU devices are not returning value.\r\n\r\nI'll notice you after I fix all the things!", "Looks like the original issue was resolved by importing the nccl_ops library.  Closing this for now, please re-open if the issue persists.", "I am facing the same problem, did you solve the problem? What is the final solution? @skang29 "]}, {"number": 22142, "title": "name_scope has an effect on names of Variable create by tf.keras.layers.*", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.11.0-dev20180907\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n`name_scope` has an effect on names of `Variable`s created by `tf.keras.layers.*`. The following code got different result on tf 1.8 and tf >= 1.9. \r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.name_scope('name_scope'):\r\n    tf.keras.layers.Conv2D(3, 3, name='keras')(tf.ones([1, 3, 3, 1]))\r\n    tf.layers.Conv2D(3, 3, name='legacy')(tf.ones([1, 3, 3, 1]))\r\nprint([v.name for v in tf.global_variables()])\r\n```\r\n\r\nOn tf 1.11.0-dev20180907 it prints\r\n```\r\n['name_scope/keras/kernel:0', \r\n 'name_scope/keras/bias:0',\r\n 'legacy/kernel:0', \r\n 'legacy/bias:0']\r\n```\r\n\r\nOn tf 1.8 it prints\r\n```\r\n['keras/kernel:0',\r\n 'keras/bias:0',\r\n 'legacy/kernel:0',\r\n 'legacy/bias:0']\r\n```", "comments": ["@fchollet could you please comment?", "I think this behavior change introduced at https://github.com/tensorflow/tensorflow/commit/693b339ab2f062ec5bbb29f976c5d1fd94fbffa5 . @manipopopo Do you have some issues wrt this change? Thanks. ", "@yanboliang \r\nThe following code generates video frames' features in two different ways.\r\n\r\nIt calls `Conv2D` once to get all the frames' features during training. However, the input videos for evaluation could be very long, it uses `tf.while_loop` to apply `Conv2D` frame-by-frame so that the model can be placed on a GPU with limited memory.\r\n\r\n`tf.while_loop` creates a name scope `while` which changes the names of `Conv2D` weights. We can't directly load the model checkpoints for evaluation without specifying `var_list` for `tf.train.Saver`.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nBATCH_SIZE = 1\r\nNUM_FRAMES = 5\r\n\r\nCONV_FILTERS = 32\r\n\r\n\r\ndef build_graph_train(video_frames):\r\n  model = tf.keras.layers.Conv2D(CONV_FILTERS, 3, padding='same')\r\n\r\n  unfolded_video_frames = tf.reshape(video_frames, [-1, 720, 1080, 3])\r\n  frames_features = tf.reshape(\r\n      model(unfolded_video_frames),\r\n      [BATCH_SIZE, NUM_FRAMES, 720, 1080, CONV_FILTERS])\r\n\r\n  # optimize loss(frames_features)\r\n  # ...\r\n\r\n\r\ndef build_graph_evaluate(video_frames):\r\n  model = tf.keras.layers.Conv2D(CONV_FILTERS, 3, padding='same')\r\n\r\n  num_frames = tf.shape(video_frames)[1]\r\n  frame_ta = tf.TensorArray(video_frames.dtype, size=num_frames)\r\n  frame_ta = frame_ta.unstack(tf.transpose(video_frames, [1, 0, 2, 3, 4]))\r\n\r\n  def process_frame_by_frame(frame_index, frame_features_ta):\r\n    frame = frame_ta.read(frame_index)\r\n    # the shape of frame is [BATCH_SIZE, 720, 1080, 3]\r\n    return frame_index + 1, frame_features_ta.write(frame_index, model(frame))\r\n\r\n  _, frame_features_ta = tf.while_loop(\r\n      lambda frame_index, *_: frame_index < num_frames,\r\n      process_frame_by_frame,\r\n      (0, tf.TensorArray(video_frames.dtype, size=num_frames)),\r\n      back_prop=False)\r\n  frame_features = tf.transpose(frame_features_ta.stack(), [1, 0, 2, 3, 4])\r\n\r\n  # predict(frame_features)\r\n  # ...\r\n\r\n\r\nif __name__ == '__main__':\r\n  with tf.Graph().as_default():\r\n    video_frames = tf.zeros([BATCH_SIZE, NUM_FRAMES, 720, 1080, 3])\r\n    build_graph_train(video_frames)\r\n\r\n    print(tf.global_variables())\r\n    # [<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32>,\r\n    #  <tf.Variable 'conv2d/bias:0' shape=(32,) dtype=float32>]\r\n\r\n  with tf.Graph().as_default():\r\n    video_frames = tf.zeros([BATCH_SIZE, NUM_FRAMES, 720, 1080, 3])\r\n    build_graph_evaluate(video_frames)\r\n\r\n    print(tf.global_variables())\r\n    # [<tf.Variable 'while/conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32>,\r\n    #  <tf.Variable 'while/conv2d/bias:0' shape=(32,) dtype=float32>]\r\n```", "Nagging Assignee @fchollet: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 22141, "title": "Predictions from keras h5 file is different than its tflite version", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04.1 LTS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: See source code\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nI am following this video to convert my ML model to TensorFLow Lite:\r\n\r\nhttps://www.youtube.com/watch?v=MZx1fhbL2q4\r\n\r\nI noticed that if I am making predictions by reading the saved h5 file, the result is correct. This is not the case when I make predictions by reading the saved tflite file. I expect the predictions from both files to be the same.\r\n\r\n### Source code / logs\r\n```\r\nimport numpy as np\r\nfrom tensorflow import keras\r\nfrom tensorflow.contrib import lite\r\n\r\nmodel = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\r\nmodel.compile(optimizer='sgd', loss='mean_squared_error')\r\n\r\nxs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=np.float32)\r\nys = np.array([-3.0, -1.0, 0.0, 3.0, 5.0, 7.0], dtype=np.float32)\r\n\r\nmodel.fit(xs, ys, epochs=500)\r\n\r\n# Write out to keras save file\r\nkeras_file = 'linear.h5'\r\nkeras.models.save_model(model, keras_file)\r\n\r\n# Convert the keras file to TensorFlow Lite\r\nconverter = lite.TocoConverter.from_keras_model_file(keras_file)\r\ntflite_model = converter.convert()\r\nopen('linear.tflite', 'wb').write(tflite_model)\r\n\r\n# input to predict\r\nmodel_input = np.array([10.0], dtype=np.float32)\r\n\r\n# make predictions from the h5 file\r\nh5_model = keras.models.load_model(keras_file)\r\nh5_prediction = h5_model.predict(model_input)\r\n\r\n# make predictions fromm the tflite file\r\ninterpreter = lite.Interpreter(model_path='linear.tflite')\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\ninterpreter.set_tensor(input_details[0]['index'], [model_input])\r\ninterpreter.invoke()\r\ntflite_prediction = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n# h5_prediction will always be around 19, which is correct\r\nprint('Prediction from h5 model: {}'.format(h5_prediction))\r\n\r\n# tflite_prediction is some random value\r\nprint('Prediction from tflite model: {}'.format(tflite_prediction))\r\n```\r\n", "comments": ["This should be resolved in master (try the nightly). @gargn can comment further.", "There was a bug in `TocoConvert.from_keras_model_file` in TensorFlow 1.10. It will be fixed in TensorFlow 1.11 (when that is released). For now, as @aselle mentioned, it was fixed in August 9th's nightly build (`pip install tf-nightly`)."]}, {"number": 22140, "title": "Eager mode model.save will not save layer vars if the layer is in list of list", "body": "tf 1.10.1 \r\nlike below model.save will not save CuDNNGRU vars if you use MyLayer in a tf.keras.Model.\r\n\r\n\r\n    class MyLayer(keras.Model):\r\n       def __init__(self):\r\n        super(MyLayer, self).__init__()\r\n          self.encodes =  [(keras.layers.CuDNNGRU(units=100, \r\n                                        return_sequences=True, \r\n                                        return_state=False, \r\n                                        recurrent_initializer='glorot_uniform'),)]\r\n\r\n      def call(self, x):\r\n        return self.encodes[0][0](x)", "comments": ["A list works and a list of lists works but a list containing a tuple does not, correct? Tracking tuples would be nice but isn't being actively worked on. Happy to advise/review PRs.", "@allenlavoie I think it would be nice to track and record any trainable layer variables, no matter what data structure they are in.\r\nI'm also interested in the mechanism of saving vars.\r\n1. Seems if use another layer like B in Layer A(B is a member of A), then we do not save any vars of B.\r\n2. Then in order to save vars of B I have to make A a Model instead of a Layer, but for Model seems \r\n    we can not use add_weight, then I have to wrap members using add_weight using Layer.. A bit complex. \r\n", "Right, Layer doesn't have a __setattr__ override. Originally they were going to, but just defining it is a performance issue for functional layers (which create Layer objects in a loop and share variables by some other means).\r\n\r\nYou can add variables to a Model by assigning them to an attribute just like with sub-Layers. `add_weight` should work too; what's the issue?", "@allenlavoie How to add variables to a Model by assigning them to an attribute just like with sub-Layers ?  I could not find any related examples.  Something like self.abc = self.add_variable(\"abc\", [1, 100], initializer=tf.ones_initializer(dtype=tf.float32)) in Model.__init__() will fail and error say '`add_variable` is not supported on Networks. However, you may assign variables to attributes and they will show up in the weights and variables properties'  \r\nAnother question is if I want to use a Dense layer in a Model,  but the Dense layer out dim is unknown until I see the first input example, say if input is [batch_size, dim] then I will use Dense(dim) , if using Layer\r\nI think I can do this using build method, for Model how to do this ? I need to write another Layer to do this at first ?", "I'd use `tf.Variable`:\r\n\r\n```\r\n>>> m = tf.keras.Model()\r\n>>> m.v = tf.Variable(1.)\r\n>>> m.trainable_variables\r\n[<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>]\r\n```\r\n\r\nIt's fine to create new Layers in your call() method for subclassed Models. Just make sure they only get created on the first call. I think you can also define a build() method for Models if you'd like.", "@allenlavoie I see, thanks!", "@chenghuige ok to close?", "@drpngx  Fine close this."]}, {"number": 22139, "title": "Fix NoneType error in tf.nn.depthwise_conv2d with unknown shape", "body": "This fix tries to address the issue raised in #22110 where tf.nn.depthwise_conv2d thowns out NoneType error when the input shape is unknown.\r\n\r\nThis fix fixes #22110.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 22138, "title": "Array mul  lacking min/max data", "body": "\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.10.1\r\n- **Python version**:2.7.3\r\n- **Bazel version (if compiling from source)**:0.12.0\r\n- **GCC/Compiler version (if compiling from source)**:c++11\r\n- **CUDA/cuDNN version**:7.5.18\r\n- **GPU model and memory**:TITAN,12GB\r\n- **Exact command to reproduce**:\r\n ./bazel-bin/tensorflow/contrib/lite/python/tflite_convert --output_file=~/foo.cc   --graph_def_file!/frozen_inference_graph.pb   --inference_type=QUANTIZED_UINT8   --input_arrays=Imageholder --output_arrays=ResizeBilinear_2   --mean_values=127.5   --std_dev_values=127.5\r\n\r\n\r\n### Describe the problem\r\n\r\nI modified deeplabv3+(mobilenetv2) model, and train/eval a quantized model with \"tf.contrib.quantize.create_training_graph(quant_delay=0)\r\n tf.contrib.quantize.create_eval_graph\". The graph show as below:\r\n\r\n\r\nPlaceholder->mul->sub->mobilenetv2\r\n\r\n\r\n\r\nWhen I convert this frozen model to tflite quantized model, I use input_node = Placeholer. There are error occurred. Does any one know how to solve this problem?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\n\r\n2018-09-07 10:59:40.172018: F tensorflow/contrib/lite/toco/tooling_util.cc:1635] Array mul, which is an input to the Sub operator producing the output array sub, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\ntf.estimator package not installed.\r\nAborted (core dumped)\r\n\r\n\r\n", "comments": ["Over to Suharsh to advise.", "The issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet. In particular the Mul and Sub at the start of your network need quantization information for TOCO. This can be resolved by either manually adding a FakeQuantWithMinMaxVars node after mul and sub, see how the rewriter does it: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/quantize.py#L475\r\n\r\nThat being said this can be complicated, if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a *floating point* version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case.", "Nagging Assignee @suharshs: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "In feature_extractor.py, there is a function named '_preprocess_zero_mean_unit_range'. 'return (2.0 / 255.0) * tf.to_float(inputs) - 1.0' here will cause this problem. Make this function do nothing, just return the input and modify the input in export_model.py from 'input_image = tf.placeholder(tf.uint8, [1, None, None, 3], name=_INPUT_NAME)' to  'input_image = tf.placeholder(tf.float32, [1, SIZE_YOU_WANT, SIZE_YOU_WANT, 3], name=_INPUT_NAME)' will help", "@suharshs I'm facing the similar problem due to a layer consistent with several basic tensorflow ops (like Add, Mul, Sigmoid, etc.) Is there any update on the contrib/quantize tool? Or we still have to manually add FakeQuantWithMinMaxVars? \r\nThanks! ", "> The issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet. In particular the Mul and Sub at the start of your network need quantization information for TOCO. This can be resolved by either manually adding a FakeQuantWithMinMaxVars node after mul and sub, see how the rewriter does it: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/quantize.py#L475\r\n> \r\n> That being said this can be complicated, if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a _floating point_ version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case.\r\n\r\nI'm wondering how to manually adding a FakeQuantWithMinMaxVars node. I've looked into the link you provided and still cannot find that code ."]}, {"number": 22136, "title": "Fix ps0 OOM when workers too many.", "body": "When training with MonitoredTrainingSession in distributed mode,\r\nall ops related with report_uninitialized_xxx will be placed on ps0,\r\nwhich may lead to OOM when workers up to thousands.\r\n\r\nWe use a environment variable to solve it. When OOM happens, users can\r\nset the local device to fix the problem, such as\r\nos.environ['TF_LOCAL_DEVICE'] = '/job:worker/task:index'.\r\n\r\nFixes #22137", "comments": ["@martinwicke ", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Something is wrong with the sync infra. This should have merged but didn't, I'm looking into it.", "Weird, this PR should have been merged by that merge. Not sure what happened."]}, {"number": 22135, "title": "CopyFromGpuToHost and other operations can't be parallelized", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.8.0\r\n- **Python version**:3.5\r\n- **Bazel version (if compiling from source)**:0.10.0\r\n- **GCC/Compiler version (if compiling from source)**:c++11\r\n- **CUDA/cuDNN version**:9/7\r\n- **GPU model and memory**:1080ti,11G\r\n- **Exact command to reproduce**:N/A\r\n\r\n### Describe the problem\r\nFollowing the photo, why is the \"CopyFromGpuToHost\" function not working with other operations?\r\nThe operation can execute after sending data finish. Can they be parallelized?\r\nIs this a limitation of gpu card? or Tensorflow no support?\r\n\r\n![screenshot from 2018-09-07 10-20-43](https://user-images.githubusercontent.com/34928698/45194964-0ab91c00-b288-11e8-9564-e8071145ef27.png)\r\n", "comments": []}, {"number": 22134, "title": "tensorflow.python.framework.errors_impl.NotFoundError: No registered '_CopyFromGpuToHost' OpKernel for CPU devices compatible with node swap_out_gradients", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes. Added some custom TF operators. \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**:1.10.1\r\n- **Python version**:3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:9.2\r\n- **GPU model and memory**: GeForce GTX 970\r\n- **Exact command to reproduce**:python3 train/train.py --gpu 0 --model frustum_pointnets_v2 --log_dir train/log_v2 --num_point 1024 --max_epoch 201 --batch_size 24 --decay_step 800000 --decay_rate 0.5\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nGetting an error that \"No registered '_CopyFromGpuToHost' OpKernel\". Not able to find the refernce to this OpKernel anywhere. \r\n\r\n### Source code / logs\r\npid: 16638\r\nWARNING:tensorflow:From /home/rishi/sw/frustum-pointnets/models/pointnet_util.py:126: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nkeep_dims is deprecated, use keepdims instead\r\nWARNING:tensorflow:From /home/rishi/sw/frustum-pointnets/models/pointnet_util.py:212: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nkeep_dims is deprecated, use keepdims instead\r\nWARNING:tensorflow:From /home/rishi/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:519: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`NHWC` for data_format is deprecated, use `NWC` instead\r\n2018-09-06 19:00:38.848283: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-09-06 19:00:38.920714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-06 19:00:38.921052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \r\nname: GeForce GTX 970 major: 5 minor: 2 memoryClockRate(GHz): 1.1775\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.94GiB freeMemory: 3.56GiB\r\n2018-09-06 19:00:38.921067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-09-06 19:00:39.115930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-06 19:00:39.115958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-09-06 19:00:39.115967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-09-06 19:00:39.116154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3282 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\n**** EPOCH 000 ****\r\n2018-09-06 19:00:42.022472\r\n2018-09-06 19:00:42.305938: W tensorflow/core/framework/allocator.cc:108] Allocation of 50331648 exceeds 10% of system memory.\r\n2018-09-06 19:00:42.322253: W tensorflow/core/framework/allocator.cc:108] Allocation of 50331648 exceeds 10% of system memory.\r\n2018-09-06 19:00:42.337831: W tensorflow/core/framework/allocator.cc:108] Allocation of 100663296 exceeds 10% of system memory.\r\n2018-09-06 19:00:42.368540: W tensorflow/core/framework/allocator.cc:108] Allocation of 100663296 exceeds 10% of system memory.\r\n2018-09-06 19:00:42.401274: W tensorflow/core/framework/allocator.cc:108] Allocation of 50331648 exceeds 10% of system memory.\r\n2018-09-06 19:00:52.106763: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Not found: No registered '_CopyFromGpuToHost' OpKernel for CPU devices compatible with node swap_out_gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2 = _CopyFromGpuToHost[T=DT_FLOAT, _class=[\"loc@gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fa_layer3/truediv_2/_91)\r\n\t.  Registered:  device='GPU'\r\n\r\n\t [[Node: swap_out_gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2 = _CopyFromGpuToHost[T=DT_FLOAT, _class=[\"loc@gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fa_layer3/truediv_2/_91)]]\r\nTraceback (most recent call last):\r\n  File \"/home/rishi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1278, in _do_call\r\n    return fn(*args)\r\n  File \"/home/rishi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1263, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/rishi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered '_CopyFromGpuToHost' OpKernel for CPU devices compatible with node swap_out_gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2 = _CopyFromGpuToHost[T=DT_FLOAT, _class=[\"loc@gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fa_layer3/truediv_2/_91)\r\n\t.  Registered:  device='GPU'\r\n\r\n\t [[Node: swap_out_gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2 = _CopyFromGpuToHost[T=DT_FLOAT, _class=[\"loc@gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fa_layer3/truediv_2/_91)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train/train.py\", line 376, in <module>\r\n    train()\r\n  File \"train/train.py\", line 201, in train\r\n    train_one_epoch(sess, ops, train_writer)\r\n  File \"train/train.py\", line 256, in train_one_epoch\r\n    feed_dict=feed_dict)\r\n  File \"/home/rishi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/home/rishi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/rishi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/home/rishi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered '_CopyFromGpuToHost' OpKernel for CPU devices compatible with node swap_out_gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2 = _CopyFromGpuToHost[T=DT_FLOAT, _class=[\"loc@gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fa_layer3/truediv_2/_91)\r\n\t.  Registered:  device='GPU'\r\n\r\n\t [[Node: swap_out_gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2 = _CopyFromGpuToHost[T=DT_FLOAT, _class=[\"loc@gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fa_layer3/truediv_2/_91)]]\r\n\r\n", "comments": ["I tried this with r1.11 also and issue is there with latest release also. ", "Any update on this?", "Can you please check that you still see this issue with r1.12?\r\n/cc @rmlarsen @azaks2 ", "\r\n@tatianashp I tried this with r1.12 but I still see same issue. Please note that I am using CUDA 10.0 with r 1.12. \r\n\r\n`2018-11-04 10:53:31.772782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2018-11-04 10:53:32.225588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-04 10:53:32.225607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n2018-11-04 10:53:32.225612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n2018-11-04 10:53:32.225778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3293 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\n**** EPOCH 000 ****\r\n2018-11-04 10:53:34.929683\r\n2018-11-04 10:53:35.282864: W tensorflow/core/framework/allocator.cc:122] Allocation of 100663296 exceeds 10% of system memory.\r\n2018-11-04 10:53:35.314713: W tensorflow/core/framework/allocator.cc:122] Allocation of 100663296 exceeds 10% of system memory.\r\n2018-11-04 10:53:35.524664: W tensorflow/core/framework/allocator.cc:122] Allocation of 100663296 exceeds 10% of system memory.\r\n2018-11-04 10:53:35.556079: W tensorflow/core/framework/allocator.cc:122] Allocation of 100663296 exceeds 10% of system memory.\r\n2018-11-04 10:53:35.586546: W tensorflow/core/framework/allocator.cc:122] Allocation of 100663296 exceeds 10% of system memory.\r\n2018-11-04 10:53:43.540498: E tensorflow/core/common_runtime/executor.cc:623] Executor failed to create kernel. Not found: No registered '_CopyFromGpuToHost' OpKernel for CPU devices compatible with node {{node swap_out_fa_layer3/ThreeInterpolate_2}} = _CopyFromGpuToHost[T=DT_FLOAT, _class=[\"loc@fa_layer3/ThreeInterpolate_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fa_layer3/truediv_2/_91)\r\n\t.  Registered:  device='GPU'\r\n\r\n\t [[{{node swap_out_fa_layer3/ThreeInterpolate_2}} = _CopyFromGpuToHost[T=DT_FLOAT, _class=[\"loc@fa_layer3/ThreeInterpolate_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fa_layer3/truediv_2/_91)]]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered '_CopyFromGpuToHost' OpKernel for CPU devices compatible with node {{node swap_out_fa_layer3/ThreeInterpolate_2}} = _CopyFromGpuToHost[T=DT_FLOAT, _class=[\"loc@fa_layer3/ThreeInterpolate_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fa_layer3/truediv_2/_91)\r\n\t.  Registered:  device='GPU'\r\n\r\n\t [[{{node swap_out_fa_layer3/ThreeInterpolate_2}} = _CopyFromGpuToHost[T=DT_FLOAT, _class=[\"loc@fa_layer3/ThreeInterpolate_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fa_layer3/truediv_2/_91)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train/train.py\", line 376, in <module>\r\n    train()\r\n  File \"train/train.py\", line 201, in train\r\n    train_one_epoch(sess, ops, train_writer)\r\n  File \"train/train.py\", line 256, in train_one_epoch\r\n    feed_dict=feed_dict)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered '_CopyFromGpuToHost' OpKernel for CPU devices compatible with node {{node swap_out_fa_layer3/ThreeInterpolate_2}} = _CopyFromGpuToHost[T=DT_FLOAT, _class=[\"loc@fa_layer3/ThreeInterpolate_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fa_layer3/truediv_2/_91)\r\n\t.  Registered:  device='GPU'\r\n\r\n\t [[{{node swap_out_fa_layer3/ThreeInterpolate_2}} = _CopyFromGpuToHost[T=DT_FLOAT, _class=[\"loc@fa_layer3/ThreeInterpolate_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fa_layer3/tr`", "I have the same problem,  has anyone solved it?", "batch size set small,and  try  again", "Reducing the batch size from 32 to 8 worked for me. Also make sure to have set your CUDA environment variables. For me, CUDA 10, cuDNN 7.2, and Tensorflow 1.12.0 worked.", "it seems like:\r\n1. reduce the `batch_size`\r\n2. reduce some convs outputs to reduce the params.\r\n\r\n", "Hi, I got the same error, TF 1.13, CUDA 10, Cudnn 7, Ubuntu 16.04, GeForce GTX 1050 Ti/PCIe/SSE2, \r\n\r\nDeprecated in favor of operator or tf.math.divide.\r\n2019-03-28 08:51:16.696019: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-03-28 08:51:16.880719: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x7674990 executing computations on platform CUDA. Devices:\r\n2019-03-28 08:51:16.880745: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1\r\n2019-03-28 08:51:16.911657: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3200180000 Hz\r\n2019-03-28 08:51:16.914697: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x76e0580 executing computations on platform Host. Devices:\r\n2019-03-28 08:51:16.914727: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-03-28 08:51:16.915057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.4805\r\npciBusID: 0000:4e:00.0\r\ntotalMemory: 3.94GiB freeMemory: 2.30GiB\r\n2019-03-28 08:51:16.915091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-03-28 08:51:16.917505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-28 08:51:16.917522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-03-28 08:51:16.917528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-03-28 08:51:16.917648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2054 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:4e:00.0, compute capability: 6.1)\r\n**** EPOCH 000 ****\r\n2019-03-28 08:51:19.152603\r\n2019-03-28 08:51:19.897926: W tensorflow/core/framework/allocator.cc:124] Allocation of 268435456 exceeds 10% of system memory.\r\n2019-03-28 08:51:20.070020: W tensorflow/core/framework/allocator.cc:124] Allocation of 268435456 exceeds 10% of system memory.\r\n2019-03-28 08:51:20.148307: W tensorflow/core/framework/allocator.cc:124] Allocation of 268435456 exceeds 10% of system memory.\r\n2019-03-28 08:51:20.214870: W tensorflow/core/framework/allocator.cc:124] Allocation of 268435456 exceeds 10% of system memory.\r\n2019-03-28 08:51:20.283382: W tensorflow/core/framework/allocator.cc:124] Allocation of 268435456 exceeds 10% of system memory.\r\n2019-03-28 08:51:26.414952: E tensorflow/core/common_runtime/executor.cc:624] Executor failed to create kernel. Not found: No registered '_CopyFromGpuToHost' OpKernel for CPU devices compatible with node {{node swap_out_fa_layer3/ThreeInterpolate_2}}\r\n\t.  Registered:  device='GPU'\r\n\r\n\t [[{{node swap_out_fa_layer3/ThreeInterpolate_2}}]]\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 322, in <module>\r\n    train()\r\n  File \"train.py\", line 160, in train\r\n    train_one_epoch(sess, ops, train_writer)\r\n  File \"train.py\", line 205, in train_one_epoch\r\n    ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered '_CopyFromGpuToHost' OpKernel for CPU devices compatible with node {{node swap_out_fa_layer3/ThreeInterpolate_2}}\r\n\t.  Registered:  device='GPU'\r\n\r\n\t [[{{node swap_out_fa_layer3/ThreeInterpolate_2}}]]\r\nMy batch size is 1.. any resolution?\r\n", "well, i think your `2054 MB memory` of '`1050 Ti` is incapable for your project..\r\n1. level up your GPU.\r\n2. reduce your img's size.\r\n3. reduce some net parts' output number. \r\n", "Closing because this issue is quite old.  Please reopen if you're still running into the same problem.", "> I have the same problem, has anyone solved it?\r\n\r\nsorry do you have solved it?", "have you solved it? maybe I need your help.\r\n"]}, {"number": 22133, "title": "_beam_search_ops.so and PyFunc don't work with TF Serving (Was: In-Graph GatherTree)", "body": "Hi, i want to implement `tensorflow/contrib/seq2seq/python/ops/beam_search_ops.py >> gather_tree()`, because of `_beam_search_ops.so` and `PyFunc` are not worked in tf-serving, so is there any implementation of GatherTree by while_loop?", "comments": ["I am using `python2.7 + tensorflow-1.2.1 + tf-serving-1.10`. Develop with `macOS + tensorflow-cpu-1.2.1`.\r\nAnd the project is a seq2seq use beamSearch decoder.", "I have figured out by double whlie_loop, but i don't think it's the very effective way.", "This is gonna be super slow.  I'm going to rename this bug to \"_beam_search_ops.so and PyFunc don't work with TF Serving\".", "Can you provide a simple repro for trying to load this in TF Serving so we can check the failure?", "@martinwicke @qlzh727 we may be able to resolve this by moving the c++ kernels in tf.contrib.seq2seq to core.", "TensorFlow Serving already links against `tensorflow/contrib:contrib_kernels`:\r\n\r\nhttps://github.com/tensorflow/serving/blob/r1.10/tensorflow_serving/model_servers/BUILD#L260L263\r\n\r\nHowever, the `beam_search_ops` kernels were added to this rule in TensorFlow 1.3.", "Thank you for your reply, so if i re-train the model with higher version, this will be solved ? @guillaumekln \r\nThe code is what i do and my solution in project. @ebrevdo (I'm sorry for my terrible code ^_^)\r\n```\r\n\tbeam_decoder = BeamSearchDecoder(\r\n\t    cell=dec_cell,\r\n\t    embedding=emb_dec,\r\n\t    start_tokens=tf.ones_like(qes_seq_len) * go_id, \r\n\t    end_token=eos_id,\r\n\t    initial_state=init_state,\r\n\t    beam_width=beam_width,\r\n\t    output_layer=output_layer,\r\n\t    diversity_promoting=diversity_promoting,\r\n\t    length_penalty_weight=0.6)\r\n\tbeam_output_idx, beam_output_state, beam_lengths = tf.contrib.seq2seq.dynamic_decode(\r\n\t    decoder=beam_decoder,\r\n\t    maximum_iterations=max_decode_len)\r\n\r\n\twith tf.variable_scope(\"beamSearch\"):\r\n\t    beam_out = index2symbol.lookup(\r\n\t        tf.cast(beam_output_idx.predicted_ids, tf.int64),\r\n\t        name='beam_out')\r\n\t    beam_scores = beam_output_idx.beam_search_decoder_output[0]\r\n\t    beam_log_probs = beam_output_state[1]\r\n\r\n\t    beam_out_trans = tf.transpose(beam_out, perm=[0, 2, 1])\r\n\t    beam_scores_trans = tf.transpose(beam_scores, perm=[0, 2, 1])\r\n\t......\r\n\tout_infers  = tf.saved_model.utils.build_tensor_info(\r\n\t                tensor=graph.get_tensor_by_name(\"beamCandidate/chosenAnswers/Merge:0\"))\r\n\tout_score = tf.saved_model.utils.build_tensor_info(\r\n\t                tensor=graph.get_tensor_by_name(\"beamCandidate/chosenAnswers/Merge_1:0\"))\r\n\tprediction_signature = tf.saved_model.signature_def_utils.build_signature_def(\r\n\t    inputs={\r\n\t        'question': question_placeholder,\r\n\t        'answer': answer_placeholder,\r\n\t        'question_len': qes_len_placeholder,\r\n\t        'answer_len': ans_len_placeholder,\r\n\t        'answer_threshold': answer_threshold,\r\n\t    },\r\n\t    outputs={\r\n\t        'output': out_infers,\r\n\t        'score': out_score,\r\n\t    },\r\n\t    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\r\n```\r\nThis is my solution, and it's not super slow at all, but it will be nice if `_beam_search_ops` worked in higher version, i will take this step later.\r\n```\r\n\tdef gather_tree_tf(step_ids, parent_ids, sequence_lengths):\r\n\t    with tf.variable_scope(name_or_scope=\"GatherTree\", reuse=None):\r\n\t        idx_shape = tf.shape(step_ids)      # 4, 2, 3\r\n\t        idx_dtype = step_ids.dtype\r\n\r\n\t        max_time    = idx_shape[0]  # 4\r\n\t        batch_size  = idx_shape[1]  # 2\r\n\t        beam_width  = idx_shape[2]  # 3\r\n\r\n\t        init_res = tf.zeros(idx_shape, dtype=idx_dtype)\r\n\t        init_cnt = tf.constant(0)\r\n\r\n\t        a = tf.reshape(tf.tile(tf.expand_dims(tf.range(beam_width), axis=1), [1, batch_size]), [-1, ])\r\n\t        b = tf.tile(tf.range(batch_size), [beam_width])\r\n\t        idx_arr = tf.concat(\r\n\t            [\r\n\t                tf.expand_dims(a, axis=1),  # beam_width\r\n\t                tf.expand_dims(b, axis=1),  # batch_size\r\n\t            ], axis=-1)\r\n\t        max_cnt = tf.shape(idx_arr)[0]  # 8\r\n\r\n\t        def condition(cnt, res_arr):\r\n\t            return tf.less(cnt, max_cnt)\r\n\r\n\t        def body(cnt, res_arr):\r\n\t            beam_id = idx_arr[cnt][0]   # 0\r\n\t            batch   = idx_arr[cnt][1]   # 0\r\n\r\n\t            max_len = sequence_lengths[batch][beam_id]  # 4\r\n\t            parent = parent_ids[max_len - 1][batch][beam_id]    # 2\r\n\t            temp_idx = tf.concat([tf.expand_dims(batch, axis=0), tf.expand_dims(beam_id, axis=0)], axis=-1)\r\n\t            temp_a = tf.expand_dims(tf.range(start=max_len-1, limit=max_time), axis=1)\r\n\t            temp_b = tf.tile(tf.expand_dims(temp_idx, axis=0), [max_time-max_len+1, 1])\r\n\t            mask_idx = tf.concat([temp_a, temp_b], axis=1)\r\n\t            mask = indice2mask3D(idxs=mask_idx, shape=idx_shape)\r\n\t            new_res_arr = tf.where(mask, step_ids, res_arr)\r\n\r\n\t            def condition2(cnt2, parent2, res_arr2):\r\n\t                return tf.greater(cnt2, 0)\r\n\r\n\t            def body2(cnt2, parent2, res_arr2):\r\n\t                level = cnt2 - 1\r\n\t                new_res_arr2 = tf.reshape(\r\n\t                    tf.tile(tf.expand_dims(step_ids[level][batch][parent2], axis=0), [tf.reduce_prod(idx_shape)]),\r\n\t                    idx_shape)\r\n\t                temp_idx2 = tf.expand_dims(tf.concat(\r\n\t                    [\r\n\t                        tf.expand_dims(level, axis=0),  # time = 2\r\n\t                        tf.expand_dims(batch, axis=0),  # batch = 0\r\n\t                        tf.expand_dims(beam_id, axis=0),  # parent_beam = 2, beam_id = 0\r\n\t                    ], axis=0), axis=0)\r\n\t                mask2 = indice2mask3D(idxs=temp_idx2, shape=idx_shape)\r\n\t                new_res_arr2 = tf.where(mask2, new_res_arr2, res_arr2)\r\n\t                new_parent = parent_ids[level][batch][parent2]\r\n\t                return tf.subtract(cnt2, 1), new_parent, new_res_arr2\r\n\r\n\t            out_cnt2, out_new_parent2, out_res_arr2 = tf.while_loop(\r\n\t                cond=condition2,\r\n\t                body=body2,\r\n\t                loop_vars=[max_len-1, parent, new_res_arr],\r\n\t                name=\"SubLoop2\")\r\n\t            new_cnt = tf.add(cnt, 1, name=\"newCount\")\r\n\t            return new_cnt, out_res_arr2\r\n\r\n\t        final_cnt, final_res_arr = tf.while_loop(\r\n\t            cond=condition,\r\n\t            body=body,\r\n\t            loop_vars=[init_cnt, init_res],\r\n\t            name=\"MainLoop\")\r\n\t        return final_res_arr\r\n\r\n\tdef indice2mask3D(idxs, shape):\r\n\t    with tf.variable_scope(\"Mask3D\"):\r\n\t        t = tf.range(tf.reduce_prod(shape))\r\n\t        max_cnt = tf.shape(idxs)[0]\r\n\t        init_mask = tf.zeros(shape=shape, dtype=tf.bool)\r\n\t        def cond(cnt, mask):\r\n\t            return tf.less(cnt, max_cnt)\r\n\t        def body(cnt, mask):\r\n\t            idx = idxs[cnt]\r\n\t            pos = idx[0] * shape[1] * shape[2] + idx[1] * shape[2] + idx[2]\r\n\t            cur_mask = tf.reshape(tf.equal(t, pos), shape)\r\n\t            return tf.add(cnt, 1), tf.logical_or(mask, cur_mask)\r\n\t        final_cnt, final_mask = tf.while_loop(\r\n\t            cond=cond, body=body, loop_vars=[0, init_mask],\r\n\t            name=\"ResultLoop\")\r\n\t        return final_mask\r\n```", "I had success using `BeamSearchDecoder` in Serving with TensorFlow 1.4 and above.", "Nice, thank you very much! And it's time to update my tensorflow."]}, {"number": 22132, "title": "Fix ps0 OOM when workers too many.", "body": "When training with MonitoredTrainingSession in distributed mode,\r\nall ops related with report_uninitialized_xxx will be placed on ps0,\r\nwhich may lead to OOM when workers up to thousands.\r\n\r\nWe use a environment variable to solve it. When OOM happens, users can\r\nset the local device to fix the problem, such as\r\nos.environ['TF_LOCAL_DEVICE'] = '/job:worker/task:index'.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "I signed it!"]}, {"number": 22131, "title": "Automated rollback of commit 24787842adfefe35f5a520313d775b14c29f143a", "body": "PiperOrigin-RevId: 211895566\r\n\r\nI had to manually resolve a couple of merge issues, which all ended up being simple.", "comments": ["I'm going to merge this and run tests before I leave for the day. If anything is wrong, it's easy to reverse this particular commit."]}, {"number": 22130, "title": "HLO serialization insufficiently validated on deserialization (e.g. for xrt)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: bc7c47ccddbf351d17b0d2d61cde3d48e2d530d6\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See below \r\n\r\n### Describe the problem\r\nThe new xrt ops expose serialized HLO Snapshots as an interface over the distributed TF interface. Some validation of the serialization is performed and returned as an error to the TF client. However, other format errors lead to assertions instead, killing the TF server and requiring a restart. It would be desirable to instead validate the HLO more thoroughly and return any errors to the client.\r\n\r\ncc @michaelisard \r\n\r\n### Source code / logs\r\nI have four minimal example HLO modules that trigger assertions that I encountered when using xrt. Though I encountered them using `xrt`, I'll be using `//tensorflow/compiler/xla/tools:replay_computation_cpu` for easy reproducability in these examples. For each of these examples, I'll show the textual hlo (if dump_computation_to_text was able to process the pb), the .pbtext of the HloSnapshot, and the output of `replay_computation_cpu`. To reproduce,\r\n\r\n```\r\ncd tensorflow\r\nprotoc --encode=xla.HloSnapshot -I=$PWD $PWD/tensorflow/compiler/xrt/xrt.proto < x.pbtext > x.pb\r\n./bazel-bin/tensorflow/compiler/xla/tools/replay_computation_cpu x.pb\r\n```\r\n\r\nwhere x.pbtext is the textual pb from this issue.\r\n\r\n#### Add operation with missing parameters:\r\n```\r\nHloModule test\r\n\r\nENTRY comp {\r\n  ROOT op = f64[] add()\r\n}\r\n```\r\n\r\n```\r\nhlo {\r\n  hlo_module {\r\n    name: \"test\"\r\n    entry_computation_name: \"op\"\r\n    computations {\r\n      name: \"comp\"\r\n      instructions {\r\n        name: \"op\"\r\n        opcode: \"add\"\r\n        shape {\r\n          element_type: F64\r\n          layout {\r\n            format: DENSE\r\n          }\r\n        }\r\n        id: 2\r\n      }\r\n      root_id: 2\r\n    }\r\n    program_shape {\r\n      result {\r\n        element_type: F64\r\n        layout {\r\n          format: DENSE\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n```\r\nbazel-bin/tensorflow/compiler/xla/tools/replay_computation_cpu ~/XLAHacks.jl/op_missing_args.pb\r\n2018-09-06 20:18:45.853017: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\r\n2018-09-06 20:18:45.854635: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x556b26f45230 executing computations on platform Host. Devices:\r\n2018-09-06 20:18:45.854662: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): <undefined>, <undefined>\r\n2018-09-06 20:18:45.854726: I tensorflow/compiler/xla/tools/replay_computation.cc:272] Compiling 1 modules in parallel.\r\n2018-09-06 20:18:45.855744: F tensorflow/compiler/xla/service/hlo_instruction.cc:1931] Check failed: 2 == operand_count() (2 vs. 0)\r\nAborted\r\n```\r\n\r\n#### Parameter reference when function has no parameters\r\n```\r\nHloModule test\r\n\r\nENTRY comp {\r\n  ROOT op = f64[] parameter(0)\r\n}\r\n```\r\n\r\n```\r\nhlo {\r\n  hlo_module {\r\n    name: \"test\"\r\n    entry_computation_name: \"op\"\r\n    computations {\r\n      name: \"comp\"\r\n      instructions {\r\n        name: \"op\"\r\n        opcode: \"parameter\"\r\n        shape {\r\n          element_type: F64\r\n          layout {\r\n            format: DENSE\r\n          }\r\n        }\r\n        id: 2\r\n      }\r\n      root_id: 2\r\n    }\r\n    program_shape {\r\n      result {\r\n        element_type: F64\r\n        layout {\r\n          format: DENSE\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n```\r\n2018-09-06 20:20:29.529757: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\r\n2018-09-06 20:20:29.531397: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x562efc8c8230 executing computations on platform Host. Devices:\r\n2018-09-06 20:20:29.531425: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): <undefined>, <undefined>\r\n2018-09-06 20:20:29.531492: I tensorflow/compiler/xla/tools/replay_computation.cc:272] Compiling 1 modules in parallel.\r\nSegmentation fault\r\n```\r\n\r\n#### Parameter reference out of bounds\r\nBasically an add of parameters 1, 2 for a one parameter function (i.e. I had an off by one in my parameter numbers):\r\n\r\n```\r\nhlo {\r\n  hlo_module {\r\n    name: \"test\"\r\n    entry_computation_name: \"op\"\r\n    computations {\r\n      name: \"comp\"\r\n      instructions {\r\n        name: \"parameter0\"\r\n        opcode: \"parameter\"\r\n        shape {\r\n          element_type: F64\r\n          dimensions: 1\r\n          layout {\r\n            minor_to_major: 0\r\n            format: DENSE\r\n          }\r\n        }\r\n        parameter_number: 1\r\n      }\r\n      instructions {\r\n        name: \"parameter1\"\r\n        opcode: \"parameter\"\r\n        shape {\r\n          element_type: F64\r\n          dimensions: 1\r\n          layout {\r\n            minor_to_major: 0\r\n            format: DENSE\r\n          }\r\n        }\r\n        parameter_number: 2\r\n        id: 1\r\n      }\r\n      instructions {\r\n        name: \"add2\"\r\n        opcode: \"add\"\r\n        shape {\r\n          element_type: F32\r\n          layout {\r\n            format: DENSE\r\n          }\r\n        }\r\n        id: 2\r\n        operand_ids: 0\r\n        operand_ids: 1\r\n      }\r\n      root_id: 2\r\n    }\r\n    program_shape {\r\n      parameters {\r\n        element_type: F64\r\n        dimensions: 1\r\n        layout {\r\n          minor_to_major: 0\r\n          format: DENSE\r\n        }\r\n      }\r\n      parameters {\r\n        element_type: F64\r\n        dimensions: 1\r\n        layout {\r\n          minor_to_major: 0\r\n          format: DENSE\r\n        }\r\n      }\r\n      result {\r\n        element_type: F32\r\n        layout {\r\n          format: DENSE\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n```\r\n2018-09-06 20:25:29.866942: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\r\n2018-09-06 20:25:29.868569: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x560a8b2b7230 executing computations on platform Host. Devices:\r\n2018-09-06 20:25:29.868598: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): <undefined>, <undefined>\r\n2018-09-06 20:25:29.868667: I tensorflow/compiler/xla/tools/replay_computation.cc:272] Compiling 1 modules in parallel.\r\n2018-09-06 20:25:29.869118: F tensorflow/compiler/xla/service/hlo_computation.cc:78] Check failed: param_no >= 0 && param_no < parameter_count\r\nERROR: invalid parameter number.  Expected [0, 2), got 2\r\nAborted\r\n```\r\n\r\n#### Missing dot dimension numbers\r\n```\r\nHloModule test\r\n\r\nENTRY comp {\r\n  parameter0 = f64[2,2]{0,1} parameter(0)\r\n  parameter1 = f64[2,2]{0,1} parameter(1)\r\n  ROOT dot2 = f64[1]{0} dot(parameter0, parameter1)\r\n}\r\n```\r\n\r\n```\r\nhlo {\r\n  hlo_module {\r\n    name: \"test\"\r\n    entry_computation_name: \"op\"\r\n    computations {\r\n      name: \"comp\"\r\n      instructions {\r\n        name: \"parameter0\"\r\n        opcode: \"parameter\"\r\n        shape {\r\n          element_type: F64\r\n          dimensions: 2\r\n          dimensions: 2\r\n          layout {\r\n            minor_to_major: 0\r\n            minor_to_major: 1\r\n            format: DENSE\r\n          }\r\n        }\r\n      }\r\n      instructions {\r\n        name: \"parameter1\"\r\n        opcode: \"parameter\"\r\n        shape {\r\n          element_type: F64\r\n          dimensions: 2\r\n          dimensions: 2\r\n          layout {\r\n            minor_to_major: 0\r\n            minor_to_major: 1\r\n            format: DENSE\r\n          }\r\n        }\r\n        parameter_number: 1\r\n        id: 1\r\n      }\r\n      instructions {\r\n        name: \"dot2\"\r\n        opcode: \"dot\"\r\n        shape {\r\n          element_type: F64\r\n          dimensions: 1\r\n          layout {\r\n            minor_to_major: 0\r\n            format: DENSE\r\n          }\r\n        }\r\n        id: 2\r\n        operand_ids: 0\r\n        operand_ids: 1\r\n      }\r\n      root_id: 2\r\n    }\r\n    program_shape {\r\n      parameters {\r\n        element_type: F64\r\n        dimensions: 2\r\n        dimensions: 2\r\n        layout {\r\n          minor_to_major: 0\r\n          minor_to_major: 1\r\n          format: DENSE\r\n        }\r\n      }\r\n      parameters {\r\n        element_type: F64\r\n        dimensions: 2\r\n        dimensions: 2\r\n        layout {\r\n          minor_to_major: 0\r\n          minor_to_major: 1\r\n          format: DENSE\r\n        }\r\n      }\r\n      result {\r\n        element_type: F64\r\n        dimensions: 1\r\n        layout {\r\n          minor_to_major: 0\r\n          format: DENSE\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n```\r\n2018-09-06 20:30:04.934720: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\r\n2018-09-06 20:30:04.936398: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x55fd5deeb230 executing computations on platform Host. Devices:\r\n2018-09-06 20:30:04.936426: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): <undefined>, <undefined>\r\n2018-09-06 20:30:04.936502: I tensorflow/compiler/xla/tools/replay_computation.cc:272] Compiling 1 modules in parallel.\r\n2018-09-06 20:30:04.937560: F ./tensorflow/compiler/xla/service/hlo_instruction.h:1105] Check failed: dot_dimension_numbers_ != nullptr\r\nAborted\r\n```", "comments": ["Thanks, Keno, for the detailed report! You're right that HLO protos haven't been an external interface so little effort has been spent validating them on input. We could hammer out fixes for your few examples, but I think a more principled fix is in order to avoid playing whack a mole. A big underlying issue is that proto deserialization ends up calling HloInstruction::Create* methods which just die if something is wrong  (or worse, keep on trucking with malformed instructions) rather than percolate up an error status. We need to add some form of these methods with beefed up validation and which return a Status value for proper error reporting. This will take some time, but needs to be done. Sorry for the inconvenience in the meantime.", "Found another one that I thought was a bit more insidious and is easy to miss. If you have an `HloMap` instruction, the computation it references must be serialized in the .proto before the computation that references it, otherwise you get a segfault (order of ids doesn't matter though)", "Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@denisvnukov who just joined the XLA team has set up a fuzzer which runs internally at Google to exercise our HloProto deserialization. He's found and fixed a bunch of issues already so the situation should rapidly get better. I'll make sure we cover the computation order issue you mention.", "Yep, I noticed 05812d761031b108b43560c90867b96dc4f030eb. Great work, thank you! Something else I had lying around was \r\n```\r\ndiff --git a/tensorflow/compiler/xla/service/hlo_instruction.cc b/tensorflow/compiler/xla/service/hlo_instruction.cc\r\nindex 6d13f85cbb..daa7cebb73 100644\r\n--- a/tensorflow/compiler/xla/service/hlo_instruction.cc\r\n+++ b/tensorflow/compiler/xla/service/hlo_instruction.cc\r\n@@ -285,6 +288,9 @@ StatusOr<std::unique_ptr<HloInstruction>> HloInstruction::CreateFromProto(\r\n       TF_RET_CHECK(proto.operand_ids_size() == 1)\r\n           << \"GetTupleElement instruction should have 1 operand but sees \"\r\n           << proto.operand_ids_size();\r\n+      TF_RET_CHECK(ShapeUtil::IsTuple(operands(0)->shape()))\r\n+          << \"GetTupleElement instruction should operate on a tuple, got \"\r\n+          << operands(0)->shape();\r\n       instruction = CreateGetTupleElement(proto.shape(), operands(0),\r\n                                           proto.tuple_index());\r\n```\r\nif either of you want to pick that up (or I can submit a PR if you'd prefer). I suspect the tuple_index needs to be validated there as well. ", "I think maybe a more general fix is to remove CHECKs from the constructor and rely on the later call to the HloVerifier to catch problems. HloInstruction::CreateFromProto would do the absolute minimum amount of verification that is required to call the constructors. In any case, I can take care of it.", "We added some verification and will continue adding more over time. \r\n\r\nAll four cases described in this issue are now handled properly (although tools might not properly check the status returned by the compiler). \r\n\r\nClosing the issue, feel free to file new one if you come across any not yet handled cases.", "Yup thanks, I'm happy with the amount of validation. Should I find something missing, I'll file an issue or a PR.", "Ran into my first TensorFlow server crash in quite a while today. Error was:\r\n```\r\n2018-10-31 20:41:43.571978: F ./tensorflow/compiler/xla/service/hlo_computation.h:154] Check failed: param_no < static_cast<int64>(param_instructions_.size()) (0 vs. 0)Computation comp2 has no parameter number 0\r\n```\r\nThe full HLO that causes the problem for me is here: https://gist.github.com/Keno/3bba34b85f80d37c59a9ffcb39c06439. I believe the problem is that the `conditional` computation does not reference its parameter.", "On Wed, Oct 31, 2018 at 5:54 PM Keno Fischer <notifications@github.com>\nwrote:\n\n> Ran into my first TensorFlow server crash in quite a while today. Error\n> was:\n>\nFYI, I submitted a fix for this internally yesterday morning. I presume it\nhas propagated out externally at this point. Let me know if you still have\nproblems.\n\nMark\n\n\n> 2018-10-31 20:41:43.571978: F ./tensorflow/compiler/xla/service/hlo_computation.h:154] Check failed: param_no < static_cast<int64>(param_instructions_.size()) (0 vs. 0)Computation comp2 has no parameter number 0\n>\n> The full HLO that causes the problem for me is here:\n> https://gist.github.com/Keno/3bba34b85f80d37c59a9ffcb39c06439. I believe\n> the problem is that the conditional computation does not reference its\n> parameter.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22130#issuecomment-434894875>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AH4Cvjcu7w7ffF6NAcuETi2s08Og8TmVks5uqkZAgaJpZM4Wd_9_>\n> .\n>\n", "Yup. https://github.com/tensorflow/tensorflow/commit/a169b7aa78650f0cceb0753af68a7c1162ff4738 I believe. Thanks!", "Next one: \r\n```\r\n2018-11-04 00:48:51.514113: F tensorflow/compiler/xla/shape_util.cc:391] Check failed: IsTuple(shape)\r\n```\r\non an infeed where the shape was accidentally not a tuple.", "Next one. I (accidentally) tried to compile the following against TPUs and the remote crashed:\r\n```\r\nHloModule test\r\n\r\nENTRY comp {\r\n  comp0_after-all11 = token[] after-all()\r\n  ROOT comp0_infeed12 = ((f32[10,10]{0,1}), token[]) infeed(comp0_after-all11)\r\n}\r\n```\r\nI assume having a token as the return value of the entry computation should be illegal. (Interestingly this doesn't crash the CPU backend).", "FYI, I just sent out a fix for the couple failures you saw most recently (non-tuple-shaped infeed and token in the output). Should land externally shortly."]}, {"number": 22128, "title": "[Feature Request] retrieving exact node names from multiple outputs (`tf.nn.top_k`)", "body": "### System information\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow version (use command below)**: '1.9.0-rc1'  I also believe it happens r1.10\r\n- **Python version**: 2.7 \r\n- **CUDA/cuDNN version**: 7.1\r\n- **Bazel version**: 0.16.1\r\n- **GPU model and memory** : TitanV 12 G\r\n- **Exact command to reproduce**: I added python sample code \r\n- **Mobile device**:  n/a\r\n\r\n\r\n\r\n### Describe the problem\r\nWhen a tensor op returns multiple outputs such as  `tf.nn.top_k`, \r\nit would be great to retrieve exact individual input nodes of its output recursively. \r\n\r\nAs addressed here, https://www.tensorflow.org/api_docs/python/tf/nn/top_k, \r\n`tf.nn.top_k` returns \r\n- values: The k largest elements along each last dimensional slice.\r\n- indices: The indices of values within the last dimension of input.\r\n\r\nWith below sample code, \r\n'Namespace/TopKV2' returns two outputs: 'Namespace/probabilities', 'Namespace/indices'\r\n(From the two output nodes, )\r\n'Namespace/probabilities' takes 'Namespace/TopKV2' **values** as input, \r\n'Namespace/indices' takes  'Namespace/TopKV2' **indices** as input. \r\n\r\nHowever, only 'Namespace/TopKV2' can be retrieved from the two output nodes. \r\nI also checked graph structure on tensorboard; it seems that same tensor/node (i.e., 'Namespace/TopKV2') goes to 'Namespace/probabilities' and  'Namespace/indices', both, however, one is values and, the other indices. \r\n\r\nIf differentiable names for multiple outputs are used, that would be great. \r\nOr, if there is workaround, please advise me. \r\nThis issue also causes key error in TensorRT graphsurgeon module. \r\n\r\n```\r\ndef _map_nodes(nodes):\r\n    return {node.name: node for node in nodes}\r\n\r\ndef main():\r\n   # simple network \r\n    with tf.variable_scope(\"Namespace\"):\r\n        inputs = tf.placeholder(tf.float32, [1, 3, 16, 16], name=\"input\")\r\n        convolved = tf.layers.conv2d(inputs, 4, [1, 1], data_format='channels_first')\r\n        reshaped = tf.reshape(convolved, [1, 256, 4])\r\n        best_probabilities, best_indices = tf.nn.top_k(reshaped, k=1)\r\n        output1 = tf.reshape(best_probabilities, [256], name='probabilities')\r\n        output2 = tf.reshape(best_indices, [256], name='indices')\r\n\r\n    graph = tf.get_default_graph()\r\n    graphdef = tf.get_default_graph().as_graph_def()\r\n\r\n    # Freeze the graph.\r\n    output_node_names = ['Namespace/probabilities', 'Namespace/indices']\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        frozen_graph = tf.graph_util.convert_variables_to_constants(\r\n            sess, graphdef, output_node_names\r\n        )\r\n        # Remove training nodes.\r\n        infer_graph = tf.graph_util.remove_training_nodes(frozen_graph)\r\n        graph_io.write_graph(infer_graph, \"./\", \"test.pb\", as_text=False)\r\n        node_map = _map_nodes(infer_graph.node)\r\n        for node in infer_graph.node:\r\n            for input_name in node.input:\r\n                # Recursively check against all of this node's inputs\r\n                input_node = node_map[input_name]\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\nI also attached TF board events file. \r\nhttps://drive.google.com/file/d/1HdV_eqMloqXgLenZSZHmKwME_oSnJbFb/view?usp=sharing ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nTensorFlow installed from\nBazel version\nGPU model and memory\nExact command to reproduce\nMobile device", "Updated ", "@joshsuihn -- sorry, but I'm having a little trouble understanding why the current behavior is a problem. Can you clarify-- what is the problem with the current behavior, and what which nodes do you want to be able to address?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22127, "title": "Support 3/4/5 dimensional input for bias_add with NCHW data format", "body": "This fix tries to address part of the issue raised in #20527 where biad_add only support 4 dimensional input with NCHW data format, and is causing tf.layers.conv3d to not suppport dynamic shapes with `channel_first` data format.\r\n\r\nThis fix add the 3/4/5 dimensional input for bias_add with NCHW data format as the first step to address the issue. Follow up PR will be added to fix tf.layers.conv3d issue.\r\n\r\nThis fix also only adds bias_add support. BiasGradOp still only works with 4D+NCHW. The 3D/5D+NCHW+BiadGradOp will be worked on later (TODO).\r\n\r\nThis fix is part of the effort for #20527.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Hi @yongtang Can you fix the test and try again? The code looks good otherwise.\r\n\r\n```tensorflow/core/framework/common_shape_fns_test.cc:374\r\nExpected equality of these values:\r\n  3\r\n  c.Value(c.Dim(output, 0))\r\n    Which is: 6```", "Nagging Assignee @yifeif: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@yifeif The PR has been rebased to resolve the merge conflict.", "This looks like it might be breaking biasadd_matmul_test.py, since TensorRT has a different idea of what NCHW means when it comes to 5-dimension and 3-dimension Tensors. I am not sure what is the best way forward here. \r\n\r\ncc @jjsjann123, @aaroey, @yifeif ", "@aaroey fixed the test internally, so we are good. This should go in shortly...", "Thanks all for the help! \ud83d\udc4d \u2764\ufe0f \ud83c\udf89 "]}, {"number": 22126, "title": "Ensure all ValueErrors are raised", "body": "Some ValueErrors are currently not thrown (or raised) when they are detected. We want to avoid this and raise all ValueErrors given the chance.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it.\r\n", "CLAs look good, thanks!\n\n<!-- ok -->", "Hi @anj-s  Are you going to review?", "@anj-s Hi, I just want to confirm something. Would this change require tests? Or should I do something else?", "There are some failures that are newly uncovered from this. Can you fix the new errors? I've retriggered the tests. There is a failure in contrib/quantize:fold_batch_norms_test\r\n\r\n", "When I look at:\r\n\r\nhttps://source.cloud.google.com/results/invocations/66bfcd5b-c469-4f4e-8794-0a0de124bc41/targets/%2F%2Ftensorflow%2Fcontrib%2Fquantize:fold_batch_norms_test/tests\r\n\r\nI don't see link for the console output for the test.\r\nHow can I access the console output ?\r\n\r\nthanks", "@martinwicke Outside of this test error, should anything else be done?", "@martinwicke @anj-s Should the test be retriggered? Usually, QA should not take this long.", "@anj-s \r\nCan you take another look at the PR ?\r\n\r\nThanks", "Removing ready to pull and and re-triggering tests at @anj-s request.", "Unless this hits unexpected flakes in testing, this should submit soon. "]}, {"number": 22125, "title": "Correctly tag tests that break internal testing for 1.11", "body": "", "comments": []}, {"number": 22124, "title": "python -c \"import tensorflow as tf\" fails when run in Dockerfile, works when inside container.", "body": "\r\n\r\nSo I can import tensorflow inside the docker container that I've pulled from DockerHub, but not in the Dockerfile.\r\n```\r\nthomas@fw0013591:~/code/tf_docker_bug$ nvidia-docker build .\r\nSending build context to Docker daemon  2.048kB\r\nStep 1/2 : FROM tensorflow/tensorflow:latest-devel-gpu\r\n ---> f73df9e66ebb\r\nStep 2/2 : RUN python -c \"import tensorflow as tf\"\r\n ---> Running in 18e621bcd3b1\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nThe command '/bin/sh -c python -c \"import tensorflow as tf\"' returned a non-zero code: 1\r\nthomas@fw0013591:~/code/tf_docker_bug$ nano Dockerfile\r\nthomas@fw0013591:~/code/tf_docker_bug$ nvidia-docker build .\r\nSending build context to Docker daemon  2.048kB\r\nStep 1/1 : FROM tensorflow/tensorflow:latest-devel-gpu\r\n ---> f73df9e66ebb\r\nSuccessfully built f73df9e66ebb\r\nthomas@fw0013591:~/code/tf_docker_bug$ nvidia-docker run -it f73df9e66ebb /bin/bash\r\nroot@68ed17426105:~# python -c \"import tensorflow as tf\"\r\nroot@68ed17426105:~# \r\n```\r\n\r\nThe dockerfile is incredibly simple. Looks like this:\r\n```\r\nFROM tensorflow/tensorflow:latest-devel-gpu\r\nRUN python -c \"import tensorflow as tf\"\r\n```\r\n\r\nIs this behavior to be expected?", "comments": ["I can compile code with `nvcc` through `RUN nvcc ...` within the Dockerfile and `nvidia-smi` has no problem finding the GPUs on my machine when called from inside the container. Though tensorflow is able to be loaded inside the container (not in Dockerfile) it can't locate the GPU. Falling back to tensorflow/tensorflow:latest-gpu resolved the issue of being able to find the GPUs once inside the container, but `RUN python -c \"import tensorflow as tf\"` still fails from inside the Dockerfile.", "I think this is due to actual nvidia libraries not being available during \"docker build\" time.\r\nAFAIK, only stubs exist for cuda libraries when running docker build.\r\n@flx42 (from NVIDIA) has more in depth information on this.", "@gunan is correct, since TensorFlow does not lazily load `libcuda.so.1`, you need a GPU to load the Python module when it was compiled with GPU support.\r\n\r\nA workaround used by the official Dockerfile is this:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu#L117-L118\r\nBut if you actually try to use any CUDA call, it will obviously fail :)", "Alright closing. The tensorflow nightly docker builds enabled a workaround anyway. Thanks everybody."]}, {"number": 22123, "title": "Incorrect results on GPU for reduce_* methods on large tensors", "body": "-----------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9.0 / 1.5.0 / 1.10.1\r\n- **Python version**: 2.7.15 / 3.6.3\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: CUDA 9.0\r\n- **GPU model and memory**: K10, P100\r\n- **Exact command to reproduce**: see code below\r\n\r\n### Describe the problem\r\nFor tensors with large amount of values (2^28 seems to be the threshold) a few of the reduce_* (tested max, sum, min) methods don't work correctly. The incorrect values appear to be read randomly from memory. The issue was only observed on GPU. Other observations are that a tensor with int types works as expected. Also no problem if the tensor is read as tf.constant. \r\nThese posts seem to be related:\r\nhttps://github.com/tensorflow/tensorflow/issues/20094\r\nhttps://stackoverflow.com/questions/49520394/tf-reduce-sum-on-gpu-fails-in-combination-with-placeholder-as-input-shape\r\nhttps://stackoverflow.com/questions/50806742/tensorflow-reduce-sum-returns-partially-incorrect-output-given-a-large-input\r\n\r\n### Source code / logs\r\nhttps://colab.research.google.com/drive/1GNqzUfRTRRohyjiF8HqvWzvKE6LasQ-4\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nn = np.random.randint(1,9)\r\ninp = np.ones(shape=(140000000, 2)) * n\r\ntensor = tf.placeholder(tf.float32, shape=(None, None))\r\noutput = tf.reduce_max(tensor, axis=-1)\r\n\r\nwith tf.Session() as sess:\r\n    result = sess.run(output, feed_dict={tensor: inp})\r\n    print('output values')\r\n    print(result)\r\n    print('number of incorrect values')\r\n    print(np.sum(result != inp.max(axis=-1)))\r\n```\r\nOutput:\r\n```\r\noutput values\r\n[5. 5. 5. ... 0. 0. 0.]\r\nnumber of incorrect values\r\n5782272\r\n```\r\n\r\nPinging:\r\n@nuance-research", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nMobile device", "updated", "possibly related: https://github.com/tensorflow/tensorflow/issues/19155", "@reedwm can you please take a look? Seems relevant.", "/CC @ekelsen, can you take a look?", "Nagging Assignees @ekelsen, @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry for the delay. I should have a fix for this shortly.", "This should be fixed in https://github.com/tensorflow/tensorflow/commit/fc44600e5c3ccf1de1e3d4792a00d3578311d3f6\r\n\r\nI'm closing this issue, please let us know if you still experience this issue."]}, {"number": 22122, "title": "Add float16 support on GPU for tf.contrib.image.transform", "body": "This fix tries to address the issue raised in #22115 where there were no float16 support on GPU for tf.contrib.image.transform.\r\n\r\nThis fix fixes #22115.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 22121, "title": "Unable to profile tf.keras Model. Am I missing something or is it a known issue?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 7\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: No\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: Below code\r\n\r\n### Describe the problem\r\n**tf.contrib.tfprof** does not profile tf.keras Model. Am I missing something or is it a known issue?\r\n\r\n### Source code / logs\r\nThe goal is to find what % of total time is spent in the input pipeline (mostly IO) and what % of total time is spent on model training \u2013 ideally a chart like the one presented in the [Training Performance session](https://www.youtube.com/watch?v=SxOsJPaxHME&feature=youtu.be&t=977) at tf dev summit 2018.\r\n\r\n**Model Training:**\r\n```python\r\ntraining_set = tfdata_generator_with_interleave([\u201cf1.avro\u201d, \u201cf2.avro\u201d, \u2026], is_training=True, batch_size=512, preprocess_fn=preprocess_fn_dense)\r\nvalidation_set = tfdata_generator_with_interleave([\u201cf1_val.avro\u201d, \u201cf2_val.avro\u201d, \u2026], is_training=False, batch_size=512, preprocess_fn=preprocess_fn_dense)\r\nmodel = my_tf_keras_dense_deep_model()\r\nmodel.compile(loss='categorical_crossentropy', optimizer=\u201dadam\u201d, metrics=['accuracy'])\r\n\r\nwith tf.contrib.tfprof.ProfileContext( './profiles/dense_model\u2019) ) as pctx:\r\n   model.fit(\r\n                training_set.make_one_shot_iterator(),\r\n                steps_per_epoch=TOTAL_TRAIN_RECORDS // _BATCH_SIZE, # need this as we are feeding data using a generator\r\n                epochs=_EPOCHS,\r\n                validation_data= validation_set.make_one_shot_iterator(),\r\n                validation_steps=TOTAL_TEST_RECORDS // _BATCH_SIZE,\r\n                callbacks=[tensorboard])\r\n```\r\n\r\n**Input Pipeline:**\r\n```python\r\ndef tfdata_generator_with_interleave(data_files, is_training, batch_size=128, preprocess_fn=preprocess_fn_dense):\r\n    def wrap_generator(filename):\r\n        return tf.data.Dataset.from_generator(read_local_avro_data_gen, (tf.float32, tf.float64),\r\n                                              args=[filename, batch_size / 4])\r\n\r\n    files = tf.data.Dataset.from_tensor_slices(data_files)\r\n    dataset = files.apply(tf.contrib.data.parallel_interleave(wrap_generator, cycle_length=4, sloppy=True))\r\n    dataset = dataset.flat_map(lambda *x: tf.data.Dataset.from_tensor_slices(x))\r\n\r\n    if is_training:\r\n        dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(1000, -1, 42))\r\n\r\n    dataset = dataset.apply(tf.contrib.data.map_and_batch(preprocess_fn, batch_size,\r\n                                                          num_parallel_batches=num_parallel_data_fetch,\r\n                                                          drop_remainder=True if is_training else False)\r\n                            )\r\n\r\n    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\r\n    return dataset\r\n```\r\n\r\nBut I do not see any files to load into tensorboard or chrome in ./profiles directory. Does the profile not work with tf.keras models? Or Am I missing any setup?\r\n\r\nAlso tried contacting the email address mentioned on the [Profiler README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/README.md) - xpan@google.com, but message sending failed.\r\n", "comments": ["I had the same problem, but it actually created the profile after the epoch was done. If this does not work you can also use the python API entirely. Something like this might work for you:\r\n```\r\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\nrun_metadata = tf.RunMetadata()\r\nProfileOptionBuilder = tf.profiler.ProfileOptionBuilder\r\nopts = ProfileOptionBuilder(ProfileOptionBuilder.time_and_memory()\r\n                                    ).with_node_names(show_name_regexes=['.*']).build()\r\n## model is some keras.models.Model instance\r\nmodel.compile(\r\n            ...,\r\n            options=run_options,\r\n            run_metadata=run_metadata\r\n        )\r\n### train model, or something else which calls tf.Session.run()\r\ntf.profiler.profile(keras.backend.get_session().graph, run_metadata, cmd=\"code\", options=opts)\r\n```\r\nor if you want to have multiple steps and save the profiling data (for example to open it with the ProfilerUI):\r\n```\r\nprof = tf.profiler.Profiler(graph=keras.backend.get_session().graph)\r\n### train model, or something else which calls tf.Session.run()\r\nprof.add_step(1, run_metadata)\r\n### train model, or something else which calls tf.Session.run()\r\nprof.add_step(1, run_metadata)\r\n### continue as long as you want\r\nwith open('./profile', 'wb') as f:\r\n    f.write(prof.serialize_to_string())\r\n``` ", "--------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-10-78121b9dd84b> in <module>\r\n----> 1 from keras import layers, models, optimizers\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\__init__.py in <module>\r\n      1 from __future__ import absolute_import\r\n      2 \r\n----> 3 from . import utils\r\n      4 from . import activations\r\n      5 from . import applications\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py in <module>\r\n      4 from . import data_utils\r\n      5 from . import io_utils\r\n----> 6 from . import conv_utils\r\n      7 \r\n      8 # Globally-importable utils.\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py in <module>\r\n      7 from six.moves import range\r\n      8 import numpy as np\r\n----> 9 from .. import backend as K\r\n     10 \r\n     11 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py in <module>\r\n     87 elif _BACKEND == 'tensorflow':\r\n     88     sys.stderr.write('Using TensorFlow backend.\\n')\r\n---> 89     from .tensorflow_backend import *\r\n     90 else:\r\n     91     # Try and load external backend.\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in <module>\r\n      3 from __future__ import print_function\r\n      4 \r\n----> 5 import tensorflow as tf\r\n      6 from tensorflow.python.framework import ops as tf_ops\r\n      7 from tensorflow.python.training import moving_averages\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 try:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 from tensorflow.python.tools import component_api_helper\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "Hi @Nithanaroy ! \r\nContrib has been removed in 2.x versions. Please refer [migration document](https://www.tensorflow.org/guide/migrate/migrate_tf2) to upgrade your codebase .  Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 22120, "title": "Modify tags for internal CI for tflite_driver_test", "body": "", "comments": []}, {"number": 22119, "title": "XLA commonly aborts with nvidia error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.15\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.2 / 7.2.1.38\r\n- **GPU model and memory**: Titan\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nWhen running XLA via the session config we commonly hit the following error:\r\n\r\n`2018-09-06 17:28:38.814423: F tensorflow/stream_executor/cuda/cuda_dnn.cc:211] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.\r\n`\r\n\r\nHowever, sometimes we get further than this error and see significant speed up with XLA and the loss decreasing.\r\n\r\nWhat could be causing this error?\r\n\r\nThe docker image is located at: https://hub.docker.com/r/joeyearsley/tf-builds/\r\n\r\nI can't diverge the exact script, however I do have the following:\r\n- NCCL for multi-gpu reduction like TF-Benchmark's Variable Manager\r\n- `tf.data` APIs including the string handles to alter between train and validation datasets.\r\n\r\n", "comments": ["@jlebar I got around to trying XLA again after #16045, works nicely when I get past this error, however stumped as this errors pops up frequently. Around 80% likely when running a script compared to the 20% which actually get past this error and continue. ", "> Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.\r\n\r\nIt's possible that there's an error earlier in execution, and this is just fallout.  Once there's some error on a stream, any operation on the stream will fail.\r\n\r\nCould you look through the logs carefully for warnings?  And/or run with CUDA_LAUNCH_BLOCKING=1, which may make it easier to find \"up-stream\" errors.", "Running it with that command still results in the same error.\r\n\r\nIt runs consistently when I include these flags `TF_XLA_FLAGS=\"--parallel_check_failfast=false\"`", "I changed from using the session approach to the scoped approach to try and narrow down potential places the error could occur. \r\n\r\nIt seems to be when I have `opt.compute_gradients(loss, params)` compiled into XLA code that I see a weird 3x speed-up but the loss doesn't decrease, and occasionally get the `CUDNN_STATUS_SUCCESS`.\r\n\r\nIf I just have the jit_scope around my model then it runs fine.", "> It runs consistently when I include these flags TF_XLA_FLAGS=\"--parallel_check_failfast=false\"\r\n\r\nThe parallel_check_op is deleted at HEAD because it was unused.  If it were affecting things, then you should see in your logs `failfast on first parallel-check failure`.  If you don't see that, I don't think this flag can possibly be changing anything.\r\n\r\nCan you confirm that you have looked carefully through the logs when running with CUDA_LAUNCH_BLOCKING and do not notice anything else suspicious?  Is it possible to post (or send privately) your full log, or if you're not comfortable with that, the log of all lines that start with `W` or `E` (i.e., all warnings and errors)?\r\n\r\n> It seems to be when I have opt.compute_gradients(loss, params) compiled into XLA code that I see a weird 3x speed-up but the loss doesn't decrease\r\n\r\nWeird.\r\n\r\nIt could totally be a bug in XLA (or cudnn), but it may be pretty hard to debug without the ability to reproduce.", "Just to double check, I can gather the logs by setting `tf.logging.set_verbosity('DEBUG')`? - I missed changing this previously..\r\n\r\nI'll try making the exact copy of my script with the mnist_xla script but making use of all things like nccl for multi-gpu.", ">  I can gather the logs by setting tf.logging.set_verbosity('DEBUG')? - I missed changing this previously..\r\n\r\nThat would be sufficient.\r\n\r\n> I'll try making the exact copy of my script with the mnist_xla script but making use of all things like nccl for multi-gpu.\r\n\r\nOK, thanks!", "No errors or warnings when logging and `CUDA_LAUNCH_BLOCKING=1`.\r\n\r\nWorking on a mnist_xla script now ", "Repo: https://github.com/joeyearsley/mnist-xla/tree/master\r\n\r\nWhen running with XLA (`--xla True`) it has a high loss and mediocre accuracy, however, when running without  it gets a consistent high accuracy. ", "@d0k spent some time looking at this.  He was able to reproduce the high loss, with both XLA:GPU *and* XLA:CPU.  Still unclear what's going on.\r\n\r\nOne question for you is, do you have a way we can reproduce the `Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)` error?  That may be separate.", "I\u2019ll try reproducing the error with the mnist example.\r\n\r\nWe\u2019ve also been seeing weird things with TF 1.10 (compared to 1.9) and cuda streams but that\u2019ll be another issue created after a bit more investigation - the CUDA error might be more related to that.", "Trying to reproduce the CUDNN error, so far no luck as it is extremely stochastic.\r\n\r\nSeems to be happening more frequently when there's a batch size which causes:\r\n`Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.06GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.`\r\n\r\nand when there are multiple GPUs.\r\n\r\nFinding it hard to make a reproducible example.", "Anymore information on the XLA not working?", "We haven't forgotten about this, but I don't have an update.\n\nI don't know what's going wrong for you; a bunch of people are using XLA\ninternally without issue.  It's pretty surprising.\n\nOn Fri, Sep 28, 2018 at 7:35 AM Joe Yearsley <notifications@github.com>\nwrote:\n\n> Anymore information on the XLA not working?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22119#issuecomment-425453273>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAJMhyyuF_j_61OrW7IRbX8ntMgxbxXQks5ufjOugaJpZM4WdaOL>\n> .\n>\n", "Ok, thank you for the update!", "@jlebar I notice that from TF 1.12 that it will be built with XLA by default.\r\n\r\nIs this really a good idea with a bug such as this is still unsolved?", "> @jlebar I notice that from TF 1.12 that it will be built with XLA by default.\r\n> \r\n> Is this really a good idea with a bug such as this is still unsolved?\r\n\r\nIt *builds* with XLA, it doesn't *run* with XLA by default. I'm quite unhappy with this bug still being around, but failed to ever reproduce the crash you're seeing.", "Xla will be built, but it will not be enabled in models unless you ask for\nthat.\n\nOn Fri, Oct 19, 2018, 12:45 PM Joe Yearsley <notifications@github.com>\nwrote:\n\n> @jlebar <https://github.com/jlebar> I notice that from TF 1.12 that it\n> will be built with XLA by default.\n>\n> Is this really a good idea with a bug such as this is still unsolved?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22119#issuecomment-431475854>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAJMh0cl-c2pDpq5zMM0mX-0RUYGSx3iks5umivhgaJpZM4WdaOL>\n> .\n>\n", "Fair points. \r\n@d0k do you have any hypothesis of what the bug could be? \r\n\r\nI\u2019ve ignored the crash for now as I\u2019m putting it down to some weird clash in the environment. ", "> @d0k do you have any hypothesis of what the bug could be?\r\n\r\nThe status you're seeing is CUDNN_STATUS_MAPPING_ERROR, which hints at an internal error in cudnn, so the bug is most likely there. But it's hard to say, there are a lot of moving parts.", "@tfboyd I noticed from the mailing list that you are experimenting a lot with XLA atm. Could you please try running the mnist-XLA repo I\u2019ve posted above, do you see the same problems as myself? ", "I think I can make time to do that since I have a setup. Added myself to the assignee list for more visual nagging.  :-)  I run the ResNet example with tf.cnn.benchmarks a lot but I am using CUDA 10 because of the ptxas 9.0 vs 9.2 problem.  If you want to use CUDA 10 I and others have put up some binaries.  We will move TF nightly to CUDA 10 in mid/late November and then release it in January.  I wanted to go earlier but the timing did not work out.  \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/22706", "Thanks, I tried running the TF docker image for 1.12 expecting it to have XLA and the cherry picked ptxas but unfortunately it doesn't. \r\n\r\nI've raised this in #23252 . \r\n\r\nI'll try CUDA 10 later today with your binaries, need to find time to update my driver.", "@joeyearsley  The docker image does not have the correct XLA due to some difficulties.  I need to see if I can pull it off with just apt-get, I cannot just copy the files over due to process and previous legal issues that may be getting resolved.", "Ah ok, thanks for the help and updates!", "@joeyearsley,\r\n\r\nWe are checking to see if this is still an issue. Can you try running `docker pull tensorflow/tensorflow` to get the latest stable version of TF, and let us know if you face the XLA error with new docker images? You can take a look at this [guide](https://www.tensorflow.org/install/docker) for your reference.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22119\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22119\">No</a>\n"]}, {"number": 22118, "title": "tf.flags.FLAGS bug for verison 1.10.0, got the message of  UnrecognizedFlagError: Unknown command line flag 'f'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Nagging Assignee @rohan100jain: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22117, "title": "Bad output for sqrt?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:\r\nv1.10.1-0-g4dcfddc5d1 1.10.1\r\nI used pip3 install tensorflow-gpu\r\n- **Python version**:Python 3.5.2 (default, Nov 23 2017, 16:37:01) \r\n[GCC 5.4.0 20160609] on linux\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n9.0.176/7.0.5\r\n- **GPU model and memory**:GeForce GTX 1050 Ti/PCIe/SSE2 4GB\r\n- **Exact command to reproduce**:\r\nsee below\r\n\r\n### Describe the problem\r\nIf i run the below script with tf 1.10.1 it erroneously prints:\r\n[0.58032876 1.1248689  0.685996  ] [1.1606575 2.2497377 1.371992 ]\r\nWhile when using tf 1.8 it outputs the mathematically correct:\r\n[0.58032876 1.1248689  0.685996  ] [0.8615806  0.44449627 0.72886723]\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\ndef test_unary_sqrt_with_gradients():\r\n    floats = tf.placeholder(dtype=tf.float32, shape=[3], name='floats')\r\n    output0 = tf.sqrt(floats)\r\n    const1 = tf.constant(value=[1.0], dtype=tf.float32, shape=[3])\r\n    mul1 = tf.multiply(const1, y=0.5)\r\n    grad_floats = tf.divide(mul1, output0)\r\n\r\n    return [grad_floats, output0]\r\n\r\ngrad, out = test_unary_sqrt_with_gradients()\r\nwith tf.Session() as sess:\r\n    [grad_, out_] = sess.run([grad, out], feed_dict={'floats:0': [0.7423212,  0.19757693, 0.53124744]})\r\n    print(grad_, out_)\r\n\r\n```", "comments": ["Thank you very much for reporting this bug. We are working on a fix.", "Nagging Assignee @bignamehyp: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Bug fixed and it would be released soon."]}, {"number": 22116, "title": "[1.10.1] op_level_cost_estimator.cc:404] Check failed: 0 < gflops (0 vs. 0)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: debian experimental\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: not related\r\n- **Bazel version (if compiling from source)**: not related\r\n- **GCC/Compiler version (if compiling from source)**: gcc8\r\n- **CUDA/cuDNN version**: not related\r\n- **GPU model and memory**: not related\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nI built tensorflow from source with a customized build system instead of bazel. https://salsa.debian.org/science-team/tensorflow . I can successfully build libtensorflow_cc.so according to the bazel query result. However when running `benchmark_model` test I encountered this problem:\r\n\r\n```\r\n$ LD_LIBRARY_PATH=. ./tf_benchmark_model --graph=../tensorflow_inception_graph.pb --num_threads=1\r\n2018-09-06 15:45:23.316799: I tensorflow/tools/benchmark/benchmark_model.cc:469] Graph: [../tensorflow_inception_graph.pb.pb]\r\n2018-09-06 15:45:23.316843: I tensorflow/tools/benchmark/benchmark_model.cc:470] Init ops:\r\n2018-09-06 15:45:23.316848: I tensorflow/tools/benchmark/benchmark_model.cc:471] Input layers: [input:0]\r\n2018-09-06 15:45:23.316852: I tensorflow/tools/benchmark/benchmark_model.cc:472] Input shapes: [1,224,224,3]\r\n2018-09-06 15:45:23.316855: I tensorflow/tools/benchmark/benchmark_model.cc:473] Input types: [float]\r\n2018-09-06 15:45:23.316859: I tensorflow/tools/benchmark/benchmark_model.cc:474] Output layers: [output:0]\r\n2018-09-06 15:45:23.316862: I tensorflow/tools/benchmark/benchmark_model.cc:475] Target layers: []\r\n2018-09-06 15:45:23.316868: I tensorflow/tools/benchmark/benchmark_model.cc:476] Num runs: [1000]\r\n2018-09-06 15:45:23.316874: I tensorflow/tools/benchmark/benchmark_model.cc:477] Inter-inference delay (seconds): [-1.0]\r\n2018-09-06 15:45:23.316898: I tensorflow/tools/benchmark/benchmark_model.cc:478] Inter-benchmark delay (seconds): [-1.0]\r\n2018-09-06 15:45:23.316903: I tensorflow/tools/benchmark/benchmark_model.cc:480] Num threads: [1]\r\n2018-09-06 15:45:23.316907: I tensorflow/tools/benchmark/benchmark_model.cc:481] Benchmark name: []\r\n2018-09-06 15:45:23.316911: I tensorflow/tools/benchmark/benchmark_model.cc:482] Output prefix: []\r\n2018-09-06 15:45:23.316915: I tensorflow/tools/benchmark/benchmark_model.cc:483] Show sizes: [0]\r\n2018-09-06 15:45:23.316932: I tensorflow/tools/benchmark/benchmark_model.cc:484] Warmup runs: [1]\r\n2018-09-06 15:45:23.316937: I tensorflow/tools/benchmark/benchmark_model.cc:251] Loading TensorFlow.\r\n2018-09-06 15:45:23.316950: I tensorflow/tools/benchmark/benchmark_model.cc:258] Got config, 0 devices\r\n2018-09-06 15:45:23.317031: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-09-06 15:45:23.380132: I tensorflow/tools/benchmark/benchmark_model.cc:496] Initialized session in 0.063179s\r\n2018-09-06 15:45:23.380206: I tensorflow/tools/benchmark/benchmark_model.cc:327] Running benchmark for max 1 iterations, max -1 seconds without detailed stat logging, with -1s sleep between inferences\r\n2018-09-06 15:45:23.516527: F tensorflow/core/grappler/costs/op_level_cost_estimator.cc:404] Check failed: 0 < gflops (0 vs. 0)type: \"CPU\"\r\nvendor: \"GenuineIntel\"\r\nmodel: \"110\"\r\nnum_cores: 4\r\nenvironment {\r\n  key: \"cpu_instruction_set\"\r\n  value: \"SSE, SSE2\"\r\n}\r\nenvironment {\r\n  key: \"eigen\"\r\n  value: \"3.3.90\"\r\n}\r\nl1_cache_size: 32768\r\nl2_cache_size: 262144\r\nl3_cache_size: 6291456\r\nmemory_size: 8558383104\r\n\r\nAborted\r\n```\r\n\r\nthe `tensorflow_inception_graph.pb` files comes from `tensorflow/contrib/makefile/README.md`\r\n\r\n### Source code / logs\r\n\r\nhttps://salsa.debian.org/science-team/tensorflow\r\n\r\nFor error log see above.\r\n", "comments": ["The cause of this problem is that `device.frequency()` returned `0`.\r\n```c++\r\nt/c/g/c/op_level_cost_estimator.cc \ue0b0                               \ue0b2\ue0b2 [buffers] \r\n 363 \r\n 364   if (device.type() == \"CPU\") {\r\n 365     // Check if vector instructions are available, and refine performance\r\n 366     // prediction based on this.\r\n 367     // Frequencies are stored in MHz in the DeviceProperties.\r\n 368     gflops = device.num_cores() * device.frequency() * 1e-3;\r\n 369     if (gb_per_sec < 0) {\r\n 370       if (device.bandwidth() > 0) {\r\n 371         gb_per_sec = device.bandwidth() / 1e6;\r\n 372       } else {\r\n 373         gb_per_sec = 32;\r\n 374       }\r\n 375     }\r\n```", "It seems that I have a similar problem.\r\n`device.frequency()` returned `0` probably because `NominalCPUFrequency()` returned `1`. `TENSORFLOW_USE_ABSL` turns on abseil support and it is used only in Bazel-based compilation.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.10.1/tensorflow/core/platform/posix/port.cc#L196 :\r\n```\r\ndouble NominalCPUFrequency() {\r\n#ifdef TENSORFLOW_USE_ABSL\r\n  return absl::base_internal::NominalCPUFrequency();\r\n#else\r\n  return 1.0;\r\n#endif\r\n}\r\n```\r\n\r\nFixed here for Makefile build but not for cmake build: https://github.com/tensorflow/tensorflow/commit/75adfa5ca180abdd1ccf9408078906e5874c9e2a", "I have similar issue..\r\nI am getting following error:\r\n\r\n`\r\nI tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)`\r\n`W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz`\r\n`F tensorflow/core/grappler/costs/op_level_cost_estimator.cc:710] Check failed: 0 < gflops (0 vs. 0)type: \"CPU\"\r\n`\r\n\r\nI am using mac with M1 chip. I don't understand these errors much. Is there any way to fix it from my end?", "Same issues. How is going on guys?\r\n\r\nI also use mac with M1 chip.", "> Same issues. How is going on guys?\r\n> \r\n> I also use mac with M1 chip.\r\n\r\nJust need to run it:\r\n```\r\nexport CUDA_VISIBLE_DEVICES=\"\"\r\n```", "Have a look at https://stackoverflow.com/questions/67379519/cannot-use-keras-models-on-mac-m1-with-bigsur", "> > Same issues. How is going on guys?\r\n> > I also use mac with M1 chip.\r\n> \r\n> Just need to run it:\r\n> \r\n> ```\r\n> export CUDA_VISIBLE_DEVICES=\"\"\r\n> ```\r\n\r\nCould you talk about where to run it?", "Have you checked this [thread ](https://developer.apple.com/metal/tensorflow-plugin/)on installing  Tensorflow in MacOs?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 22115, "title": "Feature Request: fp16 support for tf.contrib.image.transform on gpu", "body": "### System information\r\n- **Have I written custom code**: yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **Mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.10.0-0-g656e7a2b34', '1.10.0')\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**:  9.0  /  7.1.4.18-1\r\n- **GPU model and memory**: GeForce GTX 1080 8097MB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n`tf.contrib.image.transform` does not support fp16 on GPU. \r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\n\r\nsession_config = tf.ConfigProto(\r\n    allow_soft_placement=False,\r\n    log_device_placement=True)\r\n\r\nwith tf.device(\"/device:GPU:0\"):\r\n  images =tf.random_normal([512,512], dtype=tf.float16)\r\n  y = tf.contrib.image.transform(images,[1]*8)\r\n  with tf.Session(config=session_config) as sess:\r\n    print(sess.run(y))\r\n```\r\n \r\n```\r\nCaused by op u'transform/ImageProjectiveTransform', defined at:\r\n  File \"./test.py\", line 11, in <module>\r\n    y = tf.contrib.image.transform(images,[1]*8)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/image/python/ops/image_ops.py\", line 269, in transform\r\n    images, transforms, interpolation=interpolation.upper())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/image/ops/gen_image_ops.py\", line 235, in image_projective_transform\r\n    interpolation=interpolation, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'transform/ImageProjectiveTransform': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='GPU'; dtype in [DT_DOUBLE]\r\n  device='GPU'; dtype in [DT_FLOAT]\r\n  device='GPU'; dtype in [DT_INT64]\r\n  device='GPU'; dtype in [DT_INT32]\r\n  device='GPU'; dtype in [DT_UINT8]\r\n  device='CPU'; dtype in [DT_DOUBLE]\r\n  device='CPU'; dtype in [DT_FLOAT]\r\n  device='CPU'; dtype in [DT_HALF]\r\n  device='CPU'; dtype in [DT_INT64]\r\n  device='CPU'; dtype in [DT_INT32]\r\n  device='CPU'; dtype in [DT_UINT8]\r\n\r\n\t [[Node: transform/ImageProjectiveTransform = ImageProjectiveTransform[dtype=DT_HALF, interpolation=\"NEAREST\", _device=\"/device:GPU:0\"](transform/strided_slice, transform/strided_slice_1)]]\r\n```\r\n", "comments": ["Added #22122 for float16 support on GPU.", "This seems very reasonable. @reedwm I'm guessing you might care about this?", "Nagging Assignee @robieta: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 22114, "title": "[Keras / Cloud TPU]: Correct indexing for software pipelining.", "body": "PiperOrigin-RevId: 211724843", "comments": []}]