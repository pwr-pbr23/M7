[{"number": 25330, "title": "Generalize MinMax monotonic optimizer", "body": "This PR generalizes the MinMax monotonic optimizer to support `SegmentMax`, `UnsortedSegmentMax` and `ArgMax` operations.\r\n\r\nThis will improve performance of such operations after monotonic function.", "comments": ["Thanks for the fast merge! You might want to check out #25332 too, which adds support for more monotonic ops.", "@ezhulenev Can you tell me the reason why this PR was reverted in 0507dba17e223053f7d7f19e8e546ca6a97b1d8b?\r\n\r\nIt would be great to get feedback so I can fix it.", "@lgeiger it failed in https://gist.github.com/ezhulenev/0b8ab39e1533933197a61f23675e719b\r\n\r\n```AssertionError: 0.5 != 0.90499997 within 2 places```\r\n\r\nSorry didn't have time to dig into it myself, had to rollback to unblock other teams. Probably something simple.", "No worries, thanks for the link. I'll take a look.", "It seems like my changes are fine but surfaced a bug in the existing algorithmic optimizer.\r\nI made a PR to fix this: #25535\r\n\r\nI'll reopen this PR after #25535 and #25438 are merged or rejected."]}, {"number": 25329, "title": "tensorflow-gpu 1.13.0rc0 requires minimum CUDA compute capability 6.0", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.13.0rc0\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 10.0, CUDNN 7.4.1\r\n- GPU model and memory: GeForce GTX 980 Ti\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nThe 1.13 release candidate requires a minimum compute capability of 6.0 (so it ignores lower-level devices).  Is this an intentional change that we can expect to see in the full release? Or just an artifact of the release candidate build?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nfrom tensorflow.python.client import device_lib\r\ndevice_lib.list_local_devices()\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n2019-01-30 14:29:07.690292: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-01-30 14:29:07.883552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:\r\nname: GeForce GTX 980 Ti major: 5 minor: 2 memoryClockRate(GHz): 1.291\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 6.00GiB freeMemory: 4.97GiB\r\n2019-01-30 14:29:07.888807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Ignoring visible gpu device (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0, compute capability: 5.2) with Cuda compute capability 5.2. The minimum required Cuda capability is 6.0.\r\n2019-01-30 14:29:07.894771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-30 14:29:07.898191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0\r\n2019-01-30 14:29:07.901776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 8036038049545966896\r\n]\r\n```", "comments": ["I think I'm experiencing a similar issue, on 1.13.0rc0. I compiled TensorFlow with [this](https://pastebin.com/csWDkhvU) script on Ubuntu 18.04.\r\n\r\n```\r\n>>> from tensorflow.python.client import device_lib\r\n>>> device_lib.list_local_devices()\r\n2019-01-30 22:09:44.883940: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394450000 Hz\r\n2019-01-30 22:09:44.884404: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55e57095ebb0 executing computations on platform Host. Devices:\r\n2019-01-30 22:09:44.884426: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-01-30 22:09:44.930354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-01-30 22:09:44.931240: I tensorflow/compiler/xla/service/platform_util.cc:194] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0\r\n2019-01-30 22:09:44.931352: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\r\n```", "@gunan Can you  PLTAL? The cuda compute capability is 3.5 or higher for TF-GPU support right? Thanks\r\n", "I think our windows package has this problem. This is a duplicate of #18652\r\n@EWouters unfortunately in your case your GPU is too old. for 3.0, you will need to rebuild TF.", "I think this is a different issue.  If I'm understanding 18652 correctly, the problem there is that compute capabilities > 6.0 had a performance issue (dating back to TF 1.7).  The problem here is that GPUs with compute capability < 6.0 are no longer supported (can't use the GPU at all).  And this is a new issue introduced in TF 1.13.", "What happened is, looks like our builders are only building for the GPU they have.\r\nThey used to have K80s, so anything never than K80 was slow to startup waiting for JIT compiler.\r\nBut we switched our builders to P100s a while ago. Anything newer than P100 still waits for JIT compilation, but older ones are cannot do this so they fail.\r\n\r\neither way, the solution is for us to fix the CUDA capabilities we need to build for.", "Seeing this error in 2.0-preview '2.0.0-dev20190130' too, \"The minimum required Cuda capability is 6.0.\" - my GPU is 5.2, but that's definitely faster than my CPU.", "We just checked in a change to update our windows build bots to include more cuda compute capabilities.\r\nIf you upgrade to a dev version 20190205, it should be resolved.\r\nThis both applies to 2.0 preview and tf-nightly.\r\n\r\n1.13.0rc1 will also have more compute capabilities.", "Thanks! yep that's working!\n\nOn Wed, 6 Feb 2019 at 02:54, Gunhan Gulsoy <notifications@github.com> wrote:\n\n> We just checked in a change to update our windows build bots to include\n> more cuda compute capabilities.\n> If you upgrade to a dev version 20190205, it should be resolved.\n> This both applies to 2.0 preview and tf-nightly.\n>\n> 1.13.0rc1 will also have more compute capabilities.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25329#issuecomment-460735683>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEirHyravyj7e-2OCmmEyXeblaFlNWevks5vKcU7gaJpZM4aa0OF>\n> .\n>\n", "Had the same problem with the CUDA 6.0 minimum requirement with the 1.12.0 Java libs from Maven Central. As a temporary workaround, a downgrade to 1.11.0 fixed the issue. Please replace the 1.12.0 JARs in Maven central.", "The pre-build binaries ([`libtensorflow-gpu-linux-x86_64-1.13.1.tar.gz`](https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.13.1.tar.gz)) for the C API have the same limitation:\r\n```\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce GTX 970 major: 5 minor: 2 memoryClockRate(GHz): 1.304\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.94GiB freeMemory: 2.81GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Ignoring visible gpu device (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2) with Cuda compute capability 5.2. The minimum required Cuda capability is 6.0.\r\n```", "I have the same issue (The minimum required Cuda capability is 6.0) with a bazel built tensorflow 1.13 for c++ (libtensorflow_cc.so)", "I would like to confirm presence of this regression in r1.13.1 \r\nI've built TF 1.13.1 from sources yesterday, specifically for Cuda3.0 support.\r\nAfter install, following check failed the same way as for many people in this thread.\r\n```python\r\nfrom tensorflow.python.client import device_lib\r\ndevice_lib.list_local_devices()\r\n```\r\nI've got **StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0** followed by python crash.\r\n```\r\n2019-05-07 21:19:04.588288: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394450000 Hz\r\n2019-05-07 21:19:04.588828: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2db6890 executing computations on platform Host. Devices:\r\n2019-05-07 21:19:04.588859: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-05-07 21:19:04.618339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-05-07 21:19:04.618854: I tensorflow/compiler/xla/service/platform_util.cc:194] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0\r\n2019-05-07 21:19:04.618926: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\r\nAborted (core dumped)\r\n```\r\n\r\nThen I've found this thread and tried to follow the same way.\r\n\r\nI've checked out **r1.12.2**, apply same settings and build it over night.\r\nSame test is passing without error.\r\n```\r\n2019-05-08 13:26:11.192724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 1290 MB memory) -> physical GPU (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0, compute capability: 3.0)\r\n```", "System reboot worked for me", "@drasmuss Is this still an issue? Thanks!", "This was fixed after this change https://github.com/tensorflow/tensorflow/issues/25329#issuecomment-460735683 as far as I've seen.", "@drasmuss, not true, https://repo1.maven.org/maven2/org/tensorflow/libtensorflow_jni_gpu/1.14.0/libtensorflow_jni_gpu-1.14.0.jar was published with 6.0 requirements.", "That seems like a problem specific to the Java implementation, which is different than what is described in this issue.", "Same issue, which is just a bit more broadly spread out in the build infrastructure than the initial report.", "Then I'd recommend making a new issue specific to your problem, since it doesn't match the initial report here, to avoid confusion and help your problem get resolved faster. But up to you \ud83d\ude04 ", "@drasmuss Thanks for confirmation. Closing the issue since its fixed.\r\n@mikaelhg, Can you please post a new issue by providing the information asked by the template?\r\nThe reason for this is we can focus on your specific configuration and problem since the root cause can be unrelated even though the error messages are similar. Thanks!", "> The pre-build binaries ([`libtensorflow-gpu-linux-x86_64-1.13.1.tar.gz`](https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.13.1.tar.gz)) for the C API have the same limitation:\r\n> \r\n> ```\r\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\n> name: GeForce GTX 970 major: 5 minor: 2 memoryClockRate(GHz): 1.304\r\n> pciBusID: 0000:01:00.0\r\n> totalMemory: 3.94GiB freeMemory: 2.81GiB\r\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Ignoring visible gpu device (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2) with Cuda compute capability 5.2. The minimum required Cuda capability is 6.0.\r\n> ```\r\n\r\n@christian-rauch I have now opened an issue for this: https://github.com/tensorflow/tensorflow/issues/36853 (although for the 1.14 and 1.15 releases)"]}, {"number": 25328, "title": "TFTRT: Add helper method to validate input count/tensor or weight", "body": "This helper function removes a lot of duplicate code in the conversion functions. It will check that the number of inputs is correct, and it will check that each input is either a tensor or weight depending on what you specify.\r\n\r\nFuture enhancements:\r\nIn TF-TRT, we can convert a weight to a tensor using ConstLayer, but there is no way to get a weight from a tensor. So CheckInputWeights only needs to be able to require that an input is a weight (for certain layers which require weights). For other inputs which we need tensors, we should add a method, something like `TRT_TensorOrWeights::as_tensor()` which uses ConstLayer to automatically convert to tensor if needed. But we will need some sort of caching mechanism to prevent duplicate constants.", "comments": ["@smit-hinsu ", "Just realized I will have to update all of the error messages in convert_nodes_test, I will work on that.\r\nEdit: done.", "@trevor-m  Can you rebase to resolve the merging conflicts."]}, {"number": 25327, "title": "Error when restoring model using an Embedding layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: RTX 2070 (8GB)\r\n\r\n**Describe the current behavior**\r\nAttempts to restore a frozen Keras model including an \"Embedding\" layer fail. The same problem appears when `tf.graph_util.convert_variables_to_constants()` and `tf.import_graph_def()` are used.\r\nThe error is:\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-11-95d358f9d586>\", line 3, in <module>\r\n    tf.import_graph_def(sg)\r\n\r\n  File \"/home/werner/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n\r\n  File \"/home/werner/.local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 431, in import_graph_def\r\n    raise ValueError(str(e))\r\n\r\nValueError: Input 0 of node import/state/embedding_lookup was passed float from import/state/embeddings:0 incompatible with expected resource.\r\n```\r\nI tried:\r\n* Saving the model with `tensorflow.saved_model.simple_save()`, then running the `freeze_graph` script to generate a frozen model, then loading the frozen model again in C++ / Python. The same error.\r\n\r\n* `convert_variables_to_constants()`, and ` tf.import_graph_def()` the graph -> same error.\r\n\r\n**Code to reproduce the issue**\r\n\r\nMinimal model:\r\n```\r\nfrom tensorflow.keras.layers import Embedding, Dense, Input\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras import backend as K\r\nimport tensorflow as tf\r\n\r\nsinput = Input(shape=(1,),  name='state_input', dtype='int32')\r\nsinput_em = Embedding(output_dim=16, input_dim=100, input_length=1, name=\"state\")(sinput)\r\nmodel = Model(inputs=[sinput], outputs=[sinput_em]) \r\nmodel.compile(loss={'state': 'sparse_categorical_crossentropy' },   optimizer='adam')\r\n```\r\n\r\n* Steps when `save_model()` is used:\r\n\r\n```\r\n### Save model\r\nfrom tensorflow.saved_model import simple_save\r\nsimple_save(K.get_session(),\r\n            \"SavedModel/testembedding\",\r\n            inputs={\"state_input\": sinput },\r\n            outputs={'state': sinput_em})\r\n# then run the script:\r\n#  ~/.local/bin/freeze_graph  --input_saved_model_dir=SavedModel/testembedding --output_graph=frozenemb.pb  --output_node_names=state/embedding_lookup/Identity_1 --clear_devices\r\n\r\n# try to load again:\r\nfrozen_graph=\"frozenemb.pb\"\r\nwith tf.gfile.GFile(frozen_graph, \"rb\") as f:\r\n    restored_graph_def = tf.GraphDef()\r\n    restored_graph_def.ParseFromString(f.read())\r\n\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    tf.import_graph_def(\r\n        restored_graph_def,\r\n        input_map=None,\r\n        return_elements=None,\r\n        name=\"\"\r\n        )\r\n\r\n# -> error in import_graph_def()\r\n```\r\n\r\n* OR steps when `convert_variables_to_constants()` is used:\r\n\r\n```\r\ndef serialize_graph(model):\r\n    g = tf.graph_util.convert_variables_to_constants(\r\n        tf.keras.backend.get_session(),\r\n        tf.keras.backend.get_session().graph.as_graph_def(),\r\n        #[n.name for n in tf.keras.backend.get_session().graph.as_graph_def().node],\r\n        [t.op.name for t in model.outputs]\r\n    )\r\n    return g\r\n\r\nsg = serialize_graph(model)\r\n\r\nnewg = tf.Graph()\r\nwith newg.as_default():\r\n   tf.import_graph_def(sg) # -> error\r\n   \r\n```\r\n\r\n**Other info / logs**\r\nSeems to be related to #21889 (the error message is different)\r\n", "comments": ["Can you try using the [tf.keras.experimental.export](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/experimental/export)? This handles Keras models more gracefully.", "Hi, thanks for the suggestion!\r\nI tried `tf.keras.experimental.export`, and the export itself works fine. However, the same error remains when I try to load the graph after freezing. Here is my code:\r\n\r\n```\r\n# define the model, the one used now has an additional softmax layer, then:\r\nsaved_to_path = tf.keras.experimental.export(\r\n      model, 'SavedModel/testembedding')\r\n\r\n# import works fine\r\n# Load the saved keras model back.\r\nmodel_prime = tf.keras.experimental.load_from_saved_model(saved_path_model)\r\nmodel_prime.summary()\r\n\r\n# freezing:\r\nfrom tensorflow.python.tools import freeze_graph\r\nfrom tensorflow.python.saved_model import tag_constants\r\n\r\ninput_saved_model_dir = saved_to_path\r\noutput_graph_filename = \"SaveFiles/testemb.pb\"\r\noutput_node_names = \"out/Softmax\"  \r\ninput_binary = False\r\ninput_saver_def_path = False\r\nrestore_op_name = None\r\nfilename_tensor_name = None\r\nclear_devices = False\r\ninput_meta_graph = False\r\ncheckpoint_path = None\r\ninput_graph_filename = None\r\nsaved_model_tags = tag_constants.SERVING\r\n\r\nfreeze_graph.freeze_graph(input_graph_filename, input_saver_def_path,\r\n                            input_binary, checkpoint_path, output_node_names,\r\n                              restore_op_name, filename_tensor_name,\r\n                              output_graph_filename, clear_devices, \"\", \"\", \"\",\r\n                              input_meta_graph, input_saved_model_dir,\r\n                            saved_model_tags)\r\n\r\n# load again, -> ERROR: ValueError: Input 0 of node state/embedding_lookup was passed float from state/embeddings:0 incompatible with expected resource.\r\nfrozen_graph=\"SaveFiles/testemb.pb\"\r\nwith tf.gfile.GFile(frozen_graph, \"rb\") as f:\r\n    restored_graph_def = tf.GraphDef()\r\n    restored_graph_def.ParseFromString(f.read())\r\n\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    tf.import_graph_def(\r\n        restored_graph_def,\r\n        input_map=None,\r\n        return_elements=None,\r\n        name=\"\"\r\n        )\r\n\r\n```\r\n\r\nAny more ideas? ", "@werner-rammer , I am getting the same error, did you find a solution for this?", "@disha3 , unfortunately not! Bad thing is that in the not-too-distant future I'll really need this ...", "cebce4a should fix this issue. Please reopen if it's still an issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25327\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25327\">No</a>\n", "Thanks! When I use the changed  version of `graph_util_impl.py` (by copying the file to my local pyhton lib) it works!\r\nLooks as if the mentioned commit above fixes the issue.\r\nThanks again!", "Can you please merge this to master, it's been since April 18, the merge is tagged with 1.12.1 but it's not even in master, so we can't verify or even use graph freezing (if it works, I don't know if it does).\r\n\r\nWhat happened to this PR?\r\n\r\nWhy is it only in this \"testkevinbonz repo\"?\r\n\r\nThis is a breaking bug that makes it impossible to freeze saved_models that contain embeddings, so much for the \"keras by default\", also, the bug breaks BERT for me", "> Thanks! When I use the changed version of `graph_util_impl.py` (by copying the file to my local pyhton lib) it works!\r\n> Looks as if the mentioned commit above fixes the issue.\r\n> Thanks again!\r\n\r\nchanged version of `graph_util_impl.py`\uff1fwhich vesion\uff1f i have same problem need your help\u3002", "Hi,\r\nwell I simply copied from the github-repo:\r\nhttps://github.com/tensorflow/tensorflow/blob/cebce4a2b5e33a06a1c92772008082895568f10a/tensorflow/python/tools/freeze_graph.py\r\n\r\nand updated the file locally in the python lib (maybe just search for freeze_graph.py on your machine).\r\n", "problem solved -- replace local graph_util_impl.py with https://github.com/tensorflow/tensorflow/blob/cebce4a2b5e33a06a1c92772008082895568f10a/tensorflow/python/framework/graph_util_impl.py"]}, {"number": 25326, "title": "Binary prediction with LSTM", "body": "I'm trying to implement an LSTM model to make binary (or multiclass) classification from raw log data(Mooc courses log data -> user-level droput/grade prediction ). I have read lots of publication and tutorials which seems to be what I'm looking for, but couldn't find any example on how to use it.\r\n\r\nDo you have a link or something about this topic? (RNN, ConvLSTM2D, LSTM, GRU on Keras or TF)", "comments": ["This can be a good [document](https://www.tensorflow.org/tutorials/keras/basic_text_classification) to start binary text classification problems."]}, {"number": 25325, "title": "Custom metric mix train and validation data", "body": "**System information**\r\n\r\n- Linux ubuntu, CPU\r\n- installed packages using anaconda with python 3.6\r\n- Tensorflow version v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Keras version  2.2.4\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using a custom metric, the metric is computed on training data and validation data together.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe train metric should be on train data only and validation metric (*_val) should be on validation data only.\r\n\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 25324, "title": "Keras optimizer.adam is not usable with Eager Execution", "body": "**System information**\r\n- See the custom complete snippet below\r\n- Linux Ubuntu 16.04\r\n- TensorFlow installed via pip gpu version \r\n- TensorFlow version v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: 9.0.176 / 7.4.1\r\n- GPU model and memory: GeForce GTX 1080  with 11178MiB\r\n\r\n**Describe the current behavior**\r\nIf using `tf.enable_eager_execution()` the code expects non Keras optimizer,\r\nif using \"standard\" behavior (compiled graph) the code runs fine.\r\nSee the code and logs below.\r\n\r\n**Describe the expected behavior**\r\n`tf.enable_eager_execution()` should not affect the behavior and the code should run in both cases.\r\n\r\n**Code to reproduce the issue**\r\nCopied from my repository https://github.com/oplatek/tf-eager-playground/blob/master/eagerplayer/test_train.py\r\n\r\n```\r\n#!/usr/bin/env python3\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sys\r\n\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.models import Model\r\n\r\n\r\ndef fit_keras_model():\r\n    data = np.random.random((1000, 32))\r\n    labels = np.random.random((1000, 10))\r\n    # source https://keras.io/getting-started/functional-api-guide/\r\n    # This returns a tensor\r\n    inputs = Input(shape=(32,))\r\n\r\n    # a layer instance is callable on a tensor, and returns a tensor\r\n    x = Dense(64, activation='relu')(inputs)\r\n    x = Dense(64, activation='relu')(x)\r\n    predictions = Dense(10, activation='softmax')(x)\r\n\r\n    # This creates a model that includes\r\n    # the Input layer and three Dense layers\r\n    model = Model(inputs=inputs, outputs=predictions)\r\n    model.compile(optimizer='adam',\r\n                  loss='categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\n    model.fit(data, labels)  # starts training\r\n    return model\r\n\r\n\r\nif __name__ == '__main__':\r\n    if len(sys.argv) > 1 and sys.argv[1] == 'eager':\r\n        tf.enable_eager_execution()  # fails with eager execution enabled\r\n    fit_keras_model()\r\n    print('success')\r\n```\r\n\r\n**Other info / logs**\r\nThe logs are copied also from my repository https://github.com/oplatek/tf-eager-playground/blob/master/eagerplayer/test_train.md\r\n\r\nWithout eager execution the code works fine\r\n-------------------------------------------\r\n\r\n\toplatek@gpu:master:tf-eager-playground$ ./eagerplayer/test_train.py\r\n\tEpoch 1/1\r\n\t2019-01-30 16:49:23.921582: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n\t2019-01-30 16:49:24.032120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n\t2019-01-30 16:49:24.032526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\n\tname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6575\r\n\tpciBusID: 0000:01:00.0\r\n\ttotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n\t2019-01-30 16:49:24.032540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n\t2019-01-30 16:49:24.230759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n\t2019-01-30 16:49:24.230787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n\t2019-01-30 16:49:24.230792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n\t2019-01-30 16:49:24.230947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10407 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n\t1000/1000 [==============================] - 1s 745us/step - loss: 11.4860 - acc: 0.0810\r\n\tsuccess\r\n\r\n\r\nWith eager execution non Keras optimizer is expected\r\n----------------------------------------------------\r\n\r\n\toplatek@gpu:master:tf-eager-playground$ ./eagerplayer/test_train.py eager\r\n\t2019-01-30 16:49:33.983911: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n\t2019-01-30 16:49:34.069258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n\t2019-01-30 16:49:34.069657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\n\tname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6575\r\n\tpciBusID: 0000:01:00.0\r\n\ttotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n\t2019-01-30 16:49:34.069671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n\t2019-01-30 16:49:34.266286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n\t2019-01-30 16:49:34.266311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n\t2019-01-30 16:49:34.266316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n\t2019-01-30 16:49:34.266465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10407 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n\tTraceback (most recent call last):\r\n\t  File \"./eagerplayer/test_train.py\", line 35, in <module>\r\n\t\tfit_keras_model()\r\n\t  File \"./eagerplayer/test_train.py\", line 27, in fit_keras_model\r\n\t\tmetrics=['accuracy'])\r\n\t  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpointable/base.py\", line 474, in _method_wrapper\r\n\t\tmethod(self, *args, **kwargs)\r\n\t  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\", line 410, in compile\r\n\t\t'a %s' % type(optimizer))\r\n\tValueError: optimizer must be an instance of tf.train.Optimizer, not a <class 'str'>", "comments": ["Possible duplicate to https://github.com/tensorflow/tensorflow/issues/25066 I think the underlying cause may be the same but the symptoms are definitely different.", "Yes, keras optimizers are known not to work with eager execution. This has been fixed in nightly by @tanzhenyu and will make it to 1.14 (I don't think it'll make it to 1.13 as that was cut way too long ago).", "In general I don't think any keras optimizer will support eager execution at 1.12?", "@alextp @tanzhenyu can you point me to docs / explain me the process how can I figure out what is supported with eager execution for each release 1.12, 1.13 etc?\r\n\r\nThank you!"]}, {"number": 25323, "title": "Fail to build from source with gcc 7.3.1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.5.1804 run as image under Singularity and/or Docker on Scientific Linux CERN SLC release 6.10 (Carbon)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master, on commit 8eb3cbcb423c4d840e88a910c5db47404207b8a4\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?: manually compiling using bazel\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): GCC 7.3.1\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the problem**\r\nWhen compiling with gcc 7.3.1 rev 257125 (from gcc.gnu.org/svn) build chokes like this \r\n```\r\nERROR: /tensorflow/tensorflow/core/kernels/BUILD:3206:1: C++ compilation of rule '//tensorflow/core/kernels:reduction_ops' failed (Exit 1)\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:124:0,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/kernels/reduction_ops_common.h:27,\r\n                 from tensorflow/core/kernels/reduction_ops_sum.cc:16:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h: In static member function 'static void std::_Function_handler<void(_ArgTypes ...), _Functor>::_M_invoke(const std::_Any_data&, _ArgTypes&& ...) [with _Functor = Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable, Tileable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2index<0> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer> >; bool Vectorizable = true; bool Tileable = false]::<lambda(Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2index<0> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, false>::StorageIndex, Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2index<0> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, false>::StorageIndex)>; _ArgTypes = {long int, long int}]':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:801:9: internal compiler error: in emit_move_insn, at expr.c:3698\r\n         values[i] = internal::InnerMostDimReducer<Self, Op>::reduce(*this, firstIndex + i * num_values_to_reduce,\r\n         ^~~~~~\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 74.923s, Critical Path: 51.44s\r\nINFO: 2018 processes: 2018 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\nexactly as described here:\r\nhttps://github.com/sylabs/singularity/issues/2536\r\nWhen we changed the \r\ngcc to \r\n8.2.0 tag 2d79333765b691fa27d82c1737cb2f00ec6a4499 (from https://github.com/gcc-mirror/gcc)\r\nand to\r\n7.4.0 rev 268351 (from gcc.gnu.org/svn)\r\nthe build went fine. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\nexport CXX_OPT_FLAGS=-std=c++11\r\nBAZEL_OPTS=\"--output_user_root ../build build -s --verbose_failures -c opt --cxxopt=${CXX_OPT_FLAGS}\"\r\nBAZEL_EXTRA_OPTS=\"--action_env PYTHONPATH=${PYTHON27PATH} --distinct_host_configuration=false\"\r\nbazel $BAZEL_OPTS $BAZEL_EXTRA_OPTS //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This might be gcc's fault, not Tensorflow's.", "With tf 1.12.0 works fine, but with tf 1.13.0-rc2 I got same issue in my project [tensorflow-on-arm](https://github.com/lhelontra/tensorflow-on-arm/)", "@lhelontra please see: https://github.com/tensorflow/tensorflow/issues/24318 and also https://docs.bazel.build/versions/master/command-line-reference.html\r\nI'm trying this just now.", "@mrodozov works with gcc-8.2 only.", "@lhelontra for me it was working with 8.2 also without the extra flags, so local resources flags didn't help. it's something else (gcc 7.3.x related)", "> @lhelontra for me it was working with 8.2 also without the extra flags, so local resources flags didn't help. it's something else (gcc 7.3.x related)\r\n\r\nIt looks like we are reproducing that with GCC 7.2 from Linaro as well. Looks to be confirmed as an upstream GCC issue: https://bugzilla.redhat.com/show_bug.cgi?id=1570308", "Hello,\r\n\r\nI can confirm TensorFlow cannot be built anymore on Debian Stable (Stretch) and Ubuntu LTS (Bionic) due to this \"internal compiler error\":\r\n\r\n```\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h: In static member function 'static void std::_Function_handler<void(_ArgTypes ...), _Functor>::_M_invoke(const std::_Any_data&, _ArgTypes&& ...) [with _Functor = Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable, Tileable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2index<0l> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer> >; bool Vectorizable = true; bool Tileable = false]::<lambda(Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2index<0l> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, false>::StorageIndex, Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2index<0l> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, false>::StorageIndex)>; _ArgTypes = {long int, long int}]':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:801:9: internal compiler error: in emit_move_insn, at expr.c:3547\r\n         values[i] = internal::InnerMostDimReducer<Self, Op>::reduce(*this, firstIndex + i * num_values_to_reduce,\r\n         ^~~~~~\r\n```\r\n\r\nDebian Stable comes with GCC 6.3 while Ubuntu LTS uses 7.3.\r\n\r\nRegards,", "@mrodozov @lhelontra ... what flavor compiler are you using in the module list? I'm using gcc/g++ 8.2 from the ARM website and `/opt/arm/gcc-8.2.0_Generic-AArch64_Ubuntu-16.04_aarch64-linux/bin/gcc` and I still get the error but on a different file/function (`internal compiler error: in emit_move_insn`), whilst on branch v1.13.1 (and bazel 0.19.0)\r\n\r\n```\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/products/GeneralBlockPanelKernel.h: In member function 'void Eigen::internal::gebp_kernel<LhsScalar, RhsScalar, Index, DataMapper, mr, nr, ConjugateLhs, ConjugateRhs>::operator()(const DataMapper&, const LhsScalar*, const RhsScalar*, Index, Index, Index, Eigen::internal::gebp_kernel<LhsScalar, RhsScalar, Index, DataMapper, mr, nr, ConjugateLhs, ConjugateRhs>::ResScalar, Index, Index, Index, Index) [with LhsScalar = Eigen::half; RhsScalar = Eigen::half; Index = long int; DataMapper = Eigen::internal::blas_data_mapper<Eigen::half, long int, 0, 0>; int mr = 2; int nr = 4; bool ConjugateLhs = false; bool ConjugateRhs = false]':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/products/GeneralBlockPanelKernel.h:1879:3: internal compiler error: in emit_move_insn, at expr.c:3722\r\n   }\r\n   ^\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\n```", "7.4 and 8.3 would do", "Even with gcc 7.2, TF 2.0.0-alpha0 seems to be working. Not sure which TF commit has fixed the issue.", "@mrodozov I rebuilt gcc 8.3.0 from sources this morning (6hr compile time!) and still the same error. \r\n\r\nSo now I've seen the error on TF v1.13.1 on my Ubuntu 16.04 AWS a4.xlarge compiling with native gcc 7.3.0, Linaro's generic gcc-8.2.0 (_Generic-AArch64_Ubuntu-16.04_aarch64-linux), and GCC's natively built 8.3.0. (And yes, I verified that the failing command is, in fact, using the correct compiler path). They all generate the same compiler error on the same line of code.\r\n\r\nI suppose I could compile gcc 7.4.0, but I'm beginning to think this is not GCC issue. ~Would be great if you could make your 1.13.1 aarch64 wheel file available if you've been successful on multiple compilers.~\r\n\r\n**EDIT:  I just found @lhelontra's wheel files for 1.13.1 at\r\nhttps://github.com/lhelontra/tensorflow-on-arm/releases**\r\n\r\n```\r\n$ gcc -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nTarget: aarch64-linux-gnu\r\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu/Linaro 7.3.0-27ubuntu1~18.04' --with-bugurl=file:///usr/share/doc/gcc-7/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-7 --program-prefix=aarch64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libquadmath --disable-libquadmath-support --enable-plugin --enable-default-pie --with-system-zlib --enable-multiarch --enable-fix-cortex-a53-843419 --disable-werror --enable-checking=release --build=aarch64-linux-gnu --host=aarch64-linux-gnu --target=aarch64-linux-gnu\r\nThread model: posix\r\ngcc version 7.3.0 (Ubuntu/Linaro 7.3.0-27ubuntu1~18.04) \r\n\r\n$ /usr/local/bin/gcc -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=/usr/local/bin/gcc\r\nCOLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/aarch64-unknown-linux-gnu/8.3.0/lto-wrapper\r\nTarget: aarch64-unknown-linux-gnu\r\nConfigured with: ./configure\r\nThread model: posix\r\ngcc version 8.3.0 (GCC) \r\n\r\n$ /opt/arm/gcc-8.2.0_Generic-AArch64_Ubuntu-16.04_aarch64-linux/bin/gcc -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=/opt/arm/gcc-8.2.0_Generic-AArch64_Ubuntu-16.04_aarch64-linux/bin/gcc\r\nCOLLECT_LTO_WRAPPER=/opt/arm/gcc-8.2.0_Generic-AArch64_Ubuntu-16.04_aarch64-linux/bin/../libexec/gcc/aarch64-linux-gnu/8.2.0/lto-wrapper\r\nTarget: aarch64-linux-gnu\r\nConfigured with: ../configure --prefix /tmp/plgbuild/abs_build/761279_27578/trunk/rel_work/arm_builder/components/builder/work/current_tree/AArch64/opt/arm/gcc-8.2.0_Generic-AArch64_Ubuntu-16.04_aarch64-linux --enable-languages=c,c++,go,fortran --disable-multilib --enable-lto --disable-werror --enable-stage1-checking --enable-checking=release --with-build-config=bootstrap-debug --enable-plugins --enable-gnu-indirect-function --target aarch64-linux-gnu --host aarch64-linux-gnu --enable-multiarch --with-build-sysroot=/tmp/plgbuild/abs_build/761279_27578/trunk/rel_work/arm_builder/components/builder/roots/armada/SysRoots/Ubuntu/Ubuntu_16.04_aarch64.rootfs --with-pkgversion=ARM-build-7\r\nThread model: posix\r\ngcc version 8.2.0 (ARM-build-7) \r\n```\r\n\r\n", "We also run into the same problem trying to compile TensorFlow in our CentOS 7 with devtoolset-7. Does anyone have a solution for this? ", "It might because the usable memory of your system is too low compared to the number of parallel Bazel compilation tasks. \r\n\r\nYou could try setting it to a smaller value using: ``bazel build --jobs 4 //...`` or ``bazel build --ram_utilization_factor 50``.\r\n\r\nEDIT: wrong solution. Adding ``--config=opt`` seems to solve the problem with my `gcc 6.3`. Not sure why though.", "Using Ubuntu 18.04, GCC-7.3.0, faced the same issue on building **TF Serving 1.13** with\r\n`bazel build -c opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" tensorflow_serving/...`\r\nand also\r\n`bazel build --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" tensorflow_serving/...`\r\n\r\nSimilar to @byronyi's comment,  adding `--config=nativeopt` does not give any error.\r\n\r\nHowever, I have also used `--cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"` to let the build be compatible with older ABI. Refer [here](https://www.tensorflow.org/install/source#bazel_build_options)\r\n\r\nMy **_final_** bazel build command,\r\n`bazel build --config=nativeopt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" tensorflow_serving/...`\r\n\r\nBuild completed _successfully_.\r\n\r\nI installed the **.deb** file generated under `tensorflow_serving/model_servers/` in another system with same configuration(Ubuntu 18.04, GCC-7.3.0). That worked fine.\r\n\r\nInstalling the same file under the Ubuntu 16.04, GCC-5.4.0 says the `'CXXABI_1.3.11' not found (required by tensorflow_model_server)`.\r\n\r\nDon't `--cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"` in the build command have any effect?\r\nor is that being used incorrectly with `--config=nativeopt` in the **_final_** build command?", "I think there is a known gcc 7 issue. It is fixed in the later versions of gcc 7 and 8.\r\n\r\nCXXABI issue is different. If you build TF on ubuntu 18, that binary wont run on ubuntu 16 or 14. Using D_GLIBCXX_USE_CXX11_ABI will only make it possible for TF to be able to run properly with binaries that are compiled using older libstdc++ versions.\r\n\r\nAs the main issue reported is a gcc issue, and fixed with patch releases in gcc, I will close this bug.", "> @mrodozov I rebuilt gcc 8.3.0 from sources this morning (6hr compile time!) and still the same error.\r\n> \r\n> So now I've seen the error on TF v1.13.1 on my Ubuntu 16.04 AWS a4.xlarge compiling with native gcc 7.3.0, Linaro's generic gcc-8.2.0 (_Generic-AArch64_Ubuntu-16.04_aarch64-linux), and GCC's natively built 8.3.0. (And yes, I verified that the failing command is, in fact, using the correct compiler path). They all generate the same compiler error on the same line of code.\r\n> \r\n> I suppose I could compile gcc 7.4.0, but I'm beginning to think this is not GCC issue. ~Would be great if you could make your 1.13.1 aarch64 wheel file available if you've been successful on multiple compilers.~\r\n> \r\n> **EDIT: I just found @lhelontra's wheel files for 1.13.1 at https://github.com/lhelontra/tensorflow-on-arm/releases**\r\n> \r\n> ```\r\n> $ gcc -v\r\n> Using built-in specs.\r\n> COLLECT_GCC=gcc\r\n> Target: aarch64-linux-gnu\r\n> Configured with: ../src/configure -v --with-pkgversion='Ubuntu/Linaro 7.3.0-27ubuntu1~18.04' --with-bugurl=file:///usr/share/doc/gcc-7/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-7 --program-prefix=aarch64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libquadmath --disable-libquadmath-support --enable-plugin --enable-default-pie --with-system-zlib --enable-multiarch --enable-fix-cortex-a53-843419 --disable-werror --enable-checking=release --build=aarch64-linux-gnu --host=aarch64-linux-gnu --target=aarch64-linux-gnu\r\n> Thread model: posix\r\n> gcc version 7.3.0 (Ubuntu/Linaro 7.3.0-27ubuntu1~18.04) \r\n> \r\n> $ /usr/local/bin/gcc -v\r\n> Using built-in specs.\r\n> COLLECT_GCC=/usr/local/bin/gcc\r\n> COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/aarch64-unknown-linux-gnu/8.3.0/lto-wrapper\r\n> Target: aarch64-unknown-linux-gnu\r\n> Configured with: ./configure\r\n> Thread model: posix\r\n> gcc version 8.3.0 (GCC) \r\n> \r\n> $ /opt/arm/gcc-8.2.0_Generic-AArch64_Ubuntu-16.04_aarch64-linux/bin/gcc -v\r\n> Using built-in specs.\r\n> COLLECT_GCC=/opt/arm/gcc-8.2.0_Generic-AArch64_Ubuntu-16.04_aarch64-linux/bin/gcc\r\n> COLLECT_LTO_WRAPPER=/opt/arm/gcc-8.2.0_Generic-AArch64_Ubuntu-16.04_aarch64-linux/bin/../libexec/gcc/aarch64-linux-gnu/8.2.0/lto-wrapper\r\n> Target: aarch64-linux-gnu\r\n> Configured with: ../configure --prefix /tmp/plgbuild/abs_build/761279_27578/trunk/rel_work/arm_builder/components/builder/work/current_tree/AArch64/opt/arm/gcc-8.2.0_Generic-AArch64_Ubuntu-16.04_aarch64-linux --enable-languages=c,c++,go,fortran --disable-multilib --enable-lto --disable-werror --enable-stage1-checking --enable-checking=release --with-build-config=bootstrap-debug --enable-plugins --enable-gnu-indirect-function --target aarch64-linux-gnu --host aarch64-linux-gnu --enable-multiarch --with-build-sysroot=/tmp/plgbuild/abs_build/761279_27578/trunk/rel_work/arm_builder/components/builder/roots/armada/SysRoots/Ubuntu/Ubuntu_16.04_aarch64.rootfs --with-pkgversion=ARM-build-7\r\n> Thread model: posix\r\n> gcc version 8.2.0 (ARM-build-7) \r\n> ```\r\n\r\ndid you resolve this issue? I can not compile it  with 7.4.0 7.3.0 too. I believe it is not gcc problem. Please do reopen this ticket. @gunan ", "Try with gcc 5.4. It worked for me.", "Ubuntu 18.04.2 ,TF 1.13\r\nbazel 0.19.2\r\ngcc 7.4 & gcc 8.3 meet same this error\r\n\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/products/GeneralBlockPanelKernel.h:1879:3: internal compiler error: in emit_move_insn, at expr.c:3723\r\n   }\r\n   ^\r\n", "yes we are building with 7.4.1\r\n@npanpaliya sure, I might as well try assembler or something :laughing: ", "I did apply the patch from\nhttps://gcc.gnu.org/bugzilla/show_bug.cgi?id=89752 and now the compiling\nworks.\nThis patch hasn't been applied on branch of 7.x or lower version. Please do\nit manually, and re-compile gcc on arm.\n\nOn Wed, Jun 12, 2019 at 11:21 PM Mircho Rodozov <notifications@github.com>\nwrote:\n\n> yes we are building with 7.4.1\n> @npanpaliya <https://github.com/npanpaliya> sure, I might as well try\n> assembler or something \ud83d\ude06\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25323?email_source=notifications&email_token=AA5BADBJFQCEYV4QRJS7YVLP2EH6FA5CNFSM4GTJ54WKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXQYLOQ#issuecomment-501319098>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA5BADHWYAX2SGJN4LHNHLDP2EH6FANCNFSM4GTJ54WA>\n> .\n>\n", "With this workaround, you can compile tensorflow with gcc 6.3 \r\n\r\nIn external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:801\r\n(you can find the file in a subdirectory of ~/.cache/bazel )\r\n\r\nreplace\r\n\r\nvalues[i] = internal::InnerMostDimReducer<Self, Op>::reduce(*this, firstIndex + i * num_values_to_reduce,\r\n                                                                    num_values_to_reduce, reducer);\r\n\r\nby two instructions instead\r\n\r\nSelf::CoeffReturnType a = internal::InnerMostDimReducer<Self, Op>::reduce(*this, firstIndex + i * num_values_to_reduce,\r\n\t\t\t\t\t\t\t\t    num_values_to_reduce, reducer);\r\nvalues[i] = a;\r\n\r\n\r\n", "> With this workaround, you can compile tensorflow with gcc 6.3\r\n> \r\n> In external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:801\r\n> (you can find the file in a subdirectory of ~/.cache/bazel )\r\n> \r\n> replace\r\n> \r\n> values[i] = internal::InnerMostDimReducer<Self, Op>::reduce(*this, firstIndex + i * num_values_to_reduce,\r\n> num_values_to_reduce, reducer);\r\n> \r\n> by two instructions instead\r\n> \r\n> Self::CoeffReturnType a = internal::InnerMostDimReducer<Self, Op>::reduce(*this, firstIndex + i * num_values_to_reduce,\r\n> num_values_to_reduce, reducer);\r\n> values[i] = a;\r\n\r\nit works\uff0c thanks", "The workaround didn't work for me.\r\n\r\nI'm working from a stretch-based Docker instance, looking to install TF from source.\r\n\r\nWhat worked for me was upgrading to gcc 8.3.0\r\n\r\n## Instructions\r\n\r\n(for other Debian rooks out there like me)\r\n\r\nFirst I added the following line to the bottom of my `/etc/apt/sources.list`:\r\n\r\n```\r\ndeb http://ftp.us.debian.org/debian testing main contrib non-free\r\n```\r\n\r\nThen I ran:\r\n\r\n```\r\n# apt-get update\r\n# apt-get install gcc-8 g++-8\r\n```\r\n\r\nThe default gcc for me was 6.3.0:\r\n\r\n```\r\n# gcc --version\r\ngcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n\r\nSo I needed to update my gcc to point to the new one:\r\n\r\n```\r\n# update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 80 --slave /usr/bin/g++ g++ /usr/bin/g++-8 --slave /usr/bin/gcov gcov /usr/bin/gcov-8\r\nupdate-alternatives: using /usr/bin/gcc-8 to provide /usr/bin/gcc (gcc) in auto mode\r\n# update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-6 60 --slave /usr/bin/g++ g++ /usr/bin/g++-6 --slave /usr/bin/gcov gcov /usr/bin/gcov-6\r\n```\r\n\r\nThe default is now 8.3.0:\r\n\r\n```\r\n# gcc --version\r\ngcc (Debian 8.3.0-26) 8.3.0\r\nCopyright (C) 2018 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n\r\nIf I wanted to default to a different gcc version, I can now use:\r\n\r\n```\r\n# update-alternatives --config gcc\r\n```\r\n\r\n## References\r\n\r\n- https://stackoverflow.com/questions/43151627/installing-g-7-0-1-on-debian-8-7/43151826#43151826\r\n- https://linuxize.com/post/how-to-install-gcc-compiler-on-ubuntu-18-04/#installing-multiple-gcc-versions\r\n", "> We also run into the same problem trying to compile TensorFlow in our CentOS 7 with devtoolset-7. Does anyone have a solution for this?\r\n\r\nhave you slove this problem?", "> With this workaround, you can compile tensorflow with gcc 6.3\r\n> \r\n> In external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:801 (you can find the file in a subdirectory of ~/.cache/bazel )\r\n> \r\n> replace\r\n> \r\n> values[i] = internal::InnerMostDimReducer<Self, Op>::reduce(*this, firstIndex + i * num_values_to_reduce, num_values_to_reduce, reducer);\r\n> \r\n> by two instructions instead\r\n> \r\n> Self::CoeffReturnType a = internal::InnerMostDimReducer<Self, Op>::reduce(*this, firstIndex + i * num_values_to_reduce, num_values_to_reduce, reducer); values[i] = a;\r\n\r\nthanks, it's work"]}, {"number": 25322, "title": "ImportError: No module named 'pandas'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): pip/binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5.6\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\n$ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 88, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/tensorflow/python/keras/__init__.py\", line 29, in <module>\r\n    from tensorflow.python.keras import datasets\r\n  File \"/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/tensorflow/python/keras/datasets/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.keras.datasets import imdb\r\n  File \"/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/tensorflow/python/keras/datasets/imdb.py\", line 25, in <module>\r\n    from tensorflow.python.keras.preprocessing.sequence import _remove_long_seq\r\n  File \"/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/tensorflow/python/keras/preprocessing/__init__.py\", line 30, in <module>\r\n    from tensorflow.python.keras.preprocessing import image\r\n  File \"/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/tensorflow/python/keras/preprocessing/image.py\", line 23, in <module>\r\n    from keras_preprocessing import image\r\n  File \"/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/keras_preprocessing/image/__init__.py\", line 8, in <module>\r\n    from .dataframe_iterator import DataFrameIterator\r\n  File \"/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/keras_preprocessing/image/dataframe_iterator.py\", line 11, in <module>\r\n    from pandas.api.types import is_numeric_dtype\r\nImportError: No module named 'pandas'\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n`import tensorflow` should work.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\npip install tensorflow\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThis is via Travis.\r\nSee [here](https://travis-ci.org/rwth-i6/returnn/jobs/486014809) for the full Travis log.\r\n\r\nThe TF package it installs is this:\r\n```\r\n$ pip install tensorflow\r\nCollecting tensorflow\r\n  Downloading https://files.pythonhosted.org/packages/b1/ad/48395de38c1e07bab85fc3bbec045e11ae49c02a4db0100463dd96031947/tensorflow-1.12.0-cp35-cp35m-manylinux1_x86_64.whl (83.1MB)\r\n...\r\nInstalling collected packages: gast, protobuf, termcolor, astor, grpcio, werkzeug, markdown, tensorboard, keras-preprocessing, absl-py, keras-applications, tensorflow\r\nSuccessfully installed absl-py-0.7.0 astor-0.7.1 gast-0.2.2 grpcio-1.18.0 keras-applications-1.0.7 keras-preprocessing-1.0.6 markdown-3.0.1 protobuf-3.6.1 tensorboard-1.12.2 tensorflow-1.12.0 termcolor-1.1.0 werkzeug-0.14.1\r\n```\r\n\r\nI guess the `pandas` dependency is just missing?\r\n", "comments": ["This was an issue in the `keras-preprocessing` package. It has been fixed. In the minor release 1.0.7 \r\n\r\nhttps://github.com/keras-team/keras-preprocessing/releases/tag/1.0.7", "Since you are using python3, can you try,\r\n >pip3 install tensorflow\r\n\r\nAnd if the importing pandas error still persists, further you can try,\r\n>pip3 install pandas", "This should be resolved at this time; try reinstalling TF from pip.", "Or just upgrade to the latest keras_preprocessing: `pip install keras_preprocessing --upgrade`.", "I have tried installing both ways.\r\nNow I have installed tensorflow-gpu by conda. It is installed in a new environment which I created for tensorflow.\r\nAnd I did this - pip install keras_preprocessing --upgrade\r\nAs well as this - pip3 install pandas.\r\nStill it shows the same error\r\nPlease help."]}, {"number": 25321, "title": "Fix incorrect comparison in tensorflow/lite/kernels/slice.cc", "body": "Issue #25214", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Thank you for PR , @shibe2 please CLA", "@shibe2  gentle ping to sign cla", "Sorry, I didn't have time for this. I think, it will be faster if one of developers fix it, so I'm closing my pull request. Please refer to the bug report."]}, {"number": 25320, "title": "tf.data.Dataset error in documentation", "body": "**System information**\r\n- TensorFlow version: 1.12.0\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip\r\n\r\n\r\n**Describe the documentation issue**\r\nI have tested the code provided in the documentation. Here the code : \r\n\r\n```\r\nfrom tensorflow.data import Dataset # i have added my own\r\n\r\n\r\n# NOTE: The following examples use `{ ... }` to represent the\r\n# contents of a dataset.\r\na = { 1, 2, 3 }\r\nb = { 4, 5, 6 }\r\nc = { (7, 8), (9, 10), (11, 12) }\r\nd = { 13, 14 }\r\n\r\n# The nested structure of the `datasets` argument determines the\r\n# structure of elements in the resulting dataset.\r\nDataset.zip((a, b)) == { (1, 4), (2, 5), (3, 6) }\r\nDataset.zip((b, a)) == { (4, 1), (5, 2), (6, 3) }\r\n\r\n# The `datasets` argument may contain an arbitrary number of\r\n# datasets.\r\nDataset.zip((a, b, c)) == { (1, 4, (7, 8)),\r\n                            (2, 5, (9, 10)),\r\n                            (3, 6, (11, 12)) }\r\n\r\n# The number of elements in the resulting dataset is the same as\r\n# the size of the smallest dataset in `datasets`.\r\nDataset.zip((a, d)) == { (1, 13), (2, 14) }\r\n```\r\n\r\nHere the error provided by the system:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 13, in <module>\r\n    Dataset.zip((a, b)) == { (1, 4), (2, 5), (3, 6) }\r\n  File \"/home/idolon/.virtualenvs/research/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 612, in zip\r\n    return ZipDataset(datasets)\r\n  File \"/home/idolon/.virtualenvs/research/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2037, in __init__\r\n    raise TypeError(message)\r\nTypeError: The argument to `Dataset.zip()` must be a nested structure of `Dataset` objects.\r\n```\r\n\r\nI don't really understand what is Dataset and how it works, there's no explicit examples of its functionalities. What can we do with it ? How to build a dataset, how it's used, and how to show data. We have only examples on official datasets but what about making our own datasets ? Maybe i've not found the correct documentation page because of my bad english mastering. \r\n\r\nSo, is it an error of the documentation or just me that have not followed correctly the doc ?\r\nBest regards. \r\n", "comments": ["Yes, this part of the comment was supposed to imply that the code is not runnable as is:\r\n\r\n```python\r\n# NOTE: The following examples use `{ ... }` to represent the\r\n# contents of a dataset.\"\r\n```\r\n\r\nWe did this to keep the examples short,  but I can see why this might cause confusion! We'd welcome contributions to improve the readability (and testability) of the comments here.\r\n\r\n/cc @jsimsa FYI", "@mrry I was interested to work on this for my first commit to TensorFlow. What do you think of the following? I can make a pull request if you like\r\n\r\n```\r\na = Dataset.from_tensor_slices(tf.range(1, 4))  # [ 1, 2, 3 ]\r\nb = Dataset.from_tensor_slices(tf.range(4, 7))  # [ 4, 5, 6 ]\r\nc = Dataset.from_tensor_slices(tf.reshape(tf.range(7,13), (3,2)))  # [ [7, 8], [9, 10], [11, 12] ]\r\nd = Dataset.from_tensor_slices(tf.range(13, 15))  # [ 13, 14 ]\r\n\r\n# The nested structure of the `datasets` argument determines the\r\n# structure of elements in the resulting dataset.\r\nDataset.zip((a, b)) # ==> [ [1, 4], [2, 5], [3, 6] ]\r\nDataset.zip((b, a)) # ==> [ [4, 1], [5, 2], [6, 3] ]\r\n\r\n# The `datasets` argument may contain an arbitrary number of\r\n# datasets.\r\nDataset.zip((a, b, c))  # ==> [ [1, 4, [7, 8]],\r\n                        #       [2, 5, [9, 10]],\r\n                        #       [3, 6, [11, 12]] ]\r\n    \r\n# The number of elements in the resulting dataset is the same as\r\n# the size of the smallest dataset in `datasets`.\r\nDataset.zip((a, d))  # ==> [ [1, 13], [2, 14] ]\r\n```", "Thanks @ThomasHagebols! That mostly looks good with just one nit: the difference between tuples and lists is significant here, so please use `(...)` when the elements will be made up of tuples, e.g.:\r\n\r\n```\r\nDataset.zip((a, b, c))  # ==> [ (1, 4, [7, 8]),\r\n                        #       (2, 5, [9, 10]),\r\n                        #       (3, 6, [11, 12]) ]\r\n```", "@mrry Thanks for the feedback. I noticed that there are several code examples that are not executable and a bit vague in python/data/ops/dataset_ops.py. If you want I can fix the documentation of the methods concatenate, map, flat_map and interleave in the same pull request. ", "The more the merrier, thanks!", "I guess this can be closed, is there anything remaining for this issue?\r\n"]}, {"number": 25319, "title": "CollectiveAllReduceStrategy", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 25318, "title": "Updated the README.md file", "body": "typo error fixed", "comments": []}, {"number": 25317, "title": "Can't launch TF Detect Android sample with Yolov3-tiny model", "body": "- OS Platform - Linux Ubuntu 16.04;\r\n- Tools: Android Studio 3.3 (Android Studio 3.3 Build #AI-182.5107.16.33.5199772)\r\n- Mobile device Huawei P20 lite, Android Emulator (Nexus 5X, API 28)\r\n- use the TensorFlow from Jcenter\r\n- TensorFlow android version 1.12.0\r\n- Python version: Python 3.6.8\r\n\r\nI converted Yolov3-tiny model (downloaded from https://pjreddie.com/darknet/yolo/) with [DW2TF](https://github.com/jinyu121/DW2TF). Then I put .pb file to assets and changed MODE == DetectorMode.YOLO in DetectorActivity.java.\r\n\r\nWhen I start TF Detect app I have a crash:\r\n```E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.demo, PID: 4643\r\n    java.lang.IllegalArgumentException: No Operation named [input] in the Graph\r\n        at org.tensorflow.Session$Runner.operationByName(Session.java:372)\r\n        at org.tensorflow.Session$Runner.feed(Session.java:142)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.addFeed(TensorFlowInferenceInterface.java:577)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.feed(TensorFlowInferenceInterface.java:318)\r\n        at org.tensorflow.demo.TensorFlowYoloDetector.recognizeImage(TensorFlowYoloDetector.java:159)\r\n        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:289)\r\n        at android.os.Handler.handleCallback(Handler.java:873)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:193)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n```\r\nI added logs and i have params `inputName` and `inputSize`. Can you help me with this issue?\r\n", "comments": ["Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. Github is mainly for addressing bugs in installation and performance. Thanks!", "Do you solve the problem?\r\n"]}, {"number": 25316, "title": "Tensorflow lite logs do not get enabled until any LOG(FATAL) command is not encountered", "body": "Tensorflow version 1.13.0 rc0 and 1.12.0 cpu version(cuda disable) for Linux ubuntu 16.0\r\n\r\nI am unable to generate any logs on console using toco tool of tensorflow.\r\n_ use export TF_CPP_MIN_LOG_LEVEL=1  , the info logs do not start to appear on console.\r\n\r\nUntil I any error case while using toco is not hit in the source code i,e when LOG(FATAL)  is encountered then all debug logs appear on console.\r\n\r\nLet me know how to enable all the logs", "comments": ["@abhajaswal Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!", "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04.1 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No , Its on Linux PC\r\n- TensorFlow installed from (source or binary): Installed from source 1.13.rc0\r\n- TensorFlow version: 1.13.rc0\r\n- Python version: Python 3.6.5 :: Anaconda, Inc\r\n- Installed using virtualenv? pip? conda?: Installed using Pip in conda virtual enviourment as well as also tried with installing using pip without conda virtual env both reproduce the behaviour\r\n- Bazel version (if compiling from source):  Build label: 0.21.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\n- CUDA/cuDNN version:  N/A compiled from source without cuda option \r\n- GPU model and memory: N/A compiled from source without cuda option \r\n\r\nCompiled the Source code using -  bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package \r\nAnd installed wheel using pip ,\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nRan the toco for quantized pb for fast style transfer model using -\r\n\r\nFOLLOWING COMMAND IS THE SUCCESSFUL CASE IN WHICH I HAD ENABLED THE MARCO TO PRINT TENSORFLOW AND TENSORFLOW LITE LOGS BUT ONLY FEW XLA LOGS APPEAR NO TENSORFLWO LITE LOGS\r\n\r\ntoco --graph_def_file=/home/sri/Dev/test/frozen_FST_deo.pb --output_file=/home/sri/Dev/test/quantized_FST_15.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=QUANTIZED_UINT8 --output_arrays=Tanh --input_arrays=input --mean_values=0 --std_dev_values=1 --default_ranges_min=-10 --default_ranges_max=10\r\n/home/sri/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2019-02-01 11:12:06.069365: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz\r\n2019-02-01 11:12:06.069729: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x555d576a8080 executing computations on platform Host. Devices:\r\n2019-02-01 11:12:06.069744: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n\r\n\r\n\r\n\r\nIN CASE OF ERROR CASE -\r\n\r\n\r\n\r\n\r\ntoco --graph_def_file=/home/sri/Dev/test/frozen_FST_deo.pb --output_file=/home/sri/Dev/test/quantized_FST_15.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=QUANTIZED_UINT8 --output_arrays=Tanh --input_arrays=input --mean_values=0 --std_dev_values=1 --default_ranges_min=0 --default_ranges_max=1\r\n/home/sri/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2019-02-01 11:08:07.749245: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz\r\n2019-02-01 11:08:07.749801: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55c91359c6d0 executing computations on platform Host. Devices:\r\n2019-02-01 11:08:07.749826: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nsri@sri-MS-7A74:~$ toco --graph_def_file=/home/sri/Dev/test/frozen_FST_deo.pb --output_file=/home/sri/Dev/test/quantized_FST_15.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=QUANTIZED_UINT8 --output_arrays=Tanh --input_arrays=input --mean_values=0 --std_dev_values=1\r\n/home/sri/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2019-02-01 11:08:20.645381: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz\r\n2019-02-01 11:08:20.646016: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5629ddb2ebc0 executing computations on platform Host. Devices:\r\n2019-02-01 11:08:20.646043: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nTraceback (most recent call last):\r\n  File \"/home/sri/anaconda3/bin/toco\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/sri/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 442, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/sri/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/sri/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 438, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/sri/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 191, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/home/sri/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 455, in convert\r\n    **converter_kwargs)\r\n  File \"/home/sri/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 442, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/home/sri/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 205, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\nI am here ABHA ImportTensorFlowGraphDef\r\nABHA I am here\r\n################ABHA####################\r\n################ABHA####################\r\n\r\n2019-02-01 11:08:21.876746: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 387 operators, 583 arrays (0 quantized)\r\n2019-02-01 11:08:21.880104: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 387 operators, 583 arrays (0 quantized)\r\n2019-02-01 11:08:21.893831: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 186 operators, 318 arrays (1 quantized)\r\n2019-02-01 11:08:21.895988: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 186 operators, 318 arrays (1 quantized)\r\n2019-02-01 11:08:21.896947: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 170 operators, 302 arrays (1 quantized)\r\n2019-02-01 11:08:21.898291: F tensorflow/lite/toco/tooling_util.cc:1702] Array moments/SquaredDifference, which is an input to the Mean operator producing the output array moments/variance, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\nAborted (core dumped)\r\n\r\n\r\n\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n", "I Cross checked again by forcefully putting a dumpy LOG(FATAL) <<\" printing \"  in the code flow of tensorflow lite and rebuilt and installed. \r\nLogs are being printed on console only if it hits any LOG(FATAL) for tensorflow lite convertor code.\r\nI even tried putting printf and std:cout its also does not being reflected on the console.\r\n\r\nIts very difficult to debug and understand an issue in tensorflow lite convertor code without getting  Logs\r\n\r\n\r\n\r\nOn the other hand all the logs in tensorflow lite Interperator code FLOW :  inculding printf's are working fine\r\n\r\n", "Please attach long logs in future, as requested in the template. ", "This is a function of the way the TensorFlow Lite converter is wired into the Python API. At the moment, the converter runs in a separate process, and we capture the stdout/stderr output only at the end of the execution. Looks like we just need to log the output in the successful case (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/convert.py#L196)", "@gargn should we output the TOCO output even in the successful conversion case?", "TOCO is no longer the default conversion backend. Much of the logging should be dramatically improved with the new converter backend (which leverages MLIR)."]}, {"number": 25315, "title": "Error importing tensorflow.", "body": "I installed tensorflow using `python3 -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl`, but importing tensorflow gives me:\r\n```\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/ayush99/anaconda3/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/ayush99/anaconda3/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/ayush99/anaconda3/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/ayush99/anaconda3/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\nPython: 3.7\r\nLinux(Ubuntu), trying to install tensorflow for CPU.", "comments": ["It seems like you are using python 3.7, but there is no (official) python 3.7 support (yet). If you want to workaround, you may have a look at the threads where people try that.\r\nSee for example: #17022 or #20517 \r\n\r\nOtherwise: try to use python 3.6, this should work\r\n\r\n(*sidenote: pip install could currently [fails due to a bug](https://github.com/keras-team/keras-preprocessing/pull/156) (just a few hours ago introduced) in another package. Make sure, if it fails with python 3.6, that you also install pandas (as a temporary hack)*)", "For future reference, installing tensorflow-nightly (`pip install tf-nightly`) fixes this issue.", "I tried pip install tf-nightly , but still getting same error, \"no matching distribution found for tf-nightly\"", "Currently , I have python 2.7, 3,5, and 3,7 all installed in my machine (ubuntu) running on Oracle VM which is running on Windows 10 Laptop (have 12 GB, 4 Cores, VM has 100 GB assignd,,,,etc). Just to see if my setup is a factor. Many thanks for any help!", "Now TensorFlow 1.13 works with Python 3.7 (the normal version). But if you have 3 python versions installed, I can imagine quite a chaos. Are you using any kind of virtualization _inside_ the vm? Like virtualenv or conda?\r\n\r\nWhich python version is \"pip\"? Can you do `which pip` in a terminal and see (at the end of the output) which version is picked up?", "> It seems like you are using python 3.7, but there is no (official) python 3.7 support (yet). If you want to workaround, you may have a look at the threads where people try that.\r\n> See for example: #17022 or #20517\r\n> \r\n> Otherwise: try to use python 3.6, this should work\r\n> \r\n> (_sidenote: pip install could currently [fails due to a bug](https://github.com/keras-team/keras-preprocessing/pull/156) (just a few hours ago introduced) in another package. Make sure, if it fails with python 3.6, that you also install pandas (as a temporary hack)_)\r\n\r\nTried 3.6. It is giving the same error as well", "How did you install it exactly? Do you have the logs?", "> For future reference, installing tensorflow-nightly (`pip install tf-nightly`) fixes this issue.\r\n\r\nNo matching distribution found for tf-nightly\r\n"]}, {"number": 25314, "title": "label_image segmentation issue for operator code 25 which is softmax", "body": "When using the standard label_image to test the mobilenet tflite file in the default P OS android NN.\r\nwe are facing the segmentation issue for the softmax operation with the default command as specified in the readme.md.\r\n\r\nWe are using latest Android NN on P Os, for Arm64-v8a series soc.\r\n\r\nThe following is the crash snippet:\r\n```\r\n01-24 15:28:12.701 I/ExecutionBuilder( 4787): ExecutionBuilder::startCompute (from plan, iteratively)\r\n01-24 15:28:12.701 I/ExecutionBuilder( 4787): looking for next StepExecutor\r\n01-24 15:28:12.702 I/ExecutionPlan( 4787): ExecutionPlan::next(0x7c00830460, 0x7bfe80c308): mNextStepIndex = 0\r\n01-24 15:28:12.702 I/ExecutionBuilder( 4787): input[0] = POINTER(0x7bfcf4ac00)\r\n01-24 15:28:12.702 I/ExecutionBuilder( 4787): output[0] = POINTER(0x7bfce00fc0)\r\n01-24 15:28:12.707 I/CpuExecutor( 4787): CpuExecutor::run() with request({.inputs = [1]{{.hasNoValue = 0, .location = {.poolIndex = 0, .offset = 0, .length = 602112}, .dimensions = [4]{1, 224, 224, 3}}}, .outputs = [1]{{.hasNoValue = 0, .location = {.poolIndex = 1, .offset = 0, .length = 4004}, .dimensions = [2]{1, 1001}}}, .pools = [0]{}})\r\n01-24 15:28:12.707 I/CpuExecutor( 4787): CpuExecutor::initializeRunTimeInfo\r\n01-24 15:28:12.763 D/SDHMS:ComponentLevelSetter(22281): mWifiTotalUsage = 0, throughput = 0.0\r\n01-24 15:28:12.928 I/CpuExecutor( 4787): Completed run normally\r\n01-24 15:28:12.932 I/ExecutionBuilder( 4787): looking for next StepExecutor\r\n01-24 15:28:12.932 I/ExecutionPlan( 4787): ExecutionPlan::next(0x7c00830460, 0x7bfe80c308): mNextStepIndex = 1\r\n01-24 15:28:12.934 I/ExecutionBuilder( 4787): ExecutionBuilder::ExecutionBuilder\r\n01-24 15:28:12.936 I/ExecutionBuilder( 4787): ExecutionBuilder::startCompute (from plan, iteratively)\r\n01-24 15:28:12.936 I/ExecutionBuilder( 4787): looking for next StepExecutor\r\n01-24 15:28:12.936 I/ExecutionPlan( 4787): ExecutionPlan::next(0x7c00830460, 0x7bfe80c308): mNextStepIndex = 0\r\n01-24 15:28:12.936 I/ExecutionBuilder( 4787): input[0] = POINTER(0x7bfcf4ac00)\r\n01-24 15:28:12.936 I/ExecutionBuilder( 4787): output[0] = POINTER(0x7bfce00fc0)\r\n01-24 15:28:12.940 I/CpuExecutor( 4787): CpuExecutor::run() with request({.inputs = [1]{{.hasNoValue = 0, .location = {.poolIndex = 0, .offset = 0, .length = 602112}, .dimensions = [4]{1, 224, 224, 3}}}, .outputs = [1]{{.hasNoValue = 0, .location = {.poolIndex = 1, .offset = 0, .length = 4004}, .dimensions = [2]{1, 1001}}}, .pools = [0]{}})\r\n01-24 15:28:12.941 I/CpuExecutor( 4787): CpuExecutor::initializeRunTimeInfo\r\n01-24 15:28:13.295 I/UiThreadMonitor(20720): setAwake 103 5001\r\n01-24 15:28:13.301 I/CpuExecutor( 4787): Completed run normally\r\n01-24 15:28:13.305 I/ExecutionBuilder( 4787): looking for next StepExecutor\r\n01-24 15:28:13.305 I/ExecutionPlan( 4787): ExecutionPlan::next(0x7c00830460, 0x7bfe80c308): mNextStepIndex = 1\r\n01-24 15:28:13.306 I/ExecutionBuilder( 4787): ExecutionBuilder::ExecutionBuilder\r\n01-24 15:28:13.307 I/ExecutionBuilder( 4787): ExecutionBuilder::startCompute (from plan, iteratively)\r\n01-24 15:28:13.307 I/ExecutionBuilder( 4787): looking for next StepExecutor\r\n01-24 15:28:13.307 I/ExecutionPlan( 4787): ExecutionPlan::next(0x7c00830460, 0x7bfe80c308): mNextStepIndex = 0\r\n01-24 15:28:13.307 I/ExecutionBuilder( 4787): input[0] = POINTER(0x7bfcf4ac00)\r\n01-24 15:28:13.307 I/ExecutionBuilder( 4787): output[0] = POINTER(0x7bfce00fc0)\r\n01-24 15:28:13.308 I/CpuExecutor( 4787): CpuExecutor::run() with request({.inputs = [1]{{.hasNoValue = 0, .location = {.poolIndex = 0, .offset = 0, .length = 602112}, .dimensions = [4]{1, 224, 224, 3}}}, .outputs = [1]{{.hasNoValue = 0, .location = {.poolIndex = 1, .offset = 0, .length = 4004}, .dimensions = [2]{1, 1001}}}, .pools = [0]{}})\r\n01-24 15:28:13.308 I/CpuExecutor( 4787): CpuExecutor::initializeRunTimeInfo\r\n01-24 15:28:13.547 I/CpuExecutor( 4787): Completed run normally\r\n01-24 15:28:13.551 I/ExecutionBuilder( 4787): looking for next StepExecutor\r\n01-24 15:28:13.551 I/ExecutionPlan( 4787): ExecutionPlan::next(0x7c00830460, 0x7bfe80c308): mNextStepIndex = 1\r\n--------- beginning of crash\r\n01-24 15:28:13.554 F/libc    ( 4787): Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x3d38 in tid 4787 (label_image), pid 4787 (label_image)\r\n01-24 15:28:13.636 I/crash_dump64( 5086): obtaining output fd from tombstoned, type: kDebuggerdTombstone\r\n01-24 15:28:13.639 I//system/bin/tombstoned( 5616): received crash request for pid 4787\r\n01-24 15:28:13.641 I/crash_dump64( 5086): performing dump of process 4787 (target tid = 4787)\r\n01-24 15:28:13.644 F/DEBUG   ( 5086): *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n01-24 15:28:13.645 F/DEBUG   ( 5086): Build fingerprint: 'samsung/beyond2ltexx/beyond2:9/PPR1.180610.011/G975FXXE1ASAM:eng/test-keys'\r\n01-24 15:28:13.645 F/DEBUG   ( 5086): Revision: '20'\r\n01-24 15:28:13.645 F/DEBUG   ( 5086): ABI: 'arm64'\r\n01-24 15:28:13.645 F/DEBUG   ( 5086): pid: 4787, tid: 4787, name: label_image  >>> ./label_image <<<\r\n01-24 15:28:13.645 F/DEBUG   ( 5086): signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x3d38\r\n01-24 15:28:13.646 F/DEBUG   ( 5086):     x0  0000005dc122ebf8  x1  0000000000000000  x2  0000000000000000  x3  0000007c00ef33c8\r\n01-24 15:28:13.646 F/DEBUG   ( 5086):     x4  0000007fd73faad8  x5  0000000000000020  x6  0000007fd73faaa8  x7  0000000000000010\r\n01-24 15:28:13.646 F/DEBUG   ( 5086):     x8  0000000000003d38  x9  385838b172084ce5  x10 00000000000012b3  x11 00000000000012b3\r\n01-24 15:28:13.646 F/DEBUG   ( 5086):     x12 0000000000000003  x13 00000000ffffffff  x14 0000007fd73fa6a4  x15 0000007fd73fa23c\r\n01-24 15:28:13.646 F/DEBUG   ( 5086):     x16 0000007c00ef11b0  x17 0000007c00e87120  x18 0000007fd73fa23a  x19 0000007c0082d2c0\r\n01-24 15:28:13.646 F/DEBUG   ( 5086):     x20 0000005dc122ebf8  x21 0000007c0082b0b0  x22 0000007c0082b0a0  x23 0000005dc11cf6ff\r\n01-24 15:28:13.647 F/DEBUG   ( 5086):     x24 0000005dc11dbf2a  x25 0000005dc11cf78e  x26 000000000000028d  x27 0000007c00831480\r\n01-24 15:28:13.647 F/DEBUG   ( 5086):     x28 0000007c008314a8  x29 0000000000000018\r\n01-24 15:28:13.647 F/DEBUG   ( 5086):     sp  0000007fd73fab90  lr  0000005dc105c084  pc  0000005dc105c090\r\n01-24 15:28:13.648 I/unwind  ( 5086): Malformed section header found, ignoring...\r\n01-24 15:28:13.651 F/DEBUG   ( 5086): \r\n01-24 15:28:13.651 F/DEBUG   ( 5086): backtrace:\r\n01-24 15:28:13.651 F/DEBUG   ( 5086):     #00 pc 0000000000017090  /data/local/tmp/androidnn/label_image\r\n01-24 15:28:13.651 F/DEBUG   ( 5086):     #01 pc 00000000000179e0  /data/local/tmp/androidnn/label_image\r\n01-24 15:28:13.651 F/DEBUG   ( 5086):     #02 pc 0000000000017a88  /data/local/tmp/androidnn/label_image\r\n01-24 15:28:13.652 F/DEBUG   ( 5086):     #03 pc 00000000000aeeb4  /system/lib64/libc.so (__libc_init+88)\r\n01-24 15:28:14.039 I/crash_dump64( 5086): Start dumpstate\r\n01-24 15:28:14.054 W/NativeCrashListener( 5488): Couldn't find ProcessRecord for pid 4787\r\n01-24 15:28:14.057 E//system/bin/tombstoned( 5616): Tombstone written to: /data/tombstones/tombstone_01\r\n01-24 15:28:14.062 E/audit   ( 5261): type=1701 audit(1548323894.051:192): auid=4294967295 uid=0 gid=0 ses=4294967295 subj=u:r:su:s0 pid=4787 comm=\"label_image\" exe=\"/data/local/tmp/androidnn/label_image\" sig=11 res=1\r\n01-24 15:28:14.071 I/BootReceiver( 5488): Copying /data/tombstones/tombstone_01 to DropBox (SYSTEM_TOMBSTONE)\r\n01-24 15:28:14.117 I/HqmInfo::LogAnalyzer( 5488): ContextBroadcastReceiver : received DropBoxManager \"SYSTEM_TOMBSTONE\" event\r\n01-24 15:28:14.118 I/HqmInfo::LogAnalyzer( 5488): MSG_TYPE:MSG_APP_CRASH_CHECK_REQ\r\n01-24 15:28:14.120 W/BroadcastQueue( 5488): Background execution not allowed: receiving Intent { act=android.intent.action.DROPBOX_ENTRY_ADDED flg=0x10 (has extras) } to com.google.android.gms/.stats.service.DropBoxEntryAddedReceiver\r\n01-24 15:28:14.121 I/HqmInfo::AppCrashAnalyzer( 5488): checkAppError: list is null\r\n01-24 15:28:14.123 D/dumpstate( 5093): Loading stats from /data/log/dumpstate-stats.txt\r\n01-24 15:28:14.124 I/dumpstate( 5093): Average max progress: 1492 in 1 runs; estimated max: 1492\r\n01-24 15:28:14.132 I/dumpstate( 5093): begin\r\n01-24 15:28:14.133 I/dumpstate( 5093): dumpstate info: id=2, args='/system/bin/dumpstate -k -z', extra_options= app_native)\r\n01-24 15:28:14.133 I/dumpstate( 5093): bugreport format version: 2.0\r\n01-24 15:28:14.134 D/dumpstate( 5093): Bugreport dir: /data/log\r\n01-24 15:28:14.134 D/dumpstate( 5093): Base name: dumpstate_app_native\r\n01-24 15:28:14.134 D/dumpstate( 5093): Suffix: 2019-01-24-15-28-14\r\n01-24 15:28:14.134 D/dumpstate( 5093): Log path: /data/log/dumpstate_app_native-2019-01-24-15-28-14-dumpstate_log-5093.txt\r\n01-24 15:28:14.134 D/dumpstate( 5093): Temporary path: /data/log/dumpstate_app_native-2019-01-24-15-28-14.tmp\r\n01-24 15:28:14.134 D/dumpstate( 5093): Screenshot path: \r\n01-24 15:28:14.134 D/dumpstate( 5093): Creating initial .zip file (/data/log/dumpstate_app_native-2019-01-24-15-28-14.zip)\r\n01-24 15:28:14.135 D/dumpstate( 5093): Adding zip text entry version.txt\r\n01-24 15:28:14.136 I/dumpstate( 5093): Vibrate: 'cmd vibrator vibrate 150 dumpstate'\r\n01-24 15:28:14.143 I/ActivityManager( 5488): Killing 3567:com.samsung.android.bixby.agent/5018 (adj 906): empty #31\r\n01-24 15:28:14.146 W/libprocessgroup( 5488): kill(-3567, 9) failed: No such process\r\n01-24 15:28:14.154 W/BroadcastQueue( 5488): Background execution not allowed: receiving Intent { act=android.intent.action.DROPBOX_ENTRY_ADDED flg=0x10 (has extras) } to com.google.android.gms/.chimera.GmsIntentOperationService$PersistentTrustedReceiver\r\n01-24 15:28:14.196 I/chatty  ( 5488): uid=1000(system) ActivityManager identical 5 lines\r\n01-24 15:28:14.202 W/libprocessgroup( 5488): kill(-3567, 9) failed: No such process\r\n01-24 15:28:14.206 V/VibratorService( 5488): vibrate - package: dumpstate, token: com.android.server.VibratorService@62e6ba4, usage: 0, effect: OneShot{mDuration=150, mAmplitude=-1, mMagnitude=-1, mMagnitudeType=TYPE_EXTRA}, mag: 10000, TYPE_EXTRA\r\n01-24 15:28:14.206 D/VibratorService( 5488): Turning vibrator off\r\n01-24 15:28:14.207 D/SecVibrator-HAL( 5296): writeNode node:/sys/class/timed_output/vibrator/enable val:0\r\n01-24 15:28:14.211 W/libprocessgroup( 5488): kill(-3567, 9) failed: No such process\r\n01-24 15:28:14.213 D/VibratorService( 5488): vibratorOn() : 150ms, amplitude :-1, mag :10000, f : 0\r\n01-24 15:28:14.215 D/SecVibrator-HAL( 5296): writeNode node:/sys/class/timed_output/vibrator/multi_freq val:0\r\n01-24 15:28:14.217 D/SecVibrator-HAL( 5296): writeNode node:/sys/class/timed_output/vibrator/intensity val:10000\r\n01-24 15:28:14.217 D/SecVibrator-HAL( 5296): writeNode node:/sys/class/timed_output/vibrator/enable val:150\r\n01-24 15:28:14.219 W/libprocessgroup( 5488): kill(-3567, 9) failed: No such process\r\n01-24 15:28:14.293 I/chatty  ( 5488): uid=1000(system) ActivityManager identical 9 lines\r\n01-24 15:28:14.301 W/libprocessgroup( 5488): kill(-3567, 9) failed: No such process\r\n01-24 15:28:14.305 V/[DMS]SemDesktopModeStateNotifier( 5488): binderDied(): DesktopModeListenerInfo(name=com.samsung.android.bixby.agent.app.BixbyApplication$$Lambda$1@bfd57fd, pid=3567, uid=5018)\r\n01-24 15:28:14.307 W/libprocessgroup( 5488): kill(-3567, 9) failed: No such process\r\n01-24 15:28:14.309 D/ForegroundUtils(21076): Process died; UID 5018 PID 3567\r\n01-24 15:28:14.309 D/ForegroundUtils(21076): could not check pending caller\r\n01-24 15:28:14.309 D/ForegroundUtils(21076): Foreground changed, PID: 3567 UID: 5018 foreground: false\r\n01-24 15:28:14.309 D/ForegroundUtils(21076): Foreground UID/PID combinations:\r\n01-24 15:28:14.310 D/ForegroundUtils(21076): UID: 10110 PID: 23150\r\n01-24 15:28:14.323 W/libprocessgroup( 5488): kill(-3567, 9) failed: No such process\r\n01-24 15:28:14.323 I/libprocessgroup( 5488): Successfully killed process cgroup uid 5018 pid 3567 in 177ms\r\n01-24 15:28:14.328 I/Zygote  ( 5267): Process 3567 exited due to signal (9)\r\n01-24 15:28:14.369 D/VibratorService( 5488): Turning vibrator off\r\n01-24 15:28:14.370 D/SecVibrator-HAL( 5296): writeNode node:/sys/class/timed_output/vibrator/enable val:0\r\n01-24 15:28:14.376 D/[a11y]  ( 5488): getUserAccounts0\r\n01-24 15:28:14.379 I/chatty  ( 5488): uid=1000(system) Binder:5488_11 identical 1 line\r\n01-24 15:28:14.381 D/[a11y]  ( 5488): getUserAccounts0\r\n01-24 15:28:14.401 E/dumpstate( 5093): can't find the pid\r\n01-24 15:28:14.402 E/dumpstate( 5093): Failed to find: /data/misc/anrd/\r\n01-24 15:28:14.402 D/dumpstate( 5093): Skipping systrace because '/sys/kernel/debug/tracing/tracing_on' content is '0'\r\n01-24 15:28:14.402 D/dumpstate( 5093): /data/misc/raft does not exist or is not a directory\r\n01-24 15:28:14.403 D/dumpstate( 5093): Adding dir /cache/recovery (recursive: 1)\r\n01-24 15:28:14.406 D/[a11y]  ( 5488): getUserAccounts0\r\n01-24 15:28:14.649 D/dumpstate( 5093): Duration of '/cache/recovery': 0.246s\r\n01-24 15:28:14.649 D/dumpstate( 5093): Adding dir /data/misc/recovery (recursive: 1)\r\n01-24 15:28:14.650 D/dumpstate( 5093): Duration of '/data/misc/recovery': 0.001s\r\n01-24 15:28:14.650 D/dumpstate( 5093): Adding dir /data/misc/update_engine_log (recursive: 1)\r\n01-24 15:28:14.650 D/dumpstate( 5093): Duration of '/data/misc/update_engine_log': 0.000s\r\n01-24 15:28:14.650 D/dumpstate( 5093): Adding dir /data/misc/logd (recursive: 0)\r\n01-24 15:28:14.651 D/dumpstate( 5093): Duration of '/data/misc/logd': 0.000s\r\n01-24 15:28:14.651 D/dumpstate( 5093): Adding dir /data/misc/profiles/cur (recursive: 1)\r\n01-24 15:28:14.765 D/dumpstate( 5093): Duration of '/data/misc/profiles/cur': 0.114s\r\n01-24 15:28:14.765 D/dumpstate( 5093): Adding dir /data/misc/profiles/ref (recursive: 1)\r\n01-24 15:28:14.815 D/dumpstate( 5093): Duration of '/data/misc/profiles/ref': 0.050s\r\n01-24 15:28:14.858 D/dumpstate( 5093): Adding zip text entry systemserver_fileinfo.txt\r\n01-24 15:28:14.858 D/dumpstate( 5093): Duration of 'FILE INFO': 0.043s  ```\r\n\r\nThe default AI Benchmark successfully runs all the models which definitely has softmax operation also.\r\nIs there any known reason for softmax failure via lable_image.\r\n\r\n", "comments": ["It seems it's caused by NNAPI stuff not TFLite.", "@praveen-dn : @freedomtan is correct.\r\n@praveen-dn : Which label example were you testing and how did you enable NNAPI?", "@praveen-dn: Please reopen if you are still seeing the issue."]}, {"number": 25313, "title": "FastParseExample hash collision", "body": "**System information**\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):  1.12.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): GCC4.8.5\r\n\r\n**Describe the current behavior**\r\nbuild config index retry always failed.\r\ncode in `tensorflow/core/util/example_proto_fast_parsing.cc` function `FastParseExample`\r\n[https://github.com/tensorflow/tensorflow/blob/a93f06d160955c99bc279a419902ac40824a2cab/tensorflow/core/util/example_proto_fast_parsing.cc#L988](url)\r\nok flag is not reset to true. And this will fail for 1000 times, if hash collision is found at first time.\r\n**Describe the expected behavior**\r\n reset ok flag and rehash with new seed, so that retry logic will work\r\n\r\n", "comments": ["@yann-yy were you able to build and install TF? Please provide OS? If the build is failing, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#linux). Thanks!", "> @yann-yy were you able to build and install TF? Please provide OS? If the build is failing, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#linux). Thanks!\r\n\r\nhi, @jvishnuvardhan ,\r\n    Thanks for your reply. This is not a build issue. And I have submit a [pull request](https://github.com/tensorflow/tensorflow/pull/25449) to fix this. This issue could be closed.", "@yann-yy Thank you! I am closing this issue. Thanks again!", "I think this should be kept open until the PR with the fix lands", "Thank you for #29121, fixing this"]}, {"number": 25312, "title": "Tensorflow ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Scientific Linux 7.6, but also tested on Centos 7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12.0\r\n- Python version: 2.6\r\n- Installed using virtualenv? pip? conda?: manually compiling using bazel\r\n- Bazel version (if compiling from source): 1.16.1, but also tried 1.15.0 and 1.18.0\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: Cuda 9 CuDNN 7\r\n- GPU model and memory: Compute Cluster: multpile Tesla K80 (24GB) or Tesla P100 (16GB).\r\n\r\n**Describe the problem**\r\nDuring compilation I'm encountering the following ERROR messages:\r\n[205 / 208] Compiling tensorflow/contrib/image/kernels/adjust_hsv_in_yiq_op_gpu.cu.cc; 10s local\r\nERROR: /tmp/Tensorflow/PACKAGES/tensorflow/tensorflow/contrib/image/BUILD:115:1: undeclared inclusion(s) in rule '//tensorflow/contrib/image:python/ops/_distort_image_ops_gpu':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/contrib/image/kernels/adjust_hsv_in_yiq_op_gpu.cu.cc':\r\n  '/tmp/bazel/userid_output/external/eigen_archive/Eigen/Core'\r\n  '/tmp/bazel/userid_output/external/eigen_archive/Eigen/src/Core/util/DisableStupidWarnings.h'\r\n  '/tmp/bazel/userid_output/external/eigen_archive/Eigen/src/Core/util/ReenableStupidWarnings.h\r\nHowever, when I add ---verbose_failures to bazel and execute the failing compilation job manually to debug the issue, the compilation is executed correctly. So I'm assuming only some check of bazel is failing, although the compilation itself could be executed correctly.\r\n\r\nRestarting the build (after my manually compilation) will also continue, until the next job where Eigen is used again. The build is failing with a similar error. Previous versions, including 1.11.0, are not encountering this issue. I could not find any noticeable changes in the Eigen build files for bazel or related to Eigen. Therefore I'm currently out of ideas.....\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel --output_base=/tmp/bazel/userid_output build --jobs 12 -c opt --copt=-mavx2 --copt=-O --copt=-msse4.2 --copt=-mfma -config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n(I'm using bazel output_base because my home directory is on NFS)\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This is really interesting. I have never seen this issue.\r\nWhile this may cost you quite some time, could you try running:\r\n`bazel clean --expunge`\r\nThen rerunning configure, then build?\r\n\r\nAlso, did you modify the source code at all?", "I'm having the same issue, CentOS 7, Tensorflow 1.12, GCC 7.3.0, CUDA 9, cuDNN 7. Something breaks eigen include paths when using --output_base. I found this when trying to build using EasyBuild and replicated the steps when building manually to chase what was failing the build. The following works for me:\r\n\r\n`bazel build --compilation_mode=opt --action_env=PYTHONPATH --config=opt --config=cuda --distinct_host_configuration=false --config=mkl //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nAdding --output_base=/home/jojuh/test gives me (the actual rule name varies as these are build parallel?):\r\n\r\n```\r\nERROR: /home/jojuh/tensorflow-1.12.0/tensorflow/tools/pip_package/BUILD:48:1 undeclared inclusion(s) in rule '//tensorflow/core:lib_internal_impl':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/lib/random/distribution_sampler.cc':\r\n  '/home/jojuh/test/external/eigen_archive/Eigen/Core'\r\n  '/home/jojuh/test/external/eigen_archive/Eigen/src/Core/util/DisableStupidWarnings.h'\r\n  '/home/jojuh/test/external/eigen_archive/Eigen/src/Core/util/Macros.h'\r\n  '/home/jojuh/test/external/eigen_archive/Eigen/src/Core/util/MKL_support.h'\r\n  ....\r\n```\r\n\r\nApparently eigen is included via a wrapper (//third_party/eigen3) which points to eigen_archive somehow (https://github.com/tensorflow/tensorflow/issues/3743#issuecomment-239580919)\r\n\r\nI'm new to Bazel and this is currently blocking our progress so I'm happy to provide more information if necessary.\r\n\r\n\r\n", "Thank you both for your reply/additional information. \r\n@gunan My buildscript already executes the bazel clean --expunge step to ensure it starts from scratch. The only source change I'm making is adding the --output_base=${BAZEL_OUTPUT_BASE} option to the bazel command in configure.py:\r\n**sed -i -r \"s#\\['bazel', '--batch', '--bazelrc=/dev/null', 'version'\\]#\\['bazel'    ,  '--output_base=${BAZEL_OUTPUT_BASE}', '--batch', '--bazelrc=/dev/null', 'version'\\]#    \" configure.py**\r\n\r\n@Jojuh The actual rules indeed vary due to the parallel build. You could force a build using a single thread by adding --jobs 1to bazel for debugging purposes. But it takes a while before the actual error pops up.\r\n\r\nAs explained before, I also require the --output_base because the shares/disks I have write access to are located on external file servers (except the /tmp folder, which I now use for the bazel output base).", "I finally got my build to complete, had to add compiler flags for the crosstool_wrapper_driver_is_not_gcc. Not absolutely sure if I understood correctly, but by passing -MMD instead of -MD to the compiler makes it ignore system headers -> no undeclared dependency errors. Would be happy if someone would ELI5 this to me.\r\n\r\n```\r\ndiff -ru tensorflow-1.12.0.orig/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl tensorflow-1.12.0/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl\r\n--- tensorflow-1.12.0.orig/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl      2018-11-02 03:35:10.000000000 +0200\r\n+++ tensorflow-1.12.0/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl  2019-02-18 09:20:02.114287752 +0200\r\n@@ -258,6 +261,7 @@\r\n   cpu_compiler_flags = [flag for flag in sys.argv[1:]\r\n                              if not flag.startswith(('--cuda_log'))]\r\n\r\n+  cpu_compiler_flags = ['-MMD' if flag == '-MD' else flag for flag in cpu_compiler_flags]\r\n   return subprocess.call([CPU_COMPILER] + cpu_compiler_flags)\r\n\r\n if __name__ == '__main__':\r\n```\r\n\r\nThis led me to the solution: https://stackoverflow.com/questions/35256110/tensorflow-build-fails-with-missing-dependency-error", "@Jojuh Thank you, I will try your suggestion. What I do not understand is how this could be an issue for bazel and not for the compilation step itself. Because, like mentioned before, if I executed the failing step manually, it compiles without any issue. But hopefully the additional information will help to find the issue, because it was not an issue before release 1.12.0.....", "I tried @Jojuh 's solution but it did not solve the problem for me. What did work in my case was to make a symbolic link from the normal cache directory to the folder on another disk I was trying to use for the output directory. Then I could run without the --output_base flag but still use the storage location I wanted. I was also running out of memory, which I fixed by allocating some swap. Not sure if that was related.\r\n\r\nFor the workaround:\r\ncd ~/\r\nmv .cache/bazel /path/to/new/folder/\r\nln -s /path/to/new/folder/bazel .cache/bazel\r\n\r\nIt seems like there is a clear bug with the --output_base flag, but since Jojuh's fix (hopefully I applied it properly) didn't work in my case, I wanted to share my workaround.\r\n\r\nDetails:\r\nTensorflow 1.12\r\nCuda 9.0\r\nCPU aarch64 (Jetson TX1)\r\nPython 3.5.2\r\nUbuntu from JetPack 3.3\r\n\r\nWhere I was also plagued with \"undeclared inclusion(s) in rule {..some build rule..}:\r\nthis rule is missing dependency declarations for the following files included by ....\" errors followed by a list of files from \"...external/eigen_archive/Eigen/...\" while using the --output_base flag.\r\n\r\nIn my case I was also running out of memory, though I don't know if that triggered this issue. I fixed the memory issues by allocating some swap space.\r\n\r\nAfter the workaround I mentioned and adding the swap memory, I've been able to sustain build for over an hour (it's an arm system). Previously the errors would come up after only ten minutes, so this seems like progress...", "The patch supplied by @Jojuh did indeed also not fix the issue for my. I will now also try the suggestion mentioned by @tlalexander. But clearly something must have changed related to the Eigen package, because the --output_base option was never an issue before.", "@Jojuh: I was indeed also able to compile the module by creating a symbolic link to temp instead of using the output_base. @gunan hopefully this is helpful in locating the actual issue. But at least we have a workaround for now. ", "This issue seems to be fixed for version 1.13.1. I was able to use --output_base again.", "Could someone explain the -MMD -MD difference again? ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25312\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25312\">No</a>\n", "Closing as it is reported that the issue is fixed in the newer versions."]}, {"number": 25311, "title": "tflite_convert makes no difference to converted model after changing detection_postprocess.cc", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version (or github SHA if from source): **r1.12**\r\n\r\n\r\n**Describe the issue**\r\nI am converting SSD object detection model trained through object detection api. The only difference in my custom model and other pre-trained model is that the **detection_boxes** have 6 output coords instead of 4 box coords, I am also predicting the center coordinates of the bounding box to make the model more robust. It is performing great in frozen_graph.pb but as I convert it into .tflite format by using tflite_convert, it only gives 4 box coords whereas I need 6 box coords. But, eventually I figured out that I need to change the [detection_postprocess.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/detection_postprocess.cc), but changes made in this file are not effecting the model, it seems like tflite_convert is ignoring this file while converting the model.\r\n\r\n**Steps to reproduce the issue**\r\n- tensorflow build from source and installed in a virtualenv\r\n- make some changes in the [detection_postprocess.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/detection_postprocess.cc)\r\n- uninstall tensorflow from virtualenv\r\n- again build tensorflow using bazel and install it using pip in same virtualenv\r\n- run following command to convert model:\r\n`tflite_convert \\\r\n  --output_file=detect.tflite \\\r\n  --graph_def_file=tflite_graph.pb \\\r\n  --input_shapes=1,300,300,3 \\\r\n  --input_arrays=normalized_input_image_tensor \\\r\n  --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n  --mean_values=128 \\\r\n  --std_dev_values=128 \\\r\n  --change_concat_input_ranges=false \\\r\n  --allow_custom_ops` \r\n\r\n**More information**\r\n- tflite_graph.pb is obtained by using [export_tflite_ssd_graph.py](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py)\r\n\r\n\r\nHow to make sure that changes made in detection_postprocess.cc will effect the converted model using tflite_convert?", "comments": ["As, suggested by @achowdhery in this [comment](https://github.com/tensorflow/tensorflow/issues/21230#issuecomment-456803457), I already build the whole tensorflow from source after making changes to detection_postprocess.cc but still no success."]}, {"number": 25310, "title": "i use tf.data.TFRecordDataset read tfcord , why the data i read is not right", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\ntensorrflow 1.12\r\ntensorboard 1.12\r\nwindows7 cpu\r\n\r\n**Describe the current **\r\ni use tf.data.TFRecordDataset read tfcord , why the data i read is not right\r\n\r\n**Describe the expected behavior**\r\nThe label and image is Corresponding\r\n\r\n**Code to reproduce the issue**\r\nhttps://stackoverflow.com/questions/54424335/i-use-tf-data-tfrecorddataset-read-tfcord-why-the-data-i-read-is-not-right\r\n\r\n", "comments": ["This seems to have been answered [on Stack Overflow](https://stackoverflow.com/q/54424335/3574081)."]}, {"number": 25309, "title": "Set default TF_BUILD_VERSION to r1.13 on MKL images", "body": "", "comments": []}, {"number": 25308, "title": "[tf.keras 1.8.0] the size of loaded model increases after calling model.fit()", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.5.2\r\n- **GPU model and memory**: GTX 1080 Ti\r\n\r\n### Describe the problem\r\nI'm using tf.keras. I'm working with Densenet169 from tensorflow.python.keras.applications.densenet. I checked the model size (almost 53 MB) on Keras website, and obviously the size could be verified by saving it directly (using save_model()) on my PC before training. When calling mode.fit(), I found the size of model increased up to 153 MB without any changes of the number of parameter, data type and model structure (even for 1 epoch). Do you have any idea?\r\n\r\n### The list of what I tested\r\n- Comparison model.get_config() before and after training -> no difference\r\n- The possibility that a list variable for validation set was not set free in model.fit() function -> no difference whether validation set exists or not\r\n- Data type for the weights -> fixed as float32\r\n- Callback functions (checkpoint and earlystopping) -> no effect\r\n- model.compile() was called for several times to drop learning rate after a few epochs -> no relationship between model.compile() and model size  ", "comments": ["Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. Github is mainly for addressing bugs in installation and performance. Thanks!", "I am closing this as it is not related to Build/Installation or Bug/Performance. Thanks!"]}, {"number": 25306, "title": "[Docs] Do not link to symbols within code blocks", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.12, 1.13, 2.0\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/TFRecordDataset#map\r\n\r\n\r\n**Describe the documentation issue**\r\nThe [documentation's symbol resolver](https://github.com/tensorflow/docs/blob/58e99a76ebd4b533fed5591147441ee3f6db3dae/tools/tensorflow_docs/api_generator/parser.py#L119) will automatically replace all appearances of \\`tf.symbol\\` with markdown links to the symbol's definition, which is generally great, however when these symbols appear \r\nwithin code blocks (e.g. in [usage examples](https://github.com/tensorflow/tensorflow/blob/09696450acc1a69dcac4717de87ee4f30544865a/tensorflow/python/data/ops/dataset_ops.py#L912)) the inserted link cannot be rendered correctly, and is interpreted as a plain string like `<a href=\"../../tf/Tensor\"><code>tf.Tensor</code></a>`. Thus, it would be best if symbols within code blocks are not replaced by links, or by links that do render correctly (AFAIK links cannot be placed within code blocks). \r\n\r\nNote: I have only observed this behavior in comments within code blocks.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nYes, I could submit a PR. However I'm unsure as to the best strategy to efficiently determine if the symbol lies within a code block. I imagine that one could split the strings by re matching ` ``` ` and then deciding whether or not to `re.sub` the current section of the docstring, but this much string manipulation might be somewhat costly. ", "comments": ["This has been fixed."]}, {"number": 25305, "title": "how to enable xla with estimator", "body": "cuda:9.0\r\ntensorflow:1.12.0\r\n\r\ni want to enable xla, it is my code:\r\n```\r\nsession_config = tf.ConfigProto()\r\nsession_config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\ndistribution_strategy = distribution_utils.get_distribution_strategy(flags_core.get_num_gpus(flags_obj), flags_obj.all_reduce_alg)\r\nrun_config = tf.estimator.RunConfig(\r\n      train_distribute=distribution_strategy,\r\n      session_config=session_config,\r\n      save_checkpoints_secs=60*60*24)\r\nclassifier = tf.estimator.Estimator(\r\n      model_fn=model_function, model_dir=flags_obj.model_dir, config=run_config,\r\n      warm_start_from=warm_start_settings, params={\r\n          'resnet_size': int(flags_obj.resnet_size),\r\n          'data_format': flags_obj.data_format,\r\n          'batch_size': flags_obj.batch_size,\r\n          'resnet_version': int(flags_obj.resnet_version),\r\n          'loss_scale': flags_core.get_loss_scale(flags_obj),\r\n          'dtype': flags_core.get_tf_dtype(flags_obj),\r\n          'fine_tune': flags_obj.fine_tune\r\n      })\r\n......\r\nclassifier.train(input_fn=lambda: input_fn_train(num_train_epochs),\r\n                       hooks=train_hooks, max_steps=flags_obj.max_train_steps)\r\n```\r\nand get_distribution_strategy() is \r\n```\r\ndef get_distribution_strategy(num_gpus,all_reduce_alg=None,turn_off_distribution_strategy=False):\r\n  if num_gpus == 1:\r\n      return tf.contrib.distribute.OneDeviceStrategy(\"device:GPU:0\")\r\n  else:  # num_gpus > 1 and not turn_off_distribution_strategy\r\n      return tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus,\r\n          cross_tower_ops=tf.contrib.distribute.AllReduceCrossTowerOps(\r\n              all_reduce_alg, num_packs=2))\r\n```\r\n\r\nwhen i train by one gpu, and the distribute strategy is OneDeviceStrategy(\"device:GPU:0\"), xla is ok,\r\nbut when i train by two gpus, and the distribute strategy is MirroredStrategy, xla has error:\r\n```\r\n *** WARNING *** You are using ptxas 9.0.176, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\r\n\r\nYou do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.\r\nI0130 10:55:37.183267 140192737777408 tf_logging.py:115] Running local_init_op.\r\nI0130 10:55:37.853480 140192737777408 tf_logging.py:115] Done running local_init_op.\r\nI0130 10:56:25.948692 140192737777408 tf_logging.py:115] Saving checkpoints for 20020 into /tmp/model.ckpt.\r\n2019-01-30 10:57:17.807557: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-01-30 10:57:17.807765: E tensorflow/compiler/xla/service/gpu/cudnn_convolution_algorithm_picker.cc:338] Internal: All algorithms tried for convolution %custom-call = (f32[64,256,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,64,56,56]{3,2,1,0} %reduce-window.13518.10568, f32[1,1,64,256]{1,0,2,3} %copy.56), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target=\"__cudnn$convForward\", backend_config=\"{\\\"convResultScale\\\":1}\" failed.  Falling back to default algorithm.\r\n```\r\n\r\nor\r\n\r\n```\r\n*** WARNING *** You are using ptxas 9.0.176, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\r\n\r\nYou do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.\r\nI0130 11:05:42.691364 140448661317376 tf_logging.py:115] Running local_init_op.\r\nI0130 11:05:43.474775 140448661317376 tf_logging.py:115] Done running local_init_op.\r\nI0130 11:06:26.442031 140448661317376 tf_logging.py:115] Saving checkpoints for 20020 into /tmp/model.ckpt.\r\nI0130 11:07:53.810114 140448661317376 tf_logging.py:115] cross_entropy = 2.8381975, learning_rate = 0.02560256, train_accuracy = 0.390625\r\nI0130 11:07:53.812137 140448661317376 tf_logging.py:115] loss = 6.150483, step = 20020\r\n\r\nE0130 11:09:08.093458 140448661317376 tf_logging.py:105] Model diverged with loss = NaN.\r\n\r\n```\r\n\r\n\r\n\r\n", "comments": ["@jlebar ", "> *** WARNING *** You are using ptxas 9.0.176, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\r\n\r\nWould you be willing to help me understand how I can make this error more clear?", "@jlebar  \r\nRELEASE.md:\r\n```\r\n  Google discovered in mid-December 2017 that the PTX-to-SASS compiler in CUDA 9\r\n  and CUDA 9.1 sometimes does not properly compute the carry bit when\r\n  decomposing 64-bit address calculations with large offsets (e.g. `load [x +\r\n  large_constant]`) into 32-bit arithmetic in SASS.\r\n```\r\nis a known bug, but it is not the main reason i think, because i run xla with distribute strategy in benchmark with the same environment, and also run an example of tensorpack :\r\n[https://github.com/tensorpack/tensorpack/tree/master/examples/ResNet](url)\r\nand they have the same warning, but work fine.\r\n", "Hi, how am I supposed to help you debug this problem when the possibility exists that you're hitting a deep compiler bug?", "@jlebar Hi, can you explain why run benchmark is ok, it also has the warning, but has no error. \r\n", "If literally anything about the GPU environment changes, then XLA may generate different PTX and trigger the bug, or the memory buffers in question may have different addresses and trigger the bug.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 25304, "title": "Failed to build tensorflow pip package from source in dockerfile provided under tools folder", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nDocker image nvidia/cuda:9.0-base-ubuntu16.04 provided by tensorflow/tensorflow/tools/dockerfiles/dockerfiles/nvidia-devel.Dockerfile\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNA\r\n- TensorFlow installed from (source or binary):\r\nNA\r\n\r\n- TensorFlow version:\r\nv1.12.0\r\n\r\n- Python version:\r\n3.5\r\n\r\n- Installed using virtualenv? pip? conda?:\r\nNONE\r\n\r\n- Bazel version (if compiling from source):\r\n20.0\r\n\r\n- GCC/Compiler version (if compiling from source):\r\ngcc 5.4\r\n\r\n- CUDA/cuDNN version:\r\nCUDA 9\r\n\r\n- GPU model and memory:\r\n1050ti\r\n\r\n\r\n\r\n**Describe the problem**\r\nBuild failed in Bazel.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Checkout tensorflow release tag v.1.12 commit a6d8ffae097d0132989ae4688d224121ec6d8f35.\r\n1. Build image `docker build -f ./dockerfiles/nvidia-devel.Dockerfile --build-arg USE_PYTHON_3_NOT_2=True -t tf-nvidia-devel-python3 .`\r\n1. Run container `docker run --runtime=nvidia  -v /home/Ricky/repo/github/tensorflow:/tensorflow -it tf-nvidia-devel-python3`\r\n1. Run `\\.configure` as a sane person, eg enable CUDA and etc.\r\n1. Run ` bazel build -k //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nroot@252b84b2aa65:/tensorflow# bazel build -k //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/tensorflow/tools/bazel.rc\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=130\r\nINFO: Reading rc options for 'build' from /tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages --python_path=/usr/bin/python3 --define with_ignite_support=true --define with_xla_support=true --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_ROCM=0 --action_env TF_NEED_CUDA=1 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda --action_env TF_CUDA_VERSION=9.0 --action_env CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu --action_env TF_CUDNN_VERSION=7 --action_env TENSORRT_INSTALL_PATH=/usr/lib/x86_64-linux-gnu --action_env TF_TENSORRT_VERSION=4.1.2 --action_env NCCL_INSTALL_PATH=/usr/lib/x86_64-linux-gnu --action_env NCCL_HDR_PATH=/usr/include --action_env TF_NCCL_VERSION=2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --action_env TF_CUDA_CLANG=0 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/gcc --config=cuda\r\nERROR: Config value cuda is not defined in any .rc file\r\nINFO: Invocation ID: ef2903c5-eec1-4d50-8f50-cb88cfc320e1\r\n```\r\n\r\nPlease don't take it personally. But you guys write a very very very bad dockerfile which doesn't handle the build tools dependency carefully. I found that bazel is 0.22!\r\n\r\n```\r\nroot@effd8d74a22c:/# bazel version\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: f740cfea-bba1-4a08-b04c-c41c4cd39a8b\r\nBuild label: 0.22.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Mon Jan 28 12:58:08 2019 (1548680288)\r\nBuild timestamp: 1548680288\r\nBuild timestamp as int: 1548680288\r\n```", "comments": ["@rickyzhang82 Could you try the latest docker version. There were some issues that were cleared in the recent docker file. Please let us know how it progresses. Thanks!", "I am closing this hoping it was resolved by using newer docker. Please let me know if I am mistaken. Thanks!"]}, {"number": 25303, "title": "How can I pass a parameter for ever examples in a tensor?", "body": "I have a many sequence sentence with different length\uff0cand I padded all sentence with same length 100 \uff0cI  got a input tensor with shape:  (1000, 100, 10) \uff0cbut I want pass the real valid length of every length information together with input tensor, and use the parameter to do some calculates, how can I suppose to do?\r\n\r\nI have a one way that is create a new input with shape(1000,100,1)\uff0cpadded with all valid length of every examples. But when I want use the parameter as a indice to do the slice of the input tensor, how can I convert a tensor to a number to do,say : tensor[: ,slice,: ]?", "comments": ["@poppintiger Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "@jvishnuvardhan OK, thank you so much!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 25302, "title": "lite: promote precision of quantization multiplier (master branch)", "body": "This is https://github.com/tensorflow/tensorflow/pull/24036 (`1.12` branch) for `master` branch.\r\n\r\n*Btw, though this (and some other potential) precision issue exist on branches like `1.11`, `1.9` and so on, I will personally only look into master branch after...*", "comments": ["Any further update needed? I am wondering patch may need rebase as `master` moves quickly..."]}, {"number": 25301, "title": "TensorFLow v1.13.0rc0 tflite_convert fails to convert TensorFlow model with FusedBatchNormalization layers", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 26\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.0rc0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Nvidia Tesla P100\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\ntflite_convert fails to convert simple conv2d model with batch norm file to .tflite. The trained model was frozen to a .pb file using freeze_graph. It fails with the following errors:\r\n\r\ntflite_convert --graph_def_file=frozen_saved_model.pb --output_file=frozen_saved_model.tflite --input_shapes=\"1,28,28,1\" --input_arrays=conv2d_input --output_arrays=dense_1/Softmax\r\n\r\nTraceback (most recent call last):\r\n  File \"/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 426, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input 0 of node batch_normalization_v1/cond/ReadVariableOp/Switch was passed float from batch_normalization_resource.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/virtualenv/tf-1.13.0-gpu/bin/tflite_convert\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 442, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 438, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 122, in _convert_model\r\n    converter = _get_toco_converter(flags)\r\n  File \"/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 109, in _get_toco_converter\r\n    return converter_fn(**converter_kwargs)\r\n  File \"/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 274, in from_frozen_graph\r\n    _import_graph_def(graph_def, name=\"\")\r\n  File \"/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 501, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 430, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: Input 0 of node batch_normalization_v1/cond/ReadVariableOp/Switch was passed float from batch_normalization_v1/gamma:0 incompatible with expected resource.\r\n\r\n\r\n**Describe the expected behavior**\r\nA TensorFlow Lite version of the frozen model file should have been generated by the tool.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.keras.optimizers as optimizers\r\nimport tensorflow.keras.losses as losses\r\nfrom tensorflow.keras.datasets import mnist\r\n\r\n(train_x, train_y), _ = mnist.load_data()\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, kernel_size=(3, 3),\r\n                 activation='relu',\r\n                 input_shape=(28,28,1),\r\n                 kernel_regularizer=regularizers.l1(reg_weight)))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l1(reg_weight)))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l1(reg_weight)))\r\nmodel.add(Dense(10, activation='softmax', kernel_regularizer=regularizers.l1(reg_weight)))\r\n\r\nmodel.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\r\n\r\nmodel.fit(train_x, train_y, batch_size=64, epochs=2, verbose=1)\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Any update or workarounds for this?", "The workaround I'm using is to replace tf.keras.layers.BatchNormalization layers with tf.layers.batch_normalization(fused=False). tflite_convert converts the model successfully with this. \r\n\r\n\"fused=False\" is needed if converting to a quantized flatbuffer file (in my case, I'm training with fake quantization nodes). \r\n\r\nOtherwise, there will be errors regarding missing min/max information in the graph:\r\n\r\n`2019-03-25 15:04:13.845816: F tensorflow/lite/toco/tooling_util.cc:1702] Array batch_normalization_v1/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array batch_normalization_v1/FusedBatchNorm, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.`", "> The workaround I'm using is to replace tf.keras.layers.BatchNormalization layers with tf.layers.batch_normalization(fused=False). tflite_convert converts the model successfully with this.\r\n\r\nyou just add the only property \"fused=False\" in tf.layers.batch_normalization when you training???", "@ruffles1 Thank you for the workaround. I noticed tf.keras.layer.BatchNormalization also has the \"fused\" parameter and I tried to set \"fused=False\". But now the tf.contrib.quantize.create_training_graph() only inserts fakequant nodes to partial of my graph... Did you try to use BatchNorm from tf.keras? ", "> @ruffles1 Thank you for the workaround. I noticed tf.keras.layer.BatchNormalization also has the \"fused\" parameter and I tried to set \"fused=False\". But now the tf.contrib.quantize.create_training_graph() only inserts fakequant nodes to partial of my graph... Did you try to use BatchNorm from tf.keras?\r\n\r\n\u4ed6\u4e0d\u662f\u8bf4\u7528tf.layer.bn\u4ee3\u66ff\u4e48\uff1f", "@ruffles1 @hahadashi  I figured out my problem. Currently (r1.13), BN layer has to be followed by an activation layer (say relu) to be recognized by contrib/quantize. Since my network does have linear Conv-BN layer, it will complain about lacking min/max for FusedBN node when convert to quantized TFLite model. ", "@NEU-Gou I just tried your suggestion, and unfortunately, fake quant ops are still failing to be added to the batch norm ops. I get the same error when converting to tflite. Could you please share an example of a working model that includes fused batch norm and fake quant ops?", "> @NEU-Gou I just tried your suggestion, and unfortunately, fake quant ops are still failing to be added to the batch norm ops. I get the same error when converting to tflite. Could you please share an example of a working model that includes fused batch norm and fake quant ops?\r\n@ruffles1\r\nyou try the 1.13?? ", "@hahadashi Yes, I'm on 1.13.1", "@ruffles1 Double checked your error massage. Seems that it's a problem related to tf.keras.backend.learning_phase. Did you try to run \"tf.keras.backend.set_learning_phase(1)\" in training and when converting, BEFORE loading the .pb model, run \"tf.keras.backend.set_learning_phase(0)\". ", "> @ruffles1 Double checked your error massage. Seems that it's a problem related to tf.keras.backend.learning_phase. Did you try to run \"tf.keras.backend.set_learning_phase(1)\" in training and when converting, BEFORE loading the .pb model, run \"tf.keras.backend.set_learning_phase(0)\".\r\n@NEU-Gou\r\ncan you try a training experiment on you env, a net including  tf.nn.conv2d + tf.layers.batch_normalization + tf.nn.relu. ", "I resolved it by saving the trained weights, clearing the session, setting the learning phase, loading the model back, then restoring the trained weights into the (now evaluation) model. Doing this removes all of the 'switch' ops from the graph which relate to the BatchNorm layers and converts your dropout nodes to identity ops. The model can then be frozen like usual.\r\n\r\n```\r\ntrained_model.save_weights(output_weights)\r\n\r\ntf.keras.backend.clear_session()\r\ntf.keras.backend.set_learning_phase(0)    # This is the important part\r\neval_model = model_build_function()\r\neval_model.load_weights(output_weights, by_name=True)\r\n```", "@LukeBolly This is more or less what I've been dong to quantize aware train a model with batch norm. My problem is when training with fused batch norm layers. In this case, the conversion will fail, even with using the method you've described. Have you tested with fused batch norm layers?\r\n", "@ruffles1 @hahadashi I tried a few different setting and figured some \"requirements\" for the contrib/quantize/create_train(eval)_graph to work:\r\n- For CONV2D+BN+ReLU, the CONV2D op has to be \"use_bias=False\"\r\n- For CONV2D with bias, it CANNOT followed by BN layer\r\n\r\nHope this is helpful. Good luck", "> @ruffles1 @hahadashi I tried a few different setting and figured some \"requirements\" for the contrib/quantize/create_train(eval)_graph to work:\r\n> \r\n> * For CONV2D+BN+ReLU, the CONV2D op has to be \"use_bias=False\"\r\n> * For CONV2D with bias, it CANNOT followed by BN layer\r\n> \r\n> Hope this is helpful. Good luck\r\n@NEU-Gou \r\nthanks\uff0c\r\n\r\nwhich api to call the BN layer??(tf.layers.batch_normalization or??) and which tf version you using??\r\n", "@hahadashi I'm using r1.13 and tf.keras.layers ", "thanks\r\n\r\n\r\n\u53d1\u81ea\u6211\u7684iPhone\r\n\r\n------------------ Original ------------------\r\nFrom: Mengran Gou <notifications@github.com>\r\nDate: Mon,May 6,2019 5:46 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: hahadashi <397270265@qq.com>, Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] TensorFLow v1.13.0rc0 tflite_convert fails to convert TensorFlow model with FusedBatchNormalization layers (#25301)\r\n\r\n\r\n\r\n\r\n@hahadashi I'm using r1.13 and tf.keras.layers\r\n \r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or mute the thread.", "It seems the original script has some bugs (Keras complaining shape mismatch etc). I fixed the script and tried to call `TFLiteConverter.from_keras_model_file` directly, and everything works fine (with 1.13.0rc0 and 1.13.1). \r\n\r\nThe takeaway is you don't need to call freeze_graph by yourself when using a Keras model. It seems `TFLiteConverter` does a better job for you. \r\n \r\nThe script that works on my machine:\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras.optimizers as optimizers\r\nimport tensorflow.keras.losses as losses\r\nfrom tensorflow.keras.datasets import mnist\r\n\r\n(train_x, train_y), _ = mnist.load_data()\r\n\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras.models import *\r\nregularizers = tf.keras.regularizers\r\n\r\nreg_weight = 0.00001\r\n\r\nmodel = Sequential()\r\nmodel.add(Reshape((28, 28, 1), input_shape=(28, 28)))\r\nmodel.add(Conv2D(32, kernel_size=(3, 3),\r\n                 activation='relu',\r\n                 input_shape=(28,28,1),\r\n                 kernel_regularizer=regularizers.l1(reg_weight)))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l1(reg_weight)))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l1(reg_weight)))\r\nmodel.add(Dense(10, activation='softmax', kernel_regularizer=regularizers.l1(reg_weight)))\r\n\r\nmodel.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\r\n\r\ntrain_x = train_x[:100, :, :]\r\ntrain_y = train_y[:100]\r\nmodel.fit(train_x, tf.keras.utils.to_categorical(train_y), batch_size=64, epochs=2, verbose=1)\r\nmodel.save(\"/tmp/25301.h5\")\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(\"/tmp/25301.h5\")\r\ntfl = converter.convert()\r\nprint(len(tfl))\r\n```\r\n\r\nClosing this. Feel free to reopen if there's unresolved questions. "]}, {"number": 25300, "title": "Add optimization to convert Square(Sub(x, y)) to SquaredDifference(x, y)", "body": "This PR adds a graph optimization that converts `Square(Sub(x, y))` to `Identity(SquaredDifference(x, y))` for improved performance.\r\n\r\nThis is basically a generalization of #25256 making this optimization available for user code as well.\r\n\r\nNote: I haven't rigorously benchmarked `tf.square(x - y)` against `tf.squared_difference(x, y)` to verify the performance boost for every case. Also, this is my first PR that touches C++ in the TF codebase, so please keep in mind that there may be some subtleties that I miss that would prevent this optimization from working well in all cases.", "comments": ["Rebased! @rthadur @hgadig Anything else I can do to speedup review?"]}]