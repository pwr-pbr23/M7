[{"number": 27710, "title": "Added IsFinite Support for the tflite.", "body": "This is part of #21526", "comments": ["@renjie-liu , can you please review the PR and provide your feedback.\r\n\r\nRegards\r\nAmit", "@renjie-liu  thanks for the feedback i have made the changes as per your suggestion, kindly check.\r\n\r\n\r\nRegards\r\nAmit", "@renjie-liu  , rebased and pushed again.\r\n\r\nRegards\r\nAmit", "> lgtm, but since it involves in schema change, please wait for another review as well.\r\n\r\n@renjie-liu , thanks alot for the review, highly appreciate your comments. I have made all the modifications, will wait as suggested by you.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 could you please resolve the conflicts? Thank you !", "@gbaned , thanks for pointing this out, i have resolved the conflicts, kindly check.\r\n\r\nRegards\r\nAmit", "@renjie-liu Can you please review this PR ?", "@renjie-liu , thanks for approving the PR, there was one small sanity failure due line 271 of file tensorflow/lite/testing/generate_examples_lib.py due to column exceeding 80 characters, this is fixed now, can you, please re approve the PR.\r\n\r\nRegards\r\nAmit ", "@renjie-liu , i have resolved the merge conflicts, can you please check and approve.\r\n\r\n\r\nRegards\r\nAmit", "@renjie-liu , i have resolved the merge conflicts, can you please check and re approve.\r\n\r\nRegards\r\nAmit", "Which model(s) are you using this operator? Our general policy for builtin op additions is:\r\n\r\n1) Prefer using select TF ops for one-off cases.\r\n2) Provide a custom op if needed for less common ops if some amount of optimization is required.\r\n3) Add op as builtin if there is significant demand from multiple clients and models.\r\n\r\nI'm not sure this case satisfies (3).", "@amitsrivastava78 Could you please resolve the conflicts? Thanks!", "Can you respond per [this comment](https://github.com/tensorflow/tensorflow/pull/27710#issuecomment-513290343)? ", "@jdduke , sorry for the late response, the model that i can see which uses this operator are t2t  transformer model (machine translation models) , there are other models which are based on machine translation (T2T) which also contains this operator.\r\n Hope this is ok.If you are ok then i will rebase the code and upstream again.\r\n\r\nRegards\r\nAmit", "Thanks, can you provide a link to the model(s)? I want to confirm that the operator is part of the inference subgraph, not just the training subgraph.", "@jdduke , please find the link for the model\r\n\r\nwget -qO- --show-progress https://dl.dropbox.com/s/efv2gmq5hu3np43/log.tar.gz | tar xz\r\n\r\n\r\nRegards\r\nAmit", "But it is this operator is used in gradient calculations\r\n\r\nRegards \r\nAmit ", "@amitsrivastava78 Could you please resolve the conflicts? Thanks!", "> But it is this operator is used in gradient calculations\r\n\r\nRight, which we shouldn't need for inference with TFLite. Let's close this until we find a compelling inference need. Thanks!"]}, {"number": 27709, "title": "Link to the definiton tf.image broken", "body": "### **On the page** \r\nhttps://www.tensorflow.org/api_docs/python/tf/image\r\n\r\n### **The link Under heading**\r\n `Module\r\n    tf.Image`\r\n\r\nhttps://www.tensorflow.org/code/stable/tensorflow/_api/v1/image/__init__.py\r\n**is broken.**\r\n\r\n\r\n", "comments": ["This one looks fixed: https://www.tensorflow.org/api_docs/python/tf/image"]}, {"number": 27708, "title": "[TF 2.0] Build issue for r2.0 branch", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version:\r\nBranch r2.0\r\n- Python version:\r\n3.5.6 (from Anaconda)\r\n- Installed using virtualenv? pip? conda?:\r\nminiconda\r\n- Bazel version (if compiling from source):\r\n0.24.0\r\n- GCC/Compiler version (if compiling from source):\r\n5.4.0\r\n- CUDA/cuDNN version:\r\nCPU version\r\n- Android SDK build tools version\r\n28.0.3\r\n\r\n**Describe the problem**\r\n\r\nWhile building TensorFlow from `r2.0` branch the build fails because of problems with Basel build files (it seems like this).\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n**Any other info / logs**\r\n```\r\nDEBUG: Rule 'build_bazel_rules_swift' indicated that a canonical reproducible form can be obtained by modifying arguments commit = \"001736d056d7eae20f1f4da41bc9e6f036857296\", shallow_since = \"1547844730 -0800\" and dropping [\"tag\"]\r\nDEBUG: /home/p.vytovtov/.cache/bazel/_bazel_p.vytovtov/bd2de6272d8995218e0b1ae22a8f3506/external/build_bazel_rules_apple/apple/repositories.bzl:35:5: \r\nWARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.\r\n\r\nERROR: /home/p.vytovtov/.cache/bazel/_bazel_p.vytovtov/bd2de6272d8995218e0b1ae22a8f3506/external/hwloc/BUILD.bazel:189:54: The `+` operator for dicts is deprecated and no longer supported. Please use the `update` method instead. You can temporarily enable the `+` operator by passing the flag --incompatible_disallow_dict_plus=false\r\nERROR: /home/p.vytovtov/.cache/bazel/_bazel_p.vytovtov/bd2de6272d8995218e0b1ae22a8f3506/external/hwloc/BUILD.bazel:200:9: Traceback (most recent call last):\r\n        File \"/home/p.vytovtov/.cache/bazel/_bazel_p.vytovtov/bd2de6272d8995218e0b1ae22a8f3506/external/hwloc/BUILD.bazel\", line 195\r\n                template_rule(name = \"include_private_hwloc_au...\", <3 more arguments>)\r\n        File \"/home/p.vytovtov/.cache/bazel/_bazel_p.vytovtov/bd2de6272d8995218e0b1ae22a8f3506/external/hwloc/BUILD.bazel\", line 199, in template_rule\r\n                if_cuda(_INCLUDE_PRIVATE_HWLOC_AUTOIGEN_..., ...)\r\n        File \"/home/p.vytovtov/.cache/bazel/_bazel_p.vytovtov/bd2de6272d8995218e0b1ae22a8f3506/external/hwloc/BUILD.bazel\", line 200, in if_cuda\r\n                _INCLUDE_PRIVATE_HWLOC_AUTOIGEN_CONFIG_H_CUDA_SUBS\r\nname '_INCLUDE_PRIVATE_HWLOC_AUTOIGEN_CONFIG_H_CUDA_SUBS' is not defined\r\nERROR: /home/p.vytovtov/Documents/tensorflow/tensorflow/core/BUILD:2319:1: Target '@hwloc//:hwloc' contains an error and its package is in error and referenced by '//tensorflow/core:lib_internal_impl'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Cannot compute config conditions\r\nINFO: Elapsed time: 0.395s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded, 2 targets configured)\r\n```", "comments": ["I  have the same issues with you. Did you solve it?", "I build r2.0 with bazel 0.22.0 on Ubuntu 18.04 from the master with the following command.\r\n\r\nbazel build --config=opt --config=cuda --config=v2 //tensorflow/tools/pip_package:build_pip_package\r\n\r\nBuilt it today, worked. Guess the 2.0 branch is not fully stable. Tensorflow shows the version to be 1.13.1 but it's behavior will be like 2.0", "It is possible that our branches do not become stable for all builds until the rc or final releases. Please try syncing with the tags on the branches if you need to build them."]}, {"number": 27707, "title": "convert to tflite in tensorflow 2.0 not fold batchnorm", "body": "pip install tf-nightly-2.0-preview==2.0.0.dev20190405, it don't fold BatchNorm therefore it generate error tflife not support BatchNormalization. It must auto fold BatchNorm?\r\ncode:\r\nmodel is tensorflow.keras model have BatchNormalization layer\r\n```\r\n@tf.function(input_signature=[tf.TensorSpec([None,300,300,3], tf.float32, name='fts_input_images'), tf.TensorSpec([], tf.float32, name='fts_input_threshold_confident')])\r\ndef func(x, confident_threshold):\r\n\tx = model(x)\r\n\tx = tf.identity(x, name='fts_output_no_merge')\r\n\tx = fn(x, confident_threshold)\r\n\tx = tf.identity(x, name='final_fts_output_full')\r\n\treturn x\r\nconcrete_func = func.get_concrete_function()\r\nconverter = tf.lite.TFLiteConverter.from_concrete_function(concrete_func)\r\ntflite_model = converter.convert()\r\n```\r\n\r\nlog error:\r\n```\r\n2019-04-12 16:48:44.545659: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-04-12 16:48:44.545815: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-04-12 16:48:44.617968: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:684] Optimization results for grappler item: graph_to_optimize\r\n2019-04-12 16:48:44.618014: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   function_optimizer: Graph size after: 1313 nodes (0), 1509 edges (0), time = 8.142ms.\r\n2019-04-12 16:48:44.618027: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   function_optimizer: Graph size after: 1313 nodes (0), 1509 edges (0), time = 3.993ms.\r\n2019-04-12 16:48:45.135108: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-04-12 16:48:45.135255: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-04-12 16:48:45.667690: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:684] Optimization results for grappler item: graph_to_optimize\r\n2019-04-12 16:48:45.667785: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   model_pruner: Graph size after: 1285 nodes (-28), 1493 edges (-16), time = 33.469ms.\r\n2019-04-12 16:48:45.667826: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   implementation_selector: Graph size after: 1285 nodes (0), 1493 edges (0), time = 4.814ms.\r\n2019-04-12 16:48:45.667843: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   function_optimizer: Graph size after: 1285 nodes (0), 1493 edges (0), time = 4.937ms.\r\n2019-04-12 16:48:45.667857: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   constant folding: Graph size after: 1216 nodes (-69), 1426 edges (-67), time = 194.179ms.\r\n2019-04-12 16:48:45.667880: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   shape_optimizer: Graph size after: 1216 nodes (0), 1426 edges (0), time = 7.638ms.\r\n2019-04-12 16:48:45.667897: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   arithmetic_optimizer: Graph size after: 727 nodes (-489), 1397 edges (-29), time = 37.714ms.\r\n2019-04-12 16:48:45.667912: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   loop_optimizer: Graph size after: 727 nodes (0), 1397 edges (0), time = 7.935ms.\r\n2019-04-12 16:48:45.667927: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   dependency_optimizer: Graph size after: 717 nodes (-10), 1318 edges (-79), time = 14.987ms.\r\n2019-04-12 16:48:45.668006: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   memory_optimizer: Graph size after: 717 nodes (0), 1318 edges (0), time = 80.326ms.\r\n2019-04-12 16:48:45.668021: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   model_pruner: Graph size after: 717 nodes (0), 1318 edges (0), time = 6.515ms.\r\n2019-04-12 16:48:45.668032: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   implementation_selector: Graph size after: 717 nodes (0), 1318 edges (0), time = 2.238ms.\r\n2019-04-12 16:48:45.668043: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   function_optimizer: Graph size after: 717 nodes (0), 1318 edges (0), time = 2.07ms.\r\n2019-04-12 16:48:45.668054: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   constant folding: Graph size after: 717 nodes (0), 1318 edges (0), time = 30.632ms.\r\n2019-04-12 16:48:45.668065: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   shape_optimizer: Graph size after: 717 nodes (0), 1318 edges (0), time = 5.239ms.\r\n2019-04-12 16:48:45.668075: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   arithmetic_optimizer: Graph size after: 717 nodes (0), 1318 edges (0), time = 32.872ms.\r\n2019-04-12 16:48:45.668086: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   dependency_optimizer: Graph size after: 717 nodes (0), 1318 edges (0), time = 20.462ms.\r\nTraceback (most recent call last):\r\n  File \"convert_h5_to_tflite.py\", line 192, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 276, in convert\r\n    **converter_kwargs)\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 410, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 176, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-04-12 16:13:57.979333: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 652 operators, 925 arrays (0 quantized)\r\n2019-04-12 16:13:57.999356: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 652 operators, 925 arrays (0 quantized)\r\n2019-04-12 16:13:58.034861: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 486 operators, 761 arrays (0 quantized)\r\n2019-04-12 16:13:58.058513: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 486 operators, 761 arrays (0 quantized)\r\n2019-04-12 16:13:58.074463: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 486 operators, 761 arrays (0 quantized)\r\n2019-04-12 16:13:58.101510: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 32761600 bytes, theoretical optimal value: 28800000 bytes.\r\n2019-04-12 16:13:58.107182: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, ARG_MAX, BATCH_TO_SPACE_ND, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FLOOR_DIV, FLOOR_MOD, GATHER, GREATER, GREATER_EQUAL, LOGICAL_AND, LOGICAL_NOT, LOGISTIC, MAXIMUM, MEAN, MINIMUM, MUL, PACK, RANGE, REDUCE_ANY, REDUCE_MAX, REDUCE_PROD, RESHAPE, RSQRT, SHAPE, SPACE_TO_BATCH_ND, SQUARED_DIFFERENCE, STRIDED_SLICE, SUB, SUM, TILE, TOPK_V2. Here is a list of operators for which you will need custom implementations: BatchNormalization.\r\nTraceback (most recent call last):\r\n  File \"/docker_environment/home/docker/anaconda3/bin/toco_from_protos\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/docker/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/docker/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, ARG_MAX, BATCH_TO_SPACE_ND, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FLOOR_DIV, FLOOR_MOD, GATHER, GREATER, GREATER_EQUAL, LOGICAL_AND, LOGICAL_NOT, LOGISTIC, MAXIMUM, MEAN, MINIMUM, MUL, PACK, RANGE, REDUCE_ANY, REDUCE_MAX, REDUCE_PROD, RESHAPE, RSQRT, SHAPE, SPACE_TO_BATCH_ND, SQUARED_DIFFERENCE, STRIDED_SLICE, SUB, SUM, TILE, TOPK_V2. Here is a list of operators for which you will need custom implementations: BatchNormalization.\r\n```", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "The issue should have been resolved within the last week. Please try running your code using the latest nightly. If that doesn't work, please update this issue with the model or a minimally reproducible example (the weights don't need to be correct).\r\n\r\nAdditionally, the converter functionality was changed in https://github.com/tensorflow/tensorflow/commit/312222d2a0657d4e0086ec0add4331063d0b2264. `from_concrete_function` is now `from_concrete_functions`. `from_keras_model` and `from_saved_model` have been added.", "This should work after https://github.com/tensorflow/tensorflow/commit/c767cbf14c6133e6cfc782bc373b0ef0f4b1e2c6.", "@anhtu812 - Is this still an issue ? Please let us know. Thanks!", "in version: pip install tf-nightly-2.0-preview==2.0.0.dev20190416 it out same error.\r\nin version: pip install tf-nightly-2.0-preview==2.0.0.dev20190422 have new errors\r\n```\r\n2019-04-23 09:23:23.090499: I tensorflow/core/grappler/devices.cc:61] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-04-23 09:23:23.090648: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-04-23 09:23:23.141247: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:709] Optimization results for grappler item: graph_to_optimize\r\n2019-04-23 09:23:23.141290: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:711]   function_optimizer: Graph size after: 1488 nodes (0), 1753 edges (0), time = 2.873ms.\r\n2019-04-23 09:23:23.141305: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:711]   function_optimizer: Graph size after: 1488 nodes (0), 1753 edges (0), time = 3.673ms.\r\nTraceback (most recent call last):\r\n  File \"convert_h5_to_tflite.py\", line 221, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 289, in convert\r\n    self._funcs[0])\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 133, in convert_variables_to_constants_v2\r\n    \"data\": tensor.numpy(),\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```\r\nwhen i run: out = func(tf.ones((2,300,300,3)), 0.5) before convert, it out error relate to Conv2D with dilation_rate!=1 (forward_6_1b_conv2D). It can relate to https://github.com/tensorflow/tensorflow/issues/27711\r\n```\r\nTraceback (most recent call last):\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 61, in quick_execute\r\n    num_outputs)\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: forward_6_1b_conv2D/stack:0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"convert_h5_to_tflite.py\", line 163, in <module>\r\n    out = func(tf.ones((2,img_size,img_size,3)), 0.5)\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 454, in __call__\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 566, in _filtered_call\r\n    (t for t in nest.flatten((args, kwargs), expand_composites=True)\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 646, in _call_flat\r\n    outputs = self._inference_function.call(ctx, args)\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 422, in call\r\n    ctx=ctx)\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 70, in quick_execute\r\n    raise core._SymbolicException\r\ntensorflow.python.eager.core._SymbolicException\r\n```", "@anhtu812 Can you provide the `model` in your code so that I can reproduce your error? Or provide a minimal example that reproduces your error?", "version \"pip install tf-nightly-2.0-preview==2.0.0.dev20190423\" fixed all. Thanks"]}, {"number": 27706, "title": "nvcc error   : 'cudafe++' died with status 0xC0000005 (ACCESS_VIOLATION)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro, 1809, 17763.379\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v1.13.1\r\n- Python version: v3.5.0:374f501f4567\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.20.0\r\n- GCC/Compiler version (if compiling from source): Microsoft (R) C/C++ Optimizing Compiler Version 19.00.24234.1 for x64\r\n- CUDA/cuDNN version: 10.0/7.5\r\n- GPU model and memory: NVIDIA GeForce RTX 2080 Ti\r\n\r\n\r\n\r\n**Describe the problem**\r\nBuild of kernels:transpose_functor_gpu  fails with the following error\r\n```\r\nnvcc error   : 'cudafe++' died with status 0xC0000005 (ACCESS_VIOLATION)\r\n```\r\nand this error:\r\n```\r\nERROR: D:/tensorflow/tensorflow/core/kernels/BUILD:1687:1: C++ compilation of rule '//tensorflow/core/kernels:transpose_functor_gpu' failed (Exit 5): python.exe failed: error executing command\r\n```\r\n\r\nThe problem seems very similar to [this](https://github.com/tensorflow/tensorflow/issues/27576) issue.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n(tensorflow-v1.13) d:\\tensorflow>python ./configure.py\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nnul\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: e482c527-4d04-440e-b726-4dcee4cabbba\r\nYou have bazel 0.20.0 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\admin\\tensorflow-v1.13\\Scripts\\python.exe]:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: module 'site' has no attribute 'getsitepackages'\r\nFound possible Python library paths:\r\n  C:\\Users\\admin\\tensorflow-v1.13\\Lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\admin\\tensorflow-v1.13\\Lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]: 10.0\r\n\r\n\r\nPlease specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7.5\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 7.5\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]: /arch:AVX2\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apacha Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\n(tensorflow-v1.13) d:\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nERROR: D:/tensorflow/tensorflow/core/kernels/BUILD:1687:1: C++ compilation of rule '//tensorflow/core/kernels:transpose_functor_gpu' failed (Exit 5): python.exe failed: error executing command\r\n\r\nnvcc error   : 'cudafe++' died with status 0xC0000005 (ACCESS_VIOLATION)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 542.611s, Critical Path: 293.00s\r\nINFO: 3067 processes: 3067 local.\r\nFAILED: Build did NOT complete successfully", "comments": ["You need to [Install Visual C++ Build Tools 2015](https://www.tensorflow.org/install/source_windows#install_visual_c_build_tools_2015) Please take a look at these [instructions](https://www.tensorflow.org/install/source_windows). ", "Visual C++ Build Tools 2015 are installed. I was following the [instructions](https://www.tensorflow.org/install/source_windows) and [this](https://medium.com/@amsokol.com/update-2-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-61c26553f7e8) article as a reference (without the Eigen patch). ", "I'm continually receiving this error as well building 2.0b1 on windows, same configure settings, but compute capability 6.1 and bazel 0.26 (per the error I get while installing according to the instructions on https://www.tensorflow.org/install/source_windows).", "Can you please try this workaround and see if it helps? Thanks!\r\nhttps://github.com/tensorflow/tensorflow/issues/27576#issuecomment-504703397", "This is a bug in cuda 10 distribution. you can try building with cuda 10.1, or simply replace the cudafe++ binary in your cuda 10 distribution with the cuda 10.1 version.\r\n\r\nClosing the issue as a duplicate."]}, {"number": 27705, "title": "Keras subclassing and explicit dtype of Input", "body": "**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n**OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Arch Linux\r\n**Tensorflow Version:** 2.0.0-alpha0\r\n**Description**\r\nWhen using Keras subclassing there is no apparent way of defining the dtype of the Input node of the network. In some cases, it would be neccecary to use tf.float16 instead of 32 but as of now i cannot find any way to adjust this. Also trying to set the dtype using self.dtype = tf.float16 is not permitted.", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "**Method 1:**\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nclass SomeClass(tf.keras.Model):\r\n\r\n    def __init__(self, dtype):\r\n        super(SomeClass, self).__init__(dtype=dtype)  # Set Input of dtype either like this\r\n\r\n        self.h_1 = tf.keras.layers.Dense(10, dtype=dtype)\r\n\r\n    def call(self, inputs):\r\n        return self.h_1(inputs)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    model = SomeClass(dtype=tf.float16)\r\n    model([1, 2, 0])\r\n```\r\nException:\r\n```\r\nTypeError: _init_subclassed_network() got an unexpected keyword argument 'dtype'\r\n```\r\n\r\n**Method 2**\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nclass SomeClass(tf.keras.Model):\r\n\r\n    def __init__(self, dtype):\r\n        self.dtype = dtype  # Or this?\r\n\r\n        self.h_1 = tf.keras.layers.Dense(10, dtype=dtype)\r\n\r\n    def call(self, inputs):\r\n        return self.h_1(inputs)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    model = SomeClass(dtype=tf.float16)\r\n    model([1, 2, 0])\r\n```\r\nException:\r\n```\r\nAttributeError: can't set attribute\r\n```\r\n\r\n**Use case**\r\nThe use case is when you want specifically to use custom dtypes for the whole net, typically when running on TensorCores etc.\r\n\r\n**Freetext**\r\nI dont know if this would be the correct way of defining this, but it should be possible (if its not already) to set the dtype of the input excplictly ( i assume there is a tf.cast in there somewhere anyways...)\r\n", "Hi! A workaround you can do now is to directly set the private property `_dtype`. I'll make a change that adds `dtype` as one of the allowed keyword arguments super().__init__.", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Reopening as I find some issues are there still with `tf-nightly-gpu-2.0-preview==2.0.0.dev20190723` . Here is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/4ee37c87967ef4abfa7c7cb3bed63c0c/tf_27705.ipynb). Thanks!", "For method 2:\r\ndtype is a property therefore we see error\r\nThe error message is fixed with latest tf-nightly 2.0 build version '2.0.0-dev20190809'\r\n```python\r\nAttributeError: Can't set the attribute \"dtype\", likely because it conflicts with an existing read-only @property of the object. Please choose a different name..\r\n```", "@jvishnuvardhan Use method 1. \r\n\r\n```\r\nValueError: Layer dense expects 1 inputs, but it received 3 input tensors. Inputs received: [<tf.Tensor: id=1, shape=(), dtype=int32, numpy=1>, <tf.Tensor: id=2, shape=(), dtype=int32, numpy=2>, <tf.Tensor: id=3, shape=(), dtype=int32, numpy=0>]\r\n```\r\n\r\nThe above error is appearing because [1, 2, 0] is being converted into separate tensors. Using `tf.constant` to convert the entire array into a tensor (try `model(tf.constant([[1, 2, 0]]))`).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27705\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27705\">No</a>\n"]}, {"number": 27704, "title": "XLA: build failed on 32bits platform", "body": "**System information**\r\n- OS Platform and Distribution: Linux\r\n- TensorFlow installed from (source or binary): build tensorflow on arm cortex-a15\r\n- TensorFlow version: 1.13.1\r\n- Bazel version (if compiling from source): binary, 0.19.2\r\n- GCC/Compiler version (if compiling from source):  binary, gcc 5.3\r\n\r\n**Describe the problem**\r\nBuild failed at tensorflow/compiler/xla/service/llvm_ir/llvm_util.cc:378\r\n```c++\r\n378                 b->CreateIntToPtr(b->getInt64(absl::bit_cast<int64>(&LogS64)),\r\n379                                   log_function_type->getPointerTo()),\r\n```\r\nLooks like XLA doesn't support 32bits platform?\r\n\r\n**Any other info / logs**\r\n**external/com_google_absl/absl/base/casts.h:179:3: error: static assertion failed: Source and destination types should have equal sizes.**\r\n```\r\ntensorflow/compiler/xla/service/llvm_ir/llvm_util.cc:378:76:   required from here\r\n...\r\nIn instantiation of 'Dest absl::bit_cast(const Source&) [with Dest = long long int; Source = const char*; typename std::enable_if<(! absl::internal_casts::is_bitcastable<Dest, Source>::value), int>::type <anonymous> = 0]':\r\n```\r\n- Note: related code snippet (tensorflow/compiler/xla/service/llvm_ir/llvm_util.cc:378)\r\n```c++\r\n374 void EmitLogging(const char* tag, llvm::Value* value, llvm::IRBuilder<>* b) {\r\n375   llvm::FunctionType* log_function_type = llvm::FunctionType::get(\r\n376       b->getVoidTy(), {b->getInt64Ty(), b->getInt64Ty()}, /*isVarArg=*/false);\r\n377   b->CreateCall(log_function_type,\r\n378                 b->CreateIntToPtr(b->getInt64(absl::bit_cast<int64>(&LogS64)),\r\n379                                   log_function_type->getPointerTo()),\r\n380                 {b->getInt64(absl::bit_cast<int64>(tag)), value});\r\n381 }\r\n```\r\n", "comments": ["@cee1, Request you to go through this [link](https://www.tensorflow.org/xla/overview#supported_platforms) for XLA support platforms. Thanks !", "> @cee1, Request you to go through this [link](https://www.tensorflow.org/xla/overview#supported_platforms) for XLA support platforms. Thanks !\r\n\r\nThanks for the reply.\r\n\r\nSome questions:\r\na) For AOT, tfcompile is needed, it can be built from tensorflow source code, right?\r\nb) AOT requires \"XLA option\" enabled during \"./configure\"?  If yes, the problem still exists (unable to compile due to assertion failure)", "@jlebar ", "> TensorFlow installed from (source or binary): build tensorflow on arm cortex-a15\r\n\r\nBuilding TF on ARM is not supported (meaning, \"good luck\").  However, targeting ARM *using tfcompile* from an x86 host is supported, and I agree this code is broken even in that case.  We don't specify whether we support 32-bit ARM, but it would strike me as pretty odd if we only could target 64-bit ARM.\r\n\r\nI am a bit confused, though, because EmitLogging does not appear to be called from anywhere?\r\n\r\nIn any case, would you be willing to send a patch?  We don't readily have capability to test this platform.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27702, "title": "fix typo", "body": "fix typo", "comments": []}, {"number": 27701, "title": "Bug for tf.data.experimental.TFRecordWriter(filename)", "body": "Thanks for your attention! I'm using Windows and my tensorflow version is 1.13.1 installed using pip, and the following issue occurred: when I was testing the API tf.data.experimental.TFRecordWriter with the folloing codes:\r\n```\r\nfilename = 'hello'\r\nwriter = tf.data.experimental.TFRecordWriter(filename)\r\n```\r\nmy interpreter raised the following error information:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\writers.py\", line 35, in __init__\r\n    filename, dtypes.string, name=\"filename\")\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1039, in convert_to_tensor\r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1097, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1175, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 304, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 245, in constant\r\n    allow_broadcast=True)\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 283, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 501, in make_tensor_proto\r\n    (dtype, nparray.dtype, values))\r\nTypeError: Incompatible types: <dtype: 'string'> vs. object. Value is hello\r\n```\r\nI can't quite understand the difference between the type of 'hello' and \"<dtype:'string'>\". Then I tried the API tf.python_io.TFRecordWriter(filename) with the following codes:\r\n```\r\ntf.python_io.TFRecordWriter(filename)\r\n```\r\nno error information raised. \r\nSo I'm pulling the issue to look for your help. Thanks for your time and attention again.", "comments": ["@eecshope i ran this `filename = 'hello'\r\nwriter = tf.data.experimental.TFRecordWriter(filename)`\r\nand my system didn't gave any error!!\r\n\r\nCan you tell me your system specs like what tensorflow version you're using and everything?", "My system is Windows10 with python( version 3.7) installed using anaconda3. My TensorFlow version is 1.13.1. I ran the test program using the console provided by PyCharm Profession", "I was not able to reproduce the error reported using [google colab](https://colab.sandbox.google.com).  Please try using google colab or terminal. I suspect it is an ide issue. You can also try [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.Thanks!\r\n"]}, {"number": 27700, "title": "optimize node placement policy in initialized_value stage.", "body": "Optimize node placement policy in initialized_value stage.\r\nBefore optimize, the graph is as follows:\r\n![before-optimize](https://user-images.githubusercontent.com/6914481/55848387-49e17f80-5b7f-11e9-985f-396f3e654b40.png)\r\nThe variable 'global_step' is placed to ps1, and 'global_step/initial_value' is placed to worker. The 'IsVariableInitialized' and 'cond/read/Switch' nodes will be colocated to 'global_step', the 'cond/Switch_1' will be colocated to 'global_step/initial_value'. The left three nodes are not specified to which device, so they will be placed to ps0 by Placer in default. The placement brings some Send/Recv nodes between ps0 and ps1. So I think we can optimize node placement policy in initialized_value stage by colocate these three nodes with the variable(here is global_step). :)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27700) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27700) for more info**.\n\n<!-- ok -->", "Sorry for the spammy comments, github was not letting me comment for a while."]}, {"number": 27699, "title": "Fix negative axis issue with ragged tensor and reduce_sum", "body": "This fix tries to address the issue raised in #27497 where `tf.reduce_sum` with multiple negative axes and ragged tensor does not produce correct result.\r\n\r\nThe issue is that during reduce op, ragged tensor will reduce one axis at a time. However, for negative axis, sort result is reversed so order is different.\r\n\r\nThis fix convert to positive before the sort to make sure the order.\r\n\r\nThis fix fixes #27497.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang can you please check build failures "]}, {"number": 27698, "title": "[Intel MKL] Enabling BFloat16 versions of Convolutions, Concat and Id\u2026", "body": "\u2026entity - Round 3\r\n\r\nThis PR enables BFloat16 versions of 2D and 3D Convolution and their gradient\r\noperators, Concat, and Identity operations.", "comments": []}, {"number": 27697, "title": "tensorflow/core/framework/op_def.pb.h:10:40: fatal error: google/protobuf/port_def.inc: No such file or directory", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): use plugin\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): tensorflow/tensorflow:latest-gpu-py3\r\n- TensorFlow version (use command below): 1.14.1-dev20190409\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nAs of `1.14.1-dev20190409`, custom plugins fail to build with the following error:\r\n\r\n```\r\n  In file included from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_def_builder.h:24:0,\r\n                   from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\r\n                   from horovod/tensorflow/mpi_ops.cc:22:\r\n  /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_def.pb.h:10:40: fatal error: google/protobuf/port_def.inc: No such file or directory\r\n   #include <google/protobuf/port_def.inc>\r\n                                          ^\r\n  compilation terminated.\r\n```\r\n\r\nThis seems to be related to https://github.com/tensorflow/tensorflow/commit/6168f476b52d6d40eeff1823943ed2c0ea28adde.  It appears that not all the files are placed in the proper locations after the installation.\r\n\r\n**Describe the expected behavior**\r\nPlugins should build successfully.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\n$ docker run -it --rm tensorflow/tensorflow:latest-gpu-py3\r\n# apt install -y mpich\r\n# HOROVOD_WITH_TENSORFLOW=1 pip install -v horovod\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\ncc @martinwicke @gunan ", "comments": ["This is also breaking https://github.com/tensorflow/addons on tf2-nightly.\r\n\r\nLogs:\r\nhttps://source.cloud.google.com/results/invocations/1fcf8227-5161-4cb8-a017-a1a906c9429d/targets/tensorflow_addons%2Fubuntu%2Fgpu%2Fpy3%2Fcontinuous/log", "Assigning to @yifeif who reviewed cl/242492263, and also cc @gunan. ", "Un-assigning so this'll get into our GitHub triaging system (pinging @rthadur for this).", "Also: @meteorcloudy ", "@seanpmorgan @gunan Is tf addons somehow using this BUILD file?\r\nhttps://github.com/tensorflow/tensorflow/blob/3fb94abb517fc054bc299aeac841c87be326b88a/third_party/systemlibs/protobuf.BUILD#L15-L25\r\n@acozzette Dose it make sense to you to add `google/protobuf/port_def.inc` here?", "We don't use the BUILD file itself. We link to the pre-compiled `libtensorflow_framework`. \r\n\r\nIt looks like the op_def.pb header file is referencinng a file which is not where it is expected after the new protobuf install.\r\n```\r\ntensorflow/core/framework/op_def.pb.h:10:40: fatal error: google/protobuf/port_def.inc: No such file or directory\r\n #include <google/protobuf/port_def.inc>\r\n```", "Same here, we're not using Bazel.", "Looks like the problem is when creating the pip package, we missed to copy header files that has `.inc` extension.\r\nhttps://github.com/tensorflow/tensorflow/blob/1cdae4922cfe6821eafb163d9f825e29b4f1fc80/tensorflow/tools/pip_package/build_pip_package.sh#L128-L130\r\n\r\nI'll send a fix for this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27697\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27697\">No</a>\n", "Still seeing this issue in the 20190412 nightly. Was this merged in for that nightly? If so I'm not sure the correct fix is in place.\r\n\r\n\r\n`https://files.pythonhosted.org/packages/d4/c9/0fb753c0d182cfa10294edaa610d19f236da0396c636c24dccaa669bc81d/tf_nightly_2.0_preview-2.0.0.dev20190412-cp34-cp34m-manylinux1_x86_64.whl (86.5MB)`\r\n\r\n`bazel-out/k8-opt/genfiles/external/local_config_tf/include/tensorflow/core/framework/types.pb.h:10:40: fatal error: google/protobuf/port_def.inc: No such file or directory`\r\n\r\nhttps://source.cloud.google.com/results/invocations/fb7847e9-3cd6-4b07-9394-efe9f948f42e/targets/tensorflow_addons%2Fubuntu%2Fgpu%2Fpy3%2Fcontinuous/log", "+1, it's still broken in the current tf-nightly.", "Turned out we also need to fix `steup.py`:\r\nhttps://github.com/tensorflow/tensorflow/blob/1cdae4922cfe6821eafb163d9f825e29b4f1fc80/tensorflow/tools/pip_package/setup.py#L240", "I verified on my local machine, fixing this file works. Sending a fix from internal.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27697\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27697\">No</a>\n", "@meteorcloudy today I tried building our package on OS X and ran into this error again\r\n\r\n```\r\nIn file included from bazel-out/darwin-fastbuild/genfiles/external/local_config_tf/include/tensorflow/core/framework/op.h:23:\r\nIn file included from bazel-out/darwin-fastbuild/genfiles/external/local_config_tf/include/tensorflow/core/framework/op_def_builder.h:24:\r\nbazel-out/darwin-fastbuild/genfiles/external/local_config_tf/include/tensorflow/core/framework/op_def.pb.h:10:10: fatal error: 'google/protobuf/port_def.inc' file not found\r\n#include <google/protobuf/port_def.inc>\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```\r\n```\r\n$ pip list | grep tf-nightly\r\ntf-nightly-2.0-preview           2.0.0.dev20190419 \r\n```\r\n\r\nThis isn't immedietely obvious to me why this would be happening unless the regex was failing, or osx protobuf is different? I tried purging bazel cache etc. to make sure it wasn't a my environment issue.", "Just saw https://github.com/tensorflow/custom-op/issues/13 which looks to be another example of this happening on OSX nightly.", "pb.h looks suspicious to me. I think we only try to use proto.h headers for protos.\r\n@yifeif @martinwicke ", "@seanpmorgan I don't know why mac should be any different, but I don't have a mac machine to debug this. Can you try to build the pip package on mac and check if the head files are correctly included in the whl file? (You can just unzip it and check the include folder).", "@gunan Where can I download the nightly binaries for all platforms?", "@meteorcloudy binaries:\r\nhttps://pypi.org/project/tf-nightly/#files\r\n\r\nSo the linux whl has `port_def.inc` in both:\r\n```\r\ntf_nightly-1.14.1.dev20190423.data/purelib/tensorflow/include/external/protobuf_archive/src/google/protobuf\r\ntf_nightly-1.14.1.dev20190423.data/purelib/tensorflow/include/google/protobuf\r\n```\r\n\r\nWhile mac has `port_def.inc` in only:\r\n```\r\ntf_nightly-1.14.1.dev20190423.data/purelib/tensorflow/include/external/protobuf_archive/src/google/protobuf\r\n```", "@seanpmorgan \r\nLooks like `regex` in `find` does behave differently on macos:\r\nhttps://stackoverflow.com/questions/5905493/why-isnt-this-regex-working-find-regex-m-h\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/d65760b805cdd729743ba3d7c7b4ae8897304dd6/tensorflow/tools/pip_package/build_pip_package.sh#L128\r\n\r\nI will change it to \r\n```\r\nfind protobuf_archive -type f,l \\( -name \"*.h\" -o -name \"*.inc\" \\)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27697\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27697\">No</a>\n", "@meteorcloudy, I'm afraid the latest change has broken Linux and did not fix Mac:\r\n\r\n```\r\n$ unzip -l tf_nightly-1.14.1.dev20190425-cp27-cp27m-macosx_10_9_x86_64.whl | grep \\\\.inc | grep port_def\r\n    11642  04-26-2019 00:53   tf_nightly-1.14.1.dev20190425.data/purelib/tensorflow/include/external/protobuf_archive/src/google/protobuf/port_def.inc\r\n$ unzip -l tf_nightly-1.14.1.dev20190425-cp27-cp27mu-manylinux1_x86_64.whl | grep \\\\.inc | grep port_def\r\n    11642  04-26-2019 00:37   tf_nightly-1.14.1.dev20190425.data/purelib/tensorflow/include/external/protobuf_archive/src/google/protobuf/port_def.inc\r\n$ unzip -l tf_nightly-1.14.1.dev20190424-cp27-cp27mu-manylinux1_x86_64.whl | grep \\\\.inc | grep port_def\r\n    11642  04-24-2019 08:45   tf_nightly-1.14.1.dev20190424.data/purelib/tensorflow/include/google/protobuf/port_def.inc\r\n    11642  04-24-2019 08:45   tf_nightly-1.14.1.dev20190424.data/purelib/tensorflow/include/external/protobuf_archive/src/google/protobuf/port_def.inc\r\n```", "@alsrgv This is strange.. I tested locally, let me take a look now", "@meteorcloudy, I do see this warning in your `devel-gpu` container during pip package build, perhaps it will help diagnose the issue:\r\n```\r\nfind: Arguments to -type should contain only one letter\r\n```", "Ah, I see. The `find` command is different on all kinds of platform. I'll try to figure out a fix.", "Can anyone verify `find protobuf_archive -name \"*.h\" -o -name \"*.inc\"` works for their platform?", "It worked on my Mac, Ubuntu 18.04 and your `devel` container, so seems promising :-)", "@alsrgv Thanks! Hopefully this time it will work", "@meteorcloudy, I can confirm that both Mac and Linux nightly builds are operational.  Could you pick this fix to the 1.4 branch?", "@gunan @yifeif Can you help pick this fix to 1.4 branch?", "@bananabowl can you cherry-pick this fix?", "Sure - this is part of https://github.com/tensorflow/tensorflow/pull/28296", "Hi,\r\nI experience this problem with 1.14.0 . From what I see in the sources, as well as bananabowl's comment / reference to the cherry picks, the needed changes are in 1.14.0. Still getting the problem.\r\nHere is the error:\r\n```\r\nIn file included from ./deps/tensorflow-headers-1.14.0/tensorflow/core/framework/tensor_shape.h:22:0,\r\n                 from ./deps/tensorflow-headers-1.14.0/tensorflow/core/framework/tensor.h:23,\r\n                 from ./deps/tensorflow-headers-1.14.0/tensorflow/cc/framework/ops.h:21,\r\n                 from ./deps/tensorflow-headers-1.14.0/tensorflow/cc/client/client_session.h:24,\r\n                 from ./include/detect_tensorflow.hpp:5,\r\n                 from src/detect_tensorflow.cpp:1:\r\n./deps/tensorflow-headers-1.14.0/tensorflow/core/framework/types.pb.h:10:10: fatal error: google/protobuf/port_def.inc: No such file or directory\r\n #include <google/protobuf/port_def.inc>\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```\r\nI am using the headers generated by //tensorflow:install_headers .\r\nPlatform : AARCH64\r\nOS : Ubuntu 18.04 LTS\r\nHardware: Nvidia Xavier AGX\r\nSoftware: TensorFlow 1.14.0 from a source release. Compiled with Bazel 0.24.1 (from a dist package).\r\n", "Sorry to hear that Andrey :(\n\n@mihaimaruseac - I noticed you made some changes since the original fix,\nwas this\n<https://github.com/tensorflow/tensorflow/commit/fd5d844765a1d41f18c4f3f712f28a7c0220dc33#diff-aa53f753fc372beaeb504d505c278203>\nor\na subsequent change meant to fix this issue?\n\nOn Mon, Jun 24, 2019 at 7:00 AM Andrey Hristov <notifications@github.com>\nwrote:\n\n> Hi,\n> I experience this problem with 1.14.0 . From what I see in the sources, as\n> well as bananabowl's comment / reference to the cherry picks, the needed\n> changes are in 1.14.0. Still getting the problem.\n> Here is the error:\n>\n> In file included from ./deps/tensorflow-headers-1.14.0/tensorflow/core/framework/tensor_shape.h:22:0,\n>                  from ./deps/tensorflow-headers-1.14.0/tensorflow/core/framework/tensor.h:23,\n>                  from ./deps/tensorflow-headers-1.14.0/tensorflow/cc/framework/ops.h:21,\n>                  from ./deps/tensorflow-headers-1.14.0/tensorflow/cc/client/client_session.h:24,\n>                  from ./include/detect_tensorflow.hpp:5,\n>                  from src/detect_tensorflow.cpp:1:\n> ./deps/tensorflow-headers-1.14.0/tensorflow/core/framework/types.pb.h:10:10: fatal error: google/protobuf/port_def.inc: No such file or directory\n>  #include <google/protobuf/port_def.inc>\n>           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n>\n> I am using the headers generated by //tensorflow:install_headers .\n> Platform : AARCH64\n> OS : Ubuntu 18.04 LTS\n> Hardware: Nvidia Xavier AGX\n> Software: TensorFlow 1.14.0 from a source release. Compiled with Bazel\n> 0.24.1 (from a dist package).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27697?email_source=notifications&email_token=AKEVL2DVBJ3KOFTZLYRTIZDP4DHRTA5CNFSM4HEW4MV2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYM7ZXQ#issuecomment-505019614>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKEVL2HIORNS6JG5AEYT4CTP4DHRTANCNFSM4HEW4MVQ>\n> .\n>\n", "Hi, the fd5d844765a1d41f18c4f3f712f28a7c0220dc33 change is for virtual pip, should be on master only, not on a release branch.\r\n\r\nThere's also some changes related with moving protobuf definitions around, but again, that should only be on master.", "So, the commits that I know can affect this are fd5d844765a1d41f18c4f3f712f28a7c0220dc33, 338c2f269b1dfc13fe8f94c6cf9c64b4ae82b928 and 508f76b1d9925304cedd56d51480ec380636cb82\r\n\r\nCould try patching each one of these manually and see which works", "I tried the master branch. Interestingly it seems to be 1.13.1 (because the generated PIP file is tensorflow-1.13.1-cp36-cp36m-linux_aarch64.whl). The problem persist but it could be that master is not the right branch (is not actively used). Is it?", "@andreyhristov The master branch is the develop branch. \r\nI tried to build `//tensorflow:include_headers` on the latest master branch, it does contain the `port_def.inc`\r\n\r\n```\r\n$ bazel build --config=opt //tensorflow:install_headers\r\n...\r\nTarget //tensorflow:install_headers up-to-date:\r\n  bazel-genfiles/tensorflow/include\r\nINFO: Elapsed time: 0.239s, Critical Path: 0.00s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\n$ ls ./bazel-genfiles/tensorflow/include/external/com_google_protobuf/src/google/protobuf/port_def.inc\r\n./bazel-genfiles/tensorflow/include/external/com_google_protobuf/src/google/protobuf/port_def.inc\r\n```", "And in 1.14 branch, the \".inc\" file headers also exist but under `external/protobuf_archive` directory.\r\n```\r\nls ./bazel-genfiles/tensorflow/include/external/protobuf_archive/src/google/protobuf/port_def.inc\r\n./bazel-genfiles/tensorflow/include/external/protobuf_archive/src/google/protobuf/port_def.inc\r\n```", "ok guys, right, I found the file. The problem was that I needed to add the path to protobuf_archive to the include path of the compiler. I compiles now and it works! (I had asserts with 1.12.0 on batch inference which are gone).\r\n\r\nLast question, why does master seem to be with version 1.13.1 ? Do you guys change the version after branching for release?", "Yes, it's embedded here:\r\nhttps://github.com/tensorflow/tensorflow/blob/c19265ba0ee2622e391bd319a74c14ad2eb79017/tensorflow/tools/pip_package/setup.py#L50\r\n\r\n@gunan Is this intentionally?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27697\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27697\">No</a>\n", "Fixed the version number in 4a017ecb42f536a63de9387c67dbec704dfd9188", "Hi,\r\nwhat I did for my program was to pass a -I param to the compiler, so it does know where to look for the files.\r\nI use a Makefile, and also export a TENSORFLOW_PATH variable, which contains the path to the headers. The headers can be extracted with a bazel target. For me the flag to gcc is :\r\n-I$(TENSORFLOW_PATH)/external/protobuf_archive/src\r\n", "Hello, I'm still seeing this issue with tf1.14.0 while compiling a custom op kernel. \r\n`site-packages/tensorflow/include/tensorflow/core/framework/op_def.pb.h:10:10: fatal error: google/protobuf/port_def.inc: No such file or directory\r\n #include <google/protobuf/port_def.inc>\r\ncompilation terminated.`\r\n\r\nI'm on windows, installed TF using pip. \r\nThe .inc files still reside in \"site-packages\\tensorflow\\include\\google\\protobuf_archive\\src\\google\\protobuf\".\r\n\r\nIs there a way to fix this issue without building the sources manually (by using bazel)?\r\n\r\nAny help is appreciated. Thanks!", "> Hello, I'm still seeing this issue with tf1.14.0 while compiling a custom op kernel.\r\n> `site-packages/tensorflow/include/tensorflow/core/framework/op_def.pb.h:10:10: fatal error: google/protobuf/port_def.inc: No such file or directory #include <google/protobuf/port_def.inc> compilation terminated.`\r\n> \r\n> I'm on windows, installed TF using pip.\r\n> The .inc files still reside in \"site-packages\\tensorflow\\include\\google\\protobuf_archive\\src\\google\\protobuf\".\r\n> \r\n> Is there a way to fix this issue without building the sources manually (by using bazel)?\r\n> \r\n> Any help is appreciated. Thanks!\r\n\r\ntry include the dir solve this issue? like: `-I..\\site-packages\\tensorflow\\include\\google\\protobuf_archive\\src`"]}, {"number": 27696, "title": "tf.keras.estimator.model_to_estimator crashing when using tf.distribute.MirroredStrategy() with Only TensorFlow native optimizers are supported with DistributionStrategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI am using a keras model with TF 2.0. I am converting the model to estimator:\r\n`tf.keras.estimator.model_to_estimator `\r\n\r\nthen it is crashing when running `estimator_train_model.train(..)` when using `tf.distribute.MirroredStrategy()` (after migrating the code to TF 2.0 of course). \r\n\r\nIt works fine when using `None` as strategy\r\n\r\nI tried to follow the instruction: https://www.tensorflow.org/alpha/guide/distribute_strategy\r\n\r\n**Describe the expected behavior**\r\nThe same was working with TF 1.x\r\n\r\n**Code to reproduce the issue**\r\nWork in progress notebook can be found here:\r\nhttp://localhost:8888/notebooks/proj_DL_models_and_pipelines_with_GCP/notebook/TF_2.0/08-Mnist_keras_estimator.ipynb\r\n\r\nI am using a very basic Keras model with \r\n\r\n```\r\noptimiser = tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, epsilon=1e-07)\r\n# Compile model\r\nmodel.compile(loss='categorical_crossentropy',\r\n                  optimizer=optimiser,\r\n                  metrics=['accuracy'])\r\n\r\nstrategy=None # working\r\n#strategy = tf.distribute.MirroredStrategy() # crashing with TF 2.0 but working with TF 1.X\r\n\r\n# config tf.estimator to use a give strategy\r\ntraining_config = tf.estimator.RunConfig(train_distribute=strategy,\r\n                                         model_dir=FLAGS.model_dir,\r\n                                         save_summary_steps=1,\r\n                                         save_checkpoints_steps=100,\r\n                                         keep_checkpoint_max=3,\r\n                                         log_step_count_steps=10)\r\n\r\n# transfor keras model to estimator model\r\nestimator_train_model = tf.keras.estimator.model_to_estimator(keras_model=model_opt_tf,\r\n                                         config=training_config)\r\n\r\n# Fit the model (using estimator.train and data.Dataset)\r\nestimator_train_model.train(input_fn=lambda:mnist_v1.input_mnist_tfrecord_dataset_fn(path_train_tfrecords+'*', FLAGS, mode=tf.estimator.ModeKeys.TRAIN, batch_size=FLAGS.batch_size),\r\n                            steps=1000)\r\n```\r\n**Other info / logs**\r\nI0409 22:06:27.293305 4531660224 model.py:211] input_dataset_fn: TRAIN, train\r\nI0409 22:06:27.469371 123145492971520 estimator.py:1126] Calling model_fn.\r\nI0409 22:06:27.475003 123145492971520 coordinator.py:219] Error reported to Coordinator: Only TensorFlow native optimizers are supported with DistributionStrategy.\r\nTraceback (most recent call last):\r\n  File \"/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 882, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1127, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py\", line 278, in model_fn\r\n    raise ValueError('Only TensorFlow native optimizers are supported with '\r\nValueError: Only TensorFlow native optimizers are supported with DistributionStrategy.\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<timed eval> in <module>\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    357 \r\n    358       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 359       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    360       logging.info('Loss for final step: %s.', loss)\r\n    361       return self\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1135   def _train_model(self, input_fn, hooks, saving_listeners):\r\n   1136     if self._train_distribution:\r\n-> 1137       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1138     else:\r\n   1139       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)\r\n   1198       self._config._train_distribute.configure(self._config.session_config)\r\n   1199       return self._actual_train_model_distributed(\r\n-> 1200           self._config._train_distribute, input_fn, hooks, saving_listeners)\r\n   1201     # pylint: enable=protected-access\r\n   1202 \r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _actual_train_model_distributed(self, strategy, input_fn, hooks, saving_listeners)\r\n   1267                     labels,  # although this will be None it seems\r\n   1268                     ModeKeys.TRAIN,\r\n-> 1269                     self.config))\r\n   1270           loss = strategy.reduce(reduce_util.ReduceOp.SUM,\r\n   1271                                  grouped_estimator_spec.loss)\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)\r\n   1076     if kwargs is None:\r\n   1077       kwargs = {}\r\n-> 1078     return self._call_for_each_replica(fn, args, kwargs)\r\n   1079 \r\n   1080   def _call_for_each_replica(self, fn, args, kwargs):\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py in _call_for_each_replica(self, fn, args, kwargs)\r\n    663   def _call_for_each_replica(self, fn, args, kwargs):\r\n    664     return _call_for_each_replica(self._container_strategy(), self._device_map,\r\n--> 665                                   fn, args, kwargs)\r\n    666 \r\n    667   def _configure(self,\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py in _call_for_each_replica(distribution, device_map, fn, args, kwargs)\r\n    191     for t in threads:\r\n    192       t.should_run.set()\r\n--> 193     coord.join(threads)\r\n    194 \r\n    195   return values.regroup(device_map, tuple(t.main_result for t in threads))\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py in join(self, threads, stop_grace_period_secs, ignore_live_threads)\r\n    387       self._registered_threads = set()\r\n    388       if self._exc_info_to_raise:\r\n--> 389         six.reraise(*self._exc_info_to_raise)\r\n    390       elif stragglers:\r\n    391         if ignore_live_threads:\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/six.py in reraise(tp, value, tb)\r\n    691             if value.__traceback__ is not tb:\r\n    692                 raise value.with_traceback(tb)\r\n--> 693             raise value\r\n    694         finally:\r\n    695             value = None\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py in stop_on_exception(self)\r\n    295     \"\"\"\r\n    296     try:\r\n--> 297       yield\r\n    298     except:  # pylint: disable=bare-except\r\n    299       self.request_stop(ex=sys.exc_info())\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py in run(self)\r\n    880               self._captured_var_scope, reuse=self.replica_id > 0), \\\r\n    881           variable_scope.variable_creator_scope(self.variable_creator_fn):\r\n--> 882         self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n    883         self.done = True\r\n    884     finally:\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n   1125 \r\n   1126     logging.info('Calling model_fn.')\r\n-> 1127     model_fn_results = self._model_fn(features=features, **kwargs)\r\n   1128     logging.info('Done calling model_fn.')\r\n   1129 \r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py in model_fn(features, labels, mode)\r\n    276         not isinstance(keras_model.optimizer,\r\n    277                        (tf_optimizer_module.Optimizer, optimizers.TFOptimizer)):\r\n--> 278       raise ValueError('Only TensorFlow native optimizers are supported with '\r\n    279                        'DistributionStrategy.')\r\n    280 \r\n\r\nValueError: Only TensorFlow native optimizers are supported with DistributionStrategy.\r\n\r\n\r\n", "comments": ["Here a code based on examples from TF 2.0 official doc to reproduce the issue:\r\n\r\npip install tensorflow==2.0.0-alpha0 \r\npip install tensorflow-datasets\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nfrom absl import logging\r\n\r\nlogging.set_verbosity(logging.INFO)\r\n# Define the estimator's input_fn\r\nSTEPS_PER_EPOCH = 5\r\n#BUFFER_SIZE = 10 # Use a much larger value for real code. \r\nBATCH_SIZE = 64\r\nNUM_EPOCHS = 5\r\n\r\n\r\ndef input_fn():\r\n    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n    mnist_train, mnist_test = datasets['train'], datasets['test']\r\n\r\n    BUFFER_SIZE = 10000\r\n    BATCH_SIZE = 64\r\n\r\n    def scale(image, label):\r\n        image = tf.cast(image, tf.float32)\r\n        image /= 255\r\n    \r\n        return image, label[..., tf.newaxis]\r\n\r\n    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n    return train_data.repeat()\r\n\r\n# Define train & eval specs\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=input_fn,\r\n                                    max_steps=STEPS_PER_EPOCH * NUM_EPOCHS)\r\neval_spec = tf.estimator.EvalSpec(input_fn=input_fn,\r\n                                  steps=STEPS_PER_EPOCH)\r\n\r\ndef make_model():\r\n    return tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32, 3, activation='relu',\r\n                               kernel_regularizer=tf.keras.regularizers.l2(0.02),\r\n                               input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dropout(0.1),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.BatchNormalization(),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n\r\nmodel = make_model()\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\n#####\r\n## works\r\n#strategy=None \r\n## crash\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\n# config tf.estimator to use a give strategy\r\ntraining_config = tf.estimator.RunConfig(train_distribute=strategy)\r\n#####\r\n\r\nestimator = tf.keras.estimator.model_to_estimator(\r\n    keras_model = model,\r\n    config=training_config\r\n)\r\n\r\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```", "I have the same problem - on colab this is my code\r\nhttps://colab.research.google.com/drive/1mf-PK0a20CkObnT0hCl9VPEje1szhHat#scrollTo=MMbPOC3f5ku3", "Correct link - https://colab.research.google.com/drive/1mf-PK0a20CkObnT0hCl9VPEje1szhHat", "Have you tried the [suggestion from the docs](https://www.tensorflow.org/alpha/guide/distribute_strategy#using_tfdistributestrategy_with_keras) of placing the code inside the strategy scope:\r\n\r\n```python\r\nmirrored_strategy = tf.distribute.MirroredStrategy()\r\nwith mirrored_strategy.scope():\r\n  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\r\n  model.compile(loss='mse', optimizer='sgd')\r\n ```\r\n\r\nI came here because I'm seeing this same error even when I do the scoping above. ", "@mckinziebrandon , thanks for the idea. From the same doc (see the section estimator):\r\n\"The usage of tf.distribute.Strategy with Estimator is slightly different than the Keras case. Instead of using strategy.scope, now we pass the strategy object into the RunConfig for the Estimator.\"\r\nThis is what I did.", "This appears to be a real issue.  We flagged it internally and someone will hopefully get to it soon.  Thanks for reporting.\r\n\r\nIf you are curious about fixing it, then my guess is that `keras.optimizer_v2.OptimizerV2` needs to be allowed in the set of classes that are allowed right where that ValueError is thrown.  It should work. ", "This should be fixed in master now. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27696\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27696\">No</a>\n", "I still have the problem with pip install tf-nightly-gpu on https://colab.research.google.com/drive/1mf-PK0a20CkObnT0hCl9VPEje1szhHat\r\n\r\nValueError: Only TensorFlow native optimizers are supported with DistributionStrategy.", "@guptapriya same observation as @AntGul I am getting the same error:\r\n`ValueError: Only TensorFlow native optimizers are supported with DistributionStrategy.`\r\n\r\nI am using tf-nightly-2.0-preview-2.0.0.dev20190509", "The fix is in estimator: https://github.com/tensorflow/estimator/commit/818c6163ba759dc257b7c27421bc46ef8259e217#diff-93ed941ec864a6ef78bd656293c99cea\r\n\r\nSometimes the version of estimator pip package might be stale. Would you mind updating the estimator package directly and testing then? \r\n\r\nhttps://pypi.org/project/tf-estimator-nightly/", "@guptapriya you are right. When I did my test I had:\r\n```\r\ntensorflow-estimator-2-0-preview 1.14.0.dev2019042600\r\ntf-nightly-2-0-preview    2.0.0.dev20190509\r\n```\r\n\r\nI am mot using:\r\n```\r\ntfds-nightly==1.0.2.dev201905160105\r\ntf-nightly-2.0-preview==2.0.0.dev20190516\r\ntf-estimator-nightly==1.14.0.dev2019051601\r\n```\r\n\r\nand it works.\r\n\r\nWhat is very weird is now I have 2 differents version of estimators (I recreated everything from scratch)\r\n\r\n```\r\ntensorflow-estimator-2-0-preview 1.14.0.dev2019051600          pypi_0    pypi\r\ntf-estimator-nightly      1.14.0.dev2019051601          pypi_0    pypi\r\n```\r\n\r\nIs it expected ? Should I report it ?\r\n\r\n", "It's likely that you did pip install on tf-estimator-nightly, while tensorflow-2.0 did an extra tensorflow-estimator-2.0-preview under the hood. Can you pip uninstall everything, and then just pip install tf-nightly-2.0-preview only? ", "@tanzhenyu  you are right. Now I got\r\n```\r\ntensorflow-estimator-2-0-preview 1.14.0.dev2019051600          pypi_0    pypi\r\ntensorflow-metadata       0.13.0                   pypi_0    pypi\r\ntf-nightly-2-0-preview    2.0.0.dev20190516          pypi_0    pypi\r\n```\r\n\r\nAll good. I misinterpret what was said earlier.", "I am facing the same error when using tensorflow run time version 1.13 on AI platform.", "For anyone facing this issue, you can simply change the optimizer to be from tf.train instead. This will be a compatible optimizer with tf.keras code (worked for me on 1.13).\r\n\r\nI found this post helpful on the subject:\r\n\r\nhttps://medium.com/tensorflow/multi-gpu-training-with-estimators-tf-keras-and-tf-data-ba584c3134db"]}, {"number": 27695, "title": "Failed to load the native TensorFlow runtime, Ubuntu 16.04, in virtualenv ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary, using  pip3 install --upgrade --ignore-installed tensorflow-gpu==1.5\r\n\r\n- TensorFlow version:1.5\r\n- Python version: tried in both Python 2.7 and 3.5\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- CUDA/cuDNN version: CUDA 10.0. cuDNN: cuDNN v7.5.0 (Feb 21, 2019), for CUDA 10.0\r\n- GPU model and memory: NVIDIA TITAN Xp\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI**nstalled Tensorflow and tried to run it via python, and using \"import tensorflow as tf\". The error message details are shared below.**\r\n\r\nPython 3.5.2 (default, Nov 12 2018, 13:43:14) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/rrku16sys3/venv/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/rrku16sys3/venv/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/rrku16sys3/venv/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/rrku16sys3/venv/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> \r\n\r\n**Need help.**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["seems like version 1.5 you are trying to install is looking for cublas 9.0. Did you install cublas 9.0 ? Install from here : https://developer.nvidia.com/cublas", "Please switch to cuda 9.0 since TF 1.5 pre built binaries support cuda 9.0 and cudnn 7. Also take a look at [release notes](https://github.com/tensorflow/tensorflow/blob/r1.5/RELEASE.md#breaking-changes) to know more. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27695\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27695\">No</a>\n"]}, {"number": 27694, "title": "pip_package: Remove duplicated libtensorflow_framework.so", "body": "The current pip wheel has libtensorflow_framework.so duplicated three\r\ntimes with the different versioned libs. The pip wheel is reaching size\r\nlimits so needs to be as small as possible. To run, the only requried\r\nlib is libtensorflow_framework.so.1 (since it matches the soname).\r\nLinking when libtensorflow_framework.so exists is done with\r\ngcc -ltensorflow_framework. Since we only have the .so.1, we need to\r\nchange the linker flags to gcc -l:libtensorflow_framework.so.1.\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>", "comments": ["Are the test failures known and unrelated?", "@martinwicke IIRC, the absl failures are known internally, and shouldn't be related to this. Keras... seems unrelated, but I don't know for certain those tests are tracked internally. Thanks for reviewing this so quickly!\r\n\r\nI don't think we have presubmit tests that verify `.so.1` exists (it's been built since https://github.com/tensorflow/tensorflow/pull/27493 was merged, and I've seen it built, but it's not explicitly checked in our CI).", "@martinwicke the keras failure is unrelated im pretty sure. if this was broken literally nothing would work. I could add a test sure, but where? basically if `import tensorflow` works then this is correct since _pywrap links to libtensorflow_framework.so.1. if there are already (internal?) tests that try and install and run hello world on the pip wheel then it should be okay.", "I suppose we already have the pip build which should test this too, so I\nguess all is good.\n"]}, {"number": 27693, "title": "import error", "body": "Traceback (most recent call last):\r\n  File \"C:/Users/GAURAV/PycharmProjects/Object_Detection/object\", line 9, in <module>\r\n    from utils import label_map_util\r\nImportError: cannot import name 'label_map_util'", "comments": ["@gauravbisht098 hey! try this `from object_detection.utils import label_map_util` and see if it works!", "I did but then it said no object detection module found\n\nOn Wed, 10 Apr, 2019, 9:59 AM Aman Patel, <notifications@github.com> wrote:\n\n> @gauravbisht098 <https://github.com/gauravbisht098> hey! try this from\n> object_detection.utils import label_map_util and see if it works!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27693#issuecomment-481528138>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYOKGq2vAmh51hBuU80DSSDnxKO_-BT4ks5vfWiwgaJpZM4clLtn>\n> .\n>\n", "@gauravbisht098 pip install utils again and see if it works! also update me if it worked", "I already did\n\nOn Wed, 10 Apr, 2019, 10:30 AM Aman Patel, <notifications@github.com> wrote:\n\n> @gauravbisht098 <https://github.com/gauravbisht098> pip install utils\n> again and see if it works! also update me if it worked\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27693#issuecomment-481533623>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYOKGuroJe2OyIDYnPKQBrpd5cMCC_1Pks5vfW_2gaJpZM4clLtn>\n> .\n>\n", "This issue is more suitable on [tensorflow/models](https://github.com/tensorflow/models/issues/new) repo. Please take a look at duplicate issue https://github.com/tensorflow/models/issues/1990 ", "Can you send me a best object detection code..???\n\nOn Thu, 11 Apr, 2019, 9:00 AM ymodak, <notifications@github.com> wrote:\n\n> This issue is more suitable on tensorflow/models\n> <https://github.com/tensorflow/models/issues/new> repo. Please take a\n> look at duplicate issue tensorflow/models#1990\n> <https://github.com/tensorflow/models/issues/1990>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27693#issuecomment-481950610>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYOKGgGNyntkoiaHEdDwg9Bf2LFM9yjfks5vfqxygaJpZM4clLtn>\n> .\n>\n"]}, {"number": 27692, "title": "Converting Tensor to Numpy array extremely slow in TF 2.0", "body": "**Current Issue / Bug Report**\r\nI tried to convert a list of tensors to a numpy array.\r\nThis was no issue in Tensorflow 1.13.x but is now orders of magnitudes slower in Tensorflow 2.0.0-dev20190405\r\n\r\nI linked a piece of code to reproduce the issue in Colab (no HW acceleration required) and to also show the difference in execution time between 1.13.x and 2.0.0/nightly\r\n\r\nI tested the issue with \r\nnp.array(LIST_OF_TENSORS)\r\nnp.shape(LIST_OF_TENSORS)\r\n\r\n**System information**\r\nColab w.o. HW acceleration and TF 2.0.0-dev20190405\r\n\r\n**Code to reproduce the issue**\r\n[Code Example](https://github.com/markste-in/colab/blob/master/bug_tensor_numpy.ipynb\r\n)\r\n.\r\n", "comments": ["Wow, this is really bad. Thanks for bringing it up!", "The underlying issue, after running your repro through a profiler, seems to be that Tensor's array conversion is done through slicing instead of the fastpath.", "Interestingly, if you do np.array(tensor) there's no slowdown, it's only if you do np.array(list_of_tensors).", " @superbobry do you have any idea why converting a tensor to an ndarray goes through __array__ and is fast while converting a list of tensors goes through __iter__ and is slow?", "Not sure, will have a look at [`PyArray_FromAny`](https://github.com/numpy/numpy/blob/15b092f5541e80d7c3d0108957406c6f8686aba0/numpy/core/src/multiarray/ctors.c#L1793).\r\n\r\nAlso, here's a smaller reproducer:\r\n\r\n```python\r\nt = tf.random.uniform([256])\r\n%timeit np.array(t)\r\n%timeit np.array([t])\r\n100000 loops, best of 3: 5.21 \u00b5s per loop\r\n10 loops, best of 3: 22.1 ms per loop\r\n```\r\n\r\n**Update 1**: I think this is happening because an `EagerTensor` is a `Sequence` (`__getitem__`, `__len__`) so `[t]` is no different from the nested list case. Recursive copying logic is implemented in [`setArrayFromSequence`](https://github.com/numpy/numpy/blob/master/numpy/core/src/multiarray/ctors.c#L435). There is a [branch](https://github.com/numpy/numpy/blob/master/numpy/core/src/multiarray/ctors.c#L453) special-casing ndarrays but any other object (including `EagerTensor`) is copied via the `Sequence` protocol.\r\n\r\nThe remaining question is why does the same code path work for 1.X.\r\n\r\n**Update 2**: Actually, the reason it's fast in 1.X is because `Tensor` is not a `Sequence` so `setArrayFromSequence` does not recurse.\r\n\r\n**Update 3**: The remaining bit is where does `__iter__` come from in the profiler output. The answer is: [`PySequence_Fast`](https://github.com/python/cpython/blob/a24107b04c1277e3c1105f98aff5bfa3a98b33a0/Objects/abstract.c#L1940) which (in the above example) converts a single-element tensor to a single-element list. ", "I'll see if it's easy to generalize `setArrayFromSequence` to arbitrarily nested sequence of array-like objects. Sidenote: could we also be triggering numpy/numpy#8562?\r\n\r\nIn the meantime, I wonder if there is a way to detect `np.array([t, t, t...])` calls and error or emit or warning?", "There seem to be two distinct issues: \r\n\r\n1. `PyArray_FromAny` copies its argument to a list when it does not implement `__array_interface__`, that is exactly numpy/numpy#8562 and it does affect `Tensor` which only implements `__array__`.\r\n2. `setArrayFromSequence` ignores `__array_*__` and `__array__` and copies its argument (again!) unless it's an array (or an array subtype).\r\n\r\nI have sketched a fix for both issues. Will post an update once I do more testing.", "You can wrap each item in the list with `np.array()` first then wrap the whole list once again with `np.array()`. I know it does not looks good, but at least it works for now.", "The TF side of the fix has been merged. Closing the issue as the remaining part is on the NumPy side. \r\n\r\nNote that `np.array([t])` will continue to be slow until a new NumPy version is released with numpy/numpy#13399 incorporated.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27692\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27692\">No</a>\n"]}, {"number": 27691, "title": "Unable to do transpose on a  yolo model ", "body": "Hello,\r\n\r\nI am trying to add a transpose layer at the end of Yolo model.\r\n\r\nI am using the following script, \r\n\r\nmodel = load_model(filepath=\"yolov2-tiny-voc.h5\")\r\noutput = model.output\r\noutput_shape = model.output.shape\r\ntranspose = Lambda(lambda x: tf.transpose(output, prem=(0,3,1,2),name='transpose'))(output)\r\nfinal_model = Model(inputs= model.input, outputs= transpose )\r\nsave_model(model=final_model, filepath= \"trial_attempt.h5\")\r\n\r\n\r\nBut I am getting the following error:\r\nAttributeError: 'Conv2D' object has no attribute 'outbound_nodes'\r\n\r\n\r\nCould someone help on this issue?\r\n\r\n\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@gadagashwini ,\r\n\r\nThe platform is macOS High Sierra.\r\nThe tensorflow version is 1.13.1.\r\nI have installed using pip.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: tensorflow 1.13.1\r\n- **Python version**:  python 3.6.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: \r\n- **Exact command to reproduce**:\r\n\r\nmodel = load_model(filepath=\"yolov2-tiny-voc.h5\")\r\noutput = model.output\r\noutput_shape = model.output.shape\r\ntranspose = Lambda(lambda x: tf.transpose(output, prem=(0,3,1,2),name='transpose'))(output)\r\nfinal_model = Model(inputs= model.input, outputs= transpose )\r\nsave_model(model=final_model, filepath= \"trial_attempt.h5\")\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nmodel = load_model(filepath=\"yolov2-tiny-voc.h5\")\r\noutput = model.output\r\noutput_shape = model.output.shape\r\ntranspose = Lambda(lambda x: tf.transpose(output, prem=(0,3,1,2),name='transpose'))(output)\r\nfinal_model = Model(inputs= model.input, outputs= transpose )\r\nsave_model(model=final_model, filepath= \"trial_attempt.h5\")\r\n\r\nLet me know if you need any further information.", "@Rathna21 Thanks for providing the details.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}, {"number": 27690, "title": "Problem with Keras sparse input", "body": "In keras.backend.GraphExecutionFunction.__call__ function, when this function receives sparse input, it will transform sparse input into coo_matrix, and extract (indices, data, shape) from it. But this function will try to invoke numpy.asarray function on value tuple: (indices, data, shape), and will cause error here.\r\n\r\n  def __call__(self, inputs):\r\n    inputs = nest.flatten(inputs)\r\n\r\n    session = get_session()\r\n    feed_arrays = []\r\n    array_vals = []\r\n    feed_symbols = []\r\n    symbol_vals = []\r\n    for tensor, value in zip(self.inputs, inputs):\r\n      if value is None:\r\n        continue\r\n      if is_sparse(tensor):\r\n        sparse_coo = value.tocoo()\r\n        indices = np.concatenate((np.expand_dims(sparse_coo.row, 1),\r\n                                  np.expand_dims(sparse_coo.col, 1)), 1)\r\n        value = (indices, sparse_coo.data, sparse_coo.shape)\r\n      if tensor_util.is_tensor(value):\r\n        # Case: feeding symbolic tensor.\r\n        feed_symbols.append(tensor)\r\n        symbol_vals.append(value)\r\n      else:\r\n        # Case: feeding Numpy array.\r\n        feed_arrays.append(tensor)\r\n        # We need to do array conversion and type casting at this level, since\r\n        # `callable_fn` only supports exact matches.\r\n        tensor_type = dtypes_module.as_dtype(tensor.dtype)\r\n        \r\n        # ------------------------------------\r\n        #  SHOULD BE THAT?\r\n        # if is_sparse(tensor):\r\n        #   array_vals.append(value)\r\n        # else:\r\n        # ------------------------------------\r\n\r\n        array_vals.append(np.asarray(value,\r\n                                     dtype=tensor_type.as_numpy_dtype))", "comments": ["@ZhangZhiPku Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27690\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27690\">No</a>\n"]}, {"number": 27689, "title": "ImportError: DLL load failed: Unable to find specific module.", "body": "Hi all,\r\n\r\nIm unable to import keras or tensorflow in jupyter:\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: HP Omen, i7 7700HQ, 16GB RAM\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu\r\n- **TensorFlow version (use command below)**: 1.13.1\r\n- **Python version**: 3.7.3\r\n- **CUDA/cuDNN version**: 10.1\r\n- **GPU model and memory**: GTX 1060 laptop\r\n- **Exact command to reproduce**: import tensorflow as tf\r\n\r\n---TRACE---\r\n\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed: No se puede encontrar el m\u00f3dulo especificado.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-6-9ab9087e201e> in <module>\r\n----> 1 import tensorflow\r\n      2 print(tensorflow.__version__)\r\n      3 # Puedes a\u00f1adir todos los imports adicionales que necesites aqu\u00ed\r\n      4 import keras\r\n      5 from keras.datasets import fashion_mnist\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 from tensorflow._api.v1 import app\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\dizquierdo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\dizquierdo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\dizquierdo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\dizquierdo\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\dizquierdo\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: No se puede encontrar el m\u00f3dulo especificado.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nRegards", "comments": ["@daniziz can you be more specific to which tensorflow-gpu version you're using??", "@amanp592 My Bad, I was to lazy to check it because I was running out of time when I opened the post.\r\n\r\nHere it goes:\r\n\r\ntensorflow-gpu            1.13.1                   pypi_0    pypi\r\n\r\nRegards!", "Acording to NVIDIA doc, I've compiled debugQuery.exe in order to check if cuda is correctly installed. \r\n\r\nHere goes te output:\r\n\r\n(base) C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.1\\bin\\win64\\Debug>deviceQuery.exe\r\ndeviceQuery.exe Starting...\r\n\r\n CUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\nDetected 1 CUDA Capable device(s)\r\n\r\nDevice 0: \"GeForce GTX 1060 with Max-Q Design\"\r\n  CUDA Driver Version / Runtime Version          10.1 / 10.1\r\n  CUDA Capability Major/Minor version number:    6.1\r\n  Total amount of global memory:                 6144 MBytes (6442450944 bytes)\r\n  (10) Multiprocessors, (128) CUDA Cores/MP:     1280 CUDA Cores\r\n  GPU Max Clock rate:                            1342 MHz (1.34 GHz)\r\n  Memory Clock rate:                             4004 Mhz\r\n  Memory Bus Width:                              192-bit\r\n  L2 Cache Size:                                 1572864 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  2048\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Texture alignment:                             512 bytes\r\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Device supports Compute Preemption:            Yes\r\n  Supports Cooperative Kernel Launch:            No\r\n  Supports MultiDevice Co-op Kernel Launch:      No\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.1, NumDevs = 1\r\nResult = PASS", "@daniziz, Hi, Did you download and install the Microsoft Visual C++ 2015 Redistributable Update 3? Can you please confirm?", "> @daniziz, Hi, Did you download and install the Microsoft Visual C++ 2015 Redistributable Update 3? Can you please confirm?\r\n\r\nHi @gadagashwini ,\r\n\r\nNo, I don't. This is what I have installed:\r\n\r\n![image](https://user-images.githubusercontent.com/39950437/55944878-da849200-5c49-11e9-8717-f36bb0683478.png)\r\n\r\nshould I do it?\r\n\r\nregards!!", "Hi @gadagashwini ,\r\n\r\nSame error with Microsoft Visual C++ 2015 Redistributable Update 3 installed... \r\n\r\n:(\r\n", "Thanks for confirming. ", "duplicate #26364", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27689\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27689\">No</a>\n", "My local development environment is ok, but publish to other environment has this problem, please help me to have a look\r\n\r\nUnable to load DLL 'tensorflow' or one of its"]}, {"number": 27688, "title": "RuntimeError: Unable to create link (name already exists) during model saving with ModelCheckpoint", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): TF 2.0 downloaded from repo\r\n- TensorFlow version (use command below): \r\ntf-nightly-gpu-2.0-preview --> 2.0.0.dev20190314 \r\ntensorflow-hub  --> 0.4.0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: CUDA Version 10.0.130/ cuDNN 7.5.0\r\n- GPU model and memory: Nvidia RTX 2080 Ti 11GB (and GTX 1060 6GB) \r\n\r\n**Describe the current behavior**\r\nI've downloaded an inception model from TF-Hub (specifically this one: https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2), I have added to it two Keras layers (a Dropout layer and a Dense layer) and during the training, I'm trying to save the model using the `ModelCheckpoint` Keras callback. Unfortunately, after one epoch and during the model saving I receive the following error:\r\n``` python\r\nTraceback (most recent call last):\r\n  File \"/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-3945e7fb8367>\", line 1, in <module>\r\n    runfile('/run/media/federico/XData/PycharmProjectsXData/ash/ash/prova_gan_plain_test.py', wdir='/run/media/federico/XData/PycharmProjectsXData/ash/ash')\r\n  File \"/opt/pycharm-professional/helpers/pydev/_pydev_bundle/pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"/opt/pycharm-professional/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/run/media/federico/XData/PycharmProjectsXData/ash/ash/prova_gan_plain_test.py\", line 98, in <module>\r\n    main()\r\n  File \"/run/media/federico/XData/PycharmProjectsXData/ash/ash/prova_gan_plain_test.py\", line 90, in main\r\n    logdir,\r\n  File \"/run/media/federico/XData/PycharmProjectsXData/ash/ash/testers/gan_plain.py\", line 85, in __init__\r\n    self._model = self._download_and_train_model()\r\n  File \"/run/media/federico/XData/PycharmProjectsXData/ash/ash/testers/gan_plain.py\", line 255, in _download_and_train_model\r\n    callbacks=[cback_checkpoint],\r\n  File \"/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1508, in fit_generator\r\n    steps_name='steps_per_epoch')\r\n  File \"/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 324, in model_iteration\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 290, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 892, in on_epoch_end\r\n    self.model.save_weights(filepath, overwrite=True)\r\n  File \"/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1395, in save_weights\r\n    hdf5_format.save_weights_to_hdf5_group(f, self.layers)\r\n  File \"/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 693, in save_weights_to_hdf5_group\r\n    param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)\r\n  File \"/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/h5py/_hl/group.py\", line 139, in create_dataset\r\n    self[name] = dset\r\n  File \"/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/h5py/_hl/group.py\", line 371, in __setitem__\r\n    h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)\r\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/h5o.pyx\", line 202, in h5py.h5o.link\r\nRuntimeError: Unable to create link (name already exists)\r\n```\r\n\r\nHighlighting the error: **RuntimeError: Unable to create link (name already exists)**\r\n\r\n**Describe the expected behavior**\r\nI'm expecting to be able to use the `ModelCheckpoint` callback to save the (best) model.\r\nAlternatively, I'm also expecting to be able to save the model with the `model.save(filepath)` function.\r\n\r\n**Code to reproduce the issue**\r\nYou could reproduce the error on the following TF-Hub Colab page: https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_image_retraining.ipynb#scrollTo=CCpdfXPsh47Q \r\n\r\nadding a cell with the `ModelCheckpoint` code\r\n\r\n``` python\r\ncback_checkpoint = tf.keras.callbacks.ModelCheckpoint(\r\n            filepath=\"best.h5\",\r\n            verbose=1,\r\n            save_best_only=True,\r\n        )  \r\n```\r\n\r\nand then adding the callback to the `fit_generator` function of the model:\r\n\r\n``` python\r\nsteps_per_epoch = train_generator.samples // train_generator.batch_size\r\nvalidation_steps = valid_generator.samples // valid_generator.batch_size\r\nhist = model.fit_generator(\r\n    train_generator,\r\n    epochs=5, steps_per_epoch=steps_per_epoch,\r\n    validation_data=valid_generator,\r\n    validation_steps=validation_steps,\r\n    callbacks=[cback_checkpoint]).history\r\n```\r\n\r\n**Other info / logs**\r\nI've found some issues online regarding a similar problem like [this](https://groups.google.com/forum/#!topic/keras-users/my13TUe2dlU) and issues: [#5280](https://github.com/keras-team/keras/issues/5820), [#6844](https://github.com/keras-team/keras/issues/6844) and the more recent #26811. Many of them speak about some problem regarding the naming of the layers (or weights) or about creating the `ModelCheckpoint` with `save_best_only=True` or `save_weights_only=True`. I have tried all the proposed approaches but without success.\r\nEven with `model.save(filepath)` Keras function I face the same problem.\r\n\r\n**EDIT**: Please follow [this Colab link](https://colab.research.google.com/drive/1_sDTb1DLaoBlNGbRfKqMWgWV6B2DaYUv) to have all the code set up to be reproduced.\r\n", "comments": ["@iLeW When I try to run the google colab that you have provided, unfortunately I did not get any error. I recommend you to run it once and check. Thanks!", "@gadagashwini I have just tried again, and it gives me the same error:\r\n```python\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/h5o.pyx in h5py.h5o.link()\r\n\r\nRuntimeError: Unable to create link (name already exists) \r\n```\r\nHave you added, before running it, the line of code as indicated in my post? For your convenience, I filled out the Colab for you. You can find it at [this link](https://colab.research.google.com/drive/1_sDTb1DLaoBlNGbRfKqMWgWV6B2DaYUv) with the necessary changes (i.e.: the callback added to the model). Please try to run this configuration. It should give you the same `RuntimeError` error.\r\n\r\n\r\n", "I am having the same problem at #27909 with 1.13.1 and provided the reproducible codes and some analysis.", "The same problem happened to me, Tensorflow2.0, Cuda:10.0, Python3.6.8.\r\n\r\nUpdate: previously I used Pycharm2019.1 to write and run the code, but now when I work in the Jupyter Notebook with the same system configuration, the problem seems to be resolved. it's weird.", "I also encountered the same problem.\r\nTensorflow:2.0.0a0\r\nCuda:10.0\r\nPython:3.6.8\r\nIDE:Jupyter  NoteBook", "Thanks for the report. Confirming I can repro. Feel free to add more information while we're looking into it.", "The crash of Model.save() when using hub.KerasLayer likely needs to be fixed in tensorflow-hub, not tensorflow. I've filed issue tensorflow/hub#287 for that.\r\n\r\nCan this issue here be closed then?\r\n", "> The crash of Model.save() when using hub.KerasLayer likely needs to be fixed in tensorflow-hub, not tensorflow. I've filed issue [tensorflow/hub#287](https://github.com/tensorflow/hub/issues/287) for that.\r\n> \r\n> Can this issue here be closed then?\r\n\r\nSounds good - thanks arnoegw@! Let's follow up in tensorflow/hub#287 for this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27688\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27688\">No</a>\n", "FYI - issue https://github.com/tensorflow/hub/issues/287 has been solved (upgrade Hub module version to /3).", "> FYI - issue [tensorflow/hub#287](https://github.com/tensorflow/hub/issues/287) has been solved (upgrade Hub module version to /3).\r\n\r\nThanks for the updates!", "I also encountered the same problem. Note it **only occurs with eager execution enabled.**\r\nTensorflow:1.13.1\r\nPython:3.6.8 Anaconda Inc.\r\nIDE:Jupyter NoteBook\r\n\r\nI was able to solve the .save error with the following snippet. Is this a good idea?\r\n\r\n```\r\nfrom tensorflow.python.keras import backend as K\r\n   \r\nwith K.name_scope(model.optimizer.__class__.__name__):\r\n    for i, var in enumerate(model.optimizer.weights):\r\n        name = 'variable{}'.format(i)\r\n        model.optimizer.weights[i] = tf.Variable(var, name=name)\r\n```", "For the error with eager execution, make sure you're using OptimizerV2 from here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras/optimizer_v2. The optimizers in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizers.py don't inherit from Trackable so the variables aren't named correctly.", "@ppham27 I can save keras model in `tensorflow-gpu-1.13` fine until I trigger `tf.enable_eager_execution()` that is when I cannot save the model. When I call `model.save(path)` the `Unable to create link` error immediately occurs if eager execution is enabled. If I set `include_optimizer` to False then it will let me save. But I think this is a stupid workaround.\r\n\r\nI see that it's related to the optimizer somehow like you mentioned in your comment. So I tried importing the Adam optimizer from `tensorflow.python.keras.optimizer_v2.adam.Adam` and it allows me to save the model. But another error occurs when I try to load the model: `TypeError: Unexpected keyword argument passed to optimizer: name`\r\n\r\nSo what I do now is the stupid workaround:\r\n```py\r\n# save\r\nmodel.save('mnist.h5', include_optimizer=False)\r\n\r\n#load \r\nmod = tf.keras.models.load_model('mnist.h5') \r\nmod.compile('adam', 'categorical_crossentropy', metrics=['acc']) # compile again because there is no optimizer\r\n```\r\nI don't know if just compiling the model is enough though.", "Hi @off99555,\r\n\r\nSorry, can't seem to figure out why that would be occurring without more code.\r\n\r\nWe have at least one test in the test suite that saves the model with an optimizer: https://github.com/tensorflow/tensorflow/blob/3cb03d093610e51cf2d36bfbf43c446a5de52941/tensorflow/python/keras/saving/hdf5_format_test.py#L810-L811\r\n\r\nI changed the line `SGD` to `Adam` and the test still passes. Maybe try upgrading to a later version of Tensorflow?\r\n\r\n", "It seems to be a version issue. I tested with tensorflow2.0 and there is no problem (because there is no tf.enable_eager_execution()). I cannot upgrade the version easily considering that I followed this installation guide that only has version 1.13 as the latest from conda: https://www.pugetsystems.com/labs/hpc/The-Best-Way-to-Install-TensorFlow-with-GPU-Support-on-Windows-10-Without-Installing-CUDA-1187/\r\n\r\nHere is the exact code I used to reproduce the error: (Tensorflow-gpu-1.13 installed using conda)\r\n```py\r\nimport tensorflow as tf   # only work from tensorflow==1.9.0-rc1 and after\r\ntf.enable_eager_execution()\r\nfrom tensorflow.python.keras.optimizer_v2.adam import Adam\r\nimport numpy as np\r\n\r\ndef keras_model():\r\n    from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, Input\r\n\r\n    inputs = Input(shape=(28, 28, 1))\r\n    x = Conv2D(32, (3, 3),activation='relu', padding='valid')(inputs)\r\n    x = MaxPool2D(pool_size=(2, 2))(x)\r\n    x = Conv2D(64, (3, 3), activation='relu')(x)\r\n    x = MaxPool2D(pool_size=(2, 2))(x)\r\n    x = Flatten()(x)\r\n    x = Dense(512, activation='relu')(x)\r\n    x = Dropout(0.5)(x)\r\n    outputs = Dense(10, activation='softmax')(x)\r\n\r\n    return tf.keras.Model(inputs, outputs)\r\n\r\nmodel = keras_model()\r\nmodel.compile(Adam(), 'categorical_crossentropy', metrics=['acc'])\r\n# model.fit(np.random.random((1000, 28, 28, 1)), np.random.random((1000, 10)))\r\nmodel.save('mnist.h5') # will raise RuntimeError: Unable to create link (if optimizer is 'adam' and model.fit() is run)\r\nmodel2 = tf.keras.models.load_model('mnist.h5') # it will raise TypeError: Unexpected keyword argument passed to optimizer: name (if optimizer is Adam() from optimizer_v2)\r\n```\r\n\r\nIf I replace `Adam()` with `'adam'` there would be no TypeError mentioned above. But the problem arises when I try to fit the keras model. If I call `model.fit(dataset)`, I won't be able to save the model anymore. The `RuntimeError: Unable to create link` will appear.\r\n\r\nTo show the  `RuntimeError`, you just need to uncomment model.fit() and change Adam() to 'adam'.\r\nThe goal is to be able to save and load the model after model.fit() with eager execution. So please help me figuring it out.", "Hmm, there appears to be a mismatch in the optimizer being loaded. Really, my advice would be to upgrade to at least TF1.14, but it might work passing in `custom_objects={'adam': Adam}` as a keyword argument into `tf.keras.models.load_model`, where `Adam` is the v2 version.\r\n\r\nOr putting this at the top of your program:\r\n```\r\nimport os\r\n\r\nos.environ['TF2_BEHAVIOR'] = '1'\r\n```", "That trick about `custom_objects` seem to work!\r\nI changed the loading code to `model2 = tf.keras.models.load_model('mnist.h5', custom_objects=dict(adam=Adam))` and it is able to load using Adam v2 optimizer.\r\nI hope that there is no problem with this approach so I would use it for now.", "I was using a TF.Hub model based on this tutorial https://www.tensorflow.org/tutorials/images/hub_with_keras  and change the TF hub model from version 2 to 4.\r\nand now works for me:\r\nhttps://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4", "WORKING SOLUTION:\r\n\r\n1. Go to: Miniconda3\\Lib\\site-packages or wherever h5py is installed.\r\n2. Delete h5py Folder\r\n3. Delete h5py-2.10.0.dist-info Folder (This might be different if you have a different version).\r\n4. Install h5py again.\r\n\r\nIt worked for me after a long search. Finally, I tried to delete it manually and reinstall it. IT WORKED!!!!!\r\n\r\nBy uninstalling them using cmd, pip, anaconda Some files might be stuck and will cause errors even after installing a different version.", "I got the same error in Tensorflow 2.1 when calling `save_weigths(\"model.h5\", overwrite=True)`.\r\nNone of the above mentioned fixes worked for me (I am not using hub and optimizer is passed as `SGD(...)`).\r\nAre there any other solutions to this?\r\n\r\nUpdate:\r\nLikely related to #26811 as I am using customer layer in which I call `tf.Variable` instead of `self.add_weight`.", "The reason why it didn't work was because I had the same namestrings for all the weights in my custom keras layers. \r\nLesson: If you make keras layers, give them different name strings!\r\n\r\n\r\n    class LayerNormA(tf.keras.Model):\r\n        def __init__(self, features, name, initial_values=None, eps=1e-6):\r\n            super(LayerNormA, self).__init__()\r\n            if initial_values is None:\r\n                gamma_init = tf.ones_initializer()\r\n                gamme_initial_value = gamma_init(shape=[features])\r\n                beta_init = tf.zeros_initializer()\r\n                beta_initial_value = beta_init(shape=[features])\r\n            else:\r\n                gamme_initial_value, beta_initial_value = initial_values\r\n            # make different name\r\n            self.gamma = tf.Variable(initial_value=gamme_initial_value, trainable=True, name=name + \"gamma\")\r\n            self.beta = tf.Variable(initial_value=beta_initial_value, trainable=True, name=name + \"beta\")\r\n            self.eps = eps\r\n\r\n        def call(self, x):\r\n            mean = tf.math.reduce_mean(x, axis=-1, keepdims=True)\r\n            # std = tf.math.reduce_std(x, axis=-1, keepdims=True)\r\n            N = x.shape[-1] - 1\r\n            std = tf.math.sqrt(tf.math.reduce_sum(tf.math.pow((x - mean), 2), axis=-1, keepdims=True)/N)\r\n            return self.gamma * (x - mean) / (std + self.eps) + self.beta\r\n\r\n\r\n", "I get the same problem when I enable eager execution, without it everything works fine ", "> I also encountered the same problem. Note it **only occurs with eager execution enabled.**\r\n> Tensorflow:1.13.1\r\n> Python:3.6.8 Anaconda Inc.\r\n> IDE:Jupyter NoteBook\r\n> \r\n> I was able to solve the .save error with the following snippet. Is this a good idea?\r\n> \r\n> ```\r\n> from tensorflow.python.keras import backend as K\r\n>    \r\n> with K.name_scope(model.optimizer.__class__.__name__):\r\n>     for i, var in enumerate(model.optimizer.weights):\r\n>         name = 'variable{}'.format(i)\r\n>         model.optimizer.weights[i] = tf.Variable(var, name=name)\r\n> ```\r\n\r\nI've been trying to resolve this issue since past few hours. Thank you for saving me!", "You can also face this problem in case of no-unique 'group' part of name _in weights_ in case of using multi-input models with transfer learning (for example - several MobileNets at input).\r\n\r\nWrote this code, it helped (summary, with changing other meta, such as model name and layer names):\r\n\r\n```\r\nimport uuid\r\ndef unique_name():\r\n    return uuid.uuid4().hex.upper()[0:10]\r\n\r\ndef postprocess_weight_name(name):\r\n    group, name = name.split('/')\r\n    return f'{group}{unique_name()}/{name}'\r\n    print((group, name))\r\n\r\nmodel._name = model._name + unique_name()\r\n        \r\nfor layer in model.layers:\r\n    layer._name = layer._name + unique_name()\r\n        \r\nfor i in range(len(model.weights)):\r\n    model.weights[i]._handle_name = postprocess_weight_name(model.weights[i].name)\r\n```"]}, {"number": 27687, "title": "quantized mobilenet_v1_1.0_224 is 4x slower than un-qualtized version (both testing on tflite)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):1.13.0-rc0\r\n- Python version:Python 3.5.5 :: Anaconda, Inc\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10/7.4.1\r\n- GPU model and memory:Titan V, 12 GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nquantized mobilenet_v1_1.0_224 is 4x slower than un-qualtized version (both testing on tflite)\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n```\r\n# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport numpy as np\r\n\r\nfrom PIL import Image\r\nimport tensorflow as tf\r\nimport time\r\nimport os\r\n\r\ndef Covert_to_tflite():    \r\n    graph_def_file = os.path.join(os.getcwd(), 'mobilenet_v1_1.0_224/mobilenet_v1_1.0_224_frozen.pb')\r\n    input_arrays = [\"input\"]\r\n    output_arrays = [\"MobilenetV1/Predictions/Softmax\"]\r\n\r\n    converter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n    graph_def_file, input_arrays, output_arrays)\r\n    tflite_model = converter.convert()\r\n    open(\"mobilenet_v1_1.0_224/converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n    converter.post_training_quantize=True\r\n    tflite_quantized_model=converter.convert()\r\n    open(\"mobilenet_v1_1.0_224/quantized_converted_model.tflite\", \"wb\").write(tflite_quantized_model)\r\ndef load_labels(filename):\r\n  my_labels = []\r\n  input_file = open(filename, 'r')\r\n  for l in input_file:\r\n    my_labels.append(l.strip())\r\n  return my_labels\r\nif __name__ == '__main__':\r\n    \r\n    # print('### for coverting to tflite - START ###')\r\n    # Covert_to_tflite()\r\n    # print('### for coverting to tflite - END ###')\r\n    \r\n    print('### for testing performance/accuracy - START###')\r\n    file_name = \"./grace_hopper.jpg\"\r\n    # model_file = \"mobilenet_v1_1.0_224/mobilenet_v1_1.0_224.tflite\"\r\n    # model_file = \"mobilenet_v1_1.0_224/mobilenet_v1_1.0_224_quant.tflite\"\r\n    label_file = \"mobilenet_v1_1.0_224/labels.txt\"\r\n    input_mean = 127.5\r\n    input_std = 127.5\r\n    floating_model = False\r\n\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--graph\", help=\".tflite model to be executed\")\r\n    args = parser.parse_args()\r\n\r\n    if args.graph:\r\n        model_file = args.graph\r\n  \r\n    interpreter = tf.lite.Interpreter(model_path=model_file)\r\n    interpreter.allocate_tensors()\r\n\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    # check the type of the input tensor\r\n    if input_details[0]['dtype'] == type(np.float32(1.0)):\r\n        floating_model = True\r\n\r\n    # NxHxWxC, H:1, W:2\r\n    height = input_details[0]['shape'][1]\r\n    width = input_details[0]['shape'][2]\r\n    img = Image.open(file_name)\r\n    img = img.resize((width, height))\r\n\r\n    # add N dim\r\n    input_data = np.expand_dims(img, axis=0)\r\n\r\n    if floating_model:\r\n        input_data = (np.float32(input_data) - input_mean) / input_std\r\n\r\n    start = time.time()\r\n    interpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\n    interpreter.invoke()\r\n    end = time.time()\r\n\r\n    output_data = interpreter.get_tensor(output_details[0]['index'])\r\n    results = np.squeeze(output_data)\r\n\r\n    top_k = results.argsort()[-5:][::-1]\r\n    labels = load_labels(label_file)\r\n    for i in top_k:\r\n        if floating_model:\r\n            print('{0:08.6f}'.format(float(results[i]))+\":\", labels[i])\r\n        else:\r\n            print('{0:08.6f}'.format(float(results[i]/255.0))+\":\", labels[i])\r\n    print(\"{}, Inference time: {} sec\".format(model_file, end-start))\r\n    print('### for testing performance/accuracy - END###')\r\n```\r\n\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n**Other info / logs**\r\n```\r\n$ python convert_and_test_tflite.py --graph mobilenet_v1_1.0_224/converted_model.tflite\r\n### for testing performance/accuracy - START###\r\n0.445140: 653:military uniform\r\n0.162699: 458:bow tie, bow-tie, bowtie\r\n0.137379: 907:Windsor tie\r\n0.121642: 466:bulletproof vest\r\n0.047150: 668:mortarboard\r\nmobilenet_v1_1.0_224/converted_model.tflite, Inference time: 0.056842803955078125 sec\r\n### for testing performance/accuracy - END###\r\n\r\n$ python convert_and_test_tflite.py --graph mobilenet_v1_1.0_224/quantized_converted_model.tflite\r\n### for testing performance/accuracy - START###\r\n0.259418: 653:military uniform\r\n0.239987: 466:bulletproof vest\r\n0.239753: 907:Windsor tie\r\n0.180003: 458:bow tie, bow-tie, bowtie\r\n0.016782: 668:mortarboard\r\nmobilenet_v1_1.0_224/quantized_converted_model.tflite, Inference time: 0.21327567100524902 sec\r\n### for testing performance/accuracy - END###\r\n```\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["We report performance numbers on mobile devices. Seems like you're running this test on a desktop. We don't have clear baseline for desktop performance for now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27687\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27687\">No</a>\n"]}, {"number": 27685, "title": "tf.layers.batch_normalization missing moving variance and mean, low convolutional model accuracy", "body": "**System information**\r\n- I have written custom code\r\n- Linux Ubuntu 16.04\r\n- TensorFlow installed via pip from binary\r\n- TensorFlow version: 1.7.0\r\n- Python version: 3.6.3\r\n- Bazel: 0.14.1\r\n- GCC: 5.4.0\r\n- CUDA 9.1.0, cuDNN: 7.1.2\r\n- GPU model and memory: NVIDIA Quadro P6000/PCIe/SSE2, 24GB memory\r\n\r\n**Describe the current behavior**\r\nA custom, deep convolutional model, with batch normalization, does not converge. It is a recreated version of a Keras model that reaches accuracy of > 0.8 on the same data, whereas the tensorflow model built from tf.layers gets stuck under accuracy of < 0.3.\r\n\r\nPresumed issue is lack of the moving mean and variance operation in the update ops, but all recommended methods of including them (https://towardsdatascience.com/pitfalls-of-batch-norm-in-tensorflow-and-sanity-checks-for-training-networks-e86c207548c8) have failed.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model should converge to a much higher accuracy and the moving mean and variance operations should be visible in the update ops.\r\n\r\n**Code to reproduce the issue**\r\nI've created a github gist file that can be run on randomly generated data and prints reports about the ops:\r\n\r\nhttps://gist.github.com/mateuszjurewicz/881d13e066f00b349b3eaafe901207f4\r\n\r\nUsage:\r\n\r\npython train_tensorflow.py --random_data=True\r\n\r\nAlternatively the preprocessed data can be downloaded from:\r\nhttps://archive.org/details/voice_data\r\n\r\nYou can also easily use it in combination with tensorboard, just ask the command argument parser for some --help.\r\n\r\n**Other info / logs**\r\nLarger context about this and the pure keras script that converges can be found here:\r\nhttps://github.com/mateuszjurewicz/tensorflow_speech_recognition/blob/master/train_keras.py", "comments": ["Apologies for the delay in response. Can you please try to replicate this issue with smaller example, possibly related to tf.layers.batch_normalization? It's difficult to debug larger models. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Hey, thank you for the response!\r\nI don't think I can replicate this with a smaller model as the example is the only stage where the behavior occurs - a smaller model wouldn't converge with the Keras implementation, thus making the discrepancy between the Keras and pure Tensorflow disappear. I realize it can seem complex, I tried to make it as easy to run as possible. I understand if you can't diagnose it, but I'll appreciate any advice."]}, {"number": 27684, "title": "nan appearing on False fork of tf.where propgates to e", "body": "https://github.com/tensorflow/tensorflow/issues/20091\r\n\r\nSame issue, but that one was closed. The fix is to protect every single argument against the fork.\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nx = tf.convert_to_tensor(np.random.randn(100).astype(np.float32))\r\nalpha = tf.Variable(1, name='asdf', dtype=tf.float32, trainable=True)\r\nwith tf.GradientTape(persistent=True) as t:\r\n    safe_x = tf.where(x > 0, x, tf.ones_like(x))\r\n    # x = safe_x # this fixes it\r\n    s = tf.where(x >= 0, alpha * tf.math.log(x), tf.zeros_like(x))\r\ngradients = t.gradient(s, [alpha])\r\n\r\n\r\nIn [326]: reload(d); d.gradients\r\nOut[326]: [<tf.Tensor: id=2832835, shape=(), dtype=float32, numpy=-35.19606>]\r\n\r\nIn [327]: reload(d); d.gradients\r\nOut[327]: [<tf.Tensor: id=2832875, shape=(), dtype=float32, numpy=nan>]\r\n\r\nIn [346]: sys.version\r\nOut[346]: '3.7.3 (default, Mar 27 2019, 16:54:48) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]'\r\n\r\nIn [347]: platform.platform()\r\nOut[347]: 'Darwin-17.7.0-x86_64-i386-64bit'\r\n\r\nIn [348]: tf.__version__\r\nOut[348]: '2.0.0-alpha0'\r\n\r\n```\r\n\r\n", "comments": ["@cottrell Thanks for trying TF 2.0 Alpha. The code snippet you provided looks incomplete. Can you please provide minimal code snippet to reproduce the issue reported here? Thanks!", "I was missing a \"np.\"float32 but added that. Otherwise I think it is complete (I just copy pasted it and it ran). I think the confusing thing could be I saved it in a file called \"d.py\" or something like and was imp.reloading it from the REPL to show the changes.", "To be clear, this one works (with the bit uncommented).\r\n\r\n```\r\nIn [130]: import tensorflow as tf\r\n     ...: import numpy as np\r\n     ...: x = tf.convert_to_tensor(np.random.randn(100).astype(np.float32))\r\n     ...: alpha = tf.Variable(1, name='asdf', dtype=tf.float32, trainable=True)\r\n     ...: with tf.GradientTape(persistent=True) as t:\r\n     ...:     safe_x = tf.where(x > 0, x, tf.ones_like(x))\r\n     ...:     x = safe_x # this fixes it\r\n     ...:     s = tf.where(x >= 0, alpha * tf.math.log(x), tf.zeros_like(x))\r\n     ...: gradients = t.gradient(s, [alpha])\r\n```", "Does this replicate in TF2.0 nightly or has it been fixed?", "Up until about a month(?) ago, your solution was the only solution.  But the nightly branch now implements a safe gradient operation that should zero out during backprop.  But maybe not directly for the gradient of tertiary where.  @rmlarsen maybe we need the mul_no_nan in the SelectOp gradient as well?", "I believe we decided that the \"automatically safe solution\" hides some true error cases by converting gradients of real nans to 0s.\r\n\r\nSo for now we've disabled the automatic zero-ing out of grad(x)*dy where dy  == 0.\r\n\r\nInstead we should update the docstring of tf.math.log to suggest using tf.math.xlogy, and the division operator to use tf.math.xdivy if y is expected to be 0.\r\n\r\n@rmlarsen @ezhulenev should we do an automatic grappler rewrite for x*log(y) -> xlogy(x,y)?  i think it would lead to tf.function mode being inconsistent with eager mode.  Perhaps that's ok?", "I recommend reading https://stackoverflow.com/a/42497444/433287 for a description of the most common workaround to the nan-where problem.\r\n\r\nEugene, I think the automatic rewrite is a little dangerous because not every single case of x log y wants to compute the (cross) entropy.", "Was able to reproduce the issue with TF v2.2 and TF-nightly i.e. v2.4.0-dev20200722. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/151e1acbe8b0513458f9c7da13187a2f/27684.ipynb#scrollTo=UGHJaUo0xHGm). Thanks!", "@cottrell Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "Looks like it still persists (am on a different machine now, Linux with GPU but I think that shouldn't matter for this problem).\r\n\r\n```\r\nIn [1]: import tensorflow as tf \r\n   ...: import numpy as np \r\n   ...: x = tf.convert_to_tensor(np.random.randn(100).astype(np.float32)) \r\n   ...: alpha = tf.Variable(1, name='asdf', dtype=tf.float32, trainable=True) \r\n   ...: with tf.GradientTape(persistent=True) as t: \r\n   ...:     safe_x = tf.where(x > 0, x, tf.ones_like(x)) \r\n   ...:     # x = safe_x # this fixes it \r\n   ...:     s = tf.where(x >= 0, alpha * tf.math.log(x), tf.zeros_like(x)) \r\n   ...: t.gradient(s, [alpha])                                                                                                                                                                                     \r\n\r\nOut[1]: [<tf.Tensor: shape=(), dtype=float32, numpy=nan>]\r\n\r\nIn [2]: import tensorflow as tf \r\n   ...: import numpy as np \r\n   ...: x = tf.convert_to_tensor(np.random.randn(100).astype(np.float32)) \r\n   ...: alpha = tf.Variable(1, name='asdf', dtype=tf.float32, trainable=True) \r\n   ...: with tf.GradientTape(persistent=True) as t: \r\n   ...:     safe_x = tf.where(x > 0, x, tf.ones_like(x)) \r\n   ...:     x = safe_x # this fixes it \r\n   ...:     s = tf.where(x >= 0, alpha * tf.math.log(x), tf.zeros_like(x)) \r\n   ...: t.gradient(s, [alpha])                                                                                                                                                                                     \r\nOut[2]: [<tf.Tensor: shape=(), dtype=float32, numpy=-38.554565>]\r\n\r\nIn [3]: import sys, platform                                                                                                                                                                                       \r\n\r\nIn [4]: sys.version                                                                                                                                                                                                \r\nOut[4]: '3.8.3 (default, May 19 2020, 18:47:26) \\n[GCC 7.3.0]'\r\n\r\nIn [5]: platform.platform()                                                                                                                                                                                        \r\nOut[5]: 'Linux-5.4.0-65-generic-x86_64-with-glibc2.10'\r\n\r\nIn [6]: tf.__version__                                                                                                                                                                                             \r\nOut[6]: '2.5.0'\r\n```\r\n", "Issue still persists in `TF 2.6.0` and `TF-nightly`. Please find the [gist here](https://colab.research.google.com/gist/sanatmpa1/9be46297dafef158ba1e2ffb0cd96524/27684.ipynb). Thanks!", "We rolled back the \"safe rewrite\" because, as @alextp mentioned, the assumption doesn't always hold.\r\n\r\nIn other words, this behavior is backprop working as intended!  Trying to automagically fix it breaks folks who rely on the non-safe multiply during backprop.  Closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27684\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27684\">No</a>\n"]}, {"number": 27683, "title": "KeyError in tensorflow\\contrib\\layers\\python\\layers\\feature_column.py", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.0 cuDNN 7.4.2\r\n- GPU model and memory: GeForce GTX 960M totalMemory: 4.00GiB freeMemory: 3.34GiB\r\n\r\n**Describe the current behavior**\r\nHere is my code, I use a random forest estimator \r\n\r\n`feature_col = [real_valued_column(column_name=\"x\", dimension=38)]`\r\n`model = tf_random_forest.TensorForestEstimator(\r\n    hparams,\r\n    model_dir=dataset_path,\r\n    feature_columns=feature_col,\r\n    report_feature_importances=True)`\r\n\r\nI have 38 features, when I train the model, I got a KeyError.\r\n\r\n> Traceback (most recent call last):\r\n  File \"tf_dataset.py\", line 130, in <module>\r\n    model.train(input_fn=train_input_fn, steps=num_steps)\r\n  File \"D:\\venv\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 358, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"D:\\venv\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1124, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"D:\\venv\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1154, in _train_model_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"D:\\venv\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1112, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"D:\\venv\\project\\Malicious_TLS_Detection\\machine_learning\\tf_random_forest.py\", line 162, in _model_fn\r\n    layers.transform_features(features, feature_columns))\r\n  File \"D:\\venv\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column_ops.py\", line 656, in transform_features\r\n    transformer.transform(column)\r\n  File \"D:\\venv\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column_ops.py\", line 848, in transform\r\n    feature_column.insert_transformed_feature(self._columns_to_tensors)\r\n  File \"D:\\venv\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column.py\", line 1873, in insert_transformed_feature\r\n    input_tensor = self._normalized_input_tensor(columns_to_tensors[self.name])\r\nKeyError: 'x'\r\n\r\nI followed the instructions below, but I got `KeyError: ''`\r\n[KeyError_stackoverflow](https://stackoverflow.com/questions/39687554/keyerror-in-tensorflow-when-calling-predict-on-trained-model)\r\n\r\nwhatever I changed the column name, I always got a KeyError. Can someone explain why?\r\n\r\n\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27682, "title": "R2.0 tf lite cross compiling", "body": "This pull request add the support of cross compiling variables.\r\nIt allow to cross compile TFLite library and python module for platforms other than aarch64, Rpi and iOS\r\nIt has been  successfully tested with Yocto Openembedded environment to build TensorFlow Lite for the new STM32MP1 Microprocessor target from STMicroelectronics.\r\n\r\nBR\r\nVincent Abriou\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27682) for more info**.\n\n<!-- need_sender_cla -->", "@vinceab please sign CLA ", "@martinwicke how do we handle PRs to 2.0 branch?\r\nShould we just accept changes to master, then merge the changes from master to 2.0?", "We should reject all PRs to 2.0. The branch will be fast forwarded while we're in alpha, and only become a true release branch once we have an RC. All changes should go into master. \r\n\r\n@vinceab please resend against master. "]}, {"number": 27681, "title": "Hi Everyone. I am working on deploying tensorflow lite model file on Raspberry Pi 3. When I try to import a tflite model into the tflite interpreter, it throws an error. Please help!!!", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): resbian 9.0\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below):1.13.1\r\n- Python version:3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\ninterpreter=tf.lite.Interpreter(\"/home/pi/download/mobilenet_v1_1.0_224_quant.tflite\")\r\n\r\nERROR//\r\n\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\r\n  return f(*args, **kwds)\r\n1.13.1\r\nTraceback (most recent call last):\r\n  File \"tensorlite.py\", line 5, in <module>\r\n    interpreter = tf.lite.Interpreter(\"/home/pi/mobilenet_v1_1.0_224_quant.tflite\")\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/lite/python/interpreter.py\", line 54, in __init__\r\n    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/python/util/lazy_loader.py\", line 61, in __getattr__\r\n    module = self._load()\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/python/util/lazy_loader.py\", line 44, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.5/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 673, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 28, in <module>\r\n    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 693, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: /home/pi/.local/lib/python3.5/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils24NeonVectorScalarMultiplyEPKaifPf\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}, {"number": 27680, "title": "tf.data.Dataset.shuffle produces the same results at each dataset iteration in tensorflow 2 alpha", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes. Create a simple dataset, shuffle it and iterate through it.  \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): \r\n2.0.0-alpha0\r\n- Python version:\r\nPython 3.7.3\r\n- CUDA/cuDNN version:\r\nCUDA10.0\r\nNot relevant.\r\n- GPU model and memory:\r\nNot relevant.\r\n\r\n**Describe the current behavior**\r\nwhen using tf.data.Dataset.shuffle and iterating through the dataset multiple times the shuffled order is always the same.\r\n\r\n**Describe the expected behavior**\r\nThe order should change for every iteration.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow as tf\r\ninput_array = tf.range(10)\r\nprint(input_array)\r\ndataset = tf.data.Dataset.from_tensor_slices(input_array).shuffle(5)\r\n\r\ndef print_values():\r\n    for val in dataset:\r\n        print(val.numpy(), end=\" \")\r\n    print()\r\n\r\nprint_values()\r\nprint_values()\r\n```\r\n\r\noutput:\r\n```\r\ntf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)\r\n2 1 4 6 0 8 9 5 7 3 \r\n2 1 4 6 0 8 9 5 7 3 \r\n\r\n```\r\n\r\n\r\n**Other info / logs**\r\nBehaviour is correct in tensorflow 1.13.", "comments": ["Thanks for trying TF 2.0 alpha. I was able to reproduce your behavior in TF 2.0 alpha however it executed successfully using TF 1.13.1 (after enabling eager mode) .", "@fabio12345 when you say the behavior is \"correct\" in TF 1.13, are you talking about graph mode or eager mode? Could you provide an example of the program that you ran with TF 1.13?\r\n\r\nWhen I try the following eager mode program against TF nightly, I see the same behavior as you do with TF 2.0:\r\n\r\n```\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\ninput_array = tf.range(10)\r\nprint(input_array)\r\ndataset = tf.data.Dataset.from_tensor_slices(input_array).shuffle(5)\r\n\r\ndef print_values():\r\n  for val in iter(dataset):\r\n    print(val.numpy(), end=\" \")\r\n  print()\r\n\r\nprint_values()\r\nprint_values()\r\n```\r\n\r\noutput:\r\n```\r\ntf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)\r\n0 5 3 2 4 8 9 1 7 6 \r\n0 5 3 2 4 8 9 1 7 6 \r\n```\r\n\r\nThe behavior you are seeing is expected (both for TF 2.0 and TF 1.13 eager mode) because your program creates multiple separate iterators and the order of iteration of each is deterministic.\r\n\r\nTo see different order of elements for each epoch, you can make use the `.repeat()` transformation:\r\n```\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\n\r\ntf.enable_v2_behavior()\r\n\r\ninput_array = tf.range(10)\r\nprint(input_array)\r\ndataset = tf.data.Dataset.from_tensor_slices(input_array).shuffle(5).repeat(2)\r\n\r\ndef print_values():\r\n  for val in iter(dataset):\r\n    print(val.numpy(), end=\" \")\r\n  print()\r\n\r\nprint_values()\r\n```\r\n\r\noutput:\r\n```\r\ntf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)\r\n0 2 6 5 7 8 9 1 3 4 2 5 1 3 6 8 9 0 7 4 \r\n0 2 6 5 7 8 9 1 3 4 2 5 1 3 6 8 9 0 7 4 \r\n```", "@fabio12345 Could you try this\r\n\r\n```\r\n!pip install tensorflow==2.0.0-alpha0\r\nimport tensorflow as tf\r\ninput_array = tf.range(10)\r\nprint(input_array)\r\ndataset = tf.data.Dataset.from_tensor_slices(input_array)\r\nprint(dataset)\r\n\r\n\r\ndef print_values():\r\n  for val in dataset:\r\n    print(val.numpy(), end=\" \")\r\n\r\nfor i in range(3):\r\n  dataset=dataset.shuffle(5,reshuffle_each_iteration=True)\r\n  print_values()\r\n  print()\r\n```\r\n\r\nThanks!", "I'm also experiencing this problem. [API Docs](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle) state clearly that the intended behavior is that the dataset should be re-shuffled at each _iteration_ \r\n\r\n> \"the dataset should be pseudorandomly reshuffled each time it is iterated over\".\r\n\r\nI noticed the reshuffle_each_iteration parameter has no effect (tried either True, False and None), subsequent iterations of the dataset always lead the same result.\r\n\r\nThe only workaround that came to my mind is to create a lambda which calls the dataset creation function before every iteration, but I worry this would be inefficient (and not elegant):\r\n\r\n```\r\ndata = np.arange(100)\r\ndata = data.reshape([10, 10])\r\ndef read_dataset():\r\n    return tf.data.Dataset.from_tensor_slices(data).shuffle(100, reshuffle_each_iteration=True).batch(2)\r\ndataset = read_dataset()\r\ndataset_shuf = lambda: read_dataset()\r\n# This doesn't work\r\nfor d in dataset:\r\n    tf.print(d)\r\n\r\n# This Works\r\nfor d in dataset_shuf():\r\n    tf.print(d)\r\n\r\n```\r\n\r\n@jsimsa If the behavior would be expected as you say, then there's little use in having a \"reshuffle_each_iteration\" parameter in a framework that is executing eagerly by default. Moreover, repeat() is not a viable option if you need to alternate different type of computations after each epoch, because every time you iterate again it would lead the same sequence of samples.", "@edoardogiacomello \r\n\r\nI agree with you that `reshuffle_each_iteration` in eager mode does not have the expected effect. The documentation is misleading because \"iteration\" in the documentation is meant to imply repetition based on `repeat` and not based on Python iteration.\r\n\r\nYour example of \"This Works\" works because a new dataset graph is created for each iteration. The value of `reshuffle_each_iteration` is still not respected -- setting `reshuffle_each_iteration` to `False` in your input pipeline will not result in identical order of batches between iterations.\r\n\r\nI will create a PR that updates the documentation and renames the argument to `reshuffle_on_repeat` to better communicate what the argument does.\r\n\r\nFinally, to achieve the desired effect of reshuffling across different Python iterations of a dataset, your approach based on a dataset factory is idiomatic:\r\n\r\n```\r\ndef make_dataset():\r\n  return tf.data.Dataset.from_tensor_slices(data).shuffle(100).batch(2)\r\n\r\n# illustrate that batches are shuffled in a different order\r\nfor _ in range(2):\r\n  for d in make_dataset():\r\n    tf.print(d)\r\n```", "It would be really great to have a way to iterate multiple times over a dataset with different orders.\r\n\r\nIn the tutorials for TensorFlow 2.0, the recommended way to train a model is:\r\n```python\r\ndataset = ...\r\ndataset = dataset.shuffle(buffer_size)\r\ndataset = dataset.batch(batch_size)\r\n\r\nfor epoch in range(num_epochs):\r\n    for x, y in dataset:\r\n        train_step(model, x, y, loss_fn)\r\n```\r\n\r\nThe issue with this code is that at each epoch, the model sees the batches in the same order.\r\n\r\nIt would be very useful to have a parameter like `reshuffle_each_iteration`, but that would work for each time we iterate over the dataset in eager mode.\r\n\r\n---\r\nThe current solution described above involves creating a new dataset each time, which doesn't seem ideal:\r\n\r\n```python\r\ndataset = ...\r\n\r\n\r\nfor epoch in range(num_epochs):\r\n    dataset_epoch = dataset.shuffle(buffer_size)\r\n    dataset_epoch = dataset_epoch.batch(batch_size)\r\n    for x, y in dataset_epoch:\r\n        train_step(model, x, y, loss_fn)\r\n```", "We are actively working on this and plan to have this ready by TF 2.0 RC0. Most likely, the mechanism for controlling this will be a `tf.data.Option` option (as opposed to `shuffle` option). cc @rohan100jain ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27680\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27680\">No</a>\n", "For future reference, this behavior was fixed in this commit: e7206a7d8efceda25bce5a530fc4e79a5582be13\r\n\r\nNow when `reshuffle_each_iteration=True`, iterating over the dataset multiple times in eager mode will work as expected.\r\n\r\n---\r\n\r\nThe `reshuffle_each_iteration` argument is True by default.  \r\nAnd the fix from the commit mentioned above is included in the TF 2.0 release candidate ! \ud83c\udf7e \r\n\r\n```python\r\n# pip install tensorflow==2.0.0-rc0\r\n\r\ndataset = tf.data.Dataset.range(10)\r\ndataset = dataset.shuffle(buffer_size=5, reshuffle_each_iteration=False)\r\n\r\nprint([x.numpy() for x in dataset])\r\nprint([x.numpy() for x in dataset])\r\n\r\ndataset = tf.data.Dataset.range(10)\r\ndataset = dataset.shuffle(buffer_size=5, reshuffle_each_iteration=True)\r\n\r\nprint([x.numpy() for x in dataset])\r\nprint([x.numpy() for x in dataset])\r\n```\r\n\r\nThis will output something like:\r\n>[3, 1, 2, 6, 8, 7, 5, 0, 9, 4]\r\n>[3, 1, 2, 6, 8, 7, 5, 0, 9, 4]\r\n>[4, 3, 5, 1, 8, 7, 0, 2, 6, 9]\r\n>[3, 0, 2, 5, 8, 4, 9, 6, 1, 7]", "@jsimsa : could it be possible to include an example explaining the behavior of `tf.data.Dataset.shuffle` in eager mode with tf 2.0 in the documentation?\r\n\r\nThe current documentation only mention:\r\n>reshuffle_each_iteration: (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to True.)\r\n\r\nwhich doesn't really explain in full length the new behavior", "Sounds good. @aaudiber could you please extend the `shuffle` documentation to provide additional information about the semantics of `reshuffle_each_iteration` for both repeat-based epochs and iteration-based epochs? Thank you.", "> We are actively working on this and plan to have this ready by TF 2.0 RC0. Most likely, the mechanism for controlling this will be a `tf.data.Option` option (as opposed to `shuffle` option). cc @rohan100jain\r\n\r\nDear @jsimsa , I am working on a project that requires me to balance the classes in each batch after the dataset is being shuffled. Ideally, I don't want to create a new dataset at the beginning of each epoch. At first I considered the following approach:\r\n```python\r\ndef batch_balance(input: tf.data.Dataset):\r\n    #create a function that can re-organize the elements from the shuffled dataset\r\n    #and return a new one that is class-balanced within each batch but still random (to some extent)\r\n    return a_new_dataset\r\ntfds = tf.data.Dataset.from_tensor_slices([[0,0,1],[0,0,1],[0,1,0],[0,1,0],[1,0,0],[1,0,0]])\r\ntfds = tfds.shuffle(buffer_size=6, reshuffle_each_iteration=True)\r\ntfds = tfds.apply(batch_balance)\r\ntfds = tfds.batch(3)\r\nlist(tfds.as_numpy_iterator())\r\nlist(tfds.as_numpy_iterator())\r\n```\r\nand I get the following output:\r\n>[0,1,0], [0,0,1], [1,0,0]\r\n>[1,0,0], [0,0,1], [0,1,0] (first iteration)\r\n\r\n>[0,1,0], [0,0,1], [1,0,0]\r\n>[1,0,0], [0,0,1], [0,1,0] (second iteration)\r\n\r\nThis does the trick for the first pass, but in fact, after applying the `batch_balance` function, the returned dataset no longer has the` reshuffle_each_iteration` feature, which leads to two identical sequences.\r\n\r\nTo preserve this reshuffle feature, I've considered writing a modified `shuffle` / `batch`, and then I notice these two functions are both implemented in the `gen_dataset_ops` file, which is machine generated (and very difficult to read...). Therefore, I guess the question is, are there any built-in functions that can allow me to modify the dataset after it is being shuffled w/o losing the `reshuffle_each_iteration` feature? You hinted earlier that `tf.data.Option` can be used, would you mind elaborating a bit more on this?\r\n\r\nThank you!\r\n", "@ff4456 `reshuffle_each_iteration` should continue to work even after applying additional transformations. Can you share a colab to reproduce the issue?", "Hi @aaudiber , sure! Here is the link: https://colab.research.google.com/drive/1gWxyYHhh5dFTmFbZiEyXwlU4SuUyPY-B?usp=sharing", "The function passed to `apply` is only called once, so you're only ever iterating over the shuffled dataset once. That first iteration is stored into `balanced_list`, which gets reused each time you create an iterator over the `tf.data.Dataset.from_generator`.\r\n\r\nTo fix this, you can update the generator function to re-iterate over the input to `apply` each time. Here is an updated example of your code that works as expected: https://colab.research.google.com/drive/1Xy8rYbrn2wyJVmvn2uFrSOo2AUHdtxuk", "> The function passed to `apply` is only called once, so you're only ever iterating over the shuffled dataset once. That first iteration is stored into `balanced_list`, which gets reused each time you create an iterator over the `tf.data.Dataset.from_generator`.\r\n> \r\n> To fix this, you can update the generator function to re-iterate over the input to `apply` each time. Here is an updated example of your code that works as expected: https://colab.research.google.com/drive/1Xy8rYbrn2wyJVmvn2uFrSOo2AUHdtxuk\r\n\r\nah I see... Thank you so much!", "Hi,\r\n\r\nHow could we reshuffle dataset in every epoch without using eager mode? \r\n```\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_eager_execution()\r\n```\r\n\r\nIn my case, data remains in same sequence after every iteration. I need to reshuffle at position **#Reshuffle is required here** .\r\n\r\n```\r\ndata = [...]\r\ndata = data.shuffle(self.batch_size * 2, reshuffle_each_iteration=True)\r\ndata = data.batch(self.batch_size)\r\n\r\niterator = tf.data.Dataset.make_initializable_iterator(data)\r\nbatch = iterator.get_next()\r\n\r\nepoch = 0\r\nwith tf.Session() as sess:\r\n     while epoch < 2:\r\n            batch_count = 0\r\n            sess.run(iterator.initializer)\r\n            # Reshuffle is required here \r\n\r\n\r\n\r\n            while True:  \r\n                try:\r\n                    **batch_val = sess.run(batch)**\r\n                    print(\"\\nEpoch: {} -- Batch {} :  \".format(epoch, batch_count))\r\n\r\n                except tf.errors.OutOfRangeError:\r\n                    epoch += 1\r\n                    break\r\n```\r\nThanks in advance. ", "@engrmz To get different orders you can use `data = data.repeat(num_epochs)`, to repeat the dataset `num_epochs` times, with each repetition doing a reshuffle.", "> @engrmz To get different orders you can use `data = data.repeat(num_epochs)`, to repeat the dataset `num_epochs` times, with each repetition doing a reshuffle.\r\n\r\nHi @aaudiber,\r\nThanks for your prompt response. \r\n**Point 1:** When I use repeat(5), it repeats the dataset 5 times and shuffles it too. Doing this, I can get shuffled data in every batch. My code (as given above) considers this step as epoch 1 (as I am trying to repeat dataset using epoch loop).\r\n\r\n**Point 2:** However, after 5 repetitions of whole dataset, in epoch 2, I still get the same sequence (5 repatriations) as in **Point1:** - same as epoch 1.\r\n\r\nIs there any way to re-shuffle dataset after every epoch iteration (after `sess.run(iterator.initializer)`) without using repeat function?\r\n ", "The TF1 way of doing things is to handle multiple iterations of the dataset by using `repeat()`. Creating a new iterator in each epoch loop is the TF2 way. If you'd like to discuss further please open a separate issue so we can avoid hijacking this one."]}, {"number": 27679, "title": "Passing a dictionary of tensors to py_function", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.0\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0 alpha\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\ntf.data.Dataset supports dictionary as a valid type of its elements, which is convenient when your data source have multiple tensors and you want to assign a name to each of them. However if I want to manipulate these tensors by mapping the dataset to a tf.py_function, it complains that the dictionary is not compatible with the Tensor, even though its values are Tensors.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef make_dictionary(inp):\r\n    out = {}\r\n    out[\"image1\"] = inp[0]\r\n    out[\"image2\"] = inp[1]\r\n    out[\"image3\"] = inp[2]\r\n    return out\r\n\r\n\r\ndef process_data(data):\r\n    print(\"image1\", data[\"image1\"])\r\n    print(\"image2\", data[\"image2\"])\r\n    print(\"image3\", data[\"image3\"])\r\n    return 0.0\r\n\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(np.arange(10))\r\n\r\ndataset = tf.data.Dataset.zip((dataset, dataset, dataset))\r\ndataset = dataset.map(lambda *stuff: make_dictionary(stuff))\r\n# now each element in dataset is a dictionary\r\n# processing the dictionary however is not supported by py_function:\r\n\r\ndataset = dataset.map(\r\n      lambda data: tf.py_function(\r\n          process_data, [data], tf.float32))\r\n\r\nfor image in dataset.take(4):\r\n    print(image)\r\n```\r\n\r\nThe error I get is:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 559, in make_tensor_proto\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 559, in <listcomp>\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/util/compat.py\", line 61, in as_bytes\r\n    (bytes_or_text,))\r\nTypeError: Expected binary or unicode string, got {'image1': <tf.Tensor 'args_0:0' shape=() dtype=int64>, 'image2': <tf.Tensor 'args_1:0' shape=() dtype=int64>, 'image3': <tf.Tensor 'args_2:0' shape=() dtype=int64>}\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 471, in _apply_op_helper\r\n    as_ref=input_arg.is_ref)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1251, in internal_convert_n_to_tensor\r\n    ctx=ctx))\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1186, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 304, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 245, in constant\r\n    allow_broadcast=True)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 283, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 563, in make_tensor_proto\r\n    \"supported type.\" % (type(values), values))\r\nTypeError: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'image1': <tf.Tensor 'args_0:0' shape=() dtype=int64>, 'image2': <tf.Tensor 'args_1:0' shape=() dtype=int64>, 'image3': <tf.Tensor 'args_2:0' shape=() dtype=int64>}. Consider casting elements to a supported type.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tmp.py\", line 28, in <module>\r\n    lambda data: tf.py_function(\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1012, in map\r\n    return MapDataset(self, map_func, preserve_cardinality=True)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2986, in __init__\r\n    use_legacy_function=use_legacy_function)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2388, in __init__\r\n    self._function = wrapper_fn._get_concrete_function_internal()\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1319, in _get_concrete_function_internal\r\n    *args, **kwargs)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1313, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1580, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1512, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 694, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2381, in wrapper_fn\r\n    ret = _wrapper_helper(*args)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2326, in _wrapper_helper\r\n    ret = func(*nested_args)\r\n  File \"tmp.py\", line 29, in <lambda>\r\n    process_data, [data], tf.float32))\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 389, in eager_py_func\r\n    return _internal_py_func(func=func, inp=inp, Tout=Tout, eager=True, name=name)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 278, in _internal_py_func\r\n    input=inp, token=token, Tout=Tout, name=name)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py\", line 74, in eager_py_func\r\n    \"EagerPyFunc\", input=input, token=token, Tout=Tout, name=name)\r\n  File \"/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 502, in _apply_op_helper\r\n    \"%s that are invalid. Tensors: %s\" % (prefix, values))\r\nTypeError: Tensors in list passed to 'input' of 'EagerPyFunc' Op have types [<NOT CONVERTIBLE TO TENSOR>] that are invalid. Tensors: [{'image1': <tf.Tensor 'args_0:0' shape=() dtype=int64>, 'image2': <tf.Tensor 'args_1:0' shape=() dtype=int64>, 'image3': <tf.Tensor 'args_2:0' shape=() dtype=int64>}]\r\n```\r\n\r\n\r\n\r\n", "comments": ["Any updates on this?", "This is a limitation of `py_function` which only supports a list of `Tensor` inputs and I don't need see a straightforward way to extend its implementation to support dictionaries.\r\n\r\nFWIW, you could deconstruct the dictionary structure ahead of invoking the `py_function`:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_v2_behavior()\r\n\r\ndef deconstruct(x):\r\n  return x['a'], x['b']\r\n\r\ndef process(*args):\r\n  a, b = args\r\n  return a + b\r\n\r\ndataset = tf.data.Dataset.from_tensors({'a': 1, 'b': 2})\r\ndataset = dataset.map(deconstruct)\r\ndataset = dataset.map(lambda *args: tf.py_function(process, args, tf.int32))\r\n\r\nfor elem in dataset:\r\n  print(elem.numpy())\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27679\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27679\">No</a>\n", "In a related feature request (Support Sparse Tensors in py_function, #30069), @alextp noted:\r\n\r\n> I think now that we have compositetensor and typespec it should be straightforward to implement sparsetensor support in py_function by treating all composites generically.\r\n\r\nCould the same mechanism provide a path to implementing dictionary handling by `py_function`?", "Not until:\r\na) `CompositeTensor` inputs are supported to `py_function`\r\nb) a subclass of `CompositeTensor` exists that can represent a dictionary", "Most APIs that support composite tensors also support nested structures (as defined by [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest).  In particular, the tf.nest functions include an \"expand_composites\" argument that tells tf.nest that it should treat composite tensors as nested structures.\r\n\r\nTherefore, we don't need a CompositeTensor for dictionaries -- we can just use dictionaries as is.  But what we do ned is to extend py_function to use tf.nest to do appropriate flattening and repacking of tensors at the input & output boundaries.  (Using \"expand_composites=True\" will ensure that this supports both \"normal\" nested structures and composite tensors.)", "Here's an example showing how the existing py_function could be wrapped to handle nested structures, including \"normal\" nested structures like dictionaries or tuples and also composite tensors:\r\n\r\n```\r\ndef new_py_function(func, inp, Tout, name=None):\r\n  def wrapped_func(*flat_inp):\r\n    reconstructed_inp = tf.nest.pack_sequence_as(inp, flat_inp,\r\n                                                 expand_composites=True)\r\n    out = func(*reconstructed_inp)\r\n    return tf.nest.flatten(out, expand_composites=True)\r\n  flat_Tout = tf.nest.flatten(Tout, expand_composites=True)\r\n  flat_out = tf.py_function(\r\n      func=wrapped_func, \r\n      inp=tf.nest.flatten(inp, expand_composites=True),\r\n      Tout=[_tensor_spec_to_dtype(v) for v in flat_Tout],\r\n      name=name)\r\n  spec_out = tf.nest.map_structure(_dtype_to_tensor_spec, Tout, \r\n                                   expand_composites=True)\r\n  out = tf.nest.pack_sequence_as(spec_out, flat_out, expand_composites=True)\r\n  return out\r\n\r\ndef _dtype_to_tensor_spec(v):\r\n  return tf.TensorSpec(None, v) if isinstance(v, tf.dtypes.DType) else v\r\n\r\ndef _tensor_spec_to_dtype(v):\r\n  return v.dtype if isinstance(v, tf.TensorSpec) else v\r\n```\r\n\r\n(Warning: I tested this out on your example, but haven't done thorough testing, so there might be a bug or two in this code; but it should give you the basic idea of what needs to be done.)", "@eldar Is this still an issue for you? did you try the suggestions from @jsimsa @edloper .\r\nCan you try recently released `TF2.0` and let us know whether the issue persists with the latest version? Thanks!", "I just tested it using v2.0, and it still isn't possible to supply dictionaries to a `py_function` as inputs. Attempting to do so still results in:\r\n\r\n> TypeError: Tensors in list passed to 'input' of 'EagerPyFunc' Op have types [<NOT CONVERTIBLE TO TENSOR>] that are invalid.\r\n\r\n@edloper's workaround seems like it may work (at least for the simple case that I have tried it with so far), but I still believe that `py_function` itself should be modified to be able to accept (and return, for that matter) dictionaries and other composite tensors.", "@novog If you're interested in submitting a pull request that updates `py_function` to support nested structures, it looks like you could probably fold the suggestions I made above into [_internal_py_func](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/script_ops.py#L251).  (You'd need to add tests etc. as well.)", "@edloper Thanks again.\r\n\r\nYour example code looks like it might be able to handle nested return types, but I can't make it work with a function that returns a `Dict[str, tf.Tensor]`. Should this work, and if so, what should be specified for Tout?", "You can use something like: Tout={\"foo\": tf.int32, \"bar\": tf.float32}.\r\n\r\nNote that you'll need to know the keys of the dict and the types of the tensor values statically.  In particular, if we're in graph mode, then we need to know what this structure looks like statically, so we can create the output tensors.  If the size/set of keys in the dict can change each time the python function is called, then there's no sensible way to hook it up to the output.\r\n\r\nIn the general case, you can use nested structure in your output, but only if the \"structure\" part of that nested structure is fixed and known in advance.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27679\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27679\">No</a>\n", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "So, I created a patch thanks to @edloper suggestion [here](https://github.com/minhtriet/tensorflow/blob/master/tensorflow/python/ops/script_ops.py#L373). That is the only change\r\n\r\n```\r\n2019-12-14 21:35:12.013946: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at iterator_ops.cc:929 : Invalid argument: {{function_node __inference_Dataset_map_<lambda>_19}} 0-th value returned by pyfunc_0 is double, but expects float\r\n\t [[{{node PyFuncStateless}}]]\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\cool\\Miniconda3\\envs\\tf_contrib_conda\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\", line 666, in next\r\n    return self._next_internal()\r\n  File \"C:\\Users\\cool\\Miniconda3\\envs\\tf_contrib_conda\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\", line 651, in _next_internal\r\n    output_shapes=self._flat_output_shapes)\r\n  File \"C:\\Users\\cool\\Miniconda3\\envs\\tf_contrib_conda\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\", line 2672, in iterator_get_next_sync\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __inference_Dataset_map_<lambda>_19}} 0-th value returned by pyfunc_0 is double, but expects float\r\n\t [[{{node PyFuncStateless}}]] [Op:IteratorGetNextSync]\r\n```\r\n`pyfunc_0` returns a double, but expects float, but I cannot debug this function. Could someone give me a pointer? Thank you", "Add a tf.cast?", "Hello @edloper,\r\n\r\nI did like you pointed out in a similar case:\r\n```python\r\nTout={\"input_ids\": tf.int32, \"attention_mask\": tf.int32}, {\"input_ids\": tf.int32, \"attention_mask\": tf.int32}\r\n```\r\nBut I got the following error:\r\n\r\n```python\r\nValueError: Attempt to convert a value \r\n({\r\n'input_ids': [101, 13366, 2131, 1035, 6819, 2094, 1035, 2013, 1035, 24471, 2140, 1006, 24471, 2140, 1007, 102], \r\n'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\r\n}) \r\nwith an unsupported type (<class 'dict'>) to a Tensor.\r\n```\r\n\r\nAny suggestion?", "@jeromerg This issue was about passing values in & out of `tf.py_function`, and not about `tf.function`.  Passing nested dictionaries, namedtules, lists, and tuples of tensors in & out of `tf.function` should work fine.", "@Ceceu The following works fine for me, using the `new_py_function` defined above:\r\n\r\n```\r\ndef my_function(x):\r\n  return ({\"input_ids\": x, \"atention_mask\": x},\r\n          {\"input_ids\": x, \"atention_mask\": x})\r\n  \r\nnew_py_function(my_function, [tf.constant([5, 6, 7])], Tout=(\r\n    {\"input_ids\": tf.int32, \"attention_mask\": tf.int32},\r\n    {\"input_ids\": tf.int32, \"attention_mask\": tf.int32}))\r\n```\r\n", "@edloper Thanks a lot for your new_py_function, which is very very useful to work with dicts !!\r\nIt should definitely be integrated in Tensorflow ! \r\nFor those who would like another example, I used it with map to pass input and output dicts, add new data to the output dict and return the 2 dicts, like this :\r\n```python\r\ndef my_py_func_adding_data(self, dicts):\r\n\tdict_inputs, dict_outputs = dicts\r\n\tnew_data = some_function(dict_outputs[\"output1\"].numpy())\r\n\tdict_outputs[\"output2\"] = new_data\r\n\treturn dict_inputs, dict_outputs\r\n\r\nds = ds.map(lambda dict_in, dict_out: new_py_function(my_py_func_adding_data,\r\n                                                      inp=[(dict_in, dict_out)], \r\n\t\t\t\t\t\t      Tout=({\"img\": tf.float32}, {\"output1\": tf.float32, \"output2\": tf.float32})))\r\n```\r\nAnd it worked like a charm :)", "```\r\ndef tf_parser_func(img_id):\r\n    (dict1, dict2) = tf.py_function(\r\n        func = self.parser_func, \r\n        inp = [img_id],\r\n        Tout = ({'input_1':tf.float32}, {'classification':tf.float32,'regression':tf.float32})\r\n    )\r\n    return dict1, dict2\r\n\r\ndef parser_func(img_id):\r\n    if type(img_id) != int:\r\n        img_id = int(img_id.numpy())\r\n        assert type(img_id) == int \r\n    \r\n    image = load_image(img_id) # get image based img_id\r\n    annos = load_annotations(img_id) # get annos based img_id\r\n\r\n    inputs, cls_targets, reg_targets = compute_inputs_targets(image, annos) # generate inputs and labels\r\n\r\n    return ({'input_1':inputs}, {'classification':cls_targets,'regression':reg_targets})\r\n\r\ndef get_dataset(imgae_ids):\r\n    dataset = tf.data.Dataset.from_tensor_slices(image_ids) # create dataset from a list\r\n    dataset = dataset.map(tf_parser_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    dataset = dataset.batch(1, drop_remainder=True).repeat(1)\r\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n    return dataset\r\n```\r\n\r\n@ismael-elatifi  And could you help me find out where is wrong in above code? The dataset that I want to make is create a input and two ouput, and I want make every output has a name so I can pass this dataset into keras model.fit func."]}]