[{"number": 27339, "title": "TPU training with Keras API raises error in Tensorflow 2.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): no \r\n- TensorFlow version (use command below): 2.0.0.dev20190330\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: Google Colab Colud TPU\r\n\r\n**The current behavior**\r\n\r\nBy following the [official](https://www.tensorflow.org/alpha/guide/distribute_strategy) guide on Tensorflow 2.0 distributed training, the `model.fit()` method raises an error regarding unregistered op type *( Op type not registered 'ExperimentalRebatchDataset' in binary running on n-23c14aca-w-0 )*.\r\n\r\n**The expected behavior**\r\n\r\nThe expected bahaviour is running code without any exception.\r\n\r\n**Code to reproduce the issue**\r\n\r\nThe code in the [google colab](https://gist.github.com/Mrpatekful/60401e8c7b8965bb3624c2c2e9b8df0f).\r\n\r\n```python\r\n!pip install tf-nightly-2.0-preview\r\nfrom __future__ import absolute_import, division, print_function\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\ntpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\nwith tpu_strategy.scope():\r\n  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\r\n  model.compile(loss='mse', optimizer='sgd')\r\n\r\nBATCH_SIZE_PER_REPLICA = 10\r\nbatch_size = BATCH_SIZE_PER_REPLICA * tpu_strategy.num_replicas_in_sync\r\ndataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(batch_size, drop_remainder=True)\r\nmodel.fit(dataset, epochs=2)\r\nmodel.evaluate(dataset)\r\n```\r\n**Other info / logs**\r\n\r\nThe full exception:\r\n\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0331 08:24:17.732600 140554474739584 training_utils.py:1268] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1336     try:\r\n-> 1337       return fn(*args)\r\n   1338     except errors.OpError as e:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1319       # Ensure any changes to the graph are reflected in the runtime.\r\n-> 1320       self._extend_graph()\r\n   1321       return self._call_tf_sessionrun(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _extend_graph(self)\r\n   1354     with self._graph._session_run_lock():  # pylint: disable=protected-access\r\n-> 1355       tf_session.ExtendSession(self._session)\r\n   1356 \r\n\r\nNotFoundError: Op type not registered 'ExperimentalRebatchDataset' in binary running on n-23c14aca-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-7-60cafe7e9d96> in <module>()\r\n      2 batch_size = BATCH_SIZE_PER_REPLICA * tpu_strategy.num_replicas_in_sync\r\n      3 dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(batch_size, drop_remainder=True)\r\n----> 4 model.fit(dataset, epochs=2)\r\n      5 model.evaluate(dataset)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    745             steps_per_epoch=steps_per_epoch,\r\n    746             validation_steps=validation_steps,\r\n--> 747             validation_freq=validation_freq)\r\n    748 \r\n    749     batch_size = self._validate_or_infer_batch_size(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py in fit_distributed(model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\r\n    117         steps_per_epoch=steps_per_epoch,\r\n    118         validation_steps=validation_steps,\r\n--> 119         validation_freq=validation_freq)\r\n    120   else:\r\n    121     return training_arrays.fit_loop(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py in experimental_tpu_fit_loop(model, dataset, epochs, verbose, callbacks, initial_epoch, steps_per_epoch, val_dataset, validation_steps, validation_freq)\r\n    310   # TODO(fchollet): add support for `steps_per_epoch=None` in TPU loops.\r\n    311   current_strategy = model._distribution_strategy\r\n--> 312   iterator = distributed_training_utils.get_iterator(dataset, current_strategy)\r\n    313   steps_per_epoch = training_utils.infer_steps_for_dataset(\r\n    314       dataset, steps_per_epoch, epochs, steps_name='steps_per_epoch')\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/distributed_training_utils.py in get_iterator(dataset, distribution_strategy)\r\n    519   with distribution_strategy.scope():\r\n    520     iterator = distribution_strategy.make_dataset_iterator(dataset)\r\n--> 521   initialize_iterator(iterator, distribution_strategy)\r\n    522   return iterator\r\n    523 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/distributed_training_utils.py in initialize_iterator(iterator, distribution_strategy)\r\n    527     init_op = control_flow_ops.group(iterator.initialize())\r\n    528     if not context.executing_eagerly():\r\n--> 529       K.get_session((init_op,)).run(init_op)\r\n    530 \r\n    531 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    930     try:\r\n    931       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 932                          run_metadata_ptr)\r\n    933       if run_metadata:\r\n    934         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1153     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1154       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1155                              feed_dict_tensor, options, run_metadata)\r\n   1156     else:\r\n   1157       results = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1329     if handle is None:\r\n   1330       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1331                            run_metadata)\r\n   1332     else:\r\n   1333       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1349           pass\r\n   1350       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1351       raise type(e)(node_def, op, message)\r\n   1352 \r\n   1353   def _extend_graph(self):\r\n\r\nNotFoundError: Op type not registered 'ExperimentalRebatchDataset' in binary running on n-23c14aca-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n```\r\n", "comments": ["Hi @gadagashwini,\r\n\r\nPublic releases of Cloud TPUs (for TF 1.11, 1.12, and 1.13) does not currently support TensorFlow 2.0-alpha releases, as there are some op differences. That would change when a final release of TensorFlow 2 happens, but for now the use case is not supported.\r\n\r\nThat said, you may want to try this again with a TensorFlow 2.0 nightly and a Cloud TPU 1.14 release when it is released around May.\r\n\r\nThanks,\r\n\r\nFrank", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27339\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27339\">No</a>\n"]}, {"number": 27338, "title": "Sequence generator as validation_data not working if Flatten() involved in model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: GTX1060 6GB\r\n\r\n**Describe the current behavior**\r\n\r\nSequence generator as validation_data not working if Flatten() layer involved in model\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect it to work as expected\r\n\r\n**Code to reproduce the issue**\r\n\r\nDefine basic stuffs\r\n```python3\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.keras as tfk\r\nSequence = tfk.utils.Sequence\r\n\r\nDense = tfk.layers.Dense\r\nInput = tfk.layers.Input\r\nFlatten = tfk.layers.Flatten\r\nModel = tfk.models.Model\r\n\r\n\r\nclass CustomGenerator(Sequence):\r\n    def __init__(self, batch_size, shuffle, steps_per_epoch, data):\r\n        self.inputs = data[0]\r\n        self.labels = data[1]\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.steps_per_epoch = steps_per_epoch\r\n\r\n        # initial idx\r\n        self.idx_list = self._get_exploration_order(range(self.inputs.shape[0]))\r\n        self.current_idx = 0\r\n\r\n    def __len__(self):\r\n        return self.steps_per_epoch\r\n\r\n    def _get_exploration_order(self, idx_list):\r\n        \"\"\"\r\n        :param idx_list:\r\n        :return:\r\n        \"\"\"\r\n        # shuffle (if applicable) and find exploration order\r\n        if self.shuffle is True:\r\n            idx_list = np.copy(idx_list)\r\n            np.random.shuffle(idx_list)\r\n\r\n        return idx_list\r\n\r\n    def _data_generation(self, inputs, labels, idx_list_temp):\r\n        x = inputs[idx_list_temp]\r\n        y = labels[idx_list_temp]\r\n        return x, y\r\n\r\n    def __getitem__(self, index):\r\n        x, y = self._data_generation(self.inputs,\r\n                                     self.labels,\r\n                                     self.idx_list[self.current_idx:self.current_idx + self.batch_size])\r\n        self.current_idx += self.batch_size\r\n        return x, y\r\n\r\n    def on_epoch_end(self):\r\n        # shuffle the list when epoch ends for the next epoch\r\n        self.idx_list = self._get_exploration_order(range(self.inputs.shape[0]))\r\n        # reset counter\r\n        self.current_idx = 0\r\n\r\n# Model 1 which does not have Flatten\r\ninput_tensor = Input(shape=[200], name='input')\r\noutput_tensor = Dense(units=10, name='output')(input_tensor)\r\nneuralnet = Model(inputs=input_tensor, outputs=output_tensor)\r\nneuralnet.compile(loss='mse', optimizer='adam')\r\n\r\n# Model 2 which has Flatten\r\ninput_tensor = Input(shape=[200, 1], name='input')\r\nflat = Flatten()(input_tensor)\r\noutput_tensor = Dense(units=10, name='output')(flat)\r\nneuralnet_flat = Model(inputs=input_tensor, outputs=output_tensor)\r\nneuralnet_flat.compile(loss='mse', optimizer='adam')\r\n```\r\n\r\nThis works as expected\r\n```python3\r\npredgen = CustomGenerator(batch_size=64, shuffle=True, steps_per_epoch=10, data=[np.random.normal(size=(700, 200, 1)), np.random.normal(size=(700, 10))])\r\n\r\nneuralnet_flat.fit_generator(generator=predgen, epochs=5)\r\n```\r\n\r\nThis also works\r\n```python3\r\npredgen = CustomGenerator(batch_size=64, shuffle=True, steps_per_epoch=10, data=[np.random.normal(size=(700, 200)), np.random.normal(size=(700, 10))])\r\nvalgen = CustomGenerator(batch_size=64, shuffle=True, steps_per_epoch=1, data=[np.random.normal(size=(64, 200)), np.random.normal(size=(64, 10))])\r\n\r\nneuralnet.fit_generator(generator=predgen, validation_data=valgen, epochs=5)\r\n```\r\n\r\nBut this is not working\r\n```python3\r\npredgen = CustomGenerator(batch_size=64, shuffle=True, steps_per_epoch=10, data=[np.random.normal(size=(700, 200, 1)), np.random.normal(size=(700, 10))])\r\nvalgen = CustomGenerator(batch_size=64, shuffle=True, steps_per_epoch=1, data=[np.random.normal(size=(64, 200, 1)), np.random.normal(size=(64, 10))])\r\n\r\n# does not work\r\nneuralnet_flat.fit_generator(generator=predgen, validation_data=valgen, epochs=5)\r\n```\r\n\r\n\r\n**Other info / logs**\r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-2-ced9d06428c4> in <module>\r\n      3\r\n      4 # does not work\r\n----> 5 neuralnet_flat.fit_generator(generator=predgen, validation_data=valgen, epochs=5)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1424         use_multiprocessing=use_multiprocessing,\r\n   1425         shuffle=shuffle,\r\n-> 1426         initial_epoch=initial_epoch)\r\n   1427\r\n   1428   def evaluate_generator(self,\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\r\n    223           use_multiprocessing=use_multiprocessing,\r\n    224           max_queue_size=max_queue_size,\r\n--> 225           mode='test')\r\n    226\r\n    227       if not isinstance(val_results, list):\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\r\n    189       progbar.on_batch_begin(step, batch_logs)\r\n    190\r\n--> 191       batch_outs = batch_function(*batch_data)\r\n    192       if not isinstance(batch_outs, list):\r\n    193         batch_outs = [batch_outs]\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in test_on_batch(self, x, y, sample_weight, reset_metrics)\r\n   1254       else:\r\n   1255         self._make_eval_function()\r\n-> 1256         outputs = self._eval_function(inputs)  # pylint: disable=not-callable\r\n   1257\r\n   1258     if reset_metrics:\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py in __call__(self, inputs)\r\n   3074\r\n   3075     fetched = self._callable_fn(*array_vals,\r\n-> 3076                                 run_metadata=self.run_metadata)\r\n   3077     self._call_fetch_callbacks(fetched[-len(self._fetches):])\r\n   3078     return nest.pack_sequence_as(self._outputs_structure,\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in __call__(self, *args, **kwargs)\r\n   1437           ret = tf_session.TF_SessionRunCallable(\r\n   1438               self._session._session, self._handle, args, status,\r\n-> 1439               run_metadata_ptr)\r\n   1440         if run_metadata:\r\n   1441           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\r\n    526             None, None,\r\n    527             compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 528             c_api.TF_GetCode(self.status.status))\r\n    529     # Delete the underlying status object from memory otherwise it stays alive\r\n    530     # as there is a reference to status from this from the traceback due to\r\n\r\nInvalidArgumentError: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\r\n         [[{{node flatten/Reshape}}]]\r\n         [[{{node ConstantFoldingCtrl/loss_1/output_loss/MeanSquaredError/weighted_loss/broadcast_weights/assert_broadcastable/AssertGuard/Switch_0}}]]\r\n```", "comments": ["I found out the cause of error. Turns out on_epoch_end() is not reached in validation generation so it yield empty data. Nothing is wrong on Tensorflow side.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27338\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27338\">No</a>\n", "> I found out the cause of error. Turns out on_epoch_end() is not reached in validation generation so it yield empty data. Nothing is wrong on Tensorflow side.\r\n\r\nHi @henrysky, I'm facing a similar issue that validation's on_epoch_end is not reached, do you recall why and how to fix that? Thanks!"]}, {"number": 27337, "title": "[Intel MKL] Modified mkl concat op to also use eigen when input dimensions is 2.", "body": "", "comments": ["Closing"]}, {"number": 27336, "title": "Add examples of loading model that has been saved with save_keras_model", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.13\r\n- Doc Link: https://www.tensorflow.org/tutorials/images/hub_with_keras\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nThe above example provides no example of loading the model after it's saved. I'm used to using the \"old style\" of freezing, optimizing (`optimize_for_inference_lib.optimize_for_inference`), and saving models with:\r\n`tf.train.write_graph(output_graph_def, output_dir, 'model.pb', as_text=False)`\r\n\r\nWhich gives me a graph: `model.pb`. I run predictions on this graph locally on my mobile applications with full Tensorflow (in Unity, so not using TF-Lite).\r\n\r\nIn my local prediction script, I load the graph like this:\r\n```python\r\ndef load_graph(model_file):\r\n  graph = tf.Graph()\r\n  graph_def = tf.GraphDef()\r\n\r\n  with open(model_file, \"rb\") as f:\r\n    graph_def.ParseFromString(f.read())\r\n  with graph.as_default():\r\n    tf.import_graph_def(graph_def)\r\n  return graph\r\n```\r\nThen grab the graph input and output ops and run a prediction session. This works pretty well.\r\n\r\nHowever, this new example uses `tf.contrib.saved_model.save_keras_model` to export the model, which I would love to use because it exports all kinds of useful information (which I'd like to use in conjunction with TF Serving for certain applications), and it looks to be making it into TF 2.0 core.\r\n\r\nWith `save_keras_model`, I end up with:\r\n`assets  labels.csv  saved_model.pb  variables`\r\n\r\nWhen trying to use my old graph loading script with this newly generated model with `save_keras_model`, I get an error like this:\r\n```bash\r\n...\r\n    graph_def.ParseFromString(f.read())\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n```\r\nThis had me puzzled, as I expected this `.pb` graph generated with the new API to work the same as the one I generated with `tf.train.write_graph`. Browsing through their contents, they look the same, but the newer API's model is smaller in file size. Which makes me wonder if it's missing something.\r\n\r\nI found an alternative method to load it as a graph anyways, but I ran into issues down the road with running it's ops, so I'm not sure I'm approaching this correctly by loading it as a graph, or if there's a \"new, Keras way\" of loading a model and running predictions. Maybe I'm too far in the weeds with trying to load this model as a graph and run it's ops, I'd expect there's a higher level API to do this.\r\n\r\n\r\nSo I have a few questions:\r\n- Is the exported `saved_model.pb` frozen?\r\n- Is it optimized? If not, I can convert the model I have to a graph, then back to a model for something like `save_keras_model`?\r\n\r\n**Ultimately**, can an example be added to this tutorial that demonstrates how to load a saved model?\r\n\r\n\r\nI wrote our initial codebase using TF 1.7 a little while back and I haven't changed much since then, but the API is changing a lot and I want to take full advantage of the advancements. Also I understand if this issue should be moved, I assumed this was appropriate since there was no documentation demonstrating loading a saved model with the new API.\r\n\r\nThank you! \r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nYes\r\n", "comments": ["you want an example describing the loading saved models using save_keras_model to be added in its description.", "Thanks, I've added that to the footer of my issue and amended my title.\r\n\r\nI just looked at the Codelab in the example, and it looks to use: [load_from_saved_model](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model) which is listed as experimental, and I can't seem to access any of it's methods.", "@lkuich This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "@lkuich Could you please take a look at this [tutorial](https://www.tensorflow.org/tutorials/keras/save_and_restore_models) ? Also, check under tutorials tab, there several tutorials that might be helpful. Please let me know what you think. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 27335, "title": "tf.compat.path_to_str  #25826", "body": "Added some usage examples #25826", "comments": ["@rthadur  please assign reviewers please", "please somebody review", "Please @xiejw  review this pr", "Please somebody review this PR @rthadur , @tomhennigan , @mihaimaruseac ", "@mihaimaruseac  I have done the changes requested by you, now can you please review it\r\n", "@rthadur , @mihaimaruseac please somebody review it.\r\n", "Please @mihaimaruseac  review it now.", "@mihaimaruseac I am not able to remove that white trailing space, I tried for 10 times atleast how to remove that there is nothing to be removed , please help. ", "In your editor, you go to that line and press delete/backspace a few times.\r\n\r\nMost editors can be configured to remove trailing whitespace on save. What editor are you using?", "I am using vim editor", "Done with changes @mihaimaruseac , now please approve it and merge it.", "@mihaimaruseac,@rthadur why there is ubuntu sanity check , please somebody guide me.", "Please, @mihaimaruseac, @rthadur help me with this ubuntu sanity check, I'm not getting the reason for the error.", "Click on details then on log\r\n\r\n```\r\n1. do_pylint PYTHON2: Python 2 pylint\r\n  FAIL\r\n2. do_pylint PYTHON3: Python 3 pylint\r\n  FAIL\r\n```\r\n\r\nBasically you're not following PEP8 style guide so code cannot be merged. Please fix.", "What's wrong in my style I am not able to understand.", "I have tried to change a little bit can you please tell me what can I do further.", "@mihaimaruseac  please review it once and tell me further changes.", "You didn't fix the error, instead changed irrelevant lines.\r\n\r\nIf you look at the [error log](https://source.cloud.google.com/results/invocations/7a6a3039-f030-40c3-a3d0-20605ad514e2/log) it says right there\r\n\r\n```\r\nFAIL: Found 1 non-whitelited pylint errors:\r\ntensorflow/python/util/compat.py:123: [C0301(line-too-long), ] Line too long (89/80)\r\n```\r\n\r\nYou can run the linter locally too, see https://www.pylint.org/ (and https://pypi.org/project/pep8/ for the standard)", "@mihaimaruseac  I have reduced the lengths of all the lines(<80) according to pylint norms, so now can you please review it.", "@mihaimaruseac I have done the changes requested by you now you can review it.", "@rthadur please apply kokoro force run", "Please @rthadur apply all checks", "It's weekend, people are not going to work on this until Monday. Please wait.", "The Windows Bazel GPU check: don't worry about it, it's not your PR's fault. But until Monday/Tuesday there likely won't be a fix to it.", "Please somebody check the internal CI build please.", "@mihaimaruseac please now its more than 2 days please assign someone to check import/copybara", "It's Monday morning, people have not yet come to the office. When @rthadur comes in he will apply the copybara checks.\r\n\r\nThere is one issue though, which you should fix before: since you include commits changing other files, copybara sees a change in `api_def_Bitcast.pbtxt` which shouldn't be (but it is included because of d3c1f4a).\r\n\r\nYou could try rebasing/squashing these commits to only have one single commit doing the exact change this PR does. Not fixing this will make merging the PR slower. See https://github.com/tensorflow/tensorflow/pull/27637#issuecomment-480879996", "@mihaimaruseac why merging is still blocked when all required checks are done?", "Because `\ufffcWindows Bazel GPU \u2014 Internal CI build failed`", "@mihaimaruseac  what can I do for Windows Bazel GPU test.", "Just wait: https://github.com/tensorflow/tensorflow/pull/27335#issuecomment-480556443", "@rthadur please do something about windows bazel GPU test.", "That test fails in several PRs, just wait for it to be fixed and then the PR will be merged automatically. Unless mentioned again, you don't have anything else to do for this PR (except maybe clean it up, squash/remove old commits)", "@mihaimaruseac what happened to it now 4 tests are failing why?", "Other tests failed. They were run because your PR touched other files so they were marked as required to run before merging. Please try to have clean PRs in the future.\r\n\r\nYou don't have to do anything here though, this will get merged in a few minutes", "See 66d7a531f as the commit merging this.", "@mihaimaruseac sure, I will try to submit PRs with no unwanted commits."]}, {"number": 27334, "title": "Lite: Util new test cases added to improve coverage", "body": "6 new test cases added", "comments": ["@achowdhery : Gentle Reminder!", "Can one of the admins verify this patch?", "Thank you @petewarden for your help in merging this PR. @ANSHUMAN87 sorry for the delay and thank you for patience."]}, {"number": 27333, "title": "Updated subgraph.cc", "body": "", "comments": []}, {"number": 27332, "title": "Comment updated", "body": "", "comments": ["Closing to merge with  #27331"]}, {"number": 27331, "title": "Comments updated", "body": "", "comments": ["@ANSHUMAN87 thank you for your contribution , is it possible to merge this PR #27332 in this ", "> @ANSHUMAN87 thank you for your contribution , is it possible to merge this PR #27332 in this\r\n\r\nSure will do that.", "> > @ANSHUMAN87 thank you for your contribution , is it possible to merge this PR #27332 in this\r\n> \r\n> Sure will do that.\r\n\r\n@rthadur : Its merged together now, please check, Thanks!"]}, {"number": 27330, "title": "[Features] Support gradients reduction in tf.gradients according to immediate_grads_reduction in CollectiveAllReduceStrategy and MirroredStrategy", "body": "Currently, gradients_reduction will be called in `apply_gradients` when using `DistributionStrategy`. It will block computation and communication overlapping with `clip_by_global_norm`. In this pull request, the `immediate_grads_reduction` has been add to `MirroredStrategy` and `CollectiveAllReduceStrategy`. When it is True, the gradients reduction will be called in `tf.gradients` so that the clipping operation will be executed after communication. Otherwise, it preserves the original cases. \r\n\r\n`immedeiate_grads_reduction` has been only enabled in graph mode. We will also add support in eager mode in the future.\r\n\r\nThis PR will change some API. Could you please take a look at this PR?  This performance feature has been widely used in Alibaba. Any comments are welcome.\r\n\r\n@yuefengz ", "comments": ["Thank siyu for the nice work.\r\n\r\n@yuefengz \r\n\r\nCould you please kindly take a look at this PR?\r\n\r\nRegards", "It sounds like Yuefeng's review is requested.", "@wangsiyu can you please resolve conflicts.", "> @wangsiyu can you please resolve conflicts.\r\nOK. I will solve it and come back soon.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27329, "title": "Unable to convert frozen graph to usable model on iPhone", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 2.7.15rc1\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GeForce GTX 1080\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n\r\nI am fine-tuning the [pretrained SSD-MobileNetV1](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz ) model using the `/models/research/object_detection/samples` [config file](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config) for detecting the bounding boxes of objects in an image. \r\n\r\n\r\n```\r\npython model_main.py --logtostderr --model_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config\r\n```\r\n\r\n\r\n\r\nAfter training, I am freezing the model using \r\n\r\n```\r\npython export_inference_graph.py --input_type=image_tensor --pipeline_config_path=training/ssd_mobilenet_v1_pets.config --output_directory=inference_graph --trained_checkpoint_prefix=training/model.ckpt-3740 \r\n```\r\n\r\nAfter the `frozen_inference_graph.pb`  is being generated, I'd like to convert the model to a model format that is compatible with iPhones. I tried the following approaches: \r\n\r\n### Approach 1 - Convert to mlmodel\r\n\r\n```\r\n$ python convert_to_mlmodel.py \r\nWARNING:root:TensorFlow version 1.13.1 detected. Last version known to be fully compatible is 1.12.0 .\r\n\r\nLoading the TF graph...\r\n2019-03-30 18:39:48.956255: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-03-30 18:39:49.156054: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5592d6f7e880 executing computations on platform CUDA. Devices:\r\n2019-03-30 18:39:49.156083: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2019-03-30 18:39:49.177932: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3398155000 Hz\r\n2019-03-30 18:39:49.178509: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5592d6fdf420 executing computations on platform Host. Devices:\r\n2019-03-30 18:39:49.178548: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-03-30 18:39:49.178847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 211.44MiB\r\n2019-03-30 18:39:49.178889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-03-30 18:39:49.179849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-30 18:39:49.179872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-03-30 18:39:49.179882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-03-30 18:39:49.180081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 211 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nGraph Loaded.\r\nTraceback (most recent call last):\r\n  File \"convert_to_mlmodel.py\", line 4, in <module>\r\n    output_feature_names = ['softmax:0'])\r\n  File \"/home/deepvision/.local/lib/python2.7/site-packages/tfcoreml/_tf_coreml_converter.py\", line 586, in convert\r\n    custom_conversion_functions=custom_conversion_functions)\r\n  File \"/home/deepvision/.local/lib/python2.7/site-packages/tfcoreml/_tf_coreml_converter.py\", line 167, in _convert_pb_to_mlmodel\r\n    OPS = _topological_sort_ops(OPS)\r\n  File \"/home/deepvision/.local/lib/python2.7/site-packages/tfcoreml/_tf_graph_transform.py\", line 194, in _topological_sort_ops\r\n    _push_stack(stack, node, in_stack)\r\n  File \"/home/deepvision/.local/lib/python2.7/site-packages/tfcoreml/_tf_graph_transform.py\", line 38, in _push_stack\r\n    raise ValueError('Graph has cycles.')\r\nValueError: Graph has cycles.\r\n```\r\n\r\nThe conversion code looks like:\r\n\r\n```\r\nimport tfcoreml as tf_converter\r\ntf_converter.convert(tf_model_path = 'frozen_inference_graph.pb',\r\n                     mlmodel_path = 'my_model.mlmodel',\r\n                     output_feature_names = ['softmax:0'])\r\n```\r\n\r\nTried the above with Faster RCNN ResNet101 and SSD MobileNet - both give the same error.\r\n\r\nNot sure why the `output_feature_names` has to be a `softmax` as its a detection problem.\r\n\r\n\r\n\r\n### Approach 2 - Convert to tflite \r\n\r\n```\r\n$ tflite_convert --output_file=output.tflite --saved_model_dir=/home/deepvision/code/models/research/object_detection/inference_graph/saved_model/\r\n2019-03-30 19:22:30.354332: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-03-30 19:22:30.757830: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5592ab466420 executing computations on platform CUDA. Devices:\r\n2019-03-30 19:22:30.757861: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2019-03-30 19:22:30.785890: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3398155000 Hz\r\n2019-03-30 19:22:30.791145: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5592ab498d20 executing computations on platform Host. Devices:\r\n2019-03-30 19:22:30.791161: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-03-30 19:22:30.791279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 69.38MiB\r\n2019-03-30 19:22:30.791294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-03-30 19:22:30.791745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-30 19:22:30.791755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-03-30 19:22:30.791762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-03-30 19:22:30.791844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 69 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nWARNING:tensorflow:From /home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert_saved_model.py:61: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\n2019-03-30 19:22:31.298973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-03-30 19:22:31.299020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-30 19:22:31.299032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-03-30 19:22:31.299040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-03-30 19:22:31.299186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 69 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nWARNING:tensorflow:From /home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert_saved_model.py:275: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.convert_variables_to_constants\r\nWARNING:tensorflow:From /home/deepvision/.local/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.extract_sub_graph\r\nTraceback (most recent call last):\r\n  File \"/home/deepvision/.local/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 442, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/deepvision/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 438, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 191, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/lite.py\", line 411, in convert\r\n    \"invalid shape '{1}'.\".format(_tensor_name(tensor), shape_list))\r\nValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'.\r\n```\r\n\r\nAlso, tried the following:\r\n\r\n` tflite_convert --output_file=output.tflite --graph_def_file=frozen_inference_graph.pb --input_arrays=input --output_arrays=`\r\n\r\nBut, I am not sure what the last layer is, as the model is fairly convoluted. Tried visualizing with Netron but \r\n\r\n**Describe the expected behavior**\r\nSeamless conversion to tflite and mlmodel files.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nNA\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nNA\r\n", "comments": ["Hello. We've been stuck with this issue for 5 days now. Can you please provide an update on this?\r\ncc: @gadagashwini @ymodak ", "When you use the tflite converter to convert from saved_model, the input_arrays/output_arrays are derived from the SignatureDef. [1] And input_shape is automatically determined, but in this case it seems the 2nd, 3rd dimension is 'None'. I think TF Lite currently only support 'None' shape for the batch_size dimension.\r\n\r\nCould you try using tflite converter to convert from the frozen graph (your third approach) and also supply the input_shape as well? The output_arrays are needed, which you could check from Netron, or from the SignatureDef of the SavedModel.\r\n\r\nLet me know if you need any help further.\r\n\r\n[1]:https://www.tensorflow.org/lite/convert/cmdline_examples#convert_a_tensorflow_savedmodel_", "[1]https://www.tensorflow.org/lite/convert/cmdline_examples#convert_a_tensorflow_savedmodel_", "Solution: There is an `export_tflite_ssd_graph.py` file provided in the repository which needs to be used in place of the previous script `export_inference_graph`, which allows conversion to `tflite` format. \r\n\r\n```\r\npython export_tflite_ssd_graph.py --pipeline_config_path=training/pipeline.config --trained_checkpoint_prefix=training/model.ckpt-20000  --output_directory=tflite_model --add_postprocessing_op=true\r\n```\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27329\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27329\">No</a>\n"]}, {"number": 27327, "title": "Added support for `TensorStridedSliceUpdate`", "body": "This adds support for using the `StridedSliceAssign` op on tensors, similar to `TensorScatterUpdate`. To remain consistent, the new op is named `TensorStridedSliceUpdate`. We need this to add support for efficient tensor strided slice setters in Swift for TensorFlow. The relevant PR and comments can be found [here](https://github.com/apple/swift/pull/23684).\r\n\r\n@alextp does this look ok? I cannot currently compile and test on my machine, but I hope I'll be able to do so tomorrow.", "comments": ["Thanks Alex! I made some fixes and added support for the op in the public python API (through the strided slice assignment operator).", "@alextp there was a typo and a test that does not apply anymore that should be fixed now.", "(for tf-api-owners) This should not be visible in the Python API except as a raw_op. Can you remove the exposure via Python and re-run the API compatibility test to regenerate the goldens?", "@karmel Do I need to do something about this or is it just the tf-api-owners?", "@eaplatanios I think you need to rerun the api_compatibility_test with --update_goldens=True", "@alextp @karmel I rerun the API compatibility test and committed the updated files.", "Can we run the tests again to verify there are no remaining issues?", "```\r\n=== Sanity check step 4 of 13: do_buildifier (buildifier check) ===\r\nRunning do_buildifier on 440 files\r\ntensorflow/core/kernels/BUILD # listsort unsafesort sort:tf_kernel_library.deps\r\nbuildifier took 0 s\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\n151a152\r\n>         \":inplace_ops\",\r\n154d154\r\n<         \":inplace_ops\",\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n\r\n```", "@alextp If this was due to the alphabetical ordering, it should have been fixed now. :)", "All tests seem to pass now. Not sure what's wrong with the \"android\" and the \"feedback\" checks.", "@alextp Is this ready to be merged now or should I do something further for the failing checks?", "@eaplatanios it should be good to go, I'll retry internally", "@alextp Sounds good, thanks! :)", "This should be ready to merge now I guess.", "When is this expected to be merged? I'm asking because it's currently blocking some features I'm working on for Swift for TensorFlow.", "> When is this expected to be merged? I'm asking because it's currently blocking some features I'm working on for Swift for TensorFlow.\r\n\r\nThanks for your contribution, this should be merged on Monday.", "Thanks @rthadur! :)"]}, {"number": 27326, "title": "DLL load failure", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6.4\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA v9.0\r\n- GPU model and memory: GTX 1060 6gb\r\n\r\n\r\n\r\n**Describe the problem**\r\nBasically trying to import tensor results in an \"ImportError: DLL load failure.\" I already have cudart64_90.dll installed to its destination along with other necessary files. A complete re-install of TensorFlow does not fix this problem. Checked the CUDA paths only to find no issues.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`import tensorflow`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: The specified module could not be found.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:/PycharmProjects/TensPlease/CanITens.py\", line 7, in <module>\r\n>     import tensorflow as tf\r\n>   File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n>   File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: The specified module could not be found.\r\n\r\n", "comments": ["Fixed the issue.\r\nDowngraded to Tensorflow 1.5, for some reason it works, and restarted the system."]}, {"number": 27325, "title": "TFLite delegates/gpu/libmetal_delegate.a is missing", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave (10.14.1)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Not installed.\r\n- TensorFlow version: master branch (latest commit 6353d940289a225cfbc104cc647b3c6970077faa)\r\n- Python version: 3.7.2\r\n- Installed using virtualenv? pip? conda?: Not installed, just calling shell scripts from repo\r\n- Bazel version (if compiling from source): 0.18.0\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\nAttempting to build iOS Metal TFLite Delegate with `create_ios_frameworks.sh` yields the following error:\r\n\r\n```\r\n$ tensorflow/lite/lib_package/create_ios_frameworks.sh -g\r\nStarting\r\nFile /path/to/tensorflow/tensorflow/lite/lib_package/../delegates/gpu/libmetal_delegate.a doesn't exist.\r\nIt's requried for building TFLite Framework with GPU. Aborting.\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nThis is how I am attempting to create the iOS framework:\r\n```\r\n$ tensorflow/lite/tools/make/download_dependencies.sh\r\n$ tensorflow/lite/tools/make/build_ios_universal_lib.sh\r\n$ tensorflow/lite/lib_package/create_ios_frameworks.sh -g\r\n```\r\n\r\nI saw that the `-g` flag got added to `create_ios_frameworks.sh` in 59d535a0df17eaf3033bbff73ef4e1e1988c454e. Without the flag, I am able to successfully build the framework but, as expected, the GPU is not utilized.\r\n\r\nI know the GPU delegates only got open-sourced a couple of days ago, and before that I had unsurprisingly been getting the same error but with `metal_delegate.h`, which was added in fb772b781b011471dec443e1f3cd6b664958b767. Is `libmetal_delegate.a` supposed to be present or is it still pending open-sourcing?\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nN/A", "comments": ["With one line modification to\r\n```diff\r\ndiff --git a/tensorflow/lite/delegates/gpu/metal_delegate.mm b/tensorflow/lite/delegates/gpu/metal_delegate.mm\r\nindex 66bfb043e5..b71881b73f 100644\r\n--- a/tensorflow/lite/delegates/gpu/metal_delegate.mm\r\n+++ b/tensorflow/lite/delegates/gpu/metal_delegate.mm\r\n@@ -23,7 +23,7 @@\r\n #include <string>\r\n #include <vector>\r\n \r\n-#include \"third_party/absl/types/span.h\"\r\n+#include \"absl/types/span.h\"\r\n #include \"tensorflow/lite/builtin_ops.h\"\r\n #include \"tensorflow/lite/c/c_api_internal.h\"\r\n #include \"tensorflow/lite/delegates/gpu/common/convert.h\"\r\n```\r\n\r\nWith something like\r\n```bash\r\nbazel build --apple_platform_type ios --ios_multi_cpus=arm64 \\\r\ntensorflow/lite/delegates/gpu:metal_delegate\r\n```\r\nyou can get arm64 `bazel-tensorflow/bazel-out/apl-ios_arm64-opt/bin/tensorflow/lite/delegates/gpu/libmetal_delegate.a`.", "@freedomtan thank you, your solution did work. After modifying `metal_delegate.mm` I was able to build `libmetal_delegate.a` which allowed `create_ios_frameworks.sh` to run with the `-g` flag.\r\n\r\nThis issue should probably remain open until this workaround is no longer necessary.", "I sent a PR just now :-)", "@fredbertsch \r\n![image](https://user-images.githubusercontent.com/17869361/59912414-8e737c00-9448-11e9-8e6a-04ecb53303f2.png)\r\n\r\n![image](https://user-images.githubusercontent.com/17869361/59912439-97fce400-9448-11e9-9acf-c8d408694f3b.png)\r\n\r\nwhat was wrong ?\r\n", "@weinixuehao How did you build your `libmeta_delegate.a`? iPhone emulator is x86_64 based, make sure you have x86_64 stuff in it.", "@freedomtan \r\nI built with below commands\r\n![image](https://user-images.githubusercontent.com/17869361/60001804-ddfcb680-9699-11e9-97e9-2285c27e62de.png)\r\narm64 and x86_64 is ok but i need armv7 armv7s as my xcode project valid architecture is \r\n![image](https://user-images.githubusercontent.com/17869361/60001879-0edceb80-969a-11e9-9f8b-233b304b18c0.png)\r\n", "interesting, why do you need 32-bit binaries? iPhone 6s has 64-bit CPUs. I think you can simply remove armv7 and armv7s. Anyway, something like \r\n```\r\nbuild --apple_platform_type ios --ios_multi_cpus=armv7 \r\n//tensorflow/lite/delegates/gpu:metal_delegate \r\n```\r\nshould work. And as far as I can remember bazel doesn't support armv7s.", "@freedomtan\r\nI am newbie to ios and tensorflowlite binary like \r\n![image](https://user-images.githubusercontent.com/17869361/60025814-ffc36100-96cc-11e9-8b59-b23825a1f983.png)\r\nSo i am worried the following two problems.\r\n1\u3001if metal_delegate binary only arm64 will cause compilation problems? \r\n2\u3001iphone5 has 32-bit CPUS will cause crash problems?\r\n\r\nWhy tensorflowlite has armv7, armv7s, arm64 ... but metal_delegate only has arm64 is OK? \r\nI`m very puzzled!\r\n", "I don't think Metal (the API) supports iOS devices older than A7 (iPhone 5s). That is, there is no reason to worry about iPhone 5 (A6).", "@freedomtan\r\nThanks! \r\nPlease help me debug this issue If you have enough time.\r\nhttps://github.com/tensorflow/tensorflow/issues/29864", "@freedomtan \r\nIt need too many dependency libs after reference metal lib otherwise throw undefined symbol ...\r\nWhy bazel do not merge to one lib after built successfully? Maybe bazel has this option but i do not know\r\n![image](https://user-images.githubusercontent.com/17869361/60067704-4dc47d00-973e-11e9-9b5b-910ca61a1492.png)\r\n", "@weinixuehao instead of building the bazel target `metal_delegate`, build `metal_delegate_fully_linked.a` \u2014\u00a0this gives you a single archive file with all dependencies included. https://docs.bazel.build/versions/master/be/objective-c.html#objc_library_implicit_outputs", "> With one line modification to\r\n> \r\n> ```diff\r\n> diff --git a/tensorflow/lite/delegates/gpu/metal_delegate.mm b/tensorflow/lite/delegates/gpu/metal_delegate.mm\r\n> index 66bfb043e5..b71881b73f 100644\r\n> --- a/tensorflow/lite/delegates/gpu/metal_delegate.mm\r\n> +++ b/tensorflow/lite/delegates/gpu/metal_delegate.mm\r\n> @@ -23,7 +23,7 @@\r\n>  #include <string>\r\n>  #include <vector>\r\n>  \r\n> -#include \"third_party/absl/types/span.h\"\r\n> +#include \"absl/types/span.h\"\r\n>  #include \"tensorflow/lite/builtin_ops.h\"\r\n>  #include \"tensorflow/lite/c/c_api_internal.h\"\r\n>  #include \"tensorflow/lite/delegates/gpu/common/convert.h\"\r\n> ```\r\n> \r\n> With something like\r\n> \r\n> ```shell\r\n> bazel build --apple_platform_type ios --ios_multi_cpus=arm64 \\\r\n> tensorflow/lite/delegates/gpu:metal_delegate\r\n> ```\r\n> \r\n> you can get arm64 `bazel-tensorflow/bazel-out/apl-ios_arm64-opt/bin/tensorflow/lite/delegates/gpu/libmetal_delegate.a`.\r\n\r\nI run the command below : \r\n\r\n```\r\nbazel build --apple_platform_type ios --ios_multi_cpus=arm64 \\\r\ntensorflow/lite/delegates/gpu:metal_delegate\r\n```\r\nbut there is an error like this :\r\n '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'ios_arm64'\r\n\r\nCould you tell me how to fix it ? thx a lot", "> Could you tell me how to fix it ? thx a lot\r\n\r\nLooks like this is a question for @freedomtan ?\r\n\r\n> @freedomtan thank you, your solution did work. After modifying `metal_delegate.mm` I was able to build `libmetal_delegate.a` which allowed `create_ios_frameworks.sh` to run with the `-g` flag.\r\n> \r\n> This issue should probably remain open until this workaround is no longer necessary.\r\n\r\nThe workaround is [no longer necessary](https://github.com/tensorflow/tensorflow/blob/a592eff24bdae523970ec636d38d8c6b221de40a/tensorflow/lite/delegates/gpu/metal_delegate.mm#L29) so I'm closing the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27325\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27325\">No</a>\n", "@sanjoy oops, didn't notice that. \r\n\r\n@thameslu in case didn't find the solution yet, I think you probably need to \"run the `./configure` script in the root TensorFlow checkout directory, and answer \"Yes\" when the script asks if you wish to build TensorFlow with iOS support\", as described in the [build TF Lite for iOS](https://www.tensorflow.org/lite/guide/build_ios#configure_workspace_and_bazelrc) guide", "> ensorFlow with iOS support\", as described in the [build TF Lite for iOS](https://www.tensorflow.org/lite/guide/build_ios#configure_workspace_and_bazelrc) guide\r\n\r\nthank you, i will have a try @sanjoy  @freedomtan ", "@thameslu \uff0chi\uff0cHow did you solve the problem? when i use metal delegate on ios i have the similar problem as follow,\r\n\r\n`_OBJC_CLASS_$_TFLInferenceContext\", referenced from:\r\n      objc-class-ref in libmetal_delegate.a(metal_delegate_bf7d4aad90502bdf5c67c8a69622de9d.o)\r\n  \"tflite::gpu::BuildModel(TfLiteContext*, TfLiteDelegateParams const*, tflite::gpu::Model<tflite::gpu::TensorRef<tflite::gpu::StrongShape<(tflite::gpu::Layout)10> > >*)\", referenced from:\r\n      tflite::gpu::metal::(anonymous namespace)::DelegatePrepare(TfLiteContext*, TfLiteDelegate*)::$_1::__invoke(TfLiteContext*, char const*, unsigned long) in libmetal_delegate.a(metal_delegate_bf7d4aad90502bdf5c67c8a69622de9d.o)\r\n  \"_OBJC_CLASS_$_TFLBufferConvert\", referenced from:\r\n      objc-class-ref in libmetal_delegate.a(metal_delegate_bf7d4aad90502bdf5c67c8a69622de9d.o)\r\n  \"tflite::gpu::GetOpsToReplace(TfLiteContext*)\", referenced from:\r\n      tflite::gpu::metal::(anonymous namespace)::DelegatePrepare(TfLiteContext*, TfLiteDelegate*) in libmetal_delegate.a(metal_delegate_bf7d4aad90502bdf5c67c8a69622de9d.o)\r\n  \"tflite::gpu::GetElementsSizeForPHWC4(tflite::gpu::StrongShape<(tflite::gpu::Layout)10> const&)\", referenced from:\r\n      tflite::gpu::metal::(anonymous namespace)::DelegatePrepare(TfLiteContext*, TfLiteDelegate*)::$_1::__invoke(TfLiteContext*, char const*, unsigned long) in libmetal_delegate.a(metal_delegate_bf7d4aad90502bdf5c67c8a69622de9d.o)\r\n  \"tflite::gpu::metal::ValidateOptimizeModel(std::__1::vector<unsigned int, std::__1::allocator<unsigned int> > const&, std::__1::vector<unsigned int, std::__1::allocator<unsigned int> > const&, std::__1::vector<std::__1::shared_ptr<tflite::gpu::metal::ComputeTaskDescriptor>, std::__1::allocator<std::__1::shared_ptr<tflite::gpu::metal::ComputeTaskDescriptor> > > const&, std::__1::vector<std::__1::shared_ptr<tflite::gpu::metal::ComputeTaskDescriptor>, std::__1::allocator<std::__1::shared_ptr<tflite::gpu::metal::ComputeTaskDescriptor> > >*)\", referenced from:\r\n      tflite::gpu::metal::(anonymous namespace)::DelegatePrepare(TfLiteContext*, TfLiteDelegate*)::$_1::__invoke(TfLiteContext*, char const*, unsigned long) in libmetal_delegate.a(metal_delegate_bf7d4aad90502bdf5c67c8a69622de9d.o)\r\n  \"tflite::gpu::ApplyGeneralTransformations(tflite::gpu::ModelTransformer*)\", referenced from:\r\n      tflite::gpu::metal::(anonymous namespace)::DelegatePrepare(TfLiteContext*, TfLiteDelegate*)::$_1::__invoke(TfLiteContext*, char const*, unsigned long) in libmetal_delegate.a(metal_delegate_bf7d4aad90502bdf5c67c8a69622de9d.o)\r\n  \"tflite::gpu::metal::Compile(tflite::gpu::Model<tflite::gpu::TensorRef<tflite::gpu::StrongShape<(tflite::gpu::Layout)10> > > const&, tflite::gpu::metal::RuntimeOptions const&, std::__1::vector<std::__1::shared_ptr<tflite::gpu::metal::ComputeTaskDescriptor>, std::__1::allocator<std::__1::shared_ptr<tflite::gpu::metal::ComputeTaskDescriptor> > >*)\", referenced from:\r\n      tflite::gpu::metal::(anonymous namespace)::DelegatePrepare(TfLiteContext*, TfLiteDelegate*)::$_1::__invoke(TfLiteContext*, char const*, unsigned long) in libmetal_delegate.a(metal_delegate_bf7d4aad90502bdf5c67c8a69622de9d.o)\r\n  \"tflite::gpu::metal::CreateComputeProgram(id<MTLDevice>, NSString*, NSString, NSDictionary<NSString, NSString>*, id<MTLComputePipelineState> __autoreleasing*)\", referenced from:\r\n      _TFLGpuDelegateCreate in libmetal_delegate.a(metal_delegate_bf7d4aad90502bdf5c67c8a69622de9d.o)\r\nld: symbol(s) not found for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)`\r\n\r\nand i have tried this,\r\ntensorflow-1.15.4 % lipo -info tensorflow/lite/gen/ios_frameworks/tensorflow_lite_gpu.framework/libmetal_delegate.a\r\nNon-fat file: tensorflow/lite/gen/ios_frameworks/tensorflow_lite_gpu.framework/libmetal_delegate.a is architecture: arm64\r\n\r\ncould you help me build the metal delegate framework correctly and run the tflite model with gpu on iphone?"]}, {"number": 27324, "title": "Moving Test from TimeDistributed to wrapper_test.py", "body": "This was in one of the TODOs.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27324) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!\r\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27324) for more info**.\n\n<!-- ok -->", "The original test was failing under tensorflow 2.0 mode, which is why we split it into individual module, so that we can disable it and do further investigation. I don't think we can merge it back until we solve the test failure issue."]}, {"number": 27323, "title": "There is no accuracy graph on the Tensorboard", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": []}, {"number": 27322, "title": "Unable to import tensorflow on googlecloud with gpu", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 on gcloud\r\n-\r\n- TensorFlow installed from (source or binary): not sure \r\n- TensorFlow version: 1.13.\r\n- Python version: 3.5 \r\n- Installed using virtualenv? pip? conda?: pip \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: 10 \r\n- GPU model and memory: Nvidia K-80 15 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nI created a gpu vm instance on google cloud and followed the steps to install tensorflow-gpu \r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n# update apt-get\r\nsudo apt-get update\r\n \r\n# work as root\r\nsudo su\r\n\r\n#!/bin/bash\r\necho \"Checking for CUDA and installing.\"\r\n# Check for CUDA and try to install.\r\nif ! dpkg-query -W cuda; then\r\n    curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.0.176-1_amd64.deb\r\n    sudo dpkg -i cuda-repo-ubuntu1604_9.0.176-1_amd64.deb\r\n    sudo apt-get update\r\n    sudo apt-get install cuda-9-0\r\n    sudo nvidia-smi -pm 1\r\nfi\r\n\r\n\r\nTest that your GPU is sucessfully installed:\r\n# check that GPU is working\r\nnvidia-smi\r\n\r\n\r\nInstall your Deep Neural Network (cuDNN) binaries that you uploaded earlier (check your version):\r\n\r\nsudo dpkg -i libcudnn7_7.4.2.24-1+cuda10.0_amd64.deb\r\n\r\necho 'export CUDA_HOME=/usr/local/cuda' >> ~/.bashrc\r\necho 'export PATH=$PATH:$CUDA_HOME/bin' >> ~/.bashrc\r\necho 'export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\r\nsource ~/.bashrc\r\n\r\n# Install tensorflow\r\nsudo apt-get install python3-dev python3-pip libcupti-dev\r\n\r\n# install tensorflow-gpu\r\nsudo pip3 install tensorflow-gpu\r\n\r\n# install ipython3\r\napt install ipython3\r\n\r\n**Any other info / logs**\r\n\r\nhere is my error file when i do python3 import tensorflow as tf \r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <modul\r\ne>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_i\r\nmport_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\nFailed to load the native TensorFlow runtime.\r\n", "comments": ["duplicate #26059", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27322\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27322\">No</a>\n"]}, {"number": 27321, "title": "Correct Cuda Download Link", "body": "Put the link for all Cuda releases instead of hardcoding the 9.0 release to avoid misleading.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27321) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27321) for more info**.\n\n<!-- ok -->", "The [CUDA\u00ae Toolkit](https://developer.nvidia.com/cuda-toolkit-archive) in the TF website now points to cuda toolkit archive instead of hard coded cuda versions (cuda zone) as before. Thanks!"]}, {"number": 27320, "title": "Can not build lite/delegate/gpu on Raspberry Pi", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux (Raspbian)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1bf6646b871d0ce601715f8ed2f50430ca504da7\r\n- Python version: -\r\n- Installed using virtualenv? pip? conda?: -\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source):  gcc version 6.3.0 20170516 (Raspbian 6.3.0-18+rpi1+deb9u1)\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the problem**\r\n\r\nProbably, version of gcc is older on Raspbian OS. I attached log on this issue (see bottom line of this description).\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nSorry, I didn't use bazel since bazel take huge CPU/Memory resources for Raspberry Pi. I tried to build gpu with following Makefile. I put this Makefile on lite/delegate/gpu. However, this is not a matter for this error may be.\r\n\r\nhttps://gist.github.com/mattn/0dfdee17e025066e4a2386ee3a6af3b0\r\n\r\n**Any other info / logs**\r\n\r\n<details>\r\n<pre>\r\ng++ -std=c++14 -c -DTF_COMPILE_LIBRARY -DGL_GLEXT_PROTOTYPES -I/mnt/usbdisk2/tensorflow -I/mnt/usbdisk2/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/mnt/usbdisk2/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/mnt/usbdisk2/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I. gl/api.cc -o gl/api.o\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/object_manager.h:27:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:29,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h: In constructor \u2018tflite::gpu::gl::gl_buffer_internal::BufferId::BufferId()\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:154:10: error: request for member \u2018IgnoreError\u2019 in \u2018tflite::gpu::gl::gl_call_internal::CallAndCheckError<void (*)(int, unsigned int*), int (*)(), {int, unsigned int*}>(std::__cxx11::basic_string<char>(((const char*)\"glGenBuffers in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:153\"), std::allocator<char>()), glGenBuffers, tflite::gpu::gl::GetOpenGlErrors, 1, (&((tflite::gpu::gl::gl_buffer_internal::BufferId*)this)->tflite::gpu::gl::gl_buffer_internal::BufferId::id_))\u2019, which is of non-class type \u2018int\u2019\r\n         .IgnoreError();\r\n          ^~~~~~~~~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h: In destructor \u2018tflite::gpu::gl::gl_buffer_internal::BufferId::~BufferId()\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:162:52: error: request for member \u2018IgnoreError\u2019 in \u2018tflite::gpu::gl::gl_call_internal::CallAndCheckError<void (*)(int, const unsigned int*), int (*)(), {int, unsigned int*}>(std::__cxx11::basic_string<char>(((const char*)\"glDeleteBuffers in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:162\"), std::allocator<char>()), glDeleteBuffers, tflite::gpu::gl::GetOpenGlErrors, 1, (&((tflite::gpu::gl::gl_buffer_internal::BufferId*)this)->tflite::gpu::gl::gl_buffer_internal::BufferId::id_))\u2019, which is of non-class type \u2018int\u2019\r\n       TFLITE_GPU_CALL_GL(glDeleteBuffers, 1, &id_).IgnoreError();\r\n                                                    ^~~~~~~~~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h: In constructor \u2018tflite::gpu::gl::gl_buffer_internal::BufferBinder::BufferBinder(GLenum, GLuint)\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:182:51: error: request for member \u2018IgnoreError\u2019 in \u2018tflite::gpu::gl::gl_call_internal::CallAndCheckError<void (*)(unsigned int, unsigned int), int (*)(), {const unsigned int&, unsigned int&}>(std::__cxx11::basic_string<char>(((const char*)\"glBindBuffer in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:182\"), std::allocator<char>()), glBindBuffer, tflite::gpu::gl::GetOpenGlErrors, ((tflite::gpu::gl::gl_buffer_internal::BufferBinder*)this)->tflite::gpu::gl::gl_buffer_internal::BufferBinder::target_, id)\u2019, which is of non-class type \u2018int\u2019\r\n     TFLITE_GPU_CALL_GL(glBindBuffer, target_, id).IgnoreError();\r\n                                                   ^~~~~~~~~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h: In destructor \u2018tflite::gpu::gl::gl_buffer_internal::BufferBinder::~BufferBinder()\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:186:50: error: request for member \u2018IgnoreError\u2019 in \u2018tflite::gpu::gl::gl_call_internal::CallAndCheckError<void (*)(unsigned int, unsigned int), int (*)(), {const unsigned int&, int}>(std::__cxx11::basic_string<char>(((const char*)\"glBindBuffer in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:186\"), std::allocator<char>()), glBindBuffer, tflite::gpu::gl::GetOpenGlErrors, ((tflite::gpu::gl::gl_buffer_internal::BufferBinder*)this)->tflite::gpu::gl::gl_buffer_internal::BufferBinder::target_, 0)\u2019, which is of non-class type \u2018int\u2019\r\n     TFLITE_GPU_CALL_GL(glBindBuffer, target_, 0).IgnoreError();\r\n                                                  ^~~~~~~~~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h: In destructor \u2018tflite::gpu::gl::gl_buffer_internal::BufferMapper::~BufferMapper()\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:200:64: error: request for member \u2018IgnoreError\u2019 in \u2018tflite::gpu::gl::gl_call_internal::CallAndCheckError<unsigned char (*)(unsigned int), int (*)(), {const unsigned int&}>(std::__cxx11::basic_string<char>(((const char*)\"glUnmapBuffer in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:200\"), std::allocator<char>()), glUnmapBuffer, tflite::gpu::gl::GetOpenGlErrors, ((tflite::gpu::gl::gl_buffer_internal::BufferMapper*)this)->tflite::gpu::gl::gl_buffer_internal::BufferMapper::target_)\u2019, which is of non-class type \u2018int\u2019\r\n   ~BufferMapper() { TFLITE_GPU_CALL_GL(glUnmapBuffer, target_).IgnoreError(); }\r\n                                                                ^~~~~~~~~~~\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h: In function \u2018int tflite::gpu::gl::CreateReadWriteShaderStorageBuffer(uint32_t, tflite::gpu::gl::GlBuffer*)\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member \u2018ok\u2019 in \u2018status2\u2019, which is of non-class type \u2018const int\u2019\r\n     if (!status2.ok()) return status2; \\\r\n                  ^\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:217:3: note: in expansion of macro \u2018RETURN_IF_ERROR\u2019\r\n   RETURN_IF_ERROR(TFLITE_GPU_CALL_GL(glBufferData, GL_SHADER_STORAGE_BUFFER,\r\n   ^~~~~~~~~~~~~~~\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/object_manager.h:28:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:29,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h: In constructor \u2018tflite::gpu::gl::gl_texture_internal::TextureId::TextureId()\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h:165:10: error: request for member \u2018IgnoreError\u2019 in \u2018tflite::gpu::gl::gl_call_internal::CallAndCheckError<void (*)(int, unsigned int*), int (*)(), {int, unsigned int*}>(std::__cxx11::basic_string<char>(((const char*)\"glGenTextures in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h:164\"), std::allocator<char>()), glGenTextures, tflite::gpu::gl::GetOpenGlErrors, 1, (&((tflite::gpu::gl::gl_texture_internal::TextureId*)this)->tflite::gpu::gl::gl_texture_internal::TextureId::id_))\u2019, which is of non-class type \u2018int\u2019\r\n         .IgnoreError();\r\n          ^~~~~~~~~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h: In destructor \u2018tflite::gpu::gl::gl_texture_internal::TextureId::~TextureId()\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h:172:53: error: request for member \u2018IgnoreError\u2019 in \u2018tflite::gpu::gl::gl_call_internal::CallAndCheckError<void (*)(int, const unsigned int*), int (*)(), {int, unsigned int*}>(std::__cxx11::basic_string<char>(((const char*)\"glDeleteTextures in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h:172\"), std::allocator<char>()), glDeleteTextures, tflite::gpu::gl::GetOpenGlErrors, 1, (&((tflite::gpu::gl::gl_texture_internal::TextureId*)this)->tflite::gpu::gl::gl_texture_internal::TextureId::id_))\u2019, which is of non-class type \u2018int\u2019\r\n       TFLITE_GPU_CALL_GL(glDeleteTextures, 1, &id_).IgnoreError();\r\n                                                     ^~~~~~~~~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h: In constructor \u2018tflite::gpu::gl::gl_texture_internal::TextureBinder::TextureBinder(GLenum, GLuint)\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h:192:52: error: request for member \u2018IgnoreError\u2019 in \u2018tflite::gpu::gl::gl_call_internal::CallAndCheckError<void (*)(unsigned int, unsigned int), int (*)(), {const unsigned int&, unsigned int&}>(std::__cxx11::basic_string<char>(((const char*)\"glBindTexture in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h:192\"), std::allocator<char>()), glBindTexture, tflite::gpu::gl::GetOpenGlErrors, ((tflite::gpu::gl::gl_texture_internal::TextureBinder*)this)->tflite::gpu::gl::gl_texture_internal::TextureBinder::target_, id)\u2019, which is of non-class type \u2018int\u2019\r\n     TFLITE_GPU_CALL_GL(glBindTexture, target_, id).IgnoreError();\r\n                                                    ^~~~~~~~~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h: In destructor \u2018tflite::gpu::gl::gl_texture_internal::TextureBinder::~TextureBinder()\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h:196:51: error: request for member \u2018IgnoreError\u2019 in \u2018tflite::gpu::gl::gl_call_internal::CallAndCheckError<void (*)(unsigned int, unsigned int), int (*)(), {const unsigned int&, int}>(std::__cxx11::basic_string<char>(((const char*)\"glBindTexture in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h:196\"), std::allocator<char>()), glBindTexture, tflite::gpu::gl::GetOpenGlErrors, ((tflite::gpu::gl::gl_texture_internal::TextureBinder*)this)->tflite::gpu::gl::gl_texture_internal::TextureBinder::target_, 0)\u2019, which is of non-class type \u2018int\u2019\r\n     TFLITE_GPU_CALL_GL(glBindTexture, target_, 0).IgnoreError();\r\n                                                   ^~~~~~~~~~~\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/runtime/shared_buffer.h: In member function \u2018int tflite::gpu::gl::SharedBufferData::CreateSharedGlBuffer(tflite::gpu::gl::GlBuffer*)\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member \u2018ok\u2019 in \u2018status2\u2019, which is of non-class type \u2018const int\u2019\r\n     if (!status2.ok()) return status2; \\\r\n                  ^\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/runtime/shared_buffer.h:62:5: note: in expansion of macro \u2018RETURN_IF_ERROR\u2019\r\n     RETURN_IF_ERROR(TFLITE_GPU_CALL_GL(glBufferData, GL_SHADER_STORAGE_BUFFER,\r\n     ^~~~~~~~~~~~~~~\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/runtime.h:30:0,\r\n                 from gl/api.cc:35:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/runtime/shared_buffer.h:67:21: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     return OkStatus();\r\n                     ^\r\ngl/api.cc: In member function \u2018virtual int tflite::gpu::gl::{anonymous}::InferenceContextImpl::Execute()\u2019:\r\ngl/api.cc:61:69: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n       return FailedPreconditionError(\"InferenceContext is not reset\");\r\n                                                                     ^\r\ngl/api.cc: In member function \u2018virtual int tflite::gpu::gl::{anonymous}::InferenceContextImpl::Reset()\u2019:\r\ngl/api.cc:71:21: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     return OkStatus();\r\n                     ^\r\ngl/api.cc: In member function \u2018virtual int tflite::gpu::gl::{anonymous}::InferenceContextWithBatchImpl::Execute()\u2019:\r\ngl/api.cc:97:69: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n       return FailedPreconditionError(\"InferenceContext is not reset\");\r\n                                                                     ^\r\ngl/api.cc:113:78: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n             \"Object \", id, \" does not match expected byte size: \", byte_size));\r\n                                                                              ^\r\ngl/api.cc:122:35: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n               \" vs \", num_batches));\r\n                                   ^\r\ngl/api.cc:137:67: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n                 absl::StrCat(\"Reference to \", id, \" is not found\"));\r\n                                                                   ^\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member \u2018ok\u2019 in \u2018status2\u2019, which is of non-class type \u2018const int\u2019\r\n     if (!status2.ok()) return status2; \\\r\n                  ^\r\ngl/api.cc:139:11: note: in expansion of macro \u2018RETURN_IF_ERROR\u2019\r\n           RETURN_IF_ERROR(buffer->MakeView(b * byte_size, byte_size, ref));\r\n           ^~~~~~~~~~~~~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member \u2018ok\u2019 in \u2018status2\u2019, which is of non-class type \u2018const int\u2019\r\n     if (!status2.ok()) return status2; \\\r\n                  ^\r\ngl/api.cc:142:7: note: in expansion of macro \u2018RETURN_IF_ERROR\u2019\r\n       RETURN_IF_ERROR(runtime_->Execute());\r\n       ^~~~~~~~~~~~~~~\r\ngl/api.cc:144:21: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     return OkStatus();\r\n                     ^\r\ngl/api.cc: In member function \u2018virtual int tflite::gpu::gl::{anonymous}::InferenceContextWithBatchImpl::Reset()\u2019:\r\ngl/api.cc:151:21: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     return OkStatus();\r\n                     ^\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,\r\n                 from gl/api.cc:16:\r\ngl/api.cc: In member function \u2018int tflite::gpu::gl::{anonymous}::CompiledModelImpl::Add(const tflite::gpu::gl::WorkgroupsCalculator&, tflite::gpu::gl::ShaderCode)\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member \u2018ok\u2019 in \u2018status2\u2019, which is of non-class type \u2018const int\u2019\r\n     if (!status2.ok()) return status2; \\\r\n                  ^\r\ngl/api.cc:212:5: note: in expansion of macro \u2018RETURN_IF_ERROR\u2019\r\n     RETURN_IF_ERROR(\r\n     ^~~~~~~~~~~~~~~\r\ngl/api.cc:221:21: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     return OkStatus();\r\n                     ^\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,\r\n                 from gl/api.cc:16:\r\ngl/api.cc: In member function \u2018int tflite::gpu::gl::{anonymous}::CompiledModelImpl::AddFullShader(const string&, const uint3&, size_t*)\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member \u2018ok\u2019 in \u2018status2\u2019, which is of non-class type \u2018const int\u2019\r\n     if (!status2.ok()) return status2; \\\r\n                  ^\r\ngl/api.cc:232:7: note: in expansion of macro \u2018RETURN_IF_ERROR\u2019\r\n       RETURN_IF_ERROR(\r\n       ^~~~~~~~~~~~~~~\r\ngl/api.cc:240:21: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     return OkStatus();\r\n                     ^\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,\r\n                 from gl/api.cc:16:\r\ngl/api.cc: In member function \u2018virtual int tflite::gpu::gl::{anonymous}::CompiledModelImpl::NewRun(const tflite::gpu::gl::RuntimeOptions&, const tflite::gpu::gl::ObjectManager*, tflite::gpu::gl::CommandQueue*, std::unique_ptr<tflite::gpu::gl::InferenceContext>*) const\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member \u2018ok\u2019 in \u2018status2\u2019, which is of non-class type \u2018const int\u2019\r\n     if (!status2.ok()) return status2; \\\r\n                  ^\r\ngl/api.cc:256:9: note: in expansion of macro \u2018RETURN_IF_ERROR\u2019\r\n         RETURN_IF_ERROR(buffer->MakeView(0, s.second, &ref));\r\n         ^~~~~~~~~~~~~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member \u2018ok\u2019 in \u2018status2\u2019, which is of non-class type \u2018const int\u2019\r\n     if (!status2.ok()) return status2; \\\r\n                  ^\r\ngl/api.cc:257:9: note: in expansion of macro \u2018RETURN_IF_ERROR\u2019\r\n         RETURN_IF_ERROR(refs->RegisterBuffer(s.first, std::move(ref)));\r\n         ^~~~~~~~~~~~~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member \u2018ok\u2019 in \u2018status2\u2019, which is of non-class type \u2018const int\u2019\r\n     if (!status2.ok()) return status2; \\\r\n                  ^\r\ngl/api.cc:263:7: note: in expansion of macro \u2018RETURN_IF_ERROR\u2019\r\n       RETURN_IF_ERROR(runtime->AddProgram(shaders_[c.shader_idx], c.parameters,\r\n       ^~~~~~~~~~~~~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member \u2018ok\u2019 in \u2018status2\u2019, which is of non-class type \u2018const int\u2019\r\n     if (!status2.ok()) return status2; \\\r\n                  ^\r\ngl/api.cc:266:5: note: in expansion of macro \u2018RETURN_IF_ERROR\u2019\r\n     RETURN_IF_ERROR(runtime->PrepareForExecution());\r\n     ^~~~~~~~~~~~~~~\r\ngl/api.cc:274:21: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     return OkStatus();\r\n                     ^\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,\r\n                 from gl/api.cc:16:\r\ngl/api.cc: In member function \u2018virtual int tflite::gpu::gl::{anonymous}::CompiledModelImpl::OnProgram(const std::vector<tflite::gpu::gl::UniformParameter>&, const std::vector<tflite::gpu::gl::Object>&, const uint3&, const uint3&, size_t)\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member \u2018ok\u2019 in \u2018status2\u2019, which is of non-class type \u2018const int\u2019\r\n     if (!status2.ok()) return status2; \\\r\n                  ^\r\ngl/api.cc:290:5: note: in expansion of macro \u2018RETURN_IF_ERROR\u2019\r\n     RETURN_IF_ERROR(AddFullShader(partial_shaders_[partial_shader_index],\r\n     ^~~~~~~~~~~~~~~\r\ngl/api.cc:299:21: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     return OkStatus();\r\n                     ^\r\ngl/api.cc: In member function \u2018virtual int tflite::gpu::gl::{anonymous}::CompiledModelImpl::Serialize(std::vector<unsigned char>*) const\u2019:\r\ngl/api.cc:339:21: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     return OkStatus();\r\n                     ^\r\ngl/api.cc: In member function \u2018virtual int tflite::gpu::gl::{anonymous}::CompiledModelImpl::OnShader(absl::Span<const char>)\u2019:\r\ngl/api.cc:345:21: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     return OkStatus();\r\n                     ^\r\ngl/api.cc: In function \u2018int tflite::gpu::gl::Compile(const tflite::gpu::gl::CompilationOptions&, const GraphFloat32&, const tflite::gpu::gl::NodeShader&, const tflite::gpu::gl::WorkgroupsCalculator&, std::unique_ptr<tflite::gpu::gl::CompiledModel>*)\u2019:\r\ngl/api.cc:389:78: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     return InvalidArgumentError(\"Only identical batch dimension is supported\");\r\n                                                                              ^\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member \u2018ok\u2019 in \u2018status2\u2019, which is of non-class type \u2018const int\u2019\r\n     if (!status2.ok()) return status2; \\\r\n                  ^\r\ngl/api.cc:392:3: note: in expansion of macro \u2018RETURN_IF_ERROR\u2019\r\n   RETURN_IF_ERROR(RequestGpuInfo(&gpu_info));\r\n   ^~~~~~~~~~~~~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member \u2018ok\u2019 in \u2018status2\u2019, which is of non-class type \u2018const int\u2019\r\n     if (!status2.ok()) return status2; \\\r\n                  ^\r\ngl/api.cc:396:3: note: in expansion of macro \u2018RETURN_IF_ERROR\u2019\r\n   RETURN_IF_ERROR(compiler->Compile(model, [&](ShaderCode code) -> Status {\r\n   ^~~~~~~~~~~~~~~\r\ngl/api.cc:400:19: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n   return OkStatus();\r\n                   ^\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,\r\n                 from gl/api.cc:16:\r\ngl/api.cc: In function \u2018int tflite::gpu::gl::ReadSerializedModel(const std::vector<unsigned char>&, std::unique_ptr<tflite::gpu::gl::CompiledModel>*)\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member \u2018ok\u2019 in \u2018status2\u2019, which is of non-class type \u2018const int\u2019\r\n     if (!status2.ok()) return status2; \\\r\n                  ^\r\ngl/api.cc:407:3: note: in expansion of macro \u2018RETURN_IF_ERROR\u2019\r\n   RETURN_IF_ERROR(RequestGpuInfo(&gpu_info));\r\n   ^~~~~~~~~~~~~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member \u2018ok\u2019 in \u2018status2\u2019, which is of non-class type \u2018const int\u2019\r\n     if (!status2.ok()) return status2; \\\r\n                  ^\r\ngl/api.cc:409:3: note: in expansion of macro \u2018RETURN_IF_ERROR\u2019\r\n   RETURN_IF_ERROR(DeserializeCompiledModel(\r\n   ^~~~~~~~~~~~~~~\r\ngl/api.cc:412:19: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n   return OkStatus();\r\n                   ^\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:25:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/object_manager.h:27,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:29,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h: In instantiation of \u2018int tflite::gpu::gl::gl_call_internal::Caller<void>::operator()(const string&, F, ErrorF, Params&& ...) [with F = void (*)(int, unsigned int*); ErrorF = int (*)(); Params = {int, unsigned int*}; std::__cxx11::string = std::__cxx11::basic_string<char>]\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:88:24:   required from \u2018int tflite::gpu::gl::gl_call_internal::CallAndCheckError(const string&, F, ErrorF, Params&& ...) [with F = void (*)(int, unsigned int*); ErrorF = int (*)(); Params = {int, unsigned int*}; std::__cxx11::string = std::__cxx11::basic_string<char>]\u2019\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:153:5:   required from here\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:16: error: request for member \u2018ok\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     if (status.ok()) return OkStatus();\r\n         ~~~~~~~^~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:38: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     if (status.ok()) return OkStatus();\r\n                                      ^\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:26: error: request for member \u2018code\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n                   ~~~~~~~^~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:41: error: request for member \u2018error_message\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n                                  ~~~~~~~^~~~~~~~~~~~~\r\nIn file included from /usr/include/EGL/eglplatform.h:119:0,\r\n                 from /usr/include/EGL/egl.h:39,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/portable_gl31.h:21,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_shader.h:23,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_program.h:24,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/command_queue.h:23,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:26,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:12: error: expression list treated as compound expression in functional cast [-fpermissive]\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n            ^\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:25:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/object_manager.h:27,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:29,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h: In instantiation of \u2018int tflite::gpu::gl::gl_call_internal::Caller<void>::operator()(const string&, F, ErrorF, Params&& ...) [with F = void (*)(int, const unsigned int*); ErrorF = int (*)(); Params = {int, unsigned int*}; std::__cxx11::string = std::__cxx11::basic_string<char>]\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:88:24:   required from \u2018int tflite::gpu::gl::gl_call_internal::CallAndCheckError(const string&, F, ErrorF, Params&& ...) [with F = void (*)(int, const unsigned int*); ErrorF = int (*)(); Params = {int, unsigned int*}; std::__cxx11::string = std::__cxx11::basic_string<char>]\u2019\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:162:7:   required from here\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:16: error: request for member \u2018ok\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     if (status.ok()) return OkStatus();\r\n         ~~~~~~~^~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:38: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     if (status.ok()) return OkStatus();\r\n                                      ^\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:26: error: request for member \u2018code\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n                   ~~~~~~~^~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:41: error: request for member \u2018error_message\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n                                  ~~~~~~~^~~~~~~~~~~~~\r\nIn file included from /usr/include/EGL/eglplatform.h:119:0,\r\n                 from /usr/include/EGL/egl.h:39,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/portable_gl31.h:21,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_shader.h:23,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_program.h:24,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/command_queue.h:23,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:26,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:12: error: expression list treated as compound expression in functional cast [-fpermissive]\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n            ^\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:25:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/object_manager.h:27,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:29,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h: In instantiation of \u2018int tflite::gpu::gl::gl_call_internal::Caller<void>::operator()(const string&, F, ErrorF, Params&& ...) [with F = void (*)(unsigned int, unsigned int); ErrorF = int (*)(); Params = {const unsigned int&, unsigned int&}; std::__cxx11::string = std::__cxx11::basic_string<char>]\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:88:24:   required from \u2018int tflite::gpu::gl::gl_call_internal::CallAndCheckError(const string&, F, ErrorF, Params&& ...) [with F = void (*)(unsigned int, unsigned int); ErrorF = int (*)(); Params = {const unsigned int&, unsigned int&}; std::__cxx11::string = std::__cxx11::basic_string<char>]\u2019\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:182:5:   required from here\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:16: error: request for member \u2018ok\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     if (status.ok()) return OkStatus();\r\n         ~~~~~~~^~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:38: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     if (status.ok()) return OkStatus();\r\n                                      ^\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:26: error: request for member \u2018code\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n                   ~~~~~~~^~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:41: error: request for member \u2018error_message\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n                                  ~~~~~~~^~~~~~~~~~~~~\r\nIn file included from /usr/include/EGL/eglplatform.h:119:0,\r\n                 from /usr/include/EGL/egl.h:39,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/portable_gl31.h:21,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_shader.h:23,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_program.h:24,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/command_queue.h:23,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:26,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:12: error: expression list treated as compound expression in functional cast [-fpermissive]\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n            ^\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:25:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/object_manager.h:27,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:29,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h: In instantiation of \u2018int tflite::gpu::gl::gl_call_internal::Caller<void>::operator()(const string&, F, ErrorF, Params&& ...) [with F = void (*)(unsigned int, unsigned int); ErrorF = int (*)(); Params = {const unsigned int&, int}; std::__cxx11::string = std::__cxx11::basic_string<char>]\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:88:24:   required from \u2018int tflite::gpu::gl::gl_call_internal::CallAndCheckError(const string&, F, ErrorF, Params&& ...) [with F = void (*)(unsigned int, unsigned int); ErrorF = int (*)(); Params = {const unsigned int&, int}; std::__cxx11::string = std::__cxx11::basic_string<char>]\u2019\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:186:5:   required from here\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:16: error: request for member \u2018ok\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     if (status.ok()) return OkStatus();\r\n         ~~~~~~~^~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:38: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     if (status.ok()) return OkStatus();\r\n                                      ^\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:26: error: request for member \u2018code\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n                   ~~~~~~~^~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:41: error: request for member \u2018error_message\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n                                  ~~~~~~~^~~~~~~~~~~~~\r\nIn file included from /usr/include/EGL/eglplatform.h:119:0,\r\n                 from /usr/include/EGL/egl.h:39,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/portable_gl31.h:21,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_shader.h:23,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_program.h:24,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/command_queue.h:23,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:26,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:12: error: expression list treated as compound expression in functional cast [-fpermissive]\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n            ^\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:25:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/object_manager.h:27,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:29,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h: In instantiation of \u2018int tflite::gpu::gl::gl_call_internal::Caller<void>::operator()(const string&, F, ErrorF, Params&& ...) [with F = unsigned char (*)(unsigned int); ErrorF = int (*)(); Params = {const unsigned int&}; std::__cxx11::string = std::__cxx11::basic_string<char>]\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:88:24:   required from \u2018int tflite::gpu::gl::gl_call_internal::CallAndCheckError(const string&, F, ErrorF, Params&& ...) [with F = unsigned char (*)(unsigned int); ErrorF = int (*)(); Params = {const unsigned int&}; std::__cxx11::string = std::__cxx11::basic_string<char>]\u2019\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:200:21:   required from here\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:16: error: request for member \u2018ok\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     if (status.ok()) return OkStatus();\r\n         ~~~~~~~^~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:38: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     if (status.ok()) return OkStatus();\r\n                                      ^\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:26: error: request for member \u2018code\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n                   ~~~~~~~^~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:41: error: request for member \u2018error_message\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n                                  ~~~~~~~^~~~~~~~~~~~~\r\nIn file included from /usr/include/EGL/eglplatform.h:119:0,\r\n                 from /usr/include/EGL/egl.h:39,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/portable_gl31.h:21,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_shader.h:23,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_program.h:24,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/command_queue.h:23,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:26,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:12: error: expression list treated as compound expression in functional cast [-fpermissive]\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n            ^\r\nIn file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:25:0,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/object_manager.h:27,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:29,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h: In instantiation of \u2018int tflite::gpu::gl::gl_call_internal::Caller<void>::operator()(const string&, F, ErrorF, Params&& ...) [with F = void (*)(unsigned int, long int, const void*, unsigned int); ErrorF = int (*)(); Params = {int, unsigned int, unsigned char*, int}; std::__cxx11::string = std::__cxx11::basic_string<char>]\u2019:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:88:24:   required from \u2018int tflite::gpu::gl::gl_call_internal::CallAndCheckError(const string&, F, ErrorF, Params&& ...) [with F = void (*)(unsigned int, long int, const void*, unsigned int); ErrorF = int (*)(); Params = {int, unsigned int, unsigned char*, int}; std::__cxx11::string = std::__cxx11::basic_string<char>]\u2019\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/runtime/shared_buffer.h:62:5:   required from here\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:16: error: request for member \u2018ok\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     if (status.ok()) return OkStatus();\r\n         ~~~~~~~^~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:38: error: cannot convert \u2018tflite::gpu::Status\u2019 to \u2018int\u2019 in return\r\n     if (status.ok()) return OkStatus();\r\n                                      ^\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:26: error: request for member \u2018code\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n                   ~~~~~~~^~~~\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:41: error: request for member \u2018error_message\u2019 in \u2018status\u2019, which is of non-class type \u2018const int\u2019\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n                                  ~~~~~~~^~~~~~~~~~~~~\r\nIn file included from /usr/include/EGL/eglplatform.h:119:0,\r\n                 from /usr/include/EGL/egl.h:39,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/portable_gl31.h:21,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_shader.h:23,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_program.h:24,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/command_queue.h:23,\r\n                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:26,\r\n                 from gl/api.cc:16:\r\n/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:12: error: expression list treated as compound expression in functional cast [-fpermissive]\r\n     return Status(status.code(), status.error_message() + \": \" + context);\r\n            ^\r\nMakefile:120: \u30bf\u30fc\u30b2\u30c3\u30c8 'gl/api.o' \u306e\u30ec\u30b7\u30d4\u3067\u5931\u6557\u3057\u307e\u3057\u305f\r\nmake: *** [gl/api.o] \u30a8\u30e9\u30fc 1\r\n</pre>\r\n</details>", "comments": ["FYI, I could build gl_delegate.dll on Windows using this Makefile. But currently it does not work on Windows. (but this should be another issue)", "Compilation issues aside, gl_delegate uses OpenGLES compute shaders for GPU computations. These require OpenGL ES 3.1, and AFAIK Raspberry Pi supports only OpenGLES 2.0", "Ah. Thanks your pointing this.", "Without an issue of GLES's version, current implementation of gpu/gl always require gcc8. Older version of gcc(or clang) can not handle `Caller<void>(....)` and calling with non-void. This is defined in gl/gl_caller.h:88", "@mattn \r\n\r\nThe GPU delegates have only been tested with very specific use cases in mind: on Android and iOS.  For Android, you want the right NDK/SDK versions for it to build, and we haven't made too much effort to \"maybe??? work on some other platforms\".  Having said that, we heavily rely on the specific NDK/SDK toolchains that can cross compile for Android ARM 64-bit architectures with bazel and that also makes an assumption of the supported GCC, I guess.\r\n\r\nYou're essentially trying to make it work for an unsupported case and you would be on your own.", "Thanks. Yes, I understand that Raspberry Pi does not have enough feature to do this. I'm just thinking whether there are possibilities that can be backported to non-compute-shader.", "FYI, yesterday, I could build & run simple app gpu enabled on Linux (not raspberry pi) with #27504.\r\n\r\nhttps://github.com/mattn/tflite-sin\r\n\r\nThis works without errors. (But I didn't make sure this used GPU for this simple app using sigmoid/sgd)", "@mattn \r\n\r\nFYI In a previous implementation, we did have a non-compute shader-based implementation for GLES3.  However, on those \"phones\", the GPU was not powerful enough so that it didn't give a big performance boost either, that we eventually dropped that completely.\r\n\r\nOf course, I have no idea about how GLES2 on RPi would perform; that's why I put \"phones\" in quotes ;)", "Hmm, I hope good to get performance on Raspberry Pi. :/\r\n", "> @mattn\r\n> \r\n> The GPU delegates have only been tested with very specific use cases in mind: on Android and iOS. For Android, you want the right NDK/SDK versions for it to build, and we haven't made too much effort to \"maybe??? work on some other platforms\". Having said that, we heavily rely on the specific NDK/SDK toolchains that can cross compile for Android ARM 64-bit architectures with bazel and that also makes an assumption of the supported GCC, I guess.\r\n> \r\n> You're essentially trying to make it work for an unsupported case and you would be on your own.\r\n\r\nwhich version of NDK do we need to build libtflite_gpu_gl?", "The version we're now recommending is NDK r17c or r18b, with a target NDK API version of 18.", "> The version we're now recommending is NDK r17c or r18b, with a target NDK API version of 18.\r\n\r\nThanks for the reply, I will try to build with ndk r18b~", "I already replied to @mattn in #27504, but in case people are experiencing a similar problem but not aware of that thread:\r\n\r\n> I ran into this problem of Status conflicting with X11 int when trying to use TFLite GPU on desktop. It turns out EGL/egl.h pulls in EGL/eglplatform.h which pulls in X11/Xlib.h & X11/Xutil.h. To bypass that, I built things with with\r\n>\r\n> -DMESA_EGL_NO_X11_HEADERS\r\n> \r\n> Of course, this may be different on RPi, but I would suggest looking tat the EGL include headers and make things work to exclude X11 includes.", "Is there roadmap visibility when Raspberry Pi 4 GPU will be a supported TFLite GPU delegate? ", "Nothing planned as for now, so no roadmap visibility to share.", "It has been confirmed that the videocore gpu on Raspberry Pi 3/4 does not perform well with performance even when enabled.", "Thank you for the update. Would you mind publishing your findings so we can\nget the Raspberry Pi authors and community to work on improvements?\n\nOn Mon, Jul 13, 2020 at 7:39 PM mattn <notifications@github.com> wrote:\n\n> It has been confirmed that the videocore gpu on Raspberry Pi 3/4 does not\n> perform well with performance even when enabled.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27320#issuecomment-657900346>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AARBUFL4WKZCWSQQLBOTUYLR3OSLVANCNFSM4HCPF2YQ>\n> .\n>\n", "As far as I can tell the primary requirement for GPU accelerated TFlite is an openGLES 3.1 capable GPU with a corresponding API. \r\nThe RPi4 VC6 GPU now has OpenGLES 3.1 support using [recent Mesa 3D builds](https://docs.mesa3d.org/relnotes/19.3.0.html).\r\nI haven't got a RPi4 to hand at the moment to try it out, but are there any other roadblocks to successful GPU acceleration, ie will the source compile correctly if OpenGLES 3.1 is supported?\r\nCheers"]}, {"number": 27319, "title": "[MMAP allocation] Error compilation for Window 10 using MVSC 2017", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10 Pro\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.12.0\r\n- Python version: No\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): MVSC 2017\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n- IDE: Qt 4.8,2\r\n- Build system: Cmake 3.14\r\n\r\n\r\n\r\n**Describe the problem**\r\nTrying to compile Tensorflow Lite for Windows I am having the next error in the lines 17 and 20 of the file <em>mmap_allocation.cc</em>: \r\n- `sys/mman.h no such file or directory`\r\n-` unistd.h file not found `\r\n\r\n<img width=\"1920\" alt=\"error_tflite\" src=\"https://user-images.githubusercontent.com/9532966/55277989-1301a300-530f-11e9-9621-86144bd2e34e.png\">\r\n", "comments": ["We no longer officially support the cmake build, and suggest that you use bazel instead: https://www.tensorflow.org/install/source_windows", "Okey, thank you for the info. Let's start with Bazel.\r\nApart from that, I solved the problem by setting CXX17 like below is shown:\r\n   ```\r\n set_property(TARGET tensorflow-lite PROPERTY CXX_STANDARD 17)\r\n    set_property(TARGET tensorflow-lite PROPERTY CXX_STANDARD_REQUIRED ON)\r\n```\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27319\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27319\">No</a>\n"]}, {"number": 27318, "title": "Callback object has no attribute 'validation_data' error", "body": "Since version 1.11.0 in tensorflow keras \"training_arrays.py\" module was removed this part of code:\r\n\r\n```\r\nfor cbk in callbacks:\r\n    cbk.validation_data = val_ins\r\n```\r\nIn original keras library this code still presented.\r\n\r\nAnd now we can't access from callback to validation data. In Callback class self.validation_data property exist, but is always None.  This is bug or planned behavior?", "comments": ["@unameisfine Could you explain the issue and its context with little more details? Thanks!", "> @unameisfine Could you explain the issue and its context with little more details? Thanks!\r\n\r\nFo example, before version 1.11.0 this callback code worked:\r\n\r\n```\r\nclass AUCCallback(callbacks.Callback):\r\n    def __init__(self, out_path='./', patience=10):\r\n        self.auc = 0\r\n        self.patience = patience\r\n\r\n    def on_train_begin(self, logs={}):\r\n        return\r\n\r\n    def on_train_end(self, logs={}):\r\n        return\r\n\r\n    def on_epoch_begin(self, epoch, logs={}):\r\n        return\r\n\r\n    def on_batch_begin(self, batch, logs={}):\r\n        return\r\n\r\n    def on_batch_end(self, batch, logs={}):\r\n        return\r\n\r\n    def on_epoch_end(self, epoch, logs={}):\r\n        cv_pred = self.model.predict(self.validation_data[0], batch_size=1024)\r\n        cv_true = self.validation_data[1]\r\n        auc_val = roc_auc_score(cv_true, cv_pred)\r\n        if self.auc < auc_val:\r\n            self.no_improve = 0\r\n            print(\"Epoch %s - best AUC: %s\" % (epoch, round(auc_val, 4)))\r\n            self.auc = auc_val\r\n        else:\r\n            self.no_improve += 1\r\n            print(\"Epoch %s - current AUC: %s\" % (epoch, round(auc_val, 4)))\r\n            if self.no_improve >= self.patience:\r\n                self.model.stop_training = True\r\n        return\r\n```\r\n\r\nbut after update, we have error:\r\n` AUCCallback object has no attribute 'validation_data'`", "@unameisfine Could you fill the issue template. Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "So, is it possible to have the validation data available for custom callbacks by default based on the `validation_data` param in `Model.fit` and `Model.fit_generator`. It seems it has been removed for no clear reason and now this data has to be \"manually\" provided to the custom callback.", "```\r\nfrom keras.callbacks import Callback\r\nfrom sklearn.metrics import roc_curve, auc\r\n\r\nclass Metrics_AUC(Callback):\r\n  \r\n  def on_train_begin(self, logs={}):\r\n    self.auc = []\r\n \r\n  def on_epoch_end(self, epoch, logs={}):\r\n    val_predict = (np.asarray(self.model.predict(self.model.validation_data[0]))).round()\r\n    val_targ = self.model.validation_data[1]\r\n    fpr, tpr, _ = roc_curve(val_targ, val_predict)\r\n    _auc = auc(fpr, tpr)\r\n   \r\n    self.auc.append(_auc)\r\n    print(\"auc: %f \" %(_auc))\r\n    return\r\n\r\nauc_custom = `Metrics_AUC()\r\n```\r\n\r\nim getting below error when this function is called :\r\nAttributeError: 'Model' object has no attribute 'validation_data'\r\n\r\nAnyone found solution to it ?", "Bhisham-Sharma did you find the solution for validation_data?\r\n"]}, {"number": 27317, "title": "Please add \"Abandon 996\" badge in this project to support us.", "body": "---------------------------\r\n- Add this [badge](https://github.com/996icu/996.ICU/blob/master/externals/instruction.md) - to your project to support 996.ICU.  \r\n- \u628a\u8fd9\u4e2a[\u5fbd\u7ae0](https://github.com/996icu/996.ICU/blob/master/externals/instruction.md)\u52a0\u5165\u4f60\u7684\u9879\u76ee\u6765\u652f\u6301996.ICU\r\n![https://996.icu](https://img.shields.io/badge/link-996.icu-red.svg)\r\n--------------------------------------------\r\nChinese software engineers are protesting against the country \u2019 s 996 work schedule.\r\nAn unleagal work schedule (9a.m. ~ 9p.m., 6 days a week) .\r\nSearch 996.icu to support us !!\r\n\r\n\u4e2d\u56fd\u7684\u7a0b\u5e8f\u5458\u6b63\u5728\u7f51\u4e0a\u53d1\u8d77\u53cd\u6297 996 \u5de5\u4f5c\u5236\u7684\u8fd0\u52a8\r\n\u201c 996 \u201d\u5de5\u4f5c\u5236\uff0c\u5373\u6bcf\u5929\u65e9 9 \u70b9\u5230\u5c97\uff0c\u4e00\u76f4\u5de5\u4f5c\u5230\u665a\u4e0a 9 \u70b9\uff0c\u6bcf\u5468\u5de5\u4f5c 6 \u5929\u3002\r\n\u8bf7\u641c\u7d22 996.icu \u652f\u6301\u6211\u4eec \uff01\r\n---------------------------\r\n# Working Men of All developers, Unite!", "comments": ["Please support us.", "illegal not unleagal.", "I guess human evolution left some individuals behind.\r\n\r\nDamn : (", "TensorFlow issues isn't the right place for this discussion."]}, {"number": 27316, "title": "Weights returned from `keras.model.Model.get_weights()` are different from the ones fetched from the graph directly.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v1.13.1-0-g6612da8951  1.13.1\r\n- Python version:  3.6.5 [Conda virtual env]\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n**Current behavior**\r\n```python\r\nimport tf.keras.layers as L\r\ndef feed_forward_nn():\r\n    with tf.variable_scope(\"keras_\"):\r\n        _input_ = L.Input(shape=(3,))\r\n        layer_1 = L.Dense(5, Z.relu)(_input_)\r\n        layer_2 = L.Dense(5, Z.relu)(layer_1)\r\n        output = L.Dense(1, Z.tanh)(layer_2)\r\n    return Model(inputs=_input_, outputs=output)\r\n\r\nmodel = feed_forward_nn()\r\nsess.run(tf.global_variables_initializer())\r\n```\r\n**Getting weights from the graph itself vs using model.get_weights(): **\r\n```bash\r\nIn [5]: sess.run(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))                                                                                                       \r\nOut[5]: \r\n[array([[ 0.06312895,  0.41315466, -0.7228936 ,  0.66872925, -0.29116392],\r\n        [ 0.4684649 ,  0.07688528, -0.6403172 ,  0.22407418,  0.2667293 ],\r\n        [ 0.6107295 , -0.12038571,  0.6050673 , -0.423037  ,  0.13438004]],\r\n       dtype=float32),\r\n array([0., 0., 0., 0., 0.], dtype=float32),\r\n array([[ 0.02425474, -0.45923662,  0.57829964, -0.346116  ,  0.1658817 ],\r\n        [ 0.45619428, -0.7142852 ,  0.09248799,  0.10972124, -0.32121253],\r\n        [ 0.4513328 ,  0.7124406 ,  0.620286  ,  0.41065276, -0.26047498],\r\n        [ 0.31083477, -0.46835992, -0.20313907,  0.48168743,  0.4530183 ],\r\n        [-0.4258671 , -0.18113756,  0.18505335, -0.49022397,  0.70809317]],\r\n       dtype=float32),\r\n array([0., 0., 0., 0., 0.], dtype=float32),\r\n array([[ 0.14051151],\r\n        [ 0.326164  ],\r\n        [ 0.7357073 ],\r\n        [ 0.82876945],\r\n        [-0.52645826]], dtype=float32),\r\n array([0.], dtype=float32)]\r\nIn [6]: model.get_weights()                                                                                                                                              \r\nOut[6]: \r\n[array([[-0.7260302 , -0.8412575 ,  0.461235  , -0.18713814,  0.04823786],\r\n        [-0.7028123 , -0.7654265 , -0.4894964 ,  0.14831334,  0.49071664],\r\n        [-0.3141371 ,  0.23999351,  0.00772947,  0.49302799,  0.20739347]],\r\n       dtype=float32),\r\n array([0., 0., 0., 0., 0.], dtype=float32),\r\n array([[-0.6223821 , -0.5421302 ,  0.25800073, -0.56548995, -0.31957147],\r\n        [ 0.12645936,  0.16026676, -0.5968463 , -0.4449416 , -0.5069353 ],\r\n        [ 0.7126628 ,  0.36706662,  0.16168451,  0.6257292 ,  0.6289371 ],\r\n        [-0.41813722, -0.46337304,  0.62247336, -0.7190004 , -0.5288886 ],\r\n        [ 0.35295987, -0.68682575,  0.7552402 , -0.728891  , -0.285271  ]],\r\n       dtype=float32),\r\n array([0., 0., 0., 0., 0.], dtype=float32),\r\n array([[0.18540406],\r\n        [0.84715295],\r\n        [0.11520958],\r\n        [0.47841287],\r\n        [0.46228695]], dtype=float32),\r\n array([0.], dtype=float32)]\r\n```\r\n**Using model.trainable_weights property:**\r\n```bash\r\nIn [14]: sess.run(model.trainable_weights)                                                                                                                               \r\nOut[14]: \r\n[array([[ 0.06312895,  0.41315466, -0.7228936 ,  0.66872925, -0.29116392],\r\n        [ 0.4684649 ,  0.07688528, -0.6403172 ,  0.22407418,  0.2667293 ],\r\n        [ 0.6107295 , -0.12038571,  0.6050673 , -0.423037  ,  0.13438004]],\r\n       dtype=float32),\r\n array([0., 0., 0., 0., 0.], dtype=float32),\r\n array([[ 0.02425474, -0.45923662,  0.57829964, -0.346116  ,  0.1658817 ],\r\n        [ 0.45619428, -0.7142852 ,  0.09248799,  0.10972124, -0.32121253],\r\n        [ 0.4513328 ,  0.7124406 ,  0.620286  ,  0.41065276, -0.26047498],\r\n        [ 0.31083477, -0.46835992, -0.20313907,  0.48168743,  0.4530183 ],\r\n        [-0.4258671 , -0.18113756,  0.18505335, -0.49022397,  0.70809317]],\r\n       dtype=float32),\r\n array([0., 0., 0., 0., 0.], dtype=float32),\r\n array([[ 0.14051151],\r\n        [ 0.326164  ],\r\n        [ 0.7357073 ],\r\n        [ 0.82876945],\r\n        [-0.52645826]], dtype=float32),\r\n array([0.], dtype=float32)]\r\n```", "comments": ["@abhayraw1 Could you provide a code to reproduce bug? Please create a GitHub gist or any other way to share code to reproduce bug. Thanks!", "@jvishnuvardhan  [Here](https://gist.github.com/abhayraw1/af7ab2bb524b0392c6495c4d8d90c3f4) is the gist to reproduce the issue", "@abhayraw1 random number generator generates different number for line #2 and line#3. So eventhough, weights are same for model1 and model2, but the inputs (random numbers) are different and hence, the outputs are also different.\r\nLine1:    print(\"\\nIf YES then these should be equal!!!\")\r\nLine2:    print(model1.predict(np.random.random((1, 3))))\r\nLine3:    print(model2.predict(np.random.random((1, 3))))\r\nPlease let me know whether I missed anything here. Thanks!", "@jvishnuvardhan Hey, sorry for the terrible blunder. I actually deviated from what I experienced before. I have updated the [gist](https://gist.github.com/abhayraw1/af7ab2bb524b0392c6495c4d8d90c3f4)\r\n\r\nThe doubt that I have is that why are the weights returned by `model.trainable_weights` different from `model.get_weights()`?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27316\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27316\">No</a>\n", "@abhayraw1 I found the same issue. Do you know the answer? Thank you.", "@lululxvi Please create a new issue with a simple standalone code to reproduce the issue. Thanks!", "@lululxvi Try running the gist that I shared in the previous comments. This was a long time ago so I don't really remember the details. I thought switched to PyTorch as few weeks after opening this issue I guess."]}, {"number": 27315, "title": "this two message one after another ", "body": "installing tensorflow-gpu on python 3.6.5 conda \r\n\r\n> when try install \r\n> tensorflow 0.9.0 has requirement protobuf==3.0.0b2, but you'll have protobuf 3.7.1 which is incompatible.\r\n\r\nafter I install protobuf 3.0.0b2\r\n\r\n> tensorflow-gpu 1.12.0 has requirement protobuf>=3.6.1, but you'll have protobuf 3.0.0b2 which is incompatible.\r\n> tensorboard 1.12.2 has requirement protobuf>=3.4.0, but you'll have protobuf 3.0.0b2 which is incompatible.\r\n> tb-nightly 1.14.0a20190319 has requirement protobuf>=3.6.0, but you'll have protobuf 3.0.0b2 which is incompatible.\r\n\r\nwhat should i do ?\r\n", "comments": ["You might need to install Protocol Buffers 3.6.1 or above as it describes", "Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27315\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27315\">No</a>\n"]}, {"number": 27313, "title": "Error if input size (of test set) not divisible by TPU core count. \"InvalidArgumentError: slice index 0 of dimension 0 out of bounds.\" or \"AssertionError: batch_size must be divisible by the number of TPU cores in use (7 vs 8)\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary):  Google Colab\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: release 10.0, V10.0.130\r\n- GPU model and memory: Google Colab TPU\r\n\r\n**Describe the current behavior**\r\n\r\nThe following error occurs:\r\n\r\n```\r\n\r\nTrain on 10000 samples, validate on 1153031 samples\r\nEpoch 1/25\r\nINFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 650), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='activation_7_target_10')]\r\nINFO:tensorflow:Overriding default placeholder.\r\nINFO:tensorflow:Remapping placeholder for input_1\r\nINFO:tensorflow:Started compiling\r\nINFO:tensorflow:Finished compiling. Time elapsed: 7.617780447006226 secs\r\nINFO:tensorflow:Setting weights on TPU model.\r\n 8192/10000 [=======================>......] - ETA: 4s - loss: 1.5425 - mean_squared_error: 1.5425INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(98,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(98, 650), dtype=tf.float32, name='input_10'), TensorSpec(shape=(98, 1), dtype=tf.float32, name='activation_7_target_10')]\r\nINFO:tensorflow:Overriding default placeholder.\r\nINFO:tensorflow:Remapping placeholder for input_1\r\nINFO:tensorflow:Started compiling\r\nINFO:tensorflow:Finished compiling. Time elapsed: 6.491861820220947 secs\r\n 9216/10000 [==========================>...] - ETA: 2s - loss: 1.4734 - mean_squared_error: 1.4734INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 650), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='activation_7_target_10')]\r\nINFO:tensorflow:Overriding default placeholder.\r\nINFO:tensorflow:Remapping placeholder for input_1\r\nINFO:tensorflow:Started compiling\r\nINFO:tensorflow:Finished compiling. Time elapsed: 5.56870174407959 secs\r\nINFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(0,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(0, 650), dtype=tf.float32, name='input_10'), TensorSpec(shape=(0, 1), dtype=tf.float32, name='activation_7_target_10')]\r\n\r\n---------------------------------------------------------------------------\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1658   try:\r\n-> 1659     c_op = c_api.TF_FinishOperation(op_desc)\r\n   1660   except errors.InvalidArgumentError as e:\r\n\r\nInvalidArgumentError: slice index 0 of dimension 0 out of bounds. for 'strided_slice_3' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n```\r\nSo basically the input shapes gets remapped to something nonesensical:  (0,)\r\n\r\n**Describe the expected behavior**\r\n\r\nInput shapes get remapped to something that makes sense.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\ndef train_on_TPU_regression(model,model_save_loc, X_train, y_train, X_test, y_test, batch_size=1024, epochs=25, save_best_only=True, period=1, train_patience=5, mse=True):\r\n \r\n  #Identify TPU worker\r\n  TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\n  tf.logging.set_verbosity(tf.logging.INFO)\r\n  \r\n  #Useful to avoid clutter from old models / layers.\r\n  tf.keras.backend.clear_session()\r\n\r\n  #Convert model to TPU model\r\n  tpu_model = tf.contrib.tpu.keras_to_tpu_model(model,strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\r\n  \r\n  print(\"\\n\")\r\n\r\n  if mse:\r\n    #Compile the model\r\n    tpu_model.compile(\r\n      optimizer=tf.train.AdamOptimizer(), \r\n      loss=tf.keras.losses.mean_squared_error,\r\n      metrics=['mse']\r\n    )\r\n  else:\r\n    #Compile the model\r\n    tpu_model.compile(\r\n      optimizer=tf.train.AdamOptimizer(), \r\n      loss=tf.keras.losses.mean_absolute_error,\r\n      metrics=['mae']\r\n    )\r\n  \r\n  #Configure how to save model and early stopping\r\n  callbacks_list = [\r\n      tf.keras.callbacks.ModelCheckpoint(\r\n          filepath=model_save_loc,\r\n          save_weights_only=True,\r\n          monitor='val_loss', \r\n          save_best_only=save_best_only,\r\n          mode='auto',\r\n          period=period),\r\n      tf.keras.callbacks.EarlyStopping(monitor='val_loss', \r\n                                       patience=train_patience,\r\n                                       mode='auto')\r\n  ]\r\n  \r\n  history = tpu_model.fit(X_train,\r\n                          y_train,\r\n                          validation_data=(X_test,y_test),\r\n                          epochs=epochs,\r\n                          batch_size=batch_size,\r\n                          callbacks=callbacks_list,\r\n                          verbose=1)\r\n\r\n  return tpu_model, history\r\n```\r\n```\r\nprint(X_train.shape)\r\nprint(y_train_scaled.shape)\r\nprint(X_test.shape)\r\nprint(y_test_scaled.shape)\r\n\r\n(6533755, 650)\r\n(6533755,)\r\n(1153031, 650)\r\n(1153031,)\r\n```\r\n\r\n```\r\ninputs = tf.keras.layers.Input(shape=(SEQUENCE_LEN,))\r\n\r\nx = tf.keras.layers.Embedding(CLASSES, 8, input_length=SEQUENCE_LEN)(inputs) \r\nx = tf.keras.layers.Conv1D(128, 7)(x)\r\nx = tf.keras.layers.BatchNormalization()(x)\r\nx = tf.keras.layers.Activation(\"relu\")(x)\r\nx = tf.keras.layers.Conv1D(128, 3)(x)\r\nx = tf.keras.layers.BatchNormalization()(x)\r\nx = tf.keras.layers.Activation(\"relu\")(x)\r\nx = tf.keras.layers.Conv1D(128, 3)(x)\r\nx = tf.keras.layers.BatchNormalization()(x)\r\nx = tf.keras.layers.Activation(\"relu\")(x)\r\n\r\nx = tf.keras.layers.MaxPooling1D(3)(x)\r\nx = tf.keras.layers.Conv1D(256, 3)(x)\r\nx = tf.keras.layers.BatchNormalization()(x)\r\nx = tf.keras.layers.Activation(\"relu\")(x)\r\nx = tf.keras.layers.Conv1D(256, 3)(x)\r\nx = tf.keras.layers.BatchNormalization()(x)\r\nx = tf.keras.layers.Activation(\"relu\")(x)\r\nx = tf.keras.layers.Conv1D(256, 3)(x)\r\nx = tf.keras.layers.BatchNormalization()(x)\r\nx = tf.keras.layers.Activation(\"relu\")(x)\r\n\r\nx = tf.keras.layers.GlobalAveragePooling1D()(x)\r\nx = tf.keras.layers.Dense(256)(x)\r\nx = tf.keras.layers.BatchNormalization()(x)\r\nx = tf.keras.layers.Activation(\"relu\")(x)\r\nx = tf.keras.layers.Dense(1)(x)\r\nx = tf.keras.layers.Activation(\"linear\")(x)\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=x)\r\nmodel.summary()\r\n```\r\n\r\n```\r\ntpu_model, history = train_on_TPU_regression(model,model_saves_folder_location+\"model_#01_08.hdf5\", X_train[:10000], y_train_scaled[:10000], X_test, y_test_scaled, train_patience=10, batch_size=1024)\r\n```\r\n\r\n**How it can be prevented apparently**\r\n\r\n```\r\ntpu_model, history = train_on_TPU_regression(model,model_saves_folder_location+\"model_#01_08.hdf5\", X_train[:10000], y_train_scaled[:10000], X_test[:-7], y_test_scaled[:-7], train_patience=10, batch_size=1024)\r\n```\r\nBy changing the size of the training data this error can be prevented? In this case I made the number of training examples divisible by 1024, however, it worked too by just using 10000: `X_test[:10000], y_test_scaled[:10000]`, which is how i noticed this in the first place.\r\n\r\n**Question:**\r\nIt this really a bug or am I missing something?\r\n\r\n**UPDATE**\r\n\r\nWhen trying to predict, I get the following error:\r\n\r\n```\r\nTPU worker setup:\r\nINFO:tensorflow:Querying Tensorflow master (grpc://10.14.138.218:8470) for TPU system metadata.\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 13438656196918098020)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 12429978056820721148)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 3813600547282296097)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 18120823110831817385)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 429246965946293269)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5942823189813875074)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 1996329778228354684)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 15775279347369926923)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6250325552725876457)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 17570123092856457982)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 3640626136020975642)\r\nWARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\r\n\r\n\r\nCalculating predictions:\r\nINFO:tensorflow:New input shapes; (re-)compiling: mode=infer (# of cores 8), [TensorSpec(shape=(4, 650), dtype=tf.float32, name='input_10')]\r\nINFO:tensorflow:Overriding default placeholder.\r\nINFO:tensorflow:Remapping placeholder for input_1\r\nINFO:tensorflow:Started compiling\r\nINFO:tensorflow:Finished compiling. Time elapsed: 2.8085508346557617 secs\r\nINFO:tensorflow:Setting weights on TPU model.\r\n1152832/1153031 [============================>.] - ETA: 0s\r\n\r\n---------------------------------------------------------------------------\r\n\r\nAssertionError                            Traceback (most recent call last)\r\n\r\n<ipython-input-36-aa8045e6a687> in <module>()\r\n----> 1 y_preds, rmse_err, mae_err = calc_reg_pred(model, weights_location, X_test, y_test, mse=True, scaler=y_train_scaled)\r\n\r\n<ipython-input-35-fc3b6ce0949b> in calc_reg_pred(model, weight_loc, X_test, y_test, batch_size, mse, scaler)\r\n     32 \r\n     33   print(\"Calculating predictions:\")\r\n---> 34   y_pred = cpu_model.predict(X_test, verbose=1)\r\n     35 \r\n     36   if not(scaler is None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in predict(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\r\n   1982           max_queue_size=max_queue_size,\r\n   1983           workers=workers,\r\n-> 1984           use_multiprocessing=use_multiprocessing)\r\n   1985 \r\n   1986   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\r\n   1111     else:\r\n   1112       return training_arrays.predict_loop(\r\n-> 1113           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\r\n   1114 \r\n   1115   def reset_metrics(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\r\n    327 \r\n    328         # Get outputs.\r\n--> 329         batch_outs = f(ins_batch)\r\n    330         if not isinstance(batch_outs, list):\r\n    331           batch_outs = [batch_outs]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in __call__(self, inputs)\r\n   1254     infeed_manager = self._lookup_infeed_manager(inputs)\r\n   1255     input_tensors, inputs = self._construct_input_tensors_and_inputs(inputs)\r\n-> 1256     infeed_instance = infeed_manager.make_infeed_instance(inputs)\r\n   1257     del inputs  # To avoid accident usage.\r\n   1258     input_specs = infeed_instance.make_input_specs(input_tensors)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in make_infeed_instance(self, inputs)\r\n    663 \r\n    664   def make_infeed_instance(self, inputs):\r\n--> 665     sharded_inputs = self._split_tensors(inputs)\r\n    666     return self.NumpyInfeedInstance(sharded_inputs)\r\n    667 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in _split_tensors(self, inputs)\r\n    652     assert batch_size % self._tpu_assignment.num_towers == 0, (\r\n    653         'batch_size must be divisible by the number of TPU cores in use (%s '\r\n--> 654         'vs %s)' % (batch_size, self._tpu_assignment.num_towers))\r\n    655     shard_size = batch_size // self._tpu_assignment.num_towers\r\n    656     input_list = []\r\n\r\nAssertionError: batch_size must be divisible by the number of TPU cores in use (7 vs 8)\r\n```\r\n\r\nI get the error, even though my batch size was 128 and later 1024. Then I again applied `X_test[:-7], y_test[:-7]` and the error is gone. \r\n*So I would conclude that the TPU currently cant handle imput sizes not divisible by muber of TPUs for prediction.*", "comments": ["i have got the same issue, but the error occurs when i am trying \"fit_generator\" method on TPU model (the same one with \"fit\" works fine)", "Hi, I have the same problem when training the model on colab with TPU.\r\n1.Using Tensorflow 1.13.1 (Other lib are colab's Default)\r\n2.Using tf.Keras and using `tf.contrib.tpu.keras_to_tpu_model`\r\n3.Using fit_generator(Because my data is too big)\r\n\r\n**Problem will happened after the training run for a while.**\r\nAt begining I think there's some data error.\r\nBut then I training the same data and code with GPU and success.\r\nSo during training of TPU some error happend.\r\n\r\nlooks like reason from the data generator.\r\nI build a data generator by class \r\n\r\n`DataGenerator(tf.keras.utils.Sequence):`\r\n\r\nAnd because reading data is slow(big image data),\r\nI use use_multiprocessing , workers and max_queue_size .\r\n\r\n```\r\nmodel.fit_generator(train_data,\r\n                    steps_per_epoch = steps_per_epoch,\r\n                    epochs = 30,\r\n                    callbacks=callbacks,\r\n                    validation_data = validation_data,\r\n                    validation_steps = validation_steps,\r\n                    use_multiprocessing = True,\r\n                    workers = 8,\r\n                    max_queue_size = 24\r\n                   )\r\n```\r\n\r\nHere is my log when training.\r\n\r\n```\r\ny_pred shape (?, 32, 80, 80, 2)\r\ny_true shape (?, ?, ?, ?, ?)\r\nINFO:tensorflow:Querying Tensorflow master (grpc://10.79.62.194:8470) for TPU system metadata.\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 4592675977746479807)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 3835905418955304939)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9973634016239195625)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 7412254046126854739)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 1251060567783699712)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 159592003167631933)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 3555001218257168182)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 497511677534294237)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3996482485963028891)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 14345708154009247026)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 13869207026421776388)\r\nWARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\r\nINFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\r\nINFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\r\n\r\nEpoch 1/30\r\nINFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(1,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(1, 32, 80, 80, 1), dtype=tf.float32, name='input_10'), TensorSpec(shape=(1, 32, 80, 80, 2), dtype=tf.float32, name='segmentation_2classes_target_30')]\r\n\r\nINFO:tensorflow:Overriding default placeholder.\r\nINFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\r\nINFO:tensorflow:Remapping placeholder for input\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nINFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7ff0efe11d30> []\r\ny_pred shape (8, 32, 80, 80, 2)\r\ny_true shape (8, 32, 80, 80, 2)\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDeprecated in favor of operator or tf.math.divide.\r\nINFO:tensorflow:Started compiling\r\nINFO:tensorflow:Finished compiling. Time elapsed: 67.50986790657043 secs\r\nINFO:tensorflow:Setting weights on TPU model.\r\nINFO:tensorflow:CPU -> TPU lr: 0.0010000000474974513 {0.001}\r\nINFO:tensorflow:CPU -> TPU beta_1: 0.8999999761581421 {0.9}\r\nINFO:tensorflow:CPU -> TPU beta_2: 0.9990000128746033 {0.999}\r\nINFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\r\nWARNING:tensorflow:Cannot update non-variable config: epsilon\r\nWARNING:tensorflow:Cannot update non-variable config: amsgrad\r\n233/363 [==================>...........] - ETA: 7:52 - loss: 0.4663 - dice: 0.4089INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(0,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(0, 32, 80, 80, 1), dtype=tf.float32, name='input_10'), TensorSpec(shape=(0, 32, 80, 80, 2), dtype=tf.float32, name='segmentation_2classes_target_30')]\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1658   try:\r\n-> 1659     c_op = c_api.TF_FinishOperation(op_desc)\r\n   1660   except errors.InvalidArgumentError as e:\r\n\r\nInvalidArgumentError: slice index 0 of dimension 0 out of bounds. for 'strided_slice_1' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n```", "I find the reason!\r\nThe error said new input size.\r\nSo there must be something wrong with input data.\r\nI finally checked out my batch generator didn't worked well.\r\nWhen using fit_generator with  Class `DataGenerator(tf.keras.utils.Sequence):`\r\n,steps_per_epoch  didn't work.\r\nExample I set steps_per_epoch  362 but still in training is 363.\r\n\r\nYou need to Check your generator's len function\r\n```\r\ndef __len__(self):\r\n        return math.ceil(len(self.datas) / float(self.batch_size))-1\r\n```\r\n\r\n```\r\n233/363 [==================>...........] - ETA: 7:52 - loss: 0.4663 - dice: 0.4089\r\nINFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(0,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(0, 32, 80, 80, 1), dtype=tf.float32, name='input_10'), TensorSpec(shape=(0, 32, 80, 80, 2), dtype=tf.float32, name='segmentation_2classes_target_30')]\r\n```\r\n\r\n", "@AaronSpieler Is this still an issue? Can you please provide the complete code to reproduce the issue. Thanks!\r\n", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I have a similar issue. In my case I have a Dataset with shape (None, 5, 128, 128, 1). If I try to train a Sequence keras Model with fit, I get the error:\r\n\r\nInvalidArgumentError: First dim of input shape: 5 is not divisible by num_workers: 8\r\n\r\nI use the TPU colaboratory environment in eager mode and with a TPUStrategy.", "After some tests, I have found that the issue in my case is related with the batch size. Calling the method Dataset.batch with a multiple of 8 number has solved the problem.", "@jmgc, \r\nCan you please post a new issue by providing the information asked by the [template](https://github.com/tensorflow/tensorflow/issues/new/choose)?\r\nThe reason for this is we can focus on your specific configuration and problem since the root cause can be unrelated even though the error messages are similar. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27312, "title": "import results in Recursion-Error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n    `import tensorflow`\r\n    `import kivy`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Windows 10\r\n- TensorFlow installed from (source or binary): pip install (I guess it's binary)\r\n- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415, 2.0.0-alpha0\r\n- Python version: 3.7 (Anaconda Win64)\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n**Describe the current behavior**\r\nHundreds of messages ` WARNING: Logging before flag parsing goes to stderr.`, finally `RecursionError: maximum recursion depth exceeded while calling a Python object`. Output attached here: [tst.txt](https://github.com/tensorflow/tensorflow/files/3025303/tst.txt)\r\n\r\n\r\n**Describe the expected behavior**\r\n- no error by importing both packages\r\n- independent behaviour from the order of imports\r\n\r\n**Code to reproduce the issue**\r\n\r\n    import tensorflow\r\n    import kivy\r\norder is important. kivy version is 1.10.1 - not sure if installed by pip or conda...\r\n\r\n**Other info / logs**\r\n\r\nSeems to be an issue with incompatible setup of `logging` - but as the error is raised in Tensorflow code, that's where it needs to be fixed.\r\n", "comments": ["@jaba166 Is there any error if you run only \"import tensorflow\". Were you able to import each of those two packages separately. It might be an issue from kivy package as all error log points to kivy.  It is better to post in kivy repo [here](https://github.com/kivy/kivy/issues). Thanks! ", "I can import each one separately, no problem. I can also import them in the\nother order.\n\nI can also import kivy with the latest tensorboard 1 without issues in any\norder. That alone clearly indicates a tensorflow2 issue.\n\nIn addition, if you look at the recursion error traceback its tensorflow2\ncreating the unbounded recursion.\n\nEven if kivy would do sth wrong, its not correct that tensorflow2 is\nentering a loop. And tf1 didn't.\n\njjm\n\n\nVishnuvardhan Janapati <notifications@github.com> schrieb am Di., 2. Apr.\n2019, 23:40:\n\n> @jaba166 <https://github.com/jaba166> Is there any error if you run only\n> \"import tensorflow\". Were you able to import each of those two packages\n> separately. It might be an issue from kivy package as all error log points\n> to kivy. It is better to post in kivy repo here\n> <https://github.com/kivy/kivy/issues>. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27312#issuecomment-479213320>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AofDLGW53m5GxCM2JQLmZ8g2ZRRsqmg8ks5vc847gaJpZM4cTpS2>\n> .\n>\n", "not sure if the build/install tag is correct as the package is correctly built and installed. Instead it seems the initialization of the `logging` module the culprit.", "This also happens with tf-nightly and seems to do with absl library that we depend on:\r\nhttps://github.com/abseil/abseil-py", "Hello, everyone. Just checked - kivy with  tensorflow==1.13.0rc0 works fine. \r\nIf I'm trying to use it with tensorflow-1.14.0, I'm getting the following error:\r\nHundreds of messages WARNING: Logging before flag parsing goes to stderr. \r\nand recursion limit exceeding error after that.\r\nBoth packages are installed via PIP in Anaconda environment. I hope it will help someone.", "@jaba166,\r\nYour code could be executed successfully, without any error. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/49a05fb1caf13edcc1c1aa7dd3d74c55/gh_27312.ipynb) of the working code. Thanks! ", "Works in tensorflow 2.4.1 - thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27312\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27312\">No</a>\n"]}, {"number": 27311, "title": "[TF 2.0] tf.summary.histogram error", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Testing\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): '2.0.0-dev20190327'\r\n- Python version: 3.7\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI want to log gradients and weights of the model as it is training, using `tf.summary.histogram()`.\r\nIt seems that the function is not properly implemented, however, as using it results in an error:\r\n```\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorboard/plugins/histogram/summary_v2.py\", line 67, in histogram\r\n    tensor = _buckets(data, bucket_count=buckets)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorboard/plugins/histogram/summary_v2.py\", line 85, in _buckets\r\n    with tf.name_scope('buckets', values=[data, bucket_count]):\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n`tf.summary.histogram()` should properly log histogram data.\r\n\r\n**Code to reproduce the issue**\r\n``` python3\r\nimport tensorflow as tf\r\n\r\ntf.summary.histogram(\r\n    'test',\r\n    [1.0, 2.0, 3.0],\r\n    step=1,\r\n)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\n  File \"dat.py\", line 6, in <module>\r\n    step=1,\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorboard/plugins/histogram/summary_v2.py\", line 67, in histogram\r\n    tensor = _buckets(data, bucket_count=buckets)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorboard/plugins/histogram/summary_v2.py\", line 85, in _buckets\r\n    with tf.name_scope('buckets', values=[data, bucket_count]):\r\nTypeError: __init__() got an unexpected keyword argument 'values'\r\n```\r\n", "comments": ["Example:- \r\nsummary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\r\nwriter.add_summary(summary, step)\r\nwriter.flush()\r\n\r\nplaceholder = tf.placeholder(tf.float32)\r\n\r\ntf.summary.histogram('N(0,1)', placeholder)\r\nsummaries = tf.summary.merge_all()\r\nwriter = tf.summary.FileWriter('./summaries')\r\n\r\n__Last two line are what you are missing__", "@shashvatshahi1998 I used your code and I get the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"dat.py\", line 3, in <module>\r\n    ph = tf.placeholder(tf.float32)\r\nAttributeError: module 'tensorflow' has no attribute 'placeholder'\r\n```\r\n\r\nIt seems that `tf.placeholder()` does not exist anymore in the TF 2.0 API.", "import tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n\r\nThis is the way in which you can use this function in tensorflow 2.x.", "But then will the rest of my code work? Since it is written in the 2.0 API?", "pip install --upgrade tb-nightly\r\ntry this for upgrading tb nightly and then run your initial code. ", "I can confirm that this fixed the problem!\r\n\r\nWith `tb-nightly` version `1.14.0a20190319`, the code now runs correctly.\r\n\r\nThank you very much for the help :)"]}, {"number": 27310, "title": "ModuleNotFoundError: No module named 'tensorflow.optimizers'", "body": "  my tensorflow version is tf-nightly-2.0-preview-2.0.0.dev20190329\r\n\r\nbut when i try 'import tensorflow.optimizers.Adam',it raise ModuleNotFoundError: No module named 'tensorflow.optimizers'. \r\n\r\n", "comments": ["Reinstall upgraded version of everything.", "@zhaoyingjun \r\n1. Were you able to run \"import tensorflow as tf\"\r\n2. Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!", "Just tried in Google Colab, tf version 2.0.0-dev20190405\r\n`from tensorflow.optimizers import Adam`\r\n```\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-24-40e5ae37d9cb> in <module>()\r\n----> 1 from tensorflow.optimizers import Adam\r\n\r\nModuleNotFoundError: No module named 'tensorflow.optimizers'\r\n```\r\n\r\nsame error with `import tensorflow.keras.optimizers.Adam` and `import tensorflow.optimizers.Adam`.. this will be bad way of importing though as I always recommend from x.y import z.\r\n\r\nHowever this works: `from tensorflow.keras.optimizers import Adam`\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@zhaoyingjun In TF2.0, keras is going to play a major role. Most of the functions (layers, optimizer...etc) are moved under keras. So you can do what @armando-fandango suggested or something like below\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom keras.optimizers import Adam\r\n```\r\nI am closing this as it was resolved. Please let me know if I am mistaken. Thanks!\r\n", "actually, I have the same problem but your idea doesn't work.", "@sassan72 Please create a new issue with a simple standalone code to reproduce the issue. Thanks!"]}, {"number": 27309, "title": "BasicLSTMCell performances questions", "body": "I was reading tf code now, but not understand following comments: why \"+\", \"*\" performances are bad? can anyone help to explain? Thanks!\r\n\r\n\r\n\r\n**Note that using `add` and `multiply` instead of `+` and `*` gives a\r\nperformance improvement. So using those at the cost of readability.**\r\n\r\n    add = math_ops.add\r\n    multiply = math_ops.multiply\r\n    new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))),\r\n                multiply(sigmoid(i), self._activation(j)))\r\n    new_h = multiply(self._activation(new_c), sigmoid(o))", "comments": ["Most of the tf.math_ops are optimized for performance and are also very useful for graph based calculations. \r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "> Most of the tf.math_ops are optimized for performance and are also very useful for graph based calculations.\r\n> This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n\r\nthanks!", "I am closing this issue as it is a support question that is suited for Stackoverflow. Thanks!"]}, {"number": 27308, "title": "support freeze graph where there are control ops between ReadVariableOp and VarHandleOp", "body": "", "comments": ["@gargn would you help review this?", "@drpngx @annarev Hi guys, it's been 10 days since I created this pr, is there any progress on the reviewing process?  Looking forward to your feedback.", "As far as I know, this module is mostly only used by TensorFlow Lite conversion now. \r\nTensorFlow Lite is going to support control flow V2 (e.g. If, While) but not V1 (e.g. Enter, Exit). \r\nCould you help us to better understand your use cases?", "@miaout17 I see what you are saying. In our case, we want to freeze a tensorflow model and load it on java side for inference. But it has problems with some tf.keras layers as described in  https://github.com/tensorflow/tensorflow/issues/25721 and other isssues. I think it would be nice to support this as much of our code is depending on this feature.", "Any update on this one? I'm having the same problem as yangw1234; trying to freeze the graph consisting the tf.keras.layers.LSTM layer throws \"Cannot find the variable that is an input to the ReadVariableOp.\" exception.", "Any progress on this issue? ", "Closing this PR. This functionality was added with slightly more streamlined logic recently."]}]