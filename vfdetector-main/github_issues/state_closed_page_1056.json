[{"number": 21608, "title": "avoid url error exception", "body": "## Summary\r\nTo Avoid falling into url error exception, I changed the order of exceptions.\r\n\r\n**Reference**\r\nhttps://github.com/keras-team/keras/pull/10887", "comments": ["@protoget  @olicht \r\nWhat is the current status?"]}, {"number": 21607, "title": "Sample", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 21606, "title": "Sample", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 21605, "title": "Mirroed strategy", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Nagging Assignee @bignamehyp: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "See #20370\r\n", "Sorry i think i accidentally clicked something and it has created this issue."]}, {"number": 21604, "title": "how to automatically restart worker/master after occasional crash", "body": "HI! I'm running distributed training of tensorflow, I have a very simple question, does TF provide some feature for monitor and automatically restart crashed worker/master all all training process, as currently I can only manually restart failed worker.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@mrry do we have any such facility?", "No, we rely on a cluster manager such as Kubernetes to restart crashed processes. See the KubeFlow project for a high-level example of using Kubernetes to manage a set of TensorFlow jobs:\r\n\r\nhttps://github.com/kubeflow/kubeflow"]}, {"number": 21603, "title": "Default device_filters are only set if session_config is None", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nThe changelog for 1.10 mentions that starting from this version `RunConfig` comes with a default value for `device_filters`. However, the default [is only applied](https://github.com/tensorflow/tensorflow/blob/ffc9be66bc1503ec12f10f43ff622d9193062e97/tensorflow/python/estimator/run_config.py#L525-L530) if the user did not provide any value for `session_config`, i.e. if the following code does not get the default:\r\n\r\n```\r\nconfig = tf.estimator.RunConfig(session_config=tf.ConfigProto())\r\n```\r\n\r\nI think it might be more useful to patch the `session_config` instead of replacing it fully.\r\n\r\nSide question: why are the following two lines needed? Why isn't it enough to just set `device_filters`?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/ffc9be66bc1503ec12f10f43ff622d9193062e97/tensorflow/python/estimator/run_config.py#L538-L540", "comments": ["Nagging Assignee @xiejw: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "For the sake of backward compatibility. we should and cannot overwrite user provided the session_config. The approach we took now is safe as it flipped the default value only when session-config is not present. \r\n\r\nThinking forward for TF 2.0, this is a good default for between-graph training. Distribution strategy should overwrite it as Distritubiton strategy knows more information than the default Estimator framework.\r\n\r\n \r\n\r\n ", "@xiejw do you assume that if the session_config has been provided, and devide_filters are empty, then the user explicitly did not set them? ", "@superbobry I think the answer is yes to your question.\r\n\r\nFor now, if you use distribution strategy, it will override the device filters according to the strategy you use.", "I see, thank you very much for the clarification. Closing the issue for now."]}, {"number": 21602, "title": "Android build failed using custom tensorflow model. Conflict workspace Error", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\n   Yes, I have modified no. of classes to be detect while training the model\r\n- **OS Platform and Distribution: 14.04 LTS\r\n- **Mobile device if the issue happens on mobile device**: Motoroloa, Android version 7.1.1\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.0\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: 4GB RAM, 64-bit\r\n\r\n### Describe the problem\r\nI had trained my custom model with different objects for the detection purpose. I had used ssd_mobilenet_v2_coco for my training. I could pull out the object detection on desktop with the customised trained model by using the webcam or passing an image. But I am unable to port it on android app. \r\nIt is interesting that I am able to convert the file in .tflite format (the one which will be used by bazel) and for android. However, after the app installation it is getting crashed. The app is forcibly closed. It may be noted here that the classifier and speech app is working fine and there is no force shut down, but the problem remained the same with detect app. I then followed this [link](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) for change in the DetectorActivity.java inside  tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/DetectorActivity.java as below `private static final int TF_OD_API_INPUT_SIZE = 300;\r\n  private static final boolean TF_OD_API_IS_QUANTIZED = true;\r\n  private static final String TF_OD_API_MODEL_FILE = \"detect.tflite\";\r\n  //private static final String TF_OD_API_LABELS_FILE = \"file:///android_asset/coco_labels_list.txt\";\r\n  private static final String TF_OD_API_LABELS_FILE = \"file:///android_asset/labelmap.txt\";`\r\n\r\nwhere detect.tflite and labelmap.txt both files are from the customized model.\r\n\r\nThen I ran the command \r\n`bazel build -c opt --cxxopt='--std=c++11' \"//tensorflow/contrib/lite/examples/android:tflite_demo\"` and this successfully build the apk file and other dependencies. Meanwhile, it showed a conflict error. Errors read as below:\r\n`CONFLICT: asset:WORKSPACE is provided with ambiguous priority from:\r\n\texternal/tflite_mobilenet/WORKSPACE\r\n\texternal/tflite_conv_actions_frozen/WORKSPACE\r\n\r\nCONFLICT: asset:WORKSPACE is provided with ambiguous priority from:\r\n\texternal/tflite_mobilenet_ssd/WORKSPACE\r\n\texternal/tflite_conv_actions_frozen/WORKSPACE`\r\n\r\nWhen I tried to build the apk over phone, the app is installed but it is getting closed forcefully. However,  the classifier and speech apps are working perfectly fine.\r\n\r\n\r\nNOTE: The pre trained models are working absolutely fine on both desktop and as an app. However, the issue is with importing the custom app on phone.\r\n\r\n", "comments": ["> When I tried to build the apk over phone, the app is installed but it is getting closed forcefully. However, the classifier and speech apps are working perfectly fine.\r\n\r\nCan you provide more details about the crash? Would you mind capturing the logcat entry for the crash? Or capturing and attaching a bug report?", "Hi jdduke,\r\nThank you for your response. After the installation on android device, other two apps are opening fine. However, when I am opening the detection app it is saying **\"TFLite Demo has stopped\"** close the app.\r\n\r\nI have followed all the steps [Link](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193)  under the section **\"Running our model on Android\"**. I have changed as instructed in the blog for the custom models. Following steps have been followed.\r\nstep 1.\r\n`assets = [\r\n   \"//tensorflow/contrib/lite/examples/android/app/src/main/assets:labels_mobilenet_quant_v1_224.txt\",\r\n     \"@tflite_mobilenet//:mobilenet_quant_v1_224.tflite\",\r\n     \"@tflite_conv_actions_frozen//:conv_actions_frozen.tflite\",\r\n     \"//tensorflow/contrib/lite/examples/android/app/src/main/assets:conv_actions_labels.txt\",\r\n     \"@tflite_mobilenet_ssd//:mobilenet_ssd.tflite\",\r\n     \"//tensorflow/contrib/lite/examples/android/app/src/main/assets:detect.tflite\",\r\n     \"//tensorflow/contrib/lite/examples/android/app/src/main/assets:box_priors.txt\",\r\n     \"//tensorflow/contrib/lite/examples/android/app/src/main/assets:labelmap.txt\",\r\n],`\r\n\r\nstep 2.\r\n`private static final int TF_OD_API_INPUT_SIZE = 300;\r\nprivate static final boolean TF_OD_API_IS_QUANTIZED = true;\r\nprivate static final String TF_OD_API_MODEL_FILE = \"detect.tflite\";\r\nprivate static final String TF_OD_API_LABELS_FILE = \"file:///android_asset/labelmap.txt\";\r\n`\r\nStep 3. Building the apk \r\nbazel build -c opt --cxxopt='--std=c++11' \"//tensorflow/contrib/lite/examples/android:tflite_demo\".\r\n\r\nHowever, it throws following conflict warnings.\r\n\r\n`INFO: Analysed target //tensorflow/contrib/lite/examples/android:tflite_demo (64 packages loaded).\r\nINFO: Found 1 target...\r\nINFO: From Processing Android resources for //tensorflow/contrib/lite/examples/android:tflite_demo:\r\nAug 16, 2018 10:12:23 AM com.google.devtools.build.android.AndroidDataMerger doMerge\r\nWARNING: \r\nCONFLICT: asset:WORKSPACE is provided with ambiguous priority from:\r\n\texternal/tflite_mobilenet/WORKSPACE\r\n\texternal/tflite_conv_actions_frozen/WORKSPACE\r\n\r\nCONFLICT: asset:WORKSPACE is provided with ambiguous priority from:\r\n\texternal/tflite_mobilenet_ssd/WORKSPACE\r\n\texternal/tflite_conv_actions_frozen/WORKSPACE\r\nTarget //tensorflow/contrib/lite/examples/android:tflite_demo up-to-date:\r\n  bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo_deploy.jar\r\n  bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo_unsigned.apk\r\n  bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk\r\nINFO: Elapsed time: 88.021s, Critical Path: 24.14s\r\nINFO: 10 processes: 9 local, 1 worker.\r\nINFO: Build completed successfully, 17 total actions`.\r\n\r\nAttaching here the logcat file named android_build.txt\r\n\r\n\r\n\r\n\r\n[android_build.txt](https://github.com/tensorflow/tensorflow/files/2292450/android_build.txt)\r\n\r\nThank you,\r\n", "Did it work with ssd_mobilenet_v1_coco? Can you provide the exact instruction of how you generated or what link did you download the tflite file? I want to make sure that the tflite was generated correctly?", "Hi @achowdhery ,\r\nThank you for your response. I had not tried with ssd_mobilenet_v1_coco. But I had used ssd_mobilnet_v2_coco and it worked perfect on my device. Every apps are running perfectly fine for this case. However, when I am  trying to port the custom model using tflite; installed app is getting closed repeatedly. \r\n\r\nI was trying again. While converting from .pb file to tflite format, I used inference_type=FLOAT and made a change in the DetectorActivity.java file for `private static final boolean TF_OD_API_IS_QUANTIZED = false;`. After this I rebuild the apk and installed on my device. The app is installed, but after sometime (10 sec) it is getting closed again  with a message \"TFLite Demo has stopped\" close the app. Since, I have not changed anything for the Classifier and speech files, both are working fine. Only concern is with the Detection.\r\n\r\nPlease find the detailed steps below from converting frozen graph into the tflite format. (I retrained my model with my data set and have frozen_graph.pb files and labelmap.pbtxt). It may be noted here that I have not downloaded any tflite file from any source. \r\n\r\nStep 1:\r\n`DETECT_PB=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph.pb`\r\n`STRIPPED_PB=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph_stripped.pb`\r\n\r\nstep 2:\r\n`cd $HOME/tensorflow-master`\r\n`touch WORKSPACE`\r\n\r\nstep 3 :\r\n`bazel build tensorflow/python/tools:optimize_for_inference && \\\r\nbazel-bin/tensorflow/python/tools/optimize_for_inference \\\r\n--input=$DETECT_PB \\\r\n--output=$STRIPPED_PB \\\r\n--frozen_graph=True \\\r\n--input_names=Preprocessor/sub \\\r\n--output_names=concat,concat_1 \\\r\n--alsologtostderr`\r\n\r\nstep 4:\r\n`bazel run -c opt tensorflow/python/tools/optimize_for_inference -- \\\r\n--input=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph.pb \\\r\n--output=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph_stripped.pb \\\r\n--frozen_graph=True \\\r\n--input_names=Preprocessor/sub \\\r\n--output_names=concat,concat_1 \\\r\n--alsologtostderr`\r\n\r\nstep 5: Converting in tflite format\r\n`bazel run -c opt //tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph_stripped.pb \\\r\n--output_file=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/detect.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=Preprocessor/sub \\\r\n--output_arrays=concat,concat_1 --inference_type=FLOAT --logtostderr`\r\n\r\nLet me know if you need any further information.\r\n\r\nThank you,\r\n\r\n\r\n", "It looks like this is an issue with a mismatched Tensor + input buffer size.\r\n\r\nWhen you use ByteBuffer as the input argument, you need to explicitly resize the input Tensor to match the size/shape of the ByteBuffer (assuming the runtime input size might differ from the default size expected by the model). See also https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java#L176.\r\n\r\nThe relevant error in the logs is \r\n\r\n> 08-16 10:26:42.107 28242 28257 E AndroidRuntime: FATAL EXCEPTION: inference\r\n> 08-16 10:26:42.107 28242 28257 E AndroidRuntime: Process: org.tensorflow.lite.demo, PID: 28242\r\n> 08-16 10:26:42.107 28242 28257 E AndroidRuntime: java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 1080000 bytes and a ByteBuffer with 270000 bytes.", "Hi @jdduke,\r\n\r\nThank you for your help and response. I could figure out the error in the log file. However, I am unable to fix this mismatch. Could you please suggest/share instructions about fixing this mismatch error.\r\n\r\nI would appreciate your pain if you could share which file has to be changed for the FLOAT inference. Is the error is applicable to all models at tensorflow zoo directory which are not quantized? I had tried with ssd_mobilnet_v1_coco also, but the QUANTIZED_UINT8 is not working for the custom trained model.\r\n\r\nLook forward to learn about this. ", "Just to confirm, for the FLOAT inference with the float model that you created, did you double check that `TF_OD_API_IS_QUANTIZED = false` is set in [DetectorActivity.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/DetectorActivity.java#L53)? That should cause the ByteBuffer to be sized to 1080000 (300 * 300 * 3 * 4), whereas from the error message it looks to be using the quantized size (300 * 300 * 3).\r\n\r\n", "@alokranjan007 \r\nIn DetectorActivity.java if you have a float model, the following lines should be used to change the size:\r\n\r\n  private static final boolean TF_OD_API_IS_QUANTIZED = false;\r\n  private static final String TF_OD_API_MODEL_FILE = \"detect.tflite\";\r\n\r\nThis should automatically correct line 128 in TFLiteObjectDetectionAPIModel.java\r\n// Pre-allocate buffers.\r\n    int numBytesPerChannel;\r\n    if (isQuantized) {\r\n      numBytesPerChannel = 1; // Quantized\r\n    } else {\r\n      numBytesPerChannel = 4; // Floating point\r\n    }", "Hello @achowdhery,\r\n\r\nThank you for your reply. Yes, I had changed the mentioned lines in the DetectorActivity.Java filE ( This was mentioned in first response to your question). However, as mentioned for ssd_mobilenet_v2 the app is getting closed after 10 sec. \r\n\r\nI am currently training with the quantized model (the one from tensorflow zoo , ssd_mobilenet_v1_quantized). I will share the details with you people.\r\n\r\nI wonder if one can convert float type model (for example ssd_mobilenet_v1_coco and ssd_mobilenet_v2_coco) into qunatized one by following some modifications in the scripts. Whats your suggestion?   ", "Hi @jdduke,\r\n\r\nThanks for your help. I am currently training with the quantized model and will port the model on android. I will share the results soon for both float and quant inferences.  ", "Hello @achowdhery @jdduke,\r\n\r\nAs mentioned in the previous comment, I trained ssd_mobilenet_v1_0.75_depth_quantized_coco model downloaded from the tensorflow zoo [Download Link](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md). I changed the config file for my custom classes and changed the paths for training and testing records. \r\n\r\nAfter the training I converted the frozen graph into .tflite format (as mentioned in the blog by @achowdhery and have also mentioned in the above comments). Unfortunately after the necessary changes also in the `DetectorActivity.java` file the installed app is getting closed. \r\n\r\nNote: Since the model i have downloaded is quantized, i used `inference_type=QUANTIZED_UINT8` but it threw errors as:\r\n\r\n> Array FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6, which is an input to the DepthwiseConv operator producing the output array FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\r\nAborted (core dumped).\r\n\r\nThen I changed the `inference_type=FLOAT` and it created the `detect.tflite` file of size 12.4 Mb. I could not understand this particular reason. Due to this I changed \r\n\r\n> private static final boolean TF_OD_API_IS_QUANTIZED = false;\r\nprivate static final String TF_OD_API_MODEL_FILE = \"detect.tflite\";\r\nafter this also the installed detect app is getting stopped again and again.\r\n\r\nCould you help me to understand why such behavior of qunatized model. \r\n\r\nI am attaching here my `pipeline.config` file used for train the model. I have used 2500 training steps.\r\n\r\n> model {\r\n  ssd {\r\n    inplace_batchnorm_update: true\r\n    freeze_batchnorm: false\r\n    num_classes: 5\r\n    box_coder {\r\n      faster_rcnn_box_coder {\r\n        y_scale: 10.0\r\n        x_scale: 10.0\r\n        height_scale: 5.0\r\n        width_scale: 5.0\r\n      }\r\n    }\r\n    matcher {\r\n      argmax_matcher {\r\n        matched_threshold: 0.5\r\n        unmatched_threshold: 0.5\r\n        ignore_thresholds: false\r\n        negatives_lower_than_unmatched: true\r\n        force_match_for_each_row: true\r\n        use_matmul_gather: true\r\n      }\r\n    }\r\n    similarity_calculator {\r\n      iou_similarity {\r\n      }\r\n    }\r\n    encode_background_as_zeros: true\r\n    anchor_generator {\r\n      ssd_anchor_generator {\r\n        num_layers: 6\r\n        min_scale: 0.2\r\n        max_scale: 0.95\r\n        aspect_ratios: 1.0\r\n        aspect_ratios: 2.0\r\n        aspect_ratios: 0.5\r\n        aspect_ratios: 3.0\r\n        aspect_ratios: 0.3333\r\n      }\r\n    }\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 300\r\n        width: 300\r\n      }\r\n    }\r\n    box_predictor {\r\n      convolutional_box_predictor {\r\n        min_depth: 0\r\n        max_depth: 0\r\n        num_layers_before_predictor: 0\r\n        use_dropout: false\r\n        dropout_keep_probability: 0.8\r\n        kernel_size: 1\r\n        box_code_size: 4\r\n        apply_sigmoid_to_scores: false\r\n        class_prediction_bias_init: -4.6\r\n        conv_hyperparams {\r\n          activation: RELU_6,\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.00004\r\n            }\r\n          }\r\n          initializer {\r\n            random_normal_initializer {\r\n              stddev: 0.01\r\n              mean: 0.0\r\n            }\r\n          }\r\n          batch_norm {\r\n            train: true,\r\n            scale: true,\r\n            center: true,\r\n            decay: 0.97,\r\n            epsilon: 0.001,\r\n          }\r\n        }\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: 'ssd_mobilenet_v1'\r\n      min_depth: 16\r\n      depth_multiplier: 0.75\r\n      conv_hyperparams {\r\n        activation: RELU_6,\r\n        regularizer {\r\n          l2_regularizer {\r\n            weight: 0.00004\r\n          }\r\n        }\r\n        initializer {\r\n          truncated_normal_initializer {\r\n            stddev: 0.03\r\n            mean: 0.0\r\n          }\r\n        }\r\n        batch_norm {\r\n          train: true,\r\n          scale: true,\r\n          center: true,\r\n          decay: 0.97,\r\n          epsilon: 0.001,\r\n        }\r\n      }\r\n      override_base_feature_extractor_hyperparams: true\r\n    }\r\n    loss {\r\n      classification_loss {\r\n        weighted_sigmoid_focal {\r\n          alpha: 0.75,\r\n          gamma: 2.0\r\n        }\r\n      }\r\n      localization_loss {\r\n        weighted_smooth_l1 {\r\n          delta: 1.0\r\n        }\r\n      }\r\n      classification_weight: 1.0\r\n      localization_weight: 1.0\r\n    }\r\n    normalize_loss_by_num_matches: true\r\n    normalize_loc_loss_by_codesize: true\r\n    post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 1e-8\r\n        iou_threshold: 0.6\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n  }\r\n}\r\ntrain_config: {\r\n  fine_tune_checkpoint: \"/home/alok/tensorflow_master/models/research/object_detection/ssd_mobilenet_v1_0.75_quantized/model.ckpt\"\r\n  fine_tune_checkpoint_type:  \"detection\"\r\n  batch_size: 24\r\n  sync_replicas: true\r\n  startup_delay_steps: 0\r\n  replicas_to_aggregate: 8\r\n  num_steps: 2500\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n  data_augmentation_options {\r\n    ssd_random_crop {\r\n    }\r\n  }\r\n  optimizer {\r\n    momentum_optimizer: {\r\n      learning_rate: {\r\n        cosine_decay_learning_rate {\r\n          learning_rate_base: 0.7\r\n          total_steps: 2500\r\n          warmup_learning_rate: 0.3\r\n          warmup_steps: 50\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.9\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  max_number_of_boxes: 100\r\n  unpad_groundtruth_tensors: false\r\n}\r\ntrain_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"/home/alok/tensorflow_master/models/research/object_detection/customized_model/train.record\"\r\n  }\r\n  label_map_path: \"/home/alok/tensorflow_master/models/research/object_detection/customized_model/training/label_map.pbtxt\"\r\n}\r\neval_config: {\r\n  metrics_set: \"coco_detection_metrics\"\r\n  use_moving_averages: false\r\n  num_examples: 81\r\n}\r\neval_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"/home/alok/tensorflow_master/models/research/object_detection/customized_model/test.record\"\r\n  }\r\n  label_map_path: \"/home/alok/tensorflow_master/models/research/object_detection/customized_model/training/label_map.pbtxt\"\r\n  shuffle: false\r\n  num_readers: 1\r\n}\r\n\r\nEdit: Is this is something related to some specific details about quantization which is missing in the above config file. I found that in the blog you have mentioned this regarding quantization that should be tune in the config file \r\n\r\n> graph_rewriter {\r\n  quantization {\r\n    delay: 48000\r\n    weight_bits: 8\r\n    activation_bits: 8\r\n  }\r\n\r\nPlease share you expertise. Let me know if any further information or file is needed.\r\n\r\n\r\nThank you,\r\n\r\n", "Hi @achowdhery @jdduke: I have resolved the error with the inference type, but now facing other problem.\r\n\r\n\r\n Could you please help me regarding the results of the trained model. \r\n\r\nI trained two different models from tensorflow zoo directory named `ssd_mobilenet_v1_0.75_depth_quantized_coco` and `ssd_mobilenet_v1_quantized_coco` each with 10000 training steps. I had changed the no of classes to 5 (for my data set) and used a batch size of 10 for both the models. However, the total no of of images are limited to 205.\r\n\r\n\r\nBoth the model trained well with all files generated. I tested the trained model using webcam and it worked perfectly fine.\r\n\r\nAs a next step, I used your blog [Link here](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) and converted the .pb file into `.tflite` format. The generated `detect.tflite` is of size 3.2 Mb (This is what you have also suggested in the blog that the file should be less than 4 Mb). However, after build over android device, there is no detection at all.\r\n\r\nI tried to hold it for at least 2 minutes for each classes (by showing images of each class), but there is no result in detection. It did not even overfit of showed any false detections. \r\n\r\n\r\nWhat I have done wrong? Please share your thoughts. \r\n\r\nI am trying with both the models but no luck at all. I wonder the same model (frozen graph) is working perfectly fine for webcam based detection.\r\n\r\nThank you,", "Hi @jdduke and @achowdhery ,\r\nSince the quantized model is not detecting any thing after building over android. I tried my luck with `ssd_mobilenet_v2_coco` model. I  trained the model for 20000 training steps. \r\n\r\nThen, I converted the trained model to get a frozen graph using `export_tflite_ssd_graph` then converted the .pb file into `tflite`. \r\n\r\nI followed your suggestion and have used `inference_type=FLOAT` and made a change accordingly in the DetectorActivity.java file i.e. `private static final boolean TF_OD_API_IS_QUANTIZED = false;`.\r\n\r\nThen i build it over phone. The detector app is installed.  But the concern is the app is getting crashed after a minute. It shows the detection, but getting crashed.\r\n\r\nI used the logcat and found a fatal error about \r\n\r\n> 08-27 12:13:54.129 11179 11208 E AndroidRuntime: java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 1080000 bytes and a ByteBuffer with 602112 bytes.\r\n\r\n\r\nI am not getting I have used float inference and have changed the detector file also then why this is throwing such error.\r\n\r\nNote: For configuration I had used image size as 300 and 300. But learning from the log file it is taking somewhere (224 *224* 3* 4=602112). For the conversion also I had used `--input_shapes=1,300,300,3`.\r\n\r\nI am attaching the txt file for your perusal.\r\n[ssd_mobilenet_v2_27_Aug.txt](https://github.com/tensorflow/tensorflow/files/2323215/ssd_mobilenet_v2_27_Aug.txt)\r\n\r\nCould you please share your views on this? How should I rectify this?\r\n\r\nThank you for your help.", "Hello all, is there any update ?", "Right, the problem is that the model expects a buffer of shape 300x300x3, but you're feeding it an image buffer of shape 224x224x3. The shape has to match exactly. You need to crop/resize your image to 300x300 first.", "@jdduke  I completely agree with you. Correct me if I am wrong, the image_resizer will take care of the size of the input image? \r\n\r\nIf this is the case then one should not train the model with large input size for eg. 512x512? \r\nDoes it mean that if you have a image size other than 300x300 then you have to resize it manually and then use as training and testing data set?\r\n\r\nThank you, ", "I'll defer to @achowdhery for the specifics of your pipeline and whether image_resizer is baked into the model or is pre-processing.\r\n\r\nIf a resize is baked into the converted model, then you can let it handle the resize, but otherwise you'll need to do the cropping/resizing manually before feeding it to inference. I believe existing mobilenet models do *not* perform an implicit resize, so you'll need to crop/resize before feeding the input.\r\n", "@jdduke Thank you so much for your prompt reply. I am excited to know more details from @achowdhery about this image input.  I wonder if we  could handle this dynamically.  Do all ssd_mobilnet models on the tensorflow zoo directory is strictly limited to 300x300 image size?\r\n", "Hey @jdduke,  I would appreciate your pain, if you could share some insights about the input image size for training and annotations. I have followed the tutorial about data processing available in the object detection API, but could not find about the image size for SSD_mobilenet_v2. ", "I don't believe the image_resizer gets backed into the model, so you need to make sure that 1) you're training on the same size that you feed into --input_shapes, and that the dimensions you give to --input_shapes match that specified in [TF_OD_API_INPUT_SIZE](https://github.com/tensorflow/tensorflow/blob/b390be62ad0514f4fd3347b9db3446c84e08a38e/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/DetectorActivity.java#L52)", "Nagging Assignees @jdduke, @achowdhery: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Hello guys!! I am facing same issue with TFL detect app.\r\nI trained ssd mobilenet v2 for my custom dataset for 5 classes.\r\nThe TFL detect app is getting crashed by showing the bug mentioned below.Please help me out..\r\n\r\njava.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 1080000 bytes and a ByteBuffer with 270000 bytes.\r\n\tat org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:251)\r\n\tat org.tensorflow.lite.Tensor.setTo(Tensor.java:110)\r\n\tat org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:145)\r\n\tat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n\tat org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:193)\r\n\tat org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:247)\r\n\tat android.os.Handler.handleCallback(Handler.java:742)\r\n\tat android.os.Handler.dispatchMessage(Handler.java:95)\r\n\tat android.os.Looper.loop(Looper.java:157)\r\n\tat android.os.HandlerThread.run(HandlerThread.java:61)", "@M1thun Please ensure that input data type Quantized/Float in the app matches the model you have provided.  This seems to be data type mismatch at input", "yeah thanks!! @achowdhery  i was able to resolve this issue", "Hi all!\r\nI use inception_v3 model in a simple app for smartphone (Android OS).\r\nAn input images have an arbitrary sizes and I have some trouble and confuse.\r\n\r\ntflite.run(imgData, labelProbArray);\r\n\r\nThis instruction generates the exception:\r\n```\r\nW/System.err: java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 602112 bytes and a ByteBuffer with 338688 bytes.\r\nW/System.err:     at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:221)\r\n                  at org.tensorflow.lite.Tensor.setTo(Tensor.java:93)\r\n                  at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:136)\r\n                  at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:216)\r\n                  at org.tensorflow.lite.Interpreter.run(Interpreter.java:195)\r\n                  at com.example.root.neuralnetworks01.MainActivity.classifyFrame(MainActivity.java:239)\r\n                  at com.example.root.neuralnetworks01.MainActivity$2.onClick(MainActivity.java:169)\r\n```\r\nI suppose an author of the model has created a config file and written something like I wrote bellow:\r\n\r\n   ```\r\n image_resizer {\r\n        fixed_shape_resizer {\r\n                width: 224\r\n                height: 224\r\n        }\r\n    }\r\n```\r\nDo I understand it right?\r\nI think so because 602112 = 224 * 224 * 3 * 4\r\nTherefore I can't use this tflite-file for images with size other than 224*224 pixels. Am I right?\r\nThank you!", "> Therefore I can't use this tflite-file for images with size other than 224*224 pixels. Am I right?\r\n\r\nYou can, but you'll need to manually crop and resize them first. You can look at our samples as a guide for doing this, either by way of [TextureView](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/Camera2BasicFragment.java#L824) or directly with a [Canvas transform](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/android/app/src/main/java/org/tensorflow/demo/ClassifierActivity.java#L110).", "Thanks.\r\n", "how did you solve the problem with  Cannot convert between a TensorFlowLite buffer with 1080000 bytes and a ByteBuffer with 270000 bytes.? @M1thun ?", "> how did you solve the problem with Cannot convert between a TensorFlowLite buffer with 1080000 bytes and a ByteBuffer with 270000 bytes.? @M1thun ?\r\n\r\n@inakaaay  this error occur because of mismatch in the input shape given to the model.", "> Hi @jdduke and @achowdhery ,\r\n> Since the quantized model is not detecting any thing after building over android. I tried my luck with `ssd_mobilenet_v2_coco` model. I trained the model for 20000 training steps.\r\n> \r\n> Then, I converted the trained model to get a frozen graph using `export_tflite_ssd_graph` then converted the .pb file into `tflite`.\r\n> \r\n> I followed your suggestion and have used `inference_type=FLOAT` and made a change accordingly in the DetectorActivity.java file i.e. `private static final boolean TF_OD_API_IS_QUANTIZED = false;`.\r\n> \r\n> Then i build it over phone. The detector app is installed. But the concern is the app is getting crashed after a minute. It shows the detection, but getting crashed.\r\n> \r\n> I used the logcat and found a fatal error about\r\n> \r\n> > 08-27 12:13:54.129 11179 11208 E AndroidRuntime: java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 1080000 bytes and a ByteBuffer with 602112 bytes.\r\n> \r\n> I am not getting I have used float inference and have changed the detector file also then why this is throwing such error.\r\n> \r\n> Note: For configuration I had used image size as 300 and 300. But learning from the log file it is taking somewhere (224 _224_ 3* 4=602112). For the conversion also I had used `--input_shapes=1,300,300,3`.\r\n> \r\n> I am attaching the txt file for your perusal.\r\n> [ssd_mobilenet_v2_27_Aug.txt](https://github.com/tensorflow/tensorflow/files/2323215/ssd_mobilenet_v2_27_Aug.txt)\r\n> \r\n> Could you please share your views on this? How should I rectify this?\r\n> \r\n> Thank you for your help.\r\n==================\r\n----Error is----\r\n E AndroidRuntime: FATAL EXCEPTION: inference\r\n E AndroidRuntime: Process: org.tensorflow.lite.demo, PID: 6556\r\n E AndroidRuntime: java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 5\r\n-----Solution-----\r\nSolved by Adding a \"???\" string inside my Label file, it seems that the App follows the pbtxt format of having the first ID which is 1.\r\n===================", "> > Hi @jdduke and @achowdhery ,\r\n> > Since the quantized model is not detecting any thing after building over android. I tried my luck with `ssd_mobilenet_v2_coco` model. I trained the model for 20000 training steps.\r\n> > Then, I converted the trained model to get a frozen graph using `export_tflite_ssd_graph` then converted the .pb file into `tflite`.\r\n> > I followed your suggestion and have used `inference_type=FLOAT` and made a change accordingly in the DetectorActivity.java file i.e. `private static final boolean TF_OD_API_IS_QUANTIZED = false;`.\r\n> > Then i build it over phone. The detector app is installed. But the concern is the app is getting crashed after a minute. It shows the detection, but getting crashed.\r\n> > I used the logcat and found a fatal error about\r\n> > > 08-27 12:13:54.129 11179 11208 E AndroidRuntime: java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 1080000 bytes and a ByteBuffer with 602112 bytes.\r\n> > \r\n> > \r\n> > I am not getting I have used float inference and have changed the detector file also then why this is throwing such error.\r\n> > Note: For configuration I had used image size as 300 and 300. But learning from the log file it is taking somewhere (224 _224_ 3* 4=602112). For the conversion also I had used `--input_shapes=1,300,300,3`.\r\n> > I am attaching the txt file for your perusal.\r\n> > [ssd_mobilenet_v2_27_Aug.txt](https://github.com/tensorflow/tensorflow/files/2323215/ssd_mobilenet_v2_27_Aug.txt)\r\n> > Could you please share your views on this? How should I rectify this?\r\n> > Thank you for your help.\r\n> > ==================\r\n> > ----Error is----\r\n> > E AndroidRuntime: FATAL EXCEPTION: inference\r\n> > E AndroidRuntime: Process: org.tensorflow.lite.demo, PID: 6556\r\n> > E AndroidRuntime: java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 5\r\n> > -----Solution-----\r\n> > Solved by Adding a \"???\" string inside my Label file, it seems that the App follows the pbtxt format of having the first ID which is 1.\r\n> > ===================\r\n\r\nhie \r\ncould you solve it somehow?\r\nI followed the same process just for SSDmobilenetV1. The app is getting crashed."]}, {"number": 21601, "title": "Enqueuing tensors by value with tf.add no longer works on 1.10", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes (see snippet below)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2 1.10.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.16.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.2\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: See below.\r\n\r\n\r\n### Describe the problem\r\nThere seems to be a possible regression affecting the use of tf.add to enqueue tensors by value instead of by reference within a same device. Check the following snippet.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.Session() as sess:\r\n  v = tf.get_variable('v', shape=(), dtype=tf.int32)\r\n  sess.run(v.assign(123))\r\n  q = tf.FIFOQueue(10, tf.int32)\r\n  sess.run(q.enqueue(tf.add(v, 0)))\r\n  sess.run(v.assign(456))\r\n  print(sess.run(q.dequeue()))\r\n```\r\n\r\nIn previous versions of TensorFlow (last checked was probably 1.8) the above snippet returned 123 since the value of the variable was copied when enqueued thanks to the use of tf.add. However, on TensorFlow 1.10 the snippet returns 456 if using a tf.Session block and 123 if using a tf.InteractiveSession. Enabling or disabling GPU with CUDA_VISIBLE_DEVICES does not seem to affect.\r\n\r\nIs this an intentional change of behavior? If so, is there any new or better way to enqueue tensors by value instead of by reference?", "comments": ["An update on this. It seems that the core problem is not in the behavior of tf.add or enqueuing, but rather in underlying optimizations regarding constant tensors. If in the snippet above I replace the enqueue line with this, it works fine:\r\n\r\n```python\r\nsess.run(q.enqueue(tf.add(v, tf.placeholder_with_default(0, ()))))\r\n```\r\n\r\nI've also found a different scenario where this problem manifests: in shape deduction for ranges.\r\nFor example:\r\n```python\r\nz = tf.zeros((), tf.int32)\r\ntf.range(0, 2 + z).shape  # Returns TensorShape([Dimension(2)]) in 1.10, returned TensorShape([Dimension(None)]) in 10.8\r\n\r\nz = tf.placeholder_with_default(z, z.shape)\r\ntf.range(0, 2 + z).shape  # Returns TensorShape([Dimension(None)])\r\n```\r\n\r\nSo, it seems like using tf.placeholder_with_default is a viable solution for my immediate problem, but this is nevertheless exposing a behavioral change with possible subtle implications (like the enqueue case), and an API behavior inconsistency between tf.Session and tf.InteractiveSession. I leave up to you to decide what to do regarding this bug.", "alextp@ rmlarsen@ can either of you comment on this? Thanks.", "These problems disappear if you use resource variables, so closing as obsolete, as resource variables become default in tfv2.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=21601\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=21601\">No</a>\n"]}, {"number": 21600, "title": "CUDA unified memory fail to increase on demand in r1.10 on PPC64le", "body": "### Description\r\n\r\nBased on the commit to [introduce an option to allocate CUDA unified memory](https://github.com/tensorflow/tensorflow/commit/b1139814f91c5216eb5ff229ee7e1982e5f4e888?diff=split), theoretically we could expect a memory swap between CPU and GPU while running datasets with high memory demands. \r\n\r\nHowever, my tensorflow build still hits OOM. \r\nI was training [official resnet with cifar10 dataset](https://github.com/tensorflow/models/tree/master/official/resnet), with also a large batch ---- 48*128. \r\n\r\nI built another version without CUDA unified memory from commit [fixed ppc64le compile failure libpng](https://github.com/tensorflow/tensorflow/commit/5aefa441276b5fdf2fec5e7cb282630c104f6f4a), \r\nBoth tensorflow build hits OOM when I try to increase batch size from 47\\*128 to 48\\*128. \r\nBased on this comparision, I was convinced that CUDA unified memory is not functioning in my build. \r\n\r\nHowever, I tried to set `per_process_gpu_memory_fraction` to 200 and pass the config to the `tf.Session`, it worked for much larger batches. \r\nThen I realized that the UVM cannot grow on demand. Setting `allow_growth` to True won't help. \r\n\r\nIn a word, can we make the CUDA unified memory allocation grow on demand?\r\n\r\nCould you possibly suggest a reason?\r\nDid I miss out any configurations?\r\n\r\nThank you in advance!\r\n----------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:  source\r\n- **TensorFlow version (use command below)**:  r1.10\r\n- **Python version**:  3.5.2\r\n- **Bazel version (if compiling from source)**:  0.15.0\r\n- **GCC/Compiler version (if compiling from source)**:  gcc (Ubuntu/IBM 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- **CUDA/cuDNN version**:  9.2/7.1\r\n- **GPU model and memory**:   4 x Tesla P100, each has 16G memory; 1T RAM", "comments": ["@smit-hinsu \r\nThank you for the commit [ introduce an option to allocate CUDA unified memory](https://github.com/tensorflow/tensorflow/commit/b1139814f91c5216eb5ff229ee7e1982e5f4e888?diff=split)! \r\n\r\nCould you possibly suggest a reason?\r\nThank you in advance!", "Hi!\r\nI used a script to confirm that CUDA unified memory cannot grow on demand. \r\nFor your information, this is [the script I used](https://github.com/alapha23/playground/blob/master/cat_dog_tf.py)\r\n\r\nFurthermore, I found the GPU memory is poorly utilized when `per_process_gpu_memory_fraction` is set to a large value, e.g. 200. \r\n\r\nIn my case, with `batch_size` 512 and `per_process_gpu_memory_fraction` 200, around 4G gpu memory is utilized out of 16G, while around 360G of RAM is occupied. GPU memory seems to be taken by the model but the gpu memory usage is not large(4G/16G). \r\n\r\nThen I tried to know why we are not fully utilizing GPU memory and `bfc_allocator` might be the reason. \r\nFrom what I am seeing, the implementation of `bfc_allocator` could be improved ---  to be aware of the memory swapping and hence improve performance. \r\n\r\nIn brief, \r\n* Make UVM grow on demand\r\n* Improve bfc_allocator with respect of UVM\r\n\r\nAre they already implemented?\r\nIf not, I believe they should be helpful and intriguing features we could contribute to. \r\n\r\nWhat do you think?\r\n\r\nThank you in advance!", "You could try to switch to PoolAllocator if you suspect it's the problem of BFCAllocator.", "@byronyi Thank you for the reply. \r\n\r\nI am no tensorflow expert but is PoolAllocator still being used? It was mentioned in #8560\r\nI've heard that PoolAllocator is for the brave souls. \r\nFrom [line270](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_process_state.cc), I am afraid BFC allocator is the only valid allocator being used. \r\nOr could you possibly suggest how to safely switch to PoolAllocator?\r\n\r\nIn addition, what is vital, and also the title of this issue is that UVM won't grow on demand. \r\nWhat do you think of it?\r\n\r\nThank you in advance!", "Nagging Assignee @smit-hinsu: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "If I understand the experiment correctly, you are seeing that total memory usage does not go above per_process_gpu_memory_fraction with UVM. Is that correct? That seems to be working as intended and consistent with the mode without UVM.\r\n\r\ncould you describe your use-case requiring total memory to go above per_process_gpu_memory_fraction? wouldn't it be enough to set per_process_gpu_memory_fraction to a very high number and enable allow_growth option?", "@smit-hinsu \r\nThank you for your reply. \r\n\r\nTrue. Total memory usage does not go above per_process_gpu_memory_fraction with UVM.\r\n\r\nIn my experiments, it would be faster when `per_process_gpu_memory_fraction` is smaller. \r\nSetting `per_process_gpu_memory_fraction` to a very high number would make training much much slower which is not applicable. \r\n\r\nThat means I have to find the best `per_process_gpu_memory_fraction` with many times of trying and failing. \r\n\r\nCan you possibly make the total memory allocation(GPU+CPU) grow automatically when we need more memory?\r\n\r\nIn addition, could you possibly suggest:\r\n- Which memory(CPU or GPU memory) would be first used when we set `per_process_gpu_memory_fraction`  to a high number?\r\n    - As I recognized, gpu memory would not be fully utilized when we set `per_process_gpu_memory_fraction`  to a high number\r\n- What is `allow_growth` doing?\r\n\r\nThank you in advance!", "Did you try setting per_process_gpu_memory_fraction to a high number along with enabling allow_growth option?\r\n\r\nIt should make the total memory allocation(GPU+CPU) to grow automatically when more memory is required as you requested. See allow_growth documentation at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L36.\r\n\r\nMemory is first allocated on CPU and migrated to GPU on demand. It is expected that training will become slower with unified memory as size of the working memory increases. It would generate many faults and stall the training process until memory is moved to GPU.\r\n\r\nSee cuda unified memory documentation for more details:\r\nhttps://devblogs.nvidia.com/maximizing-unified-memory-performance-cuda/\r\nhttp://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf\r\n\r\nHope this helps!", "@smit-hinsu \r\nProblem solved. Thank you very much!"]}, {"number": 21599, "title": "how to remove nodes from restored graph?", "body": "hi, all:\r\n   I restored a graph from a meta file.\r\n```python\r\n    saver = tf.train.import_meta_graph(LOCAL_TRAINED_MODEL_DIR+'/saved_model-0.meta')\r\n    saver.restore(sess, tf.train.latest_checkpoint(LOCAL_TRAINED_MODEL_DIR))\r\n    graph = tf.get_default_graph()\r\n```\r\n\r\n\r\nhow could I delete some nodes from the default graph? The nodes can be accessed by get_tensor_by_name(). and could I reuse the trained weights please? \r\n    Thanks!\r\n   ", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21598, "title": "Dilation of keras.Conv2DTranspose does not do anything", "body": "### Describe the problem\r\nWell using tf.keras.Conv2DTranspose I noticed the dilation function did not dilate. No matter what size of dilation you choose the result is the same.\r\n\r\nI did found the problem in keras as well but I did not now if I should report it here too: https://github.com/keras-team/keras/issues/8159\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: v8.0\r\n- **GPU model and memory**: GeForce 1050\r\n- **Exact command to reproduce**: See below\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nConv2D = tf.keras.layers.Conv2D\r\nConv2DTranspose = tf.keras.layers.Conv2DTranspose\r\nInput = tf.keras.layers.Input\r\nModel = tf.keras.Model\r\n\r\ninput = Input(shape=(None, None, 1), name='input')\r\nx = Conv2DTranspose(8, (2, 2), dilation_rate=2, padding='valid')(input)\r\nx = Conv2DTranspose(8, (3, 3), dilation_rate=2, padding='valid')(x)\r\nx = Conv2DTranspose(8, (3, 3), dilation_rate=2, padding='valid')(x)\r\n\r\nmodel_dilation = Model(inputs=input, outputs=x, name='dil')\r\n\r\nx = Conv2DTranspose(8, (2, 2), dilation_rate=1, padding='valid')(input)\r\nx = Conv2DTranspose(8, (3, 3), dilation_rate=1, padding='valid')(x)\r\nx = Conv2DTranspose(8, (3, 3), dilation_rate=1, padding='valid')(x)\r\n\r\nmodel_no_dilation = Model(inputs=input, outputs=x, name='no_dil')\r\n\r\noutput_dil = model_dilation.predict(np.zeros((1, 5, 5, 1)))\r\noutput_no_dil = model_no_dilation.predict(np.zeros((1, 5, 5, 1)))\r\n\r\nprint(\"shape dil\", np.shape(output_dil))\r\nprint(\"shape no dil\", np.shape(output_no_dil))\r\n```\r\n\r\nresult:\r\n```\r\nshape dil (1, 10, 10, 8)\r\nshape no dil (1, 10, 10, 8)\r\n```\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nExact command to reproduce\nMobile device", "Good bot\r\nEverything is updated", "@wagenrace @robieta I have sent #22265 to fix this, would you mind to take a look? Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=21598\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=21598\">No</a>\n"]}, {"number": 21597, "title": "In Tensorflow 1.10 Keras models with datasets not working, gives  TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float64 of argument 'x'", "body": "Please go to Stack Overflow for help and support:\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\nIf you open a GitHub issue, here is our policy:\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nGoogle Colab \r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue hap\r\npens on mobile device**: Web\r\n- **TensorFlow installed from (source or binary)**:Binary\r\n- **TensorFlow version (use command below)**:1.10\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: \r\nhttps://www.tensorflow.org/guide/keras\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\nimport os\r\ndata =np.random.random((1000,32))\r\nlabels =np.random.random((1000,10))\r\ndataset1 = tf.data.Dataset.from_tensor_slices((data, labels))\r\ndataset1 = dataset1.batch(32)\r\ndataset1 = dataset1.repeat()\r\nmodel = keras.Sequential()  \r\nmodel.add(keras.layers.Dense(64,activation='relu'))\r\nmodel.add(keras.layers.Dense(64,activation='relu'))\r\nmodel.add(keras.layers.Dense(10,activation='softmax'))\r\n\r\nmodel.compile(optimizer=tf.train.AdamOptimizer(0.001),\r\n          loss='categorical_crossentropy',\r\n          metrics=['accuracy'])\r\nmodel.fit(dataset1 ,epochs=10,steps_per_epoch=30)\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nTypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float64 of argument 'x'.\r\nIf I add type 00,32))\r\ndata.astype(np.float32) does not help, looks like dataset is broken in 1.10\r\nl\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    544                   \"%s type %s of argument '%s'.\" %\r\n    545                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\r\n--> 546                    inferred_from[input_arg.type_attr]))\r\n    547 \r\n    548           types = [values.dtype]\r\n\r\nTypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float64 of argument 'x'.\r\n### Source code / logs\r\n\r\n[Stackoverflow question](https://stackoverflow.com/questions/51814224/tensorflow-keras-sample-code-throws-typeerror-input-y-of-mul-op-has-type)\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["It looks like there is a datatype mismatch-- this is likely a mismatch between the Numpy default float64 and elsewhere in the code. Can you--\r\n1. Change the use of keras to tf.keras... rather than using from tensorflow import keras, and\r\n2. Include the full stack trace so that we can narrow down where the problem is occurring? ", "Nagging Assignee @karmel: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21596, "title": "TFLite\uff1aHow to generate partial quantized model", "body": "I only found the way to generate fully quantized model.\r\nhttps://www.tensorflow.org/performance/quantization\r\n\r\nBut for high accuracy, maybe we should retrain the network by adding fake quantization for the front part and keeping the last part unchanged. Finally we'd like to generate such partial quantized TFLITE model for mobile application.\r\nIs there any tool to implement except merging two separately generated models manually ?\r\n\r\nThanks", "comments": ["Currently the TFLite converter doesn't support partially quantizing a model. We understand the need for this and plan to add support as we ramp up on other methods of quantization techniques.\r\n\r\nSo there isn't yet a tool for what you ask, but tools for better quantization options are on our roadmap.\r\n\r\nThanks!", "Since we are actively working on new tooling and this is on our roadmap I will close this issue, feel free to reopen if you have a specific related question about this. Thank you!"]}, {"number": 21595, "title": "batch_normalization layers docs (issue #21229)", "body": "Addresses issue #21229. Changes Notes to Cautions as batch normalization layers do not work properly without taking into account the caution in the Note.", "comments": ["Sorry, I forgot to tag relevant people:\r\n@nealwu \r\nCould you have a look at this please? Thanks in advance!", "@nealwu Could you take a look at this please?", "Nagging Reviewer @nealwu: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 59 days with no activity and the `awaiting review` label has been applied."]}, {"number": 21594, "title": "memory_layer bug", "body": "In Tensorflow 1.10, the following lines conflict, since `None` has no attribute `dtype`.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5326a7fb028d6a66286e9e929dc9454c12e81522/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L176\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5326a7fb028d6a66286e9e929dc9454c12e81522/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L198\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5326a7fb028d6a66286e9e929dc9454c12e81522/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L204", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "The documentation for the function calls out that if the `memory_layer` is None, the memory should assume the shape of the `query_layer` and vice-versa.  However the code ever evaluates what happens when both memory and query layers are not specified. ", "Here is a maybe patch:\r\n```\r\ndiff --git a/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py b/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\r\nindex 1c9d179e3c..d1d191691a 100644\r\n--- a/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\r\n+++ b/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\r\n@@ -185,6 +185,9 @@ class _BaseAttentionMechanism(AttentionMechanism):\r\n         `memory_sequence_length` is not None.\r\n       name: Name to use when creating ops.\r\n     \"\"\"\r\n+    if (query_layer is None and memory_layer is None):\r\n+      raise TypeError(\r\n+        \"either \\\"query_layer\\\" or \\\"memory_layer\\\" should be specified\")\r\n     if (query_layer is not None\r\n         and not isinstance(query_layer, layers_base.Layer)):\r\n       raise TypeError(\r\n@@ -195,13 +198,12 @@ class _BaseAttentionMechanism(AttentionMechanism):\r\n           \"memory_layer is not a Layer: %s\" % type(memory_layer).__name__)\r\n     self._query_layer = query_layer\r\n     self._memory_layer = memory_layer\r\n-    self.dtype = memory_layer.dtype\r\n+    self.dtype = memory_layer.dtype if memory_layer else query_layer.dtype\r\n     if not callable(probability_fn):\r\n       raise TypeError(\"probability_fn must be callable, saw type: %s\" %\r\n                       type(probability_fn).__name__)\r\n     if score_mask_value is None:\r\n-      score_mask_value = dtypes.as_dtype(\r\n-          self._memory_layer.dtype).as_numpy_dtype(-np.inf)\r\n+      score_mask_value = dtypes.as_dtype(self.dtype).as_numpy_dtype(-np.inf)\r\n     self._probability_fn = lambda score, prev: (  # pylint:disable=g-long-lambda\r\n         probability_fn(\r\n             _maybe_mask_score(score, memory_sequence_length, score_mask_value),\r\n```", "@tensorflowbutler Could you please remove the label:\"stat:awaiting response\", since I have closed this issue.", "@jiarenyf : Has this been resolved in latest? Do you guys not elaborate why an issue gets closed?"]}, {"number": 21593, "title": "[INTEL MKL] Code cleanup", "body": "Renamed INTEL_MKL_ML uses that were missed by previous commit https://github.com/tensorflow/tensorflow/commit/a8e78e2e617b6ca10f4878fe99fdf43ddedfa7c6 . Also added deprecation warning for INTEL_MKL_ML_ONLY. The DNN APIs in MKL ML have not been updated in a while and all new optimizations and enhancements are done in MKL DNN open source. ", "comments": []}, {"number": 21592, "title": "TF lite can build & run on Linux?", "body": "As the title says, is tf lite can be built and ran on non-mobile environment, such as Linux or others:\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21591, "title": "How to solve No OpKernel was registered to support Op 'CTCLoss'", "body": "Now I put the trained model on the android device, suggesting that there is no \"No OpKernel was registered to support Op 'CTCLoss' with these attrs.\" How can I get him support Op 'CTCLoss'\r\nMy PC is Ubuntu 16.04 and AndroidStudio is 3.0.\r\nTensotflow is 1.6\r\n\r\n-------------------------------------------------- ----------------------------\r\nThe error message is as follows:\r\n\r\n08-14 10:43:30.332 11166-11166/com.example.hc.digitalgesturerecognition E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[the_input], outputs:[output_node0]\r\n08-14 10:43:30.336 11166-11166/com.example.hc.digitalgesturerecognition E/AndroidRuntime: FATAL EXCEPTION: main\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Process: com.example.hc.digitalgesturerecognition, PID: 11166\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'CTCLoss' with these attrs. Registered devices: [CPU], Registered kernels:\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<no registered kernels>\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[[Node: ctc/CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false](ctc/Log, ctc/ToInt64, ctc/ToInt32_2, ctc/ToInt32_1)]]\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0At org.tensorflow.Session.run(Native Method)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0At org.tensorflow.Session.access$100(Session.java:48)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0At org.tensorflow.Session$Runner.runHelper(Session.java:298)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0At org.tensorflow.Session$Runner.run(Session.java:248)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0At org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0At org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0At org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:187)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0At com.example.hc.digitalgesturerecognition.Classifier.predict(Classifier.java:115)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0At com.example.hc.digitalgesturerecognition.CameraActivity$2.onImageAvailable(CameraActivity.java:285)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0At android.media.ImageReader$ListenerHandler.handleMessage(ImageReader.java:812)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0At android.os.Handler.dispatchMessage(Handler.java:108)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0At android.os.Looper.loop(Looper.java:166)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0At android.app.ActivityThread.main(ActivityThread.java:7425)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0At java.lang.reflect.Method.invoke(Native Method)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0At com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:245)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0At com.android.internal.os.ZygoteInit.main(ZygoteInit.java:921)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@tensorflowbutler \r\n\r\nThank you, it is like this\r\n\r\nIn Tensorflow's SO file, I didn't write my own code.\r\n\r\nMy development environment is Ubuntu 16.04, and the Android device is HUAWEI MATE 10 (Android 8.0).\r\n\r\nTensorFlow is installed using PIP\r\n\r\nTensorflow version is 1.6\r\n\r\nBazel is new master\r\n\r\nCUDA Version 9.2.148\r\n\r\nNVIDIA GeForce GTX 860M GPU memory is 4GB", "We are deprecating the mainline TensorFlow build on Android, so closing this, but this Stack Overflow answer may help:\r\nhttps://stackoverflow.com/questions/44938809/how-to-add-operations-into-tensorflow-android-build"]}, {"number": 21590, "title": "All metrics evaluate to 0.0 when using  tf.contrib.estimator.InMemoryEvaluatorHook", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Slight modification to cnn_mnist.py to include `InMemoryEvaluatorHook` . Original code can be found here [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A\r\n- **TensorFlow installed from (source or binary)**: binary (i think)\r\n- **TensorFlow version (use command below)**:1.9.0 (with PR#20822)\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**:7.5\r\n- **GPU model and memory**: 2x TITAN X (12189MiB) & Quadro P5000 (16273MiB)\r\n- **Exact command to reproduce**:  python3 cnn_mnist.py\r\n\r\n### Describe the problem\r\nWhen evaluating during training with `tf.contrib.estimator.InMemoryEvaluatorHook`, all evaluation metrics are evaluated to 0.0 after the very first time the evaluation hook is triggered. My suspicion is that it is caused by the use of `tf.estimator.inputs.numpy_input_fn`.\r\n\r\n### Source code:\r\nAttached is the script I used. Basically, just added the following line:\r\n` evaluator = tf.contrib.estimator.InMemoryEvaluatorHook(estimator=mnist_classifier, input_fn=eval_input_fn, every_n_iter=50)` and passed the hook to `mnist_classifer.train()`\r\n\r\n[cnn_mnist.txt](https://github.com/tensorflow/tensorflow/files/2285277/cnn_mnist.txt)\r\n\r\n\r\nHere is the relevant output from running the training for 200 steps:\r\n\r\nINFO:tensorflow:Starting evaluation at 2018-08-14-02:15:35\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Finished evaluation at 2018-08-14-02:15:37\r\nINFO:tensorflow:Saving dict for global step 0: accuracy = 0.0915, global_step = 0, loss = 2.30028\r\nINFO:tensorflow:loss = 2.29529, step = 0\r\n...\r\nINFO:tensorflow:Starting evaluation at 2018-08-14-02:15:37\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Finished evaluation at 2018-08-14-02:15:37\r\nINFO:tensorflow:Saving dict for global step 50: accuracy = 0.0, global_step = 50, loss = 0.0\r\n...\r\nINFO:tensorflow:Starting evaluation at 2018-08-14-02:15:38\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Finished evaluation at 2018-08-14-02:15:38\r\nINFO:tensorflow:Saving dict for global step 100: accuracy = 0.0, global_step = 100, loss = 0.0\r\nINFO:tensorflow:global_step/sec: 103.013\r\nINFO:tensorflow:loss = 2.27342, step = 100 (0.971 sec)\r\n...\r\nINFO:tensorflow:Starting evaluation at 2018-08-14-02:15:38\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Finished evaluation at 2018-08-14-02:15:38\r\nINFO:tensorflow:Saving dict for global step 150: accuracy = 0.0, global_step = 150, loss = 0.0\r\n...\r\nINFO:tensorflow:Starting evaluation at 2018-08-14-02:15:39\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Finished evaluation at 2018-08-14-02:15:39\r\nINFO:tensorflow:Saving dict for global step 200: accuracy = 0.0, global_step = 200, loss = 0.0\r\n...\r\nINFO:tensorflow:Saving checkpoints for 200 into ./tmp/mnist_convnet_model/model.ckpt.\r\nINFO:tensorflow:Starting evaluation at 2018-08-14-02:15:39\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Finished evaluation at 2018-08-14-02:15:39\r\nINFO:tensorflow:Saving dict for global step 200: accuracy = 0.0, global_step = 200, loss = 0.0\r\nINFO:tensorflow:Loss for final step: 2.25765.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\n...\r\nINFO:tensorflow:Starting evaluation at 2018-08-14-02:15:40\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from ./tmp/mnist_convnet_model/model.ckpt-200\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Finished evaluation at 2018-08-14-02:15:40\r\nINFO:tensorflow:Saving dict for global step 200: accuracy = 0.2611, global_step = 200, loss = 2.25732\r\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: ./tmp/mnist_convnet_model/model.ckpt-200\r\n{'global_step': 200, 'loss': 2.2573187, 'accuracy': 0.26109999}\r\n", "comments": ["Another experiment does point to the root cause being the `num_epochs=1` argument in `tf.estimator.inputs.numpy_input_fn()`\r\n\r\nThe evaluation metrics get evaluated properly if the input_fn is defined like this:\r\n```\r\n  eval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n      x={\"x\": eval_data},\r\n      y=eval_labels,\r\n      num_epochs=None,\r\n      shuffle=False)\r\n```\r\nAnd of course to avoid an infinite loop, one must define the steps argument in `InMemoryEvaluatorHook`:\r\n\r\n`evaluator = tf.contrib.estimator.InMemoryEvaluatorHook(estimator=mnist_classifier, input_fn=eval_input_fn, every_n_iter=50, steps 1000)`\r\n\r\nOutput with these changes looks like this:\r\nINFO:tensorflow:Saving dict for global step 0: accuracy = 0.0903359, global_step = 0, loss = 2.30868\r\nINFO:tensorflow:Saving dict for global step 50: accuracy = 0.114852, global_step = 50, loss = 2.301\r\nINFO:tensorflow:Saving dict for global step 100: accuracy = 0.149109, global_step = 100, loss = 2.29303\r\nINFO:tensorflow:Saving dict for global step 150: accuracy = 0.196461, global_step = 150, loss = 2.28513\r\nINFO:tensorflow:Saving dict for global step 200: accuracy = 0.23625, global_step = 200, loss = 2.27739", "We currently don't have anybody working on this. It would be great if you could help us by working on this and submitting a PR. Let us know if you need further clarification. Thanks!\r\n", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Contrib has been depreciated here in Tensorflow repo and moved to tensorflow/addons , please reopen the issue in https://github.com/tensorflow/addons/issues if this exists in latest version. Thank you"]}, {"number": 21589, "title": "[Java] Extend UInt8 class from Number", "body": "There is an issue right now in the Java Ops API. To enforce that in some operations the data type of an operand must be of a real-number data type, the signature of those operands only accepts generics that extends from `Number`.\r\n\r\nFor example, `DecodePng` only returns the decode image as a tensor of real numbers, so it is safe (and wanted) to declare this operation class as:\r\n```\r\npublic final class DecodePng<T extends Number> extends PrimitiveOp implements Operand<T>\r\n```\r\n\r\nThat works well with Java standard types but it should work also with Tensorflow real-number custom types, such as `UInt8`. So I added a package-private base class that implements `Number` (called `TFNumber`) that is easy to extend from our real-number custom types, `UInt8` in this case.\r\n\r\nCC: @asimshankar ", "comments": ["Exactly, you cannot instantiate a `UInt8` object, thus those methods will never be invoked.\r\n\r\nI created the `TFNumber` class to do that \u201ctrick\u201d only once if any more custom types are being added but I don\u2019t mind removing it.\r\n\r\nUpdate: Just did it."]}, {"number": 21588, "title": " ./tensorflow/contrib/lite/build_rpi_lib.sh /bin/sh: 1: [[: not found make: arm-linux-gnueabihf-: Command not found", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04 64bit\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:  NO devices\r\n- **TensorFlow installed from (source or binary)**:  pip install tensorflow-gpu==1.9\r\n- **TensorFlow version (use command below)**:  1.9 gpu\r\n- **Python version**:   python2.7\r\n- **Bazel version (if compiling from source)**: NO Bazel\r\n- **GCC/Compiler version (if compiling from source)**:   gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- **CUDA/cuDNN version**:  cuda9.0 cudnn7.0\r\n- **GPU model and memory**: gtx1070\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\n### Describe the problem\r\nget help from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/rpi.md\r\n\r\nmy work Dir is :  /home/icare/yqli/tensorflow\r\n\r\nI can successfully run the  :     \r\n./tensorflow/contrib/lite/download_dependencies.sh\r\nchmod 777 ./tensorflow/contrib/lite/build_rpi_lib.sh\r\n\r\nbut when I run the  :   \r\n./tensorflow/contrib/lite/build_rpi_lib.sh\r\n\r\nerros occurs \uff1a\r\nicare@icare-5F:tensorflow$ ./tensorflow/contrib/lite/build_rpi_lib.sh\r\n+ set -e\r\n+++ dirname ./tensorflow/contrib/lite/build_rpi_lib.sh\r\n++ cd ./tensorflow/contrib/lite\r\n++ pwd\r\n+ SCRIPT_DIR=/home/icare/yqli/tensorflow/tensorflow/contrib/lite\r\n+ cd /home/icare/yqli/tensorflow/tensorflow/contrib/lite/../../..\r\n+ CC_PREFIX=arm-linux-gnueabihf-\r\n+ make -j 3 -f tensorflow/contrib/lite/Makefile TARGET=RPI TARGET_ARCH=armv7\r\n/bin/sh: 1: [[: not found\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/bin/rpi_armv7/benchmark_model\r\narm-linux-gnueabihf- g++ --std=c++11 -O3 -DNDEBUG -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/../../../ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/allocation.cc -o /home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/allocation.o\r\narm-linux-gnueabihf- g++ --std=c++11 -O3 -DNDEBUG -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/../../../ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/arena_planner.cc -o /home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/arena_planner.o\r\nmake: arm-linux-gnueabihf-: Command not found\r\narm-linux-gnueabihf- gcc --std=c++11 -O3 -DNDEBUG -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/../../../ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/context.c -o /home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/context.o\r\nmake: arm-linux-gnueabihf-: Command not found\r\ntensorflow/contrib/lite/Makefile:203: recipe for target '/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/allocation.o' failed\r\nmake: *** [/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/allocation.o] Error 127\r\nmake: *** Waiting for unfinished jobs....\r\ntensorflow/contrib/lite/Makefile:203: recipe for target '/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/arena_planner.o' failed\r\nmake: *** [/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/arena_planner.o] Error 127\r\nmake: arm-linux-gnueabihf-: Command not found\r\ntensorflow/contrib/lite/Makefile:207: recipe for target '/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/context.o' failed\r\nmake: *** [/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/context.o] Error 127\r\n\r\nIt seems that the \u201carm-linux-gnueabihf- g++\u201d can not be found,\r\n\r\nicare@icare-5F:tensorflow$ arm-linux-gnueabihf- g++\r\narm-linux-gnueabihf-: command not found\r\n\r\nBut it can find \"arm-linux-gnueabihf-g++\"\r\nicare@icare-5F:tensorflow$ arm-linux-gnueabihf-g++\r\narm-linux-gnueabihf-g++: fatal error: no input files\r\ncompilation terminated.\r\n\r\nwhy  there is a space between the \"arm-linux-gnueabihf-\" and \"g++\",\r\nhow to solve the problem?\r\n\r\nI have never modified the build_rpi_lib.sh and teh Makefile   \r\n                                                                                                         \r\n\r\nthanks for your time", "comments": ["I  modified the tensorflow/contrib/lite/Makefile\r\nfrom CXX := $(CC_PREFIX) ${TARGET_TOOLCHAIN_PREFIX}g++\r\nto CXX := $(CC_PREFIX)${TARGET_TOOLCHAIN_PREFIX}g++\r\nso  it can successfully build the code, but new problems occurred:\r\n\r\nicare@icare-5F:tensorflow$ ./tensorflow/contrib/lite/build_rpi_lib.sh \r\n+ set -e\r\n+++ dirname ./tensorflow/contrib/lite/build_rpi_lib.sh\r\n++ cd ./tensorflow/contrib/lite\r\n++ pwd\r\n+ SCRIPT_DIR=/home/icare/yqli/tensorflow/tensorflow/contrib/lite\r\n+ cd /home/icare/yqli/tensorflow/tensorflow/contrib/lite/../../..\r\n+ CC_PREFIX=arm-linux-gnueabihf-\r\n+ make -j 3 -f tensorflow/contrib/lite/Makefile TARGET=RPI TARGET_ARCH=armv7\r\n/bin/sh: 1: [[: not found\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/bin/rpi_armv7/benchmark_model\r\narm-linux-gnueabihf-g++ --std=c++11 -O3 -DNDEBUG -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/../../../ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/interpreter.cc -o /home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/interpreter.o\r\narm-linux-gnueabihf-g++ --std=c++11 -O3 -DNDEBUG -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/../../../ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/kernels/dequantize.cc -o /home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/kernels/dequantize.o\r\narm-linux-gnueabihf-g++ --std=c++11 -O3 -DNDEBUG -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/../../../ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/kernels/depthwise_conv.cc -o /home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/kernels/depthwise_conv.o\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h: In member function \u2018bool tflite::QuantizationParameters::Verify(flatbuffers::Verifier&) const\u2019:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1724:33: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<float>*)\u2019\r\n            verifier.Verify(min()) &&\r\n                                 ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1724:31: note:   cannot convert \u2018tflite::QuantizationParameters::min()\u2019 (type \u2018const flatbuffers::Vector<float>*\u2019) to type \u2018size_t {aka unsigned int\u2019\r\n            verifier.Verify(min()) &&\r\n                               ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1724:31: note:   cannot convert \u2018tflite::QuantizationParameters::min()\u2019 (type \u2018const flatbuffers::Vector<float>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(min()) &&\r\n                               ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1726:33: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<float>*)\u2019\r\n            verifier.Verify(max()) &&\r\n                                 ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1726:31: note:   cannot convert \u2018tflite::QuantizationParameters::max()\u2019 (type \u2018const flatbuffers::Vector<float>*\u2019) to type \u2018size_t {aka unsigned int\u2019\r\n            verifier.Verify(max()) &&\r\n                               ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1726:31: note:   cannot convert \u2018tflite::QuantizationParameters::max()\u2019 (type \u2018const flatbuffers::Vector<float>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(max()) &&\r\n                               ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1728:35: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<float>*)\u2019\r\n            verifier.Verify(scale()) &&\r\n                                   ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1728:33: note:   cannot convert \u2018tflite::QuantizationParameters::scale()\u2019 (type \u2018const flatbuffers::Vector<float>*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(scale()) &&\r\n                                 ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1728:33: note:   cannot convert \u2018tflite::QuantizationParameters::scale()\u2019 (type \u2018const flatbuffers::Vector<float>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(scale()) &&\r\n                                 ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1730:40: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<long long int>*)\u2019\r\n            verifier.Verify(zero_point()) &&\r\n                                        ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1730:38: note:   cannot convert \u2018tflite::QuantizationParameters::zero_point()\u2019 (type \u2018const flatbuffers::Vector<long long int>*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(zero_point()) &&\r\n                                      ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1730:38: note:   cannot convert \u2018tflite::QuantizationParameters::zero_point()\u2019 (type \u2018const flatbuffers::Vector<long long int>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(zero_point()) &&\r\n                                      ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h: In member function \u2018bool tflite::Tensor::Verify(flatbuffers::Verifier&) const\u2019:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1841:35: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<int>*)\u2019\r\n            verifier.Verify(shape()) &&\r\n                                   ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1841:33: note:   cannot convert \u2018tflite::Tensor::shape()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(shape()) &&\r\n                                 ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1841:33: note:   cannot convert \u2018tflite::Tensor::shape()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*\u2019\r\n            verifier.Verify(shape()) &&\r\n                                 ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1845:34: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::String*)\u2019\r\n            verifier.Verify(name()) &&\r\n                                  ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1845:32: note:   cannot convert \u2018tflite::Tensor::name()\u2019 (type \u2018const flatbuffers::String*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(name()) &&\r\n                                ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1845:32: note:   cannot convert \u2018tflite::Tensor::name()\u2019 (type \u2018const flatbuffers::String*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(name()) &&\r\n                                ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h: In member function \u2018bool tflite::ConcatEmbeddingsOptions::Verify(flatbuffers::Verifier&) const\u2019:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:2287:53: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<int>*)\u2019\r\n            verifier.Verify(num_columns_per_channel()) &&\r\n                                                     ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:2287:51: note:   cannot convert \u2018tflite::ConcatEmbeddingsOptions::num_columns_per_channel()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(num_columns_per_channel()) &&\r\n                                                   ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:2287:51: note:   cannot convert \u2018tflite::ConcatEmbeddingsOptions::num_columns_per_channel()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(num_columns_per_channel()) &&\r\n                                                   ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h:2289:55: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<int>*)\u2019\r\n            verifier.Verify(embedding_dim_per_channel()) &&\r\n                                                       ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:2289:53: note:   cannot convert \u2018tflite::ConcatEmbeddingsOptions::embedding_dim_per_channel()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(embedding_dim_per_channel()) &&\r\n                                                     ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:2289:53: note:   cannot convert \u2018tflite::ConcatEmbeddingsOptions::embedding_dim_per_channel()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(embedding_dim_per_channel()) &&\r\n                                                     ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h: In member function \u2018bool tflite::ReshapeOptions::Verify(flatbuffers::Verifier&) const\u2019:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:3387:39: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<int>*)\u2019\r\n            verifier.Verify(new_shape()) &&\r\n                                       ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:3387:37: note:   cannot convert \u2018tflite::ReshapeOptions::new_shape()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(new_shape()) &&\r\n                                     ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:3387:37: note:   cannot convert \u2018tflite::ReshapeOptions::new_shape()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(new_shape()) &&\r\n                                     ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h: In member function \u2018bool tflite::SqueezeOptions::Verify(flatbuffers::Verifier&) const\u2019:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:4051:42: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<int>*)\u2019\r\n            verifier.Verify(squeeze_dims()) &&\r\n                                          ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:4051:40: note:   cannot convert \u2018tflite::SqueezeOptions::squeeze_dims()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(squeeze_dims()) &&\r\n                                        ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:4051:40: note:   cannot convert \u2018tflite::SqueezeOptions::squeeze_dims()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(squeeze_dims()) &&\r\n                                        ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h: In member function \u2018bool tflite::OperatorCode::Verify(flatbuffers::Verifier&) const\u2019:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5332:41: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::String*)\u2019\r\n            verifier.Verify(custom_code()) &&\r\n                                         ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5332:39: note:   cannot convert \u2018tflite::OperatorCode::custom_code()\u2019 (type \u2018const flatbuffers::String*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(custom_code()) &&\r\n                                       ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5332:39: note:   cannot convert \u2018tflite::OperatorCode::custom_code()\u2019 (type \u2018const flatbuffers::String*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(custom_code()) &&\r\n                                       ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h: In member function \u2018bool tflite::Operator::Verify(flatbuffers::Verifier&) const\u2019:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5621:36: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<int>*)\u2019\r\n            verifier.Verify(inputs()) &&\r\n                                    ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5621:34: note:   cannot convert \u2018tflite::Operator::inputs()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(inputs()) &&\r\n                                  ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5621:34: note:   cannot convert \u2018tflite::Operator::inputs()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(inputs()) &&\r\n                                  ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5623:37: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<int>*)\u2019\r\n            verifier.Verify(outputs()) &&\r\n                                     ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5623:35: note:   cannot convert \u2018tflite::Operator::outputs()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(outputs()) &&\r\n                                   ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5623:35: note:   cannot convert \u2018tflite::Operator::outputs()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(outputs()) &&\r\n                                   ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5628:44: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<unsigned char>*)\u2019\r\n            verifier.Verify(custom_options()) &&\r\n                                            ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5628:42: note:   cannot convert \u2018tflite::Operator::custom_options()\u2019 (type \u2018const flatbuffers::Vector<unsigned char>*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(custom_options()) &&\r\n                                          ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5628:42: note:   cannot convert \u2018tflite::Operator::custom_options()\u2019 (type \u2018const flatbuffers::Vector<unsigned char>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(custom_options()) &&\r\n                                          ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5631:54: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<unsigned char>*)\u2019\r\n            verifier.Verify(mutating_variable_inputs()) &&\r\n                                                      ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5631:52: note:   cannot convert \u2018tflite::Operator::mutating_variable_inputs()\u2019 (type \u2018const flatbuffers::Vector<unsigned char>*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(mutating_variable_inputs()) &&\r\n                                                    ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5631:52: note:   cannot convert \u2018tflite::Operator::mutating_variable_inputs()\u2019 (type \u2018const flatbuffers::Vector<unsigned char>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(mutating_variable_inputs()) &&\r\n                                                    ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h: In member function \u2018bool tflite::SubGraph::Verify(flatbuffers::Verifier&) const\u2019:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5994:37: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<flatbuffers::Offset<tflite::Tensor> >*)\u2019\r\n            verifier.Verify(tensors()) &&\r\n                                     ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5994:35: note:   cannot convert \u2018tflite::SubGraph::tensors()\u2019 (type \u2018const flatbuffers::Vector<flatbuffers::Offset<tflite::Tensor> >*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(tensors()) &&\r\n                                   ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5994:35: note:   cannot convert \u2018tflite::SubGraph::tensors()\u2019 (type \u2018const flatbuffers::Vector<flatbuffers::Offset<tflite::Tensor> >*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(tensors()) &&\r\n                                   ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5997:36: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<int>*)\u2019\r\n            verifier.Verify(inputs()) &&\r\n                                    ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5997:34: note:   cannot convert \u2018tflite::SubGraph::inputs()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(inputs()) &&\r\n                                  ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5997:34: note:   cannot convert \u2018tflite::SubGraph::inputs()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(inputs()) &&\r\n                                  ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5999:37: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<int>*)\u2019\r\n            verifier.Verify(outputs()) &&\r\n                                     ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5999:35: note:   cannot convert \u2018tflite::SubGraph::outputs()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(outputs()) &&\r\n                                   ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:5999:35: note:   cannot convert \u2018tflite::SubGraph::outputs()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(outputs()) &&\r\n                                   ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6001:39: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<flatbuffers::Offset<tflite::Operator> >*)\u2019\r\n            verifier.Verify(operators()) &&\r\n                                       ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6001:37: note:   cannot convert \u2018tflite::SubGraph::operators()\u2019 (type \u2018const flatbuffers::Vector<flatbuffers::Offset<tflite::Operator> >*\u2019) to type size_t {aka unsigned int}\u2019\r\n            verifier.Verify(operators()) &&\r\n                                     ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6001:37: note:   cannot convert \u2018tflite::SubGraph::operators()\u2019 (type \u2018const flatbuffers::Vector<flatbuffers::Offset<tflite::Operator> >*\u2019) to type const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(operators()) &&\r\n                                     ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6004:34: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::String*)\u2019\r\n            verifier.Verify(name()) &&\r\n                                  ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6004:32: note:   cannot convert \u2018tflite::SubGraph::name()\u2019 (type \u2018const flatbuffers::String*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(name()) &&\r\n                                ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6004:32: note:   cannot convert \u2018tflite::SubGraph::name()\u2019 (type \u2018const flatbuffers::String*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(name()) &&\r\n                                ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h: In member function \u2018bool tflite::Buffer::Verify(flatbuffers::Verifier&) const\u2019:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6094:34: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<unsigned char>*)\u2019\r\n            verifier.Verify(data()) &&\r\n                                  ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6094:32: note:   cannot convert \u2018tflite::Buffer::data()\u2019 (type \u2018const flatbuffers::Vector<unsigned char>*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(data()) &&\r\n                                ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6094:32: note:   cannot convert \u2018tflite::Buffer::data()\u2019 (type \u2018const flatbuffers::Vector<unsigned char>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(data()) &&\r\n                                ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h: In member function \u2018bool tflite::Model::Verify(flatbuffers::Verifier&) const\u2019:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6183:44: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<flatbuffers::Offset<tflite::OperatorCode> >*)\u2019\r\n            verifier.Verify(operator_codes()) &&\r\n                                            ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6183:42: note:   cannot convert \u2018tflite::Model::operator_codes()\u2019 (type \u2018const flatbuffers::Vector<flatbuffers::Offset<tflite::OperatorCode> >*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(operator_codes()) &&\r\n                                          ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6183:42: note:   cannot convert \u2018tflite::Model::operator_codes()\u2019 (type \u2018const flatbuffers::Vector<flatbuffers::Offset<tflite::OperatorCode> >*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(operator_codes()) &&\r\n                                          ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6186:39: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<flatbuffers::Offset<tflite::SubGraph> >*)\u2019\r\n            verifier.Verify(subgraphs()) &&\r\n                                       ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6186:37: note:   cannot convert \u2018tflite::Model::subgraphs()\u2019 (type \u2018const flatbuffers::Vector<flatbuffers::Offset<tflite::SubGraph> >*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(subgraphs()) &&\r\n                                     ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6186:37: note:   cannot convert \u2018tflite::Model::subgraphs()\u2019 (type \u2018const flatbuffers::Vector<flatbuffers::Offset<tflite::SubGraph> >*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(subgraphs()) &&\r\n                                     ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6189:41: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::String*)\u2019\r\n            verifier.Verify(description()) &&\r\n                                         ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6189:39: note:   cannot convert \u2018tflite::Model::description()\u2019 (type \u2018const flatbuffers::String*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(description()) &&\r\n                                       ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6189:39: note:   cannot convert \u2018tflite::Model::description()\u2019 (type \u2018const flatbuffers::String*\u2019) to type \u2018const uint8_t* {aka const unsigned char*\u2019\r\n            verifier.Verify(description()) &&\r\n                                       ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6191:37: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer> >*)\u2019\r\n            verifier.Verify(buffers()) &&\r\n                                     ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6191:35: note:   cannot convert \u2018tflite::Model::buffers()\u2019 (type \u2018const flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer> >*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(buffers()) &&\r\n                                   ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6191:35: note:   cannot convert \u2018tflite::Model::buffers()\u2019 (type \u2018const flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer> >*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(buffers()) &&\r\n                                   ^\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6194:45: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<int>*)\u2019\r\n            verifier.Verify(metadata_buffer()) &&\r\n                                             ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note: candidate: bool flatbuffers::Verifier::Verify(size_t, size_t) const\r\n   bool Verify(size_t elem, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1761:8: note:   candidate expects 2 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(size_t) const\r\n   template<typename T> bool Verify(size_t elem) const {\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1777:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6194:43: note:   cannot convert \u2018tflite::Model::metadata_buffer()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018size_t {aka unsigned int}\u2019\r\n            verifier.Verify(metadata_buffer()) &&\r\n                                           ^\r\nIn file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,\r\n                 from tensorflow/contrib/lite/interpreter.cc:33:\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note: candidate: bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t, size_t) const\r\n   bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {\r\n        ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1782:8: note:   candidate expects 3 arguments, 1 provided\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note: candidate: template<class T> bool flatbuffers::Verifier::Verify(const uint8_t*, flatbuffers::voffset_t) const\r\n   template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)\r\n                             ^\r\n/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1786:29: note:   template argument deduction/substitution failed:\r\nIn file included from tensorflow/contrib/lite/interpreter.cc:33:0:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:6194:43: note:   cannot convert \u2018tflite::Model::metadata_buffer()\u2019 (type \u2018const flatbuffers::Vector<int>*\u2019) to type \u2018const uint8_t* {aka const unsigned char*}\u2019\r\n            verifier.Verify(metadata_buffer()) &&\r\n                                           ^\r\ntensorflow/contrib/lite/Makefile:203: recipe for target '/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/interpreter.o' failed\r\nmake: *** [/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/interpreter.o] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\n\r\n\r\nBut, I have installed the \u2018flatbuffers,\r\nicare@icare-5F:tensorflow$ flatc --version\r\nflatc version 1.9.0 (Aug 14 2018 15:26:14)\r\n\r\nHow should I solve the problem?\r\n\r\n", "1. Be sure to install the arm cross compiler toolchain (with apt-get)\r\n2. Update to master... paths have moved: this worked for me out of the box\r\n```\r\ngit clone git@github.com:tensorflow/tensorflow\r\ncd tensorflow\r\n ./tensorflow/contrib/lite/build_rpi_lib.sh\r\n```", "@aselle one more line is needed for me because some kernels include `flatbuffers/flexbuffers.h`, others use `include/flatbuffers/flexbuffers.h `, see https://github.com/tensorflow/tensorflow/pull/21666 for error messages.", "Nagging Assignees @petewarden, @aselle: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21587, "title": "tf.keras: regularizer does not add to total losses", "body": "I try to add l2 regularizer for tf.keras.layers.CuDNNLSTM,  here is my code for creating layers:\r\n\r\n\r\n```\r\n   regulizer = tf.keras.regularizers.l2(l=1) \r\n    net = tf.keras.layers.CuDNNLSTM(lstm_units,\r\n                                    kernel_initializer=tf.keras.initializers.glorot_normal(),\r\n                                    unit_forget_bias=True,\r\n                                    recurrent_initializer=tf.keras.initializers.glorot_normal(),\r\n                                    kernel_regularizer=regulizer,\r\n                                    return_sequences=True)(input_data)\r\n```\r\nafter I create my graph, I gather all the losses with \r\n`tf.get_collection(tf.GraphKeys.LOSSES)`\r\n\r\nbut \r\n**1. only  mse error is in the return list.\r\n2. the initial losses is same as without regularizer even if I set regularizer co-efficient very big.**\r\n\r\nDoes someone know how to apply regularizer to keras layer in tensorflow?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I think the only relevant information is tensorflow version:\r\n\r\ntensorflow: 1.9.0\r\n", "@fchollet \r\n\r\nI check source code of tf.keras,  if I add a regularizer to a layer, the only thing tf.keras do is add regularizer loss to  **self._losses** of **Layer** class:\r\n\r\n```\r\ntensorflow/python/keras/engine/base_layer.py:\r\n\r\nclass Layer(checkpointable.CheckpointableBase):\r\n      .....................\r\n      def add_loss(self, losses, inputs=None):\r\n           .....................\r\n           self._losses += losses\r\n```\r\n\r\nif I train this graph with keras API, keras will collect all losses when I compile model. But this will not work if I train with native tensorflow API.\r\n\r\nIn the native tensorflow implementation, tensorflow will add regularizer loss to this collection:\r\n\r\n**REGULARIZATION_LOSSES**\r\n\r\n```\r\ntensorflow\\python\\layers\\base.py:\r\n\r\n  def add_loss(self, losses, inputs=None):\r\n    previous_losses_length = len(self._losses)\r\n    super(Layer, self).add_loss(losses, inputs=inputs)\r\n    # TODO(fchollet): deprecate collection below.\r\n    new_losses = self._losses[previous_losses_length:]\r\n    _add_elements_to_collection(new_losses, ops.GraphKeys.REGULARIZATION_LOSSES)\r\n```\r\n\r\nIs my understanding correct, or I did it in a wrong way? If this is a bug I  think it is very easy to fix.", "Hello!\r\nI have the same problem. Did you solve it? Currently I have not found this problem in 1.8.0, but it happens in 1.10.0.", "I have some hot fix to solve this, just add regularization loss to total loss. but not sure if this is right (right way of doing it in tensorflow, It should be right in math).  ", "I don't really understand the issue about keras here. But in tensorflow, LOSSES and REGULARIZATION_LOSSES are separated, thus loss collection must be done for each of them. Does keras follow this practice?", "@scotthuang1989 As far as I understand, Tensorflow will depreciate the use of global variables. I think the better way to handle this is then:\r\n```\r\ntotal_loss = loss + tf.add_n(model.losses)\r\n```\r\nWhere the second term contains weights and activations regulirization losses", "@jrabary  , This is one way to go. but I only use keras layer, not keras Model function.  Any suggestion on this scenario?", "@scotthuang1989 Most of the time you have several layers. tf.keras.Model helps you to compose these layers. For example tf.keras.Sequential is a subclass of tf.keras.Model. So, I think you should really build your neural net using tf.keras.Model c.f  https://www.tensorflow.org/guide/keras#model_subclassing\r\n", "Nagging Assignee @fchollet: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I am facing the same issue with regularization while using a `tf.keras` layer... Any fix for this?", "@ssubraveti In `tensorflow`, regularization loss is added to regularization loss key, not the main loss key. \r\nUnless you use high level API like keras Model or Estimator, all losses have to be collected explicitly. So your total loss would look like:\r\n\r\n    loss = main_loss + tf.losses.get_regularization_loss()\r\n\r\n@fchollet This is not a `tensorflow` issue, it should be closed.", "@tranhungnghiep , if you check source code of tf.losses.get_regularization_loss.  you will find that this function also get regularization loss from **REGULARIZATION_LOSSES** collection\r\n\r\n`ops.get_collection(ops.GraphKeys.REGULARIZATION_LOSSES, scope)`\r\n\r\nAs I mentioned in my comment above ,  keras layer don't add regularization loss to this collection. I am able to fix this, but I am not sure if this is the design or a bug?\r\n\r\n\r\n> @fchollet\r\n> \r\n> I check source code of tf.keras, if I add a regularizer to a layer, the only thing tf.keras do is add regularizer loss to **self._losses** of **Layer** class:\r\n> \r\n> ```\r\n> tensorflow/python/keras/engine/base_layer.py:\r\n> \r\n> class Layer(checkpointable.CheckpointableBase):\r\n>       .....................\r\n>       def add_loss(self, losses, inputs=None):\r\n>            .....................\r\n>            self._losses += losses\r\n> ```\r\n> if I train this graph with keras API, keras will collect all losses when I compile model. But this will not work if I train with native tensorflow API.\r\n> \r\n> In the native tensorflow implementation, tensorflow will add regularizer loss to this collection:\r\n> \r\n> **REGULARIZATION_LOSSES**\r\n> \r\n> ```\r\n> tensorflow\\python\\layers\\base.py:\r\n> \r\n>   def add_loss(self, losses, inputs=None):\r\n>     previous_losses_length = len(self._losses)\r\n>     super(Layer, self).add_loss(losses, inputs=inputs)\r\n>     # TODO(fchollet): deprecate collection below.\r\n>     new_losses = self._losses[previous_losses_length:]\r\n>     _add_elements_to_collection(new_losses, ops.GraphKeys.REGULARIZATION_LOSSES)\r\n> ```\r\n> Is my understanding correct, or I did it in a wrong way? If this is a bug I think it is very easy to fix.\r\n\r\n", "@scotthuang1989 I think you are right. tf's `add_loss()` adds regularization loss to `GraphKeys.REGULARIZATION_LOSSES`, but keras' `add_loss()` doesn't. So `tf.losses.get_regularization_loss()` works for tf layer but not keras layer. For keras layer, you should call `layer._losses` or `layer.get_losses_for()`.\r\n\r\nI also see @fchollet's comment that `GraphKeys.REGULARIZATION_LOSSES` is deprecated, so the correct way is to go through each and every layer/variable and get loss out manually. You can also use tf instead of keras, it works, for now.\r\n\r\n@fchollet There is really a mismatch in tf and keras behaviors. And regarding your comment on deprecation, please consider either update the code or the documentation to keep consistency and avoid confusion. Thanks.", "I just encountered the same issue. Apparently layers in native tf and keras are different in their ways of dealing with regularization loss.\r\n\r\nI found an example of dealing with such regularization loss on tf official example: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/densenet/densenet_graph_test.py\r\n\r\nThey use `tf.keras.model.losses` to collect such loss. In general I think it is nowadays a good practice to subclass tf.keras.model API for building tensorflow models. If you do have mixed sources of regularization losses, you may consider collect them using `tf.losses.get_regularization_loss` and `tf.keras.model.losses` respectively.", "although no official answer, but it does have some workaround, I currently manually add regularizer to each layer. In future I try avoid mix tf.layer and tf.keras.layers.  I am gonna close this.", "I also found this issue today and opened a feature request.  Moreso, these attributes should be read-only unless the model state is updated when they are written to.\r\n\r\nhttps://github.com/keras-team/keras/issues/12053"]}, {"number": 21586, "title": "[INTEL MKL] Enable Pooling3D ops", "body": "The enhancement within this PR enables MKL Pooling3D operations (both average pooling and max pooling). ", "comments": []}, {"number": 21585, "title": "Freezing a graph from a .pb file", "body": "Hello!\r\nThis is not a bug or a feature request but I cannot find answers on stackoverflow and have tried multiple ways of fixing this. This is my stack-overflow question: https://stackoverflow.com/questions/51826706/tensorflow-load-a-pb-file-and-then-save-it-as-a-frozen-graph-issues\r\nI went to the discussion forums and after finding some bugs in that code got redirected here. \r\nThis is the code that I have tried:\r\n```\r\nimport tensorflow as tf\r\nimport sys\r\nfrom tensorflow.python.platform import gfile\r\n\r\nfrom tensorflow.core.protobuf import saved_model_pb2\r\nfrom tensorflow.python.util import compat\r\nconstant_values = {}\r\n\r\nwith tf.Session() as sess:\r\n    model_filename ='saved_model.pb'\r\n    with gfile.FastGFile(model_filename, 'rb') as f:\r\n\r\n      \tdata = compat.as_bytes(f.read())# reads binary\r\n      \tsm = saved_model_pb2.SavedModel()\r\n        print(sm)\r\n      \tsm.ParseFromString(data)# parses through file. \r\n      \tprint(sm)\r\n      \tif 1 != len(sm.meta_graphs):\r\n      \t\tprint('More than one graph found. Not sure which to write')\r\n      \t\tsys.exit(1)\r\n            \r\n        # Attempt at initializing the variables here \r\n        g_in = tf.import_graph_def(sm.meta_graphs[0].graph_def)\r\n        constant_ops = [op for op in sess.graph.get_operations() if op.type == \"Const\"]\r\n        for constant_op in constant_ops:\r\n            constant_values[constant_op.name] = sess.run(constant_op.outputs[0])\r\n            \r\n        # Creating frozen graph.\r\n        output_graph = \"frozen_grapha.pb\"\r\n        output_nodes = [n.name for n in tf.get_default_graph().as_graph_def().node]\r\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n           sess, # The session is used to retrieve the weights\r\n           sess.graph_def,\r\n           output_nodes# The output node names are used to select the usefull nodes\r\n        ) \r\n\r\n        # Finally we serialize and dump the output graph to the filesystem\r\n        with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n            f.write(output_graph_def.SerializeToString())\r\n        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n        print(g_in)\r\nLOGDIR='.'\r\ntrain_writer = tf.summary.FileWriter(LOGDIR)\r\ntrain_writer.add_graph(sess.graph)\r\n\r\n```\r\n\r\nThe error I get:\r\n```\r\n\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value NAME_OF_NODE_IN_GRAPH\r\n```\r\n\r\nThank you for your time!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Sorry, here is all missing info! \r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: ubuntu 16.04\r\nTensorFlow installed from: pip \r\nTensorFlow version: tensorflow 1.8 (but I have tried 1.9 and 1.10)\r\nBazel version: NA\r\nCUDA/cuDNN version: 9.2 (but I don't think it matters in this case)\r\nGPU model and memory: 7gb . (but it doesn't matter) \r\nExact command to reproduce\r\n`tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value NAME_OF_NODE_IN_GRAPH\r\n`\r\nMobile device: NA", "Hello! \r\nI figured it out:\r\n\r\n```\r\ndef frozen_graph_maker(export_dir,output_graph):\r\n        with tf.Session(graph=tf.Graph()) as sess:\r\n                tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], export_dir)\r\n                output_nodes = [n.name for n in tf.get_default_graph().as_graph_def().node]\r\n                output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n                        sess, # The session is used to retrieve the weights\r\n                        sess.graph_def,\r\n                        output_nodes# The output node names are used to select the usefull nodes\r\n               )       \r\n        # Finally we serialize and dump the output graph to the filesystem\r\n        with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n                f.write(output_graph_def.SerializeToString())\r\ndef main():\r\n        export_dir='/dir/of/pb/and/variables'\r\n        output_graph = \"frozen_graph.pb\"\r\n        frozen_graph_maker(export_dir,output_graph)\r\n```\r\nI realized that I was just loading the meta graph. I would love if you could confirm my understanding of what was failing. With `compat.as_bytes` I was just loading it as a meta graph. Is there a way of integrating variables after doing that kind of loading or should I stick with `tf.saved_model.loader.load()` ? \r\nMy attempt of loading was completely wrong as it wasn't even calling the folder of variables. \r\n\r\nAnother question:  with `[n.name for n in tf.get_default_graph().as_graph_def().node]` I am putting all nodes into the output_nodes, should I just put the last node? It works with just the last node. What is the difference? I will close this and post this follow up questions in stackoverflow. Here:https://stackoverflow.com/questions/51826706/tensorflow-load-a-pb-file-and-then-save-it-as-a-frozen-graph-issues/51865645#51865645 \r\n\r\nThank you! ", "@NylaWorker thank you so much for this post."]}, {"number": 21584, "title": "Possible memory leak?", "body": "Hi I was looking into this. The fact that we are adding the loss to the total inside the gradient_tape would save all the previous iterations within the epoch. Am I wrong?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2942555be63f2dbf557cdf7be2bcbd5d4e9d0daf/tensorflow/contrib/eager/python/examples/gan/mnist.py#L243", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n\r\nThat said, Short answer: Nope, this should not be a leak.\r\n\r\nLong answer: `total_discriminator_loss += discriminator_loss_val` is shorthand for `total_discriminator_loss = tf.add(total_discriminator_loss, discriminator_loss_val)`. So, the tape only records the `add` operation, furthermore the tape is cleared on each call to `gradient` ([doc](https://www.tensorflow.org/api_docs/python/tf/GradientTape#class_gradienttape)).\r\n\r\nHope that helps."]}, {"number": 21583, "title": "python XML content control", "body": "\r\nBelow is a code snippet from the XML content that annotates the object tag and annotates it. Some XML files do not have an object tag. In this case it gives a keyError error. How can I pass such xml files without giving keyError\r\n\r\n        annotation = read_xml(annotation_path)\r\n        image = read_image(image_path)\r\n\r\n        gt_boxes = []\r\n\r\n        for b in annotation['object']:\r\n            try:\r\n                label_id = self.classes.index(b['name'])\r\n            except ValueError:\r\n                continue\r\n\r\n            gt_boxes.append({\r\n                'label': label_id,\r\n                'xmin': b['bndbox']['xmin'],\r\n                'ymin': b['bndbox']['ymin'],\r\n                'xmax': b['bndbox']['xmax'],\r\n                'ymax': b['bndbox']['ymax'],\r\n            })\r\nI want to put a control code above \"for b in annotation['object']:\" command. Help me please", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21582, "title": "Memory leak due to nsync::nsync_waiter_new_()?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nMy own build from source as well as google binary found at: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.8.0.tar.gz\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.8.0 - using C APi, not python\r\n\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nN/A, using google TF 1.8.0 cpu build, calling libtensorflow.so via C API\r\n\r\n### Describe the problem\r\n\r\nI get valgrind reported memory leak(s), when using TF 1.8 (updating from previously used TF 1.2).  Specifically, the leak comes from nsync::nsync_waiter_new_().   The valgrind output looks like this:\r\n\r\n`==1947== 400 bytes in 1 blocks are possibly lost in loss record 80,541 of 82,598\r\n==1947==    at 0x4C2DBF6: malloc (vg_replace_malloc.c:299)\r\n==1947==    by 0xC9A6C34: nsync::nsync_waiter_new_() (in /usr/local/lib/libtensorflow_framework.so)\r\n==1947==    by 0xC9A741E: nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) (in /usr/local/lib/libtensorflow_framework.so)\r\n==1947==    by 0xC9A7AE4: nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) (in /usr/local/lib/libtensorflow_framework.so)\r\n==1947==    by 0x73DCA4A: ??? (in /usr/local/lib/libtensorflow.so)\r\n==1947==    by 0x73DCA9A: ??? (in /usr/local/lib/libtensorflow.so)\r\n==1947==    by 0x73E228F: ??? (in /usr/local/lib/libtensorflow.so)\r\n==1947==    by 0x73EBCE4: ??? (in /usr/local/lib/libtensorflow.so)\r\n==1947==    by 0x5258D29: ??? (in /usr/local/lib/libtensorflow.so)\r\n==1947==    by 0x5259925: TF_SessionRun (in /usr/local/lib/libtensorflow.so)`\r\n\r\nI think I may have found the source by looking at the nsync code located at: https://github.com/google/nsync/blob/master/internal/common.c\r\n\r\nI cannot observe where the memory allocated by malloc() in nsync_waiter_new_() is freed().  Furthermore, I don't see how the memory could safely be freed because the function allows for a caller-supplied allocator, which it conditionally uses, but does not seem to record which allocator was used, or have a mechanism for calling a caller-provided free().\r\n\r\nI believe that this is a memory leak bug in Tensorflow, being caused by the nsync dependency.  \r\n\r\nBecause the optimized monolithic build of libtensorflow does not seem to have debug symbols enabled by default, and because this call can occur in so many different contexts; it is difficult to generate an appropriate valgrind suppresions file which does not suppress too much.\r\n\r\n\r\n", "comments": ["@jkeller-miovision what are the repro steps? `valgrind python -c \"import tensorflow\"`?\r\n\r\nIt would help a lot of you could rebuild tensorflow with debug symbols `bazel build -c dbg //tensorflow/tools/pip_package:build_pip_package` and run again to get the line numbers.\r\n\r\nIf you build with `asan`, there should also be some report of dangling objects at exit.\r\n\r\nIt's probable that it's one of those global static variables that we have.\r\n", "@drpngx Sorry for the delay in getting back to you.\r\n\r\nPython is not involved this application.  This is a C++ executable which links against the TF C api.   To reproduce the issue, I think you may need concurrent invocations of TF_SessionRun().  I re-ran with the Google provided binaries, and here is a sample strack trace:\r\n\r\n```\r\n==1151== 400 bytes in 1 blocks are possibly lost in loss record 83,360 of 85,527\r\n==1151==    at 0x4C2DBF6: malloc (vg_replace_malloc.c:299)\r\n==1151==    by 0xD68CC34: nsync::nsync_waiter_new_() (in /usr/local/lib/libtensorflow_framework.so)\r\n==1151==    by 0xD68E6F1: nsync::nsync_mu_lock(nsync::nsync_mu_s_*) (in /usr/local/lib/libtensorflow_framework.so)\r\n==1151==    by 0x52577A6: tensorflow::ExtendSessionGraphHelper(TF_Session*, TF_Status*) (in /usr/local/lib/libtensorflow.so)\r\n==1151==    by 0x52599B0: TF_SessionRun (in /usr/local/lib/libtensorflow.so)\r\n==1151==    by 0x4E9A5C: streamagent::SSD::detect(std::vector<miocv::PixelView<miocv::RGBA8888>, std::allocator<miocv::PixelView<miocv::RGBA8888> > > const&, std::vector<std::vector<streamagent::ObjectDetectorBox, std::allocator<streamagent::ObjectDetectorBox> >*, std::allocator<std::vector<streamagent::ObjectDetectorBox, std::allocator<streamagent::ObjectDetectorBox> >*> > const&, double) (in /home/ubuntu/miocv-builder/build/components/streamagent/streamagent-integration_tests)\r\n==1151==    by 0x49A70B: streamagent::RunningPipeline::pipelineThreadMain(long, long) (in /home/ubuntu/miocv-builder/build/components/streamagent/streamagent-integration_tests)\r\n==1151==    by 0xBF97C7F: ??? (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21)\r\n==1151==    by 0xC7876B9: start_thread (pthread_create.c:333)\r\n==1151==    by 0xCAA441C: clone (clone.S:109)\r\n==1151== \r\n{\r\n   <insert_a_suppression_name_here>\r\n   Memcheck:Leak\r\n   match-leak-kinds: possible\r\n   fun:malloc\r\n   fun:_ZN5nsync17nsync_waiter_new_Ev\r\n   fun:_ZN5nsync13nsync_mu_lockEPNS_11nsync_mu_s_E\r\n   fun:_ZN10tensorflow24ExtendSessionGraphHelperEP10TF_SessionP9TF_Status\r\n   fun:TF_SessionRun\r\n   fun:_ZN11streamagent3SSD6detectERKSt6vectorIN5miocv9PixelViewINS2_8RGBA8888EEESaIS5_EERKS1_IPS1_INS_17ObjectDetectorBoxESaISA_EESaISD_EEd\r\n   fun:_ZN11streamagent15RunningPipeline18pipelineThreadMainEll\r\n   obj:/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21\r\n   fun:start_thread\r\n   fun:clone\r\n}\r\n```\r\n\r\nThe same leak occurs in different contexts as well.   As you can see, memory is being allocated in nsync::nsync_waiter_new_().   As I mentioned in my first post, I can see by inspection of https://github.com/google/nsync/blob/master/internal/common.c that the memory is not deallocated in this case.  ", "> memory is being allocated in nsync::nsync_waiter_new_(). As I mentioned in my \r\n> first post, I can see by inspection of \r\n> https://github.com/google/nsync/blob/master/internal/common.c \r\n> that the memory is  not deallocated in this case.\r\n\r\nSummary:\r\n\r\nvalgrind's leak detector is complaining not because there's a leak,\r\nbut because it doesn't like interior  pointers, which point to places within a \r\nmalloced region, rather than the start.  \r\nThat's what  \"possibly lost\" in vagrind's report means \r\n(see http://valgrind.org/docs/manual/valgrind_manual.pdf \r\nand search for \"possibly lost\").\r\nThe memory is actually still reachable.\r\n\r\nThose blocks are not supposed to be deallocated, but when no longer in use \r\nare instead put into a free pool and reused, for reasons that I give below.\r\n\r\nnsync uses interior pointers because it puts the same struct on _multiple_\r\ndoubly-linked lists simultaneously.   It uses pointers that are embedded in the \r\nstruct (rather than separate list objects) for efficiency: \r\nit would be bad to touch twice\r\nas many cache lines while scanning a list.\r\nThe embedded pointers  for the multiple doubly-linked lists couldn't all be \r\nto the start of the objects unless the code for linked list manipulation \r\nwere duplicated (one copy of the code for each type of linked list), \r\nor took an offset argument to tell the linked-list code\r\nwhere to find the links within the object.   An alternative would be a\r\nfake linked list just for valgrind, linking through every such struct.\r\nAs you can imagine, to add such complication to the code\r\nto mollify one checker seems a little ugly, \r\nespecially because the concern it evokes is illusory: there is no leak.\r\nNotice also valgrind memcheck is being fairly zealous here, because\r\ninterior pointers are not that uncommon in normal code.  For example,\r\nnotice this text in  http://valgrind.org/docs/manual/valgrind_manual.pdf\r\n    \"The available heuristics provide detection of valid interior pointers to \r\n    std::stdstring, to new[] allocated arrays with elements having destructors \r\n    and to interior pointers pointing to an inner part of a C++ object using \r\n    multiple inheritance.\"\r\n\r\n\r\nBelow are more details than you probably want about \r\nwhy nsync does thing the way it does.\r\n\r\nThe memory involved is a \"waiter\" struct, which is used\r\nto enqueue a thread for later wakeup by another thread.\r\nRather than freeing the waiter struct after use, the struct\r\nis placed into a free pool  either by waiter_destroy() (which is invoked \r\nby thread exit), or by  nsync_waiter_free_().      The memory in the \r\npool is reused by calls to nsync_waiter_new_(); no new allocations are done unless\r\nthe free pool is empty.  No waiter struct is allowed to become unreachable.\r\n\r\n(Aside: Normally, a thread gets one struct and uses it until the thread exits.\r\nThe complication of having both waiter_destroy() and nsync_waiter_free_()\r\noccurs because when debugging nsync itself, a single thread could\r\nuse more than one waiter struct simultaneously.  But we can ignore that here.)\r\n\r\nThus, the number of waiter structs\r\nshould be at most the maximum number of concurrent threads.\r\nThere would be cause for concern only if the leak detector reported\r\nsubstantially more waiter structs than that\r\n(e.g., if the programme had used at most tens of  \r\nthreads concurrently, but the leak detector reported hundreds of waiter\r\nstructs).    Such an eventuality would have suggested that waiter structs\r\nwere not being reused effectively.   But here, you list a report for only one \r\nsuch struct, so nothing bad seems to be happening.\r\n\r\nYou might wonder why the library doesn't just free the structs,\r\nrather than putting them on an internal free list.\r\nThat would be simpler, but to free the memory would slow down the library\r\nwhen under heavy load.  The reasons are somewhat subtle.\r\n\r\n\r\nFirst, I'll explain how waiter structs are used in the code as it stands, \r\nand why that requires that the waiter structs are never freed.\r\nThen I'll explain why it would be slower to have it work in a way that\r\ndid allow the waiter structs to be freed.\r\n\r\nWhen nsync needs to block a thread (to wait on a condition variable or a mutex)\r\nit uses a waiter struct W that has an atomic word \"W.nw.waiting\", primary\r\nlink list pointers \"W.nw.q\",  and a platform-implemented semaphore \"W.sem\".\r\n\r\nA thread X wishing to wait on some wait queue Q acts as follows:\r\nX0) finds its a waiter struct W (typically via a pointer in \r\nthread-local storage),\r\nX1) sets \"W.nw.waiting\" to non-zero, \r\nX2) locks Q's spinlock,\r\nX3) inserts W.nw.q  onto Q,\r\nX4) unlocks Q spinlock, and\r\nX5) while (\"W.nw.waiting\" is non-zero)\r\n      X5a) waits on \"W.sem\"\r\n\r\nA thread Y wishing to wake a thread from a queue Q:\r\nY0) locks Q's spinlock,\r\nY1) removes a waiter struct W from Q,\r\nY2) unlocks Q spinlock, \r\nY3) zeroes \"W.nw.waiting\", and \r\nY4) wakes \"W.sem\".\r\n\r\nNotice that these actions might interleave legally in the following way:\r\n    X0; X1; X2; X3; X4; Y0; Y1; Y2; Y3; X5\r\nAt this point thread X is no longer waiting, and does not need to execute X5a.  \r\nNow suppose that thread X returns from the nsync library, and exits, \r\nand only then does thread Y run Y4, which touches W.sem.\r\n\r\nClearly, W.sem needs to exist when Y does Y4, or the programme will crash.\r\nBut W (and thus W.sem) was assigned for the use of thread X, which has exited.\r\nThis means that with this protocol, thread X must _not_ deallocate the \r\nwaiter struct it was assigned, even when thread X exits. \r\nThis is achieved by making waiter structs immortal in the way \r\ndescribed above; they are never freed, but instead reused.\r\nThus, it is guaranteed that \"W.sem\" will exist when Y4 happens, even if W is been\r\nput on the free list, or even reused by another thread X'.   This reuse is still correct,\r\neven if Y4 happens after X' has started using W: the worst that can happen is that\r\n(rarely) X' will go once more harmlessly around the loop (X5,X5a).\r\n\r\nThe obvious way to avoid this \"immortality\" of W would be to perform the wakeup of\r\n\"X.sem\" under Q's spinlocklock.  \r\nThat is, Y could use the order Y0;Y1;Y3;Y4;Y2.   This would \r\nbe correct, but performs poorly under high load.  That's because the critical section\r\nof Q's spinlock would then be substantially longer, \r\nas it would include a full thread wakeup\r\n(typically involving a scheduling decision) in addition to the simple\r\nqueue insertions and removals currently performed under Q's spinlock.\r\nDepending on load, the poor performance can be bad enough in practice\r\nthat the immortality of the waiter struct is a small price to pay.\r\n\r\n(Aside:It's important to maintain high performance at high load because high load\r\nusually occurs during system overload.   That's the time when you want \r\nthroughput to be as high as possible, because that's the best way to get out of \r\noverload again.  Systems that get a lot slower when pushed into overload \r\ntend to have unstable performance characteristics.)\r\n\r\nFor the case of a platform that has no thread-specific data,\r\nthere is a less important reason for using a free queue to reuse waiter structs.\r\nIn this case, if the memory were given back to the malloc() allocator, \r\nthen the cost of the malloc()/free() calls and the cost of re-initializing the \r\nwaiter struct (including the semaphore) every time a thread blocked\r\nwould slow things somewhat down too.\r\n\r\n\r\nYou might also wonder why the waiter struct needs to be on more than \r\nlist simultaneously.   That's because a single mutex waiter queue actually has\r\ntwo doubly-linked lists.  One is linked though all the threads' waiter structs \r\nin the usual way.\r\nThe other uses the \"same_condition\" link to connect all adjacent elements \r\non the waiter queue that have the same non-trivial waiter condition \r\n(again, see conditional critical sections in nsync's README file).\r\nThis allows the implementation to test a single condition, and skip over \r\nmany adjacent  threads waiting on that condition. \r\nThis is a surprisingly common case.  For example, consider a \r\nproducer-consumer queue, in which either all waiting threads will be \r\nwaiting for \"queue non-empty\" or all waiting threads\r\nwill be waiting for \"queue non-full\".  Thus, this is a fairly potent optimization.\r\n\r\n\r\n", "@m3bm3b \r\n\r\nThanks for the lengthly explanation.  I really do appreciate it.  I believe that I follow you completely, and I was aware of the linked lists involved.  I do not believe that valgrind is reporting this because of the 'interior pointer' use.  Rather, as you mention:\r\n\r\n> Those blocks are not supposed to be deallocated, but when no longer in use\r\n> are instead put into a free pool and reused, for reasons that I give below.\r\n\r\nThese structs are allocated in common.c line 189 or 187.  I believe that the pointer returned by that allocation, is never passed to any implementation of free() - instead, as you mention, these structs are placed on the free_waiters DLL, and re-used.  However they are not freed within the lifetime of the process.  It depends on one's semantic interpretation, but I believe this memory is 'leaked' in the same way that a dynamically allocated singleton object/struct ptr is leaked if it is never deallocated.  As you mention, there is a guaranteed limit on the amount of this memory based on the max number of concurrently executing threads.  \r\n\r\nAnother way to appease leak-checking tools in cases like this would be to deallocate members of the free_waiters list at process exit time (via a atexit() handler, or an explicit library de-initialization function).  It is understandable (to me) if that is not a desired design decision.\r\n\r\nFor my use case, I would be very happy if I could figure out the correct invocation to bazel so that I can build an optimized, monolithic, libtensorflow that still has symbols available to generate a specific valgrind suppression for this case.   So far I have not found a way to enable those three options simultaneously - but I have almost no experience with bazel, so I'm sure it can be done.  As it stands now, I cannot differentiate on anything besides the exported C API symbols, so I must suppress everything that occurs within the context of TF_SessionRun().\r\n\r\nSpecifically, I have needed to use the monolithic build to prevent libtensorflow from exporting symbols from it's dependencies (libpng, protobuf, etc...) as they clash with other versions of the same libraries that my executable links in.\r\n\r\nThanks again for your thorough explanation!\r\n\r\n", ">  I do not believe that valgrind is reporting this because of the 'interior pointer' use.\r\n\r\nI do not understand why belief comes into this.  We have data.\r\n\r\nFirst, the valgrind documentation that I cited says rather \r\nunequivocally that the \"possibly lost\" error message you quoted  \r\n_is_ about interior pointers.  It would be remarkable if the author had lied\r\nabout this topic.\r\n\r\nSecond, one can do the experiment of allocating and then failing to free a block,\r\nfirst using a pointer to the block's start, and then a pointer to the block's interior.\r\nIn the first case, valgrind does _not_ treat the unfreed memory as an error.\r\nIn the second case, it does report an error (because of the interior pointer),\r\nand it yields the \"possibly lost\" error message that you quote in\r\nyour opening message.\r\n\r\nI give examples of such an experiment at the end of this message.\r\n\r\n\r\n> Another way to appease leak-checking tools in cases like this would be to \r\n> deallocate members of the free_waiters list at process exit time \r\n> (via a atexit() handler, or an explicit library de-initialization function).\r\n\r\nThat's not going to be effective, even\r\nleaving aside the fact that valgrind is _not_ complaining that the \r\nobject is still live at exit() time.   \r\n\r\nMorevoer, as a general rule, attempting to free memory\r\nat exit time, whether in atexit() handlers, library de-initialization fuinctions,\r\nof C++ static destructors is actually dangerous,\r\nand I recommend against it.\r\n\r\n\r\nFirst, it serves no rational purpose to free memory in an address space that \r\nis about to be completely blown away.  At best, it wastes CPU cycles.\r\nA fitting analogy is:\r\n     https://en.wiktionary.org/wiki/rearrange_the_deck_chairs_on_the_Titanic\r\n\r\n\r\nIn addition, in the case you're suggesting, it doesn't actually achieve your apparent\r\ngoal of causing there to be no allocated nsync waiter structs at exit() time.   \r\nEven if the thread calling exit() removes all the waiter structs from the nsync's free list:\r\n1. every extant thread will still have a waiter struct that you cannot free safely\r\n(the only reason valgrind is not complaining about those structs is that \r\nthe thread-local pointers are not interior pointers); and\r\n2. any of those threads might put another waiter struct back on nsync's free list\r\nafter the atexit() handler had emptied it---those threads are still running while the \r\natexit() handlers are running.\r\nThus this approach does not reliably achieve the goal: no matter what you do in the\r\natexit() handler, there will always be some waiter structs not freed when the \r\naddress space exists, and sometimes there will even be some on the free\r\nlist that the atexit() handler tried in vain to empty.\r\n\r\nAnd worst of all, in general (though not in the case of the nsync waiter free list),\r\nif another thread touched a variable or data structure\r\nthat had been \"cleaned up\" by an atexit() handler (or destructor of a static C++ variable),\r\nthe process might not exit cleanly, but might instead crash or deadlock because it touched\r\na variable that was no longer valid.\r\n\r\nThe probability of such problems increases with the size of the programme,\r\nbecause the developer calling exit() and the developers writing the atexit() handlers\r\nknow less about what else is going on within the address space.\r\nThus, the issues are more pressing in large code bases.  People who write \r\nprogrammes of only modest size may not come across this issue for a long time.\r\n\r\nNotice that the problems are even more common in C++ programmes, because \r\nin that language it's easy accidentally to create a static variable with a dangerous \r\nnon-trivial destructor.\r\nThis is why C++ has quick_exit() (which is like exit() but doesn't run destructors or \r\natexit() handlers), and why Google's C++ style guide prohibits the use of \r\nnon-trivial destructors on static variables.  (see       \r\n  https://en.cppreference.com/w/cpp/utility/program/quick_exit\r\nand \r\n  https://google.github.io/styleguide/cppguide.html#Static_and_Global_Variables\r\n). \r\n\r\nTo summarize, in a large, mulitthreaded programme there is no safe way to guarantee \r\nto deallocate internal  memory reousrces at exit().   \r\natexit() handlers are best used to flush I/O buffers\r\nand if possible to release _external_ resources that might \r\notherwise have to time out (e.g., early release of a lease in a \r\ndistributed system, or to abort a database transaction that \r\nwould otherwise have to time out).\r\n\r\n\r\n                 ---------------\r\n\r\nHere's the experiment with valgrind:\r\n\r\nBelow are two programmes \"start_ptr\" and \"interior_ptr\":\r\n- Each allocates a 32-byte block, and never frees it.\r\n- Each stores a pointer to the block in a static variable.\r\n- In each, the pointer is still there when the programme exits.\r\nThe only difference is that \"start_ptr\" stores a pointer to the start of the block,\r\nwhile interior_ptr stores a pointer to the byte at offset 8 within the block.\r\n\r\nFor \"start_ptr\", valgrind memcheck reports \"0 errors\", \r\nand gives no stack trace.    It does report \"in use at exit: 32 bytes in 1 blocks\", \r\nbut it doesn't regard this as an error.    By giving an extra flag, you can get \r\nit to report the location where the allocation happened, but it reports it\r\nas \"still reachable\", not \"possibly lost\" (the error from your report), \r\nand still will not report it as an error.\r\n\r\nFor \"interior_ptr\",  valgrind memcheck reports \"1 errors\" [sic], and \r\ndetails the error saying  \"32 bytes in 1 blocks are possibly lost\"\r\n(matching your error report) at \"main (interior_ptr.c:4)\".\r\n\r\n$ cat > start_ptr.c <<EOF\r\n#include <stdlib.h>\r\nstatic char *ptr;\r\nint main (int argc, char *argv[]) {\r\n        ptr = (char *) malloc (32);\r\n        return (0);\r\n}\r\nEOF\r\n$ cc -g start_ptr.c -o start_ptr       \r\n$ valgrind --tool=memcheck --leak-check=full ./start_ptr\r\n...\r\n==17720== HEAP SUMMARY:\r\n==17720==     in use at exit: 32 bytes in 1 blocks\r\n==17720==   total heap usage: 1 allocs, 0 frees, 32 bytes allocated\r\n==17720== \r\n==17720== LEAK SUMMARY:\r\n==17720==    definitely lost: 0 bytes in 0 blocks\r\n==17720==    indirectly lost: 0 bytes in 0 blocks\r\n==17720==      possibly lost: 0 bytes in 0 blocks\r\n==17720==    still reachable: 32 bytes in 1 blocks\r\n==17720==         suppressed: 0 bytes in 0 blocks\r\n==17720== Reachable blocks (those to which a pointer was found) are not shown.\r\n==17720== To see them, rerun with: --leak-check=full --show-leak-kinds=all\r\n==17720== \r\n==17720== For counts of detected and suppressed errors, rerun with: -v\r\n==17720== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)\r\n\r\n\r\n$ cat > interior_ptr.c <<EOF\r\n#include <stdlib.h>\r\nstatic char *ptr;\r\nint main (int argc, char *argv[]) {\r\n        ptr = 8 + (char *) malloc (32);\r\n        return (0);\r\n}\r\nEOF\r\n$ cc -g interior_ptr.c -o interior_ptr \r\n$ valgrind --tool=memcheck --leak-check=full ./interior_ptr \r\n==17748== Memcheck, a memory error detector\r\n==17748== Copyright (C) 2002-2015, and GNU GPL'd, by Julian Seward et al.\r\n==17748== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info\r\n==17748== Command: ./interior_ptr\r\n==17748== \r\n==17748== \r\n==17748== HEAP SUMMARY:\r\n==17748==     in use at exit: 32 bytes in 1 blocks\r\n==17748==   total heap usage: 1 allocs, 0 frees, 32 bytes allocated\r\n==17748== \r\n==17748== 32 bytes in 1 blocks are possibly lost in loss record 1 of 1\r\n==17748==    at 0x4C2DB8F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\r\n==17748==    by 0x40053E: main (interior_ptr.c:4)\r\n==17748== \r\n==17748== LEAK SUMMARY:\r\n==17748==    definitely lost: 0 bytes in 0 blocks\r\n==17748==    indirectly lost: 0 bytes in 0 blocks\r\n==17748==      possibly lost: 32 bytes in 1 blocks\r\n==17748==    still reachable: 0 bytes in 0 blocks\r\n==17748==         suppressed: 0 bytes in 0 blocks\r\n==17748== \r\n==17748== For counts of detected and suppressed errors, rerun with: -v\r\n==17748== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)\r\n", "Thank you @m3bm3b, crystal clear!  I appreciate you taking the time to explain so thoroughly."]}, {"number": 21581, "title": "Cannot convert to TFRecords", "body": "I have three text files `A.txt, B.txt,C.txt` the labels are `A,B and C`\r\n\r\n    \r\n    def convert(file_paths, labels, out_path):\r\n\r\n        print(\"Converting: \" + out_path)\r\n\r\n        # Number of images. Used when printing the progress.\r\n        num_files = len(file_paths)\r\n\r\n        # Open a TFRecordWriter for the output-file.\r\n       with tf.python_io.TFRecordWriter(out_path) as writer:\r\n    \r\n        # Iterate over all the image-paths and class-labels.\r\n        for i, (path, label) in enumerate(zip(file_paths, labels)):\r\n               # Print the percentage-progress.\r\n               print_progress(count=i, total=num_files - 1)\r\n\r\n               lines=getModifiedLines(path)\r\n\r\n\r\n              # Create a dict with the data we want to save in the\r\n              # TFRecords file. You can add more relevant data here.\r\n              data = \\\r\n                  {\r\n                      'text': wrap_int64(lines),\r\n                      'label': wrap_int64(label)\r\n                  }\r\n\r\n              # Wrap the data as TensorFlow Features.\r\n              feature = tf.train.Features(feature=data)\r\n\r\n              # Wrap again as a TensorFlow Example.\r\n              example = tf.train.Example(features=feature)\r\n\r\n              # Serialize the data.\r\n              serialized = example.SerializeToString()\r\n              writer.write(serialized)\r\n\r\n    def getModifiedLines(filePath):\r\n        data = open(filePath, 'r', encoding='UTF8', errors='ignore').read()\r\n        lines = re.split(\"\\n\", data)\r\n        all_lines=[]\r\n        for line in lines:\r\n          _l=count_vect.fit_transform(line)\r\n          all_lines.append(_l)\r\n\r\n    return all_lines\r\n\r\n I am getting error at line `'text': wrap_int64(lines),`\r\n\r\n>Error\r\n`[<12071x21108 sparse matrix of type '<class 'numpy.int64'>'\r\n\twith 226655 stored elements in Compressed Sparse Row format>] has type \r\n   <class 'list'>, but expected one of: (<class 'int'>,)`\r\n\r\n\r\n\r\n\r\nHave I written custom code  **Yes**\r\nOS Platform and Distribution  **Ubuntu 16.04**\r\nTensorFlow installed from   **https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp27-none-linux_x86_64.whl**\r\n\r\nTensorFlow version  **1.9**\r\nBazel version \r\nCUDA/cuDNN version   **i think 7**\r\nGPU model and memory **NVIDIA Titan V 12288MB**\r\nMobile device  **No**", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "It has been 32 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Since I never got an response I found an alternative . I am ok with this issue being closed.Thanks"]}, {"number": 21580, "title": "fix character access in autograph", "body": "fix #20703 by extending the tensor index operator.", "comments": ["@mdanatg  could you take a look?", "Nagging Reviewer @mdanatg: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 16 days with no activity and the `awaiting review` label has been applied.", "This is great! Sorry for the delay, I just got back from vacation.", "@mdanatg you are right, fixed.", "I just realized this will raise error for tensor of string. This should only apply for string with rank 0.\r\n@mdanatg "]}, {"number": 21579, "title": "[WIP] Add canned tensor forest", "body": "# Work in progress\r\n\r\n", "comments": ["Nagging Assignee @protoget: It has been 22 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}]