[{"number": 6330, "title": "Error of using a custom classifier for android demo", "body": "I want to build android demo of my classifier.\r\nFollowing the steps at [https://www.oreilly.com/learning/tensorflow-on-android](url),\r\nthe error appeared for \r\n`bazel build tensorflow/python/tools:optimize_for_inference` .\r\nMy bazel version is 0.4.2.\r\nError :\r\n> ERROR: /home/dlm/tensorflow_new/tensorflow/python/BUILD:1907:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored /usr/bin/gcc -shared -o bazel-out/local-fastbuild/bin/tensorflow/python/_pywrap_tensorflow.so -Wl,--version-script ... (remaining 14 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nbazel-out/local-fastbuild/bin/tensorflow/core/libversion_lib.a(version_info.pic.o): In function `tf_git_version()':\r\nversion_info.cc:(.text+0x0): multiple definition of `tf_git_version()'\r\nbazel-out/local-fastbuild/bin/tensorflow/core/libframework_internal.lo(version_info.pic.o):version_info.cc:(.text+0x0): first defined here\r\nbazel-out/local-fastbuild/bin/tensorflow/core/libversion_lib.a(version_info.pic.o): In function `tf_compiler_version()':\r\nversion_info.cc:(.text+0xd): multiple definition of `tf_compiler_version()'\r\nbazel-out/local-fastbuild/bin/tensorflow/core/libframework_internal.lo(version_info.pic.o):version_info.cc:(.text+0xd): first defined here\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/python/tools:optimize_for_inference failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n\r\nHow do I fix this error ?\r\n", "comments": ["Pete has a good blog post that might help. https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/\r\n\r\nOtherwise please post this question on stackoverflow with tag 'tensorflow'"]}, {"number": 6329, "title": "ValueError: Variable d_h0_conv/w/Adam/ does not exist", "body": "I got a error:\r\n\r\n`\r\nValueError: Variable d_h0_conv/w/Adam/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n`\r\n\r\nthis is DCGAN's code \r\n[dcgan-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)\r\n\r\nAnd my tensorflow version is 0.12\r\nmy ubuntu : 14.04\r\n\r\ncode detail:\r\n\r\ncon2d code\r\n\r\n`def conv2d(input_, output_dim, \r\n           k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\r\n           name=\"conv2d\"):\r\n    with tf.variable_scope(name):\r\n\r\n\r\n        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],\r\n                            initializer=tf.truncated_normal_initializer(stddev=stddev))\r\n        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\r\n\r\n        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\r\n        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\r\n\r\n        return conv`\r\n\r\nOptimizer:\r\n\r\n`\r\n        d_optim = tf.train.AdamOptimizer(config.learning_rate ,  beta1=config.beta1).minimize(self.d_loss , var_list=self.d_vars)\r\n        g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\r\n                          .minimize(self.g_loss, var_list=self.g_vars)\r\n`\r\n\r\nThanks, Now I need help!\r\n", "comments": ["This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag.", "Here is the link to StackOverflow\r\nhttp://stackoverflow.com/questions/41197939/i-got-a-error-when-running-a-github-project-in-tensorflow"]}, {"number": 6328, "title": "Fix: Remove models and core losses op from cmake build.", "body": "The sources do not exist for these anymore, therefore deleting them.", "comments": ["Ah, looking at the wrong thing. This fixes the models issues, but not the losses one.\r\nClosing this and will prepare another PR."]}, {"number": 6327, "title": "Embedding Projector: show vector labels on custom projection view", "body": "A feature request for the Embedding Projector: show vector labels as plot axis labels on custom projection view\r\n\r\nminimal example, mocked up on a screenshot:\r\n<img width=\"317\" alt=\"screen shot 2016-12-14 at 11 30 40 pm\" src=\"https://cloud.githubusercontent.com/assets/2119400/21215544/ae2c994e-c256-11e6-9a8a-5501b4592695.png\">\r\n<img width=\"1311\" alt=\"custom-projection-blocks\" src=\"https://cloud.githubusercontent.com/assets/2119400/21215550/b7a945ee-c256-11e6-9667-1ca8052c9483.png\">\r\n", "comments": ["Migrated to the TensorBoard repo"]}, {"number": 6326, "title": "Bug when generate checkpoint in distributed training", "body": "I am training model in distributed mode  with 1 ps and 2 worker all in localhost with different ports.\r\nMy model is a 4 layer with full connected network.\r\nI find that , when traing in 10,10,10,10(4 layer, each layer 10 hidden unit ), every thinks works well,\r\nBut, when enlarge network to 20,20,20,20 , the traing process works well, but can not generate checkponit any more.  with adding log , I find it's stuck at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L1372\r\nit maybe not slow , I wait for hours, nothing but stuck at it.\r\n\r\nmore:\r\nwhen traing not in distributed with same size(20,20,20,20) network,it's works well with checkpoint.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\ncannot find  any thing about this.\r\n\r\n### Environment info\r\nOperating System:\r\ncentos6.5\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nwith tf.device(tf.train.replica_device_setter(\r\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n        cluster=cluster)):\r\n    # Read TFRecords files for training\r\n    filename_queue = tf.train.string_input_producer(\r\n        tf.train.match_filenames_once(FLAGS.train),\r\n        num_epochs=epoch_number)\r\n    serialized_example = read_and_decode(filename_queue)\r\n    batch_serialized_example = tf.train.shuffle_batch(\r\n        [serialized_example],\r\n        batch_size=batch_size,\r\n        num_threads=thread_number,\r\n        capacity=capacity,\r\n        min_after_dequeue=min_after_dequeue)\r\n    features = tf.parse_example(\r\n        batch_serialized_example,\r\n        features={\r\n            \"label\": tf.FixedLenFeature([], tf.float32),\r\n            \"ids\": tf.VarLenFeature(tf.int64),\r\n            \"values\": tf.VarLenFeature(tf.float32),\r\n        })\r\n    batch_labels = features[\"label\"]\r\n    batch_ids = features[\"ids\"]\r\n    batch_values = features[\"values\"]\r\n\r\n    # Read TFRecords file for validatioin\r\n    validate_filename_queue = tf.train.string_input_producer(\r\n        tf.train.match_filenames_once(FLAGS.eval),\r\n        num_epochs=epoch_number)\r\n    validate_serialized_example = read_and_decode(validate_filename_queue)\r\n    validate_batch_serialized_example = tf.train.shuffle_batch(\r\n        [validate_serialized_example],\r\n        batch_size=validate_batch_size,\r\n        num_threads=thread_number,\r\n        capacity=capacity,\r\n        min_after_dequeue=min_after_dequeue)\r\n    validate_features = tf.parse_example(\r\n        validate_batch_serialized_example,\r\n        features={\r\n            \"label\": tf.FixedLenFeature([], tf.float32),\r\n            \"ids\": tf.VarLenFeature(tf.int64),\r\n            \"values\": tf.VarLenFeature(tf.float32),\r\n        })\r\n    validate_batch_labels = features[\"label\"]\r\n    validate_batch_ids = features[\"ids\"]\r\n    validate_batch_values = features[\"values\"]\r\n    logits = inference(batch_ids, batch_values)\r\n    batch_labels = tf.to_int64(batch_labels)\r\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,\r\n                                                                   batch_labels)\r\n    loss = tf.reduce_mean(cross_entropy, name='loss')\r\n\r\n    print(\"Use the optimizer: {}\".format(FLAGS.optimizer))\r\n\r\n    optimizer = tf.train.FtrlOptimizer(learning_rate)\r\n\r\n    global_step = tf.Variable(0, name='global_step', trainable=False)\r\n    train_op = optimizer.minimize(loss, global_step=global_step)\r\n\r\n\r\n\r\n\r\n    # Initialize saver and summary\r\n    steps_to_validate = FLAGS.steps_to_validate\r\n    init_op = tf.initialize_all_variables()\r\n\r\n    saver = tf.train.Saver(max_to_keep = 2)\r\n    keys_placeholder = tf.placeholder(\"float\")\r\n    keys = tf.identity(keys_placeholder)\r\n    tf.add_to_collection(\"inputs\", json.dumps({'key': keys_placeholder.name}))\r\n    tf.add_to_collection(\"outputs\", json.dumps({'key': keys.name,\r\n                                                'softmax': inference_softmax.name,\r\n                                                'prediction': inference_op.name}))\r\n\r\n    summary_op = tf.merge_all_summaries()\r\n\r\n\r\nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\r\n                         logdir=\"./supervisor/\",\r\n                         init_op=init_op,\r\n                         summary_op=summary_op,\r\n                         saver=saver,\r\n                         global_step=global_step,\r\n                         save_model_secs=60)\r\n\r\n# Create session to run graph\r\nwith sv.managed_session(server.target) as sess:\r\n\r\n    while not sv.should_stop():\r\n        # Get coordinator and run queues to read data\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\r\n\r\n        start_time = datetime.datetime.now()\r\n\r\n        try:\r\n            while not coord.should_stop():\r\n                _, loss_value, step = sess.run([train_op, loss, global_step])\r\n                if step % steps_to_validate == 0:\r\n                    accuracy_value, auc_value, summary_value = sess.run(\r\n                        [accuracy, auc_op, summary_op])\r\n                    end_time = datetime.datetime.now()\r\n                    print(\"[{}] Task: {}, Step: {}, loss: {}, accuracy: {}, auc: {}\".format(\r\n                        end_time - start_time,\r\n                        FLAGS.task_index,\r\n                        step, loss_value, accuracy_value,\r\n                        auc_value))\r\n\r\n                    start_time = end_time\r\n        except tf.errors.OutOfRangeError:\r\n            print(\"Done training after reading all data\")\r\n        finally:\r\n            coord.request_stop()\r\n            print(\"coord stopped\")\r\n\r\n        # Wait for threads to exit\r\n        coord.join(threads)\r\n```\r\n", "comments": ["@mrry  @michaelisard  @yaroslavvb  anyone here can help? :(", "Some random ideas -- attach to the stuck server with `gdb -p ...` and do `gdb bt` to get backtrace to see if it's problem in saver implementation. Provide which version of tensorflow you are using. Try with latest version (12rc1). Try with `sharded=True` for the Saver since it may try different codepath. Finally you could try fetching the weights manually from parameter server as numpy arrays, maybe that'll expose the problem you have with distributed receiving", "@yaroslavvb  I try add `sharded=True` it works!   But only in distribution mode need to add this. it's strange.\r\n\ud83d\udc4d you solve my problem again , thanks! ", "@yaroslavvb  sorry to reopen this issue due to the checkpoint again.\r\nafter I add sharded=True, the test seems fine with the previous 20,20,20,20 network in single machine with 2ps and 2 worker.\r\n\r\nBut when training in multi-host , the master(taskid=0) report:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"distributed_deepcake.py\", line 441, in <module>\r\n    exit(1)\r\n  File \"/home/serving/anaconda2/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 974, in managed_session\r\n    self.stop(close_summary_writer=close_summary_writer)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 802, in stop\r\n    stop_grace_period_secs=self._stop_grace_secs)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 296, in stop_on_exception\r\n    yield\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 487, in run\r\n    self.run_loop()\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 1069, in run_loop\r\n    global_step=self._sv.global_step)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1323, in save\r\n    {self.saver_def.filename_tensor_name: checkpoint_file})\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\r\n         [[Node: save/SaveV2_4 = SaveV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:2/cpu:0\"](save/ShardedFilename_4, save/SaveV2_4/tensor_names, save/SaveV2_4/shape_and_slices, global_step)]]\r\n         [[Node: save/Identity_S197 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device_incarnation=-6527834651342755590, tensor_name=\"edge_107_save/Identity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'save/SaveV2_4', defined at:\r\n  File \"distributed_deepcake.py\", line 293, in <module>\r\n    saver = tf.train.Saver(max_to_keep = 2, sharded=True)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\r\n    self.build()\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1030, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 618, in build\r\n    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 314, in _AddShardedSaveOps\r\n    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 288, in _AddShardedSaveOpsForV2\r\n    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 229, in _AddSaveOps\r\n    save = self.save_op(filename_tensor, saveables)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 172, in save_op\r\n    tensors)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 552, in save_v2\r\n    tensors=tensors, name=name)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nNotFoundError (see above for traceback): ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\r\n         [[Node: save/SaveV2_4 = SaveV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:2/cpu:0\"](save/ShardedFilename_4, save/SaveV2_4/tensor_names, save/SaveV2_4/shape_and_slices, global_step)]]\r\n         [[Node: save/Identity_S197 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device_incarnation=-6527834651342755590, tensor_name=\"edge_107_save/Identity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\n```\r\n\r\nand  the other tasks report\r\n```\r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00005-of-00006.data-00000-of-00001\r\n```\r\n\r\n\r\nI have two question:\r\n* why the master task report NotFoundError error?\r\n* why the non-master task need to access the checkpoint folder?\r\n\r\nmy code is in the original post.", "and I'm use tensorflow 0.12 rc1 .", "because the original issue seems solved . So I move the New BUG to https://github.com/tensorflow/tensorflow/issues/6374"]}, {"number": 6325, "title": "How can I know what variables or constants in a \".ckpt\" file", "body": "How can I know what variables or constants in a \".ckpt\" file?\r\n\r\nIf I restore a \".ckpt\" file without defining any variables, I will get error.\r\nHowever I just want to know what's in that \".ckpt\" file.\r\n\r\nI didn't see any function can achieve it.", "comments": ["This is a good question for stackoverflow (so that other people can\ndiscover this information easier), but there's this utility:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/inspect_checkpoint.py\n\nOn Wed, Dec 14, 2016 at 8:59 PM, oneengineer <notifications@github.com>\nwrote:\n\n> How can I know what variables or constants in a \".ckpt\" file?\n>\n> If I restore a \".ckpt\" file without defining any variables, I will get\n> error.\n> However I just want to know what's in that \".ckpt\" file.\n>\n> I didn't see any function can achieve it.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6325>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHDDC1DbzNvQ3dyXOpzYBkaU4wo5tks5rIMkfgaJpZM4LNt_j>\n> .\n>\n", "thank you yaroslavvb. I think I can close this ticket."]}, {"number": 6323, "title": "Added support for AVX512 to fixed point instructions.", "body": "", "comments": []}, {"number": 6322, "title": "No real code example for using the tensorboard embedding tab", "body": "I spent a long time yesterday trying to understand how to get something to show up in the embedding tab on Tensorboard. It is not very well documented, and there are no code examples except for the tiny, incomplete snippets in the tutorial. I was finally able to get something working, but I think it would be helpful for future users to have a real code tutorial. I could pretty easily write one for the MNIST example since I already have my own working. Is that something that people will want?", "comments": ["It also seems like there must be some code already working on MNIST to be able to generate the GIFs for the tutorial, so making those available would be nice, and maybe even less work.", "It would be great if you can provide your working version of TensorBoard embedding visualization for MNIST example. I am looking forward to playing around with it.  ", "Sure. I started with the fully_connected_feed.py file and added in the necessary stuff. It seems to work for me. I tried to follow the Google style as best as I could. I even used comments! It requires Numpy and Scipy, but that isn't too much to ask.\r\n\r\n```python\r\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n#\r\n# @author: Daniel Gordon <xkcd@cs.washington.edu>\r\n#\r\n# ==============================================================================\r\n\r\n\"\"\"Trains and Evaluates the MNIST network using a feed dictionary.\"\"\"\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\n# pylint: disable=missing-docstring\r\nimport argparse\r\nimport os.path\r\nimport sys\r\nimport time\r\n\r\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.contrib.tensorboard.plugins import projector\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nfrom tensorflow.examples.tutorials.mnist import mnist\r\n\r\nimport numpy as np\r\nimport scipy.misc\r\n\r\n\r\n# Basic model parameters as external flags.\r\nFLAGS = None\r\n\r\n\r\ndef placeholder_inputs(batch_size):\r\n  \"\"\"Generate placeholder variables to represent the input tensors.\r\n\r\n  These placeholders are used as inputs by the rest of the model building\r\n  code and will be fed from the downloaded data in the .run() loop, below.\r\n\r\n  Args:\r\n    batch_size: The batch size will be baked into both placeholders.\r\n\r\n  Returns:\r\n    images_placeholder: Images placeholder.\r\n    labels_placeholder: Labels placeholder.\r\n  \"\"\"\r\n  # Note that the shapes of the placeholders match the shapes of the full\r\n  # image and label tensors, except the first dimension is now batch_size\r\n  # rather than the full size of the train or test data sets.\r\n  images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\r\n                                                         mnist.IMAGE_PIXELS))\r\n  labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\r\n  return images_placeholder, labels_placeholder\r\n\r\n\r\ndef fill_feed_dict(data_set, images_pl, labels_pl):\r\n  \"\"\"Fills the feed_dict for training the given step.\r\n\r\n  A feed_dict takes the form of:\r\n  feed_dict = {\r\n      <placeholder>: <tensor of values to be passed for placeholder>,\r\n      ....\r\n  }\r\n\r\n  Args:\r\n    data_set: The set of images and labels, from input_data.read_data_sets()\r\n    images_pl: The images placeholder, from placeholder_inputs().\r\n    labels_pl: The labels placeholder, from placeholder_inputs().\r\n\r\n  Returns:\r\n    feed_dict: The feed dictionary mapping from placeholders to values.\r\n  \"\"\"\r\n  # Create the feed_dict for the placeholders filled with the next\r\n  # `batch size` examples.\r\n  images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size,\r\n                                                 FLAGS.fake_data)\r\n  feed_dict = {\r\n      images_pl: images_feed,\r\n      labels_pl: labels_feed,\r\n  }\r\n  return feed_dict\r\n\r\n\r\ndef do_eval(sess,\r\n            eval_correct,\r\n            images_placeholder,\r\n            labels_placeholder,\r\n            data_set,\r\n            return_results=False):\r\n  \"\"\"Runs one evaluation against the full epoch of data.\r\n\r\n  Args:\r\n    sess: The session in which the model has been trained.\r\n    eval_correct: The Tensor that returns the number of correct predictions.\r\n    images_placeholder: The images placeholder.\r\n    labels_placeholder: The labels placeholder.\r\n    data_set: The set of images and labels to evaluate, from\r\n      input_data.read_data_sets().\r\n    return_results: True if the results should be returned for the embedding.\r\n\r\n  Returns:\r\n    all_images: A list of batches of images.\r\n    all_labels: A list of batches of labels.\r\n    all_hidden1_outputs: A list of batches of embeddings from the first hidden\r\n      layer.\r\n    all_hidden2_outputs: A list of batches of embeddings from the second hidden\r\n      layer.\r\n  \"\"\"\r\n  # And run one epoch of eval.\r\n  true_count = 0  # Counts the number of correct predictions.\r\n  steps_per_epoch = data_set.num_examples // FLAGS.batch_size\r\n  num_examples = steps_per_epoch * FLAGS.batch_size\r\n  if return_results:\r\n    all_images = []\r\n    all_labels = []\r\n    all_hidden1_outputs = []\r\n    all_hidden2_outputs = []\r\n    # Get the outputs before the ReLU.\r\n    hidden1_outputs = tf.get_default_graph().get_tensor_by_name('hidden1/add:0')\r\n    hidden2_outputs = tf.get_default_graph().get_tensor_by_name('hidden2/add:0')\r\n  for step in xrange(steps_per_epoch):\r\n    feed_dict = fill_feed_dict(data_set,\r\n                               images_placeholder,\r\n                               labels_placeholder)\r\n    if return_results:\r\n      all_images.append(feed_dict[images_placeholder])\r\n      all_labels.append(feed_dict[labels_placeholder])\r\n      curr_count, hidden1_output, hidden2_output = sess.run(\r\n              [eval_correct, hidden1_outputs, hidden2_outputs],\r\n              feed_dict=feed_dict)\r\n      true_count += curr_count\r\n      all_hidden1_outputs.append(hidden1_output)\r\n      all_hidden2_outputs.append(hidden2_output)\r\n    else:\r\n      true_count += sess.run(eval_correct, feed_dict=feed_dict)\r\n  precision = float(true_count) / num_examples\r\n  print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\r\n        (num_examples, true_count, precision))\r\n  if return_results:\r\n    return (all_images, all_labels, all_hidden1_outputs, all_hidden2_outputs)\r\n\r\n\r\ndef images_to_sprite(data):\r\n    \"\"\"Creates the sprite image along with any necessary padding\r\n\r\n    Args:\r\n      data: NxHxW[x3] tensor containing the images.\r\n\r\n    Returns:\r\n      data: Properly shaped HxWx3 image with any necessary padding.\r\n    \"\"\"\r\n    if len(data.shape) == 3:\r\n        data = np.tile(data[...,np.newaxis], (1,1,1,3))\r\n    data = data.astype(np.float32)\r\n    min = np.min(data.reshape((data.shape[0], -1)), axis=1)\r\n    data = (data.transpose(1,2,3,0) - min).transpose(3,0,1,2)\r\n    max = np.max(data.reshape((data.shape[0], -1)), axis=1)\r\n    data = (data.transpose(1,2,3,0) / max).transpose(3,0,1,2)\r\n    # Inverting the colors seems to look better for MNIST\r\n    data = 1 - data\r\n\r\n    n = int(np.ceil(np.sqrt(data.shape[0])))\r\n    padding = ((0, n ** 2 - data.shape[0]), (0, 0),\r\n            (0, 0)) + ((0, 0),) * (data.ndim - 3)\r\n    data = np.pad(data, padding, mode='constant',\r\n            constant_values=0)\r\n    # Tile the individual thumbnails into an image.\r\n    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3)\r\n            + tuple(range(4, data.ndim + 1)))\r\n    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\r\n    data = (data * 255).astype(np.uint8)\r\n    return data\r\n\r\ndef run_training():\r\n  \"\"\"Train MNIST for a number of steps.\"\"\"\r\n  # Get the sets of images and labels for training, validation, and\r\n  # test on MNIST.\r\n  data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\r\n\r\n  # Tell TensorFlow that the model will be built into the default Graph.\r\n  with tf.Graph().as_default():\r\n    # Generate placeholders for the images and labels.\r\n    images_placeholder, labels_placeholder = placeholder_inputs(\r\n        FLAGS.batch_size)\r\n\r\n    # Build a Graph that computes predictions from the inference model.\r\n    logits = mnist.inference(images_placeholder,\r\n                             FLAGS.hidden1,\r\n                             FLAGS.hidden2)\r\n\r\n    # Add to the Graph the Ops for loss calculation.\r\n    loss = mnist.loss(logits, labels_placeholder)\r\n\r\n    # Add to the Graph the Ops that calculate and apply gradients.\r\n    train_op = mnist.training(loss, FLAGS.learning_rate)\r\n\r\n    # Add the Op to compare the logits to the labels during evaluation.\r\n    eval_correct = mnist.evaluation(logits, labels_placeholder)\r\n\r\n    # Build the summary Tensor based on the TF collection of Summaries.\r\n    summary = tf.summary.merge_all()\r\n\r\n    # Add the variable initializer Op.\r\n    init = tf.global_variables_initializer()\r\n\r\n    # Create a saver for writing training checkpoints.\r\n    saver = tf.train.Saver()\r\n\r\n    # Create a session for running Ops on the Graph.\r\n    sess = tf.Session()\r\n\r\n    # Instantiate a SummaryWriter to output summaries and the Graph.\r\n    summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\r\n\r\n    # And then after everything is built:\r\n\r\n    # Run the Op to initialize the variables.\r\n    sess.run(init)\r\n\r\n    # Start the training loop.\r\n    for step in xrange(FLAGS.max_steps):\r\n      start_time = time.time()\r\n\r\n      # Fill a feed dictionary with the actual set of images and labels\r\n      # for this particular training step.\r\n      feed_dict = fill_feed_dict(data_sets.train,\r\n                                 images_placeholder,\r\n                                 labels_placeholder)\r\n\r\n      # Run one step of the model.  The return values are the activations\r\n      # from the `train_op` (which is discarded) and the `loss` Op.  To\r\n      # inspect the values of your Ops or variables, you may include them\r\n      # in the list passed to sess.run() and the value tensors will be\r\n      # returned in the tuple from the call.\r\n      _, loss_value = sess.run([train_op, loss],\r\n                               feed_dict=feed_dict)\r\n\r\n      duration = time.time() - start_time\r\n\r\n      # Write the summaries and print an overview fairly often.\r\n      if step % 100 == 0:\r\n        # Print status to stdout.\r\n        print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\r\n        # Update the events file.\r\n        summary_str = sess.run(summary, feed_dict=feed_dict)\r\n        summary_writer.add_summary(summary_str, step)\r\n        summary_writer.flush()\r\n\r\n      # Save a checkpoint and evaluate the model periodically.\r\n      if (step + 1) % 1000 == 0 or (step + 1) == FLAGS.max_steps:\r\n        checkpoint_file = os.path.join(FLAGS.log_dir, 'model.ckpt')\r\n        saver.save(sess, checkpoint_file, global_step=step)\r\n        # Evaluate against the training set.\r\n        print('Training Data Eval:')\r\n        do_eval(sess,\r\n                eval_correct,\r\n                images_placeholder,\r\n                labels_placeholder,\r\n                data_sets.train)\r\n        # Evaluate against the validation set.\r\n        print('Validation Data Eval:')\r\n        do_eval(sess,\r\n                eval_correct,\r\n                images_placeholder,\r\n                labels_placeholder,\r\n                data_sets.validation)\r\n        # Evaluate against the test set.\r\n        print('Test Data Eval:')\r\n        do_eval(sess,\r\n                eval_correct,\r\n                images_placeholder,\r\n                labels_placeholder,\r\n                data_sets.test)\r\n\r\n    # Compute embeddings and save them.\r\n    thumbnail_size = int(np.sqrt(mnist.IMAGE_PIXELS))\r\n    for data_set, name in [\r\n            (data_sets.train, 'train'),\r\n            (data_sets.validation, 'validation'),\r\n            (data_sets.test, 'test')]:\r\n      output_path = os.path.join(FLAGS.log_dir, 'embed', name)\r\n      print('Computing %s Embedding' % name)\r\n      (all_images, all_labels, hidden1_vectors, hidden2_vectors) = do_eval(\r\n              sess,\r\n              eval_correct,\r\n              images_placeholder,\r\n              labels_placeholder,\r\n              data_set,\r\n              True)\r\n      embed_tensors = []\r\n      summary_writer = tf.summary.FileWriter(output_path, sess.graph)\r\n      config = projector.ProjectorConfig()\r\n      for layer, embed_vectors in enumerate([hidden1_vectors, hidden2_vectors]):\r\n        embed_tensor = tf.Variable(\r\n                np.array(embed_vectors).reshape(\r\n                    len(embed_vectors) * embed_vectors[0].shape[0], -1),\r\n                name=('%s_layer_%s' % (name, layer)))\r\n        embed_tensors.append(embed_tensor)\r\n        sess.run(embed_tensor.initializer)\r\n        embedding = config.embeddings.add()\r\n        embedding.tensor_name = embed_tensor.name\r\n        embedding.metadata_path = os.path.join(output_path, 'labels.tsv')\r\n        embedding.sprite.image_path = os.path.join(output_path, 'sprite.png')\r\n        embedding.sprite.single_image_dim.extend(\r\n                [thumbnail_size, thumbnail_size])\r\n        projector.visualize_embeddings(summary_writer, config)\r\n      result = sess.run(embed_tensors)\r\n      saver = tf.train.Saver(embed_tensors)\r\n      saver.save(sess, os.path.join(output_path, 'model.ckpt'), layer)\r\n\r\n      # Make sprite and labels.\r\n      images = np.array(all_images).reshape(\r\n              -1, thumbnail_size, thumbnail_size).astype(np.float32)\r\n      sprite = images_to_sprite(images)\r\n      scipy.misc.imsave(os.path.join(output_path, 'sprite.png'), sprite)\r\n      all_labels = np.array(all_labels).flatten()\r\n      metadata_file = open(os.path.join(output_path, 'labels.tsv'), 'w')\r\n      metadata_file.write('Name\\tClass\\n')\r\n      for ll in xrange(len(all_labels)):\r\n        metadata_file.write('%06d\\t%d\\n' % (ll, all_labels[ll]))\r\n      metadata_file.close()\r\n\r\ndef main(_):\r\n  if tf.gfile.Exists(FLAGS.log_dir):\r\n    tf.gfile.DeleteRecursively(FLAGS.log_dir)\r\n  tf.gfile.MakeDirs(FLAGS.log_dir)\r\n  run_training()\r\n\r\n\r\nif __name__ == '__main__':\r\n  parser = argparse.ArgumentParser()\r\n  parser.add_argument(\r\n      '--learning_rate',\r\n      type=float,\r\n      default=0.01,\r\n      help='Initial learning rate.'\r\n  )\r\n  parser.add_argument(\r\n      '--max_steps',\r\n      type=int,\r\n      default=2000,\r\n      help='Number of steps to run trainer.'\r\n  )\r\n  parser.add_argument(\r\n      '--hidden1',\r\n      type=int,\r\n      default=128,\r\n      help='Number of units in hidden layer 1.'\r\n  )\r\n  parser.add_argument(\r\n      '--hidden2',\r\n      type=int,\r\n      default=32,\r\n      help='Number of units in hidden layer 2.'\r\n  )\r\n  parser.add_argument(\r\n      '--batch_size',\r\n      type=int,\r\n      default=100,\r\n      help='Batch size.  Must divide evenly into the dataset sizes.'\r\n  )\r\n  parser.add_argument(\r\n      '--input_data_dir',\r\n      type=str,\r\n      default='/tmp/tensorflow/mnist/input_data',\r\n      help='Directory to put the input data.'\r\n  )\r\n  parser.add_argument(\r\n      '--log_dir',\r\n      type=str,\r\n      default='/tmp/tensorflow/mnist/logs/fully_connected_feed',\r\n      help='Directory to put the log data.'\r\n  )\r\n  parser.add_argument(\r\n      '--fake_data',\r\n      default=False,\r\n      help='If true, uses fake data for unit testing.',\r\n      action='store_true'\r\n  )\r\n\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```", "I also was confused like hell by the description. I created a notebook, maybe it helps someone. It's not about MNIST but with CIFAR \r\n\r\nhttps://github.com/oduerr/dl_tutorial/blob/master/tensorflow/debugging/embedding.ipynb", "Thanks for ending the confusion @danielgordon10 @oduerr! I first thought there should be a direct way of writing a checkpoint from an operation's output. However, it seem you have to create a temporary variable first, initialize it with embeddings from a previous run and then save it. ", "@danielgordon10 @oduerr most wonderful, thanks!\r\nThis should definitely be part of the official docu/samples! ", "Should I make a PR or something? Would this go in Contrib somewhere or with the rest of the MNIST examples?", "Sorry for the slow response, this was assigned to me while I was on vacation.\r\n\r\nWe would be grateful to receive pull requests that improve the documentation. @dsmilkov would be the best person to review", "@dsmilkov where should this code live? I can't really do a pull request if I don't know where to put the code.", "Corresponding Stack Exchange question: [TensorBoard Embedding Example?](http://stackoverflow.com/q/41258391/395857)", "@danielgordon10 Thank you for the sample code, so far it is the only example I have gotten to work at all for visualizing the embedding.. (using TF1.0 and P36 fwiw)However while it produces a pretty picture I cannot make sense of the labels.. they are three digit numbers when hovering over the dots in the 3d graph.. Although when I select the softmax layer I get 1-two digits when hovering.. Some further description of the utility of the projection and the meaning of the various components of the model (Component 1-3 in the selector boxes for instance) would be super helpful!", "If interested, some code that uses TensorBoard embeddings to display character and word embeddings:\r\nhttps://github.com/Franck-Dernoncourt/NeuroNER\r\n\r\nExample:\r\n\r\n![image](https://cloud.githubusercontent.com/assets/15331/23913783/609a87ea-08ba-11e7-91cf-0c0ba9765b53.png)\r\n\r\n![image](https://cloud.githubusercontent.com/assets/15331/23913794/6bad7ed0-08ba-11e7-9ef2-aadc0a747bea.png)\r\n\r\nFYI: [How can I select which checkpoint to view in TensorBoard's embeddings tab?](http://stackoverflow.com/q/42679552/395857)", "For anyone struggling to get tensorboard embeddings working, I would suggest the [standalone embeddings](https://github.com/tensorflow/embedding-projector-standalone). It has example input files which were a massive help for me. For learning, it is **much easier** to create embedding data using the final testing code rather than the training code as suggested above. So the question is no longer about embedding examples but how to prepare embedding data.\r\n\r\nHere's a beautiful example of what you can do http://onefoldmedia.com/sites/default/super_t-sne/", "Simple example of embedding visualization\r\nhttps://github.com/ElchinValiyev/tf-embedding-visualization-demo", "You can also check out my [TF Dev summit talk](https://www.youtube.com/watch?v=eBbEDRsCmv4) and associated [code](https://github.com/dandelionmane/tf-dev-summit-tensorboard-tutorial).\r\n\r\nAnyway, we should add a canonical demo for the projector. I'm going to migrate this issue to tensorflow/tensorboard.", "@danielgordon10  I run the code successfully. But when i open `localhost:6006/#embeddings`, tensorboard `fetching sprite image... parsing metadata`  takes forever. There are `#graph`and `#scalars` . I reduce the `bach_size`, `step_max`,`embedding data size`, but `embedding` doesn't work. How to do?", "In case anyone is interested, I have built a [code example on how to use embeddings in TensorBoard](https://github.com/Vooban/Autoencoder-TensorBoard-t-SNE).\r\n\r\nBasically, it is based off @normanheckscher 's [code for TensorBoard](https://github.com/normanheckscher/mnist-tensorboard-embeddings) but with @pkmital 's [autoencoder](https://github.com/pkmital/tensorflow_tutorials/blob/master/python/07_autoencoder.py). Therefore the autoencoder is used for its embeddings (a.k.a. \"code\", or \"compressed representation\") to be visualized in TensorBoard. I managed to merge and document those two pieces of code in the simplest possible way just as to make a quick working demo. ", "@EddieOne  , i like your solution the best, but I am not sure what the tensor.bytes are. \r\nI am planning to use keras. What should the format of the .bytes file be ?", "While by now there are many MNIST-based examples, for me they still relied too much on pre-trained data and pre-generated sprite & metadata files. To fill this gap I created such an example myself and decided to share it here for anyone interested - the code is on [GitHub](https://github.com/altermarkive/tensorflow-embeddings-minimalistic-example) and a slide deck run-through [here](https://altermarkive.github.io/tensorflow-embeddings-minimalistic-example/). I hope it helps anyone who stumbles upon the same challenge.", "https://medium.com/looka-engineering/how-to-visualize-feature-vectors-with-sprites-and-tensorflows-tensorboard-3950ca1fb2c7\r\nhere is an article to resolve this issue on image data."]}, {"number": 6321, "title": "Fixed typo in embedding tutorial.", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 6320, "title": "Added \"Machine Learning with TensorFlow\" book", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@vincentvanhoucke, fixed!"]}, {"number": 6319, "title": "Occasional \"Connection timed out\" in distributed TensorFlow", "body": "We are occasionally observing crashes like below (training on AWS, seeing couple of crashes per day). It looks like in gRPC, recvmsg returns with \"Connection timed out\". The error points to [this line](https://github.com/grpc/grpc/blob/a98778f33dc602ad474b19e2f5243972722e758e/src/core/lib/iomgr/tcp_posix.c#L224). Default behavior in TensorFlow seems to be \"wait forever\", is there way to have same behavior for slow grpc connections?\r\n\r\n@mrry\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 972, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 954, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors.py\", line 463, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors.UnavailableError: {\"created\":\"@1481750298.553120216\",\"description\":\"OS Error\",\"errno\":110,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":229,\"grpc_status\":14,\"os_error\":\"Connection timed out\",\"syscall\":\"recvmsg\"}\r\n     [[Node: ExponentialMovingAverage/decay_S37 = _Recv[client_terminated=false, recv_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device=\"/job:worker/replica:0/task:2/cpu:0\", send_device_incarnation=3009888913849196393, tensor_name=\"edge_1032_ExponentialMovingAverage/decay\", tensor_type=DT_FLOAT, _device=\"/job:ps/replica:0/task:0/cpu:0\"]()]]\r\n     [[Node: gradients/truediv_grad/Shape_1_S63 = _HostRecv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:2/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device_incarnation=-6688974477600735349, tensor_name=\"edge_115_gradients/truediv_grad/Shape_1\", tensor_type=DT_INT32, _device=\"/job:worker/replica:0/task:2/cpu:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"bin/worker\", line 231, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"bin/worker\", line 228, in main\r\n    run_generic(server, args, trainer, num_global_steps=num_global_steps)\r\n  File \"bin/worker\", line 152, in run_generic\r\n    trainer.process(sess)\r\n  File \"/experiment/vnc-agents/trainers/base.py\", line 351, in process\r\n    fetched = sess.run(fetches, feed_dict=feed_dict)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 717, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 915, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 965, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 985, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors.UnavailableError: {\"created\":\"@1481750298.553120216\",\"description\":\"OS Error\",\"errno\":110,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":229,\"grpc_status\":14,\"os_error\":\"Connection timed out\",\"syscall\":\"recvmsg\"}\r\n     [[Node: ExponentialMovingAverage/decay_S37 = _Recv[client_terminated=false, recv_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device=\"/job:worker/replica:0/task:2/cpu:0\", send_device_incarnation=3009888913849196393, tensor_name=\"edge_1032_ExponentialMovingAverage/decay\", tensor_type=DT_FLOAT, _device=\"/job:ps/replica:0/task:0/cpu:0\"]()]]\r\n     [[Node: gradients/truediv_grad/Shape_1_S63 = _HostRecv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:2/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device_incarnation=-6688974477600735349, tensor_name=\"edge_115_gradients/truediv_grad/Shape_1\", tensor_type=DT_INT32, _device=\"/job:worker/replica:0/task:2/cpu:0\"]()]]\r\n```", "comments": ["Hm, just simulated a hang locally, and the connection resumes successfully 6 hours later so it seems manual timeout is not a problem and it's probably something in grpc/recvmsg layer", "Hi @yaroslavvb @mrry \r\n\r\nI've also came across with the issue. I've implemented an online learning service using distributed TensorFlow where the training data is available through external gRPC calls (using separate implementation from the gRPC service in TF). The code works fine when we continuously feeding training data. But when stops the data feeding for a period and then resumes, session.run operation sometimes get stuck. The training service is hosted on Azure spanning 3 nodes. TF version is 0.12RC1 and OS version Ubuntu16.04. \r\n\r\nThanks!\r\n\r\n<pre>\r\nTraceback (most recent call last):\r\n  File \"/home/XXX/OnlineLearningService/python/service/dispatcher.py\", line 75, in update_explr_rate\r\n    current_training_step = self.session.run(self.global_step)\r\n  File \"/home/XXX/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 717, in run\r\n    run_metadata_ptr)\r\n  File \"/home/XXX/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 915, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/XXX/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/XXX/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 985, in _do_call\r\n    raise type(e)(node_def, op, message)\r\nUnavailableError: {\"created\":\"@1481767018.578522717\",\"description\":\"OS Error\",\"errno\":110,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":229,\"grpc_status\":14,\"os_error\":\"Connection timed out\",\"syscall\":\"recvmsg\"}\r\n</pre>", "So my reading of this error is that `recvmsg` call at [this line](https://github.com/grpc/grpc/blob/a98778f33dc602ad474b19e2f5243972722e758e/src/core/lib/iomgr/tcp_posix.c#L224) of grpc code returned with `ETIMEDOUT` error\r\n\r\n```\r\nGPR_TIMER_BEGIN(\"recvmsg\", 0);\r\n  do {\r\n    read_bytes = recvmsg(tcp->fd, &msg, 0);\r\n  } while (read_bytes < 0 && errno == EINTR);\r\n  GPR_TIMER_END(\"recvmsg\", read_bytes >= 0);\r\n```\r\n\r\nThere's a way to set timeout for a socket using `setsockopt`, but I couldn't find TensorFlow would be doing that by default, and I tested that it can successfully hang for 6 hours. So I guess the question is -- why would `recvmsg` fail with a timeout error? Perhaps that the behavior when connection is dropped/interrupted?"]}, {"number": 6317, "title": "adding cupti_wrapper error solution", "body": "Adding solution instructions of #6287 to \"Common Problems\"", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?"]}, {"number": 6316, "title": "training on one GPU, but the other one is also occupied", "body": "Hi,\r\nI have two GPUs, and I just would like to use one GPU to train a network by tensorflow. When I train it, the code use all the memories of two GPUs, but only one GPU is working:\r\n![screenshot from 2016-12-14 20-12-57](https://cloud.githubusercontent.com/assets/15608199/21197459/cfbcc86a-c23b-11e6-88d5-e04f7537e676.png)\r\n\r\n\r\nHow to solve this problem? I would like just one to work, but not occupy the other.\r\n\r\n", "comments": ["http://stackoverflow.com/questions/41150462/tensorflow-training-network-on-one-gpu-but-occupy-the-space-of-the-other", "This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag."]}, {"number": 6315, "title": "Session initialization must not require values for tensors with dtype string", "body": "### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nCUDA 8.0\r\nTF 0.12.0-rc1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI create a string variable, among others, then try to initialize the session:\r\n```\r\nmode = tf.placeholder(tf.string, name='mode')\r\n...\r\n\r\ninit = tf.global_variables_initializer()\r\nsession.run(init)\r\n```\r\n\r\nThis results in\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'mode' with dtype string\r\n\t [[Node: mode = Placeholder[dtype=DT_STRING, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\t [[Node: mode/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_360_mode\", tensor_type=DT_STRING, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```\r\n\r\nFix for me is to set `session.run(init, feed_dict={mode: 'foo'})`\r\nBut it looks like a regression since rc0.", "comments": ["That seems like intended behavior? (assuming your graph `init` has a dependency on `mode)", "Hi @yaroslavvb. Doesn't look like so.\r\nSession init doesn't require other values for variables, even if there is a dependency.\r\nIt's also never mentioned in the doc: https://www.tensorflow.org/how_tos/variables/\r\n", "Can you post reproducible example?\r\n\r\n`initialize_all_variables` is a regular tensorflow op, so it follows the regular TF semantics and all dependent nodes must be available\r\n\r\nFor instance, following two examples fail because dependent nodes are not available\r\n```\r\na = tf.Variable(tf.placeholder(tf.float32))\r\nsess.run(tf.initialize_all_variables())\r\n```\r\n\r\n```\r\na = tf.Variable(1)\r\nb = tf.Variable(a)\r\nsess.run(tf.initialize_all_variables())\r\n```", "My model is somewhat big to post it here. I have tried to make a few-lines example, but it didn't reproduce. Essentially the `mode` variable was only used in `tf.cond` op, that's all. Hope you'll be a able to find a small example.\r\n\r\nYour examples fail indeed, but differently (on my machine):\r\n```\r\n>>> a = tf.Variable(tf.placeholder(tf.float32))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 215, in __init__\r\n    dtype=dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 307, in _init_from_args\r\n    self._initial_value)\r\nValueError: initial_value must have a shape specified: Tensor(\"Placeholder:0\", dtype=float32)\r\n```\r\n\r\nIn my case the graph was successfully constructed, and the error occurred only on init step.", "Sounds like a documentation issue then? (to clarify that placeholders that are dependencies of variable init ops must be fed when running initialization)", "Why isn't it required in simple cases?\r\n\r\n```\r\nmode = tf.placeholder(tf.string, name='mode')\r\nlayer = tf.Variable(0)\r\nlayer = tf.cond(tf.equal(mode, 'bar'), lambda: layer, lambda: 2 * layer)\r\ninit = tf.global_variables_initializer()\r\nwith tf.Session() as session:\r\n  session.run(init)\r\n```\r\n", "In that example `tf.placeholder` is not a dependency of init op. My guess is that in your actual model you introduce this dependency.", "Thanks Yaroslav. This does sound like a documentation issues. Assigning to add documentation to clarify that placeholders that are dependencies of variable init must be fed when running initialization.", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow."]}, {"number": 6314, "title": "Error 404 when downloading Tensorflow on Windows", "body": "The links provided on the website to the .whl used to install Tensorflow on Windows seem to be broken : https://www.tensorflow.org/get_started/os_setup#pip_installation_on_windows\r\nI'm getting an HTTP Error 404.\r\nI found the CPU Build elsewhere but I can't find the GPU build which I would like tu use.", "comments": ["same", "Probably same issue as #6275.\r\n\r\nWhile it's getting fixed could you try with this link for CPU:\r\n`https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl`\r\n\r\nAnd for GPU downloading the nightly build wheel on:\r\n`http://ci.tensorflow.org/view/Nightly/job/nightly-win/14/DEVICE=gpu,OS=windows/`\r\n\r\nAnother alternative is to run:\r\n`pip install tensorflow`\r\nand for gpu support:\r\n`pip install tensorflow-gpu`", "Awesome! It works, thank you very much!", "@gunan @yifeif Can we modify the website to point at the existing RC? (I think it's broken for all operating systems right now, but it mostly seems to be Windows users who're affected for some reason....)", "The windows links from tensorflow website should be working now. Sorry for the inconvenience. "]}, {"number": 6313, "title": "Tensorboard has wrong logdir on Windows", "body": "I use Tensorflow 0.12rc1 on Windows. \"Tensorboard\" has problems parsing the `logdir`. My log-directory is at `E:\\tmp\\tflogs`. When I start tensorboard from the windows shell (cmd.exe) with:\r\n\r\n`tensorboard --logdir=E:\\tmp\\tflogs`\r\n\r\nit depends on the current directory / drive if it works or not (i.e., if tensorboard finds the runs and shows the data). If the current directory is, e.g., on the `C:\\` drive, tensorboard tries to add runs from `c:\\tmp\\tflogs` instead of `E:\\tmp\\tflogs`. \r\n\r\nExample:\r\n`\r\nC:\\Users\\wrammer>tensorboard --logdir=E:\\tmp\\tflearn_logs --debug\r\n`\r\nThe log contains lines such as:\r\n`\r\nINFO:tensorflow:Starting AddRunsFromDirectory: C:\\tmp\\tflearn_logs\r\n`\r\n\r\n", "comments": ["I think this is caused by the TensorBoard [syntax for the `--logdir` flag](https://github.com/tensorflow/tensorflow/blob/dbe5e17e2ed307e86e1a6e79e558ec3e335d46fc/tensorflow/tensorboard/tensorboard.py#L36):\r\n\r\n> \r\n> `logdir` specifies the directory where TensorBoard will look to find TensorFlow event files that it can display. TensorBoard will recursively walk the directory structure rooted at logdir, looking for `.*tfevents.*` files.\r\n>\r\n> You may also pass a comma separated list of log directories, and TensorBoard will watch each directory. You can also assign names to individual log directories by putting a colon between the name and the path, as in\r\n>\r\n> ```\r\n> tensorboard --logdir=name1:/path/to/logs/1,name2:/path/to/logs/2\r\n> ```\r\n\r\n...which clearly doesn't play well with Windows paths with colons :).\r\n\r\nI'll defer to the TensorBoard team about how they'd like to handle this. One workaround could be to use `file:///` URIs instead of paths, where we could add support for `|` in place of `:` in Windows filenames. (You might also be able to use an \"administrative share\" `\\\\MachineName\\e$\\tmp\\tflogs` if your machine is configured to allow that.)", "Yes this is something we should fix. But it might not be necessary to change the syntax. It might be possible to say, ok, this is a single letter followed by `:\\`, it's probably not a label.\r\n\r\nAs a workaround, are you able to say `--logdir=foo:E:\\tmp\\tflogs`?", "I had the same problem and can confirm that `--logdir foo:D:\\some\\path` does indeed work on windows with tensorflow 0.12 while `--logdir D:\\some\\path` does not.", "I had the same problem with tensorflow 0.12.1. Thanks to @kvothe it now works for me by adding a name before E:\\some\\path", "The solution of @kvothe also works for tensorflow 1.0.1 (on Windows 10)", "I migrated this issue to tensorflow/tensorboard#54 because TensorBoard has moved to a new repository. Lets continue discussion on this bug there.", "@jiyuan312986471 \uff0c what did you add before path\uff1f I came into the same problem, would you like to share your solution? ", "See @dandelion's comment here for a solution - let me know if that works: https://github.com/tensorflow/tensorboard/issues/52\r\n\r\nSpecifically, the colon in the directory path is problematic, so try relative paths. You could also always specify a run in the logdir argument."]}, {"number": 6312, "title": "Merge pull request #1 from tensorflow/master", "body": "get new code from original author", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 6311, "title": "Problem with specifying worker_device for contrib.estimator", "body": "Hello\r\n\r\nIn `tensorflow/contrib/learn/python/learn/estimator/estimator.py`, the function `_get_replica_device_setter` will specify the `worker_device` we will use. The function is as follows.\r\n\r\n```python\r\ndef _get_replica_device_setter(config):\r\n  \"\"\"Creates a replica device setter if required.\r\n\r\n  Args:\r\n    config: A RunConfig instance.\r\n\r\n  Returns:\r\n    A replica device setter, or None.\r\n  \"\"\"\r\n  ps_ops = [\r\n      'Variable', 'AutoReloadVariable', 'MutableHashTable',\r\n      'MutableHashTableOfTensors', 'MutableDenseHashTable'\r\n  ]\r\n\r\n  if config.task_type:\r\n    worker_device = '/job:%s/task:%d' % (config.task_type, config.task_id)\r\n  else:\r\n    worker_device = '/job:worker'\r\n\r\n  if config.num_ps_replicas > 0:\r\n    return device_setter.replica_device_setter(\r\n        ps_tasks=config.num_ps_replicas, worker_device=worker_device,\r\n        merge_devices=True, ps_ops=ps_ops, cluster=config.cluster_spec)\r\n  else:\r\n    return None\r\n```\r\n\r\nHowever, if our device has name like `/job:0/replica:0/task:0`, then what should do?", "comments": ["This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag."]}, {"number": 6310, "title": "Multiple GPUs: Out of Memory", "body": "Hi,\r\n\r\nI used to train my model on a single GPU with a batch size of ```20``` (images) without a problem. Now I want to take advantage of multiple GPU training. Currently I'm using 4 GPUs and want to do **data-parallel replicated training** with a batch size of ```20*4```.\r\n\r\nIf I understood it correctly, each batch will be splitted into 4 equal parts (in this case for every model on each GPU, the batch size will be ```20```) and be feeded to each GPU.  But I'm getting out of memory error:\r\n\r\n```\r\n...\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:274] ********************************************************************************************\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 1.32GiB.  See logs for memory state.\r\nW tensorflow/core/framework/op_kernel.cc:975] Resource exhausted: OOM when allocating tensor with shape[80,921600,6]\r\n...\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[80,921600,6]\r\n```\r\nany suggestions will be greatly appreciated!", "comments": ["From my understanding, you didn't catch how multi-gpus training works, imaging you want to train 10 epochs and you have 2 gpus, and then you manually set `num_epochs = 5`, both of your gpu will take 5 epochs training cocurrently and that's why your training time can be a half (neglecting network consumption). It's not like what you think about the multi-gpus training that splits a single batch to mini-batch. Back to you problem, a single gpu can only process 20 examples at a time, then you set it 80 so your gpu ran oom. The way to solve your problem is keeping  `batch_size = 20` and decreasing `num_epochs`(make sure you write code properly that collects gradients from all gpus to central cpu and do averaging gradients and updating of variables)", "This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag.", "I am getting the same problem. I want to train 30 images including two classes. I am actually running Tensorflow in a CPU version, the batch size is actually 24. Is there any help for my thing ? ", "Actually, I was running it from Virtual Machine . I just increased the internal memory and it works fine now. \r\nYou can try like this. ", "\r\n\r\nOn 22 Nov 2017 09:53, satishbro <notifications@github.com> wrote:\r\n\r\nActually, I was running it from Virtual Machine . I just increased the internal memory and it works fine now.\r\nYou can try like this.\r\n\r\n\u2014\r\nYou are receiving this because you are subscribed to this thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/6310#issuecomment-346299142>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ASmaYvAtUcEfLXXZsACodNAMnZu3CPDHks5s4-8ugaJpZM4LM1vl>.\r\n\r\n", "> From my understanding, you didn't catch how multi-gpus training works, imaging you want to train 10 epochs and you have 2 gpus, and then you manually set `num_epochs = 5`, both of your gpu will take 5 epochs training cocurrently and that's why your training time can be a half (neglecting network consumption). It's not like what you think about the multi-gpus training that splits a single batch to mini-batch. Back to you problem, a single gpu can only process 20 examples at a time, then you set it 80 so your gpu ran oom. The way to solve your problem is keeping `batch_size = 20` and decreasing `num_epochs`(make sure you write code properly that collects gradients from all gpus to central cpu and do averaging gradients and updating of variables)\r\n\r\nCan you please elaborate more on this ? Usually \"Images per GPU\" is pushed and that gets multiplied by \"no. of GPUs\" given. for examples, batch of 20 per GPU and num GPU is 4, that gets multiplied and a total of 20 x 4 = 80 samples are pushed.\r\n\r\n@zakizhou  Am I missing something in your comments ?\r\n\r\nHere is the reference from tensorflow documentation, [setup input pipeline](https://www.tensorflow.org/tutorials/distribute/keras)\r\n\r\nstackoverflow link for similar issue: [OOM error](https://datascience.stackexchange.com/questions/47274/why-i-get-oom-error-although-my-model-is-not-that-large)\r\n\r\n"]}, {"number": 6309, "title": "Expose DataTypeSize as TF_DataTypeSize in C API", "body": "DataTypeSize is a greatly useful utility function for allocating the\r\nright amount of memory when using the C API for (dynamic) language\r\nwrappers using an FFI.", "comments": ["Can one of the admins verify this patch?", "NB: This is my first pull request to tensorflow. If I missed something in some style guide or otherwise, please let me know. If there's something I can do to make it easier to merge (What's with the various checks? Are this automated?), same thing, feedback welcome and I'm happy to try to do the leg work.", "@josh11b can you take a look or designate someone who can reason sensibly about the C API?", "I force re-pushed the branch with the edits that @josh11b recommended. Let me know if this is good as is.\r\n\r\nFor reference, I also have no issues with somebody pushing a modified commit under their name - I'm not in it for the glory and this is as trivial a change as it gets.", "Jenkins, test this please.", "Let's make this a size_t return type? To match the mentioned conventions:\r\n`// * size_t is used to represent byte sizes of objects that are\r\n//   materialized in the address space of the calling process.`", "@asimshankar fyi", "Completely agreed on the size_t change. I'd kept it consistent with the C++ API for an ill-advised sense of consistency and in case I was missing some hidden reason why it would be int. Repushed the branch.", "Jenkins, test this please"]}, {"number": 6308, "title": "Bug in error message from dynamic_rnn", "body": "When using inputs of dimention 2 instead of 3 I get following error\r\n\r\n\"ValueError: Dimension must be 2 but is 3 for 'transpose' \"\r\n\r\nI believe the numbers in the error message are switched.", "comments": ["This may help you:\r\nhttps://github.com/sherjilozair/char-rnn-tensorflow/issues/58#issuecomment-257380780", "Fogelton, does that advice solve your problem?", "I did not have any problem, I just reported a bug in tensorflow, that those two numbers are switched in the error message. I found out so I reported it.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Ping. @ebrevdo , any updates?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think this can be closed. ", "Assuming this is fixed."]}, {"number": 6307, "title": "Duplicate content on the new website", "body": "In the Updates section of the new TensorFlow website, two news have the same text:\r\n\r\n[![image](https://cloud.githubusercontent.com/assets/1140359/21177890/64615340-c1ed-11e6-82bc-c440ebc9dfd2.png)](https://www.tensorflow.org/)\r\n", "comments": ["@tfboyd ", "This is fixed."]}, {"number": 6306, "title": "Windows GPU pip install http error", "body": "pip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.0rc1-cp35-cp35m-win_amd64.whl\r\n\r\n\r\nCollecting tensorflow-gpu==0.12.0rc1 from https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.0rc1-cp35-cp35m-win_amd64.whl\r\n  **HTTP error 404 while getting** https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.0rc1-cp35-cp35m-win_amd64.whl\r\n  Could not install requirement tensorflow-gpu==0.12.0rc1 from https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.0rc1-cp35-cp35m-win_amd64.whl because of error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.0rc1-cp35-cp35m-win_amd64.whl", "comments": ["Duplicate #6275"]}, {"number": 6305, "title": "RNN Variables mistakenly (?) placed on GPU when doing multi tower training. ", "body": "I'm trying to do \"data parallelism\" following the cifar-10 multi gpu example. Main difference is that I am using tensorflow's rnn abstractions. When I run the code ( [mgf.tar.gz](https://github.com/tensorflow/tensorflow/files/651211/mgf.tar.gz) ) on a CPU machine it works but when I run it on a machine with 1-4 gpus it crashes, reporting that some of the GRU/LSTm variables have not been initialized (on the gpu). \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"trainMultiGPU.py\", line 211, in <module>\r\n    tf.app.run()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"trainMultiGPU.py\", line 207, in main\r\n    train(args)\r\n  File \"trainMultiGPU.py\", line 171, in train\r\n    sess.run(init)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\n\r\n\r\n\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value RNN/GRUCell/Gates/Linear/Matrix\r\n         [[Node: RNN/GRUCell/Gates/Linear/Matrix/_52 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_149_RNN/GRUCell/Gates/Linear/Matrix\", _device=\"/job:localhost/replica:0/task:0/gpu:0\"](RNN/GRUCell/Gates/Linear/Matrix)]]\r\n         [[Node: init/NoOp_1/_62 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_164_init/NoOp_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]`**\r\n\r\n```\r\nI've tried this both with the vanilla and dynamic variations of the rnn functions and with LSTM as well as GRU. \r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nIssue #1390  \r\n\r\n\r\n### Environment info\r\nOperating System:\r\nRHEL 7.2\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n> [tal@E8A2-DL380G90-01-1G multi_gpu_lstm_fail]$ ls -l /usr/local/cuda/lib64/libcud*\r\n> -rw-r--r--. 1 root root   558720 Dec  6 10:55 /usr/local/cuda/lib64/libcudadevrt.a\r\n> lrwxrwxrwx. 1 root root       16 Dec  6 10:55 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\n> lrwxrwxrwx. 1 root root       19 Dec  6 10:55 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n> -rwxr-xr-x. 1 root root   415432 Dec  6 10:55 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n> -rw-r--r--. 1 root root   775162 Dec  6 10:55 /usr/local/cuda/lib64/libcudart_static.a\r\n> -rwxr-xr-x. 1 root root 79337624 Dec 13 10:51 /usr/local/cuda/lib64/libcudnn.so\r\n> -rwxr-xr-x. 1 root root 79337624 Dec 13 10:51 /usr/local/cuda/lib64/libcudnn.so.5\r\n> -rwxr-xr-x. 1 root root 79337624 Dec 13 10:51 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n> -rw-r--r--. 1 root root 69756172 Dec 13 10:51 /usr/local/cuda/lib64/libcudnn_static.a\r\n> \r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.0rc1-cp27-none-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n[tal@E8A2-DL380G90-01-1G multi_gpu_lstm_fail]$ python -c \"import tensorflow; \r\n\r\n> print(tensorflow.__version__)\"\r\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\n> 0.12.0-rc1\r\n> \r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI've attached a tar with a minimal reproducing example. Unfortunately I am behind a corporate proxy and can't push to github. \r\n[[mgf.tar.gz - reproducible example ](https://github.com/tensorflow/tensorflow/files/651211/mgf.tar.gz)\r\nTo recreate the example simply run \r\n\r\n> python trainMultiGPU.py\r\nIt should work fine on a CPU only machine. On a machine with gpus you should see the above traceback. \r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n- I've tried variants of rnn (dynamic, regular, bidirectional)\r\n- I've tried LSTMCell and GRUCell\r\n- I've tried passing the scope explicitly to the call to rnn\r\n- I've tried wrapping the call to rnn with a device placement on the cpu. This prevents the traceback but I think it places the OP on the cpu as well as processing becomes incredibly slow. \r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n[device_placements.txt](https://github.com/tensorflow/tensorflow/files/651206/device_placements.txt)\r\n[nvidia-smi.out.txt](https://github.com/tensorflow/tensorflow/files/651207/nvidia-smi.out.txt)\r\n[TRACEBACK.txt](https://github.com/tensorflow/tensorflow/files/651208/TRACEBACK.txt)\r\n\r\n\r\n\r\n\r\n", "comments": ["Looked at your code. Aren't you doing the same thing as Issue #1390, i.e running the op to initialize all variables before starting queue runners?", "I tried both ways, it doesn't seem to make a difference. \r\n\r\neg running like in #1390\r\n   ```\r\n    sess = tf.Session(config=tf.ConfigProto(\r\n                allow_soft_placement=True,\r\n                log_device_placement=False))\r\n\r\n        # Start the queue runners.\r\n        tf.train.start_queue_runners(sess=sess)\r\n        init = tf.global_variables_initializer()\r\n        sess.run(init)\r\n\r\n\r\n```\r\nor how I posted\r\n  ```\r\n    sess = tf.Session(config=tf.ConfigProto(\r\n                allow_soft_placement=True,\r\n                log_device_placement=False))\r\n\r\n        # Start the queue runners.\r\n        init = tf.global_variables_initializer()\r\n        sess.run(init)\r\n\r\n        tf.train.start_queue_runners(sess=sess)\r\n```\r\nDoesn't make a difference. Either way the same error appears ", "Looking at the output, it looks like a Send is consuming the variable before it is initialized, which suggests that the variable is being read as part of another initialization that is occuring on CPU.\r\n\r\nWithout being able to debug into your graph too much, I would try to find which GPU variable is being read on CPU, and to make sure that you are using https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variables.py#L515 (initialized_value) if you are trying to initialize a value based on the value of another variable, during the same initialization op.", "Thanks @vrv . \r\n\r\nAs far as I can tell, I'm not initializing any variable based on another one and I only have a single call to an init opp. \r\nYour suggestion is at the limits of my understanding of TF.  Could you point me at the right docs to understand how to go about \"try to find which GPU variable is being read on CPU, \" ? \r\nThanks \r\n", "Quick update: \r\nI removed the following lines and it worked! \r\n```\r\n        variable_averages = tf.train.ExponentialMovingAverage( args.decay_factor, global_step)\r\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\r\n\r\n```\r\nI'm not sure why, but I suspect it has somehting to do with the variables that ExponentialMovingAverage creates, per the docs:\r\n\r\n> Creates a new ExponentialMovingAverage object.\r\n> \r\n>     The `apply()` method has to be called to create shadow variables and add\r\n>     ops to maintain moving averages.\r\n> \r\n", "Closing this as it looks like you resolved it yourself. Thanks @talolard for reporting back."]}, {"number": 6304, "title": "Inference with TensorFlow does not work properly", "body": "I have a strange problem with my neural network: while CV accuracy is good, inference accuracy is at the random guess level.\r\n\r\nInference is done in a standard ImageNet style way: five 224x224 squares, corners of the image plus center crop.\r\n\r\nThe CV and inference code goes together as follows:\r\n\r\n`    inference_images = tf.placeholder(tf.float32, shape=(5, image_size_rows, image_size_cols, num_channels), name='input_images')\r\n\r\n    ...    \r\n\r\n    # Calculate CV loss and accuracy\r\n\r\n    cv_images_batch, cv_labels_batch, cv_filenames = cv_runner.get_inputs()\r\n\r\n    cv_predictions, cv_logits = network(cv_images_batch, num_labels)\r\n    cv_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(cv_logits, cv_labels_batch))\r\n\r\n    cv_correct_prediction = tf.equal(tf.argmax(cv_predictions, 1), tf.argmax(cv_labels_batch, 1))\r\n    cv_accuracy = tf.reduce_mean(tf.cast(cv_correct_prediction, 'float'))\r\n\r\n    # Calculate inference\r\n\r\n    inference_predictions, _ = network(inference_images, num_labels)\r\n    predicted_classes = tf.argmax(inference_predictions, 1, name='predicted_classes')\r\n    predicted_class = tf.argmax(tf.reduce_sum(inference_predictions, 0), 0, name='predicted_class')`\r\n\r\nTraining and CV accuracy quickly grow to >50% but inference accuracy remains at the random guess level of 10%.\r\n\r\nI used for inference the same images I use for cross-validation and training. In both cases the inference accuracy remains low while I would expect it to be at the CV and train accuracy respectively.\r\n\r\nPreprocessing code is the same for training, CV and inference. Restoring images from CV and inference matrices works fine for both types. So, the problem apparently is not in different preprocessing for train/CV and inference.\r\n\r\nTo isolate this effect, I have extracted train images out of a queue used for training and used them to make inference. Here are the results:\r\n\r\n`Step 35140: train loss: 0.170477, cv loss: 2.183839, train accuracy: 0.955357, cv accuracy: 0.535714, inference accuracy: 0.400000\r\n\r\nStep 35160: train loss: 0.233728, cv loss: 2.114637, train accuracy: 0.937500, cv accuracy: 0.517857, inference accuracy: 0.200000\r\n\r\nStep 35180: train loss: 0.362239, cv loss: 1.962119, train accuracy: 0.906250, cv accuracy: 0.544643, inference accuracy: 0.400000\r\n\r\nStep 35200: train loss: 0.190649, cv loss: 2.122064, train accuracy: 0.946429, cv accuracy: 0.540179, inference accuracy: 0.800000\r\n\r\nStep 35220: train loss: 0.161332, cv loss: 2.006443, train accuracy: 0.968750, cv accuracy: 0.513393, inference accuracy: 1.000000\r\n\r\nStep 35240: train loss: 0.185687, cv loss: 1.981431, train accuracy: 0.950893, cv accuracy: 0.535714, inference accuracy: 0.600000\r\n\r\nStep 35260: train loss: 0.123562, cv loss: 2.030116, train accuracy: 0.977679, cv accuracy: 0.562500, inference accuracy: 0.800000\r\n\r\nStep 35280: train loss: 0.216172, cv loss: 1.883790, train accuracy: 0.941964, cv accuracy: 0.549107, inference accuracy: 0.600000\r\n\r\nStep 35300: train loss: 0.256623, cv loss: 2.012454, train accuracy: 0.897321, cv accuracy: 0.549107, inference accuracy: 0.200000`\r\n\r\nEach inference batch is 5 images. As you can see, mean inference accuracy is way below train accuracy of 90%+.\r\n\r\nI have tried to add tf.get_variable_scope().reuse_variables() before the inference with no effect.\r\n\r\nStackoverflow community did not help:\r\nhttp://stackoverflow.com/questions/40820250/inference-with-tensorflow-does-not-work\r\n\r\nWhat am I doing wrong with inference here?\r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\r\nlrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\r\n-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\r\n-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 61453024 Nov 14 14:08 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 61453024 Nov 14 14:08 /usr/local/cuda/lib64/libcudnn.so.4\r\n-rwxr-xr-x 1 root root 61453024 Nov 14 14:08 /usr/local/cuda/lib64/libcudnn.so.4.0.7\r\n-rw-r--r-- 1 root root 62025862 Nov 14 14:08 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\nThe output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so.4 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so.7.5 locally\r\n0.11.0rc2", "comments": ["Please try again on stackoverflow. The focus of this list is on bugs/issues. You will get more support for this type of question on stackoverflow."]}, {"number": 6303, "title": "TensorFlowEstimator can not be found", "body": "when i am running sequence predicting program in windows10 ,it stucked in an error :AttributeError: module 'tensorflow.contrib.learn' has no attribute 'TensorFlowEstimator'. the tensorflow edition is 0.12rc  and python3.5 .how can i fix this problem? ", "comments": ["We removed the TensorFlowEstimator class in favor of Estimator. The interface is somewhat different, but it should be an easy conversion."]}, {"number": 6302, "title": "Update os_setup.md", "body": "Not yet exist rc1 for windows.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Hi @stozpark, thanks for the PR! Sorry that the rc1 windows links weren't working before, but they should work now. Could you give them another try? Closing this PR."]}, {"number": 6301, "title": "add custom loss ", "body": "I have noticed that tensorflow can add custom op, however, I want to add an custom loss function. To my\r\npoint of view, an custom op and an custom loss are different. Dose tensorflow support adding an custom loss? ", "comments": ["I think you could do it by subclassing `tf.train.Optimizer` class. ref: https://github.com/tensorflow/tensorflow/blob/72a3caa1cc223fe10ac5287083ef20a911f87400/tensorflow/python/training/optimizer.py#L364", "@bc-lee Thank you for your suggestion. Do you mean modify the source code of tensorflow?", "This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag."]}, {"number": 6300, "title": "Add orthogonal initialization", "body": "[Lasagne has orthogonal initialization](https://github.com/Lasagne/Lasagne/blob/master/lasagne/init.py#L327-L367) like in [this reference](https://arxiv.org/abs/1312.6120) and it's pretty useful (and quite intuitive: make sure random weights are orthogonal so they should tend to learn different modes in the input and not converge to being redundant).\r\n\r\n# TODO\r\n - [ ] Add unit test\r\n - [ ] Verify code style\r\n - [ ] Verify docstring style\r\n - [ ] Don't import NumPy if possible (is there a SVD in TensorFlow somewhere?)", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I think this is a duplicate of #5164.", "Very nice! Moving over to https://github.com/tensorflow/tensorflow/pull/5164."]}, {"number": 6299, "title": "Second order derivaties for Pooling operations", "body": "Fixes #6143\r\nFixes #6294 \r\n\r\nThis is still not a complete. I wanted to know whether this is the correct way? \r\nI ran the following code and it worked without any error.\r\nIf it is correct, then I will add the same for `avg_pool` too.\r\n```\r\nIn [1]: import tensorflow as tf\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so.4 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so.7.5 locally\r\n\r\nIn [2]: x = tf.placeholder(tf.float32, shape=[1, 4, 4, 1])\r\n\r\nIn [3]: y = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\r\n\r\nIn [4]: dy = tf.gradients(y, x)\r\n\r\nIn [5]: ddy = tf.gradients(dy, x)\r\n```", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please!", "I have added tests and AvgPoolGrad op. MaxPoolGrad tests are passing but AvgPoolGrad tests are failing. I have added a comment above which might explain why it is failing. @vrv what do you think?", "The operation is not implemented correctly. If tests are passing, it is only because linear pooling operations are involved which results in all zeros and ones. We need to write a custom gpu kernel for max pool grad grad. Similarly, custom cpu c++ function needs to be implemented. See theano gpu implementation for reference - https://github.com/Theano/Theano/pull/5159. For average pool grad grad, no custom kernel is needed and we can just reuse average pooling.\r\n\r\nAs I am familiar with theano, I can pick up this pull request fairly easily and add support for max pool grad grad to tensorflow if @AnishShah doesn't mind.", "@aam-at Okay. Sure. Please tag me in the PR too if it's okay. I would like to learn how you did it.\r\nThanks!", "@aam-at Also, I would like to know what is wrong?\r\nFor the max pool operation, the gradient is passed to the one who has the maximum value inside the kernel window. So, the second order derivative should be zero. This is **kind of** like ReLUs, isn't it? Can you please explain? It would help me understand better. :)", "Can one of the admins verify this patch?", "Gradient of max pooling does not always output ones. It will pass top gradients `gz` which correspond to the location of maximum values in the input `x`. If you have only linear operations, then `gz` is ones and `gx` is also ones then. But this is not true in general.\r\n\r\nGradient of the gradient needs to pass gradient of the top input gradient `ggx` which correspond to the locations of maximum values in the input `x`.\r\n\r\nFor diagram of gradient of max pooling operation, you can look up Zeiler's paper on deconvolutional networks (they have diagram for unpooling which is just basically backward pass of max pooling). Gradient of the gradient is a bit more confusing to show using diagram.", "@aam-at Thanks for the explanation. Looking forward to your PR. Please mark me too. :)"]}]