[{"number": 20478, "title": "Support dense tensors in sequence_numeric_column", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 18.04\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\nTensorflow 1.8.0\r\n- **Python version**: \r\n3.8.3\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0/cuDNN 7.0\r\n- **GPU model and memory**:\r\n1080TI 11GB\r\n- **Exact command to reproduce**:\r\nN/A\r\n\r\n### Describe the problem\r\n\r\nI am training a time series using an RNN and my input data has a fixed sequence length per batch.\r\n\r\nI am using [`parse_single_example`](https://www.tensorflow.org/api_docs/python/tf/parse_single_example) with [`regressor_parse_example_spec`](https://www.tensorflow.org/api_docs/python/tf/estimator/regressor_parse_example_spec) and [`sequence_numeric_column`](https://www.tensorflow.org/api_docs/python/tf/contrib/feature_column/sequence_numeric_column) which parses the features as sparse tensors because [`_parse_example_spec` returns `VarLenFeature`](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/feature_column/python/feature_column/sequence_feature_column.py#L419).\r\n\r\nI want to parse our data as dense tensors because the underlying data is dense, and using sparse tensors in our graph requires extra unnecessary complexity for our calculations. I want to continue to use `feature_columns` to describe our data and `sequence_input_layer` but I propose adding a new feature column type `sequence_fixed_len_numeric_column` that parses as a dense tensor.\r\n\r\n[Here is the implementation](https://gist.github.com/jperl/245c414793a5271da72183bada93c55c#file-sequence_fixed_len_numeric_column-py-L35) and I would be happy to submit a PR + add tests. But first I wanted to confirm this a idiomatic approach.\r\n\r\nAlternatively I could add a parameter to `sequence_numeric_column`, `is_fixed` to accomplish the same things.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce", "@tensorflowbutler resolved", "does adding `dense_shape` as an argument to `sequence_numeric_column` make sense?", "@ispirmustafa I like that. \r\n\r\nRight now `shape` has a default value of `(1, )`. So how should the behavior work? If you define both, it will prefer dense_shape?\r\n\r\nI'll put a PR together w/ tests this week.", "Having another thought. we can do following without a parameter:\r\nin `_get_sequence_dense_tensor`:\r\n```\r\nif input is sparse tensor:\r\n  call sparse to dense\r\n```\r\nThis will handle the dense input case but not the parsing spec. if user uses the parsing spec of feature column then they should not be aware of whether this is sparse or not. It should work for them.\r\ndoes it make sense?", "Ok I agree parsing spec should be handled separately. I put together a PR here https://github.com/tensorflow/tensorflow/pull/20860.\r\n\r\nI will add unit tests but first want to make sure that is aligned with how you are thinking about it?\r\n\r\nAlso should I move `_sequence_length_from_dense_tensor` into `tensorflow/python/feature_column/feature_column.py` since that is where [`_sequence_length_from_sparse_tensor`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/feature_column/feature_column.py#L3375-L3387) is located? Or keep it in this file?", "Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@jperl could you please update the thread?\r\nthanks", "Closing. We switched away from feature columns.", "Is there any reason that sequence_numeric_column only supports value as SparseTensor? I think sequence_numeric_columns has to accept dense tensor by default."]}, {"number": 20477, "title": "Fix matrix_inverse_tril_test handling the inverse of 0", "body": "The inverse of 0 is undefined, so when computing the inverse of a lower\r\ntriangular numpy array, we need to ensure the zeros in the upper\r\nparts of the triangle remain as zero. After doing the inverse\r\nof an array, iterator over the array and set the values above the\r\nmain diagonal to zero.\r\nSee: https://github.com/numpy/numpy/issues/11445\r\nFixes: #20013", "comments": ["I'll make the same change in https://github.com/tensorflow/probability in a PR today. I wasn't aware of the move. I'll ping you in the PR when it is ready. \r\n\r\nThank you for the review!", "Thanks William!", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20476, "title": "Improve the shape function of Bincount", "body": "This fix tries to improve the shape function of Bincount.\r\n\r\nIn the existing implementation there was not a lot of restriction in shape function of Bincount, and the output shape was unknown. But it is actually possible to get a better shape output if `size` input is known.\r\n\r\nThis fix adds enhancement to the shape function of Bincount.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @josh11b for the review. The PR has been updated with additional test cases added. Please take a look.", "Nagging Assignee @martinwicke: It has been 19 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @martinwicke: It has been 34 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@josh11b can you check whether your comments have been addressed?"]}, {"number": 20475, "title": "Add a dictionary to a collection", "body": "### System information\r\nOS Platform and Distribution Mac OS X latest\r\nTensorFlow installed from pip\r\nTensorFlow version 1.8\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce see code below\r\n\r\n### Describe the problem\r\nDear All,\r\n\r\nI am looking for a way to store and retrieve a dictionary of tensors. If I try to pass a dict to `ft.add_to_collection` I get a warning and when I load the graph again the collection does not exist. This is a simple example:\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.reset_default_graph()\r\n\r\nfoo  = tf.Variable([1,2,3], dtype=tf.float32)\r\nboo  = tf.Variable([1,2,3], dtype=tf.float32)\r\n\r\ntf.add_to_collection('test', {'foo' : foo, 'boo': [boo] })\r\ntf.add_to_collection('test2', foo)\r\n\r\nwith tf.Session() as sess:\r\n    saver = tf.train.Saver()\r\n    sess.run(tf.global_variables_initializer())\r\n    saver.save(sess,'save/model.ckpt')\r\n\r\ntf.reset_default_graph() # reset so we are sure to load a new graph\r\n\r\nwith tf.Session() as sess:\r\n    saver = tf.train.import_meta_graph(\"save/model.ckpt.meta\")\r\n    saver.restore(sess,'save/model.ckpt')\r\n    graph = tf.get_default_graph()\r\n    print(graph.get_collection('test'))\r\n    print(graph.get_collection('test2'))\r\n```\r\n\r\nOutput:\r\n\r\n```\r\nWARNING:tensorflow:Error encountered when serializing test.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef.\r\n'dict' object has no attribute 'name'\r\nINFO:tensorflow:Restoring parameters from save/model.ckpt\r\n[]\r\n[<tf.Tensor 'Variable:0' shape=(3,) dtype=float32_ref>]\r\n```\r\n\r\nAs you can see, after loading the graph, the `test` collection is empty.\r\n\r\nIs there any way to properly store/retrieve a dictionary that maps tensors? \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Done it!", "Any news?", "@FrancescoSaverioZuppichini \r\nCould you please try on latest stable version of tf and let us know if this is still an issue.Thanks!", "I switched to PyTorch years ago, you should do the same :)", "@FrancescoSaverioZuppichini \r\nPlease move this to closed status, if it is not a issue for you anymore.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20475\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20475\">No</a>\n"]}, {"number": 20474, "title": "Exclude importing tensorflow.contrib.cloud on Windows", "body": "This is a follow up to fix import error in tensorflow/contrib/__init__.py on Windows, which is caused by https://github.com/tensorflow/tensorflow/commit/654eb3dd779107eb919af1093e2a72f0ab6ba922\r\n\r\n@gunan @case540", "comments": ["The `Ubuntu Sanity` test failed in `do_bazel_nobuild`. Which is caused by\r\n```\r\nERROR: Config value hdfs is not defined in any .rc file\r\n136\r\n137\r\nFAIL: bazel build --nobuild --config=hdfs --config=gcp -- //tensorflow/... -//tensorflow/contrib/lite/java/demo/app/... -//tensorflow/contrib/lite/examples/android/... -//tensorflow/contrib/lite/schema/...\r\n138\r\n  This is due to invalid BUILD files. See lines above for details.\r\n```\r\n\r\nIt also impact other PR's (e.g., #19534) Sanity checks as well.\r\n\r\nNot very familiar with how CI setup so I haven't been able to find a fix yet.", "Sanity failure is a known issue that will be fixed by the latest push soon.", "We are also updating the google_cloud_cpp dependency. I sent a fix for their windows build and will see if the build is fixed internally later today.\r\nLet's wait on that to see if the build is fixed with cloud component.", "Actually it looks like we merged the build dependency change already, so merging this PR."]}, {"number": 20473, "title": "I want to build tensorflow-gpu 1.4.0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04.4 LTS\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:tensorflow-gpu 1.4.0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**:0.5.4\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**:8.0/6.0.21\r\n- **GPU model and memory**:GeForce GTX 980 4GB\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nI am trying to install TensorFlow from Sources. I did it with reference to https://www.tensorflow.org/install/install_sources.But I accidentally built a CPU version.\r\n\r\nI want to build tensorflow-gpu 1.4.0 from source. But I cannot find the GPU version of tensorflow.\r\n\r\nAny suggestions would help. Thanks.\r\n", "comments": ["Hi @nagata-tensor -- please make sure you have the [NIVIDIA prerequisites](https://www.tensorflow.org/install/install_linux#NVIDIARequirements) installed on your system before installing from source. Then you can proceed with the [install from source instructions](https://www.tensorflow.org/install/install_sources).", "Hi @karmel I have the NVIDIA prerequisites installed on my system. I could proceed with the install from source instructions. "]}, {"number": 20472, "title": "R1.9", "body": "update", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 20471, "title": "ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04.4 LTS\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.4.0\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**:0.5.4\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**:8.0/6.0.21\r\n- **GPU model and memory**:GeForce GTX 980 4GB\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI am trying to install  TensorFlow from Sources. I did it with reference to https://www.tensorflow.org/install/install_sources.\r\n\r\nTo build a pip package for TensorFlow with GPU support, I invoke the following command:\r\n\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nBut getting following error.(My log is shown at the bottom)\r\n\r\nAny suggestions would help. Thanks.\r\n\r\n### Source code / logs\r\nroot@nakadake:/mnt/fs4/nagata/tensorflow-1.4.0# ./configure \r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nYou have bazel 0.5.4 installed.\r\nTraceback (most recent call last):\r\n  File \"configure.py\", line 1039, in <module>\r\n    main()\r\n  File \"configure.py\", line 973, in main\r\n    reset_tf_configure_bazelrc()\r\n  File \"configure.py\", line 249, in reset_tf_configure_bazelrc\r\n    open(_TF_BAZELRC, 'w').close()\r\nIOError: [Errno 13] Permission denied: '/mnt/fs4/nagata/tensorflow-1.4.0/.tf_configure.bazelrc'\r\n", "comments": ["@nagata-tensor have you resolved the problem?"]}, {"number": 20470, "title": "Error while converting .pb model into tensorflowlite model", "body": "Command:\r\n toco --input_file=/home/p/Downloads/tensorflow/output_graph.pb --output_file=/home/p/Downloads/tensorflow/optimized.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_arrays=\"dense_1_input\" --output_arrays=\"act_4/Softmax\" --inference_type=FLOAT --input_data_type=FLOAT --input_shapes=1,40\r\n\r\n- output_graph.pb is a frozen model.\r\n\r\nError:\r\n2018-06-29 16:02:00.398756: F tensorflow/contrib/lite/toco/model_cmdline_flags.cc:260] Check failed: uses_single_input_flags \r\nAborted (core dumped)\r\n\r\nAny help will be highly appreciated.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@Jigar27: can you provide the graph that you are using.", "Closing due to lack of activity, @Jigar27 please reopen if you are still having issues."]}, {"number": 20469, "title": "total_float_ops is 0 by tf.profiler.profile", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: CUDA 9 / cuDNN 7\r\n- **GPU model and memory**: GeForce GTX 1080Ti\r\n- **Exact command to reproduce**:\r\n```\r\nimport cv2\r\nimport time\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import graph_util\r\n\r\nModelFile = \"OX_Predict_frozen.pb\"\r\n\r\ndef load_pb(pb):\r\n    with tf.gfile.GFile(pb, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(graph_def, name='')\r\n        return graph\r\n        \r\nimport math\r\nM = math.pow( 10, 6 )\r\nprint(M)\r\n\r\ndef log_FLOP():\r\n    # ***** (3) Load frozen graph *****\r\n    g2 = load_pb(ModelFile)\r\n    with g2.as_default():\r\n        flops = tf.profiler.profile(g2, options = tf.profiler.ProfileOptionBuilder.float_operation())\r\n        print('FLOP after freezing(M): ', flops.total_float_ops/ M)\r\n\r\ndef main():\r\n    log_FLOP()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n\r\n### Describe the problem\r\nI'm trying to log the number of multiply-add operations (MAC) in my network by \"tf.profiler.profile\".\r\n\r\nhere is the [model](https://drive.google.com/file/d/12-7TI7EGLuN8JgcafGPJKeugp263gVYD/view?usp=sharing)(.pb)\r\n\r\nthe model work perfect when predict, but it always return 0 flops with \"tf.profiler.profile\"\r\n\r\nany suggestion ?\r\n\r\n### Source code / logs\r\nthe sample code to get .pb is following:\r\nhttps://github.com/ChiFang/TensorFlow_XO_example\r\n\r\nit takes only few sec~~~~\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "I am also getting the same error , any update"]}, {"number": 20468, "title": "Failing to load h5 model using tf-gpu?", "body": "```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\client\\session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\client\\session.py\", line 1305, in _run_fn\r\n    self._extend_graph()\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\client\\session.py\", line 1340, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re\r\ngistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],\r\nRegistered kernels:\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n\r\n         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_FLOAT, direction=\"un\r\nidirectional\", dropout=0, input_mode=\"linear_input\", is_training=true, rnn_mode=\r\n\"gru\", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Expa\r\nndDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"Z:\\trader_connect.py\", line 157, in <module>\r\n    tick()\r\n  File \"Z:\\trader_connect.py\", line 74, in tick\r\n    model1 = keras.models.load_model('Z:\\\\Productionmodel.h5')\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\en\r\ngine\\saving.py\", line 264, in load_model\r\n    load_weights_from_hdf5_group(f['model_weights'], model.layers)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\en\r\ngine\\saving.py\", line 929, in load_weights_from_hdf5_group\r\n    K.batch_set_value(weight_value_tuples)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\ba\r\nckend\\tensorflow_backend.py\", line 2435, in batch_set_value\r\n    get_session().run(assign_ops, feed_dict=feed_dict)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\ba\r\nckend\\tensorflow_backend.py\", line 196, in get_session\r\n    [tf.is_variable_initialized(v) for v in candidate_vars])\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\client\\session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\client\\session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\client\\session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\client\\session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re\r\ngistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],\r\nRegistered kernels:\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n\r\n         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_FLOAT, direction=\"un\r\nidirectional\", dropout=0, input_mode=\"linear_input\", is_training=true, rnn_mode=\r\n\"gru\", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Expa\r\nndDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]\r\n\r\nCaused by op 'bidirectional_1/CudnnRNN_1', defined at:\r\n  File \"Z:\\trader_connect.py\", line 157, in <module>\r\n    tick()\r\n  File \"Z:\\trader_connect.py\", line 74, in tick\r\n    model1 = keras.models.load_model('Z:\\\\Productionmodel.h5')\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\en\r\ngine\\saving.py\", line 261, in load_model\r\n    model = model_from_config(model_config, custom_objects=custom_objects)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\en\r\ngine\\saving.py\", line 335, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\la\r\nyers\\__init__.py\", line 55, in deserialize\r\n    printable_module_name='layer')\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\ut\r\nils\\generic_utils.py\", line 145, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\en\r\ngine\\sequential.py\", line 293, in from_config\r\n    model.add(layer)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\en\r\ngine\\sequential.py\", line 166, in add\r\n    layer(x)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\la\r\nyers\\wrappers.py\", line 426, in __call__\r\n    return super(Bidirectional, self).__call__(inputs, **kwargs)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\en\r\ngine\\base_layer.py\", line 460, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\la\r\nyers\\wrappers.py\", line 505, in call\r\n    y_rev = self.backward_layer.call(inputs, **kwargs)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\la\r\nyers\\cudnn_recurrent.py\", line 90, in call\r\n    output, states = self._process_batch(inputs, initial_state)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\la\r\nyers\\cudnn_recurrent.py\", line 297, in _process_batch\r\n    is_training=True)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\contrib\\cudnn_rnn\\python\\ops\\cudnn_rnn_ops.py\", line 1623, in __call__\r\n    seed=self._seed)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\contrib\\cudnn_rnn\\python\\ops\\cudnn_rnn_ops.py\", line 1012, in _cudnn_rnn_no_i\r\nnput_c\r\n    direction, dropout, seed, name)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\contrib\\cudnn_rnn\\python\\ops\\cudnn_rnn_ops.py\", line 926, in _cudnn_rnn\r\n    name=name)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\ops\\gen_cudnn_rnn_ops.py\", line 143, in cudnn_rnn\r\n    is_training=is_training, name=name)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\framework\\ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\framework\\ops.py\", line 1718, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-\r\naccess\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to su\r\npport Op 'CudnnRNN' with these attrs.  Registered devices: [CPU], Registered ker\r\nnels:\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n\r\n         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_FLOAT, direction=\"un\r\nidirectional\", dropout=0, input_mode=\"linear_input\", is_training=true, rnn_mode=\r\n\"gru\", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Expa\r\nndDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]\r\n\r\n\r\n(tensorflow-gpu) C:\\Users\\xion>python Z:\\trader_connect.py --csv Y:\\EURUSD,5.c\r\nsv\r\nUsing TensorFlow backend.\r\n2018-07-01 20:58:02.203507: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_ru\r\nntime\\gpu\\gpu_device.cc:1356] Found device 0 with properties:\r\nname: GeForce GT 530 major: 2 minor: 1 memoryClockRate(GHz): 1.399\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 2.00GiB freeMemory: 1.87GiB\r\n2018-07-01 20:58:02.204507: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_ru\r\nntime\\gpu\\gpu_device.cc:1406] Ignoring visible gpu device (device: 0, name: GeFo\r\nrce GT 530, pci bus id: 0000:01:00.0, compute capability: 2.1) with Cuda compute\r\n capability 2.1. The minimum required Cuda capability is 3.0.\r\n2018-07-01 20:58:02.204507: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_ru\r\nntime\\gpu\\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1\r\nedge matrix:\r\n2018-07-01 20:58:02.204507: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_ru\r\nntime\\gpu\\gpu_device.cc:929]      0\r\n2018-07-01 20:58:02.204507: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_ru\r\nntime\\gpu\\gpu_device.cc:942] 0:   N\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\client\\session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\client\\session.py\", line 1305, in _run_fn\r\n    self._extend_graph()\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\client\\session.py\", line 1340, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re\r\ngistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],\r\nRegistered kernels:\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n\r\n         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_DOUBLE, direction=\"u\r\nnidirectional\", dropout=0, input_mode=\"linear_input\", is_training=true, rnn_mode\r\n=\"gru\", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Exp\r\nandDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"Z:\\trader_connect.py\", line 157, in <module>\r\n    tick()\r\n  File \"Z:\\trader_connect.py\", line 74, in tick\r\n    model1 = keras.models.load_model('Z:\\\\Productionmodel.h5')\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\en\r\ngine\\saving.py\", line 264, in load_model\r\n    load_weights_from_hdf5_group(f['model_weights'], model.layers)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\en\r\ngine\\saving.py\", line 929, in load_weights_from_hdf5_group\r\n    K.batch_set_value(weight_value_tuples)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\ba\r\nckend\\tensorflow_backend.py\", line 2435, in batch_set_value\r\n    get_session().run(assign_ops, feed_dict=feed_dict)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\ba\r\nckend\\tensorflow_backend.py\", line 196, in get_session\r\n    [tf.is_variable_initialized(v) for v in candidate_vars])\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\client\\session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\client\\session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\client\\session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\client\\session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re\r\ngistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],\r\nRegistered kernels:\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n\r\n         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_DOUBLE, direction=\"u\r\nnidirectional\", dropout=0, input_mode=\"linear_input\", is_training=true, rnn_mode\r\n=\"gru\", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Exp\r\nandDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]\r\n\r\nCaused by op 'bidirectional_1/CudnnRNN_1', defined at:\r\n  File \"Z:\\trader_connect.py\", line 157, in <module>\r\n    tick()\r\n  File \"Z:\\trader_connect.py\", line 74, in tick\r\n    model1 = keras.models.load_model('Z:\\\\Productionmodel.h5')\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\en\r\ngine\\saving.py\", line 261, in load_model\r\n    model = model_from_config(model_config, custom_objects=custom_objects)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\en\r\ngine\\saving.py\", line 335, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\la\r\nyers\\__init__.py\", line 55, in deserialize\r\n    printable_module_name='layer')\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\ut\r\nils\\generic_utils.py\", line 145, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\en\r\ngine\\sequential.py\", line 293, in from_config\r\n    model.add(layer)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\en\r\ngine\\sequential.py\", line 166, in add\r\n    layer(x)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\la\r\nyers\\wrappers.py\", line 426, in __call__\r\n    return super(Bidirectional, self).__call__(inputs, **kwargs)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\en\r\ngine\\base_layer.py\", line 460, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\la\r\nyers\\wrappers.py\", line 505, in call\r\n    y_rev = self.backward_layer.call(inputs, **kwargs)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\la\r\nyers\\cudnn_recurrent.py\", line 90, in call\r\n    output, states = self._process_batch(inputs, initial_state)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\la\r\nyers\\cudnn_recurrent.py\", line 297, in _process_batch\r\n    is_training=True)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\contrib\\cudnn_rnn\\python\\ops\\cudnn_rnn_ops.py\", line 1623, in __call__\r\n    seed=self._seed)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\contrib\\cudnn_rnn\\python\\ops\\cudnn_rnn_ops.py\", line 1012, in _cudnn_rnn_no_i\r\nnput_c\r\n    direction, dropout, seed, name)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\contrib\\cudnn_rnn\\python\\ops\\cudnn_rnn_ops.py\", line 926, in _cudnn_rnn\r\n    name=name)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\ops\\gen_cudnn_rnn_ops.py\", line 143, in cudnn_rnn\r\n    is_training=is_training, name=name)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\framework\\ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\xion\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorfl\r\now\\python\\framework\\ops.py\", line 1718, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-\r\naccess\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to su\r\npport Op 'CudnnRNN' with these attrs.  Registered devices: [CPU], Registered ker\r\nnels:\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n\r\n         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_DOUBLE, direction=\"u\r\nnidirectional\", dropout=0, input_mode=\"linear_input\", is_training=true, rnn_mode\r\n=\"gru\", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Exp\r\nandDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]\r\n\r\n```\r\n\r\n\r\n\r\n\r\nI've tried--fresh reinstall, change float\r\n\r\nAny fixes? No idea why Im getting this. P.S: I trained the model on a titan v and I am trying to now open it on a computer with a GeForce GT 530 Gpu.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi,\r\n\r\nHave I written custom code: yes\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from: pip install tensorflow-gpu\r\nTensorFlow version: 1.8.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: Cuda:9.0/cuDNN 7.1.4\r\nGPU model and memory: GeForce GT 530 Gpu 1gb memory\r\nExact command to reproduce: tried to open up a model with the cuDNNGRU layer in it", "I suspect there is some installation/configuration issue on your machine. From the error messages it appears that the TensorFlow libraries aren't able to detect a GPU device (`Registered devices: [CPU]`, I would have expected it to be `[CPU, GPU]`).\r\n\r\nYou can validate whether TensorFlow can detect a GPU using the following:\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.test.is_gpu_available())\r\n```\r\n\r\nIf that prints `False`, then there is some installation/configuration issue (perhaps there was a previous `pip install tensorflow` and you need a `pip uninstall tensorflow` first).\r\n\r\nLet us know.", ">>> import tensorflow as tf\r\n>>> print(tf.test.is_gpu_available())\r\n2018-07-02 17:42:04.233407: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_ru\r\nntime\\gpu\\gpu_device.cc:1356] Found device 0 with properties:\r\nname: GeForce GT 530 major: 2 minor: 1 memoryClockRate(GHz): 1.399\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 2.00GiB freeMemory: 1.90GiB\r\n2018-07-02 17:42:04.235408: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_ru\r\nntime\\gpu\\gpu_device.cc:1406] Ignoring visible gpu device (device: 0, name: GeFo\r\nrce GT 530, pci bus id: 0000:01:00.0, compute capability: 2.1) with Cuda compute\r\n capability 2.1. The minimum required Cuda capability is 3.0.\r\n2018-07-02 17:42:04.236408: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_ru\r\nntime\\gpu\\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1\r\nedge matrix:\r\n2018-07-02 17:42:04.236408: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_ru\r\nntime\\gpu\\gpu_device.cc:929]      0\r\n2018-07-02 17:42:04.237408: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_ru\r\nntime\\gpu\\gpu_device.cc:942] 0:   N\r\nFalse\r\n>>>\r\n\r\n\r\nI dont think its an issue with installing tensorflow again with pip install tensorflow-gpu because I already did this. In fact, I even created a new conda environment and did a fresh install but I got the same issue", "Nagging Assignee @asimshankar: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@X-I-O-N : Could you confirm that (as mentioned in the previous comment)\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.test.is_gpu_available())\r\n```\r\n\r\nprints `True`? I suspect it is printing false and looking at the logs in your last comment I see:\r\n\r\n```\r\n2018-07-02 17:42:04.235408: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_ru\r\nntime\\gpu\\gpu_device.cc:1406] Ignoring visible gpu device (device: 0, name: GeFo\r\nrce GT 530, pci bus id: 0000:01:00.0, compute capability: 2.1) with Cuda compute\r\ncapability 2.1. The minimum required Cuda capability is 3.0.\r\n```\r\n\r\nWhich suggests that your TensorFlow installation cannot use the GPU you have and thus ends up with a single CPU device. The reason it can't use the GPU is because the binary requires the GPU to be compatible with CUDA compute capability 3.0.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "No. GPU simply did not support CUDA--too old."]}, {"number": 20467, "title": "maybe a bug? different gradients of x**2 and x*x?", "body": "```\r\nimport tensorflow as tf\r\n\r\nsess = tf.Session()\r\n\r\nlr = 0.1\r\n\r\nx = tf.Variable(1.)\r\ny = 0.5 * x * x\r\n# y = 0.5 * x**2\r\n\r\nP = tf.global_variables()\r\n\r\ngrads = tf.gradients(y, P)\r\n\r\nops = []\r\nfor p,g in zip(P, grads):\r\n    ops.append(tf.assign(p, p - lr * g))\r\n\r\nwith tf.control_dependencies(ops):\r\n    ops2 = []\r\n    grads = tf.gradients(y, P)\r\n    for p,g in zip(P, grads):\r\n        ops2.append(tf.assign(p, p - lr * g))\r\n\r\n\r\nsess.run(tf.global_variables_initializer())\r\n\r\nsess.run(ops2)\r\n```\r\n\r\nif `y = 0.5 * x * x` it return `[0.80499995] ` but `y = 0.5 * x**2` return `[0.81]`, why?\r\n\r\nhow can I do if I want to run two steps of GD in one sess.run ?", "comments": ["there is a similar code.\r\n\r\n```\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\n\r\nx = tf.Variable(1.)\r\ny = 0.5 * x * x\r\n\r\nwith tf.control_dependencies([tf.assign(x, x - 0.1)]):\r\n    x_ = tf.identity(x)\r\n    y_ = tf.identity(y)\r\n\r\nsess.run(tf.global_variables_initializer())\r\nsess.run([x_, y_])\r\n```\r\nwhy the final `y_` is 0.5 but `x_` is 0.9 ?  if x is 0.9, should y be `0.5 * 0.9 * 0.9` ?\r\n", "In your case, I think the execution order of `y` and `assign` operator is non-determined.\r\n\r\n```python\r\nwith tf.control_dependencies([tf.assign(x, x - 0.1)]):\r\n    y = 0.5 * x * x\r\n    x_ = tf.identity(x)\r\n    y_ = tf.identity(y)\r\n\r\n# x_, y_ = [0.9, 0.40499997]\r\n```", "@facaiy Yet, I can guess the reason. But how can I promise the right order without re-defining the tensor `y` ?", "somebody has the same confusion about it. like \r\nhttps://stackoverflow.com/questions/44352654/tensorflow-gradients-across-post-processing-assign-ops", "I don't quite understand your confusion.  This is the expected behavior as @facaiy commented.\r\n", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 20466, "title": "InternalError (see above for traceback): CUB segmented reduce errorinvalid device function", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: running training step from [here](https://github.com/lengstrom/fast-style-transfer)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64 bit\r\n- **TensorFlow installed from (source or binary)**: installed using conda\r\n- **TensorFlow version (use command below)**: b'unknown' 1.8.0\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**:  CUDA 9.0 cuDNN 7.1.4\r\n- **GPU model and memory**: name: GeForce GTX 650 Ti major: 3 minor: 0 memoryClockRate(GHz): 0.928\r\n- **Exact command to reproduce**: python style.py --style examples\\poem.jpg --checkpoint-dir checkpoint\r\n\r\n\r\n### Describe the problem\r\nRunning a CNN training on fast style transfer, and using tensorflow-gpu. training crushed at moments/mean, is this a bug in tensorflow?\r\n\r\n### Source code / logs\r\n\r\n> Traceback (most recent call last):\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: CUB segmented reduce errorinvalid device function\r\n         [[Node: moments/mean = Mean[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Conv2D_16, moments/mean-1-LayoutOptimizer)]]\r\n\r\n", "comments": ["The linked code runs on TensorFlow 0.11.0, and you are using 1.8.0. I would not expect consistency over a major version change like that. I would suggest finding a more recent version of the CNN you want to run, or checking out an older version of TensorFlow. Thanks."]}, {"number": 20465, "title": "Tensorflow Import Error", "body": "I have just downloaded Anaconda 1.8.7. , using Py 3.6. Donwloaded Tensorflow pkgs. But I have importing issues. Using windows. Any help will be appraciated.Thank you. Here is the error message on Jupyter Notebook: \r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-64156d691fe5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 # pylint: disable=wildcard-import\r\n     26 from tensorflow.tools.api.generator.api import *  # pylint: disable=redefined-builtin\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "just couldn't import tensorflow, just the very beginning..\r\nimport tensorflow as tf gave me that message..\r\nOS: Windows 10 home, 64 bit.\r\nTensorflow downloaded thru Anaconda, ver: tensorflow 1.8.0.\r\nBazel version:N/A\r\nCUDA/cuDNN version:N/A\r\nGPU model and memory: Intel HD Graphics 620.", "Nagging Assignee @tatatodd: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "If you installed tensorflow through `conda install` those are packages not built/maintained by us.\r\nTherefore, we wont be able to provide support on those packages.\r\nPlease reach out to Anaconda for support with those packages."]}, {"number": 20464, "title": "[Feature request] Allow `tf.estimator.train_and_evaluate` to evaluate on multiple datasets", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\n\r\nCurrently `tf.estimator.train_and_evaluate` makes it easy to use an `Estimator` to perform both training and evaluation, possibly in a distributed environment.  However, this function only supports a single evaluation dataset.  This makes the function suboptimal because we oftentimes want to evaluate on both the training and the validation set in order to get a sense for the amount of overfitting that is happening.  It would be ideal if we could perhaps pass a list of `EvalSpec` objects to `train_and_evaluate`.", "comments": ["By the way, I ask another related question.\r\nWhen I use API `train_and_evaluate `, TenorFlow make a single node (not part of training cluster) that named `Evaluator`, it monitor the checkpoint directory and evaluate.\r\nFor large model, parameters' size cannot load by single node, Is there any suggestion about distributed evaluate in TensorFlow? Can evaluator reuse training cluster?", "Probably it is a duplicate of https://github.com/tensorflow/tensorflow/issues/16087", "/cc @xiejw ", "@jart I suppose that you want to reassign this right?", "> By the way, I ask another related question.\r\n> When I use API `train_and_evaluate `, TenorFlow make a single node (not part of training cluster) that named `Evaluator`, it monitor the checkpoint directory and evaluate.\r\n> For large model, parameters' size cannot load by single node, Is there any suggestion about distributed evaluate in TensorFlow? Can evaluator reuse training cluster?\r\n\r\nI have the same issue. Do you have any suggestion?", "@joe-antognini,\r\nSorry for the delayed response. Since we use [TF Keras](https://www.tensorflow.org/api_docs/python/tf/keras) predominantly in **`Tensorflow 2.x`** and doesn't use [TF Estimators](https://www.tensorflow.org/guide/estimator) much, can you please let us know if this feature request is still relevant? Thanks!", "Yeah, I think that this isn't needed anymore since Estimators discouraged."]}, {"number": 20463, "title": "C++ Code", "body": "Hello, thank you very much for answer me !\r\n\r\nThis is the question:\r\n\r\nThere are 2 tensors, tensor A and tensor  B with same shape and data-type,  how can I put B into A with C++ code. I know in Python, I can use \"append()\" function. In C++, which function is same  with \"append()\"?\r\n\r\nPython's code for example :\r\n```\r\nTrainImageDataArray = []\r\nImageData = mpl.image.imread(strImageInfoArray[i][0])\r\nImageData = ImageData.reshape(-1)\r\nTrainImageDataArray.append(ImageData)\r\n```\r\nAnd how can I put \"ImageData\" into \"TrainImageDataArray\" in C++.", "comments": ["Suppose you are using std::vector for that, you can do that:\r\n\r\n`TrainImageDataArray.insert(TrainImageDataArray.end(), ImageData .begin(), ImageData .end());`"]}, {"number": 20462, "title": "[Feature Request] Exponential Integral function Ei", "body": "No form because it's not a bug. -- Well, I assume it isn't a bug. As it happens, while Tensorflow does implement the log-gamma and regularized incomplete gamma functions, as far as I know no combination of provided functions can be used to obtain the exponential integral function Ei (it's a 0/0 problem for the most part). As far as I know, my only recourse at this point is writing a custom op and gradient with SciPy's expi function. It'd be cool if that were addressed in the next version.\r\n\r\nThanks!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I suppose, Mr. Bot?\r\n\r\nHave I written custom code - N/A\r\nOS Platform and Distribution - N/A\r\nTensorFlow installed from - 1.9\r\nTensorFlow version - 1.9\r\nBazel version - 0.15\r\nCUDA/cuDNN version - 9.0/7.1\r\nGPU model and memory - N/A\r\nExact command to reproduce - N/A", "Approval for API review. It would live in our new tf.math module.", "Marking as contributions welcome if anyone wants to work on this", "I can work on this. And I have question here. As [boost](https://www.boost.org/doc/libs/1_48_0/libs/math/doc/sf_and_dist/html/math_toolkit/special/expint/expint_i.html) already provided an EI calculation here. I recommend to just use that and write a wrapper instead of writing it in Python codes. Would you prefer writing pure Python codes for this ops or just use the boost implementation and add a Python wrapper? @josh11b @reedwm ", "Hi @robertmaxton42! Does [tf.math.special.expint ](https://www.tensorflow.org/api_docs/python/tf/math/special/expint)api resolve this issue?", "... It's a bit _late_, though I suppose the function was added \"only\" a year or two after the original request, but ... yes, as far as I can tell. I have thoroughly moved on from that project, however, so I'm just assuming it works the way it says.", "Thanks @robertmaxton42 for the confirmation. Closing this issue for now.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 20460, "title": "Reapplying #20295", "body": "", "comments": ["\"ERROR: Config value hdfs is not defined in any .rc file\". Any idea how this change can cause this error? Seems irrelevant.", "@yifeif I don't think it's related, but seems to be something related to configure.py. Maybe it's just a transient error?", "Yea okay this is a known issue and the fix should be in the next push. I will merge this."]}, {"number": 20459, "title": "How to link Tensorflow C++ API code with the original c++ source code?", "body": "I use TensorFlow to build an ML model and now want to use it in my application. My plan is to use the TF to implement the ML reference using my ML model.\r\nIs it possible to use TF API within my application and link TF as a library when building my application?\r\nMy application is a c++ code.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@wdong5 is TF Serving what you want? https://www.tensorflow.org/serving/", "It has been 34 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Yes you can. Take a look at this blog post: https://tuanphuc.github.io/standalone-tensorflow-cpp/", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20458, "title": "Update Google Python Style Guide link", "body": "Updated the link to Google Python Style Guide, it was pointing to the [deprecated guide](https://google.github.io/styleguide/pyguide.html).\r\nThe new [updated guide](https://github.com/google/styleguide/blob/gh-pages/pyguide.md).", "comments": []}, {"number": 20457, "title": "Krylov subspace matrix solvers?", "body": "Hi @langmore @ebrevdo,\r\n\r\nI'm interested in performing differentiable matrix solves with TensorFlow using Krylov subspace methods (for sparse or structured matrices). I was considering making a conjugate gradient solver, since that's what I need at the moment, and was wondering if that would be worthy of contributing somewhere? Or if it's already been done? I was planning to use `tf.while_loop` and `tf.linalg.LinearOperator`, and to implement the gradient using the standard $dA^{-1}/dx = -A^{-1} dA/dx A^{-1}$ formula.\r\n\r\nAny advice/pointers would be appreciated. Thanks!", "comments": ["@rmlarsen did put together a few functions.  See e.g. `tf.contrib.solvers.linear_equations.conjugate_gradient`  ([code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/solvers/python/ops/linear_equations.py)).  This was build before the `LinearOperator` class was developed, and should be ported to use `LinearOperator` (the port would be easy, simply change `operator.apply` to `operator.matvec`).  Perhaps @rmlarsen  would like to comment on the best next steps.", "Ah ha! Thanks @langmore. I'm also curious if the formula I mentioned for the derivative (which requires a second matrix solve) might be more efficient than just differentiating through all the CG iterations. I'll take a look...", "Closing for now. Thanks again."]}, {"number": 20456, "title": "add sticky version flag to https://www.tensorflow.org/", "body": "The https://www.tensorflow.org/ site would be greatly improved by a sticky flag (stored in a cookie in the browser) for the version you're using.  This would be used (as a default) for the landing page and search results.  People could choose to set it, or use have some such value as 'stable' for the latest stable release.\r\n\r\nI waste a lot of time repeatedly navigating from 1.8 to the right version for the environment I'm working in.  When reading the docs, I prefer to read them for the right version of the API ...", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: 1.7\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce:  https://www.tensorflow.org/api_docs/python/ then search (e.g. tf.layers.dense).  The version I require (1.7) isn't on the first or second pages, so the quickest route is to click on the link for any of the prior versions (e.g. first is for v1.1) and edit the URL to change /r1.1/ to /r1.7/.\r\n\r\nNasty, and not recommended as a standard practice.", "Nagging Assignee @wolffg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks for the feedback, but there are no plans to add this feature at this time.\r\nHowever, you can bookmark an older version: https://www.tensorflow.org/versions/r1.7/api_docs/python/\r\nAnd we are planning to add a /stable/ directory that points to the latest release.", "If you could make search aware of and respect this base version, that would be very helpful. ", "Good idea and will keep it in mind, thanks"]}, {"number": 20455, "title": "fix bug in maxout function", "body": "The line \"shape[axis] = -1\" will make the shape wrong when dealing with batches with arbitrary sizes.\r\nif the shape of input tensor is [None, ... , num_channels, ... ], \"shape[axis]=-1\" together with \"shape+=[num_channels // num_units]\" will make the shape become [None, ... , -1, ... , num_channels // num_units]. But when reshape the input tensor, the \"None\" element in shape will make \"-1\" become \"None\", not \"num_units\".\r\n", "comments": ["@martinwicke can I merge this?", "Check with @yifeif but I think yes. ", "You are good if it's for non-master branches.", "MacOS flake. Re-running.", "Going to merge this fix into 1.9 so we can start official release builds. Please follow-up with @pavithrasv comment and add a test case with None please though.\r\n\r\nThanks!"]}, {"number": 20454, "title": "Added minSdkVersion to the manifest", "body": "Fixes #20453", "comments": []}, {"number": 20453, "title": "Tensorflow Lite adds unnecessary permissions.", "body": "When using Tensorflow Lite 0.17.0, it adds `android.permission.READ_PHONE_STATE` and `android.permission.READ_EXTERNAL_STORAGE` because no min sdk was specified.\r\n\r\nI suggest to hotfix and release this ASAP because it's adding these permissions to apps that don't need them.\r\n\r\nHere is the log from the manifest merger:\r\n```\r\nuses-permission#android.permission.READ_PHONE_STATE\r\nIMPLIED from /home/ph1b/Dev/Yazio/app/src/main/AndroidManifest.xml:2:1-165:12 reason: org.tensorflow.lite has a targetSdkVersion < 4\r\nuses-permission#android.permission.READ_EXTERNAL_STORAGE\r\nIMPLIED from /home/ph1b/Dev/Yazio/app/src/main/AndroidManifest.xml:2:1-165:12 reason: org.tensorflow.lite requested WRITE_EXTERNAL_STORAGE\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I don't think the form has anything necessary that my bugreport hasn't.", "@aselle Can you take a look at this?"]}, {"number": 20452, "title": "Keras TimeDistributed with Input and no batch_size fails", "body": "OS: Windows 10 & Ubuntu 16.04 tested\r\nTensorflow 1.8\r\nPython 3.6.5\r\n\r\nThe following code shows the problem.\r\nWhen using `Input` with a `batch_size` everything works fine, but without it it fails.\r\nI assumed that when setting the `batch_size` in `batch_shape` to `None` or using `shape` instead that the `batch_size` would then be dynamic.\r\n\r\n```python\r\nfrom tensorflow.python.keras.layers import Input, Lambda, TimeDistributed\r\nfrom tensorflow.python.keras import backend as K\r\n\r\n\r\ni_batch = Input(batch_shape=(32, 18, 64, 512))\r\ni_batch_none_error = Input(batch_shape=(None, 18, 64, 512))\r\ni_error = Input(shape=(18, 64, 512))\r\n\r\n\r\ndef reduce_sum(x):\r\n  return K.sum(x, axis=-2)\r\n\r\n\r\nsumpool = Lambda(reduce_sum, output_shape=(512,))\r\n# Computes correct shape (?, 18, 512)\r\nprint(TimeDistributed(sumpool, name='t')(i_batch))\r\n\r\n# Both throw warnings\r\n# Computes incorrect shape (?, 18, 64, 512)\r\nprint(TimeDistributed(sumpool, name='t')(i_batch_none_error))\r\n# Computes incorrect shape (?, 18, 64, 512)\r\nprint(TimeDistributed(sumpool, name='t')(i_error))\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@sleighsoft I could not reproduce the issue. With the code snippet that you have shared here is the output I got:\r\n```\r\nTensor(\"t/transpose_1:0\", shape=(32, 18, 512), dtype=float32)\r\nTensor(\"t_1/Reshape_1:0\", shape=(?, 18, 512), dtype=float32)\r\nTensor(\"t_2/Reshape_1:0\", shape=(?, 18, 512), dtype=float32)\r\n```\r\nAre you still seeing the issue? ", "I have this issue when running it under `tensorflow-gpu==1.8.0`.\r\nI tried it with the most recent `tf-nightly==1.10.0.dev20180609` and there it works.", "Thank you for checking. Closing the issue now as it seems to have been fixed.", "I have not tried it with the GPU version. It is an assumption that it is fixed there as well", "Please feel free to re-open the issue if you see it again."]}, {"number": 20451, "title": "with quantized-training model,PC ok but tf-lite failed.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux  4.17.2-1-ARCH SMP\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:b'v1.8.0-3238-g52bf2fe0f6' 1.9.0-rc0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**:0.14.1- (@non-git)\r\n- **GCC/Compiler version (if compiling from source)**:gcc-7.1\r\n- **CUDA/cuDNN version**:cuda-9.2 cuDNN-7.1\r\n- **GPU model and memory**:16G\r\n- **Exact command to reproduce**:\r\n\r\nI trained **label_image** on mobilenetv2 backbone with quantization and everything works well on PC.\r\nThen I tried to convert it to tf-lite,even the converting processing is well-down(**no error,no unsupported ops**),but when I finally ran it I got **tensorflow/contrib/lite/kernels/conv.cc:260 real_multiplier < 1.0 was not true**\r\nThis the log:\r\n`\r\ntensors size: 174\r\nnodes size: 66\r\ninputs: 1\r\ninput(0) name: input\r\n0: MobilenetV2/Conv/Conv2D_Fold_bias, 128, 2, 0.0408043, 0\r\n1: MobilenetV2/Conv/Relu6, 100352, 3, 0.0235285, 0\r\n2: MobilenetV2/Conv/weights_quant/FakeQuantWithMinMaxVars, 864, 3, 0.0408043, 121\r\n3: MobilenetV2/Conv_1/Conv2D_Fold_bias, 5120, 2, 0.00108845, 0\r\n4: MobilenetV2/Conv_1/Relu6, 20480, 3, 0.0235285, 0\r\n5: MobilenetV2/Conv_1/weights_quant/FakeQuantWithMinMaxVars, 409600, 3, 0.00680513, 119\r\n6: MobilenetV2/Logits/AvgPool, 1280, 3, 0.0235285, 0\r\n7: MobilenetV2/Logits/Conv2d_1c_1x1/BiasAdd, 3, 3, 0.174401, 120\r\n8: MobilenetV2/Logits/Conv2d_1c_1x1/Conv2D_bias, 12, 2, 2.91482e-05, 0\r\n9: MobilenetV2/Logits/Conv2d_1c_1x1/weights_quant/FakeQuantWithMinMaxVars, 3840, 3, 0.00123885, 125\r\n10: MobilenetV2/Logits/Squeeze, 3, 3, 0.174401, 120\r\n11: MobilenetV2/Logits/Squeeze_shape, 8, 2, 0, 0\r\n12: MobilenetV2/Predictions/Reshape_1, 3, 3, 0.00390625, 0\r\n13: MobilenetV2/expanded_conv/depthwise/Relu6, 100352, 3, 0.0235285, 0\r\n14: MobilenetV2/expanded_conv/depthwise/depthwise_Fold_bias, 128, 2, 0.00805492, 0\r\n15: MobilenetV2/expanded_conv/depthwise/weights_quant/FakeQuantWithMinMaxVars, 288, 3, 0.342348, 165\r\n16: MobilenetV2/expanded_conv/project/Conv2D_Fold_bias, 64, 2, 0.00091254, 0\r\n17: MobilenetV2/expanded_conv/project/add_fold, 50176, 3, 0.354141, 130\r\n18: MobilenetV2/expanded_conv/project/weights_quant/FakeQuantWithMinMaxVars, 512, 3, 0.0387845, 150\r\n19: MobilenetV2/expanded_conv_1/depthwise/Relu6, 75264, 3, 0.0235285, 0\r\n20: MobilenetV2/expanded_conv_1/depthwise/depthwise_Fold_bias, 384, 2, 0.00060018, 0\r\n21: MobilenetV2/expanded_conv_1/depthwise/weights_quant/FakeQuantWithMinMaxVars, 864, 3, 0.0255087, 109\r\n22: MobilenetV2/expanded_conv_1/expand/Conv2D_Fold_bias, 384, 2, 0.00352435, 0\r\n23: MobilenetV2/expanded_conv_1/expand/Relu6, 301056, 3, 0.0235285, 0\r\n24: MobilenetV2/expanded_conv_1/expand/weights_quant/FakeQuantWithMinMaxVars, 1536, 3, 0.00995183, 126\r\n25: MobilenetV2/expanded_conv_1/project/Conv2D_Fold_bias, 96, 2, 0.000607076, 0\r\n26: MobilenetV2/expanded_conv_1/project/add_fold, 18816, 3, 0.294347, 131\r\n27: MobilenetV2/expanded_conv_1/project/weights_quant/FakeQuantWithMinMaxVars, 2304, 3, 0.0258018, 151\r\n28: MobilenetV2/expanded_conv_10/depthwise/Relu6, 18816, 3, 0.0235285, 0\r\n29: MobilenetV2/expanded_conv_10/depthwise/depthwise_Fold_bias, 1536, 2, 0.000711485, 0\r\n30: MobilenetV2/expanded_conv_10/depthwise/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.0302393, 140\r\n31: MobilenetV2/expanded_conv_10/expand/Conv2D_Fold_bias, 1536, 2, 0.000350018, 0\r\n32: MobilenetV2/expanded_conv_10/expand/Relu6, 18816, 3, 0.0235285, 0\r\n33: MobilenetV2/expanded_conv_10/expand/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.00174141, 149\r\n34: MobilenetV2/expanded_conv_10/project/Conv2D_Fold_bias, 384, 2, 0.000179579, 0\r\n35: MobilenetV2/expanded_conv_10/project/add_fold, 4704, 3, 0.150007, 128\r\n36: MobilenetV2/expanded_conv_10/project/weights_quant/FakeQuantWithMinMaxVars, 36864, 3, 0.00763239, 134\r\n37: MobilenetV2/expanded_conv_11/add, 4704, 3, 0.149921, 125\r\n38: MobilenetV2/expanded_conv_11/depthwise/Relu6, 28224, 3, 0.0235285, 0\r\n39: MobilenetV2/expanded_conv_11/depthwise/depthwise_Fold_bias, 2304, 2, 0.0013777, 0\r\n40: MobilenetV2/expanded_conv_11/depthwise/weights_quant/FakeQuantWithMinMaxVars, 5184, 3, 0.0585545, 92\r\n41: MobilenetV2/expanded_conv_11/expand/Conv2D_Fold_bias, 2304, 2, 0.000252474, 0\r\n42: MobilenetV2/expanded_conv_11/expand/Relu6, 28224, 3, 0.0235285, 0\r\n43: MobilenetV2/expanded_conv_11/expand/weights_quant/FakeQuantWithMinMaxVars, 55296, 3, 0.00168308, 133\r\n44: MobilenetV2/expanded_conv_11/project/Conv2D_Fold_bias, 384, 2, 0.000203408, 0\r\n45: MobilenetV2/expanded_conv_11/project/add_fold, 4704, 3, 0.102331, 126\r\n46: MobilenetV2/expanded_conv_11/project/weights_quant/FakeQuantWithMinMaxVars, 55296, 3, 0.00864519, 141\r\n47: MobilenetV2/expanded_conv_12/add, 4704, 3, 0.213277, 145\r\n48: MobilenetV2/expanded_conv_12/depthwise/Relu6, 28224, 3, 0.0235285, 0\r\n49: MobilenetV2/expanded_conv_12/depthwise/depthwise_Fold_bias, 2304, 2, 0.00214226, 0\r\n50: MobilenetV2/expanded_conv_12/depthwise/weights_quant/FakeQuantWithMinMaxVars, 5184, 3, 0.0910496, 173\r\n51: MobilenetV2/expanded_conv_12/expand/Conv2D_Fold_bias, 2304, 2, 0.000217147, 0\r\n52: MobilenetV2/expanded_conv_12/expand/Relu6, 28224, 3, 0.0235285, 0\r\n53: MobilenetV2/expanded_conv_12/expand/weights_quant/FakeQuantWithMinMaxVars, 55296, 3, 0.00144841, 139\r\n54: MobilenetV2/expanded_conv_12/project/Conv2D_Fold_bias, 384, 2, 0.000596996, 0\r\n55: MobilenetV2/expanded_conv_12/project/add_fold, 4704, 3, 0.170068, 144\r\n56: MobilenetV2/expanded_conv_12/project/weights_quant/FakeQuantWithMinMaxVars, 55296, 3, 0.0253734, 149\r\n57: MobilenetV2/expanded_conv_13/depthwise/Relu6, 9216, 3, 0.0235285, 0\r\n58: MobilenetV2/expanded_conv_13/depthwise/depthwise_Fold_bias, 2304, 2, 0.00034101, 0\r\n59: MobilenetV2/expanded_conv_13/depthwise/weights_quant/FakeQuantWithMinMaxVars, 5184, 3, 0.0144935, 90\r\n60: MobilenetV2/expanded_conv_13/expand/Conv2D_Fold_bias, 2304, 2, 0.000297714, 0\r\n61: MobilenetV2/expanded_conv_13/expand/Relu6, 28224, 3, 0.0235285, 0\r\n62: MobilenetV2/expanded_conv_13/expand/weights_quant/FakeQuantWithMinMaxVars, 55296, 3, 0.0013959, 122\r\n63: MobilenetV2/expanded_conv_13/project/Conv2D_Fold_bias, 640, 2, 0.000194787, 0\r\n64: MobilenetV2/expanded_conv_13/project/add_fold, 2560, 3, 0.144367, 122\r\n65: MobilenetV2/expanded_conv_13/project/weights_quant/FakeQuantWithMinMaxVars, 92160, 3, 0.00827876, 139\r\n66: MobilenetV2/expanded_conv_14/add, 2560, 3, 0.150496, 130\r\n67: MobilenetV2/expanded_conv_14/depthwise/Relu6, 15360, 3, 0.0235285, 0\r\n68: MobilenetV2/expanded_conv_14/depthwise/depthwise_Fold_bias, 3840, 2, 0.0010313, 0\r\n69: MobilenetV2/expanded_conv_14/depthwise/weights_quant/FakeQuantWithMinMaxVars, 8640, 3, 0.0438318, 148\r\n70: MobilenetV2/expanded_conv_14/expand/Conv2D_Fold_bias, 3840, 2, 0.000355036, 0\r\n71: MobilenetV2/expanded_conv_14/expand/Relu6, 15360, 3, 0.0235285, 0\r\n72: MobilenetV2/expanded_conv_14/expand/weights_quant/FakeQuantWithMinMaxVars, 153600, 3, 0.00245926, 111\r\n73: MobilenetV2/expanded_conv_14/project/Conv2D_Fold_bias, 640, 2, 0.000174049, 0\r\n74: MobilenetV2/expanded_conv_14/project/add_fold, 2560, 3, 0.0903757, 130\r\n75: MobilenetV2/expanded_conv_14/project/weights_quant/FakeQuantWithMinMaxVars, 153600, 3, 0.00739738, 137\r\n76: MobilenetV2/expanded_conv_15/add, 2560, 3, 0.300834, 122\r\n77: MobilenetV2/expanded_conv_15/depthwise/Relu6, 15360, 3, 0.0235285, 0\r\n78: MobilenetV2/expanded_conv_15/depthwise/depthwise_Fold_bias, 3840, 2, 0.00129889, 0\r\n79: MobilenetV2/expanded_conv_15/depthwise/weights_quant/FakeQuantWithMinMaxVars, 8640, 3, 0.0552048, 110\r\n80: MobilenetV2/expanded_conv_15/expand/Conv2D_Fold_bias, 3840, 2, 0.000226284, 0\r\n81: MobilenetV2/expanded_conv_15/expand/Relu6, 15360, 3, 0.0235285, 0\r\n82: MobilenetV2/expanded_conv_15/expand/weights_quant/FakeQuantWithMinMaxVars, 153600, 3, 0.00150359, 99\r\n83: MobilenetV2/expanded_conv_15/project/Conv2D_Fold_bias, 640, 2, 0.000805016, 0\r\n84: MobilenetV2/expanded_conv_15/project/add_fold, 2560, 3, 0.226103, 131\r\n85: MobilenetV2/expanded_conv_15/project/weights_quant/FakeQuantWithMinMaxVars, 153600, 3, 0.0342145, 139\r\n86: MobilenetV2/expanded_conv_16/depthwise/Relu6, 15360, 3, 0.0235285, 0\r\n87: MobilenetV2/expanded_conv_16/depthwise/depthwise_Fold_bias, 3840, 2, 0.0040495, 0\r\n88: MobilenetV2/expanded_conv_16/depthwise/weights_quant/FakeQuantWithMinMaxVars, 8640, 3, 0.17211, 201\r\n89: MobilenetV2/expanded_conv_16/expand/Conv2D_Fold_bias, 3840, 2, 0.000576843, 0\r\n90: MobilenetV2/expanded_conv_16/expand/Relu6, 15360, 3, 0.0235285, 0\r\n91: MobilenetV2/expanded_conv_16/expand/weights_quant/FakeQuantWithMinMaxVars, 153600, 3, 0.00191748, 125\r\n92: MobilenetV2/expanded_conv_16/project/Conv2D_Fold_bias, 1280, 2, 0.000119162, 0\r\n93: MobilenetV2/expanded_conv_16/project/add_fold, 5120, 3, 0.159945, 146\r\n94: MobilenetV2/expanded_conv_16/project/weights_quant/FakeQuantWithMinMaxVars, 307200, 3, 0.0050646, 130\r\n95: MobilenetV2/expanded_conv_2/add, 18816, 3, 0.376629, 129\r\n96: MobilenetV2/expanded_conv_2/depthwise/Relu6, 112896, 3, 0.0235285, 0\r\n97: MobilenetV2/expanded_conv_2/depthwise/depthwise_Fold_bias, 576, 2, 0.00397992, 0\r\n98: MobilenetV2/expanded_conv_2/depthwise/weights_quant/FakeQuantWithMinMaxVars, 1296, 3, 0.169153, 51\r\n99: MobilenetV2/expanded_conv_2/expand/Conv2D_Fold_bias, 576, 2, 0.00106746, 0\r\n100: MobilenetV2/expanded_conv_2/expand/Relu6, 112896, 3, 0.0235285, 0\r\n101: MobilenetV2/expanded_conv_2/expand/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.00362654, 142\r\n102: MobilenetV2/expanded_conv_2/project/Conv2D_Fold_bias, 96, 2, 0.000610138, 0\r\n103: MobilenetV2/expanded_conv_2/project/add_fold, 18816, 3, 0.342911, 133\r\n104: MobilenetV2/expanded_conv_2/project/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.0259319, 129\r\n105: MobilenetV2/expanded_conv_3/depthwise/Relu6, 28224, 3, 0.0235285, 0\r\n106: MobilenetV2/expanded_conv_3/depthwise/depthwise_Fold_bias, 576, 2, 0.000397524, 0\r\n107: MobilenetV2/expanded_conv_3/depthwise/weights_quant/FakeQuantWithMinMaxVars, 1296, 3, 0.0168954, 126\r\n108: MobilenetV2/expanded_conv_3/expand/Conv2D_Fold_bias, 576, 2, 0.00108431, 0\r\n109: MobilenetV2/expanded_conv_3/expand/Relu6, 112896, 3, 0.0235285, 0\r\n110: MobilenetV2/expanded_conv_3/expand/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.00287898, 107\r\n111: MobilenetV2/expanded_conv_3/project/Conv2D_Fold_bias, 128, 2, 0.000396253, 0\r\n112: MobilenetV2/expanded_conv_3/project/add_fold, 6272, 3, 0.20811, 126\r\n113: MobilenetV2/expanded_conv_3/project/weights_quant/FakeQuantWithMinMaxVars, 4608, 3, 0.0168414, 109\r\n114: MobilenetV2/expanded_conv_4/add, 6272, 3, 0.250818, 134\r\n115: MobilenetV2/expanded_conv_4/depthwise/Relu6, 37632, 3, 0.0235285, 0\r\n116: MobilenetV2/expanded_conv_4/depthwise/depthwise_Fold_bias, 768, 2, 0.00236945, 0\r\n117: MobilenetV2/expanded_conv_4/depthwise/weights_quant/FakeQuantWithMinMaxVars, 1728, 3, 0.100705, 79\r\n118: MobilenetV2/expanded_conv_4/expand/Conv2D_Fold_bias, 768, 2, 0.000398864, 0\r\n119: MobilenetV2/expanded_conv_4/expand/Relu6, 37632, 3, 0.0235285, 0\r\n120: MobilenetV2/expanded_conv_4/expand/weights_quant/FakeQuantWithMinMaxVars, 6144, 3, 0.0019166, 151\r\n121: MobilenetV2/expanded_conv_4/project/Conv2D_Fold_bias, 128, 2, 0.000489459, 0\r\n122: MobilenetV2/expanded_conv_4/project/add_fold, 6272, 3, 0.200968, 132\r\n123: MobilenetV2/expanded_conv_4/project/weights_quant/FakeQuantWithMinMaxVars, 6144, 3, 0.0208029, 147\r\n124: MobilenetV2/expanded_conv_5/add, 6272, 3, 0.276968, 127\r\n125: MobilenetV2/expanded_conv_5/depthwise/Relu6, 37632, 3, 0.0235285, 0\r\n126: MobilenetV2/expanded_conv_5/depthwise/depthwise_Fold_bias, 768, 2, 0.00202895, 0\r\n127: MobilenetV2/expanded_conv_5/depthwise/weights_quant/FakeQuantWithMinMaxVars, 1728, 3, 0.086234, 63\r\n128: MobilenetV2/expanded_conv_5/expand/Conv2D_Fold_bias, 768, 2, 0.000362468, 0\r\n129: MobilenetV2/expanded_conv_5/expand/Relu6, 37632, 3, 0.0235285, 0\r\n130: MobilenetV2/expanded_conv_5/expand/weights_quant/FakeQuantWithMinMaxVars, 6144, 3, 0.00144514, 120\r\n131: MobilenetV2/expanded_conv_5/project/Conv2D_Fold_bias, 128, 2, 0.000435147, 0\r\n132: MobilenetV2/expanded_conv_5/project/add_fold, 6272, 3, 0.205061, 128\r\n133: MobilenetV2/expanded_conv_5/project/weights_quant/FakeQuantWithMinMaxVars, 6144, 3, 0.0184945, 128\r\n134: MobilenetV2/expanded_conv_6/depthwise/Relu6, 9408, 3, 0.0235285, 0\r\n135: MobilenetV2/expanded_conv_6/depthwise/depthwise_Fold_bias, 768, 2, 0.000267618, 0\r\n136: MobilenetV2/expanded_conv_6/depthwise/weights_quant/FakeQuantWithMinMaxVars, 1728, 3, 0.0113742, 126\r\n137: MobilenetV2/expanded_conv_6/expand/Conv2D_Fold_bias, 768, 2, 0.000528109, 0\r\n138: MobilenetV2/expanded_conv_6/expand/Relu6, 37632, 3, 0.0235285, 0\r\n139: MobilenetV2/expanded_conv_6/expand/weights_quant/FakeQuantWithMinMaxVars, 6144, 3, 0.00190675, 128\r\n140: MobilenetV2/expanded_conv_6/project/Conv2D_Fold_bias, 256, 2, 0.00033262, 0\r\n141: MobilenetV2/expanded_conv_6/project/add_fold, 3136, 3, 0.182997, 123\r\n142: MobilenetV2/expanded_conv_6/project/weights_quant/FakeQuantWithMinMaxVars, 12288, 3, 0.0141369, 135\r\n143: MobilenetV2/expanded_conv_7/add, 3136, 3, 0.182534, 120\r\n144: MobilenetV2/expanded_conv_7/depthwise/Relu6, 18816, 3, 0.0235285, 0\r\n145: MobilenetV2/expanded_conv_7/depthwise/depthwise_Fold_bias, 1536, 2, 0.00131166, 0\r\n146: MobilenetV2/expanded_conv_7/depthwise/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.0557479, 130\r\n147: MobilenetV2/expanded_conv_7/expand/Conv2D_Fold_bias, 1536, 2, 0.000259114, 0\r\n148: MobilenetV2/expanded_conv_7/expand/Relu6, 18816, 3, 0.0235285, 0\r\n149: MobilenetV2/expanded_conv_7/expand/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.00141595, 134\r\n150: MobilenetV2/expanded_conv_7/project/Conv2D_Fold_bias, 256, 2, 0.000434401, 0\r\n151: MobilenetV2/expanded_conv_7/project/add_fold, 3136, 3, 0.150338, 112\r\n152: MobilenetV2/expanded_conv_7/project/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.0184628, 129\r\n153: MobilenetV2/expanded_conv_8/add, 3136, 3, 0.182103, 121\r\n154: MobilenetV2/expanded_conv_8/depthwise/Relu6, 18816, 3, 0.0235285, 0\r\n155: MobilenetV2/expanded_conv_8/depthwise/depthwise_Fold_bias, 1536, 2, 0.000993556, 0\r\n156: MobilenetV2/expanded_conv_8/depthwise/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.0422278, 135\r\n157: MobilenetV2/expanded_conv_8/expand/Conv2D_Fold_bias, 1536, 2, 0.000266144, 0\r\n158: MobilenetV2/expanded_conv_8/expand/Relu6, 18816, 3, 0.0235285, 0\r\n159: MobilenetV2/expanded_conv_8/expand/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.00145805, 128\r\n160: MobilenetV2/expanded_conv_8/project/Conv2D_Fold_bias, 256, 2, 0.000281633, 0\r\n161: MobilenetV2/expanded_conv_8/project/add_fold, 3136, 3, 0.122043, 130\r\n162: MobilenetV2/expanded_conv_8/project/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.0119699, 127\r\n163: MobilenetV2/expanded_conv_9/add, 3136, 3, 0.200997, 130\r\n164: MobilenetV2/expanded_conv_9/depthwise/Relu6, 18816, 3, 0.0235285, 0\r\n165: MobilenetV2/expanded_conv_9/depthwise/depthwise_Fold_bias, 1536, 2, 0.000984803, 0\r\n166: MobilenetV2/expanded_conv_9/depthwise/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.0418558, 151\r\n167: MobilenetV2/expanded_conv_9/expand/Conv2D_Fold_bias, 1536, 2, 0.000224455, 0\r\n168: MobilenetV2/expanded_conv_9/expand/Relu6, 18816, 3, 0.0235285, 0\r\n169: MobilenetV2/expanded_conv_9/expand/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.00123257, 123\r\n170: MobilenetV2/expanded_conv_9/project/Conv2D_Fold_bias, 256, 2, 0.000418117, 0\r\n171: MobilenetV2/expanded_conv_9/project/add_fold, 3136, 3, 0.157535, 127\r\n172: MobilenetV2/expanded_conv_9/project/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.0177707, 145\r\n173: input, 37632, 3, 1, 128\r\nnumber of inputs: 1\r\nnumber of outputs: 1\r\ntensorflow/contrib/lite/kernels/conv.cc:260 real_multiplier < 1.0 was not true.\r\n`\r\n\r\n**I just add** `tf.contrib.quantize.create_training_graph( input_graph=tf.get_default_graph(),         quant_delay=FLAGS.quan_delay)` in training and ` tf.contrib.quantize.create_eval_graph()`\r\nin evaluation compared to the official version.\r\nIs there any extra processes I need to care? Anyone help ?  ", "comments": ["Same situation with MobilenetV1, that is used as a backbone for separate model. Model was trained and then freezed with tf.contrib.quantize.create_training_graph and tf.contrib.quantize.create_eval_graph. Backbone then was separated and converted with flag --allow_nudging_weights_to_use_fast_gemm_kernel. My tensorflow version is 1.9.0-rc2.\r\nMy thoughts so far: in the source code for TFLite there is 2 quantization routins QuantizeMultiplierGreaterThanOne and QuantizeMultiplierSmallerThanOneExp defined here. May be one can just add logic in tensorflow/contrib/lite/kernels/conv.cc to handle various real_multiplier? If i won't be able to find out anything better, then i will probably give it a try and report here.", "Thanks for reporting this. This seems to be an issue with convs and fully connected layers that was fixed for DepthwiseConvs in this commit https://github.com/tensorflow/tensorflow/commit/3a17101171d3e51fcba2189d09416c5106bfe4ac#diff-ca0f46c80fd3cf1f2040be6147a2d8cf\r\n\r\nWe likely need a similar fix for Conv and FC as well. ", "Nagging Assignee @liyunlu0618: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@liyunlu0618 change should resolve, it will be available in the next nightly.", "@suharshs Thanks.It works now."]}, {"number": 20450, "title": "ImportError: dlopen(/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib   Referenced from: /usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so   Reason: image not found", "body": "This is my issue https://stackoverflow.com/questions/51121273/tensorflow-error-reason-image-not-found", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Please keep in mind that the TensorFlow GitHub Issues page is for tracking bugs and feature requests. It is not a way to escalate a Stack Overflow support question that is not getting enough attention. Thanks!\r\n\r\nThat said, if you are using Cuda 8, try switching to Cuda 9. The latest versions of TensorFlow do not supoprt Cuda 8, unless installing from source."]}, {"number": 20449, "title": "    .pb Model in android is not detecting images", "body": "Using Window 10\r\nTensoflow installed from its main site.\r\n2.0 version\r\nUsing Android studio\r\nNo Bazel\r\nI am using following code to train my dataset. The code first downloads the requested model and then create .pb file with label text file. The link is as follows:\r\nhttps://github.com/loicmarie/sign-language-alphabet-recognizer\r\nAfter converting my model into .pb file and getting labels, I loaded my model into following android code:\r\nhttps://github.com/Nilhcem/tensorflow-classifier-android\r\nThen I make little changes to code according to my model.\r\n![screenshot 92](https://user-images.githubusercontent.com/32578887/42205475-0617f714-7ebe-11e8-9390-a5248d8362fd.png)\r\nPlease, where I am wrong what I need to make image detection possible. As it is not giving labels...\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20448, "title": "sess.run a list with assign ?", "body": "```\r\nimport tensorflow as tf\r\n\r\nsess = tf.Session()\r\n\r\nx = tf.Variable(0.)\r\ny = tf.assign(x, 1)\r\nz = x**2\r\ng = tf.gradients(z, [x])[0]\r\n\r\nsess.run(tf.global_variables_initializer())\r\n\r\nsess.run([g, y, g])\r\n```\r\n\r\nthe code return [2, 1, 2]. but what I expect is [0, 1, 2]. \r\n\r\nif I run `[sess.run(i) for i in [g, y, g]]`, it return [0, 1, 2], but I want to get [0, 1, 2] in one `sess.run`.\r\n\r\nis it possible?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "TensorFlow uses a computation graph for dependency control, so it's a bit different than typical procedural programming. Asking for the same thing twice in one call to `session.run` will always (I think?) yield the same result twice. Maybe the following is what you're looking for?\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nsess = tf.Session()\r\n\r\nx = tf.Variable(0.)\r\nassign_one = tf.assign(x, 1)\r\nz = x**2\r\ngradients = tf.gradients(z, [x])[0]\r\nwith tf.control_dependencies([assign_one]):\r\n    z_after_assign = x**2\r\ngradients_after_assign = tf.gradients(z_after_assign, [x])[0]\r\n\r\nsess.run(tf.global_variables_initializer())\r\nsess.run([gradients, gradients_after_assign])\r\n\r\n# [0.0, 2.0]\r\n```\r\n\r\n(Also, if I understand correctly, Stack Overflow is the TF maintainers' preferred place for questions, for future reference.)", "maybe I should repeat my question in an another way.\r\n\r\nhttps://stackoverflow.com/questions/51123502/what-different-between-sess-run-two-times-or-one-time-with-tf-control-dependenci\r\n\r\nI want to use tensorflow to solve ode with RK23 or RK45 method, the ode is\r\n$$ \\dot{\\boldsymbol{\\theta}} = - \\nabla_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta}) $$\r\n(eq(4) in https://kexue.fm/archives/5655#GD%E4%B8%8EODE )\r\n\r\nso I need autograd. At echo steps of RK method, I need to get gradients at different point at least twice.\r\n", "https://github.com/tensorflow/tensorflow/issues/20467"]}]