[{"number": 1059, "title": "Fix some compilation issues found with Visual Studio 2015.", "body": "- Use standard types in allocator.h, inlined_vector.h and png_io.cc\n- Fix missing return in allocator.h, device_base.h and range_sampler.h\n- Add function NodeDefBuilder& Input(std::vectorNodeDefBuilder::NodeOut srcs)\n  so type std::vectorNodeDefBuilder::NodeOut can be solved unambiguously\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "INFO: From Compiling tensorflow/core/framework/op_segment_test.cc:\ntensorflow/core/framework/op_segment_test.cc:35:14: error: cannot declare field 'tensorflow::OpSegmentTest::device_' to be of abstract type 'tensorflow::DeviceBase'\n   DeviceBase device_;\n              ^\nIn file included from ./tensorflow/core/framework/op_kernel.h:25:0,\n                 from ./tensorflow/core/framework/op_segment.h:22,\n                 from tensorflow/core/framework/op_segment_test.cc:16:\n./tensorflow/core/framework/device_base.h:90:7: note:   because the following virtual functions are pure within 'tensorflow::DeviceBase':\n class DeviceBase {\n       ^\n./tensorflow/core/framework/device_base.h:178:35: note:     virtual const tensorflow::DeviceAttributes& tensorflow::DeviceBase::attributes() const\n   virtual const DeviceAttributes& attributes() const = 0;\n\nInteresting -- does this not happen when you compile in visual studio?\n", "> Interesting -- does this not happen when you compile in visual studio?\n\nI didn't compile the tests... I will figure it out.\n"]}, {"number": 1058, "title": "Fix a bug for an unavailable argument", "body": "The _num_classes_ argument of the _dense_to_one_hot_ function is never called by the other function.\n\nIHMO, we should set the number of classes in _extract_labels_.\nSince, the _extract_labels_ is a task-specific-function but the _dense_to_one_hot_ is not.\n", "comments": ["Can one of the admins verify this patch?\n", "Merged, thanks!\n"]}, {"number": 1057, "title": "Clean up references", "body": "Clean up references and add explicit links to pdf files for references\nthat are displayed on www.tensorflow.org.\n\nBecause having to download a pdf when you just want to see the title and abstract is mildly annoying.\n", "comments": ["Can one of the admins verify this patch?\n", "I think a general issue here is that very often new versions of the papers are uploaded to arxiv, and hardcoding a link to a specific version will often go stale (at least, that's what appears to be the case from the URLs).  Does arxiv have a 'latest' pdf link so that we don't have to hardcode a specific version?\n", "In other words, some of your changes are great -- do you think we should just always link to the arxiv front page for a paper instead of pdfs, consistently?\n", "Sometimes it's nice to have both, but I always prefer a link to the arxiv page over a link directly to the pdf. As I mentioned in the introduction, I get annoyed when i have to wait 5-20 seconds for a pdf to download if I just want to read the title and abstract.\n\nPlaying around with arxiv just now, it seems that removing the version number from a pdf link will point to the latest version. So the most recent version of http://arxiv.org/abs/1409.3215 is http://arxiv.org/pdf/1409.3215.pdf.\n", "http://arxiv.org/pdf/1409.3215 works too, but then it's not clear that it's a pdf.\n", "I too prefer the arxiv page over pdf.  Want to make that change to this PR?  If you want to have the pdf link too, include the version-less link and we're good :)\n", "I will fix the pdf links here. :-)\n", "@vrv Links to pdf files are version-less now.\n", "Merged, thanks!\n"]}, {"number": 1056, "title": "Optimized build across different architecture", "body": "I built a pip package with  \u0300-c opt` in one machine and use the same package on another machine. They don't have different version of core i7 processor. With the second machine, Tensorflow runs 10x slower than with the first. Could it be a compilation optimization problem ?\n", "comments": ["It could be any number of things -- different amount/type of memory, onboard graphics card competing, different BLAS libraries... We'd need a lot more information to diagnose this. If those were the same computer with the same OS installed, it may be mysterious.\n", "Closed for now due to lack of activity.  Feel free to reopen if more information is available.\n"]}, {"number": 1055, "title": "Upgraded to the latest version of Eigen", "body": "This version provides better support for embedded platforms and optimized implementations of the tanh function amongst many enhancements\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n\n(@benoitsteiner, did you rebase before the tests were done? It looks like Jenkins stopped the running tests or something)\n", "@martinwicke The branch was out of date so I rebased it.\n"]}, {"number": 1054, "title": "Update resources page", "body": "Fix some layout structure, add models repo info\n", "comments": ["One typo fix, otherwise LG\n", "merged.\n"]}, {"number": 1053, "title": "Completed the cleanup of the TensorFlow local copy of the Eigen codebase", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "Merged. Does this have to be synced with a CL internally?\n"]}, {"number": 1052, "title": "failed on build from source", "body": "envy@ub1404envy:~/os_prj/github/tensorflow$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nINFO: Found 1 target...\nERROR: /home/envy/os_prj/github/tensorflow/third_party/gpus/cuda/BUILD:126:1: declared output 'third_party/gpus/cuda/lib64/libcublas.so.7.0' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely).\nERROR: /home/envy/os_prj/github/tensorflow/third_party/gpus/cuda/BUILD:126:1: declared output 'third_party/gpus/cuda/lib64/libcudart.so.7.0' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely).\nERROR: /home/envy/os_prj/github/tensorflow/third_party/gpus/cuda/BUILD:126:1: declared output 'third_party/gpus/cuda/lib64/libcufft.so.7.0' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely).\nERROR: /home/envy/os_prj/github/tensorflow/third_party/gpus/cuda/BUILD:126:1: not all outputs were created.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 31.876s, Critical Path: 1.20s\nenvy@ub1404envy:~/os_prj/github/tensorflow$ \n", "comments": ["Need more info:\n- What git commit did you try to build at\n- What version of bazel are you running\n- Did you run ./configure before building (what other steps did you run) ?\n", "the latest tensorflow:\nenvy@ub1404envy:~/os_prj/github/tensorflow$ git status\nOn branch master\nYour branch is up-to-date with 'origin/master'.\n\ntry both bazel 1.4 and 1.5\n\nenvy@ub1404envy:~/os_prj/github/tensorflow$ ./configure\nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the location where cuDNN 6.5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nSetting up Cuda include\nSetting up Cuda lib64\nSetting up Cuda bin\nSetting up Cuda nvvm\nConfiguration finished\nenvy@ub1404envy:~/os_prj/github/tensorflow$ \n", "Build a bleeding edge version of tensorflow.\nThe original sources comes from tensorflow dockerfile and I revised that.\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu]\n\ngit clone --recursive https://github.com/tensorflow/tensorflow.git\ncd tensorflow\n\nsudo ./configure && \\\n    bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package && \\\n    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip\nsudo pip install --upgrade /tmp/pip/tensorflow-*.whl\n", "make sure you run the ./configure script.. double check the paths for cuda and cudnn.. if not sure try system defaults\n", "it should be some PATH issue\n\nI had the 2nd try and it passed. thanks all\n", "I am having the same errors: ERROR: /v/filer4b/v20q001/mhauskn/projects/tensorflow/third_party/gpus/cuda/BUILD:126:1: declared output 'third_party/gpus/cuda/lib64/libcublas.so' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely).\n\nCould you give any more insight into how you solved the problem?\n", "It seems that the configure script ONLY works with the defaults. The error occurs if you change anything. As of release r0.7, the configure script SAYS the default is cuda 7.0, even though the release notes say it uses cuda 7.5. When I tried to tell configure I wanted 7.5, I got the error above. If I let it use the default, it worked and I verified that the libraries in the bazel genfiles cache were actually 7.5 and cudnn 4.0.7. \n", "Thanks! As you suggested I had to not specify the Cuda/CuDNN versions (use system default) and then provide the paths to cuda/cudnn:\n\n./configure \nPlease specify the location of python. [Default is /lusr/bin/python]: \nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /opt/cuda-7.0\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /opt/cuda-7.0]: /u/mhauskn/cudnn-7.0-linux-x64-v4.0-prod\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: \nSetting up Cuda include\nSetting up Cuda lib64\nSetting up Cuda bin\nSetting up Cuda nvvm\nConfiguration finished\n", "I confirm @kbrems, I had the same problem and didn't provided the versions and it worked. \n", "@kbrems Thanks a lot for your solution. I had the same problem fixed now. Hopefully, someone with knowledge of bazel will fix this minor bug soon\n", "1) We need to figure out why the bazel genrules aren't producing the right outputs.  I wonder if the ./configure script is not modifying the CUDA_VERSION string in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/build_config.bzl#L7 ?  \n\n2) @kbrems: where does the ./configure script say 7.0 is the default?\n", "$./configure\nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \n", "@kbrems: that says 'e.g.,', which means 'for example', not the default.\n", "I guess it does not explicitly say Cuda SDK 7.0 IS the default, but that is what the text implies and what many people who run it are assuming, which it why they run into the issue. \n", "I disagree that 'e.g.' means 'default is', but we can certainly change that to be 7.5 :)\n\nI can also reproduce the problem above, it's a dumb error, will fix.\n", "Fixed in 97f6b6fb66cd8e52af4750bd183dfc555d08ef4d\n", "I recently had a very similar issue, because I still had a stale configuration file (`tensorflow/core/platform/default/build_config.bzl`) lying around. Checking it out again from master (essentially reverting all custom changes) fixed the problem.\n"]}, {"number": 1051, "title": " A few bug fixes to pip.sh and ci_parameterized_build.sh", "body": "1) Perform pip install twice, first time with --upgrade and second time without, to fix sporadic test failures related to protobuf version\n2) Correctly determine the local pip install directory based on Python version being used\n3) Always perform \"bazel test\" on non-Docker build environments, such as Mac\n", "comments": ["Can one of the admins verify this patch?\n", "I added some further changes to fix a bug related to ci_build.sh arguments containing double ampersand (&&). \n", "Let me know if there are any further comments. If there is none, please merge it to master and r0.7 branches for the release. \n", "I removed \"bazel clean\" as a compulsory step for non-Docker builds and added \"--genrule_strategy=standalone\" for such builds.  In addition, I added TF_BUILD_BAZEL_CLEAN as an optional environment variable. \n", "Rebased and squashed\n", "merged.\n", "And cherry-picked into r0.7\n"]}, {"number": 1050, "title": "removed signed unsigned comparison warnings for Move in inlined_vector.h", "body": "Changed int to size_t to remove warnings from unsigned int and signed int comparisons.  This is my first pull request for this codebase.  How would I normally test after I make changes?\n", "comments": ["Can one of the admins verify this patch?\n", "Locally, you should run `bazel test ...` to run all the tests, and if you have something affecting GPU, you should additionally run `bazel test -c opt --config=cuda ...`. Or, you can make us do it. Like this.\n\nJenkins, test this please.\n", "Thanks! Merged.\n"]}, {"number": 1049, "title": "How to set the max_grad_norm with AdamOptimizer?", "body": "New to the TensorFlow.\n\nI am working closed with RNN. To avoid overfitting, I am trying to control the max_grad_norm when the gradient passes through.\n\nHere is the code I worked with.\n\n```\n    # Define optimizer with norm limitation\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred_y, y)) # Softmax loss\ntvars = tf.trainable_variables()\ngrads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), max_grad_norm)\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) # Adam Optimizer\noptimizer.apply_gradients(zip(grads, tvars))\n```\n\nI kept receiving the error that\n\nTypeError: Fetch argument <tensorflow.python.training.adam.AdamOptimizer object at 0x7f0f02b1ce50> of <tensorflow.python.training.adam.AdamOptimizer object at 0x7f0f02b1ce50> has invalid type <class 'tensorflow.python.training.adam.AdamOptimizer'>, must be a string or Tensor. (Can not convert a AdamOptimizer into a Tensor or Operation.)\n\nAll suggestions are more than welcome.\nThanks\n", "comments": ["I have figured out by myself.\nOnce we configure the optimizer, it is always better to assign the optimizer to a new op. Then in the session, we need to change the sess.run on the new op. So the code should be\n\n**train_op** = optimizer.apply_gradients(zip(grads, tvars))\n\nAnd in the Session\n\nsess.run(**train_op**, feed_dict={x: batch_xs, y: batch_ys_one_hot, \n                           istate_fw: np.zeros((train_batch_size, 2_n_hidden)), \n                           istate_bw: np.zeros((train_batch_size, 2_n_hidden))})\n\nThanks for your attentions. I learned a lot\n"]}, {"number": 1048, "title": "Fix new typos in tensorflow/core.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Docs only, not testing, merging. Thanks! I am very impressed. You must have a spell checker in your IDE.\n", "Thank you for merging, @martinwicke . I sometime do spell checking during code reviews.\n"]}, {"number": 1047, "title": "TF 0.6.0 build fails with bazel 0.1.5", "body": "Try to build TF ver. 0.6  with Bazel 0.1.5, got this:\n\n`ERROR: /tensorflow/WORKSPACE:70:1: new_git_repository rule //external:iron-ajax'\ns name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:77:1: new_git_repository rule //external:iron-dropd\nown's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:84:1: new_git_repository rule //external:accessibil\nity-developer-tools's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:91:1: new_git_repository rule //external:iron-doc-v\niewer's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:98:1: new_git_repository rule //external:iron-icons\n's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:105:1: new_git_repository rule //external:paper-ico\nn-button's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:119:1: new_git_repository rule //external:paper-dro\npdown-menu's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:126:1: new_git_repository rule //external:iron-flex\n-layout's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:133:1: new_git_repository rule //external:iron-auto\ngrow-textarea's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:147:1: new_git_repository rule //external:iron-comp\nonent-page's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:161:1: new_git_repository rule //external:paper-sty\nles's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:168:1: new_git_repository rule //external:paper-inp\nut's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:175:1: new_git_repository rule //external:paper-ite\nm's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:182:1: new_git_repository rule //external:marked-el\nement's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:196:1: new_git_repository rule //external:paper-pro\ngress's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:203:1: new_git_repository rule //external:iron-chec\nked-element-behavior's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:210:1: new_git_repository rule //external:paper-too\nlbar's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:224:1: new_git_repository rule //external:es6-promi\nse's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:231:1: new_git_repository rule //external:promise-p\nolyfill's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:238:1: new_git_repository rule //external:font-robo\nto's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:245:1: new_git_repository rule //external:paper-men\nu's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:252:1: new_git_repository rule //external:iron-icon\n's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:259:1: new_git_repository rule //external:iron-meta\n's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:273:1: new_git_repository rule //external:iron-resi\nzable-behavior's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:280:1: new_git_repository rule //external:iron-fit-\nbehavior's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:287:1: new_git_repository rule //external:iron-over\nlay-behavior's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:294:1: new_git_repository rule //external:neon-anim\nation's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:301:1: new_git_repository rule //external:iron-a11y\n-keys-behavior's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:322:1: new_git_repository rule //external:iron-vali\ndatable-behavior's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:329:1: new_git_repository rule //external:sinon-cha\ni's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:336:1: new_git_repository rule //external:paper-but\nton's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:343:1: new_git_repository rule //external:iron-inpu\nt's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:350:1: new_git_repository rule //external:iron-menu\n-behavior's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:357:1: new_git_repository rule //external:paper-sli\nder's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:364:1: new_git_repository rule //external:iron-list\n's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:378:1: new_git_repository rule //external:paper-mat\nerial's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:385:1: new_git_repository rule //external:iron-rang\ne-behavior's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:392:1: new_git_repository rule //external:svg-typew\nriter's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:399:1: new_git_repository rule //external:web-anima\ntions-js's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:413:1: new_git_repository rule //external:web-compo\nnent-tester's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:420:1: new_git_repository rule //external:paper-tog\ngle-button's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:427:1: new_git_repository rule //external:paper-beh\naviors's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:434:1: new_git_repository rule //external:paper-rad\nio-group's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:441:1: new_git_repository rule //external:iron-sele\nctor's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:448:1: new_git_repository rule //external:iron-form\n-element-behavior's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:469:1: new_git_repository rule //external:iron-beha\nviors's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:483:1: new_git_repository rule //external:iron-coll\napse's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:490:1: new_git_repository rule //external:paper-che\nckbox's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:497:1: new_git_repository rule //external:paper-rad\nio-button's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:504:1: new_git_repository rule //external:paper-hea\nder-panel's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:511:1: new_git_repository rule //external:prism-ele\nment's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:525:1: new_git_repository rule //external:paper-men\nu-button's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:539:1: new_git_repository rule //external:paper-rip\nple's name field must be a legal workspace name.\nERROR: /tensorflow/WORKSPACE:546:1: new_git_repository rule //external:iron-icon\nset-svg's name field must be a legal workspace name.\nERROR: Error evaluating WORKSPACE file.\nERROR: no such package 'external': Package 'external' contains errors.\n____Elapsed time: 2.171s`\n\nI think should consider [this commit](https://github.com/bazelbuild/bazel/commit/d21c2d6653a3d9bc3376bcb190ba0ac31f52195b)\n", "comments": ["Yes, don't build TF with bazel 0.1.4+ before d21c2d6653a3d9bc3376bcb190ba0ac31f52195b :)\n", "(I can't find the commit right now, but there was a commit a week or so ago that essentially broke backwards compat.)\n", "This one: d6f732205870e87b495e1c01d7b79c6512339021\n", "aw, in Master.. ok =) thnx\n"]}, {"number": 1046, "title": "Graphs tab on TensorBoard failing with a JavaScript error", "body": "Built from TOT today (Feb 10th) with GPU support.  TensorBoard loads and the Events tab works, but when I go to the Graph tab, I see `Uncaught TypeError: Polymer.dom(...).unobserveNodes is not a function`.  \n\nI'm a TensorFlow rookie, so good chance I did something wrong.  \n", "comments": ["@mebersole Can you clarify \"built from TOT\"? Did you built it using `bazel` or using `gulp`? The right way is to run:\n\n```\nbazel build tensorflow/tensorboard\nbazel-bin/tensorflow/tensorboard/tensorboard --logdir path/to/log/dir\n```\n\nIf you do this, these is an unrelated issue #1076 , which makes whole tensorboard not to work, but applying patch https://github.com/tensorflow/tensorflow/commit/68e8b0f1e02f1e0e10f4fdd689ede80f973e2756 should make it work. I did this today and tensorboard is working.\n\nIn case you are building your own front-end (typescript, html), in which case you are running `bower install`, make sure you choose polymer 1.2.4 if bower asks you for which version of polymer. We have a fix that we will push soon, that will stop bower asking you which version of polymer (should be 1.2.4 by default). I actually tried running `bower install` from scratch, and purposely chose polymer 1.1.5 which resulted in the same error (Uncaught TypeError: Polymer.dom(...).unobserveNodes is not a function) that you have.\n\nHope this helps!\n", "@dsmilkov Thanks for the help.  I just cloned 967cd485 to avoid issue #1076 (I worked around it by building and installing a pip package) and built with `bazel build -c opt --config=cuda tensorflow/tensorboard`.  I then ran with `bazel-bin/tensorflow/tensorboard/tensorboard --logdir /home/ubuntu` and the Graph tab is still giving me the `Uncaught TypeError: Polymer.dom(...).unobserveNodes is not a function` error and showing a blank page other than the header bar (title and tab links).  Any suggestions?\n", "I should say I worked around #1076 when I first reported this issue.  I can now run TensorBoard directly with bazel.\n", "Also, I have this set up on a test AWS instance.  If it's easier, I can pass you the login information and you can look at it live.\n", "For what it's worth I'm having exactly the same problem with the latest build.    Building on Ubuntu 14.04.2 and running TensorBoard using bazel.  \n", "@mikehaley did you build with GPU support `-c opt --config=cuda`?  Maybe that will help narrow down the issue.\n", "Yes.  Built with CUDA.   Also updated polymer to 1.2.4 @dsmilkov suggested.\n", "Hi guys,\n\nIf you open the console while your browser is pointed to 0.0.0.0:6006 and type Polymer.version do you get 1.1.5 or 1.2.4 or something else?\n\nIf you get 1.1.5 when you need to update to 1.2.4. To do this, download polymer 1.2.4. Unzip and copy the 3 html files (polymer*.html) into the external polymer directory under site-packages dir: /usr/lib/python2.7/site-packages/external/polymer (overwrite the previous files).\n\nLet me know how it goes. We will also be releasing a patch for 0.7.0 where we ship Polymer 1.2.4 with it.\n", "Going to 1.2.4 of polymer worked for me.\n", "Thanks @dsmilkov.   Forcing polymer to 1.2.4 works.   It was indeed running 1.1.5 before despite the settings in bower.json.\n", "Closed in #1134 \n"]}, {"number": 1045, "title": "tf.train.string_input_producer breaks when num_epochs is set", "body": "I'm using a tf.train.string_input_producer to read in data from a file.  when I set num_epochs=1 instead of None it breaks and I get the following error.  My code is below as well.\n\nAny suggestions for why this is occuring?\n\n``` text\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 8\nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 8\nW tensorflow/core/common_runtime/executor.cc:1076] 0x7fdc4ba6b0f0 Compute status: Out of range: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 10, current size 0)\n     [[Node: shuffle_batch = QueueDequeueMany[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x7fdc48da9170 Compute status: Out of range: FIFOQueue '_0_file_queue' is closed and has insufficient elements (requested 1, current size 0)\n     [[Node: ReaderRead = ReaderRead[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReader, file_queue)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x7fdc4b8dbe20 Compute status: Aborted: Queue '_2_shuffle_batch/random_shuffle_queue' is already closed.\n     [[Node: shuffle_batch/random_shuffle_queue_Close = QueueClose[cancel_pending_enqueues=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue)]]\nTraceback (most recent call last):\n  File \"tensorflow_test.py\", line 51, in <module>\n    coord.join(threads)\n  File \"/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 205, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/queue_runner.py\", line 112, in _run\n    sess.run(enqueue_op)\n  File \"/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 444, in _do_run\n    e.code)\ntensorflow.python.framework.errors.FailedPreconditionError: Attempting to use uninitialized value file_queue/limit_epochs/epochs\n     [[Node: file_queue/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, limit=2, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](file_queue/limit_epochs/epochs)]]\nCaused by op u'file_queue/limit_epochs/CountUpTo', defined at:\n  File \"tensorflow_test.py\", line 6, in <module>\n    filename_queue = tf.train.string_input_producer([\"datasets_top_10_50k/VALIDATION_testset/part-00000..tf\"], num_epochs=2,capacity=10000, name=\"file_queue\")\n  File \"/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 135, in string_input_producer\n    \"fraction_of_%d_full\" % capacity)\n  File \"/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 86, in _input_producer\n    input_tensor = limit_epochs(input_tensor, num_epochs)\n  File \"/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 77, in limit_epochs\n    counter = epochs.count_up_to(num_epochs)\n  File \"/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 414, in count_up_to\n    return state_ops.count_up_to(self._variable, limit=limit)\n  File \"/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 110, in count_up_to\n    return _op_def_lib.apply_op(\"CountUpTo\", ref=ref, limit=limit, name=name)\n  File \"/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n```\n\nCode:\n\n``` python\nimport tensorflow as tf\nimport time\n\nfilename_queue = tf.train.string_input_producer([\"datasets_top_10_50k/VALIDATION_testset/part-00000..tf\"], num_epochs=2,capacity=10000, name=\"file_queue\")\nreader = tf.TFRecordReader()\n_, serialized_example = reader.read(filename_queue)\n\ndata = tf.parse_single_example(\n      serialized_example,\n      dense_keys=[\"features\",\"labels\"],\n      dense_types=[tf.float32, tf.float32],\n      dense_shapes=[(4096),(10)])\n\nfeatures = data[\"features\"]\nlabels = data[\"labels\"]\n\nbatch_size=10\ncapacity=1000\nmin_after_dequeue=100\nexample_batch, label_batch = tf.train.shuffle_batch(\n      [features, labels], \n      batch_size=batch_size, \n      capacity=capacity,\n      min_after_dequeue=min_after_dequeue)\nnum_examples = 0\nwith tf.Session() as sess:\n    # Start populating the filename queue.\n    coord = tf.train.Coordinator()  \n    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n    try:\n        step = 0\n        while not coord.should_stop():\n            start_time = time.time()\n            e, l = sess.run([example_batch, label_batch])\n            print \"grabbing\"\n            e, l = sess.run([example_batch, label_batch])\n            num_examples = num_examples + e.shape[0]\n            print \"num_examples = \" + str(num_examples)\n            duration = time.time() - start_time\n\n    except tf.errors.OutOfRangeError:\n        print('Done training for %d epochs, %d steps.' % (FLAGS.num_epochs, step))\n    finally:\n        # When done, ask the threads to stop.\n        coord.request_stop()\n\n        # Wait for threads to finish.\n        coord.join(threads)\n        sess.close()\n```\n", "comments": ["This doesn't seem to be a bug in TensorFlow (are you running initialize_all_variables?), this kind of question is better for StackOverflow\n", "I have the same issue\n", "I had the same issue. But 'tf.initialize_all_variables()' solved it. This should be mentioned in the documentation or be considered as a bug.\n", "@timmeinhardt Thank you! your comment put me on the right track. However, I think the current [master branch](https://github.com/tensorflow/tensorflow/blob/71f6bb336e5e11d6da2cedac6ba1c992ad9992bd/tensorflow/python/training/input.py#L83) put the created variables into the LOCAL_VARIABLES collection, so i had to initialize with tf.initialize_local_variables() as well.\n", "local_variable can't be initialized by tf.initialize_all_variables()\n", "I would never have been able to solve this problem if I hadn't found this issue. If `tf.initialize_all_variables()` doesn't initialize local variables, then there's either a bug in its implementation, a documentation issue (specifically, the word \"all\"), or both. \n", "Can someone elaborate why this got closed without resolving the confusion around initializing all and/or local variables? This should definitely be changed or documented accordingly!\n\nI would be happy to make a PR with the necessary changes.\n", "Sorry, I shouldn't have closed it -- I mistakenly thought the problem was user forgetting to run `initialize_all_variables`, but the real issue is that a new class of variables was added that's not part of `all_variables`, but still needs to be initialized. I'm not sure there's an easy way to salvage the meaning of `all` in `all_variables`, @vrv may have more context on this\n", "There some some talk of a deprecation of \"all\" so that initialize_all_variables would be replaced with initialize_$something_variables, but the name was never settled on.  It's also going to be hard since nearly every program in TF uses initialize_all_variables().\n\nAssigning to those who have thought about this more.\n", "From the opposite point of view there are probably only very few cases where somebody calls 'initialize_all_variables' and does not want to initialise all variables including the local variables. To ensure backwards compatibility I think 'initialize_all_variables' should be kept and do what its name suggests.\n\nBut I am curious to hear what more experienced tensorflowers might have to say!\n", "From my limited understanding, that's not the case: there was some low level library added that decided that its variables didn't want to be part of \"all\" because they are local to each replica of a model, and would break a ton of models if they were initiailzed along with \"all\".  Hence the dilemma, and why this wasn't already done.  In other words, \"local\" variables are explicitly not part of \"all\".  Unfortunately, the name \"all\" was introduced well before this new use came in, so it's extremely sad and confusing.\n", "In this case, [It seems like moving epoch counter to local variables was a fix for some issues in the tf.contrib.learn library](https://github.com/tensorflow/tensorflow/commit/4c85a95925a68fec324f70cd0d7f3d4548f97a38). I think we can just add notes to the documentation on all input_producers' num_epochs parameter to remind users that they need to initialize with initialize_local_variables() if they want to limit the epochs.[Like the ones in tf.contrib.learn.read_batch_features](https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.learn.html#read_batch_features)\n", "It feels very un-user-friendly to expect users to parse the documentation for a function that they may not even call directly (since it could be buried in a convenient utility function) while still having an `initialize_all_variables()` op that doesn't actually do what its name implies. \n\nOn the other hand, I totally understand why there are times that you would want your variables to be excluded from the global `init_op` and having local variables makes sense in that context. Based on what I've learned here, I believe the problem in this case is that 1) we need to initialize `epochs` but 2) we don't want to restore `epochs` when the model is loaded because we want to be able to run the model! \n\nIs there a middle ground here? Maybe a way of marking a variable as not being restored? The whole issue feels to me like a very heavy-handed way of dancing around the problem.\n", "I thought one should only save the relevant model variables ,like the trainable ones and the moving moments if you use `batch_norm`, while using `tf.train.Saver`. If to avoid restoring epoch while restoring model is the only concern here, I think it would be better to just exclude `epochs` from the variables being saved.\n\nHowever, I don't think this will prevent things from breaking in the distributed setting like @vrv mentioned.\n\nFor the time being, I think it would be really helpful to add the notes for now. (I could still feel the agony and pain from the hours I spent debugging this while violently scratching my head)\n", "Currently, there is still no decision on making it one way or another.\n@sherrym is responsible for finalizing this.\n", "@sherrym any decision on this?", "Just got bit by this issue as well. Thank $DEITY for Google search, the error spew is impenetrable. The fix for me was simply to change the usual `init_op = tf.global_variables_initializer()` to `init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())`, and run it when the Session is up.", "I still have this problem after I run \r\n`tf.group(tf.local_variables_initializer(), tf.global_variables_initializer())`\r\nI use tensorflow r1.0. Anyone could help?\r\nDetailed trace back as follows:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/spyderlib/widgets/externalshell/sitecustomize.py\", line 699, in runfile\r\n>     execfile(filename, namespace)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/spyderlib/widgets/externalshell/sitecustomize.py\", line 81, in execfile\r\n>     builtins.execfile(filename, *where)\r\n>   File \"/media/dl/data2/tf_projects/NUS/eval_mscoco.py\", line 138, in <module>\r\n>     coord.join(threads)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\r\n>     six.reraise(*self._exc_info_to_raise)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\r\n>     sess.run(enqueue_op)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n>     run_metadata_ptr)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n>     feed_dict_string, options, run_metadata)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n>     target_list, options, run_metadata)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n>     raise type(e)(node_def, op, message)\r\n> tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs\r\n> \t [[Node: input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs\"], limit=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs)]]\r\n> \r\n> Caused by op u'input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo', defined at:\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/spyderlib/widgets/externalshell/sitecustomize.py\", line 699, in runfile\r\n>     execfile(filename, namespace)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/spyderlib/widgets/externalshell/sitecustomize.py\", line 81, in execfile\r\n>     builtins.execfile(filename, *where)\r\n>   File \"/media/dl/data2/tf_projects/NUS/eval_mscoco.py\", line 46, in <module>\r\n>     fns, labels = tf.train.slice_input_producer([fns, labels], num_epochs=1, shuffle=False)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 305, in slice_input_producer\r\n>     shared_name=shared_name)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 262, in range_input_producer\r\n>     shared_name, name, \"fraction_of_%d_full\" % capacity)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 156, in input_producer\r\n>     input_tensor = limit_epochs(input_tensor, num_epochs)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 96, in limit_epochs\r\n>     counter = epochs.count_up_to(num_epochs)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 628, in count_up_to\r\n>     return state_ops.count_up_to(self._variable, limit=limit)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 126, in count_up_to\r\n>     result = _op_def_lib.apply_op(\"CountUpTo\", ref=ref, limit=limit, name=name)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n>     op_def=op_def)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n>     original_op=self._default_original_op, op_def=op_def)\r\n>   File \"/home/dl/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n>     self._traceback = _extract_stack()\r\n> \r\n> FailedPreconditionError (see above for traceback): Attempting to use uninitialized value input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs\r\n> \t [[Node: input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs\"], limit=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs)]]\r\n", "@jiqiujia, I previously have the same problem. Later, I found that the local variables need to be initialized before any session starts; sometimes, the starting may be hidden in some other functions. In my case, it was \"tf.train.start_queue_runners()\"; when I put the initialization before that call, it will work well. Otherwise, there will be the same error as in your post. Hope it may help.", "@jiqiujia could you start the queue runners as suggested?\r\n\r\nIt does say `input_producer`.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@jiqiujia I found the same issue, when I used:\r\n ```\r\ninit_op =tf.group(tf.local_variables_initializer(), tf.global_variables_initializer())\r\nsess.run(init_op)\r\n```\r\nbut when I tried:\r\n```\r\nsess.run(tf.local_variables_initializer())\r\nsess.run(tf.global_variables_initializer())\r\n```\r\nit works!\r\n\r\nIs there any bugs about function 'tf.group'? @yaroslavvb\r\n\r\nMy ENV is Python 3.5.3, CPU, tf-1.1.0", "@YMMS the difference is that `tf.group` runs things in arbitrary order, so it can run global variables initializer before local variables initializer", "@yaroslavvb , thanks for your replay.\r\n\r\nBut I have tried:\r\n\r\n```\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(tf.local_variables_initializer())\r\ncoord = tf.train.Coordinator()\r\nthreads = tf.train.start_queue_runners(sess, coord)\r\n```\r\n\r\nIt also works. I think maybe the order makes no difference.\r\n\r\nIn my opinion,\r\n\r\n```\r\ninit_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\r\nsess.run(init_op)\r\ncoord = tf.train.Coordinator()\r\nthreads = tf.train.start_queue_runners(sess, coord)\r\n```\r\n\r\nmaybe failed to initialize the associated local variables of  `tf.train.string_input_producer`.\r\n\r\nJust Guess.", "The order is random, so you may get different results upon reruns. I still think the difference is due to order, the actual ops to be executed should be the same. You can use linearize utility to guarantee that ops run in the same order -- https://github.com/yaroslavvb/stuff/tree/master/linearize", "I'm getting a different error as the original poster, but with the same cause - when I set num_epochs equal in `tf.contrib.slim.parallel_reader.parallel_read` to anything other than `None`, my `FIFOQueues` crash:\r\n\r\n`OutOfRangeError (see above for traceback): FIFOQueue '_1_batch_examples_using_buckets/bucket_by_sequence_length/bucket/top_queue' is closed and has insufficient elements (requested 1, current size 0)`\r\n\r\nDoes this error have the same root cause as the `tensorflow.python.framework.errors_impl.FailedPreconditionError`? I would guess not. If so, does anyone know what might be causing this error?\r\n\r\n", "For anyone else with the same error, adding `sess.run(tf.local_variables_initializer())` after `sess.run(tf.global_variables_initializer())` solved the error.", "Also for some reason, num_epochs=2 doesn't throw error.", "Hi @bcarpenter-pub @yaroslavvb @ysfseu @timmeinhardt @MycChiu \r\nI am trying to test the SketchModelling code,but stuck with error. I have tried adding \r\nsess.run(tf.local_variables_initializer()) \r\nsess.run(tf.global_variables_initializer())  but did not solve the error. Please suggest \r\n(SKetchModelling) C:\\GenAssi\\SketchModeling\\Network\\code\\MonsterNet>python main.py --test --data_dir C:\\GenAssi\\SketchModeling\\zhaoliang_lun_trainingdata\\Chair\\ --train_dir C:\\GenAssi\\SketchModeling\\Checkpoint\\Chair\\ --test_dir C:\\GenAssi\\SketchModeling\\chair_results\\\r\nC:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:455: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:456: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:457: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nstart running...\r\nLoading testing data...\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nBuilding network...\r\nTesting...\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Not found: Can not get size for: C:\\GenAssi\\SketchModeling\\zhaoliang_lun_trainingdata\\Chair\\sketch/03001627/264322794651490ec0d3c02f7e255b2b/sketch-S-0.png : The system cannot find the file specified.\r\n\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Not found: Can not get size for: C:\\GenAssi\\SketchModeling\\zhaoliang_lun_trainingdata\\Chair\\sketch/03001627/264322794651490ec0d3c02f7e255b2b/sketch-F-0.png : The system cannot find the file specified.\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1004, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_1_batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)\r\n         [[Node: batch = QueueDequeueManyV2[component_types=[DT_STRING, DT_FLOAT, DT_FLOAT, DT_BOOL, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/n)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 153, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"main.py\", line 142, in main\r\n    monnet.test(sess, views, num_test_shapes)\r\n  File \"C:\\GenAssi\\SketchModeling\\Network\\code\\MonsterNet\\monnet.py\", line 471, in test\r\n    names,results,errors,images = sess.run([self.names, self.results, self.errors, self.pngs])\r\n  File \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\n**tensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_1_batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)\r\n         [[Node: batch = QueueDequeueManyV2[component_types=[DT_STRING, DT_FLOAT, DT_FLOAT, DT_BOOL, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/n)]]**"]}, {"number": 1044, "title": "Fixing a recently-submitted Python3-breaking line", "body": "iteritems() --> items() please\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 1043, "title": "Alexnet Multi GPU", "body": "I am new to this and would like to run alexnet on multiple GPUs. Can you provide some pointers to that?\n", "comments": ["This would a better question to ask on StackOverflow, github is for bugs / feature requests.\n"]}, {"number": 1042, "title": "Fixed \"filer\" type in variable_scope how to", "body": "Changed \"filer\" to \"filter\"\n", "comments": ["merged.\n"]}, {"number": 1041, "title": "Build pip package from source failed", "body": "I got the following error while building pip package from the master version \n\n`tensorflow/core/BUILD:75:1: Target '//google/protobuf:protobuf_python_genproto' is not visible from target '//tensorflow/core:protos_all_py_genproto'. Check the visibility declaration of the former target if you think the dependency is legitimate.`\n\nBazel version 0.1.4 and 0.1.5\n", "comments": ["@damienmg: any ideas here?  Did something change about visibility targets?\n", "What Bazel version should we use? The site says 0.1.1\n", "If you are building from HEAD, you need to look at the 'master' version of the docs, not 0.6.0.\n\nWhich says:\n\n\"Follow instructions here to install the dependencies for bazel. Then download the latest stable bazel version using the installer for your system and run the installer as mentioned there\"\n\nAnd currently bazel stable is 0.1.5.\n\n(Bazel is evolving very quickly, along with us, so there's some unavoidable churn at the moment).\n", "Successfully built master TensorFlow pip package using 0.1.4 just now.\nUsed this one: https://github.com/bazelbuild/bazel/releases/download/0.1.4/bazel-0.1.4-installer-linux-x86_64.sh\n", "Does anyone have this problem and manage to fix it? I used both bazel-0.1.5-installer-linux-x86_64.sh and bazel-0.1.4-installer-linux-x86_64.sh with no luck. The version of Tensorflow I tried is c4207090d68151fb967672a90ea6d574e62cfba0 (the latest master version at the moment)\n", "What version of protobuf do you have? Did you accidentally move the\nsubmodule ref in the tensorflow repo to something too new or too old?\n\nOn Wed, Feb 10, 2016 at 11:56 PM mondatu notifications@github.com wrote:\n\n> Does anyone have this problem and manage to fix it? I used both\n> bazel-0.1.5-installer-linux-x86_64.sh and\n> bazel-0.1.4-installer-linux-x86_64.sh with no luck. The version of\n> Tensorflow I tried is c420709\n> https://github.com/tensorflow/tensorflow/commit/c4207090d68151fb967672a90ea6d574e62cfba0\n> (the latest master version at the moment)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1041#issuecomment-182755053\n> .\n", "I finally manage to build it with a fresh clone of the repository. Maybe I had an old build artifact that broke the build. My settings : bazel 0.1.5, python 3.4\n", "I think the submodule pointer to protobuf may have been confused. Closing.\n", "@martinwicke \nThis is my protobuf version:\ngit submodule status \n+55ad57a235c009d0414aed1781072adda0c89137 google/protobuf (v3.0.0-alpha-4-179-g55ad57a)\nI didn't recall moving any submodule ref. When I reverted to some previous Tensorflow version (on Feb 8), e.g., df66b9fe049cb17e58454f309b662bdaf0d14fdb, I didn't have any problem with compiling.\n\nI did a fresh clone as  @jrabary did and it works fine now. Thanks, mates!\n"]}, {"number": 1040, "title": "Unimplement: Op TopK is not available in GraphDef version 8.", "body": "Hi !\n\nI've just finish to install TensorFlow on my OSX 10.11.13.\n\nWhen I do: `bazel-bin/tensorflow/examples/label_image`\n\n`E tensorflow/core/common_runtime/executor.cc:275] Executor failed to create kernel. Unimplemented: Op TopK is not available in GraphDef version 8. It has been removed in version 7. Use TopKV2 instead.\n     [[Node: top_k = TopK[T=DT_FLOAT, k=5, sorted=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Const/_0)]]\nE tensorflow/examples/label_image/main.cc:321] Running print failed: Unimplemented: Op TopK is not available in GraphDef version 8. It has been removed in version 7. Use TopKV2 instead.\n     [[Node: top_k = TopK[T=DT_FLOAT, k=5, sorted=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Const/_0)]]`\n\nDid someone already seen this ?\n", "comments": ["@petewarden is hitting the same thing.  Looking at it now.\n", "Diagnosed: we deprecated the C++ `TopK` op in favor of `TopKV2` but that didn't affect the C++ graph builder API.  Fix for label_image coming up, and I'll file a more detailed bug to add a deprecation feature when registering ops.\n", "Fix in review.\n", "Thx, how can I access this review ?\n", "FYI if you just want to get the label_image working locally,reverting the file works for me\ngit checkout 3ede5506acf6a026f09eda33277d46e34ac7ed10 tensorflow/core/kernels/topk_op.cc\n", "Nice thx @nysalad :)\n\nI've got some trouble installing tensorflow for label_images, I'm gonna uninstall all and re-install ;)\n", "@vrv: Will there be a push to github soon?  Looks like the last one was Tuesday.\n", "Probably some time today\n\nOn Thu, Feb 11, 2016 at 8:31 AM, Geoffrey Irving notifications@github.com\nwrote:\n\n> @vrv https://github.com/vrv: Will there be a push to github soon? Looks\n> like the last one was Tuesday.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1040#issuecomment-182944819\n> .\n", "@ziiw: master should work for you now, let me know if not.\n"]}, {"number": 1039, "title": "fixed a typo", "body": "Sorry, I closed a messy PR at https://github.com/tensorflow/tensorflow/pull/987, and reopens here. Thanks.\n", "comments": ["Can one of the admins verify this patch?\n", "Merged, thanks!\n"]}, {"number": 1038, "title": "sequence_loss_by_example()  type error for rnn/ptb", "body": "Hi \n\nI downloaded 0.6.0 for python 2.7 and I tried running the rnn/ptb example but got a type error:\n\n$ python ptb_word_lm.py --data_path=/this/path/ --model small\n\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 8\nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 8\nTraceback (most recent call last):\n  File \"ptb_word_lm.py\", line 303, in <module>\n    tf.app.run()\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"ptb_word_lm.py\", line 280, in main\n    m = PTBModel(is_training=True, config=config)\n  File \"ptb_word_lm.py\", line 135, in __init__\n    [tf.ones([batch_size \\* num_steps])])\nTypeError: sequence_loss_by_example() takes at least 4 arguments (3 given)\n\nIt seems that this sequence_loss_by_example() method is imported from seq2seq which imports it from tensorflow.python.ops.seq2seq. Beyond that I'm not really sure how this happened.\n\nThanks\n", "comments": ["If you installed the 0.6.0 pip, make sure to also clone the repo at the 0.6.0 tag -- if you fetch from master, the pip binary may not currently work with master, since we're still in a rapid development phase.\n", "It's as VIjay says, it works ok on head.\n"]}, {"number": 1037, "title": "bidirectional_rnn return state", "body": "Hi I am wondering why the bi-directional rnn doesn't return a final state? From a code point of view it would be very easy to return. I.e line 301 in rnn.py  \n\n output_fw, _ = rnn(cell_fw, inputs, initial_state_fw, dtype,\n                       sequence_length)\n\nOne could just return _ (named as final_state or whatever) with the concatenated outputs at the end. Is there any theoretical reason why you would never want the final output state? I was planning on feeding the final forward state as the initial state of the forward and backward rnn's on the next pass. The reason I am wanting to do this is the sequences I am working on are much longer than the number of steps I can unroll (seqs are about 15k on average, I'm unrolling 1000 steps) so continuing state is pretty important. It seems weird that the bidirectional rnn would even have the ability to input an initial state but you can't return the output state (what state would you feed in if not the output of the last pass?) \n\nLet me know what you think, i'm no rnn expert. \n\nOff topic but does anyone have any advice for super long sequences? What is the maximum you think I could unroll? Are there other techniques I could use besides long unroll and refeeding the outputs? Can LSTM even learn 1000+ step dependencies? \n", "comments": ["Assigned to @lukaszkaiser because his name is so close to yours.\n", "@lukaszkaiser Any word on this?\n", "@ebrevdo @ludimagister too\n", "Sorry for the late reply. I think it is exactly as you said: the original reason was probably that there are 2 states, so it's not immediately clear whether to return them separately or concatenated or how, and so it was just not done until needed -- but it's clear we need this for long unrolls.\n\nEugene worked a lot recently on dynamic RNNs and it should probably also come to bidirectional RNNs. So I'm re-assinging to Eugene to make sure it's in line with the recent work. But if you have a small PR that just adds states on output from the forward and backward lines, then send it in, of course!\n", "re: long sequences, what lukasz said.  dynamic RNN is coming.\n\nre: outputting the final state.  PRs that return (outputs, final_state_fw, final_state_bw) are welcome.  please add unit tests.\n\nsince bidirectional_rnn is not yet part of the official API (not documented) we can still break the function signature.\n", "@ebrevdo @lukaszkaiser I will make the pull request with (outputs, final_state_fw, final_state_bw) as the return value. I should add the unit tests to rnn_test.py yes?\n", "Yes - I think it suffices if you modify the existing BidirectionalRNNTest there to test for the returned state. Thanks!\n", "@ebrevdo: Do we have dynamic RNNs yet? \n", "Do you need the final state?  The forward or backward?  How would you combine them?\n", "And yes, tf.nn.dynamic_rnn.\n", "See the PR coming in now that has a dynamic_bidirectional_rnn.  That will return the split final states.  We can't modify the output signature of bidirectional_rnn now for backwards compatibility reasons.\n", "Marking \"works as intended\" for now, since the dynamic birnn should solve your problem.\n"]}, {"number": 1036, "title": "Fix all but one Python 3 test", "body": "`saver_test` is still broken.\n", "comments": ["Can one of the admins verify this patch?\n", "This PR contains a lot edits in the same spots as https://github.com/tensorflow/tensorflow/pull/1034 We can look and decide which PR to merge. \n", "It's much better to open the file correctly as binary or not than to decode manually, so this PR seems better.\n", "@girving I agree that this PR is better with one exception: In scatter_ops_test.py, use \n`from six.moves import xrange`\n\nso that we can get both 2-3 compatibility and the performance of xrange. \n", "I'm closing https://github.com/tensorflow/tensorflow/pull/1034 in favor of this one. \n", "The performance difference is negligible for small loops and this is just a test, so I went for simplicity.\n", "Jenkins, test this please.\n", "merged.\n"]}, {"number": 1035, "title": "Python3 test failure: //tensorflow/python:saver_test", "body": "Using the Python-3 configure, run command:\n`bazel test -c opt //tensorflow/python:saver_test`\n\nSee below for the log of the failed test, with the key lines highlighted\n\n........../usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-opt/bin/tensorflow/python/saver_test.runfiles/tensorflow/python/training/saver.py:848: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n  logging.warning(\"Ignoring: %s\", str(e))\nWARNING:tensorflow:Ignoring: [Errno 2] No such file or directory: '/tmp/saver_test/max_to_keep_sharded/s1-00001-of-00002.meta'\nWARNING:tensorflow:Ignoring: [Errno 2] No such file or directory: '/tmp/saver_test/max_to_keep_sharded/s1-00000-of-00002.meta'\n../usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-opt/bin/tensorflow/python/saver_test.runfiles/tensorflow/python/training/saver.py:1088: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n  \"match field type in CollectionDef.\\n%s\" % str(e))\nWARNING:tensorflow:Type is unsupported, or the types of the items don't match field type in CollectionDef.\n**'hello' has type str, but expected one of: bytes**\nWARNING:tensorflow:Type is unsupported, or the types of the items don't match field type in CollectionDef.\nname 'long' is not defined\nWARNING:tensorflow:Type is unsupported, or the types of the items don't match field type in CollectionDef.\nname 'long' is not defined\nWARNING:tensorflow:Type is unsupported, or the types of the items don't match field type in CollectionDef.\nname 'long' is not defined\nWARNING:tensorflow:Type is unsupported, or the types of the items don't match field type in CollectionDef.\n**'queue_name: \"test_queue\"\\n' has type str, but expected one of: bytes**\nF/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-opt/bin/tensorflow/python/saver_test.runfiles/tensorflow/python/training/saver.py:1063: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n  \"serialized. This key has %s\" % type(key))\n**WARNING:tensorflow:Only collections with string type keys will be serialized. This key has <class 'tensorflow.python.training.saver.Saver'>**\nWARNING:tensorflow:Type is unsupported, or the types of the items don't match field type in CollectionDef.\nname 'long' is not defined\n.WARNING:tensorflow:Type is unsupported, or the types of the items don't match field type in CollectionDef.\nname 'long' is not defined\nWARNING:tensorflow:Type is unsupported, or the types of the items don't match field type in CollectionDef.\nname 'long' is not defined\nE.......W tensorflow/core/common_runtime/executor.cc:1094] 0x33a34d0 Compute status: Not found: Tensor name \"v1\" not found in checkpoint files /tmp/saver_test/basics\n         [[Node: save_1/restore_slice_1 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_1/Const_0, save_1/restore_slice_1/tensor_name, save_1/restore_slice_1/shape_and_slice)]]\nW tensorflow/core/common_runtime/executor.cc:1094] 0x33a34d0 Compute status: Not found: Tensor name \"v0\" not found in checkpoint files /tmp/saver_test/basics\n         [[Node: save_1/restore_slice = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_1/Const_0, save_1/restore_slice/tensor_name, save_1/restore_slice/shape_and_slice)]]\n# ...........\n## ERROR: testGraphExtension (**main**.MetaGraphTest)\n\nTraceback (most recent call last):\n  File \"/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-opt/bin/tensorflow/python/saver_test.runfiles/tensorflow/python/training/saver_test.py\", line 968, in testGraphExtension\n    self._testGraphExtensionRestore()\n  File \"/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-opt/bin/tensorflow/python/saver_test.runfiles/tensorflow/python/training/saver_test.py\", line 952, in _testGraphExtensionRestore\n    logits = tf.get_collection(\"logits\")[0]\n**IndexError: list index out of range**\n# \n## FAIL: testAddCollectionDef (**main**.MetaGraphTest)\n\nTraceback (most recent call last):\n  File \"/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-opt/bin/tensorflow/python/saver_test.runfiles/tensorflow/python/training/saver_test.py\", line 760, in testAddCollectionDef\n    self.assertEqual(len(collection_def), 9)\n**AssertionError: 4 != 9**\n", "comments": ["@sherrym: I fixed some of the issues in https://github.com/tensorflow/tensorflow/compare/master...girving:py3?expand=1, but more remain.\n", "With other GPU/Mac/Python3 test failures fixed or soon-to-be-fixed, this test failure is becoming the issue blocking release. \n", "I seem to be swamped today, so can you or @sherrym take a look?\n", "This has been fixed and will be available with the next sync.\n"]}, {"number": 1034, "title": "Fixing more Python-3 test errors", "body": "Some Python3 test errors introduced by recent changes are corrected. \n", "comments": []}, {"number": 1033, "title": "build_pip_package error on Mac+Python3", "body": "See detailed build log at: http://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-mac-python3-copt_pip_install-test/1/consoleFull\n\n> In file included from tensorflow/core/kernels/conv_ops.cc:22:\n> In file included from ./tensorflow/core/framework/numeric_op.h:19:\n> In file included from ./tensorflow/core/framework/op_kernel.h:22:\n> In file included from ./tensorflow/core/framework/allocator.h:25:\n> In file included from ./tensorflow/core/framework/type_traits.h:22:\n> In file included from ./tensorflow/core/framework/types.h:23:\n> In file included from third_party/eigen3/unsupported/Eigen/CXX11/Tensor:2:\n> In file included from external/eigen_archive/eigen-eigen-8cd7c2c6e9e1/unsupported/Eigen/CXX11/Tensor:92:\n> external/eigen_archive/eigen-eigen-8cd7c2c6e9e1/unsupported/Eigen/CXX11/src/Tensor/TensorContractionBlocking.h:29:31: error: no type named 'Scalar' in 'Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long>, 16> > >, Eigen::ThreadPoolDevice>, Eigen::array<long, 1>, Eigen::array<long, 1>, 4, false, false, 0>'\n>   typedef typename RhsMapper::Scalar RhsScalar;\n>           ~~~~~~~~~~~~~~~~~~~~^~~~~~\n\nThis error doesn't occur if Python2 is used on Mac.\n", "comments": ["Probably a swig difference.\n\nOn Tue, Feb 9, 2016 at 11:05 AM caisq notifications@github.com wrote:\n\n> See detailed build log at:\n> http://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-mac-python3-copt_pip_install-test/1/consoleFull\n> \n> In file included from tensorflow/core/kernels/conv_ops.cc:22:\n> In file included from ./tensorflow/core/framework/numeric_op.h:19:\n> In file included from ./tensorflow/core/framework/op_kernel.h:22:\n> In file included from ./tensorflow/core/framework/allocator.h:25:\n> In file included from ./tensorflow/core/framework/type_traits.h:22:\n> In file included from ./tensorflow/core/framework/types.h:23:\n> In file included from third_party/eigen3/unsupported/Eigen/CXX11/Tensor:2:\n> In file included from\n> external/eigen_archive/eigen-eigen-8cd7c2c6e9e1/unsupported/Eigen/CXX11/Tensor:92:\n> external/eigen_archive/eigen-eigen-8cd7c2c6e9e1/unsupported/Eigen/CXX11/src/Tensor/TensorContractionBlocking.h:29:31:\n> error: no type named 'Scalar' in\n> 'Eigen::internal::TensorContractionInputMapper, const\n> Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap, 16> > >,\n> Eigen::ThreadPoolDevice>, Eigen::array, Eigen::array, 4, false, false, 0>'\n> typedef typename RhsMapper::Scalar RhsScalar;\n> \n> ``` ^~~~\n> \n> This error doesn't occur if Python2 is used on Mac.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/1033>.\n> \n> ```\n", "@benoitsteiner have you seen this before? It only happens if you configure for python3, on a Mac, so I'm assuming it's a swig issue, but it's deep inside Eigen, so maybe you have an idea?\n", "@caisq It looks like you're fetching an older version of Eigen. How recently have you sync'ed with the TensorFlow repository? A similar issue has been reported either last week or a couple of weeks ago and fixed since. I wonder if you have the fix in your codebase.\n", "@benoitsteiner Today I ran the build again and this time the compilation error didn't occur: \nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-mac-python3-copt_pip_install-test/3/consoleFull\n\nMy branch has been synced with the TensorFlow master. So not sure what changed. Do you think adding \"bazel clean\" before every build (which I'm not currently doing) will help? \n", "I have noticed that calling bazel clean every time you sync can help. bazel caches a lot of build data internally and it may not always properly react when the WORKSPACE file changes.\n", "Should we report this to bazel? Changing the workspace file is cheating, in\na way. I don't know many build systems that can work with that, but maybe\nbazel should aspire to more?\nOn Wed, Feb 10, 2016 at 17:53 Benoit Steiner notifications@github.com\nwrote:\n\n> I have noticed that calling bazel clean every time you sync can help.\n> bazel caches a lot of build data internally and it may not always properly\n> react when the WORKSPACE file changes.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1033#issuecomment-182670866\n> .\n", "@benoitsteiner @martinwicke @vrv Hmm...  Although the C++ compilation error seems to be gone for now, there is another problem: Many of the Python unit tests fail under Mac+Python3:\n\n```\nbazel clean\nbazel test -c opt //tensorflow/python/...\n```\n\n> Executed 155 out of 155 tests: 109 tests passed and 46 failed locally. \n\nNote that the same build worked perfectly under Mac+Python2.\n\nSimilarly, a lot of the Python unit tests failed during the Python3 pip test-on-install. \n\nFor detailed error logs, see:\nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-mac-python3-copt_pip_install-test/11/console\n\nA lot of them show things like:\n\n> tensorflow.python.pywrap_tensorflow.StatusNotOK: Out of range: FIFOQueue '_20_fifo_queue' is closed and has insufficient elements (requested 1, current size 0)\n>    [[Node: ReaderRead = ReaderRead[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](test_reader, fifo_queue)]]\n> \n> tensorflow.python.pywrap_tensorflow.StatusNotOK: Invalid argument: _ProdGrad NaN test : Tensor had NaN values\n>    [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"_ProdGrad NaN test\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](CheckNumerics/tensor)]]\n> \n> ......W tensorflow/core/common_runtime/executor.cc:1094] 0x7fc314c15bd0 Compute status: Invalid argument: batch_dim == seq_dim == 0\n>    [[Node: ReverseSequence_3 = ReverseSequence[T=DT_FLOAT, batch_dim=0, seq_dim=0, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_Placeholder_6_0, _recv_Placeholder_7_0)]]\n> F.\n\n...\n", "No changes to the repo happened in the period between the last passing test and the first failing test. So it should be in the Jenkins config. Did you change anything there around 4pm EST today?\n", "@martinwicke Sorry I didn't make it clear: The passing build I showed earlier was actually using Python2, not Python3 as it was meant to use.  This is a tools issue that will be fixed in PR https://github.com/tensorflow/tensorflow/pull/1051 \nThis has nothing to do with Jenkins. I did it on the Mac slave without Jenkins or Docker and got the errors. \n\nCan you try it on your Mac with Python3? \n\n```\nbazel clean\nbazel test -c opt //tensorflow/python/...\n```\n", "@martinwicke @benoitsteiner @vrv Further information: I thought this might have to do with the special bazel flags required in Mac. So I tested the following flag combinations for \"bazel test //tensorflow/python/...\"\n\n```\n[no flag]\n-c opt  \n--linkopt=headerpad_max_install_names\n-c opt --linkopt=headerpad_max_install_names\n-c opt --genrule_strategy=standalone --linkopt=headerpad_max_install_names\n```\n\nAll of them failed in the same way, i.e., 46 of the 151 Python unit tests failed. \n", "Testing now\n\nOn Thu, Feb 11, 2016 at 8:10 AM caisq notifications@github.com wrote:\n\n> @martinwicke https://github.com/martinwicke @benoitsteiner\n> https://github.com/benoitsteiner @vrv https://github.com/vrv Further\n> information: I thought this might have to do with the special bazel flags\n> required in Mac. So I tested the following flag combinations for \"bazel\n> test //tensorflow/python/...\"\n> \n> -c opt\n> --linkopt=headerpad_max_install_names\n> -c opt --linkopt=headerpad_max_install_names\n> -c opt --genrule_strategy=standalone --linkopt=headerpad_max_install_names\n> \n> All of them failed in the same way, i.e., 46 of the 151 Python unit tests\n> failed.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1033#issuecomment-182937374\n> .\n", "It seems to be a problem with exceptions. All the errors I see are of the form:\n\n```\nFAIL: testBadDefault (__main__.SparseToDenseTest)\n----------------------------------------------------------------------\ntensorflow.python.pywrap_tensorflow.StatusNotOK: Invalid argument: default_value\n should be a scalar.\n         [[Node: SparseToDense = SparseToDense[T=DT_INT32, Tindices=DT_INT32, va\nlidate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](SparseToDe\nnse/sparse_indices, SparseToDense/output_shape, SparseToDense/sparse_values, SparseToDense/default_value)]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/private/var/tmp/_bazel_wicke/83710fbf544c1dff5a44a1c6171bc2ec/tensorflow/bazel-out/local_darwin-py3-opt/bin/tensorflow/python/sparse_to_dense_op_py_test.runfiles/tensorflow/python/framework/test_util.py\", line 497, in assertRaisesWithPredicateMatch\n    yield\n  File \"/private/var/tmp/_bazel_wicke/83710fbf544c1dff5a44a1c6171bc2ec/tensorflow/bazel-out/local_darwin-py3-opt/bin/tensorflow/python/sparse_to_dense_op_py_test.runfiles/tensorflow/python/kernel_tests/sparse_to_dense_op_py_test.py\", line 112, in testBadDefault\n    dense.eval()\n  File \"/private/var/tmp/_bazel_wicke/83710fbf544c1dff5a44a1c6171bc2ec/tensorflow/bazel-out/local_darwin-py3-opt/bin/tensorflow/python/sparse_to_dense_op_py_test.runfiles/tensorflow/python/framework/ops.py\", line 465, in eval\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\nSystemError: <built-in function delete_Status> returned a result with an error set\n```\n\nThat final system error is the problem. I think these are the tests that have at least one test method which relies on an exception somewhere.\n", "Duplicate of #830, closing this one.\n"]}, {"number": 1032, "title": "Compilation Error on Travis.ci", "body": "Hi,\n\nI'm getting a gcc error when trying to build on a docker image on travis.ci. Any ideas would be appreciated.\n\nFull Error:\n\n```\nERROR: /home/travis/build/peterbraden/node-tensorflow/lib/tensorflow/google/protobuf/BUILD:64:1: C++ compilation of rule '//google/protobuf:protobuf' failed: gcc failed: error executing command \n  (cd /home/travis/.cache/bazel/_bazel_travis/2cf95f6ff0f7dc500e0eb5aab5889c29/tensorflow && \\\n  exec env - \\\n    PATH=/home/travis/.nvm/versions/node/v4.1.2/lib/node_modules/npm/bin/node-gyp-bin:/home/travis/build/peterbraden/node-tensorflow/node_modules/.bin:/home/travis/.nvm/versions/node/v4.1.2/bin:/home/travis/bin:/home/travis/.local/bin:/home/travis/.gimme/versions/go1.4.1.linux.amd64/bin:/usr/local/phantomjs/bin:./node_modules/.bin:/usr/local/maven-3.2.5/bin:/usr/local/clang-3.4/bin:/home/travis/.gimme/versions/go1.4.1.linux.amd64/bin:/usr/local/phantomjs/bin:./node_modules/.bin:/usr/local/maven-3.2.5/bin:/usr/local/clang-3.4/bin:/home/travis/.gimme/versions/go1.4.1.linux.amd64/bin:/home/travis/.rvm/gems/ruby-1.9.3-p551/bin:/home/travis/.rvm/gems/ruby-1.9.3-p551@global/bin:/home/travis/.rvm/rubies/ruby-1.9.3-p551/bin:/usr/local/phantomjs/bin:./node_modules/.bin:/usr/local/maven-3.2.5/bin:/usr/local/clang-3.4/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/travis/.rvm/bin:/home/travis/.rvm/bin:/home/travis/.rvm/bin \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer '-std=c++0x' -iquote . -iquote bazel-out/local_linux-fastbuild/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-fastbuild/genfiles/external/bazel_tools -isystem google/protobuf/src -isystem bazel-out/local_linux-fastbuild/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-fastbuild/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/util/type_resolver_util.pic.o' -MD -MF bazel-out/local_linux-fastbuild/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/util/type_resolver_util.pic.d -fPIC -c google/protobuf/src/google/protobuf/util/type_resolver_util.cc -o bazel-out/local_linux-fastbuild/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/util/type_resolver_util.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: gcc failed: error executing command \n  (cd /home/travis/.cache/bazel/_bazel_travis/2cf95f6ff0f7dc500e0eb5aab5889c29/tensorflow && \\\n  exec env - \\\n    PATH=/home/travis/.nvm/versions/node/v4.1.2/lib/node_modules/npm/bin/node-gyp-bin:/home/travis/build/peterbraden/node-tensorflow/node_modules/.bin:/home/travis/.nvm/versions/node/v4.1.2/bin:/home/travis/bin:/home/travis/.local/bin:/home/travis/.gimme/versions/go1.4.1.linux.amd64/bin:/usr/local/phantomjs/bin:./node_modules/.bin:/usr/local/maven-3.2.5/bin:/usr/local/clang-3.4/bin:/home/travis/.gimme/versions/go1.4.1.linux.amd64/bin:/usr/local/phantomjs/bin:./node_modules/.bin:/usr/local/maven-3.2.5/bin:/usr/local/clang-3.4/bin:/home/travis/.gimme/versions/go1.4.1.linux.amd64/bin:/home/travis/.rvm/gems/ruby-1.9.3-p551/bin:/home/travis/.rvm/gems/ruby-1.9.3-p551@global/bin:/home/travis/.rvm/rubies/ruby-1.9.3-p551/bin:/usr/local/phantomjs/bin:./node_modules/.bin:/usr/local/maven-3.2.5/bin:/usr/local/clang-3.4/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/travis/.rvm/bin:/home/travis/.rvm/bin:/home/travis/.rvm/bin \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer '-std=c++0x' -iquote . -iquote bazel-out/local_linux-fastbuild/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-fastbuild/genfiles/external/bazel_tools -isystem google/protobuf/src -isystem bazel-out/local_linux-fastbuild/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-fastbuild/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/util/type_resolver_util.pic.o' -MD -MF bazel-out/local_linux-fastbuild/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/util/type_resolver_util.pic.d -fPIC -c google/protobuf/src/google/protobuf/util/type_resolver_util.cc -o bazel-out/local_linux-fastbuild/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/util/type_resolver_util.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n```\n\nAnd the offending build: \n\nhttps://travis-ci.org/peterbraden/node-tensorflow/builds/108069558\n", "comments": ["It appears this was due to an outdated compiler. Closing for now.\n"]}, {"number": 1031, "title": "Improvements to print_build_info.sh and pip.sh", "body": "This is in response to @jendap's earlier comments. Using the $() subscript syntax.\n\nAlso, making --upgrade default option for pip\n\nThis is addressing the issue that if --upgrade is not used, preexisting installation of TensorFlow PIP on the system will make the install-test results obsolete (e.g., on Mac, where we don't use Docker to run the tests)\n", "comments": ["Can one of the admins verify this patch?\n", "merged.\n"]}, {"number": 1030, "title": "problem with rnn comments regarding dynamic seq length?", "body": "I found this in the rnn comments:\n\n```\n  Dynamic calculation returns, at time t:\n    (t >= max(sequence_length)\n        ? (zeros(output_shape), zeros(state_shape))\n        : cell(input, state)\n```\n\nIs this correct? It seems unreasonable to throw away the final state by overwriting it with 0s after it ran over the maximum sequence length. From reading the later code section for _dynamic_rnn_step there's this comment:\n\n```\n  if t >= max_sequence_length:\n    return (zero_output, state)\n  if t < min_sequence_length:\n    return call_cell()\n```\n\nthat seems to suggest that the final state is actually the last state before we ran over the maximum sequence length, and only the output is zero-ed out, which would make more sense.\n\nI'm not sure which one is actually being done in the implementation, but these 2 views are not consistent.\n", "comments": ["see #1016\n"]}]