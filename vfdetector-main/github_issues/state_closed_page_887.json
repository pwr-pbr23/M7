[{"number": 26874, "title": "Compilation issue: 'string' was not declared in this scope", "body": "```\r\ntensorflow/lite/testing/kernel_test/generate_diff_report.cc:22:3: error: 'string' was not declared in this scope\r\n   string base, test, output;\r\n   ^~~~~~\r\n```\r\n\r\nThe above compilation error is fixed.", "comments": ["@karimnosseir added #include also. Thanks"]}, {"number": 26872, "title": "Keras test cases added in losses_test", "body": "", "comments": []}, {"number": 26871, "title": "Added Average Filtering with 2D filter", "body": "Average Filtering with 1D filter was added by me at #26381 \r\nThis is 2D filtering\r\nTest Code-\r\n\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot\r\nimport matplotlib.pyplot as plt\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.util.tf_export import tf_export\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import script_ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import image_ops_impl\r\nfname = 'index.jpeg'\r\nimg = matplotlib.pyplot.imread(fname)\r\nimport numpy as np\r\n\r\ntf_img = tf.convert_to_tensor(img)\r\n\r\n\r\n@tf_export('image.average_filter_2D')\r\ndef average_filter_2D(input,filter_shape=(3,3)):\r\n    \"\"\"  This methods takes 3D Tensor Images.\r\n         Other than Tensor it takes optional parameter filter_Size\r\n         Default Filter Shape = (3 , 3)\r\n         This Median Filtering is done by using 2D filters of user's choice\r\n         Filter_size should be odd\r\n         This method takes both kind of images where pixel values lie between 0 to 255 and where it lies between 0.0 and 1.0\r\n    \"\"\"\r\n\r\n    input = image_ops_impl._Assert3DImage(input)\r\n    m,no,ch = int(input.shape[0]),int(input.shape[1]),int(input.shape[2])\r\n    filter_shapex = filter_shape[0]\r\n    filter_shapey = filter_shape[1]\r\n    if m < filter_shapex or no < filter_shapey:\r\n        raise ValueError(\"No of Pixels in each dimension of the image should be more than the filter size. Got filter_shape \"\r\n                         \"(%sx\" % filter_shape[0]+\"%s).\"%filter_shape[1] +\" Image Shape (%s)\"% input.shape)\r\n    if filter_shapex % 2 == 0 or filter_shapey % 2 == 0:\r\n        raise ValueError(\"Filter size should be odd. Got filter_shape (%sx\" % filter_shape[0]+\"%s)\"%filter_shape[1] )\r\n    input = math_ops.cast(input,dtypes.float64)\r\n    def my_func (input2):\r\n        tf_i = input2.reshape(m*no*ch)\r\n        maxi = max(tf_i)\r\n        if maxi == 1:\r\n            input2 /= maxi\r\n        else :\r\n            input2 /= 255\r\n        #k and l is the Zero-padding size\r\n        res = np.empty((m,no,ch))\r\n        for a in range(ch):\r\n            img = input2[:,:,a:a+1]\r\n            img = img.reshape(m,no)\r\n            k = filter_shapex - 1\r\n            l = filter_shapey - 1\r\n            img  = np.pad(img,((k / 2, k / 2), (l / 2,l / 2)),'constant', constant_values=(0, 0))\r\n            res1 = np.empty((m,no))\r\n            for i in range(img.shape[0] - k) :\r\n                for j in range(img.shape[1] - l) :\r\n                    li = []\r\n                    for b in range(i, i + filter_shapex):\r\n                        for d in range(j, j + filter_shapey):\r\n                            li.append(img[b][d])\r\n                    res1[i][j] = sum(li) / len(li)\r\n            res1 = res1.reshape(m,no,1)\r\n            res[:,:,a:a+1] = res1\r\n        res *= 255\r\n        res = res.astype('int64')\r\n        return res\r\n\r\n    y = script_ops.py_func(my_func, [input], dtypes.int64)\r\n    return y\r\n\r\nsess = tf.InteractiveSession()\r\n\r\nmimage = average_filter_2D(tf_img,(5,5))\r\n\r\nfig = plt.figure()\r\nfig.add_subplot()\r\nplt.imshow(img,cmap='gray')\r\nplt.show()\r\nmimage = mimage.eval()\r\nfig.add_subplot()\r\nif mimage.shape[2] == 1:\r\n    mimage = mimage.reshape(mimage.shape[0],mimage.shape[1])\r\nplt.imshow(mimage,cmap = 'gray')\r\nplt.show()", "comments": ["Moved to tensorflow/ addons at pullrequest 153"]}, {"number": 26870, "title": "ImportError: DLL load failed: The specified module could not be found.  Failed to load the native TensorFlow runtime.", "body": "System information:\r\nHave I written custom code: No\r\nOS Platform and Distribution: Windows 10 Home\r\nMobile device: None\r\nTensorFlow installed from: binary with pip\r\nTensorFlow version: 1.13.1\r\nPython Version: 3.6.7\r\nBazel version: not installed\r\nCUDA/cuDNN version: CUDA 10.1, cuDNN 7.5.0.56\r\nGPU model and memory: GeForce GTX 1050 TCC/WDDM\r\nNVIDIA Driver version: 419.35\r\nTensorRT version: 5.0.4.3\r\n\r\nExact command to reproduce:\r\npip install --force-reinstall tensorflow-gpu\r\npython\r\nimport tensorflow as tf\r\n\r\nProblem:\r\nI got this error when importing tensorflow. I have checked the environment variables. I have installed Visual Studio Community 2017 and NVIDIA CUDA Visual Studio Integration 10.1.\r\nI have read and tried to follow the solutions from other similar issue (#22794 and #22872), but have not succeeded in fixing the problem.\r\n\r\nLog:\r\nC:\\Users\\Me>python\r\nPython 3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nFailed to load the native TensorFlow runtime.", "comments": ["@andmek I also run into the same problem. From the official installation page of tenserflow, the CUDA version supported is 10.0, for now 10.1 will give you issues until that is fixed.\r\nSo you have to downgrade from 10.1 and follow the setup of the CUDA/cuDNN by updating setting the system path.\r\n", "@andmek The same issue is on this thread: https://github.com/tensorflow/tensorflow/issues/26364 ", "Closing since its a duplicate of #26364 . Feel free to reopen if the solution provided on that thread doesn't work for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26870\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26870\">No</a>\n", "if you have the following configs:\r\n`cuda `and `cudnn `10.0\r\n`tensorflow-gpu`: 2.1\r\n`os` : windows 10\r\njust downgrade to `tensorflow` 2.0.0\r\nthis worked for me!\r\np.s : i think `tensorflow` has high range of sensitivity when it comes to version of the package which is pretty weird and annoying. ", "If you get this working by downgrading to 2.0.0 then you can use 2.1.0 by downloading the new MSVC redistributable from Microsoft", "there is no solution\r\ntensorflow and its errors causing that I loss my PhD\r\nfuck", "@DrKhalid75 there is a solution, simple search through the duplicates of this (since everyone encountering this just creates a duplicate) would have shown you that there are 4 likely causes of this error:\r\n\r\n1. You need to install the MSVC 2019 redistributable. This is exactly the comment above your comment\r\n2. Your CPU does not support AVX2 instructions. Here, you can build from source.\r\n3. Your CPU/Python is on 32 bits. Here, unfortunately there is no solution.\r\n4. There is a library that is in a different location/not installed on your system that cannot be loaded. Here, you have to find out which library is not found/does not exist using Windows SysInternal tools or similar and then create a new separate issue.\r\n\r\nIn any case, even if TF does not work on your machine, you can use it on Colab. There is no need to claim that a phd gets lost because of a software issue.", "1. I downloaded MSVC 2019 redistributable and no way\r\n2. My CPU is Intel (R) Core (TM) i5 CPU   M450 @ 2.4 GHz\r\n3. My CPU / Python 64 bit , Python 2.1\r\n4. I formatted my PC and reinstalled it and no way\r\n.\r\nThe problem here I'm in Arabian university, the rule is you choose your point of research and work with yourself, solve any problems with yourself, we haven't any support for you, we want publications only, If you meet funding problems or hardware problems it is not our work, and it is forbidden to change your point of research so you must lost your PhD ", "The CPU does not support AVX. Please try compiling from source.\r\n\r\nIf that still does not work, you can always use [Google Colab](https://colab.research.google.com) It is guaranteed that TensorFlow works there"]}, {"number": 26869, "title": "Converting to TF Lite with tf.lite.OpsSet.SELECT_TF_OPS is bounded", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nv1.13.1-0-g6612da8951\r\n- Python version:\r\n3.5.6 (Anaconda)\r\n- CUDA/cuDNN version:\r\nCPU version\r\n\r\nI am trying to export a very simple model (see code below) into TensorFlow Lite with loss function [1] and gradients [2] calculation support using the official tutorial [3].\r\n\r\n**Describe the current behavior**\r\nThe model can be converted only when global step variable and custom cross entropy implementation are used (but it cannot be run on Android). Without one of them or both the converting process crashes.\r\n\r\n**Describe the expected behavior**\r\nThe model should be converted with standard cross entropy implementation and without explicitly defined global step variable.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nuse_custom_crossentropy = True\r\nuse_global_step = True\r\n\r\n\r\ninputs = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\r\nlabels = np.array([[0.0], [1.0], [1.0], [0.0]])\r\n\r\nglobal_step = tf.Variable(0, trainable=False, name='global_step')\r\nx = tf.placeholder(tf.float32, shape=(None, 2), name='Input')\r\ny = tf.placeholder(tf.float32, shape=(None, 1), name='Output')\r\n\r\nwith tf.name_scope('layer1'):\r\n    w1 = tf.Variable(tf.random_uniform([2, 2], -1., 1.), name='w1')\r\n    b1 = tf.Variable(tf.zeros([2]), name='b1')\r\n    l1 = tf.add(tf.matmul(x, w1), b1, name='l1')\r\n    l1 = tf.nn.sigmoid(l1)\r\n\r\nwith tf.name_scope('layer2'):\r\n    w2 = tf.Variable(tf.random_uniform([2, 1], -1., 1.), name='w2')\r\n    b2 = tf.Variable(tf.zeros([1]), name='b2')\r\n    model = tf.add(tf.matmul(l1, w2), b2, name='model')\r\n\r\nfor variable in tf.trainable_variables():\r\n    var_name = variable.name.split(':')[0].replace('/', '_')\r\n    var_value_ph = tf.placeholder(tf.float32, shape=variable.shape, name='{}_ph'.format(var_name))\r\n    var_value_assign = tf.assign(variable, var_value_ph, name='{}_assign'.format(var_name))\r\n    print(var_name, var_value_ph.name, var_value_assign.name)\r\n\r\nif use_custom_crossentropy:\r\n    cost = tf.nn.sigmoid(model)\r\n    cost = tf.maximum(cost, 0.0) - cost * y + tf.log(1 + tf.exp(-tf.abs(cost)))\r\n    cost = tf.reduce_mean(cost, name='cost')\r\nelse:\r\n    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=model),\r\n                          name='cost')\r\noptimizer = tf.train.AdamOptimizer(name='optimizer')\r\nif use_global_step:\r\n    train_op = optimizer.minimize(cost, global_step=global_step)\r\nelse:\r\n    train_op = optimizer.minimize(cost)\r\n\r\ninit = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init)\r\n\r\nfor step in range(10000):\r\n    sess.run(train_op, feed_dict={x: inputs, y: labels})\r\nprint(sess.run(model, feed_dict={x: inputs}))\r\n\r\nconverter = tf.lite.TFLiteConverter.from_session(sess,\r\n                                                 [x, y], [model, cost, train_op])\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                        tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nwith open('xor_old.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nuse_custom_crossentropy = True\r\nuse_global_step = True\r\n```\r\n```\r\nWARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nlayer1_w1 layer1_w1_ph:0 layer1_w1_assign:0\r\nlayer1_b1 layer1_b1_ph:0 layer1_b1_assign:0\r\nlayer2_w2 layer2_w2_ph:0 layer2_w2_assign:0\r\nlayer2_b2 layer2_b2_ph:0 layer2_b2_assign:0\r\n2019-03-19 12:09:59.552926: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-03-19 12:09:59.586599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz\r\n2019-03-19 12:09:59.587967: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5601b1c8d640 executing computations on platform Host. Devices:\r\n2019-03-19 12:09:59.587988: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n[[-6.7735147]\r\n [-7.4516478]\r\n [ 4.8298645]\r\n [-6.928589 ]]\r\nWARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.convert_variables_to_constants\r\nWARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.extract_sub_graph\r\n```\r\n```\r\nuse_custom_crossentropy = False\r\nuse_global_step = True\r\n```\r\n```\r\nWARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprec\r\nated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nlayer1_w1 layer1_w1_ph:0 layer1_w1_assign:0\r\nlayer1_b1 layer1_b1_ph:0 layer1_b1_assign:0\r\nlayer2_w2 layer2_w2_ph:0 layer2_w2_assign:0\r\nlayer2_b2 layer2_b2_ph:0 layer2_b2_assign:0\r\n2019-03-19 12:10:47.189917: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-03-19 12:10:47.214732: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz\r\n2019-03-19 12:10:47.215498: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x560ffe1fcd90 executing computations on platform Host. Devices:\r\n2019-03-19 12:10:47.215538: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n[[-3.9300842]\r\n [ 3.913724 ]\r\n [ 3.9203634]\r\n [-3.8768034]]\r\nWARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_i\r\nmpl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.convert_variables_to_constants\r\nWARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_uti\r\nl_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.extract_sub_graph\r\nTraceback (most recent call last):\r\n  File \"./training_old_version.py\", line 58, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/lite.py\", line 455, in convert\r\n    **converter_kwargs)\r\n  File \"$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/convert.py\", line 442, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/convert.py\", line 205, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-03-19 12:10:51.425153: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Log1p\r\n2019-03-19 12:10:51.434088: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs\r\n2019-03-19 12:10:51.434177: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs\r\n2019-03-19 12:10:51.434246: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Reciprocal\r\n2019-03-19 12:10:51.434268: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs\r\n2019-03-19 12:10:51.434696: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs\r\n2019-03-19 12:10:51.434748: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: SigmoidGrad\r\n2019-03-19 12:10:51.434761: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs\r\n2019-03-19 12:10:51.434834: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ApplyAdam\r\n2019-03-19 12:10:51.434848: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ApplyAdam\r\n2019-03-19 12:10:51.434860: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ApplyAdam\r\n2019-03-19 12:10:51.434871: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ApplyAdam\r\n2019-03-19 12:10:51.434884: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Assign\r\n2019-03-19 12:10:51.434902: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Assign\r\n2019-03-19 12:10:51.434918: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: AssignAdd\r\n2019-03-19 12:10:51.436899: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 110 operators, 148 arrays (0 quantized)\r\n2019-03-19 12:10:51.437594: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 21 operators, 30 arrays (0 quantized)\r\n2019-03-19 12:10:51.438536: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 21 operators, 30 arrays (0 quantized)\r\n2019-03-19 12:10:51.438758: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 15 operators, 24 arrays (0 quantized)\r\n2019-03-19 12:10:51.438853: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 15 operators, 24 arrays (0 quantized)\r\n2019-03-19 12:10:51.438930: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 256 bytes, theoretical optimal value: 192 bytes.\r\n2019-03-19 12:10:51.439212: W tensorflow/lite/toco/tflite/operator.cc:1768] Op Log1p is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-19 12:10:51.439273: W tensorflow/lite/toco/tflite/operator.cc:1768] Op Log1p is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-19 12:10:51.439319: E tensorflow/lite/toco/toco_tooling.cc:421] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of h\r\now this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error w\r\nith --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, EXP, FULLY_CONNECTED, GREATER_EQUAL, LOGISTIC, M\r\nEAN, MUL, NEG, SELECT, SUB, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: Log1p.\r\nTraceback (most recent call last):\r\n  File \"$HOME/miniconda3/envs/tf-cpu/bin/toco_from_protos\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://\r\ngithub.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error w\r\nith --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, EXP, FULLY_CONNECTED, GREATER_EQUAL, LOGISTIC, M\r\nEAN, MUL, NEG, SELECT, SUB, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: Log1p.\r\n```\r\n```\r\nuse_custom_crossentropy = True\r\nuse_global_step = False\r\n```\r\n```\r\nWARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprec\r\nated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nlayer1_w1 layer1_w1_ph:0 layer1_w1_assign:0\r\nlayer1_b1 layer1_b1_ph:0 layer1_b1_assign:0\r\nlayer2_w2 layer2_w2_ph:0 layer2_w2_assign:0\r\nlayer2_b2 layer2_b2_ph:0 layer2_b2_assign:0\r\n2019-03-19 12:13:04.851757: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-03-19 12:13:04.874754: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz\r\n2019-03-19 12:13:04.875673: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55832b67a430 executing computations on platform Host. Devices:\r\n2019-03-19 12:13:04.875706: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n[[-6.0365562]\r\n [-6.613834 ]\r\n [ 5.176443 ]\r\n [-6.16683  ]]\r\nWARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_i\r\nmpl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.convert_variables_to_constants\r\nWARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_uti\r\nl_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.extract_sub_graph\r\nTraceback (most recent call last):\r\n  File \"./training_old_version.py\", line 58, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/lite.py\", line 455, in convert\r\n    **converter_kwargs)\r\n  File \"$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/convert.py\", line 442, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/convert.py\", line 205, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-03-19 12:13:09.381975: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs\r\n2019-03-19 12:13:09.391489: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs\r\n2019-03-19 12:13:09.391555: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Reciprocal\r\n2019-03-19 12:13:09.391587: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs\r\n2019-03-19 12:13:09.391865: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs\r\n2019-03-19 12:13:09.391898: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs\r\n2019-03-19 12:13:09.391924: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Sign\r\n2019-03-19 12:13:09.391942: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: SigmoidGrad\r\n2019-03-19 12:13:09.391957: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs\r\n2019-03-19 12:13:09.391988: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: SigmoidGrad\r\n2019-03-19 12:13:09.392002: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs\r\n2019-03-19 12:13:09.392080: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ApplyAdam\r\n2019-03-19 12:13:09.392096: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ApplyAdam\r\n2019-03-19 12:13:09.392108: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ApplyAdam\r\n2019-03-19 12:13:09.392120: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ApplyAdam\r\n2019-03-19 12:13:09.392134: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Assign\r\n2019-03-19 12:13:09.392148: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Assign\r\n2019-03-19 12:13:09.393103: F tensorflow/lite/toco/tooling_util.cc:905] Check failed: GetOpWithOutput(model, output_array) Specified output array \"optimizer\" is not produced by any op in this graph. Is it\r\n a typo? To silence this message, pass this flag:  allow_nonexistent_arrays\r\nAborted (core dumped)\r\n```\r\nBelow presented a log from Android for the successfully converted model.\r\n```\r\n2019-03-19 11:24:38.739 32452-32452/com.example.tflite E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.tflite, PID: 32452\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.tflite/com.example.tflite.MainActivity}: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: cannot compute AssignAdd as input #0(zero-based) was expected to be a int32_ref tensor but is a int32 tensor\r\n    \t (while executing 'AssignAdd' via Eager)Node number 15 (DELEGATE) failed to invoke.\r\n    \r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3086)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3229)\r\n        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:78)\r\n        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:108)\r\n        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:68)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1926)\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n        at android.os.Looper.loop(Looper.java:213)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6981)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1445)\r\n     Caused by: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: cannot compute AssignAdd as input #0(zero-based) was expected to be a int32_ref tensor but is a int32 tensor\r\n    \t (while executing 'AssignAdd' via Eager)Node number 15 (DELEGATE) failed to invoke.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:149)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n        at com.example.tflite.MainActivity.onCreate(MainActivity.java:53)\r\n        at android.app.Activity.performCreate(Activity.java:7326)\r\n        at android.app.Activity.performCreate(Activity.java:7317)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1271)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3066)\r\n        \t... 11 more\r\n```\r\n\r\n[1] https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/lite/toco/tflite/whitelisted_flex_ops.cc#L333-L334\r\n[2] https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/lite/toco/tflite/whitelisted_flex_ops.cc#L35\r\n[3] https://www.tensorflow.org/lite/guide/ops_select#converting_the_model", "comments": ["When you use the non-custom loss implementation, internally TF will call the log1p operator[1], which is not supported by TF Lite (it's neither built-in nor select). So this will crash the conversion process.\r\n\r\nI'm wondering if you could strip loss/train_op from the model and then convert it to TF Lite. TF Lite models can't be used for training, since it doesn't support gradient ops or variable updates.\r\n\r\n[1]https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/ops/nn_impl.py#L183", "If loss/train_op is removed from output ops list, converting works without problem.\r\n\r\nBut I kept in mind the following.\r\n\r\nThe difference between the operations set of this simple graph (it includes training and assigning operations) and the set of allowed select operations is an empty set [1]. So if I recompile TF Lite for Android with select operations support [2] it will be possible to train model with TF Lite.\r\n\r\n[1] https://colab.research.google.com/drive/1rpKNTPZ4z914PNqGzXlH3pnUDQnr5pVe\r\n[2] https://www.tensorflow.org/lite/guide/ops_select#android_aar", "Training in isn't supported in TensorFlow Lite. \r\nWe're looking into supporting Training. Stay tuned. ", "@haozha111, is it a not a good idea just to add `Log1p` operation into allowed Flex operations list?\r\nIt seems that the following modification works:\r\n```\r\n--- a/tensorflow/lite/toco/tflite/whitelisted_flex_ops.cc\r\n+++ b/tensorflow/lite/toco/tflite/whitelisted_flex_ops.cc\r\n@@ -170,6 +170,7 @@ bool IsWhitelistedFlexOp(const std::string& tensorflow_op_name) {\r\n           \"ListDiff\",\r\n           \"_ListToArray\",\r\n           \"Log\",\r\n+          \"Log1p\",\r\n           \"LogicalAnd\",\r\n           \"LogicalNot\",\r\n           \"LogicalOr\",\r\n```", "The conversion will probably succeed if you add that op into the whitelisted flex ops. However, I doubt it will work as expected if you want to train it on device. Because TF Lite graph can only be used for inference at the moment. ", "Yes, conversion finished successfully, but after recompiling `*.aar` with `bazel build --cxxopt='--std=c++11' -c opt --config=android_arm --config=monolithic //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops` I've got the error \r\n```\r\n Caused by: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Could not find valid device for node.\r\nNode: {{node Log1p}}\r\nAll kernels registered for op Log1p :\r\n  <no registered kernels>\r\n\r\n         (while executing 'Log1p' via Eager)Node number 14 (TfLiteFlexDelegate) failed to invoke.\r\n```\r\nIt seems that some modifications in [`TfLiteFlexDelegate`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/flex) are needed. Is it the right starting point?\r\n\r\n> However, I doubt it will work as expected if you want to train it on device. Because TF Lite graph can only be used for inference at the moment.\r\n\r\nI think it will be good starting point for expanding TfLite functionality."]}, {"number": 26868, "title": "What is the corresponding API for tf.nn.rnn_cell._linear in tf 2.0?", "body": "Just as the title suggests, I want to adopt an older version code to tf2.0, thanks", "comments": ["You can convert your TF 1.X code to 2.X by using [```tf_upgrade_v2 utility```](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/upgrade.md#upgrade-code-to-tensorflow-20)", "> You can convert your TF 1.X code to 2.X by using [`tf_upgrade_v2 utility`](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/upgrade.md#upgrade-code-to-tensorflow-20)\r\n\r\nSorry, I tried using the single line of command to upgrade the script, but the old issue just remains.", "Thanks for the question, @ChaoYue0307! This is an endpoint that was deprecated in an earlier version of TensorFlow. The recommended path for TF 2.0 would be to refactor your code to use `tf.keras.layers.RNN`.", "Closing this issue since it's resolved. Feel free to reopen if required. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26868\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26868\">No</a>\n"]}, {"number": 26867, "title": "transform_graph tools fails to produce output for a particular graph definition/also strange behavior of the tool", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nn/a\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version (use command below):\r\nv1.12.0-10232-g9a43dfe 1.13.1\r\n- Python version:\r\n3.7.1\r\n- Bazel version (if compiling from source):\r\nBuild label: 0.19.2\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Mon Nov 19 16:25:09 2018 (1542644709)\r\nBuild timestamp: 1542644709\r\nBuild timestamp as int: 1542644709\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\nn/a\r\n- GPU model and memory:\r\nn/a\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI was trying to use `transform_graph` tool to produce quantized version of a very simple graph. I noticed that in certain configurations of the graph(they are simple structurally) the tools complains about missing ops and so on. An example message:\r\n```\r\n2019-03-19 07:55:57.750401: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected MobilenetV1/conv1/batch_normalization_v1/beta to be preserved.\r\n2019-03-19 07:55:57.750414: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected MobilenetV1/conv1/batch_normalization_v1/gamma to be preserved.\r\n2019-03-19 07:55:57.750423: W tensorflow/tools/graph_transforms/transform_utils.cc:448] Generator function didn't preserve needed nodes, copying old replacements back in instead.\r\n```\r\nI tried to create a minimalistic reproducible example, where I started to notice more strange behavior. I will attach the python code for graph_def building below. For now this is the pb file I'm trying to work with:\r\n[saved_model.pb.tar.gz](https://github.com/tensorflow/tensorflow/files/2982144/saved_model.pb.tar.gz)\r\n\r\nIn my setup the tools fails with the following error:\r\n`2019-03-19 07:59:51.381714: E tensorflow/tools/graph_transforms/transform_utils.cc:577] Invalid input batch_normalization/gamma for node batch_normalization/FusedBatchNorm - name: \"batch_normalization/FusedBatchNorm\"`\r\n\r\nI call the the tool with the following configuration:\r\n`$tbin --in_graph=$in_graph --out_graph=$out_graph_clean_quant --inputs=$inputs --outputs=$outputs --transforms='add_default_attributes strip_unused_nodes() remove_nodes(op=Identity, op=CheckNumerics)  quantize_weights quantize_nodes fold_batch_norms fold_old_batch_norms strip_unused_nodes sort_by_execution_order'`\r\n\r\nThe tool will not fail if we change the depthwise conv to a normal conv2d. FusedBatchNorm does not lose any inputs, it just works. \r\n\r\nAnother observation: if we remove the following transformations `strip_unused_nodes() remove_nodes(op=Identity, op=CheckNumerics)` the tool will not fail.\r\n\r\nAnother observation. If we remove two aforementioned transformations, this is a piece of graph we get:\r\n![image](https://user-images.githubusercontent.com/6204851/54589454-940e8f80-4a1d-11e9-946f-a24d220908e3.png)\r\n\r\nFor some reason two FusedBatchNorm nodes receive Beta\\Gamma from the same nodes. I'm guessing this should not be the case. Furthermore, inside each of this nodes gamma\\variance and beta\\mean pairs receive values from the same reading ops. Note that there are only two additional inputs to FusedBatchNorm.\r\n![image](https://user-images.githubusercontent.com/6204851/54589580-ecde2800-4a1d-11e9-9679-b58f3b963735.png)\r\n\r\nHopefully it all makes sense.\r\n\r\n**Describe the expected behavior**\r\nTool should not fail when FusedBatchNorm is followed by DepthwiseConv2dNative.\r\n\r\n\r\n**Code to reproduce the issue**\r\nHere is some python self-contained code to reproduce the problem:\r\n\r\n```\r\nfrom tensorflow.contrib import keras\r\nimport tensorflow as tf\r\nimport os\r\n\r\n\r\ndef freeze_graph(model_dir, checkpoint_name, output_node_names):\r\n    \"\"\"Extract the sub graph defined by the output nodes and convert\r\n    all its variables into constant\r\n\r\n    Args:\r\n        model_dir: the root folder containing the checkpoint state file\r\n        output_node_names: a string, containing all the output node's names,\r\n                            comma separated\r\n    \"\"\"\r\n    if not tf.gfile.Exists(model_dir):\r\n        raise AssertionError(\r\n            \"Export directory doesn't exists. Please specify an export \"\r\n            \"directory: %s\" % model_dir)\r\n\r\n    if not output_node_names:\r\n        print(\"You need to supply the name of a node to --output_node_names.\")\r\n        return -1\r\n\r\n    absolute_model_dir = os.path.split(checkpoint_name)[0]\r\n    output_graph = os.path.join(absolute_model_dir, \"saved_model.pb\")\r\n\r\n    clear_devices = True\r\n\r\n    # We start a session using a temporary fresh Graph\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        saver = tf.train.Saver()\r\n        saver.restore(sess, checkpoint_name)\r\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n            sess,  # The session is used to retrieve the weights\r\n            tf.get_default_graph().as_graph_def(),  # The graph_def is used to retrieve the nodes\r\n            output_node_names  # The output node names are used to select the usefull nodes\r\n        )\r\n        with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n            f.write(output_graph_def.SerializeToString())\r\n\r\n        with tf.gfile.GFile(output_graph, \"rb\") as f:\r\n            output_graph_def.ParseFromString(f.read())\r\n        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n\r\n\r\nsave_folder = os.path.join(os.path.expanduser('~'), 'tmp')\r\nos.makedirs(save_folder, exist_ok=True)\r\n\r\ninitializer = tf.contrib.layers.xavier_initializer_conv2d()\r\nwdw = tf.get_variable(shape=(3, 3, 32, 1), initializer=initializer, name=\"weights\")\r\nwc = tf.get_variable(shape=(1, 1, 32, 1), initializer=initializer, name=\"weights1\")\r\n\r\nconv_out = tf.placeholder(shape=(1, 300, 300, 3), dtype=tf.float32)\r\nconv_out = tf.nn.conv2d(conv_out, tf.get_variable(shape=(1, 1, 3, 32), initializer=initializer,name='w1'),\r\n                        strides=[1, 1, 1, 1],\r\n                        padding='SAME')\r\nconv_out = keras.layers.BatchNormalization()(conv_out, training=False)\r\n\r\nconv_out = tf.nn.depthwise_conv2d(conv_out, wdw, strides=[1, 1, 1, 1], padding='SAME')\r\n# conv_out = tf.nn.conv2d(conv_out, tf.get_variable(shape=(1, 1, 32, 32), initializer=initializer, name=\"weights32\"), strides=[1, 1, 1, 1], padding='SAME')\r\nconv_out = keras.layers.BatchNormalization()(conv_out, training=False)\r\nconv_out = tf.nn.relu(conv_out)\r\n#\r\nconv_out = tf.nn.conv2d(conv_out, wc, strides=[1, 1, 1, 1], padding='SAME')\r\nconv_out = keras.layers.BatchNormalization()(conv_out, training=False)\r\nconv_out = tf.nn.relu(conv_out)\r\n\r\nsaver = tf.train.Saver()\r\ncheckpoint_name = os.path.join(save_folder, 'model.chkpt')\r\nwith tf.Session() as s:\r\n    s.run(tf.global_variables_initializer())\r\n\r\n    save_path = saver.save(s, checkpoint_name)\r\n\r\nfreeze_graph(save_folder, checkpoint_name, [conv_out.op.name])\r\nprint(conv_out.op.name)\r\n```\r\n\r\n**Other info / logs**\r\nPlease, do let me know if I missed something. \r\nAlso, I might be misunderstanding something - please let me know if this is the case", "comments": ["Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26867\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26867\">No</a>\n"]}, {"number": 26866, "title": "Tensorflow Hardware Requirements", "body": "Does the tensorflow library require a CPU with AVX support? It's not mentioned anywhere in the documentation. I've tried import tensorflow on several machines and it only fails in the machine without AVX support.", "comments": ["Please take a look at [hardware requirements for TF](https://www.tensorflow.org/install/pip#hardware-requirements).", "> Please take a look at [hardware requirements for TF](https://www.tensorflow.org/install/pip#hardware-requirements).\r\n\r\nI don't know how I missed out on that! Thanks a lot. I have been unsuccessfully trying to set up a machine with an E7 CPU that has no AVX support. I will look for alternatives.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 26865, "title": "Rename CONCAT_EMBEDDINGS to CONCATENATE_EMBEDDINGS\"", "body": "TODO item done, Rename CONCAT_EMBEDDINGS to CONCATENATE_EMBEDDINGS  ", "comments": ["@rthadur gentle ping to merge this PR. TIA", "@siju-samuel sorry for the delay , can you please resolve conflicts.", "@karimnosseir Gentle ping to check and approve again. This PR is rebased due to recent changes.\r\n\r\n", "@siju-samuel thanks for your contribution,we don't think this changes is necessary, so closing this PR."]}, {"number": 26864, "title": "[TF2] tf.saved_model.save does not support type annotations", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip wheel\r\n- TensorFlow version (use command below): `tf-nightly-2.0-preview==2.0.0.dev20190318`\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nGiven the following code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass A(tf.Module):\r\n    @tf.function\r\n    def func(self, x: int):\r\n        pass\r\n\r\n\r\na = A() \r\ntf.saved_model.save(a, export_dir=\".\")\r\n```\r\n\r\nI get the following traceback:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"scratch.py\", line 10, in <module>\r\n    tf.saved_model.save(a, export_dir=\".\")\r\n  File \"/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 812, in save\r\n    saveable_view, asset_info.asset_index)\r\n  File \"/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 561, in _serialize_object_graph\r\n    _write_object_proto(obj, obj_proto, asset_file_def_index)\r\n  File \"/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 577, in _write_object_proto\r\n    function_serialization.serialize_function(obj))\r\n  File \"/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/function_serialization.py\", line 81, in serialize_function\r\n    function_spec_proto = _serialize_function_spec(function.function_spec, coder)\r\n  File \"/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/function_serialization.py\", line 29, in _serialize_function_spec\r\n    proto.fullargspec.CopyFrom(coder.encode_structure(function_spec.fullargspec))\r\n  File \"/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py\", line 85, in encode_structure\r\n    return self._map_structure(nested_structure, self._get_encoders())\r\n  File \"/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py\", line 69, in _map_structure\r\n    return do(pyobj, recursion_fn)\r\n  File \"/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py\", line 224, in do_encode\r\n    pair.value.CopyFrom(encode_fn(named_tuple_value._asdict()[key]))\r\n  File \"/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py\", line 69, in _map_structure\r\n    return do(pyobj, recursion_fn)\r\n  File \"/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py\", line 193, in do_encode\r\n    encoded_dict.dict_value.fields[key].CopyFrom(encode_fn(value))\r\n  File \"/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py\", line 71, in _map_structure\r\n    \"No encoder for object [%s] of type [%s].\" % (str(pyobj), type(pyobj)))\r\ntensorflow.python.saved_model.nested_structure_coder.NotEncodableError: No encoder for object [<class 'int'>] of type [<class 'type'>].\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIdeally, saving this would work just fine and not crash on the type annotation.", "comments": ["@vbardiovskyg do we need an extension of structure coding for type annotations? I guess stripping them would be slightly better than throwing an exception, but long-term it seems like we want to save them. We should be able to add a Python 3-only unit test suite.", "One work-around for now is to use `str` annotations like:\r\n\r\n```python\r\ndef func(self, x: \"int\"):\r\n    ...\r\n```\r\n\r\nIn Python 3.7, doing \r\n```python\r\nfrom __future__ import annotations\r\n```\r\n\r\nwill also by-default make all the type annotations strings without having to manually quote them (this successfully saves the function without crashing).", "Oh interesting, we do save annotations, we just don't have a case for \"int\". Seems easy to add.\r\n\r\nOn the other hand, passing non-Tensors (like Python \"int\"s) to tf.functions will re-trace each time (i.e. each value of the integer will get its own graph). Out of curiosity, what is the Python int argument there for?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "I'm seeing this issue again in RC0 when I provide `tf.Tensor` type annotation in `call()`. The exact error is:\r\n```\r\ntensorflow.python.saved_model.nested_structure_coder.NotEncodableError: No encoder for object [<class 'tensorflow.python.framework.ops.Tensor'>] of type [<class 'type'>].\r\n```\r\nRemoving type annotations resolves the issue. This is, quite obviously, not ideal, so I think the issue should be re-opened.", "I have the same bug as @depthwise. Definitely should be reopened."]}, {"number": 26863, "title": "Changed TfLiteFusedActivation to TfLiteActivation", "body": "", "comments": ["@rthadur Could you please kindly check this change for merge", "> @rthadur Could you please kindly check this change for merge\r\n\r\n@jdduke can you please help merge this PR internally", "Thanks for your patience, but these kinds of minor refactorings have quite a bit of downstream impact in our internal code, so it's not a straightforward pull request.", "Can one of the admins verify this patch?", "There was too much downstream impact, so let's just leave the name as-is. Apologies for the hassle."]}, {"number": 26862, "title": "Added scenarios for negative axes,activation,dims.", "body": "This is one of the TODO in the file.", "comments": ["@rthadur , can you pls check the error has nothing to do with my code, i am having this problem in other approved PRs as well. Kindly check and help.\r\n\r\nRegards\r\nAmit", "@rthadur , this PR is approved, for long, can you please help me to get this merged.\r\n\r\nRegards\r\nAmit", "@jianlijianli  , thanks for approving the PR, this PR is not yet merge, can you please help me to get this merge.\r\n\r\nRegards\r\nAmit", "@jianlijianli , there was a merge conflict, which i have resolved now, can you please re-approve the PR.\r\n\r\nRegards\r\nAmit", "@jianlijianli , thanks for re-approving the PR.\r\n@rthadur , can you please help to merge this PR.\r\n\r\nRegards\r\nAmit", "In the future, it'd be helpful also to keep the older commits with the previous comments. Right now, I'm not able to access the older commits, which may be because you're force pushing.", "> In the future, it'd be helpful also to keep the older commits with the previous comments. Right now, I'm not able to access the older commits, which may be because you're force pushing.\r\n\r\n@alanchiao Thanks for your feedback, i have updated the code as per your suggestion, also i have not done the force push this time.\r\n\r\n\r\nRegards\r\nAmit", "@alanchiao thanks for approving the PR\r\n\r\n@rthadur , can you please help to merge this PR.\r\n\r\nRegards\r\nAmit", "@rthadur , can you please help to merge this PR.\r\n\r\nRegards\r\nAmit", "Can one of the admins verify this patch?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 26861, "title": "[ROCm] Added ROCm support for identity op", "body": "This minor mod adds ROCm support for the identity op.\r\n\r\nBackground info\r\nThese ops are fundamental to TensorFlow, and this mod has been running for more than 1 year on our ROCm port of TF.\r\n\r\nWe have published docker images at: https://hub.docker.com/r/rocm/tensorflow/tags\r\nAnd also PyPI packages: https://pypi.org/project/tensorflow-rocm/\r\n\r\nFor a sample ROCm test run you can refer to:\r\nhttp://ml-ci.amd.com:21096/job/tensorflow-upstream-unit-tests/721/console\r\n\r\n```\r\n//tensorflow/core/kernels:identity_op_test                               PASSED in 0.2s\r\n```", "comments": []}, {"number": 26860, "title": "Refactor Concatenate&SparseTensorSlice&TensorSlice&Zip DatasetOpTests", "body": "This PR refactors Concatenate&SparseTensorSlice&TensorSlice&Zip DatasetOpTests and also adds the tests for the new `node_name()` and `type_string()` APIs.\r\n\r\ncc: @jsimsa ", "comments": ["It looks like the internal checks failed. Could you please help share the log here?", "It looks like a memory leak in the ConcatenateDatasetOpTest::DatasetSave method. Do you see an obvious problem there?\r\n\r\nFor example:\r\n\r\n```\r\nRAW: Leak of 20 bytes in 1 objects allocated from:\r\n\t@ 0x55d93f03a3b8 tensorflow::data::DatasetBase::DatasetBase()\r\n\t@ 0x55d93f039b93 tensorflow::data::(anonymous namespace)::ConcatenateDatasetOp::MakeDataset()\r\n\t@ 0x7f34e3f12192 tensorflow::data::BinaryDatasetOpKernel::MakeDataset()\r\n\t@ 0x7f34e3f11f90 tensorflow::data::DatasetOpKernel::Compute()\r\n\t@ 0x7f34e55341d7 tensorflow::data::DatasetOpsTestBase::RunOpKernel()\r\n\t@ 0x7f34e55342f8 tensorflow::data::DatasetOpsTestBase::CreateDataset()\r\n\t@ 0x7f34e555f838 tensorflow::data::(anonymous namespace)::ConcatenateDatasetOpTest_DatasetSave_Test::TestBody()\r\n```", "@jsimsa Thanks for sharing the log. The failure is caused by my bad: miss the release of one allocated `concatenate_dataset`. \r\n\r\nNow the memory leak is fixed by [this commit](https://github.com/tensorflow/tensorflow/pull/26860/commits/d371ce10cfd965be7166affc9f56e0e1a4bffa70). Could you have a look at the change?   @rthadur Could you help re-trigger the test?", "@jsimsa This is a kindly reminder that the memory leak is fixed by [this commit](https://github.com/tensorflow/tensorflow/pull/26860/commits/d371ce10cfd965be7166affc9f56e0e1a4bffa70). Could you have a look and re-trigger the test when you have time? "]}, {"number": 26859, "title": "Check for memory overflow during tensor allocation", "body": "", "comments": ["@aselle Could you please review this PR", "@joyalbin can you please check review comments ?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "Thanks for the submission. I've reworked this internally based on some refactors and also optimized the code from overflow.h to not have to do some checks since we already know it is unsigned this should land soon."]}, {"number": 26858, "title": "[ROCm] Added ROCm support for nn_ops_test", "body": "This minor mod adds ROCm support for the nn_ops_test.\r\n\r\nBackground info\r\nThese ops are fundamental to TensorFlow, and this mod has been running for more than 1 year on our ROCm port of TF.\r\n\r\nWe have published docker images at: https://hub.docker.com/r/rocm/tensorflow/tags\r\nAnd also PyPI packages: https://pypi.org/project/tensorflow-rocm/\r\n\r\nFor a sample ROCm test run you can refer to:\r\nhttp://ml-ci.amd.com:21096/job/tensorflow-upstream-unit-tests/721/console\r\n\r\n```\r\n//tensorflow/core/kernels:nn_ops_test                                    PASSED in 1.0s\r\n```", "comments": []}, {"number": 26857, "title": "[ROCm] Added ROCm support for one_hot op", "body": "This minor mod adds ROCm support for the one_hot op.\r\n\r\nBackground info\r\nThese ops are fundamental to TensorFlow, and this mod has been running for more than 1 year on our ROCm port of TF.\r\n\r\nWe have published docker images at: https://hub.docker.com/r/rocm/tensorflow/tags\r\nAnd also PyPI packages: https://pypi.org/project/tensorflow-rocm/\r\n\r\nFor a sample ROCm test run you can refer to:\r\nhttp://ml-ci.amd.com:21096/job/tensorflow-upstream-unit-tests/721/console\r\n\r\n```\r\n//tensorflow/python/kernel_tests:one_hot_op_test                         PASSED in 6.6s\r\n```", "comments": []}, {"number": 26856, "title": "[ROCm] Added ROCm support for the quantized and dequantize ops", "body": "This minor mod adds ROCm support for the Quantized and Dequantize ops.\r\n\r\nBackground info\r\nThese ops are fundamental to TensorFlow, and this mod has been running for more than 1 year on our ROCm port of TF.\r\n\r\nWe have published docker images at: https://hub.docker.com/r/rocm/tensorflow/tags\r\nAnd also PyPI packages: https://pypi.org/project/tensorflow-rocm/\r\n\r\nFor a sample ROCm test run you can refer to:\r\nhttp://ml-ci.amd.com:21096/job/tensorflow-upstream-unit-tests/721/console\r\n\r\n```\r\n//tensorflow/python:quantized_ops_test                                   PASSED in 2.1s\r\n//tensorflow/python:dequantize_op_test                                   PASSED in 2.2s\r\n```", "comments": []}, {"number": 26855, "title": "[ROCm] Added ROCm support for the Pad op", "body": "This minor mod adds ROCm support for the Pad op.\r\n\r\n#### Background info ####\r\nThese ops are fundamental to TensorFlow, and this mod has been running for more than 1 year on our ROCm port of TF.\r\n\r\nWe have published docker images at: https://hub.docker.com/r/rocm/tensorflow/tags\r\nAnd also PyPI packages: https://pypi.org/project/tensorflow-rocm/\r\n\r\nFor a sample ROCm test run you can refer to:\r\nhttp://ml-ci.amd.com:21096/job/tensorflow-upstream-unit-tests/721/console\r\n\r\n```\r\n//tensorflow/python/kernel_tests:pad_op_test                             PASSED in 3.8s\r\n```", "comments": []}, {"number": 26854, "title": "Tensorflow 2.0: where is tf.contrib.layers.layer_norm?", "body": "Cannot find tf.contrib.layers.layer_norm in TF 2.0", "comments": ["Hi, it has been moved to addons: https://github.com/tensorflow/addons/tree/master/tensorflow_addons/layers", "@martinwicke @seanpmorgan Hi, Martin, Sean, can we find a way to let users easily know where those modules of contrib have gone?\r\n\r\nI know we have had https://github.com/tensorflow/community/pull/37 and https://github.com/tensorflow/community/pull/18 , but unfortunately, it seems that not all users would like to read it.", "It's been discussed that we should add deprecation warnings to tf.contrib which instruct users of the move to addons for certain functions/classes. We would need to get these in before the 1.14 release I believe, so still some time but we should probably create an issue to track that. \r\n\r\nAlso, those same warnings should be added to the t2_convert script which instruct users where they can be found.", "We could also add the replacements to the [1:1 Symbols Map](https://docs.google.com/spreadsheets/d/1FLFJLzg7WNP6JHODX5q8BDgptKafq_slHpnHVbJIteQ/edit#gid=0).", "I have noticed that it is `tf.keras.layers.experimental.LayerNormalization` in TF 2.0\r\nBut `tf.keras.layers.experimental.LayerNormalization` is not in TF 1.13\r\nWill it appear in TF 1.14?", "> I have noticed that it is `tf.keras.layers.experimental.LayerNormalization` in TF 2.0\r\n> But `tf.keras.layers.experimental.LayerNormalization` is not in TF 1.13\r\n> Will it appear in TF 1.14?\r\n\r\nI would not expect tf.keras.layers.experimental.LayerNormalization to continue to exist in TF 2.0 for much longer. See the conversation in https://github.com/tensorflow/addons/pull/14 . Its on our TODO to request the removal of it.", "So what to use instead of it???", "For TF2.x\r\nhttps://github.com/tensorflow/addons\r\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/layers/normalizations.py#L277", "So finally there will be only `tf.keras.xxx.GroupNormalization`?\r\n`tf.keras.xxx.LayerNormalization` will be removed?", "Sorry I'll try to explain more clearly. TensorFlow addons is an additional python package that contains extra functionality for core tensorflow. One of the layers available in that package is LayerNormalization. Our implementation utilizes the fact that LayerNormalization is just one configuration of GroupNormalization. Feel free to use the `LayerNormalization` end point though.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\n\r\ntfa.layers.LayerNormalization()\r\n```", "My understanding is that tfa is to replace tf.contrib, and tf.keras is to replace tf.layers?", "Yeah that's true enough. One thing to note is that layers/optimizers/losses in tensorflow_addons all subclass a tf.keras base class so they'll have the same api structure.", "Thanks a lot...\r\nBut I prefer the previous TF, which is simpler.", "Entitled to your opinion, but I think you'll find tf.keras API spec is very intuitive and a better user experience now that everything is unified under one design. That's without mentioning the many improvements  in TF2.\r\n\r\n If you are referring to the fact that contrib got to exist in the same package as core.. thats true, but it  also [became unmanagable](https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md) and there were very little guarantees that the code you were running was behaving correctly. I think you'll see going forward [all of tensorflow is moving to a more modular approach](https://github.com/tensorflow/community/pull/77)", "tf.keras still has a lot of problems in combining with other tf components...\r\nsee #27016 for example", "@chwang85 -- variable_scope had a nasty habit of not being very composable. We have removed it from the 2.0 API (it remains available in `compat.v1` just like everything else). \r\n\r\nIf you prefer the \"simpler\" lower level API, it's still all there, you can absolutely still use it. In fact, it's likely easier to use, since we cleaned up a bunch so similar things don't appear in several places any more.\r\n\r\nI think this issue is resolved, right?", "These lower level API (e.g. tf.layers.dense) is the most useful part of TF (at least for me, a ML developer), but now every time I use them, there will be a disgusting message: `xxx (from tensorflow.python.layers.core) is deprecated and will be removed in a future version. Use keras.layers.xxx instead.` However, it seems that tf.keras.layers is not equivalent with tf.layers (see the variable scope issue), are your message fooling users?", "No, the Keras layers are pretty much equivalent in functionality. That does not mean they are drop in replacements. For a drop-in replacement, use tf.compat.v1.layers. \r\n\r\nNote also that tf.layers.* are not lower level than Keras layers in any meaningful way. They are definitely more magic, since they rely on a lot of global (or graph) state to work at all.\r\n\r\nIn keras layers, we removed support for this graph based state, most visibly with variable_scope. We have also changed the way variables were exposed (not via collections any longer). We believe these changes are highly beneficial for code quality and improve your ability to reason about your code. The message therefore suggests you move to keras layers.", "I strongly wish TF 1.x should stick on \"Graph\". TF 2.x may focus on \"Eager\". Just like Python 2.x vs Python 3.x", "It will. The only thing we're doing to 1.x is to add deprecation notices. \r\n\r\nIf you'd like you can disable the deprecation warnings, you can use (it's not part of the public API, but hey):\r\n\r\n```\r\nimport tensorflow.python.util.deprecation as deprecation\r\ndeprecation._PRINT_DEPRECATION_WARNINGS = False\r\n```", "Two questions to confirm:\r\n1. Will TF 1.X still be updated after TF 2.X is officially issued? \r\n2. Will tf.layers.dense (and so on) always stay in TF 1.X? ", "By the way, before deprecating old components, would you please complete the current functionality? See #27042. It seems that TF is very good at deprecating widely used functions, but poor at improving its weak points\r\n@facaiy @seanpmorgan @dynamicwebpaige @martinwicke ", "@seanpmorgan I can't find layer normalization in tfa. \r\n\r\nmodule 'tensorflow_addons.layers' has no attribute 'LayerNormalization'. You meant in your previous message that one should use ` tf.keras.layers.LayerNormalization`, right?", "@Oktai15  apologies these comments are a bit stale. TensorFlow core actually pulled LayerNormalization in so we removed it from Addons. In the future we'll have a more stable deprecation process this was just very early on in our growth.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/normalization.py#L843\r\n\r\n`tf.keras.layers.LayerNormalization`", "tf.keras.layers.LayerNormalization takes layer as input.How can i provide similar input structure to that of tf 1.x? eg. What to do in this case.\r\n```\r\n\r\n\r\ndef layer_norm(input_tensor, name=None):\r\n  \"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\r\n  return tf.contrib.layers.layer_norm(\r\n      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\r\n\r\n\r\ndef layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\r\n  \"\"\"Runs layer normalization followed by dropout.\"\"\"\r\n  output_tensor = layer_norm(input_tensor, name)\r\n  output_tensor = dropout(output_tensor, dropout_prob)\r\n  return output_tensor\r\n```\r\nHow to convert this snippet? ", "> tf.keras.layers.LayerNormalization takes layer as input.How can i provide similar input structure to that of tf 1.x? eg. What to do in this case.\r\n> \r\n> ```\r\n> \r\n> \r\n> def layer_norm(input_tensor, name=None):\r\n>   \"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\r\n>   return tf.contrib.layers.layer_norm(\r\n>       inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\r\n> \r\n> \r\n> def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\r\n>   \"\"\"Runs layer normalization followed by dropout.\"\"\"\r\n>   output_tensor = layer_norm(input_tensor, name)\r\n>   output_tensor = dropout(output_tensor, dropout_prob)\r\n>   return output_tensor\r\n> ```\r\n> \r\n> How to convert this snippet?\r\nHello adiv5,\r\nWere you able to work around this? I assume you are trying to upgrade BERT to work in Tensnorflow 2.0. \r\n", "Hi! @adiv5, I;m working on converting BERT to tensorflow 2.0, i faced the same problem with this, have you been able to work around with this? if so please share the solution. If this is not able to do so, i think to rewrite the BERT to tensorflow 2.0 keras....but at least run it on pretraining..", "Hi.\ntf.keras.layers.LayerNormalization is the replacement.\nYou may need to wrap the layer_norm_and_dropout function as a layer and\ncreate a layer norm instance attaching to self.\nFor BERT, you should not have problem to rewrite.\nWe have the bert model in TF official models.\nSee the LayerNormalization usage in this library\nhttps://github.com/tensorflow/models/blob/master/official/nlp/modeling/networks/transformer_encoder.py#L134\n\nThanks\n\nOn Sat, Jan 11, 2020 at 7:01 PM Leow Chee Siang <notifications@github.com>\nwrote:\n\n> Hi! @adiv5 <https://github.com/adiv5>, I;m working on converting BERT to\n> tensorflow 2.0, i faced the same problem with this, have you been able to\n> work around with this? if so please share the solution. If this is not able\n> to do so, i think to rewrite the BERT to tensorflow 2.0 keras....but at\n> least run it on pretraining..\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26854?email_source=notifications&email_token=ABFFXZOWXLATYFB2VMW4L3DQ5KBXNA5CNFSM4G7MP572YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEIWQSOY#issuecomment-573376827>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABFFXZJQCVVP7I2W7LQFILTQ5KBXNANCNFSM4G7MP57Q>\n> .\n>\n", "@saberkun Thank you ! I have been able to change the below code to working on pretraning.\r\n\r\n```\r\ndef layer_norm(input_tensor, name=None):\r\n  \"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\r\n  return tf.keras.layers.LayerNormalization(name=name,axis=-1,epsilon=1e-12,dtype=tf.float32)(input_tensor)\r\n  # return tf.contrib.layers.layer_norm(\r\n  #     inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\r\n```", "> My understanding is that tfa is to replace tf.contrib, and tf.keras is to replace tf.layers?\r\n\r\nAttributeError: module 'tensorflow_addons.layers' has no attribute 'LayerNormalization'", "> > My understanding is that tfa is to replace tf.contrib, and tf.keras is to replace tf.layers?\r\n> \r\n> AttributeError: module 'tensorflow_addons.layers' has no attribute 'LayerNormalization'\r\n\r\nLayerNorm was moved to TF-core:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization\r\n\r\nSorry for the stale information.", "> > > My understanding is that tfa is to replace tf.contrib, and tf.keras is to replace tf.layers?\r\n> > \r\n> > \r\n> > AttributeError: module 'tensorflow_addons.layers' has no attribute 'LayerNormalization'\r\n> \r\n> LayerNorm was moved to TF-core:\r\n> https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization\r\n> \r\n> Sorry for the stale information.\r\n\r\nThanks for the update.\r\nUnfortunately, the inputs to LayerNormalization are different from layer_norm. Is there a workaround for it or?", "@luvwinnie Is you model converging for fine-tuning. I see that the model does not converge for SQuAD with this change for TF 1.x model. ", "@jojivk73 yeah my model is converging, i reference this [repo](https://github.com/yoheikikuta/bert-japanese) for fine tuning Japanese corpus, below is my graph.\r\n<img width=\"1544\" alt=\"Screen Shot 2020-03-18 at 10 31 26\" src=\"https://user-images.githubusercontent.com/13714992/76916441-c50cf300-6903-11ea-9715-bddcb6320503.png\">\r\n", "def instance_norm(x, scope='instance_norm'):\r\n         return tf_contrib.layers.instance_norm(x,epsilon=1e-05,center=True, scale=True,scope=scope)\r\n\r\nHow to convert this tensorflow 2.2 ??"]}, {"number": 26853, "title": "Process finished with exit code -1073741819 (0xC0000005)", "body": "This is my simple coding:-\r\n\r\n<em>import tensorflow as tf\r\nprint(\"Hi\");</em>\r\n\r\n**output**\r\nProcess finished with exit code -1073741819 (0xC0000005)\r\n\r\nThe \"Hi\" is not printed. I searched online for this issue, but seems like none of the people has this, so it has been bugging me for few days.\r\n\r\nNo GPU involved, just the CPU version of tensorflow.\r\n\r\nI tried v1.13, v1.12, v1.11, and v.1.10, all gave the me the same issue.\r\nI tried with python 3.5.*, 3.6.*, and 3.7.*, also same result.\r\nTried with LiClipse, PyCharm, Atom, all gave the same result as well.\r\n\r\nWindows 10 x64.\r\n\r\nIt works before, but suddenly one day it didn't work anymore, so I have no idea what happen.\r\nTried to uninstall any apps that installed within few weeks, but didn't solve the issue.\r\n\r\nI really wish someone can help me to solve this issue.", "comments": ["The same thing goes here. I have tried to install tensorflow through pip and tensorflow_mkl with conda. Another installation on a different machine which uses a GPU version tensorflow works just fine. Tried to remove and reinstall python environment to avoid any possible dependency issue, not working.\r\n\r\nSince 0xC0000005 often comes from DLL importing issue on Windows, I checked the event log of Windows, the error comes from python.exe obviously, and the error module is unknown(which should be the dll name if any). The error report info goes as follows,\r\n```\r\nFault bucket 1638171688499684297, type 5\r\nEvent Name: BEX64\r\nResponse: Not available\r\nCab Id: 0\r\n\r\nProblem signature:\r\nP1: python.exe\r\nP2: 3.7.2150.1013\r\nP3: 5c6f3612\r\nP4: StackHash_ac46\r\nP5: 0.0.0.0\r\nP6: 00000000\r\nP7: PCH_F0_FROM_ntdll+0x00000000000A01C4\r\nP8: c0000005\r\nP9: 0000000000000008\r\nP10: \r\n```\r\n\r\nI checked that the CPU is Core i7-4790 which has AVX2 capability, so that it shouldn'd be a problem about AVX right?", "Also it worth mentioning that a custom py37-win-x64 build on branch r1.13 with cherry-pick https://github.com/tensorflow/tensorflow/commit/ec727016282383aacf9d26386b01f6bdbd65b14b by VS2017 also facing the same problem. I haven't had time to build a cpu version on the other computer, so it is unclear whether the issue is related to code or the environment (either system or python).", "@bglee85 Could you uninstall python and tensorflow, restart system and install python and tensorflow following these [instructions](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12). This procedure is for installing TF1.12 but you can follow similar procedure to install higher version of TF. Please let me know how it progresses. Thanks!", "I was quite urgent to use the tensorflow-gpu, so I reset my windows and reinstall everything. It works with the following conditions:-\r\n\r\nCUDA 10.0, cudnn 7.5, python-3.6.8, tensorflow & tensorflow-gpu 1.13.1\r\nWindows 10, i9, GTX 1050 Ti\r\n\r\nI wish there is a better way to solve the issue without resetting the windows, but I didn't have time to try other options one by one.\r\n\r\n** On side note, I did try with CUDA 9.0 (installation failed), CUDA 9.2 (works fine with tensorflow 1.12 but didn't work with tensorflow-gpu 1.12, I think is the graphic driver is higher version) and CUDA 10.1 (similar to CUDA 9.2, but works with tensorflow-1.13.1, not 1.12). All associated with the respective cudnn-7.5.\r\n\r\n** Also, I ran the deviceQuery.exe, and the result came back with CUDA Driver Version = 10.1, CUDA Runtime = 10.0, so I tried to install the CUDA 10.1 Runtime instead, but it didn't work on python-3.6.8, I haven't try with python 3.7.2 yet.\r\n\r\nI did some search online regarding to the issue of 0x000000CD, it looks like the the process wasn't able to continue due to several issues, but some said is memory corrupted, so I could suggest uninstall the Anaconda, and reinstall everything again (I didn't try it, but I hope it works rather than reset the windows).\r\n\r\nHope this help.", "@bglee85 when you have time, please write simple step by step instructions that could help the community who have similar problems like you had earlier. Thanks!", "@jvishnuvardhan, technically I didn't solve the problem, I just reset my windows and reinstall everything. Anyway here is the step after factory reset the windows.\r\n\r\n1) Install CUDA 10.0\r\n2) Copy cudnn toolkit v7.5 into CUDA/v10.0 folder\r\n- you can check out whether your GPU is working with CUDA by executing the deviceQuery.exe at Anaconda prompt (\"CUDA/v10.0/extras/demo_suite/deviceQuery.exe\")\r\n- you also can test out the bandwidth by executing bandwidthTest.exe in the same folder.\r\n3) Install tensorflow at Anaconda prompt: pip install tensorflow (use tensorflow==xxx if you want specific version, e.g., tensorflow==1.12)\r\n4) Install tensorflow-gpu at Anaconda prompt: pip install tensorflow-gpu (similar to tensorflow if you want specific version, e.g., tensorflow-gpu==1.12)\r\n\r\nTechnically, this doesn't solve the issue of 0xC0000005 as I just factory reset my windows.", "Have you moved your files around in any way, like separate the application from the rest of the files. I moved the files back and the problem was solved."]}, {"number": 26852, "title": "For Python what happens to tf.contrib.data.AUTOTUNE in 2.0 as contrib is not there in 2.0 and what do we use instead ?", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["Try `tf.data.experimental.AUTOTUNE` maybe?", "@hadim: that worked for me .... thanks!", "Should I close the issue team @ocampesato , @hadim @jvishnuvardhan  as it is working with @hadim  suggested code please update?", "Thanks @hadim for resolving the issue. I am closing the issue. Thanks!"]}, {"number": 26851, "title": "[TF 2.0] tf.distribute.cluster_resolver.TPUClusterResolver() fails on Colab", "body": "Using latest `tf-nightly-gpu-2.0-preview` as of today.\r\n\r\nConsider the following code:\r\n\r\n```python\r\nimport tensorflow as tf\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\ntpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n```\r\n\r\nIt fails on Colab:\r\n\r\n```\r\n<ipython-input-4-8454559c747f> in <module>()\r\n----> 1 tf.distribute.cluster_resolver.TPUClusterResolver()\r\n\r\nAttributeError: module 'tensorflow._api.v1.distribute' has no attribute 'cluster_resolver'\r\n```\r\n\r\nThis code comes from the [TF 2.0 documentation](https://www.tensorflow.org/alpha/guide/distribute_strategy#tpustrategy).", "comments": ["Duplicated https://github.com/tensorflow/tensorflow/issues/26513"]}, {"number": 26850, "title": "TFTRT: Use ConvertAxis helper function for Squeeze, ExpandDims, Reduce", "body": "Also update unit tests to reflect these changes", "comments": []}, {"number": 26849, "title": "Release notes for patch release 1.12.1", "body": "", "comments": []}, {"number": 26848, "title": "TFTRT: Enable ISliceLayer for TRT version 5.1.3.1+", "body": "Also add `build` number as a parameter for `IS_TRT_VERSION_GE()`. ", "comments": []}, {"number": 26847, "title": "[TF 2.0 API Docs] Added usage and examples to tf.compat.path_to_str", "body": "Fix #25826 by adding usage and examples to `tf.compat.path_to_str`\r\n\r\nHere I have also commented the OS used (since `Path` is OS dependent)\r\nIf `tf` uses a differently flavored Markdown or my PR is not up-to-standard, kindly excuse me as I'm not used to the Tensorflow Doc Generation process, also I'd be happy to learn more about it!", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26847) for more info**.\n\n<!-- need_author_cla -->", "I have signed it! (under @Bharat123rox username, was previously @Bharat123Rox)", "> I have signed it! (under @Bharat123rox username, was previously @Bharat123rox)\r\n\r\nplease sign CLA with all the usernames used for this PR ", "How do I find out all usernames used for a PR? I just changed branches, not\nusernames!?\n\nOn Mon, 18 Mar 2019, 11:39 pm rthadur, <notifications@github.com> wrote:\n\n> I have signed it! (under @Bharat123rox <https://github.com/Bharat123rox>\n> username, was previously @Bharat123rox <https://github.com/Bharat123rox>)\n>\n> please sign CLA with all the usernames used for this PR\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/26847#issuecomment-474032746>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AMwu8Zj0LbGrDSyMEQ_GZ4ot7MiDF4zWks5vX9ZhgaJpZM4b6aTv>\n> .\n>\n", "> How do I find out all usernames used for a PR? I just changed branches, not usernames!?\r\n> [\u2026](#)\r\n> On Mon, 18 Mar 2019, 11:39 pm rthadur, ***@***.***> wrote: I have signed it! (under @Bharat123rox <https://github.com/Bharat123rox> username, was previously @Bharat123rox <https://github.com/Bharat123rox>) please sign CLA with all the usernames used for this PR \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#26847 (comment)](https://github.com/tensorflow/tensorflow/pull/26847#issuecomment-474032746)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AMwu8Zj0LbGrDSyMEQ_GZ4ot7MiDF4zWks5vX9ZhgaJpZM4b6aTv> .\r\n\r\ni see a CLA signed with this username:Bharat123Rox , can you sign with username: @Bharat123rox ", "Both the usernames (@Bharat123rox and Bharat123Rox) share the same email-id and same name, how do I create a different CLA?", "I have signed the CLA under @Bharat123rox @rthadur or @googlebot please have a look", "Superseded by #28502 "]}, {"number": 26846, "title": "Cannot feed single element in ELMo TF-HUB embedder", "body": "Hi,\r\nI am working in building a Deep Neural Network Classifier starting from an ELMo embedding module that embeds string, using Tensorflow Hub Module. I am using this model definition:\r\n\r\n```\r\nurl = \"https://tfhub.dev/google/elmo/2\"\r\nembed = hub.Module(url, trainable=True)\r\n\r\ndef make_elmo_embedding(x):\r\n    embeddings = embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"elmo\"]\r\n    return embeddings\r\n\r\n# elmo embedding dimension\r\nelmo_dim = 1024\r\n\r\n# Input Layers\r\nelmo_input = Input(shape=(None, ), dtype=\"string\")\r\n\r\n# Hidden Layers\r\nelmo_embedding = Lambda(make_elmo_embedding, output_shape=(None, elmo_dim))(elmo_input)\r\n\r\n\r\nx = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))(elmo_embedding)\r\nx = Dense(32, activation='relu')(x)\r\npredict = Dense(2, activation='sigmoid')(x)\r\nmodel = Model(inputs=[elmo_input], outputs=predict)\r\nmodel.compile(loss='mse', optimizer='sgd')\r\n```\r\nThis code works ok if the input of my network is a list with more than two strings, for example ['hello' , 'my name is Simone']. \r\nIf my input is a list with only one element (['hello']), this error appears: \r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:\r\n input must be a vector, got shape: [] .   [[{{node lambda_1/module_apply_default/StringSplit}}]]\r\n```\r\nThis error is reproducible both in training mode ( batch_size = 1) and inference mode ( if I want to make inference on 1 element for example). \r\n\r\nLooking at the error ```node lambda_1/module_apply_default/StringSplit}``` I think that the problem is in the Lambda layer, for the fact that in function ```make_elmo_embedding``` the tf.squeeze operation cancels the dimension = 1 and the input vector results of dimension = []. I tried to delete ```tf.squeeze``` but other errors occured. \r\nWhat you suggest to solve this situation in order to make this model definition compatible both for single and multiple example lists? \r\n\r\nThanks\r\nI am using Tensorflow 1.13.1\r\n\r\n\r\n", "comments": ["@simonefrancia In future, please post TF_Hub related issues in the Hub [Repo](https://github.com/tensorflow/hub/issues). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26846\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26846\">No</a>\n", "Hi, I am facing the same problem when sample size is (batch_size)*iteration + 1, it produces the same error:\r\n\r\n `tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: input must be a vector, got shape: []\r\n\t [[{{node lambda_1_1/module_1_apply_default/StringSplit}}]]\r\n  (1) Invalid argument: input must be a vector, got shape: []\r\n\t [[{{node lambda_1_1/module_1_apply_default/StringSplit}}]]\r\n\t [[_arg_bidirectional_1/keras_learning_phase_0_3/_1329]]\r\n`\r\n\r\nSince you've closed the issue already, did you fix the bug? @simonefrancia ", "I am also having the same issue. Could you address please?", "The root cause of this issue is `tf.squeeze `, that reduces all dimensions that equal 1 and if you have only 1 example, `n_samples` dimension is reduced as well.\r\nTo overcome it you should specify axis, that could be removed during the squeezing.\r\n```\r\ntf.squeeze(z, axis=1)\r\n```\r\nThis version of Keras lambda function works for me:\r\n\r\n```\r\ndef make_elmo_embedding(x):\r\n    embeddings = embed(tf.squeeze(tf.cast(x, tf.string), axis=1), signature=\"default\", as_dict=True)[\"elmo\"]\r\n    return embeddings\r\n```\r\n"]}, {"number": 26845, "title": "Force ICU to look for its data statically. This is required for Windows 10 builds. Patch suggested by github.com/cielavenir.", "body": "Fixes #23655\r\nFixes #23963", "comments": ["@lissyx can you please changes to tensorflow:master", "@rthadur This fix is from master https://github.com/tensorflow/tensorflow/commit/7e090f6a5412fd5f22b5b75ca7ae7844c3d025e9 but adapted to `r1.12`", "@hamatake I thought you already made this change?", "This should be resolved now. The original patch had to be rolled back, and\na new one was rolled forward last week.\n\n2019\u5e743\u670818\u65e5(\u6708) 18:58 Gunhan Gulsoy <notifications@github.com>:\n\n> @hamatake <https://github.com/hamatake> I thought you already made this\n> change?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/26845#issuecomment-474132137>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEnCCLmYKrDfaR1I2n7kOqvVcz81ofbTks5vYBoRgaJpZM4b6Vpr>\n> .\n>\n", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 26844, "title": "Tensorflow 2.0: please do not deprecate important functions!!!", "body": "According to Tensorflow 2.0, many important functions are deprecated, such as:\r\ntf.layers.dense()\r\ntf.layers.dropout()\r\ntf.layers.flatten()\r\ntf.layers.batch_normalization()\r\n...\r\nSince these functions are very widely used, and also very helpful for building complicted models, would you please keep them in the future versions?", "comments": ["You have to use `tf.keras.layers` instead of `tf.layers` to build the models.", "there is no dense(), dropout(), etc. to use, no matter keras or layers", "You have to use the functional version of the layers in Keras. There are no `dropout`, `dense` and so on layers, because there are the keras layers that you can instantiate and then call:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers\r\n\r\nFor example:\r\n\r\n`tf.layers.dropout` becomes `tf.keras.layers.Dropout(rate)(input)`", "Yup, @galeone is right.. TensorFlow is trying to standardize on tf.keras as the direction of general model building.\r\n\r\n@chwang85 Also checkout https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/dropout ", "Sorry, I am not asking for \"how to write a TF program\".\r\nI believe it is ridiculous to deprecate these functional layers. \r\nKeeping them in TF 2.0 is not a burden, and compatible with TF 1.x, why do you insist in deprecating them?", "\"Deprecated\" means they are still there but will go away on a subsequent release (that is, in 2.0).\r\n\r\nSo, you can still use them in TF 1.x, but they won't be available in TF 2.x and later. The deprecated message is just to notify you and give you time to update.\r\n\r\nClosing this and the duplicated #26151 and #26144", "Why not deprecate tf.Session()?", "It goes away too", "In TF 1.13, when tf.layers.dense() is used, there is a \"deprecate\" warning, but tf.Session() is OK.\r\nBy the way, I think it would be better if the difference between TF 1.x and TF 2.x is like that between Python 2.x and Python 3.x", "`tf.session` will be deprecated later, after the community switches to using TF2.x. Until then, it will still be available for some scenarios, since TF2.x has a very abrupt change in the way TF code is executed (from graph mode to eager mode; `tf.session` is needed for graph mode even in TF2.x).\r\n\r\n`tf.layers.dense` has a 100% functional equivalent. It has been there for a long time and the decision to deprecate it and favor the object oriented approach has been taken a long time ago. There is no reason to keep both available, so that's why you get this warning. Please understand that APIs change and the design choice has been taken a very long time ago and now it is too late to go back. That ship has sailed for too long.", "This decision is not reasonable at all, because the new components, tf.keras.layers, still has a lot of problems in combining with other tf components...\r\ntf.keras.layers even does not support variable scope, see #27016 for example", "To share variables using Keras layers you have to build a Keras Model - is a different way of organizing the code and to think.\r\n\r\nOnce you built the model you have to reuse the **object**. Hence don't share variables using scopes, but share variables using the `model.trainable_variables` attribute.\r\n\r\nWhile I was migrating my codebase to TF2 I faced your same issues and I wrote a blog post about this: https://pgaleone.eu/tensorflow/keras/2019/01/19/keras-not-yet-interface-to-tensorflow/\r\n\r\nI also wrote a second article about how to migrate the models from tf1 to tf2, while still using tf1: https://pgaleone.eu/tensorflow/gan/2018/11/04/tensorflow-2-models-migration-and-new-design/\r\n(therefore there are still sessions that in tf2 are going to be removed)\r\n\r\nIn practice, TF2 requires to re-learn how to use the framework if you're used to working with the static graph + session execution.", "thanks\r\ni have noticed that the change between TF 1.x and TF 2.x is too big to simply modify tf.layers to tf.keras.lyaers\r\ninstead, it requiress a totally different way of thinking and designing...\r\ni will stick on the graph + session ways", "By the way, before deprecating old components, would you please complete the current functionality? See #27042. It seems that TF is very good at deprecating widely used functions, but poor at improving its weak points\r\n@mihaimaruseac @galeone @mandroid6 @ry", "Can someone explain me. If I need to use  tf.keras.layers instead of tf.layers to build the models, and tf.keras.model.compile to build graph, and tf.keras.model.fit to train instead tf.graph and tf.Session.\r\nThen why I need Tensorflow at all? Maybe I should simply use Keras?\r\n\r\n@mihaimaruseac @galeone @mandroid6 @ry", "Keras is a high level API for TensorFlow, which will be able to solve most\nof the model building and training needs. Still it isn't an exhaustive API\nwhich has all basic ops available in TensorFlow.\n\nMost importantly Keras uses TensorFlow for building the acyclic graphs or\nnow the eager execution context.\n\nAll tf.keras.layers or tf.keras.model uses TensorFlow in the backend. You\ncan very well still directly use the low level TF ops, but why to when you\nhave a more simple abstraction available.\n\nOn Thu, 28 Mar 2019 at 1:40 PM, OgnjenTanovic <notifications@github.com>\nwrote:\n\n> Can someone explain me. If I need to use tf.keras.layers instead of\n> tf.layers to build the models, and tf.keras.model.compile to build graph,\n> and tf.keras.model.fit to train instead tf.graph and tf.Session.\n> Then why I need Tensorflow at all? Maybe I should simply use Keras?\n>\n> @mihaimaruseac <https://github.com/mihaimaruseac> @galeone\n> <https://github.com/galeone> @mandroid6 <https://github.com/mandroid6> @ry\n> <https://github.com/ry>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26844#issuecomment-477487523>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ASrYl4eMUJGFfpsRHYeXiaIb2_IPAc6pks5vbHjigaJpZM4b6T0L>\n> .\n>\n", "When will you deprecate keras?\r\nWhen will you deprecate Tensorflow?\r\nWhen will you deprecate Google?", "TensorFlow is giving up a lot of its unique features. As a TensorFlow user since 2015, I feel very sorry about that. Keras should never take the place of TensorFlow. ", "I was just working on adapting to some deprecation warnings(https://github.com/sympy/sympy/pull/17103) and was curious to find why deprecation has been done. I googled and reached this issue.\r\nSo, well many of my doubts are clear, but using, `tf.compat.v1.Session` instead of `tf.Session` is not clear(Is this related to namespace?), I will investigate more on this, though if any one can provide an answer on this, then it will be really helpful.\r\n\r\nPS - BTW, if the end result for `tf.layers` and `tf.keras.layers` is same, then I think its a one line change,\r\nCreate an alias for use, `tf = tf.keras` and most of the effort for changes can be avoided. This trick can be included somewhere in the README or warnings, probably you might have done that. Well that's just a suggestion, it can be immature, and many of you might have thought it. \r\n\r\nFound https://www.tensorflow.org/beta/guide/migration_guide, nice guide. Though tf.Session -> tf.compat.v1.Session is not clear.\r\nThanks for reading. ", "Regarding `tf.Session`: TF 2.0 is eager by default, in that world `Session` is useless. It is being moved to `compat.v1` to have some backward compatibility: instead of requiring you to rewrite your entire script immediately you can just change an import (or use a script to rename `tf.Session` into `tf.compat.v1.Session`) and then rewrite at your convenience (though, at one point you should rewrite).\r\n\r\nRegarding `tf.layers`: `tf` top level namespace was too huge, making autocomplete and human understanding very hard. We decided to split it into components.\r\n\r\nNote that doing `tf = tf.keras` will result into issues `tf.math.sin` will be `tf.keras.math.sin` which will result in `AttributeError: Module 'keras' has no attribute 'math'`.", "@mihaimaruseac Thank you for the clarification. In fact, I read [this proposal](https://github.com/tensorflow/community/blob/b1d83bf2ee3fc72650140b89656e29932db36226/rfcs/20180918-functions-not-sessions-20.md), many of my doubts are clear. I would say, it's a very good idea to migrate from `Sessions` -> `Functions`. Though I am not familiar with the terminology but examples in the proposal made the issue very clear.\r\n\r\n> It is being moved to `compat.v1` to have some backward compatibility: instead of requiring you to rewrite your entire script immediately you can just change an import (or use a script to rename `tf.Session` into `tf.compat.v1.Session`) and then rewrite at your convenience (though, at one point you should rewrite).\r\n\r\nAh! I was thinking of that. So, are there any plans to remove `compat` somewhere in future may be after `2.x`? \r\n\r\n\r\n\r\n> Note that doing `tf = tf.keras` will result into issues `tf.math.sin` will be `tf.keras.math.sin` which will result in `AttributeError: Module 'keras' has no attribute 'math'`.\r\n\r\nI see, doing, `tf = tf.keras` is really very immature, well I came across an automation script to make the code compatible with tensorflow 2.0 in   https://www.tensorflow.org/beta/guide/migration_guide and I think using that would be better, though replacing is a different issue. ", "What is the future of frozen graph? Currently there is no way to freeze a graph in TF2.0. Will it be supported in the near future?", "@czgdp1807 Yes, at one point (probably nearing TF 3.0 but don't quote me on that as I might be wrong w.r.t timeline) `compat.v1` will be removed\r\n\r\nThe migration guide is really useful and recommended\r\n\r\n@leimao I suggest opening a new issue to discuss that, I have no idea to be honest and if there's a new issue an owner of that part of the code can respond. Thanks", "@mihaimaruseac I expect that release of tf 3.0 is not very near, right? I expect it at least after 1 year and I hope that no concrete alternative is in the pipeline for `compat.v1`, if there is, then please let me know, so that I can make updates to my sympy PR to keep that part of the code base stable for a longterm. ", "Yes, TF 3.0 is going to be far in the future. There are no plans for it at the moment (we still have to actually make a release for TF 2.0, we are currently in beta)", "I think the problem here is that people have no clue how to use `tf.keras.layers.Dense` without building a Keras model.\r\n\r\nEven the official document for [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) has only examples using the Keras model. So naturally, people see this and wonder \"_How do I add the inputs?_\" They come to the conclusion that `tf.keras.layers.Dense` cannot be used with pure TensorFlow and then start using `tf.layers.dense`\r\n\r\nIf we could add examples of how to use `tf.keras.layers.Dense` with pure TensorFlow this would solve the issue. I am guessing the general expression is :\r\n\r\n```\r\noutput = tf.keras.layers.<LAYER_TYPE>(size, activation, ...)(inputs)\r\n```", "The solution to this problem:\r\nSwitch to PyTorch, you will not regret it.\r\n`torch.nn.Linear` has been there since the v0 and continues to work ever since.\r\nWith TF there have been tf.nn, tf.slim, tf.contrib, tf.layers, tf.keras, I wonder what next... Every year they announce a new, final API...\r\n\r\nUnless your company forces you to use TF there is no real reason to stay. Really.\r\n\r\n", "How does this affect Tensorflow.js?"]}]