[{"number": 4068, "title": "Branch 131437591", "body": "", "comments": ["(what's autocommit)  how come your pushes are different than the standard ones?\n"]}, {"number": 4067, "title": "GPU usage level and training speed is very different for different tensorflow version", "body": "### Environment info\n\nOperating System: Ubuntu 14.04, AWS g2.2xlarge\n\nInstalled version of CUDA and cuDNN:  CUDA 7.5, cuDNN 4\n\nTried 3 different version using pip package:\n0.8.0:\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n\n0.9.0:\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl\n\n0.10.0 rc\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nOn the same machine, install different version of tf and run the [cifar10_train.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_train.py)\n### Logs or other output that would be helpful\n\nFor version 0.8.0 and 0.9.0, the GPU usage is about 30%\n\n```\n+------------------------------------------------------+                       \n| NVIDIA-SMI 352.99     Driver Version: 352.99         |                       \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |\n| N/A   37C    P0    51W / 125W |   3854MiB /  4095MiB |     35%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     16558    C   python                                        3841MiB |\n+-----------------------------------------------------------------------------+\n```\n\nBut the speed is about ~0.24 sec/batch for version 0.8.0 and ~0.22 sec/batch for version 0.9.0\n\n```\n2016-08-26 21:20:41.309814: step 230, loss = 4.28 (522.0 examples/sec; 0.245 sec/batch)\n2016-08-26 21:20:43.625017: step 240, loss = 4.26 (582.3 examples/sec; 0.220 sec/batch)\n2016-08-26 21:20:45.953772: step 250, loss = 4.24 (544.4 examples/sec; 0.235 sec/batch)\n2016-08-26 21:20:48.302202: step 260, loss = 4.23 (540.1 examples/sec; 0.237 sec/batch)\n2016-08-26 21:20:50.643760: step 270, loss = 4.21 (554.6 examples/sec; 0.231 sec/batch)\n2016-08-26 21:20:52.955326: step 280, loss = 4.20 (545.6 examples/sec; 0.235 sec/batch)\n2016-08-26 21:20:55.399758: step 290, loss = 4.18 (476.4 examples/sec; 0.269 sec/batch)\n2016-08-26 21:20:57.825254: step 300, loss = 4.17 (548.0 examples/sec; 0.234 sec/batch)\n2016-08-26 21:21:00.453533: step 310, loss = 4.15 (543.6 examples/sec; 0.235 sec/batch)\n2016-08-26 21:21:02.876055: step 320, loss = 4.14 (513.9 examples/sec; 0.249 sec/batch)\n2016-08-26 21:21:05.229421: step 330, loss = 4.13 (580.3 examples/sec; 0.221 sec/batch)\n2016-08-26 21:21:07.614095: step 340, loss = 4.11 (528.6 examples/sec; 0.242 sec/batch)\n```\n\n---\n\nFor 0.10.0.rc0, the GPU usage is ~90\uff05\uff1a\n\n```\n+------------------------------------------------------+                       \n| NVIDIA-SMI 352.99     Driver Version: 352.99         |                       \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |\n| N/A   35C    P0    46W / 125W |   3818MiB /  4095MiB |     90%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     16702    C   python                                        3805MiB |\n+-----------------------------------------------------------------------------+\n```\n\nBut the speed is only ~0.34 sec/batch:\n\n```\n016-08-26 21:24:11.512601: step 30, loss = 4.38 (371.3 examples/sec; 0.345 sec/batch)\n2016-08-26 21:24:14.875387: step 40, loss = 4.42 (379.0 examples/sec; 0.338 sec/batch)\n2016-08-26 21:24:18.248093: step 50, loss = 4.27 (368.0 examples/sec; 0.348 sec/batch)\n2016-08-26 21:24:21.609797: step 60, loss = 4.24 (379.8 examples/sec; 0.337 sec/batch)\n2016-08-26 21:24:24.987058: step 70, loss = 4.25 (376.4 examples/sec; 0.340 sec/batch)\n2016-08-26 21:24:28.387080: step 80, loss = 4.38 (381.0 examples/sec; 0.336 sec/batch)\n2016-08-26 21:24:31.775519: step 90, loss = 4.18 (377.8 examples/sec; 0.339 sec/batch)\n```\n", "comments": ["There are some known performance regressions in the 0.10 release candidate.\nClosing as duplicate of #3974\n"]}, {"number": 4066, "title": "retrying_file_system does not use exponential backoff", "body": "In tensorflow-0.10.0rc0/tensorflow/core/platform/cloud/retrying_file_system.cc,\nthe retry logic does not insert a delay between successive attempts.\n\nStatus CallWithRetries(const std::function<Status()>& f) {\n  int attempts = 0;\n  while (true) {\n    attempts++;\n    auto status = f();\n    if (!IsRetriable(status) || attempts >= kMaxAttempts) {\n      return status;\n    }\n    LOG(ERROR) << \"The operation resulted in an error and will be retried: \"\n               << status.ToString();\n  }\n}\n", "comments": ["Yes-  this retry logic does look pretty basic.  \n@davidj-github  Could you please explain a bit more about the problem you are having.\n@rinugun  Is there another backoff mechanism at a lower level (e.g. in an RPC layer?) or does this just retry 4 times without sleeping?\n", "I was reviewing the code and noticed that it does not conform to Google's recommendation to insert a delay between retries.\n", "@davidj-github I agree this is not usually very desirable behavior.  If it's not causing you an immediate problem, then I assume there's no great urgency here?  I'll let the authors of this module comment further.\n", "That's correct, there is no backoff, it simply retries 3 times, so that there is at most 4 attempts including the original one.\n\nThe underlying layers do not attempt any retries.\n\nThe reason for the lack of a backoff was that the actual Google Cloud Storage errors don't require it -- very infrequently GCS can return an internal error for a read or write operation and an immediate retry typically succeeds.\n\nThat said, I don't mind having a backoff if it's recommended.\n", "@rinugun Thanks.   I'll leave this as a 'contributions welcome' placeholder.  Since you have LOG(ERROR) messages in the loop I think it will be obvious if/when this is causing real problems for anyone.\n", "I would like to work on this\n", "Updating CallWithRetries to do exponential backoff #4501 .\n\nCould you all please review?\n", "Schlumberger-Private\nThe Cloud storage documentation for exponential backoff recommends adding a random term to the sleep time. See https://cloud.google.com/storage/docs/exponential-backoff\n\nFrom: Balachander Ramachandran [mailto:notifications@github.com]\nSent: Tuesday, September 20, 2016 10:36 PM\nTo: tensorflow/tensorflow tensorflow@noreply.github.com\nCc: David Judson DJudson@slb.com; Mention mention@noreply.github.com\nSubject: [Ext] Re: [tensorflow/tensorflow] retrying_file_system does not use exponential backoff (#4066)\n\nUpdating CallWithRetries to do exponential backoff #4501https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_tensorflow_tensorflow_pull_4501&d=CwMCaQ&c=uGuXJ43KPkPWEl2imVFDmZQlhQUET7pVRA2PDIOxgqw&r=7r_lgzfgEAaM8ykY8TCz9g&m=1VZnPLf14kYsDIRR9L1U0Fbl0Mf2CCC0FDUL4Im_wcA&s=wMRcLblp4zkClmvOBbhBMqSZyuKK3CXrbBaX9p43BoE&e= .\n\nCould you all please review?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_tensorflow_tensorflow_issues_4066-23issuecomment-2D248502364&d=CwMCaQ&c=uGuXJ43KPkPWEl2imVFDmZQlhQUET7pVRA2PDIOxgqw&r=7r_lgzfgEAaM8ykY8TCz9g&m=1VZnPLf14kYsDIRR9L1U0Fbl0Mf2CCC0FDUL4Im_wcA&s=XxeUsnSilLQMvLPmR3DAa1j5359E7fWd_Vx0QCxcml0&e=, or mute the threadhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ATdHG-5F0xVwTbyWpqRS5JI4Z1fpiVkVvMks5qsKYmgaJpZM4JuaF2&d=CwMCaQ&c=uGuXJ43KPkPWEl2imVFDmZQlhQUET7pVRA2PDIOxgqw&r=7r_lgzfgEAaM8ykY8TCz9g&m=1VZnPLf14kYsDIRR9L1U0Fbl0Mf2CCC0FDUL4Im_wcA&s=2LRsfLEwulq84LcMiXDzcqj-icQnZwIhsDCmZyjiQ8Y&e=.\n", "@davidj-github @rinugun \n\nProposed changes to incorporate the above random sleep for milliseconds\n\n```\n#include <chrono>\n#include <random>\n#include <thread>\n\nvoid WaitBeforeRetry(const int delay_seconds) {\n    std::random_device rd;\n    std::mt19937 gen(rd());\n    std::uniform_int_distribution<> dist(1, 1000);\n\n    const int delay_milliseconds = delay_seconds * 1000;\n    const int random_milliseconds = dist(gen);\n    const int delay = std::min(delay_to_milliseconds + random_milliseconds,\n                               kMaximumBackoffSeconds);\n\n    std::this_thread::sleep_for(std::chrono::milliseconds(delay));\n}\n\nStatus CallWithRetries(const std::function<Status()>& f,\n                       const int initial_delay_seconds) {\n  int retries = 0;\n  while (true) {\n    auto status = f();\n    if (!IsRetriable(status) || retries >= kMaxRetries) {\n      return status;\n    }\n    LOG(ERROR) << \"The operation resulted in an error and will be retried: \"\n               << status.ToString();\n    WaitBeforeRetry(initial_delay_seconds << retries);\n    retries++;\n  }\n}\n\n\n\n```\n", "@davidj-github @rinugun @prb12 what do you guys think?\n", "I'm curious, is random delay better than the fixed delay that was\nimplemented in https://github.com/tensorflow/tensorflow/pull/4501? The\nrandom sub-second sleep seems to be adding a particular complexity to the\nretry logic, I wonder if this is justified and what the expected wins are.\n\nOn Sat, Sep 24, 2016 at 6:43 PM, Balachander Ramachandran <\nnotifications@github.com> wrote:\n\n> @davidj-github https://github.com/davidj-github @rinugun\n> https://github.com/rinugun @prb12 https://github.com/prb12 what do\n> you guys think?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4066#issuecomment-249397475,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABnZstqsh-CSWZmdEk4EI9lQ0CKYIw1Zks5qtdHYgaJpZM4JuaF2\n> .\n", "From the Google Cloud storage documentation - \n\n> random_number_milliseconds is a random number of milliseconds less than or equal to 1000. This helps to avoid cases where many clients get synchronized by some situation and all retry at once, sending requests in synchronized waves. The value of random_number_milliseconds is recalculated after each retry request.\n", "I could only find this text in Google Analytics docs (\nhttps://developers.google.com/analytics/devguides/reporting/core/v4/errors),\nis that also mentioned somewhere for the Google Cloud Storage docs?\nAnyways, I agree that it might be potentially useful. I have a few comments\nre the code snippet:\n- not sure <chrono> is supported in the TF code, but you could use\n  Env::Default()->SleepForMilliseconds() from env.h instead\n- other minor comments, probably could be discussed during a code review\n\nThanks\nAlex\n\nOn Sat, Sep 24, 2016 at 8:38 PM, Balachander Ramachandran <\nnotifications@github.com> wrote:\n\n> From the Google Cloud storage documentation -\n> \n> random_number_milliseconds is a random number of milliseconds less than or\n> equal to 1000. This helps to avoid cases where many clients get\n> synchronized by some situation and all retry at once, sending requests in\n> synchronized waves. The value of random_number_milliseconds is recalculated\n> after each retry request.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4066#issuecomment-249400674,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABnZsg-D-NAn5ghCaT9yHj-szBG8u23Sks5qtey6gaJpZM4JuaF2\n> .\n", "It is documented here - https://cloud.google.com/storage/docs/exponential-backoff\n\nI see it being used in a few places -\n\n> tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc\n> tensorflow/core/platform/profile_utils/cpu_utils.cc\n", "@davidj-github @rinugun @prb12 have updated the PR(#4501) to include the random millisecond delay. Please review!\n"]}, {"number": 4065, "title": "Add some more info in ISSUE_TEMPLATE.md, some text cleanup", "body": "Clarifies what kind of issues should be reported and what we will close, asks people to try searching on the web for similar issues first, asks for a minimal reproducible example.\n", "comments": []}, {"number": 4064, "title": "Fix a lost wakeup bug in `Notification::WaitForNotificationWithTimeout()`", "body": "(Cherry pick for r0.10 branch.)\n\nIf the notification was set before waiting, the waiter would never wake up.\n\nAdded a test to cover this case. Fixes #3978.\nChange: 131247138\n", "comments": []}, {"number": 4063, "title": "Branch 131404637", "body": "", "comments": []}, {"number": 4062, "title": "Tensorboard icons invisible when building from source", "body": "The refresh, configure, and plot buttons (expand, log/linear) are not visible when building tensorboard from source. However, they are clickable.\n<img width=\"1438\" alt=\"screen shot 2016-08-26 at 8 10 32 am\" src=\"https://cloud.githubusercontent.com/assets/1794423/18010495/9a428d32-6b65-11e6-8f7f-23a490ae05fb.png\">\n### Environment info\n\nBuild/execute operating System: Ubuntu 16.04\nViewing operating System: OS X 10.11.6\nViewing browsers: Chrome 52 & Safari 9.1.2\nInstalled from source from a recent master commit 99ce233191920fb0dde2d823cf493e28b89618c7\nBazel 0.3.0\n### Steps to reproduce\n1. git clone https://github.com/tensorflow/tensorflow.git\n2. cd tensorflow\n3. ./configure\n4. bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n5. pushd tensorflow/tensorboard\n6. npm run prepare\n7. npm run compile\n8. gulp regenerate\n9. popd\n10. bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n11. bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n12. pip install /tmp/tensorflow_pkg/tensorflow-0.10.0rc0-py3-none-any.whl\n13. tensorboard\n### What have you tried?\n1. Building from r0.10 branch but it doesn't work either.\n", "comments": ["Known issue. Will be fixed today at r0.10 and master.\n", "It is fixed in master now. Thanks.\n", "Fix will be merged into r0.10 by https://github.com/tensorflow/tensorflow/pull/4137.\n"]}, {"number": 4061, "title": "Use auto-detected gcc host compiler include paths in CROSSTOOL file.", "body": "This change removes the hard-coded cxx_builtin_include_directory entries and\ninstead generates cxx_builtin_include_directory entries using the compiler\ninclude dirs detected using get_cxx_inc_directories.\n\nThis change also adds the auto_configure_fail function and uses it to improve\nauto-configuration error messages.\n\nFixes #4058\n", "comments": ["Thanks! \n"]}, {"number": 4060, "title": "Nightly binary links in README are broken.", "body": "Following the links in the README. CPU-only builds work, but both Mac and Linux GPU builds return 404.\n", "comments": ["Seems to be back now.\n"]}, {"number": 4059, "title": "Building for iOs fails with \"ld: 44 duplicate symbols for architecture armv7\" error", "body": "Hi all,\n\nTrying to build Tensorflow for [iOs.](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#ios) The step `tensorflow/contrib/makefile/build_all_ios.sh` after around 20 minutes of building returns errors including `duplicate symbols for architecture armv7`. Please find the details bellow.\n### Environment info\n\nOperating System: Mac OS X 10.11.6 (15G31)\nXcode 7.3.1 (7D1014)\n### Steps to reproduce\n\nIn terminal:\n1. `git clone git@github.com:tensorflow/tensorflow.git`\n2. `cd tensorflow`\n3. `tensorflow/contrib/makefile/download_dependencies.sh`\n4. `tensorflow/contrib/makefile/build_all_ios.sh`\n### What have you tried?\n1. Removing the repository directory and going through the steps again.\n2. Going through \"Building by hand\" steps described [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#building-by-hand). Error: `ld: 44 duplicate symbols for architecture armv7s` ([Error log](https://github.com/tensorflow/tensorflow/files/439452/tensorflow-error-log2.txt)).\n### Error log ([Full log](https://github.com/tensorflow/tensorflow/files/439413/tensorflow-error-log.txt))\n\n`...`\n`ld: 44 duplicate symbols for architecture armv7`\n`clang: error: linker command failed with exit code 1 (use -v to see invocation)`\n`make: *** [/Users/b0915218/Xcode/tensorflow/tensorflow/contrib/makefile/gen/bin/ios_ARMV7/benchmark]` `Error 1`\n`+ '[' 2 -ne 0 ']'`\n`+ echo 'armv7 compilation failed.'`\n`armv7 compilation failed.`\n`+ exit 1`\n", "comments": ["Hi,\n\nI'm having the exact same error message.\n\nOS: OS X 10.11.6\nXcode 7.3.1\n\nMany thanks,\n\nJoe\n", "Today tried to pull the project again and go through all the steps from the beginning. It worked. Thanks for fixing!\n"]}, {"number": 4058, "title": "Use header include directories auto-detected from host compiler", "body": "As mentioned by @akors in #2109 and #3980, we should generate `cxx_builtin_include_directory` entries for the header include directories used by the user's host compiler of choice rather than hard-coding them as we are currently doing.\n", "comments": ["I must apologize to you guys, because it seems that since davidzchen's patch, the build is actually working without hacking. I apparently forgot a git-pull and used an older version. Sorry about the rant. I appreciate all of your work and thanks to you (the devs) in particular and google in general for open-sourcing such a great tool.\n", "No worries. I was traveling and was just going through the notification emails now. Your updated comment was a nice surprise! :)\n"]}, {"number": 4057, "title": "Fix typo in README.md", "body": "", "comments": []}, {"number": 4056, "title": "Adding more layers decreases model accuracy. ", "body": "Here are two different reproducible codes, one has two conv layers and other has 10 conv layers. Model with two conv layers reaches the result in few iterations whereas model with 10 conv layers reaches the result in more iterations, and moreover they both produce same result 62.5 % accuracy. Model with 10 conv layers should provide better accuracy(because it has more layers) but it gives same accuracy as 2 conv layer model and reaches the same result after more iterations, so adding more layers is degrading performance.\n\n Here is 2 conv layer code:\n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nimport nltk\nimport random\nbatch_size = 100\nstart = 0\nend = batch_size\nlearning_rate = 0.001\nnum_classes = 2\ntime_steps = 4\nembedding = 2\nstep = 1\n_units = 1\nnum_of_filters = 2\n\ntrain_set_x = [[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]]]\ntrain_set_y = [0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,0,0,0,1,0,1,0,1,1,1,0,1,0,1,0,1,1]\n\nX = tf.placeholder(tf.float32, [None,time_steps,embedding])\nY = tf.placeholder(tf.int32, [None])\n\n\nx = tf.expand_dims(X,3)\n\nfilter_shape = [1, embedding, 1, 64]\nconv_weights = tf.get_variable(\"conv_weights1\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases = tf.Variable(tf.constant(0.1, shape=[64]))\nconv = tf.nn.conv2d(x, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\nnormalize = tf.nn.elu(conv + conv_biases)\ntf_normalize = tf.contrib.layers.batch_norm(inputs = normalize,is_training = True)\noutputs_fed_lstm = tf_normalize\n\nfilter_shape2 = [1, 1, 64, 64]\nconv_weights2 = tf.get_variable(\"conv_weights2\" , filter_shape2, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases2 = tf.Variable(tf.constant(0.1, shape=[64]))\nconv2 = tf.nn.conv2d(outputs_fed_lstm, conv_weights2, strides=[1,1,1,1], padding = \"VALID\")\nnormalize2 = tf.nn.elu(conv2 + conv_biases2)\ntf_normalize2 = tf.contrib.layers.batch_norm(inputs = normalize2,is_training = True)\noutputs_fed_lstm2 = tf_normalize2\n\nx = tf.squeeze(outputs_fed_lstm2, [2])     \nx = tf.transpose(x, [1, 0, 2])\nx = tf.reshape(x, [-1, 64])\nx = tf.split(0, time_steps, x)\n\nlstm = tf.nn.rnn_cell.LSTMCell(num_units = _units)\n\n# multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)\n\noutputs , state = tf.nn.rnn(lstm,x, dtype = tf.float32)     \n\nweights = tf.Variable(tf.random_normal([_units,num_classes]))\nbiases  = tf.Variable(tf.random_normal([num_classes]))\n\nlogits = tf.matmul(outputs[-1], weights) + biases\n\n\n\nc_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)\nloss = tf.reduce_mean(c_loss)\n\n\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)\n# decayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)\noptimizer= tf.train.AdamOptimizer(learning_rate)\nminimize_loss = optimizer.minimize(loss, global_step=global_step)   \n#grads_and_vars = optimizer.compute_gradients(loss,[conv_weights2]) \ncorrect_predict = tf.nn.in_top_k(logits, Y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n     sess.run(init)\n     for i in range(1000):\n         for j in range(1000):\n             x = train_set_x\n             y = train_set_y\n             sess.run(minimize_loss,feed_dict={X : x, Y : y})\n             step += 1  \n             #gr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n             #print (gr_print)\n             cost = sess.run(loss,feed_dict = {X: x,Y: y})\n             accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n             print (\"Loss after one Epoch(Training) = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))\n```\n\nHere is code for 10 layer conv model:\n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nimport nltk\nimport random\nbatch_size = 100\nstart = 0\nend = batch_size\nlearning_rate = 0.001\nnum_classes = 2\ntime_steps = 4\nembedding = 2\nstep = 1\n_units = 100\nnum_of_filters = 2\n\ntrain_set_x = [[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]]]\ntrain_set_y = [0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,0,0,0,1,0,1,0,1,1,1,0,1,0,1,0,1,1]\n\nX = tf.placeholder(tf.float32, [None,time_steps,embedding])\nY = tf.placeholder(tf.int32, [None])\n\n\nx = tf.expand_dims(X,3)\n\nfilter_shape = [1, embedding, 1, 64]\nconv_weights = tf.get_variable(\"conv_weights1\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases = tf.Variable(tf.constant(0.1, shape=[64]))\nconv = tf.nn.conv2d(x, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\nnormalize = tf.nn.elu(conv + conv_biases)\ntf_normalize = tf.contrib.layers.batch_norm(inputs = normalize,is_training = True)\noutputs_fed_lstm = tf_normalize\n\nfilter_shape2 = [1, 1, 64, 64]\nconv_weights2 = tf.get_variable(\"conv_weights2\" , filter_shape2, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases2 = tf.Variable(tf.constant(0.1, shape=[64]))\nconv2 = tf.nn.conv2d(outputs_fed_lstm, conv_weights2, strides=[1,1,1,1], padding = \"VALID\")\nnormalize2 = tf.nn.elu(conv2 + conv_biases2)\ntf_normalize2 = tf.contrib.layers.batch_norm(inputs = normalize2,is_training = True)\noutputs_fed_lstm2 = tf_normalize2\n\nfilter_shape3 = [1, 1, 64, 64]\nconv_weights3 = tf.get_variable(\"conv_weights3\" , filter_shape3, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases3 = tf.Variable(tf.constant(0.1, shape=[64]))\nconv3 = tf.nn.conv2d(outputs_fed_lstm2, conv_weights3, strides=[1,1,1,1], padding = \"VALID\")\nnormalize3 = tf.nn.elu(conv3 + conv_biases3)\ntf_normalize3 = tf.contrib.layers.batch_norm(inputs = normalize3,is_training = True)\noutputs_fed_lstm3 = tf_normalize3\n\nfilter_shape4 = [1, 1, 64, 128]\nconv_weights4 = tf.get_variable(\"conv_weights4\" , filter_shape4, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases4 = tf.Variable(tf.constant(0.1, shape=[128]))\nconv4 = tf.nn.conv2d(outputs_fed_lstm3, conv_weights4, strides=[1,1,1,1], padding = \"VALID\")\nnormalize4 = tf.nn.elu(conv4 + conv_biases4)\ntf_normalize4 = tf.contrib.layers.batch_norm(inputs = normalize4,is_training = True)\noutputs_fed_lstm4 = tf_normalize4\n\nfilter_shape5 = [1, 1, 128, 128]\nconv_weights5 = tf.get_variable(\"conv_weights5\" , filter_shape5, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases5 = tf.Variable(tf.constant(0.1, shape=[128]))\nconv5 = tf.nn.conv2d(outputs_fed_lstm4, conv_weights5, strides=[1,1,1,1], padding = \"VALID\")\nnormalize5 = tf.nn.elu(conv5 + conv_biases5)\ntf_normalize5 = tf.contrib.layers.batch_norm(inputs = normalize5,is_training = True)\noutputs_fed_lstm5 = tf_normalize5\n\nfilter_shape6 = [1, 1, 128, 128]\nconv_weights6 = tf.get_variable(\"conv_weights6\" , filter_shape6, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases6 = tf.Variable(tf.constant(0.1, shape=[128]))\nconv6 = tf.nn.conv2d(outputs_fed_lstm5, conv_weights6, strides=[1,1,1,1], padding = \"VALID\")\nnormalize6 = tf.nn.elu(conv6 + conv_biases6)\ntf_normalize6 = tf.contrib.layers.batch_norm(inputs = normalize6,is_training = True)\noutputs_fed_lstm6 = tf_normalize6  \n\nfilter_shape7 = [1, 1, 128, 256]\nconv_weights7 = tf.get_variable(\"conv_weights7\" , filter_shape7, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases7 = tf.Variable(tf.constant(0.1, shape=[256]))\nconv7 = tf.nn.conv2d(outputs_fed_lstm6, conv_weights7, strides=[1,1,1,1], padding = \"VALID\")\nnormalize7 = tf.nn.elu(conv7 + conv_biases7)\ntf_normalize7 = tf.contrib.layers.batch_norm(inputs = normalize7,is_training = True)\noutputs_fed_lstm7 = tf_normalize7 \n\nfilter_shape8 = [1, 1, 256, 256]\nconv_weights8 = tf.get_variable(\"conv_weights8\" , filter_shape8, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases8 = tf.Variable(tf.constant(0.1, shape=[256]))\nconv8 = tf.nn.conv2d(outputs_fed_lstm7, conv_weights8, strides=[1,1,1,1], padding = \"VALID\")\nnormalize8 = tf.nn.elu(conv8 + conv_biases8)\ntf_normalize8 = tf.contrib.layers.batch_norm(inputs = normalize8,is_training = True)\noutputs_fed_lstm8 = tf_normalize8 \n\nfilter_shape9 = [1, 1, 256, 256]\nconv_weights9 = tf.get_variable(\"conv_weights9\" , filter_shape9, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases9 = tf.Variable(tf.constant(0.1, shape=[256]))\nconv9 = tf.nn.conv2d(outputs_fed_lstm8, conv_weights9, strides=[1,1,1,1], padding = \"VALID\")\nnormalize9 = tf.nn.elu(conv9 + conv_biases9)\ntf_normalize9 = tf.contrib.layers.batch_norm(inputs = normalize9,is_training = True)\noutputs_fed_lstm9 = tf_normalize9 \n\nfilter_shape0 = [1, 1, 256, 512]\nconv_weights0 = tf.get_variable(\"conv_weights0\" , filter_shape0, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases0 = tf.Variable(tf.constant(0.1, shape=[512]))\nconv0 = tf.nn.conv2d(outputs_fed_lstm9, conv_weights0, strides=[1,1,1,1], padding = \"VALID\")\nnormalize0 = tf.nn.elu(conv0 + conv_biases0)\ntf_normalize0 = tf.contrib.layers.batch_norm(inputs = normalize0,is_training = True)\noutputs_fed_lstm0 = tf_normalize0 \n\nx = tf.squeeze(outputs_fed_lstm0, [2])     \nx = tf.transpose(x, [1, 0, 2])\nx = tf.reshape(x, [-1, 512])\nx = tf.split(0, time_steps, x)\n\nlstm = tf.nn.rnn_cell.LSTMCell(num_units = _units)\n\n# multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)\n\noutputs , state = tf.nn.rnn(lstm,x, dtype = tf.float32)     \n\nweights = tf.Variable(tf.random_normal([_units,num_classes]))\nbiases  = tf.Variable(tf.random_normal([num_classes]))\n\nlogits = tf.matmul(outputs[-1], weights) + biases\n\n\n\nc_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)\nloss = tf.reduce_mean(c_loss)\n\n\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)\n# decayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)\noptimizer= tf.train.AdamOptimizer(learning_rate)\nminimize_loss = optimizer.minimize(loss, global_step=global_step)   \n#grads_and_vars = optimizer.compute_gradients(loss,[conv_weights2]) \ncorrect_predict = tf.nn.in_top_k(logits, Y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n     sess.run(init)\n     for i in range(1000):\n         for j in range(1000):\n             x = train_set_x\n             y = train_set_y\n             sess.run(minimize_loss,feed_dict={X : x, Y : y})\n             step += 1  \n             #gr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n             #print (gr_print)\n             cost = sess.run(loss,feed_dict = {X: x,Y: y})\n             accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n             print (\"Loss after one Epoch(Training) = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))\n```\n", "comments": ["Someone is also having this issue - http://stackoverflow.com/questions/35268167/low-accuracy-with-change-to-tensorflow-cifar10-example\n", "This is a general machine learning question, not a specific bug/issue, and therefore more suitable for StackOverflow.  Please can you re-ask there.\n"]}, {"number": 4055, "title": "Deprecate parameters from tf_workspace", "body": "There is no need for either of those parameters, use Bazel label\nresolution instead.\n", "comments": ["@tensorflow-jenkins test this please\n"]}, {"number": 4054, "title": "Fix stddev computation in TensorBoard examples.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "CLA signed.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@bamos Well, that's kind of embarrassing :-) Thanks for the fix!\n", "@tensorflow-jenkins test this please\n"]}, {"number": 4053, "title": "encounter error when session run in c++:Invalid argument: No OpKernel was registered to support Op 'Sub' with these attrs", "body": "tensorflow version :0.8.0\n\n1) use python to train model\n2) use freeze_graph.py to freeze the mode and graph def as freeze.pb\n3) load the  freeze.pb  is ok . but when  run session get this error\n\n'''CreateGlobalVariables ...\npn predict fail:Invalid argument: No OpKernel was registered to support Op 'Sub' with these attrs\n         [[Node: train_correct/dp_11/random_uniform/sub = Sub[T=DT_FLOAT](train_correct/dp_11/random_uniform/max, train_correct/dp_11/random_uniform/min)]] '''\n", "comments": ["Could you possibly try to reproduce this on a newer version of TensorFlow?\n", "I had similar problem with Dequantization Kernel not being register when trying to execute inception V3 in 8bit mode. I fixed it in the bazel script under the target and included the quantization operators as dependencies. The kernel registrations should happen as file globals (I believe) so it looks like there is a disconnect with the \"Sub\" operator and the dependencies in bazel for that particular target.\n", "@u4lr451 Have you been able to reproduce on a newer version of TensorFlow?  The latest released version is 0.10.0\n", "Automatically closing due to lack of recent activity, please reopen or provide more information when it becomes available.\n"]}, {"number": 4052, "title": "Can not compile on Mac OS X Yosemite", "body": "### Environment info\n\nOperating System: Mac OS X Yosemite\n### Steps to reproduce\n1. `git clone https://github.com/grpc/grpc.git`\n2. `git checkout 0e43d67602096a5c4aeab4632579bc56cf692553`\n3. `cd grpc`\n4. `git submodule update --init --recursive`\n5. `make`\n### Make linking errors related to protobuf (first errors)\n\n```\n[CXX]     Compiling src/cpp/ext/reflection.pb.cc\nsrc/cpp/ext/reflection.pb.cc:719:7: error: no member named 'InternalWriteMessageNoVirtualToArray' in\n      'google::protobuf::internal::WireFormatLite'\n      InternalWriteMessageNoVirtualToArray(\n      ^\nsrc/cpp/ext/reflection.pb.cc:809:35: error: no member named 'MergeFromFail' in namespace 'google::protobuf::internal'\n    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\nsrc/cpp/ext/reflection.pb.cc:826:35: error: no member named 'MergeFromFail' in namespace 'google::protobuf::internal'\n    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\nsrc/cpp/ext/reflection.pb.cc:1536:35: error: no member named 'MergeFromFail' in namespace 'google::protobuf::internal'\n    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\nsrc/cpp/ext/reflection.pb.cc:1553:35: error: no member named 'MergeFromFail' in namespace 'google::protobuf::internal'\n    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\nsrc/cpp/ext/reflection.pb.cc:1958:7: error: no member named 'InternalWriteMessageNoVirtualToArray' in\n      'google::protobuf::internal::WireFormatLite'\n      InternalWriteMessageNoVirtualToArray(\n```\n", "comments": []}, {"number": 4051, "title": "549 unsorted segment max", "body": "added unsorted segment max. Fixes #549.\n", "comments": ["Can one of the admins verify this patch?\n", "@vrv updated to current master and opened a new pull request, thanks for your time!\n", "@nikste today way too busy to get to this, and will be out of the office Monday-Tuesday. Co-assigning a few other poor unsuspecting team-mates for now, but will grab again on Wednesday if nobody else reviews it before.\n", "@rmlarsen I'm out Monday-Tuesday for possibly the same reason.\n", "Adding ashish back to the reviewers list.  I would suggest using clang-format --style=google to get the formatting more consistent with the rest of the codebase\n", "used clang-format, hope it looks more consistent now.\n", "@tensorflow-jenkins Test this please\n", "@nikste Looks like this would break the build for linux gpu :(\n", "@danmane, ok thanks, I'll take a look later today..\n", "@danmane this should work now\n", "@nikste Can you fix the conflicts with master please?\n", "@danmane, ok, conflicts are fixed now\n", "@andydavis1, thanks for the comments, you looked at an outdated diff though, you can find the most recent one here: (it uses the functors, as you suggested)\nhttps://github.com/tensorflow/tensorflow/pull/4051/files\n", "@andydavis1 , eventually figured out a way to do this without templates.\n", "Passes the functors as argument (pointers) now, instead of multiple inheritance or Templates.\n", "Jenkins, test this please.\n", "@andydavis1 could you take another look?\n", "I made a couple of small comments/nit picks, but overall looks good as long as its passing all tests.... \n", "Any updates @nikste ? (I'm optimistic we'll finally get this one in)\n", "(can you fix the pylint?)\n", "Hey guys, thx for the review! Unfortunately I only saw this right now and I'll be offline till end of November. I'll pick it back up again end of the month and hope there will be no major code changes till then! ( hope to finish it soon as well)\n", "Any updates on this? I'd love to see this integrated!", "@vrv @drpngx Had some time now to update this to the current master and incorporate the comments.", "Jenkins, test this please.", "@nikste feel free to pull rebase and push again. It looks like we were good on principle about this PR, it would be sad to see it go.", "Closing to tidy up. Feel free to reopen if/when this gets updated.", "If it's okay for @nikste , I'd continue his work?"]}, {"number": 4050, "title": "Possible improvement in documentation", "body": "I'm following the documentation [here](https://www.tensorflow.org/versions/r0.10/how_tos/adding_an_op/index.html#compiling_the_kernel_for_the_gpu_device). Specifically, I find that running the second command in the code block of the section \"Compiling the kernel for the GPU device\" returns a linker error complaining about the lack of the `lcudart` library. I have solved this issue by adding the option `-L /usr/local/cuda-8.0/lib64/` to the command, since the `libcudart.so` library is located in that folder on my machine. I'm not sure how else this library would be located (LD_LIBRARY_PATH is only searched on execution, not compile-time), so I'm thinking this problem might not be specific to my configuration. If so, adding mention of this option on the documentation might be valuable.\n", "comments": ["@zplizzi  Could you please let me know what the value was of `$TF_INC` as per the following step in the instructions:\n\n```\nTF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')\n```\n\n@keveman  Should there also be an equivalent `$TF_LIB` path? \n", "We do have `tf.sysconfig.get_lib()` that gives you the path to use for the `-L` option. However that's for the TensorFlow libraries. These instructions assume that cuda libraries are available in `/usr/local/lib64` which is typical if you install cuda with the default options. In that case, you wouldn't need to pass the extra `-L` to point to the location of cuda libraries. But @zplizzi does have a point, it might be valuable to add that to the documentation. @zplizzi do you mind sending out a PR?\n", "Okay, I've submitted a PR adding a note in the docs about this.\n\n@prb12 My `$TF_INC` is `/home/zack/venv/2/local/lib/python2.7/site-packages/tensorflow/include`, since I've installed TensorFlow into a virtual environment. The result of `tf.sysconfig.get_lib()` is `/home/zack/venv/2/local/lib/python2.7/site-packages/tensorflow/core`, as expected from @keveman's comments.\n"]}, {"number": 4049, "title": "Feature/index based unpooling 3", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Sorry, accidentally opened the pull request, was supposed to be against my fork...\n"]}, {"number": 4048, "title": "Compute Capability 3.0 not working with latest docker builds", "body": "Using the most recent docker image 'tensorflow/tensorflow:nightly-devel-gpu' doesn't work on AWS EC2 anymore, it results in the below error:\n\nIgnoring gpu device (device: 0, name: GRID K520, pci bus id: 0000:00:03.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\n", "comments": ["Building from scratch yesterday had no issues on AWS. If this is the plan moving forward though thats ridiculous. Tf needs to be able to run on AWS.\n", "Thanks, building Dockerfile.gpu from scratch now, will report back.\n", "I got further this time but now have the following error:\n\nLoaded runtime CuDNN library: 5103 (compatibility version 5100) but source was compiled with 4007 (compatibility version 4000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\n\nWhy does the docker use cudnn v5 when tensorflow isn't being built for this?\n", "Yeah, you do need to update CuDNN.\n\nhttps://developer.nvidia.com/compute/machine-learning/cudnn/secure/v5.1/prod/7.5/cudnn-7.5-linux-x64-v5.1-tgz\n", "I think the cuda auto configuration change had a bug that didn't properly propagate the cuda capabilities env variable, but we cherry-picked that into r0.10 in https://github.com/tensorflow/tensorflow/commit/761e6de4357786bee049e63e227b03d57f18bab0\n\nso hopefully the latest nightly build has that in and it should work.\n", "Is there an official image built with the CUDA compute 3.0 support? It's weird the CI dockerfiles in the repo has the `TF_CUDA_COMPUTE_CAPABILITIES 3.0,5.2` but not for those in `tools/docker`\n", "The pip wheels are built with CC3.0 support, and the Dockerfile.gpu does use those pips, so the GPU docker images should include CC3.0 support.\n\nHowever, the devel-gpu dockerfile does not set `TF_CUDA_COMPUTE_CAPABILITIES=3.0,3.5,5.2`, but it should. A PR would be welcome.\n", "@martinwicke Thanks. I'll try the binary one. Just submit a tiny PR [#4301](https://github.com/tensorflow/tensorflow/pull/4301) adding the env in devel-gpu.\n", "The PR referred to above should have addressed this issue. This should have been reflected in the latest* and nightly* tags on gcr.io and Docker Hub now. Please re-open this issue if there are any remaining problems. Thanks."]}, {"number": 4047, "title": "GPU usage is extremely unstable in distributed setup on aws", "body": "### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN:  CUDA 7.5, cuDNN 4.0\n\nIf installed from binary pip package, provide:\nInstalled from the following pip package: (0.10.0rc)\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n### Steps to reproduce\n\nI'm running distributed tf with GPU on 3 machines (the ps machine doesn't have GPU). The commands to start the cluster:\nOn ps machine0:\n\n```\npython async.py \\\n--job_name='ps' \\\n--task_id=0 \\\n--ps_hosts='machine0:2222' \\\n--worker_hosts='machine1:2222,machine2:2222'\n```\n\nOn g2.2xlarge machine 1:\n\n```\npython async.py \\\n--job_name='worker' \\\n--task_id=0 \\\n--ps_hosts='machine0:2222' \\\n--worker_hosts='machine1:2222,machine2:2222'\n```\n\nOn g2.2xlarge machine 2:\n\n```\npython async.py \\\n--job_name='worker' \\\n--task_id=1 \\\n--ps_hosts='machine0:2222' \\\n--worker_hosts='machine1:2222,machine2:2222'\n```\n\nThe code:  [async.txt](https://github.com/tensorflow/tensorflow/files/437980/cifar10_async_dist_train.txt)\n\nFor the first few thousands of iteration, it worked perfectly fine, but the GPU usage drop to almost 0 after that. (actually it fluctuated a lot from 80 -> 50 ->0 ->80)\n### What have you tried?\n1.  I tried to restart the works and it worked for a few rounds then the problem come over again.\n### Logs or other output that would be helpful\n\nThe log after restart. The speed for the first few rounds is ~0.36 sec/batch and the GPU usage is ~90%. But the speed drop down to ~1 sec/batch quickly.\n\n```\n2016-08-25 20:04:08.950429: step 110 (global_step 8318), loss = 0.85 (354.3 examples/sec; 0.361 sec/batch)\n2016-08-25 20:04:12.356555: step 120 (global_step 8338), loss = 0.77 (381.3 examples/sec; 0.336 sec/batch)\n2016-08-25 20:04:15.832075: step 130 (global_step 8358), loss = 0.93 (369.8 examples/sec; 0.346 sec/batch)\n2016-08-25 20:04:19.269496: step 140 (global_step 8377), loss = 0.92 (345.1 examples/sec; 0.371 sec/batch)\n2016-08-25 20:04:22.709284: step 150 (global_step 8398), loss = 1.00 (383.3 examples/sec; 0.334 sec/batch)\n2016-08-25 20:04:26.071762: step 160 (global_step 8418), loss = 0.75 (464.1 examples/sec; 0.276 sec/batch)\n2016-08-25 20:04:29.580850: step 170 (global_step 8438), loss = 0.99 (359.5 examples/sec; 0.356 sec/batch)\n2016-08-25 20:04:33.029791: step 180 (global_step 8457), loss = 1.04 (357.4 examples/sec; 0.358 sec/batch)\n2016-08-25 20:04:36.491314: step 190 (global_step 8478), loss = 0.93 (358.7 examples/sec; 0.357 sec/batch)\n2016-08-25 20:04:39.939553: step 200 (global_step 8498), loss = 1.08 (358.7 examples/sec; 0.357 sec/batch)\n2016-08-25 20:04:43.318268: step 210 (global_step 8518), loss = 1.09 (363.6 examples/sec; 0.352 sec/batch)\n2016-08-25 20:04:46.716990: step 220 (global_step 8538), loss = 0.98 (411.3 examples/sec; 0.311 sec/batch)\n2016-08-25 20:04:50.069852: step 230 (global_step 8557), loss = 0.95 (433.2 examples/sec; 0.295 sec/batch)\n2016-08-25 20:04:53.607222: step 240 (global_step 8577), loss = 1.02 (364.5 examples/sec; 0.351 sec/batch)\n2016-08-25 20:04:57.162343: step 250 (global_step 8597), loss = 0.96 (357.5 examples/sec; 0.358 sec/batch)\n2016-08-25 20:05:00.765804: step 260 (global_step 8617), loss = 1.07 (329.9 examples/sec; 0.388 sec/batch)\n2016-08-25 20:05:04.435059: step 270 (global_step 8637), loss = 0.85 (332.6 examples/sec; 0.385 sec/batch)\n2016-08-25 20:05:08.196024: step 280 (global_step 8656), loss = 0.89 (306.1 examples/sec; 0.418 sec/batch)\n2016-08-25 20:05:12.412322: step 290 (global_step 8676), loss = 1.10 (268.2 examples/sec; 0.477 sec/batch)\n2016-08-25 20:05:21.793992: step 300 (global_step 8696), loss = 0.72 (119.5 examples/sec; 1.071 sec/batch)\n2016-08-25 20:05:32.548282: step 310 (global_step 8716), loss = 0.80 (119.6 examples/sec; 1.070 sec/batch)\n2016-08-25 20:05:43.400207: step 320 (global_step 8736), loss = 1.03 (116.0 examples/sec; 1.104 sec/batch)\n2016-08-25 20:05:54.547412: step 330 (global_step 8755), loss = 0.84 (114.6 examples/sec; 1.117 sec/batch)\n2016-08-25 20:06:05.457404: step 340 (global_step 8775), loss = 1.09 (119.4 examples/sec; 1.072 sec/batch)\n2016-08-25 20:06:16.434271: step 350 (global_step 8795), loss = 0.83 (115.2 examples/sec; 1.111 sec/batch)\n2016-08-25 20:06:27.296998: step 360 (global_step 8815), loss = 0.90 (116.7 examples/sec; 1.097 sec/batch)\n2016-08-25 20:06:38.130229: step 370 (global_step 8835), loss = 0.99 (119.2 examples/sec; 1.074 sec/batch)\n2016-08-25 20:06:48.918992: step 380 (global_step 8855), loss = 1.09 (115.5 examples/sec; 1.108 sec/batch)\n2016-08-25 20:07:00.132937: step 390 (global_step 8874), loss = 0.87 (119.0 examples/sec; 1.076 sec/batch)\n```\n", "comments": ["@perhapszzy  I presume you're following the tutorial code from [here](https://www.tensorflow.org/versions/r0.10/how_tos/distributed/index.html)?\n\nIt looks like you have mistyped several of the command lines, especially the `worker_hosts` and `ps_hosts` args:\n`--worker_hosts='machine2:2222,machine2:2222'`\n- It's unclear what machine the PS is running on (is there a `machine0` or `machine3`?)\n- You start 'worker' processes on `machine1` and `machine2`\n- You tell the workers that the PS is on `machine1` so they connect to the first worker instead!\n- You tell each worker about the process on `machine2` twice, but don't mention `machine1`\n\nI'm not 100% sure what would happen in this case, i.e. using the same worker twice (@mrry Any ideas?) ...but it's unlikely you are going to scale well, since \na) The parameter service is unused\nb) You are either using just one worker, or trying to run two training steps simultaneously on the same worker.\n\nPlease can you double check these command lines and let me know if it fixes the problem.\nIt's very unlikely that there is a bug/issue here, though we could arguably add some argument sanity checking to spot this sort of configuration mistake.\n", "Ops, I mistyped the machines. I updated the cmd used.\nShouldn't have problem setting the cluster up. And I tried no matter how many workers are used, the speed slowed down after \uff5e10K iteration.\n\nI tried 2 workers and 1 worker. They slowed down at ~10k iteration (for 2 workers, each worker finished ~5k iteration, for 1 worker, it slowed down after 10k iteration).\n", "Can you try with the nightly build? The release candidate 0.10.0rc0 has a memory leak in the distributed code that could be causing the slowdown.\n", "Thanks for the advice. I'll try it today.\n", "I tried the following methods, none of them helped:\n1. tried to run the code using docker image tensorflow/tensorflow:nightly-gpu, the behavior is almost the same. Slowed down after ~10k iteration\n2. tried to use 0.9.0, with pip package at \n\n```\n https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl\n```\n\nThis slows down even faster. More over, the GPU usage is only ~30% at beginning and ~10% after slowed down. However, the speed is ~0.160-0.250 sec/batch; while for the 0.10.0rc version, GPU usage is ~90% but the speed is only 0.330-0.360 sec/batch. I'll report this problem in another issue. Anyhow, the slowing down problem still exists.\n\nI'll try to install tf from lastest source code and see if that helps or not. Will report later.\n\nusing nightly build didn't help. \n", "What is your memory usage like?  I have a totally random guess that this is caused by memory fragmentation and swapping.\n\nTry:\n- Enabling TCMalloc in your binary (Google how to do this)\n- Turning off swap so it hard fails when you run out of memory\n", "@mrry If I install from the master branch, the code hang at prepare_or_wait_for_session.\n", "@vrv will try this and report later.\n", "@vrv There's the memory status:\n\nKiB Mem:  15400928 total, 11319472 used,  4081456 free,   456428 buffers\nKiB Swap:        0 total,        0 used,        0 free.  7998028 cached Mem\n", "if I turn off swap using `sudo swapoff -a` it's still running\n", "Hm, did you try tcmalloc?\n", "Still trying to learn how to use it.\n", "Still have trouble installing the gperftools ... But I found if I don't run cifar10_eval with the training, the slow down became less severe. It only slows down from time to time\n\n```\n2016-08-27 00:06:55.565748: step 16220 (global_step 16220), loss = 0.92 (409.7 examples/sec; 0.312 sec/batch)\n2016-08-27 00:06:59.056398: step 16230 (global_step 16230), loss = 0.86 (336.3 examples/sec; 0.381 sec/batch)\n2016-08-27 00:07:03.097942: step 16240 (global_step 16240), loss = 0.82 (366.3 examples/sec; 0.349 sec/batch)\n2016-08-27 00:07:08.687517: step 16250 (global_step 16250), loss = 0.97 (52.4 examples/sec; 2.443 sec/batch)\n2016-08-27 00:07:12.856141: step 16260 (global_step 16260), loss = 1.05 (354.0 examples/sec; 0.362 sec/batch)\n2016-08-27 00:07:17.285705: step 16270 (global_step 16270), loss = 0.79 (101.0 examples/sec; 1.268 sec/batch)\n2016-08-27 00:07:22.531608: step 16280 (global_step 16280), loss = 0.83 (156.5 examples/sec; 0.818 sec/batch)\n2016-08-27 00:07:26.008164: step 16290 (global_step 16290), loss = 0.79 (380.7 examples/sec; 0.336 sec/batch)\n2016-08-27 00:07:31.529857: step 16300 (global_step 16300), loss = 0.92 (379.1 examples/sec; 0.338 sec/batch)\n2016-08-27 00:07:35.004310: step 16310 (global_step 16310), loss = 1.10 (358.1 examples/sec; 0.357 sec/batch)\n2016-08-27 00:07:39.766474: step 16320 (global_step 16320), loss = 0.85 (368.9 examples/sec; 0.347 sec/batch)\n2016-08-27 00:07:44.046154: step 16330 (global_step 16330), loss = 0.90 (413.7 examples/sec; 0.309 sec/batch)\n2016-08-27 00:07:49.655521: step 16340 (global_step 16340), loss = 0.71 (170.9 examples/sec; 0.749 sec/batch)\n2016-08-27 00:07:54.041737: step 16350 (global_step 16350), loss = 0.85 (171.1 examples/sec; 0.748 sec/batch)\n2016-08-27 00:07:57.530356: step 16360 (global_step 16360), loss = 0.88 (382.9 examples/sec; 0.334 sec/batch)\n```\n", "After start cifar10_eval, the speed slowed down dramatically even is cifar10_eval is not active for most of time.\n\n```\n2016-08-27 00:41:21.811675: step 20550 (global_step 20550), loss = 0.71 (236.8 examples/sec; 0.540 sec/batch)\n2016-08-27 00:41:27.253400: step 20560 (global_step 20560), loss = 0.97 (223.2 examples/sec; 0.573 sec/batch)\n2016-08-27 00:41:32.632130: step 20570 (global_step 20570), loss = 0.82 (237.2 examples/sec; 0.540 sec/batch)\n2016-08-27 00:41:38.078019: step 20580 (global_step 20580), loss = 0.92 (222.5 examples/sec; 0.575 sec/batch)\n2016-08-27 00:41:43.456795: step 20590 (global_step 20590), loss = 0.67 (235.5 examples/sec; 0.543 sec/batch)\n2016-08-27 00:41:48.906125: step 20600 (global_step 20600), loss = 0.76 (222.7 examples/sec; 0.575 sec/batch)\n2016-08-27 00:41:54.273064: step 20610 (global_step 20610), loss = 0.82 (238.0 examples/sec; 0.538 sec/batch)\n2016-08-27 00:41:59.687229: step 20620 (global_step 20620), loss = 0.70 (236.5 examples/sec; 0.541 sec/batch)\n2016-08-27 00:42:05.096757: step 20630 (global_step 20630), loss = 0.86 (237.3 examples/sec; 0.539 sec/batch)\n2016-08-27 00:42:11.584223: step 20640 (global_step 20640), loss = 0.95 (253.4 examples/sec; 0.505 sec/batch)\n2016-08-27 00:42:17.028177: step 20650 (global_step 20650), loss = 0.73 (222.9 examples/sec; 0.574 sec/batch)\n```\n", "Are the two processes sharing the same GPU?\n", "The machine only has 1 GPU, but for the cifar10_eval, I just run it using CPU by specifying `tf.device(\"/cpu:0\")`\n", "So maybe it's only slowing down when the two are doing computation at the same time?\n", "No, even the eval is sleeping, the train process is slowed down.\n", "Weird, I'm not really sure then -- you'll probably have to do some performance debugging to figure out what's going on.  The fact that it seems to run okay with just one process suggests inter-process interference.\n", "I tried to start to workers on different machine, only 1 machine run the eval job, but the speed of another machine also slowed down. I suspected the problem might be caused by ps server, but the cpu & mem usage for the ps server were very low, so it's unlikely that the problem is caused by ps server\n", "I see the same performance problem on my benchmark. I use one ps server and one worker with synchronized parameter update (SyncReplicasOptimizer). I expect no synchronization overhead because there is only one worker, but the worker throughput(examples/sec) is 0.57x in average than the case not using sync optimizer. Also the throughput fluctuates very much, just like what @perhapszzy posted.\n", "@perhapszzy @chhwang are you still seeing the same problems?  One thing to try is TensorFlow version 0.11, just to make sure we're not dealing with an issue that's been fixed already.\n\nhttps://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#download-and-setup\n", "@chhwang I've been doing similar debugging recently and on a [simple benchmark](https://gist.github.com/yaroslavvb/1124bb02a9fd4abce3d86caf2f950cb2) I observing that transferring data between TF workers on same machine is about 500-1GB/second, while making data available to Python client in another process on same machine is <100MB/second\n", "@yaroslavvb I'm not exactly sure how is your benchmark related to my problem. I think my problem is that the SyncReplicasOptimizer itself gives a significant overhead, and it is not the communication overhead. Anyway, it was a few months ago, so let me test it again with the recent version of TensorFlow as @tatatodd suggested. \n", "@chhwang : Were you able to get more recent results?\n", "@asimshankar @tatatodd  Sorry for late response. Actually I couldn't test exactly the same benchmark as before because my batchnorm layers (which worked well with TF version 0.9.0) returned \"Incompatible shapes\" error with TF version 0.10.0, and I don't have enough time to work on this...\n\nThus my new benchmark removed batchnorm layers and also used newer TF/CUDA version (0.9.0 -> 0.10.0 / 7.5 -> 8.0) and GPU (GTX Titan X -> GTX 1080). With this setting, I can't see those problems I found before. Using SyncReplicasOptimizer doesn't affect the training throughput and the worker throughput (trained samples/sec) is very consistent now.\n", "Thanks for the update. Since quite a bit of time has passed and we can't reproduce the original problem anymore, I'm going to go ahead and close this issue. If you run into this or something like it again, please do file a new one.\n\nThanks!\n"]}, {"number": 4046, "title": "Fuse resize and mirror padding ops into convolutions", "body": "Spatial transformations like padding and bilinear resizing can be merged into the im2col stage of conv2d. This reduces the memory usage considerably (from 338MB to 224MB) and latency (by 15%) on some models, and helps us avoid OOM crashes on iOS. This PR has all the changes needed to fuse these particular ops, including the kernels themselves and integration into the optimize_for_inference script.\n", "comments": ["Chad has agreed to review this.\n", "Should conv_ops_fused go in contrib instead?  In core, there is a higher commitment to keep this approach around longer.\n", "We switched over to reviewing this internally, and have now got the changes checked in there, so they should appear here in github soon. Closing this PR.\n"]}, {"number": 4045, "title": "Switch 'friend class DimensionOrConstant' to use struct, to match act\u2026", "body": "\u2026ual declaration.\n", "comments": ["Can one of the admins verify this patch?\n", "@rmlarsen can you merge if this looks okay?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 4044, "title": "tf.import_graph_def: graph_def is invalid at node", "body": "I've been trying to import a frozen graph into a new program, and do a simple forward pass, but `tf.import_graph_def` has been throwing a ValueError that I really can't make sense of.\n### Environment info\n\nOperating System: Ubuntu 14.04 LTS 64-bit\n\nInstalled version of CUDA and cuDNN: none\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`): fc9162975e52978d3af38549b570cc3cc5f0ab66\n2. The output of `bazel version`\n\n```\nBuild label: 0.3.0\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jun 10 11:38:23 2016 (1465558703)\nBuild timestamp: 1465558703\nBuild timestamp as int: 1465558703\n```\n### Steps to reproduce\n1. Copy the IPython Notebook from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb)\n2. Change `sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))` to `sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b), name=\"sample_prediction\")`\n3. Modify the code like so:\n\n``` python\nwith tf.Session(graph=graph) as session:\n  tf.initialize_all_variables().run()\n  print('Initialized')\n  mean_loss = 0\n  # code omitted (no changes)\n  # new code below:\n  saver = tf.train.Saver(tf.all_variables())\n  saver.save(session, '/home/me/Documents/checkpoint.ckpt', write_meta_graph=False)\n  tf.train.write_graph(graph.as_graph_def(), '/home/me/Documents', 'graph.pb')\n```\n1. Run, and verify that `checkpoint.ckpt` and `graph.pb` have been created\n2. Run `bazel build tensorflow/python/tools:freeze_graph && bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/home/me/Documents/graph.pb --input_checkpoint=/home/me/Documents/checkpoint.ckpt --output_graph=/home/me/Documents/frozen_graph.pb --output_node_names=sample_prediction`\n3. Verify that `frozen_graph.pb` has been created\n4. Create a new IPython Notebook with the following code:\n\n``` python\nfrom __future__ import print_function\nimport os\nimport numpy as np\nimport random\nimport string\nimport tensorflow as tf\nfrom tensorflow.python.platform import gfile\nimport zipfile\nfrom six.moves import range\nfrom six.moves.urllib.request import urlretrieve\n\ngraph = tf.Graph()\nwith graph.as_default():\n    graph_def = tf.GraphDef()\n    with open('/home/me/Documents/frozen_graph.pb', \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n        sample_prediction = tf.import_graph_def(graph_def, name=\"\", return_elements=['sample_prediction:0'])\n```\n1. Run\n### What have you tried?\n1. Originally, the graph also contained a node named `saved_sample_output`, and when I tried importing that frozen graph, the error complained about `saved_sample_output:0`. I tried removing the name, re-writing the checkpoint and graph files, re-freezing, and re-running the code. It then complained about `Variable_17:0`, which, after checking `graph.pb`, was what had originally been named `saved_sample_output`. Other than that, I haven't been able to find anything else out.\n2. Checked out #616 and looked at the solutions suggested for similar errors, but my `import_graph_def` never had an input map to begin with.\n3. Removing the name parameter, or the return_elements parameter, or both, hasn't made a difference.\n### Logs or other output that would be helpful\n\n```\nValueError                                Traceback (most recent call last)\n<ipython-input-46-3423c2073e62> in <module>()\n     53     with open('/home/me/Documents/frozen_graph.pb', \"rb\") as f:\n     54         graph_def.ParseFromString(f.read())\n---> 55         sample_prediction = tf.import_graph_def(graph_def, name=\"\", return_elements=['sample_prediction:0'])\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.pyc in import_graph_def(graph_def, input_map, return_elements, name, op_dict)\n    318           except TypeError as te:\n    319             raise ValueError(_InvalidNodeMessage(\n--> 320                 node, 'Input tensor %r %s' % (input_name, te)))\n    321 \n    322       # pylint: disable=protected_access\n\nValueError: graph_def is invalid at node u'Assign_4': Input tensor 'Variable_17:0' Cannot convert a tensor of type float32 to an input of type float32_ref.\n```\n", "comments": ["Hi, has there been any progress on this problem ? As I am getting similar symptoms.", "I've had the same problem, can anyone give a solution?", "Looks like a bug in freeze graph, right @petewarden ?", "I'm also experiencing this problem when trying to load a previously frozen facenet graph. Is there any progress on this? Is there a way to help debug this issue?", "Come across a similar issue when import_graph_def(graph_def, name=\"\"), any solution?\r\n------------\r\nMy error was caused by an incorrect \"graph.pb\" which was saved by the following script, but I used a wrong [output_node]. So when I tried to import the \"incorrect\" graph, it gives me the error message above. Hope this helps in case anyone has made the same silly mistake as I.\r\n\r\nfrom tensorflow.python.framework import graph_util\r\n    output_graph_def = graph_util.convert_variables_to_constants(sess,graph_def,[output_node])\r\nwith tf.gfile.GFile('graph1.pb',\"wb\") as f:\r\n    f.write(output_graph_def.SerializeToString())", "Does anyone have a simple repro case with the simplest graph possible?", "I have the same issue, \r\nI am using tensorflow 1 python 3.5 ubuntu 16.10\r\n\r\n    node, 'Input tensor %r %s' % (input_name, te)))\r\nValueError: graph_def is invalid at node 'word_embeddings/Variable/Assign': Input tensor 'word_embeddings/Variable:0' Cannot convert a tensor of type float32 to an input of type float32_ref.\r\n", "Closing since we don't have a simple repro case.", "How do we know the output_node name for my trained model? @hellowangqian ", "Any update on the error above?\r\nI am getting the error below:\r\n\r\nValueError: graph_def is invalid at node 'lstm_3/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3': Input tensor 'lstm_3/TensorArray_1:0' Cannot convert a tensor of type resource to an input of type float32.\r\n", "please reopen,", "I've encountered the same problem, using the example model from `tensorflow/tensorflow/examples/learn/text_classification_character_cnn.py`.\r\n\r\nAfter `greeze_graph` and `optimize_for_inference` I got the error message:\r\n`ValueError: graph_def is invalid at node 'one_hot': Input tensor 'random_shuffle_queue_DequeueUpTo:1' not found in graph_def..`\r\n\r\nOnly `greeze_graph` the loading the pb model works fine.", "It's possibly because you trained and freezed a model using an old version tensorflow, and then you want to import the graph using a new version one.\r\nTry to keep tensorflow version consistency.\r\nPlease refer to [this issue](https://github.com/davidjesusacu/polyrnn-pp/issues/3).", "Nagging Assignee @petewarden: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This issue was created for an old TensorFlow version. If you still face the same problem with the new version, please file a new issue.", "@Kongsea @petewarden @wt-huang  I have maintained version consistency , but still i am not able to solve it \r\nOperating System: Ubuntu 14.04 LTS 64-bit\r\nTensorflow - 1.4.0\r\nSo i have trained a model , froze it and performed optimization (https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/) , graph transformation (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms)  on tensorflow 1.4.0 but when it comes to quantization i face following error:\r\n\r\nGraph_def is invalid at node u'ExpandDims': Input tensor 'image_ph:0' Cannot convert a tensor of type float32 to an input of type int32.\r\n"]}, {"number": 4043, "title": "Branch 131308917", "body": "", "comments": []}, {"number": 4042, "title": "Fix flaky test: Add small absolute tolerance so test with expected zero outputs do not fail.", "body": "", "comments": ["@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "FYI, the status of the mac PR test is not shown below but can be seen at: http://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/1662/\n", "Thanks for the fix\n"]}, {"number": 4041, "title": "Cherry-pick into r0.10 cuda configure fix and gif.BUILD missing headers", "body": "", "comments": ["Doh, I squashed them instead of cherry-picking them individually.  Oh well :/\n"]}, {"number": 4040, "title": "fix small typo in contrib.metrics.confusion_matrix docs, and use less\u2026", "body": "\u2026 confusing variable\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "CLA signed\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "What do you have against the letter l?\n", "When rendered by github, in this context, the letter I looks like a vertical bar:\n\n![image](https://cloud.githubusercontent.com/assets/1590761/17975748/e3cc5f9e-6ab8-11e6-9182-83ff4c0e084d.png)\n\nand this makes it appear unfortunately like incomplete [density matrix notation](https://en.wikipedia.org/wiki/Density_matrix).\n", "OK, I don't object to k either.\n"]}, {"number": 4039, "title": "Fix docker tag step in parameterized_docker_build.sh", "body": "", "comments": []}]