[{"number": 17426, "title": "tf-slim resnet-50 pretrained model get wrong results when inference", "body": "I have asked this question on stackoverflow, but no one answer. Does any can help me.\r\nThe question link to stackoverflow is [here](https://stackoverflow.com/questions/49094123/tf-slim-resnet-pretrained-model-cant-get-correct-results).\r\nThe following is the code I used to do inference.The image preprocess method following this issue [ResNet pre-processing: VGG or Inception?](https://github.com/tensorflow/models/issues/2217)\r\n```Python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim.nets as nets\r\nimport imagenet\r\nimport urllib.request\r\nfrom preprocessing import inception_preprocessing\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nslim = tf.contrib.slim\r\nresnet = nets.resnet_v1\r\n\r\nif __name__ == '__main__':\r\n    ckpt_file_path = '../model_weights/resnet_v1_50.ckpt'\r\n    url = 'https://upload.wikimedia.org/wikipedia/commons/7/70/EnglishCockerSpaniel_simon.jpg'\r\n    image_string = urllib.request.urlopen(url).read()\r\n    image = tf.image.decode_jpeg(image_string, channels=3)\r\n    processed_image = inception_preprocessing.preprocess_image(image, 224, 224, is_training=False)\r\n    processed_images = tf.expand_dims(processed_image, 0)\r\n    with slim.arg_scope(nets.resnet_utils.resnet_arg_scope()):\r\n        resnet_50, end_points = resnet.resnet_v1_50(inputs=processed_images, num_classes=1000, scope='resnet_v1_50')\r\n        prob = tf.squeeze(resnet_50, axis=[1, 2])\r\n    probabilities = tf.nn.softmax(prob, dim=-1)\r\n    sess = tf.Session()\r\n    saver = tf.train.Saver()\r\n    saver.restore(sess, ckpt_file_path)\r\n    np_image, results = sess.run([image, probabilities])\r\n    results = results[0, 0:]\r\n\r\n    plt.figure()\r\n    plt.imshow(np_image.astype(np.uint8))\r\n    plt.axis('off')\r\n    plt.show()\r\n\r\n    sorted_inds = [i[0] for i in sorted(enumerate(-results), key=lambda x: x[1])]\r\n    names = imagenet.create_readable_names_for_imagenet_labels()\r\n    for i in range(5):\r\n        index = sorted_inds[i]\r\n        print('Probability %0.2f%% => [%s]' % (results[index] * 100, names[index]))\r\n```\r\nThe following is result:\r\n```\r\nProbability 1.00% => [moving van]\r\nProbability 0.69% => [television, television system]\r\nProbability 0.63% => [English foxhound]\r\nProbability 0.63% => [beagle]\r\nProbability 0.61% => [German short-haired pointer]\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OS platform  Ubuntu 16.04\r\nTensorflow installed from   pip \r\nTensorflow version    1.4.1\r\nBazel version    0.8\r\nCUDA/cuDNN version   CUDA 8.0  cuDNN 6.0\r\nGPU model and memory  GTX 1060 6G\r\n", "@auroua : It must be vgg preprocessing as default. Your code almost correct, except the preprocessing", "@John1231983 The preprocessing method is following the issue  [ResNet pre-processing: VGG or Inception?](https://github.com/tensorflow/models/issues/2217). I will test the vgg preprocessing.", "The following is result when I changed the preprocessing method from `inception_preprocessing` to `vgg_preprocessing`.\r\n```\r\nProbability 0.96% => [moving van]\r\nProbability 0.71% => [television, television system]\r\nProbability 0.67% => [broom]\r\nProbability 0.62% => [German short-haired pointer]\r\nProbability 0.62% => [sundial]\r\n```\r\n\r\nThis is the modified code:\r\n```Python\r\n    processed_image = vgg_preprocessing.preprocess_image(image, 224, 224, is_training=False)\r\n    # processed_image = inception_preprocessing.preprocess_image(image, 224, 224, is_training=False)\r\n    processed_images = tf.expand_dims(processed_image, 0)\r\n```\r\n\r\nI think the results is not correct.", "Automatically closing due to lack of recent activity. ", "This problem still exists. Can you solve it? @tatianashp ", "@tatianashp I want to contribute to this issue can anyone help me with this.\r\n", "I think this should be reported on https://github.com/google-research/tf-slim, not here."]}, {"number": 17425, "title": "Correct reporter name.", "body": "", "comments": []}, {"number": 17424, "title": "documentation on how to use a Tensorflow model trained using the Estimator & Dataset APIs on an Android app.", "body": "I am having a hard time finding documentation on how to use a Tensorflow model trained using the Estimator & Dataset APIs on an Android app.\r\n\r\nHaving spent hours searching for it and turned out nothing, and maybe there's a many others having the same issue, and the answer could be just a two-minutes thing for people who knows. I am raising it as an issue here. \r\n\r\nCan someone point me to a good reference and/or tutorial? I looked at the TensorflowInferenceInterface, but my understanding is it need you to specify which operator you want to feed the input to, but the Estimator/Dataset abstraction is at another level. So I am somewhat lost here.\r\n\r\nThanks.\r\n -Jerry\r\n\r\nThe version of Tensorflow I am using is v1.6.0-0-gd2e24b6039 1.6.0\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Seriously, such a robotic response is just not genuine!", "Same as https://github.com/tensorflow/tensorflow/issues/17509#issuecomment-371493772\r\n\r\nCan we improve the bot with some semantics in the template? /cc @martinwicke @av8ramit", "@rjt10 that is our standard bot reply. I'll improve the semantics and try and disable it for feature requests. ", "@asimshankar @angersson do we have a relevant Android tutorial?", "I don't believe we currently do have any explicit tutorial showing the path from Estimator to Android. I'd imagine that the path would involve something like `export_savedmodel`, followed perhaps by some graph freezing.\r\n\r\n@petewarden @suharshs @aselle may be in a better position to chime in.", "@gargn @sidneyyan ", "I got as far as using the export_savedmodel and write out a checkpoint/exported model. I ran the freeze graph script and it ran through with no error and no saved pb file. \r\nWhat are the correct options to pass to the freeze_graph script? I gave it a checkpoint and the \"graph.pbtxt\" from the export_savedmodel. ", "We're now deprecating the mainline TensorFlow build on Android and iOS in favor of TF Lite. You should be able to find better documentation on this approach here:\r\nhttps://www.tensorflow.org/lite"]}, {"number": 17423, "title": "image_retraining example does not work", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13.3\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430d84 1.5.0\r\n- **Python version**: 3.6.4\r\n- **Exact command to reproduce**:\r\n      python3 tensorflow/examples/image_retraining/retrain.py \\\r\n      --image_dir ~/Resources/tf-retrain-images/ \\\r\n      --learning_rate=0.0001 \\\r\n      --testing_percentage=10 \\\r\n      --validation_percentage=10 \\\r\n      --train_batch_size=32 \\\r\n      --validation_batch_size=-1 \\\r\n      --flip_left_right True \\\r\n      --random_scale=30 \\\r\n      --random_brightness=30 \\\r\n      --eval_step_interval=100 \\\r\n      --how_many_training_steps=500 \\\r\n      --architecture mobilenet_0.25_224\r\n\r\n### Describe the problem\r\n`retrain.py` fails with the error below when it starts to create bottleneck files for testing datasets after training is done. Looks like something wrong with making bottleneck files for test images.\r\nFYI, it works when I checkout `retrain.py` back to commit dce9a49c.\r\n\r\n### Source code / logs\r\n...\r\nINFO:tensorflow:2018-03-05 13:31:17.056049: Step 90: Validation accuracy = 89.0% (N=73)\r\nINFO:tensorflow:2018-03-05 13:31:25.350794: Step 99: Train accuracy = 96.9%\r\nINFO:tensorflow:2018-03-05 13:31:25.350940: Step 99: Cross entropy = 0.198750\r\nINFO:tensorflow:2018-03-05 13:31:25.398267: Step 99: Validation accuracy = 89.0% (N=73)\r\nModel path:  /tmp/imagenet/mobilenet_v1_0.25_224_frozen.pb\r\nINFO:tensorflow:Restoring parameters from /tmp/_retrain_checkpoint\r\nINFO:tensorflow:Creating bottleneck at /tmp/bottleneck/cat/cat.1.jpg_mobilenet_0.25_224.txt\r\nTraceback (most recent call last):\r\n  File \"/Users/tushuhei/py3env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1070, in _run\r\n    allow_operation=False)\r\n  File \"/Users/tushuhei/py3env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3323, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"/Users/tushuhei/py3env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3402, in _as_graph_element_locked\r\n    raise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\r\nValueError: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 394, in create_bottleneck_file\r\n    resized_input_tensor, bottleneck_tensor)\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 326, in run_bottleneck_on_image\r\n    {image_data_tensor: image_data})\r\n  File \"/Users/tushuhei/py3env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/Users/tushuhei/py3env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1073, in _run\r\n    + e.args[0])\r\nTypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 1486, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/tushuhei/py3env/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 124, in run\r\n    _sys.exit(main(argv))\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 1286, in main\r\n    bottleneck_tensor)\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 881, in run_final_eval\r\n    bottleneck_tensor, FLAGS.architecture))\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 567, in get_random_cached_bottlenecks\r\n    resized_input_tensor, bottleneck_tensor, architecture)\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 442, in get_or_create_bottleneck\r\n    bottleneck_tensor)\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 397, in create_bottleneck_file\r\n    str(e)))\r\nRuntimeError: Error during processing file /Users/tushuhei/Resources/tf-retrain-images/cat/cat.1.jpg (Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.)\r\n", "comments": ["hi,\r\n\r\nI think I found some solution to avoid the error. ( Or mess up with the graph\r\n\r\nI've opened an issue few days ago, but closed by official in a short time.\r\n\r\nNot sure if this is the correct solution. Refer to #17370 \r\n\r\nIn def run_final_eval function\r\nchange\r\n`(sess, bottleneck_input, ground_truth_input, evaluation_step, prediction) = build_eval_session(model_info, class_count)`\r\ninto\r\n`(eval_sess, bottleneck_input, ground_truth_input, evaluation_step, prediction) = build_eval_session(model_info, class_count)`\r\n\r\nand\r\n`test_accuracy, predictions = sess.run( [evaluation_step, prediction], feed_dict={ bottleneck_input: test_bottlenecks, ground_truth_input: test_ground_truth })`\r\ninto\r\n`test_accuracy, predictions = eval_sess.run( [evaluation_step, prediction], feed_dict={ bottleneck_input: test_bottlenecks, ground_truth_input: test_ground_truth })`\r\n\r\nI think the problem is caused by the newly built eval sess is not included the DecodeJPGInput Tensor.\r\nIt's just a guess.", "Same. The latest commit 7a2ba8e seems to have broken the evaluation. Before that commit my code works fine, now my code has error.\r\n\r\n`RuntimeError: Error during processing file`\r\n`/home/graymatics/wanqi/classifier_data/flower_photos/tulips/8712270243_8512cf4fbd.jpg (Cannot  interpret feed_dict key as Tensor: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.)\r\n`\r\n\r\nBy the way the above solution didn't work for me, still waiting for fix.", "I got the same error with:\r\n(tensorflow) mrwelph@mrwelph-810-011eo ~/www/condaTF/tensorflow $ python tensorflow/examples/image_retraining/retrain.py --architecture=mobilenet_0.25_128 --flip_left_right --random_scale=30 --random_crop=30 --how_many_training_steps=500 --image_dir ~/www/takeaway/debugSet \r\n=> strange error\r\nRuntimeError: Error during processing file ... (Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.)\r\n\r\nBut then I tried with several variations with just 3 training steps and noticed that adding random brightness removed the error, so this worked (ok it's not the same exact model but since I'am currently running training I won't test that the error is gone also with that):\r\n\r\npython tensorflow/examples/image_retraining/retrain.py --architecture=mobilenet_0.25_192 --flip_left_right --random_scale=30 --random_crop=30 --how_many_training_steps=3 --image_dir ~/www/takeaway/debugSet --random_brightness=30 \r\n", "@suharshs This issue got lost for a while.  @tensor-flower claims the problem was introduced by a commit you were involved with.", "Sorry we missed this. I think i see the issue and will submit a fix soon. Thanks!", "Adding `--random_brightness=30` didn't work for me, I have to revert code to the parent of [7a2ba8e](https://github.com/tensorflow/tensorflow/commit/7a2ba8edbaa6491ff33ae1412d9ba45e80c2cc3c) then it works.", "I use tf 1.7,also have this problem , I do like lcycoding said ,it's ok", "where can we find /tmp/ folder as formed at the end of bottlenecks? I'm not able to find this directory anywhere on my computer and I believe it's not generated, how can I save the model at the end of bottlenecks?"]}, {"number": 17422, "title": "Get cudnn version 7005 while 7101 installed only", "body": "\r\nubuntu 16.04\r\nGTX 1080ti\r\ncuda 9.1\r\ncudnn 7.1.1 ( libcudnn7_7.1.1.5-1+cuda9.1_amd64.deb & libcudnn7-dev_7.1.1.5-1+cuda9.1_amd64.deb )\r\ntensorflow 1.4.1 compiled from source with cuda 9.1 and cudnn 7.1.1\r\n\r\nI have never downloaded or installed cudnn 7.0.5. \"locate libcudnn.so.7.0\" show nothing.\r\n\r\n2018-03-05 11:06:53.859047: E tensorflow/stream_executor/cuda/cuda_dnn.cc:378] Loaded runtime CuDNN library: 7005 (compatibility version 7000) but source was compiled with 7101 (compatibility version 7100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n2018-03-05 11:06:53.859249: F tensorflow/core/kernels/conv_ops.cc:667] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) \r\nAborted (core dumped)\r\n", "comments": ["Failed trying tensorflow-1.6.0.", "I solved this problem by installing libcudnn 7.0.5 and compiling tensorflow with libcudnn 7.0.5.", "For people with similar issue issue in the future, I recommend installing the version that `tensorflow` was compiled with:\r\n```shell\r\nsudo apt install libcudnn7-dev=7.0.5.15-1+cuda9.1 libcudnn7=7.0.5.15-1+cuda9.1\r\n```\r\nAnd then freezing this version in apt so it will not automatically upgrade them:\r\n```shell\r\nsudo apt-mark hold libcudnn7 libcudnn7-dev\r\n```", "@msis \r\n\r\nI got this:\r\n\r\n```\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nE: Unable to locate package libcudnn7-dev\r\nE: Unable to locate package libcudnn7\r\n```", "@unnir, how did you install `libcudnn*`? \r\nIf you didn't add the nvidia repos to your apt sources, `apt` will not be able to locate them (they are not distributed by default)", "@msis\r\n![tf-compile-with-cudnn71](https://user-images.githubusercontent.com/4844122/38712332-7a2eb67a-3efd-11e8-93ce-7217721b813f.png)\r\n\r\nIt seems that tensorflow code cannot detect runtime CuDNN version correctly. Or maybe it is a bug of libcudnn 7.1 ?", "@HorsonLiu this error: \r\n> 2018-03-05 11:06:53.859047: E tensorflow/stream_executor/cuda/cuda_dnn.cc:378] Loaded runtime CuDNN library: 7005 (compatibility version 7000) but source was compiled with 7101 (compatibility version 7100). If using a binary install, upgrade your CuDNN library to match. If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration. \r\n\r\nclearly says that you're tf is trying to load CuDNN 7.0.5, but it found 7.1.1 . \r\nTherefore, I suppose that you might have 2 versions of CuDNN installed when you were compiling it, and the earlier was automatically selected.", "As of April 2018, and to save a lot of googling and investigating, TF 1.7 and 1.8 are not compatible with CUDA 9.1.  So you have to use 9.0 unfort.\r\nFurthermore, you have to use cuDNN 7.0 NOT 7.1 (at least on a Windows 10 machine)\r\nDon't make the mistake of installing cuDNN 7.1, as that is not yet supported.\r\n\r\nIf you've installed cuDNN 7.1, found it not to be compatible, and downloaded cuDNN 7.0, the CUDA 9.0 probably still has the old cuDNN 7.1 files installed in your CUDA\\v9.0 folder.  There are 3 files in particular you must copy from your cuDNN 7.0 folder into the CUDA\\v9.0 folder to replace the incorrect cuDNN 7.1 versions. cudnn64_7.dll, cudnn.h, & cudnn.lib\r\n\r\nThis is detailed in nVidia's pdf here (pages 6-7):  https://docs.nvidia.com/deeplearning/sdk/pdf/cuDNN-Installation-Guide.pdf\r\n\r\nAfter that, follow the next step to set the CUDA_PATH environment variable.\r\nClick OK, close, restart your machine, and should be good to go.\r\n\r\nDowngrading those 3 files from the cuDNN 7.1 back to the 7.0 was what solved the issue for me.", "I upgrade TF from 1.7.0 to **1.9.0**. The problem solved!\r\n  ", "In my case,` ubuntu 16.04, py3.6`, I have to use `tf1.7`, but my conda package is `cudatoolkit  9.0` and `cudnn     7.3.1`. at first I get error as follows:\r\n`2019-03-18 13:18:32.718709: E tensorflow/stream_executor/cuda/cuda_dnn.cc:396] Loaded runtime CuDNN library: 7301 (compatibility version 7300) but source was compiled with 7102 (compatibility version 7100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.`\r\n\r\nif I simply use:\r\n`conda install cudnn=7.1`\r\n\r\nthis may happen:\r\n```\r\n    cudatoolkit:         9.0-h13b8566_0       --> 8.0-3               \r\n    cudnn:               7.3.1-cuda9.0_0      --> 7.1.3-cuda8.0_0\r\n```\r\n\r\nwhich is not compatible. \r\n\r\nSo I changed the update into:\r\n`conda install tensorflow-gpu=1.7 cudatoolkit=9.0 cudnn=7.1`\r\n\r\nthen it work. Hope this may help\r\n", "> In my case,` ubuntu 16.04, py3.6`, I have to use `tf1.7`, but my conda package is `cudatoolkit 9.0` and `cudnn 7.3.1`. at first I get error as follows:\r\n> `2019-03-18 13:18:32.718709: E tensorflow/stream_executor/cuda/cuda_dnn.cc:396] Loaded runtime CuDNN library: 7301 (compatibility version 7300) but source was compiled with 7102 (compatibility version 7100). If using a binary install, upgrade your CuDNN library to match. If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.`\r\n> \r\n> if I simply use:\r\n> `conda install cudnn=7.1`\r\n> \r\n> this may happen:\r\n> \r\n> ```\r\n>     cudatoolkit:         9.0-h13b8566_0       --> 8.0-3               \r\n>     cudnn:               7.3.1-cuda9.0_0      --> 7.1.3-cuda8.0_0\r\n> ```\r\n> which is not compatible.\r\n> \r\n> So I changed the update into:\r\n> `conda install tensorflow-gpu=1.7 cudatoolkit=9.0 cudnn=7.1`\r\n> \r\n> then it work. Hope this may help\r\n\r\nThe last specified command solved the problem for me. Thanks @javacjh ", "> In my case,` ubuntu 16.04, py3.6`, I have to use `tf1.7`, but my conda package is `cudatoolkit 9.0` and `cudnn 7.3.1`. at first I get error as follows:\r\n> `2019-03-18 13:18:32.718709: E tensorflow/stream_executor/cuda/cuda_dnn.cc:396] Loaded runtime CuDNN library: 7301 (compatibility version 7300) but source was compiled with 7102 (compatibility version 7100). If using a binary install, upgrade your CuDNN library to match. If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.`\r\n> \r\n> if I simply use:\r\n> `conda install cudnn=7.1`\r\n> \r\n> this may happen:\r\n> \r\n> ```\r\n>     cudatoolkit:         9.0-h13b8566_0       --> 8.0-3               \r\n>     cudnn:               7.3.1-cuda9.0_0      --> 7.1.3-cuda8.0_0\r\n> ```\r\n> \r\n> which is not compatible.\r\n> \r\n> So I changed the update into:\r\n> `conda install tensorflow-gpu=1.7 cudatoolkit=9.0 cudnn=7.1`\r\n> \r\n> then it work. Hope this may help\r\n\r\nThis also solved my problem. Thanks!", "I make cudnn from 730 to 704 ; \r\nstill same problem\r\nand then I change tensorflow from 1.5 to 1.9\r\nIt solved every problem\r\n(I use CUDA 9)"]}, {"number": 17421, "title": "Unreasonable prediction on test set with resnet_v2 for specific batch size", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04.3 LTS\r\n- **TensorFlow installed from (source or binary)**: Binary from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp35-cp35m-linux_x86_64.whl\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.5.1+\r\n- **Bazel version (if compiling from source)**: /\r\n- **GCC/Compiler version (if compiling from source)**: / \r\n- **CUDA/cuDNN version**: V7.5.17\r\n- **GPU model and memory**: Tesla M40 12G\r\n- **Exact command to reproduce**: /\r\n\r\n### Describe the problem\r\nI am using resnet_v2 provided in slim to train a classifier. However, there is an unexpected behaviour when I tried to get the predictions on test set and I am sure that I turned off the training mode by pass is_training as False. \r\n\r\nMy model returned unreasonable prediction scores on specific testing mini batch size of 650 and 649. \r\nI randomly checked several testing batch sizes (from 50 - 850, which my GPU can afford ), their predictions on same input data were all consistent with each other, except for size 650 and 649.  Actually even two identical patches in the mini batch had different scores when I am using batch size 650 and 649.  To verify the issue, I also tried to train the model with different input data and different training mini batch size. However, the problem is still there with the same weird mini batch size 650 and 649 :(. \r\n\r\n", "comments": ["@derekjchow: can you take a look?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 45 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 61 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 17420, "title": "grad of variable size extract_image_patches", "body": "Fixes #11651\r\n\r\ncc: @drpngx @gpapan", "comments": ["@gpapan could you take a look?", "@robieta can you take a look?", "Apologies; this got lost in my inbox. Can you check whether this is still an issue, and if so rebase your PR?", "Any news of this PR?", "New PR has been opened with these changes , so closing this .Thank you"]}, {"number": 17419, "title": "possible issue of tensorflow.keras not handling shape correctly", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win10 1709 x64\r\n- **TensorFlow installed from (source or binary)**: binary from PyPI\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: /\r\n- **GCC/Compiler version (if compiling from source)**: /\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: GTX1060 6GB\r\n- **Exact command to reproduce**: /\r\n\r\n### Describe the problem\r\ni believe this is a bug that ``tensorflow.keras`` not handling the shape correctly, ``self.input_shape`` is provided by me which is [7514, 1] in this case. My code runs fine with ``keras`` but get this error with ``tensorflow.keras``\r\n\r\n### Source code / logs\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-21e293afadc1> in <module>()\r\n     22 # x_err[x_err<0] = 0\r\n     23 from astroNN.models import load_folder\r\n---> 24 bcnn = load_folder('astroNN_0224_run002')\r\n     25 \r\n     26 # x = np.random.rand(1000,7514)\r\n\r\nd:\\university\\ast425\\astronn\\astroNN\\models\\__init__.py in load_folder(folder)\r\n    144         pass\r\n    145 \r\n--> 146     astronn_model_obj.compile()\r\n    147     astronn_model_obj.keras_model.load_weights(os.path.join(astronn_model_obj.fullfilepath, 'model_weights.h5'))\r\n    148 \r\n\r\nd:\\university\\ast425\\astronn\\astroNN\\models\\BayesianCNNBase.py in compile(self)\r\n    238             self._last_layer_activation = 'softmax'\r\n    239 \r\n--> 240         self.keras_model, self.keras_model_predict, output_loss, variance_loss = self.model()\r\n    241 \r\n    242         if self.optimizer is None or self.optimizer == 'adam':\r\n\r\nd:\\university\\ast425\\astronn\\astroNN\\models\\Apogee_BCNN.py in model(self)\r\n     64 \r\n     65     def model(self):\r\n---> 66         input_tensor = Input(shape=self.input_shape, name='input')\r\n     67         labels_err_tensor = Input(shape=(self.labels_shape,), name='labels_err')\r\n     68 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\topology.py in Input(shape, batch_size, name, dtype, sparse, tensor, **kwargs)\r\n    621   if dtype is None:\r\n    622     dtype = K.floatx()\r\n--> 623   if not shape and tensor is None:\r\n    624     raise ValueError('Please provide to Input either a `shape`'\r\n    625                      ' or a `tensor` argument. Note that '\r\n\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n```", "comments": ["The bug description is a little unclear. It seems like you think the error is L623 in topology.py but the actual ValueError message is different than what that line produces. Are you sure that is the line that is causing the error?", "Hi, I have found out the cause of issue.\r\n\r\nThe issue is in `tensorflow.keras`, the` shape` cannot be a numpy array while `keras` can. The issue is gone now if I explicitly convert `self.input_shape` to a list,\r\n\r\nI do think this is a bug\r\n\r\nHere is an example script to reproduce the issue\r\n\r\nFor Keras which will run without error:\r\n```\r\nimport keras\r\nimport numpy as np\r\ninput = keras.layers.Input(shape=(np.array([7514,1])))\r\noutput = keras.layers.Dense(10)(input)\r\nmodel = keras.models.Model(inputs=input, outputs=output)\r\n```\r\n\r\nFor `tensorflow.keras` with the error\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\ninput = tf.keras.layers.Input(shape=(np.array([7514,1])))\r\noutput = tf.keras.layers.Dense(10)(input)\r\nmodel = tf.keras.models.Model(inputs=input, outputs=output)\r\n```", "Assigning to Francois to take a look at it.", "@fchollet  May I take a look at this, or have you already solved this? This would be my first issue.. ", "@alphaCTzo7G This issue can still be reproduced with the latest TF1.7, so the issue is not solved. The workaround is I simply convert the numpy array to python list.", "Ok.. thanks for reporting @henrysky. If its ok with @fchollet, I will take a look at it and create a PR", "```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ninput = tf.keras.layers.Input(shape=(np.array([7514,1])))\r\noutput = tf.keras.layers.Dense(10)(input)\r\nmodel = tf.keras.models.Model(inputs=input, outputs=output)\r\n```\r\n\r\nI have tested the above code runs without error in TF1.12.0 so I will close it"]}, {"number": 17418, "title": "Fix build issue with KafkaDataset", "body": "This fix tries to address the issue raised in #17210 where error of `NotFoundError: Op type not registered 'KafkaDataset' in binary.` returned from kafka ops.\r\n\r\nThe issue was that the inclusion of kafka ops was removed due to the conflict merge from the other PR. This fix fixes the issue.\r\n\r\nThis fix fixes #17210.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @mrry for the help \ud83d\udc4d . Some of the tests are failing. Seems to be some difference between clang and gcc (it runs fine on my local machine with gcc + ubuntu 16.04). I am looking into the issue now.", "The reason of the previous test failure was that, `includes` in BUILD file adds include path with `-isystem` (vs. `-I`). That makes clang unhappy. The issue has been fixed by replacing `includes` with `copt`.\r\n\r\nAll tests passed now \ud83c\udf89 "]}, {"number": 17416, "title": "Fix cmake Dockerfile issue on Linux", "body": "When running cmake on Linux with (clean build with no cached docker images):\r\n```\r\ntensorflow/tools/ci_build/ci_build.sh CMAKE tensorflow/tools/ci_build/builds/cmake.sh\r\n```\r\n\r\nThe following two issues were encountered:\r\n```\r\nStep 13/14 : RUN add-apt-repository -y ppa:ubuntu-lxc/lxd-stable\r\n ---> Running in 09301ba43a33\r\nCannot add PPA: 'ppa:~ubuntu-lxc/ubuntu/lxd-stable'.\r\nThe team named '~ubuntu-lxc' has no PPA named 'ubuntu/lxd-stable'\r\nPlease choose from the following available PPAs:\r\n * 'buildd-backports':  linuxcontainers.org: buildd backports\r\n * 'daily':  linuxcontainers.org: development builds\r\n ......\r\n ......\r\n```\r\n\r\nThe issue is that `ppa:ubuntu-lxc/lxd-stable` was used but it has been deprecated, see:\r\nhttp://lxc-users.linuxcontainers.narkive.com/IlHLLHqN/lxd-official-ppa-deprecation\r\n\r\nAnother issue is the missing wheel install:\r\n```\r\nStep 11/13 : RUN pip install --upgrade termcolor\r\n ---> Running in 838167596eb6\r\nCollecting termcolor\r\n  Downloading termcolor-1.1.0.tar.gz\r\n  ......\r\n  ......\r\n  ......\r\n  error: invalid command 'bdist_wheel'\r\n\r\n  ----------------------------------------\r\n  Failed building wheel for termcolor\r\n```\r\nThis fix updates the golang installation and use backported xenial (16.04), as was suggested in the link:\r\nhttp://lxc-users.linuxcontainers.narkive.com/IlHLLHqN/lxd-official-ppa-deprecation\r\n\r\nThis fix also adds the missing `pip install wheel`\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 17415, "title": "Dataset API 'flat_map' method producing error for same code which works with 'map' method", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n  Custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Windows 10\r\n \r\n- **TensorFlow installed from (source or binary)**: Binary\r\n   \r\n- **TensorFlow version (use command below)**:  1.6.0\r\n\r\n- **Python version**: 3.5\r\n\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: GeForce GTX 860M\r\n- **Exact command to reproduce**: dataset = dataset.flat_map(lambda file_name: tf.py_func(_get_data_for_dataset, [file_name], tf.float64))\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI am trying to create a create a pipeline to read mulitple CSV files using TensorFlow Dataset API and Pandas. However using the 'flat_map' method is producing errors. However if I am using 'map' method I am able to build the code and run it in session. This is the code I am using.\r\n\r\n### Source code / logs\r\n\r\n```\r\nfolder_name = './data/power_data/'\r\nfile_names = os.listdir(folder_name)\r\ndef _get_data_for_dataset(file_name,rows=100):#\r\n    print(file_name.decode())\r\n    \r\n    df_input=pd.read_csv(os.path.join(folder_name, file_name.decode()),\r\n                         usecols =['Wind_MWh','Actual_Load_MWh'],nrows = rows)\r\n    X_data = df_input.as_matrix()\r\n    X_data.astype('float32', copy=False)\r\n    \r\n    return X_data\r\ndataset = tf.data.Dataset.from_tensor_slices(file_names)\r\ndataset = dataset.flat_map(lambda file_name: tf.py_func(_get_data_for_dataset, [file_name], tf.float64))\r\ndataset= dataset.batch(2)\r\niter = dataset.make_one_shot_iterator()\r\nget_batch = iter.get_next()`\r\n```\r\n I get the following error: `map_func` must return a `Dataset` object. It would also great if you could provide documentation on using Dataset API with Pandas module.\r\n", "comments": ["So if your pipeline works with `map` why not just use `map`?\r\n\r\nThe function passed to `flat_map` should have the return type `tf.data.Dataset` by definition. This is not an error ([this](https://www.youtube.com/watch?v=9QveBbn7t_c) can be a fun watch if you want to learn why).", "Yes, the pipeline works without error when I use `map` but it doesn't give the output I want. For example, if Pandas is reading N rows from each of my CSV files I want the pipeline to concatenate data from B files and give me an array with shape (N*B, 2). Instead, it is giving me (B, N,2) where B is the  Batch size. `map` is adding another axis instead of concatenating on the existing axis. From what I understood in the documentation `flat_map` is supposed to give a flatted output. In the documentation, both `map` and `flat_map` returns type Dataset. So how is my code working with `map` and not with `flat_map`?", "There's a difference between what `map` and `flat_map` return, and what the function you pass to them is supposed to return. I suppose this could be clarified in their docstrings though. Maybe make a pr for this?", "So `map` doesn't need a dataset object to be passed to it while `flat_map` needs it? It would be great if this clarified in the documentation. Could you suggest what I need to change in the code for my function to return a dataset?", "Sorry, this discussion should be had at https://stackoverflow.com/questions/tagged/tensorflow instead. Could you make a question there?", "Ok, thanks for the help. Just opened a question here:\r\nhttps://stackoverflow.com/questions/49116343/dataset-api-flat-map-method-producing-error-for-same-code-which-works-with-ma", "The solution suggested on StackOverflow was to covert the output of my py_function to a dataset. So I modified my py_function as shown below.\r\n  ```\r\n    \r\n    X_data.astype('float32', copy=False)\r\n    d = tf.data.Dataset.from_tensors(X_data)\r\n    return d\r\n```\r\nHowever I am still getting the 'map_func must return a Dataset object' error. It would be great if somebody could clarify whether this a bug or some problem with my code.", "I posted [an answer](https://stackoverflow.com/a/49140725/3574081) on Stack Overflow."]}, {"number": 17414, "title": "Add missing `#define OMPI_SKIP_MPICXX` ", "body": "This fix adds the missing `#define OMPI_SKIP_MPICXX` in `tensorflow/contrib/mpi/mpi_utils.h` so that it is consistent with other usages of `mpi.h` includes. `OMPI_SKIP_MPICXX` skip the MPI C++ bindings support.\r\n\r\nThis fix fixes #17388 and #17504.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 17413, "title": "TOCO API fail with the basic Python example", "body": "### System information\r\n- **OS Platform and Distribution (Linux Ubuntu 17.10)**:\r\n- **TensorFlow installed from (pip3)**:\r\n- **TensorFlow version (1.6)**:\r\n- **Python 3.6.3**: \r\n\r\n### Describe the problem\r\nTOCO API fail with the basic Python example:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md#example-2-export-with-variables\r\n\r\n### Error\r\n```\r\nTraceback (most recent call last):\r\n  File \"fb.py\", line 15, in <module>\r\n    sess, sess.graph_def, map(canonical_name, out_tensors))\r\n  File \"/home/alex/.local/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py\", line 232, in convert_variables_to_constants\r\n    inference_graph = extract_sub_graph(input_graph_def, output_node_names)\r\n  File \"/home/alex/.local/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py\", line 176, in extract_sub_graph\r\n    nodes_to_keep = _bfs_for_reachable_nodes(dest_nodes, name_to_input_name)\r\n  File \"/home/alex/.local/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py\", line 140, in _bfs_for_reachable_nodes\r\n    next_to_visit = target_nodes[:]\r\nTypeError: 'map' object is not subscriptable\r\n```\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@alexst07 Try `list(map(canonical_name, out_tensors)))` ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 45 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 62 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 17412, "title": "Docker Jupyter: Update deprecated softmax_cross_entropy_with_logits", "body": "The current Jupyter Notebook included in the Docker image `3_mnist_from_scratch.ipynb` uses the deprecated [`tf.nn.softmax_cross_entropy_with_logits`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits).\r\n\r\nThis PR updates that Jupyter Notebook to use the new [`tf.nn.softmax_cross_entropy_with_logits_v2`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2).\r\n\r\n---\r\n\r\nI ran it to make sure everything works, but I didn't commit the changes in the cells because the image style looks a bit different, and that would create changes in the diff of the images in the Notebook. This way it should be easier for you to see the exact change.\r\n\r\nIf you want me to, I can run the cells and commit the new output (with changes in image style and size, but same content).", "comments": ["Sure @frankchn ! Done :heavy_check_mark: ", "@frankchn I see the error is related to other sections that were not modified by this PR:\r\n\r\n```\r\nWriting docs for tf.contrib.quantize (<module 'tensorflow.contrib.quantize' from '/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/tools/docs/build_docs_test.runfiles/org_tensorflow/tensorflow/contrib/quantize/__init__.py'>).\r\nWriting docs for tf.contrib.tpu.infeed_dequeue_tuple (<function infeed_dequeue_tuple at 0x1195756e0>).\r\nWritingERROR:\r\n    output file name: /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/tools/docs/build_docs_test.runfiles/org_tensorflow/tensorflow/docs_src/tutorials/kernel_methods.md\r\n    Cannot make link to \"tf.contrib.learn.estimator\": Not in index.\r\nF\r\n======================================================================\r\nFAIL: testBuildDocs (__main__.BuildDocsTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/tools/docs/build_docs_test.runfiles/org_tensorflow/tensorflow/tools/docs/build_docs_test.py\", line 70, in testBuildDocs\r\n    self.fail('Found %s Errors!' % status)\r\nAssertionError: Found 1 Errors!\r\n----------------------------------------------------------------------\r\nRan 1 test in 25.613s\r\nFAILED (failures=1)\r\n docs for tf.contrib.lite.convert_op_hints_to_stubs (<function convert_op_hints_to_stubs at 0x11a4d22a8>).\r\n```\r\n\r\nWhat should we do about it? Is it a race condition or something? Should we re-trigger the CI?", "That's fine. Unrelated test failure -- merging."]}, {"number": 17411, "title": "Illegal instruction (core dumped) after running import tensorflow", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: \r\n1.6.0-cp27-cp27mu-manylinux1_x86_64 (can only guess since `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` gives me an error already)\r\n- **Python version**: Python 2.7.12\r\n- **Exact command to reproduce**: `import tensorflow`\r\n\r\nI created a fresh virtual environment: `virtualenv -p python2 test_venv/`\r\nAnd installed tensorflow: `pip install --upgrade --no-cache-dir tensorflow`\r\n`import tensorflow` gives me `Illegal instruction (core dumped)`\r\n\r\nPlease help me understand what's going on and how I can fix it. Thank you.\r\n\r\nCPU information:\r\n```\r\n-cpu\r\n          description: CPU\r\n          product: Intel(R) Core(TM) i3 CPU       M 330  @ 2.13GHz\r\n          bus info: cpu@0\r\n          version: CPU Version\r\n          capabilities: x86-64 fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm arat cpufreq\r\n```\r\n*EDIT*\r\nStacktrace obtained with gdb:\r\n\r\n```\r\n#0  0x00007fffe5793880 in std::pair<std::__detail::_Node_iterator<std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> >, false, true>, bool> std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> >, std::allocator<std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> > >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_emplace<std::pair<tensorflow::StringPiece, std::function<bool (tensorflow::Variant*)> > >(std::integral_constant<bool, true>, std::pair<tensorflow::StringPiece, std::function<bool (tensorflow::Variant*)> >&&) ()\r\n   from /media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#1  0x00007fffe5795735 in tensorflow::UnaryVariantOpRegistry::RegisterDecodeFn(std::string const&, std::function<bool (tensorflow::Variant*)> const&) () from /media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#2  0x00007fffe5770a7c in tensorflow::variant_op_registry_fn_registration::UnaryVariantDecodeRegistration<tensorflow::Tensor>::UnaryVariantDecodeRegistration(std::string const&) ()\r\n   from /media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fffe56ea165 in _GLOBAL__sub_I_tensor.cc ()\r\n   from /media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007ffff7de76ba in call_init (l=<optimized out>, argc=argc@entry=2, argv=argv@entry=0x7fffffffd5c8, env=env@entry=0xa7b4d0)\r\n    at dl-init.c:72\r\n#5  0x00007ffff7de77cb in call_init (env=0xa7b4d0, argv=0x7fffffffd5c8, argc=2, l=<optimized out>) at dl-init.c:30\r\n#6  _dl_init (main_map=main_map@entry=0xa11920, argc=2, argv=0x7fffffffd5c8, env=0xa7b4d0) at dl-init.c:120\r\n#7  0x00007ffff7dec8e2 in dl_open_worker (a=a@entry=0x7fffffffb5c0) at dl-open.c:575\r\n#8  0x00007ffff7de7564 in _dl_catch_error (objname=objname@entry=0x7fffffffb5b0, errstring=errstring@entry=0x7fffffffb5b8, \r\n    mallocedp=mallocedp@entry=0x7fffffffb5af, operate=operate@entry=0x7ffff7dec4d0 <dl_open_worker>, args=args@entry=0x7fffffffb5c0)\r\n    at dl-error.c:187\r\n#9  0x00007ffff7debda9 in _dl_open (\r\n    file=0x7fffea7cbc34 \"/media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\", mode=-2147483646, caller_dlopen=0x51ad19 <_PyImport_GetDynLoadFunc+233>, nsid=-2, argc=<optimized out>, argv=<optimized out>, env=0xa7b4d0)\r\n    at dl-open.c:660\r\n#10 0x00007ffff75ecf09 in dlopen_doit (a=a@entry=0x7fffffffb7f0) at dlopen.c:66\r\n#11 0x00007ffff7de7564 in _dl_catch_error (objname=0x9b1870, errstring=0x9b1878, mallocedp=0x9b1868, operate=0x7ffff75eceb0 <dlopen_doit>, \r\n    args=0x7fffffffb7f0) at dl-error.c:187\r\n#12 0x00007ffff75ed571 in _dlerror_run (operate=operate@entry=0x7ffff75eceb0 <dlopen_doit>, args=args@entry=0x7fffffffb7f0) at dlerror.c:163\r\n#13 0x00007ffff75ecfa1 in __dlopen (file=<optimized out>, mode=<optimized out>) at dlopen.c:87\r\n#14 0x000000000051ad19 in _PyImport_GetDynLoadFunc ()\r\n#15 0x000000000051a8e4 in _PyImport_LoadDynamicModule ()\r\n#16 0x00000000005b7b1b in ?? ()\r\n#17 0x00000000004bc3fa in PyEval_EvalFrameEx ()\r\n#18 0x00000000004c136f in PyEval_EvalFrameEx ()\r\n#19 0x00000000004b9ab6 in PyEval_EvalCodeEx ()\r\n#20 0x00000000004b97a6 in PyEval_EvalCode ()\r\n#21 0x00000000004b96df in PyImport_ExecCodeModuleEx ()\r\n#22 0x00000000004b2b06 in ?? ()\r\n#23 0x00000000004a4ae1 in ?? ()\r\n```\r\n\r\n*EDIT 2*\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\n\r\nAfter downgrading to an older version of tensorflow the error goes away. I've been advised that my CPU (see information above) might not work with some improvements in the new API. If this is the case, I suppose there's no solution for my problem. Therefore, I will close this thread. Feel free to correct me though. Thank you for your support", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "I'm having the same (or similar) \"illegal instruction\" problem when I run\r\n\r\n    import tensorflow as tf\r\n\r\nI'm only using the CPU 1.6 version on 64 bit Ubuntu Linux.\r\n\r\nAfter downgrading to the CPU 1.5 version, it doesn't have this problem. ", "How i can downgrade to the CPU 1.5 version?", "Try running\r\npip uninstall tensorflow\r\nAnd then\r\npip install tensorflow==1.5\r\n\r\nEDIT\r\njust to give credit, solution is from here:\r\nhttps://stackoverflow.com/questions/49094597/illegal-instruction-core-dumped-after-running-import-tensorflow", "Thanks konnerthg, even i was having the same problem. Your command helped me to sort this issue. Thanks again.", "Same here. \r\nWith the latest wheel, I had the illegal instruction problem on Ubuntu 16.04, however I downgraded to tensorflow-gpu==1.5 and it works!", "downgrade to 1.5 worked for me, too", "@konnerthg Downgrading to 1.5 is just work around, this issue is not solved yet.\r\nWhich commit/PR solved this issue?", "I am also getting this error in python 3.6", "Hey !\r\nThank you for your solution ! Really. I have this problem for a week now and I was starting to become crazy ! Thx !", "THANKS for solution.It worked on my Ubuntu 16.04, 64 bit, python3.5 .", "Thanks for the solution! Downgrading to version 1.5 fixed the issue. Tested on a Ubuntu 16.04 server with python 2.7", "Same issue, downgrading from Tensorflow 1.6 to 1.5 solved it. Running Xubuntu 16.04 64-bit, Python 3.5.", "Thanks for all this solve my issue on Python 3.6 \r\n\r\n>_ (tensorflow) naniny@Aspire-E5-573:~$ pip unistall tensorflow\r\n\r\n>_(tensorflow) naniny@Aspire-E5-573:~$ pip install tensorflow==1.5\r\n\r\n>_(tensorflow) naniny@Aspire-E5-573:~$ python\r\n\r\n>_ (tensorflow) naniny@Aspire-E5-573:~$ import tensorflow as tf\r\n\r\nnow works without any problem ...\r\n\r\n", "This is really weird. Does anyone know what causes the issue? I'm surprised that TensorFlow 1.6 would have a bug this big.", "I am encountering this issue as well with tensorflow-gpu 1.6.0, on linux, using python 3.6.4.  I have installed tensorflow using pip itself.  Simply running this produces a SIGILL:\r\n\r\n```\r\n$ python3 -m tensorflow\r\nzsh: illegal hardware instruction  python3 -m tensorflow\r\n```\r\nI get stack traces similar to what is mentioned in this ticket's description.\r\n\r\nThis seems to be occurring due to the use of AVX instructions in the latest Tensorflow packages uploaded to pip.  Running python3 through GDB and disassembling the crashing function points to this instruction:\r\n\r\n```\r\n=> 0x00007fffb9689660 <+80>:    vmovdqu 0x10(%r13),%xmm1\r\n```\r\n\r\nWhich is an AVX instruction not supported on older or less-featureful CPUs that do not have AVX support.  The tensorflow(-gpu) 1.5.0 pip packages do not use AVX instructions, and thus there are no problems using it with these CPUs.\r\n\r\nThe solution would be for a build of tensorflow(-gpu) that is not compiled with AVX instructions to be published (or to build a copy locally).  The provided [installation instructions](https://www.tensorflow.org/install/install_linux) do not mention any specific CPU requirements nor how to determine compatibility with the provided binaries.\r\n\r\nIn the meantime, reverting to tensorflow(-gpu) 1.5.0 using something like what @NinemillaKA mentioned above is an effective workaround.", "I have the same issue, and, as many have commented, downgrade from `1.6.0` to `1.5.0`.\r\n\r\nFor the record, I tried running tensorflow (CPU-only version) on 2 different computers:\r\n\r\nComputer 1:\r\n```\r\nOS = Ubuntu 16.04 x64 LTS\r\nPython = Python 3.6\r\npip version = 9.0.1\r\ntensorflow version = TensorFlow 1.6.0\r\nCPU = Intel Core 2 Quad Q6600  @2.40GHz\r\n```\r\nComputer 2:\r\n```\r\nOS = Ubuntu 16.04 x64 LTS\r\nPython = Python 3.6\r\npip version = 9.0.1\r\ntensorflow version = TensorFlow 1.6.0\r\nCPU = Intel Celeron N2820 @2.413GHz\r\n```\r\n\r\nI agree with @nacl that we should have those requirements about the instruction set more clear, and if possible, a separated, updated build for processors that doesn't support AVX instructions. To be honest, I find a bit discouraging have to work with outdated version of any technology, I think many feel the same.\r\n", "The alternative to having a different build for each architecture type is to use dynamic dispatch. IE, PyTorch has one binary for all architectures and selects most efficient ops during runtime @caisq ", "Thanks", "I also encounter the same issue. I tried it on two machines, and it works on one of them.\r\n\r\nFirst, I installed it on my MacBook Pro. And I did not have any issues.\r\n```\r\nMacBook Pro (Retina, Mid 2012)\r\nCPU = 2.3 GHz Intel Core i7\r\nOS = MacOS 10.13.3\r\nPython = Python 3.6.4\r\npip version = 9.0.3\r\nTensorFlow version = 1.6.0\r\n```\r\n\r\nSo I upgraded my MacPro. But this time, I am getting `Illegal instruction: 4` when I try to import tensorflow.\r\n\r\n```\r\nMac Pro (Mid 2010)\r\nCPU = 2 x 2.4 GHz Quad-Core Intel Xeon\r\nOS = MacOS 10.13.3\r\nPython = Python 3.6.4\r\npip version = 9.0.3\r\nTensorFlow version = 1.6.0\r\n```\r\n\r\n(Update on 3/30/2018)\r\nThe same problem with TensorFlow 1.7. So I guess I use TensorFlow 1.5. ", "This is still an issue in 1.6 and potentially in 1.7.  Why is this closed?  @yaroslavvb 's solution seems reasonable.  I have downgraded to 1.5 for now.", "Not sure but from this link, since ver1.6.0, intel CPU instruction optimizer had been introduced to tensorflow. I think that probably this is the cause.\r\nhttps://software.intel.com/en-us/articles/intel-optimized-tensorflow-wheel-now-available", "@captainst that's Intel-specific release, different from the official release that you get by doing `pip install`. SIGILL issues after 1.6 upgrade are likely caused by adding AVX", "I have the same issue.\r\nUbuntu 18.04 x64\r\nPython 3.6.5rc1\r\nTensorFlow 1.7.0", "I had the same issue. Downgrade to 1.5 worked for me.", "Same here.", "Me too. Arch Linux and Intel CPU.", "Same here on Ubuntu 17. 10.1 VM, KVM hypervisor on Slackware, Intel KabyLake CPU ", "I think I figured it out. I have a G4600 CPU which lacks AVX support and this was added in 1.6. ", "Yes, I lock of AVX support too.", "To fix this on v1.6 and above, you need to compile tensorflow from source. ", "I want to use the latest tensorflow-gpu (1.7) and don't want to stick to 1.5. So is the solution to build tensorflow from source without AVX support enabled? I have an old Xeon E5420 but am using tensorflow-gpu", "dmoham1476 <notifications@github.com>\u4e8e2018\u5e744\u67087\u65e5 \u5468\u516d\u4e0a\u53482:28\u5199\u9053\uff1a\n\n> I want to use the latest tensorflow-gpu (1.7) and don't want to stick to\n> 1.5. So is the solution to build tensorflow from source without AVX support\n> enabled? I have an old Xeon E5420 but am using tensorflow-gpu\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17411#issuecomment-379337106>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGeJnptN5yMdVaVdWTsIwcjOAJuJbUDQks5tl7PKgaJpZM4SbSGu>\n> .\n>\nYou can build from source from the git clone master. It should auto adjust\nto ur cpu.\n-- \nRegards,\n\nJin\n", "Downgrading to version 1.5 fixed the issue. I am using Ubuntu 16.04  with python 2.7", "Same Problem\r\nUbuntu=16.10\r\npython=3.6\r\npip=9.0.3\r\nwith tensorflow-gpu= 1.7\r\nNVIDIA GTX 1070\r\nCUDA 9.0\r\n\r\nsolved by:\r\npip3 install tensorflow-gpu==1.5", "the same problem", "same problem\r\nUbuntu 16.04\r\nwith tensorflow-gpu 1.7 and 1.6", "Same problem on [this Docker image](https://hub.docker.com/r/ufoym/deepo/) running in a Paperspace notebook.\r\n\r\nIssue is with versions 1.6 and 1.7", "Having the same issue, downgraded to 1.5. Tensorflow imports now but I'm running a script that needs tensorflow_hub. Script is throwing RuntimeError: TensorFlow Hub depends on 'tf-nightly' build after 20180308 or 'tensorflow~=1.7'. Found tf.VERSION = 1.5.0", "thanks going back to version 1.5 worked for me also", "I had the same problem with tf`1.8 on CentOS 7, CPU only. Downgrading to 1.5 worked.", "For those that do not want to downgrade if you build from source the problem is resolved.", "Having the same problem, Downgrading to 1.5 worked for me.", "Just had this issue too, on tensorflow-gpu 1.8.0, at paperspace fast.ai template machine", "I have the same issue with tensorflow-1.8.0. The problem is I can't downgrade to version 1.5 as I want to train with retrain.py and for this tensorflow version greater than 1.7 is a requirement. So, any suggestions? Will it be helpful if I build tensorflow from source?!", "Same issue here:\r\npaperspace VM with fast.ai template\r\nGPU: nvidia quadro P4000\r\ntensorflow 1.8.0 installed in virtualenv as in https://www.tensorflow.org/install/install_linux#installing_with_virtualenv gave the error above (including when installing the recommended wheel mentioned at the bottom of the page)\r\nubuntu 16.04, python 3.6.3\r\n`pip uninstall tensorflow && pip install tensorflow-gpu==1.5.0` fixed the problem. I have not tried compiling from source yet.\r\n\r\nAs a side note, `grep avx /proc/cpuinfo` returns nothing, so my VM's cpu doesn't support avx instructions.", "Hello everyone,  building from source solved the issue for me.\n\n\u062f\u0631 \u062a\u0627\u0631\u06cc\u062e \u06cc\u06a9\u0634\u0646\u0628\u0647 \u06f1\u06f3 \u0645\u0647\u0654 \u06f2\u06f0\u06f1\u06f8\u060c\u200f \u06f1:\u06f4\u06f7 laurentS <notifications@github.com> \u0646\u0648\u0634\u062a:\n\n> Same issue here:\n> paperspace VM with fast.ai template\n> GPU: nvidia quadro P4000\n> tensorflow 1.8.0 installed in virtualenv as in\n> https://www.tensorflow.org/install/install_linux#installing_with_virtualenv\n> gave the error above\n> ubuntu 16.04, python 3.6.3\n> pip uninstall tensorflow && pip install tensorflow-gpu==1.5.0 fixed the\n> problem. I have not tried compiling from source yet.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17411#issuecomment-388583280>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APaJX3BUFgxCemEueanuAXMfqboX5MNqks5tx1FugaJpZM4SbSGu>\n> .\n>\n", "I phase the same problem, but after shift to tensorflow-1.5 it worked for me.\r\nMy Problem:- error -> core dumped\r\nProcessor: AMD\u00ae A4-3330mx apu with radeon(tm) hd graphics \u00d7 2 \r\nGraphics:  AMD\u00ae Sumo\r\nGNOME: 3.28.1\r\nOS type:  64-bit, Ubuntu 18.04 LTS\r\n\r\nCredit:-https://github.com/konnerthg", "of course it works with earlier versions of tensorflow but i need more specifically the 1.7.0 version that is compatible with the new tensorflowjs. How can?", "@mctrjalloh  buy new CPU, I guess", "I solved it. Not by buying new CPU\ud83d\ude0f\n", "You have to build from sources it's not that complicated ", "Same issue in tensorflow-gpu 1.8.0 as well\r\n\r\nOS: Ubuntu 18.04\r\nPython: 3.6.5\r\n", "Same for me:\r\n\r\nOS: Ubuntu 16.04\r\nPython: 3.5\r\nNVIDIA Driver Version: 384.130\r\n\r\n**Edit:** building from sources works", "Thank you so much!!!", "CUDA9.0+cuDNN7.1+Tensorflow-gpu1.5 is working", "Same problem. Tested on Ubuntu 16 and Ubuntu 17, on 3 different machines each with >8GB RAM and >4ghz CPU. Also getting issue with GPU machines. ", "Same problem on my Ubuntu Server 16.04. Solved by compiling TensorFlow (r1.8) from source and pip install from that locally :+1: ", "The issue comes with docker images too. I don't want to compile from source. Is there a way around it? I don't want to downgrade either.", "In my case, compiling TensorFlow (on Ubuntu 18.04) with Bazel solved the issue. The compilation did take time though.", "My case as well, recompiling TensorFlow 1.9 solved the issue. However I encountered another issue related to numpy 1.15. Downgrading to numpy 1.14.5 and the compilation worked, and been able to install the pip package.", "Why is this issue closed? It's still occurring in TensorFlow 1.10.", "I posted some links to a few community builds of tensorflow [here](https://stackoverflow.com/a/51777012/1138710) which might help avoid having to build from source.", "Indeed, I followed the instructions found at https://www.tensorflow.org/install/install_linux and get nothing more than the \"Illegal instruction (core dumped)\" when testing as requested on that installation page.\r\n\r\nThis issue should either not be closed or have an actual solution that makes sense (i.e. not installing version 1.5)\r\n\r\nIt worked on Ubuntu 18.04. So again, mentioning the version on the installation page may be a good idea as a solution about what works and what doesn't.\r\n", "well this issue seems to still going on.. but i have a very simple solution\nthat will make you follow the trends at the same time :-).\n\nINSTALL CONDA !!!\nto install conda, just google it :-)\n\ncreate a conda evironment if you like:\n\nconda create <name_of_environment>\n\nand simply run:\n\nconda install tensorflow\n\nif the last command doesn't work then search through another channel by\nrunning:\n\nanaconda search tensorflow\n\nthis outputs a list of channels from where you can download it.\nchoose one channel and run:\n\nanaconda show <channel>\n\nthis will prompt you the command you need to enter to downloading\ntensorflow.\nRun that command.\n\nThat's it !\n\nMOVE TO CONDA !!!\n\nOn Thu, Aug 23, 2018 at 10:38 PM Alexis Wilke <notifications@github.com>\nwrote:\n\n> Indeed, I followed the instructions found at\n> https://www.tensorflow.org/install/install_linux and get nothing more\n> than the \"Illegal instruction (core dumped)\" when testing as requested on\n> that installation page.\n>\n> This issue should either not be closed or have an actual solution that\n> makes sense (i.e. not installing version 1.5)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17411#issuecomment-415592389>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWESkUXD-4XomnnPgF6D5QToWVH74JzAks5uTy7PgaJpZM4SbSGu>\n> .\n>\n", "Root problem may have to do with protobuf and incompatibility around pthread_once and std::call_once. I ran into a segfault myself when importing tensorflow right after another google package called sentencepiece, and the maker of sentencepiece fixed it by making a patch for protobuf that replaces the std::call_once implementation with another.\r\n\r\nhttps://github.com/google/sentencepiece/issues/186", "To make this issue more constructive, I think it would be useful it TensorFlow would check for instructions it requires on the CPU first, and print an error if they are missing. Similarly how it currently says that some instructions are available but not compiled against. Then it would be easier to differentiate between bugs and simply not using correct binary for a given CPU.", "Mr royyannick ..Infact i have been searching google for many times and it has affected my studying both Keras and Tensoflow but today you have made my day..You are great.\r\nThanks", "Tried both virtual environment and (f...ing) conda (you, dude upstream, go f y s!) on the tensorflow 1.10.1 (latest) in Ubuntu 16.04 with the same error. Switched to the previous version 1.9 - everything works fine.", "I happened to reproduce this issue on a machine running an old CPU. Here is [the article that explains the possible options to resolve the issue](https://tech.amikelive.com/node-887/how-to-resolve-error-illegal-instruction-core-dumped-when-running-import-tensorflow-in-a-python-program/).\r\n\r\nThose who want to install the latest TensorFlow for old CPU without AVX support but does not have the time to build from source can also download the WHL file from [this Github repository](https://github.com/amikelive/tf-build).", "This is BS. I rolled my installation back to 1.9 (not 'before 1.6' as you say in this article) and the binary worked (the day before yesterday).\r\n", "If you run this on command line:\r\n\r\n1)\r\n\r\n    $ lsb_release -a| grep \"Release\" | awk '{print $2}'\r\n\r\n2)\r\n\r\n    $ grep flags -m1 /proc/cpuinfo | cut -d \":\" -f 2 | tr '[:upper:]' '[:lower:]' | { read FLAGS; OPT=\"-march=native\"; for flag in $FLAGS; do case \"$flag\" in \"sse4_1\" | \"sse4_2\" | \"ssse3\" | \"fma\" | \"cx16\" | \"popcnt\" | \"avx\" | \"avx2\") OPT+=\" -m$flag\";; esac; done; MODOPT=${OPT//_/\\.}; echo \"$MODOPT\"; }\r\n\r\nand see `16.04` for 1) and `-mavx` or `-mavx2`  for 2) in the output, it can be another problem that is not related with AVX support.\r\n\r\nIf those flags are not there, that's something that I should add into my note, thanks to you.", "Same error here,\r\n\r\nCentOS 7, Python 3.6.5, Intel CPU core2 duo e8500. pip install.\r\n\r\nversion 1.9 does't work. version 1.5 imports ok.\r\n\r\nversion 1.10 seems ok on my laptop which has Ubuntu 18.04 and Intel i5-6200U.", "This is stated at https://www.tensorflow.org/install/install_sources\r\n**Note: Starting from 1.6 release, our prebuilt binaries will use AVX instructions. Older CPUs may not be able to execute these binaries.**\r\n\r\nI think that might have been mentioned at the much more prominent location!", "This solved my issue:\r\nAfter installing NVIDIA driver, CUDA Toolkit, and CUDNN.\r\nFirst uninstall `tensorflow-gpu`: \r\n```\r\n$ pip uninstall tensorflow-gpu\r\n```\r\nThen install tensorflow-gpu using [Anaconda](https://www.anaconda.com/download/#linux):\r\n```\r\n$ conda create -n tensorflow\r\n$ conda install tensorflow-gpu -n tensorflow\r\n```", "> Try running\r\n> pip uninstall tensorflow\r\n> And then\r\n> pip install tensorflow==1.5\r\n> \r\n> EDIT\r\n> just to give credit, solution is from here:\r\n> https://stackoverflow.com/questions/49094597/illegal-instruction-core-dumped-after-running-import-tensorflow\r\n\r\nThanks it works", "Maybe related to AVX instruction. pip prebuilt tensorflow-1.6 and higher versions are built with AVX instruction, some CPUs don't have AVX instruction. pip prebuilt tensorflow-1.5is not built with AVX instruction.\r\nSuggestion: 1): use lower version of tensorflow\r\n2): compile higher version of tensorflow from source", "Yes. Indeed. It would be better, though, if the software would tell me rather than just crash. I don't have a problem with the requirement, just the way it is handled... On Linux, it would be very easy to check in `/proc/cpuinfo` for the `flags` line where `avx` would need to appear. If not, generate an error and `exit(1)`.\r\n\r\nHere is my flags on my old computer without AVX\r\n\r\n> flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm ssbd ibrs ibpb stibp kaiser tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d\r\n", "System information\r\n\r\n-\r\n   Lenovo-G500 8GB RAM          description: CPU\r\n          product: Intel(R) Core(TM) i3 CPU       M 330  @ 2.13GHz\r\n          bus info: cpu@0\r\n          version: CPU Version\r\n OS; Ubuntu-16.05\r\npip : 18 version latest\r\nI dont have gpu\r\n\r\n i also getting   illegal instruction core dumped. tensorforflow 1.5 is working for me , \r\nbut \r\nI need to install tensorflowv1.10 0r latest for my project .\r\n\r\nI tried to to install in tensorflow in different ways , those are\r\n1. without anacoda , python 2.7,   using pip  ...pip install --upgrade tensorflow\r\n2 without anacoda , python 3.5,   using pip     \"\"\r\n3.without anacoda , python 3.6,   using pip   \"\"\r\n4.with anacoda , python 2.7,   using conda     conda install -c conda-forge tensorflow \r\n5.without anacoda , python 2.7,   using pip   \"\"\r\n6.without anacoda , python 2.7,   using pip      \"\"\r\n\r\nneither worked for me,\r\nwhat is the issue.", "@bandarikanth \r\n\r\nThe manner in which you install tensorflow shouldn\u2019t matter. The problem is that the tensorflow 1.6+ prebuilt binaries require the [AVX](\r\nhttps://en.m.wikipedia.org/wiki/Advanced_Vector_Extensions) instruction set extensions, and your processor doesn\u2019t support AVX. You can either build from source, move to a computer with a new-enough processor, or stick with 1.5.", "Thanks\n\nOn Sun, 30 Dec 2018, 12:24 am Dan Stine <notifications@github.com wrote:\n\n> @bandarikanth <https://github.com/bandarikanth>\n>\n> The manner in which you install tensorflow shouldn\u2019t matter. The problem\n> is that the tensorflow 1.6+ prebuilt binaries require the AVX\n> <https://en.m.wikipedia.org/wiki/Advanced_Vector_Extensions> instruction\n> set extensions, and your processor doesn\u2019t support AVX. You can either\n> build from source, move to a computer with a new-enough processor, or stick\n> with 1.5.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17411#issuecomment-450512554>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AZf77RdABm0uQsxiPx0uxd45q_rz9pUVks5u97pPgaJpZM4SbSGu>\n> .\n>\n", "works for me if downgrade to 1.5 (pip install tensorflow==1.5)", "Related: https://github.com/tensorflow/tensorflow/issues/19584", "I have this issue with tensorflow-gpu 2.0 \r\n```\r\n\u25b6 uname -r; pacman -Q linux\r\n5.0.10-arch1-1-ARCH\r\nlinux 5.0.10.arch1-1\r\n\r\n\u25b6 conda env export\r\nname: Science\r\nchannels:\r\n  - defaults\r\ndependencies:\r\n  - cudatoolkit=10.0.130=0\r\n  - cudnn=7.3.1=cuda10.0_0\r\nprefix: /home/archangel/anaconda3/envs/Science\r\n```\r\n```\r\n\u25b6 pip freeze | ack \"tensor\"\r\ntensorflow-gpu==2.0.0a0\r\n\u25b6 ipython                                                          \r\nPython 3.7.3 (default, Mar 27 2019, 22:11:17)                      \r\nType 'copyright', 'credits' or 'license' for more information      \r\nIPython 7.4.0 -- An enhanced Interactive Python. Type '?' for help.\r\n                                                                   \r\nIn [1]: import tensorflow as tf                                    \r\n[1]    25429 illegal hardware instruction (core dumped)  ipython   \r\n```\r\n\r\n```\r\n[18556.882892] traps: ipython[25429] trap invalid opcode ip:7fc41cde1a22 sp:7ffe68904500 error:0 in libtensorflow_framework.so[7fc41c877000+104c000]\r\n[18556.885033] audit: type=1701 audit(1556951396.587:43): auid=4294967295 uid=1000 gid=1000 ses=4294967295 pid=25429 comm=\"ipython\" exe=\"/home/archangel/anaconda3/bin/python3.7\" sig=4 res=1\r\n[18556.894046] audit: type=1130 audit(1556951396.594:44): pid=1 uid=0 auid=4294967295 ses=4294967295 msg='unit=systemd-coredump@4-25462-0 comm=\"systemd\" exe=\"/usr/lib/systemd/systemd\" hostname=? addr=? terminal=?\r\n res=success'\r\n[18557.506049] audit: type=1131 audit(1556951397.204:45): pid=1 uid=0 auid=4294967295 ses=4294967295 msg='unit=systemd-coredump@4-25462-0 comm=\"systemd\" exe=\"/usr/lib/systemd/systemd\" hostname=? addr=? terminal=?\r\n res=success'\r\n\r\n\r\n```", "I'm getting this crash on an i7-3520M which [does support AVX](http://www.cpu-world.com/CPUs/Core_i7/Intel-Core%20i7-3520M%20(PGA)%20Mobile%20processor.html).\r\n\r\nEDIT: Nevermind, the crash happens on a `shlx` instruction which is part of AVX2. At least it shows that vanilla AVX support is not enough.", "Still have the problem with tensorflow 1.14.0 and 2.0.0b1.", "Same error on Linux Mint 19 with 2.0.0b1. \r\nJust installed with pip3 like instructed from the install page of the official site", "tf1.5 isn't available in the Debian 8.8 repos.\r\n\r\nTime to try avx.", "This was ridiculously hard to find on managed cluster nodes, since the OS kills the related python processes before they can even write and flush this \"Illegal instructions\" line to a output log file, and the exit code of the python process seems to be 0.\r\n\r\nI was also using 2.0.0-beta1, currently finding out if replacing it with 2.0.0 fixes this.", "I also got this problem. I'm using `python2`. Downgrading to the CPU 1.5 version helped.", "I'm having this problem with Tensorflow 2 runing in a virtual environment in Ubuntu 18.04. It just blows my mind that the Tensorflow developers would put TF 2 as ready and available with this crap happening. NOT Impressed, you TF developers .\r\n\r\n", "dmesg output (from bash):\r\n[333908.854310] traps: python[12862] trap invalid opcode ip:7f8c46e6d820 sp:7ffc87609f78 error:0 in _pywrap_tensorflow_internal.so[7f8c3e250000+a9f8000]\r\nlinuxmint 19\r\nIntel(R) Pentium(R) CPU        P6200  @ 2.13GHz\r\n8Gb ram (kingston)\r\n\r\nThis is a BIG CPU-RELATED issue.", "After reading this thread and having  the same experience, my problem is that my linux conputer is older and has a CPU which does not support the AVX instruction set. I have tensorflow 1.5 on another virtual environment, bu to use tensorflow 2, I amd going to have to run my scripts on Google Colab", "I don't have the knowledge to say if the requirement of AVX makes sense or not. What I know is that the problem presents itself not only with older CPUs, but also with fairly recent ones, like mine Intel N5000. I get that doing deep learning on a N5000 is a bit of a stretch, but if the tensorflow is supported also by the RaspberryPi, I don't see the problem.\r\n\r\nAnyway, I installed the last version of TensorFlow (2.0) on my Intel N5000 by compiling it from source. It took 14 hours because I had to run the compilation on a single core, since it needs a lot of RAM and I have only 4Gb invited to the party. \r\n\r\nI took inspiration from this guide [here](https://tech.amikelive.com/node-882/how-to-build-and-install-the-latest-tensorflow-without-cuda-gpu-and-with-optimized-cpu-performance-on-ubuntu/) but the experience was far from smooth, there were constantly dependencies missing that I need to install and re-launch the compilation. And some other stuff too that I solved when the compilation crashed.\r\n\r\nHave fun and thanks for the hassle. Providing through pip a binary already compiled for non-AVX was clearly too much to add in your continuous integration workflow ", "Thanks for your response, Luca. I have a really old Ubuntu 18.04 desktop\nwhich works really well except in running the TF2 and as a matter of fact,\nthe latest PyTorch (1.3.0) I think.\nTorch 1.1.0 works fine for me, and regarding TF2, I use Google Colab which\nworks fine. Am a retired data analyst and maintaining my hobbyist profile\n\nThans for your post\n\nOn Sat, Dec 7, 2019 at 6:52 PM Luca Olivieri <notifications@github.com>\nwrote:\n\n> I don't have the knowledge to say if the requirement of AVX makes sense or\n> not. What I know is that the problem presents itself not only with older\n> CPUs, but also with fairly recent ones, like mine Intel N5000. I get that\n> doing deep learning on a N5000 is a bit of a stretch, but if the tensorflow\n> is supported also by the RaspberryPi, I don't see the problem.\n>\n> Anyway, I installed the last version of TensorFlow (2.0) on my Intel N5000\n> by compiling it from source. It took 14 hours because I had to run the\n> compilation on a single core, since it needs a lot of RAM and I have only\n> 4Gb invited to the party.\n>\n> I took inspiration from this guide here\n> <https://tech.amikelive.com/node-882/how-to-build-and-install-the-latest-tensorflow-without-cuda-gpu-and-with-optimized-cpu-performance-on-ubuntu/>\n> but the experience was far from smooth, there were constantly dependencies\n> missing that I need to install and re-launch the compilation. And some\n> other stuff too that I solved when the compilation crashed.\n>\n> Have fun and thanks for the hassle. Providing through pip already binary\n> compiled for non-AVX was clearly too much to add in your continuous\n> integration worklow\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17411?email_source=notifications&email_token=ACYHH362LGI5EZY3G74CMVLQXQZKDA5CNFSM4ETNEGXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGGSCPQ#issuecomment-562897214>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACYHH3YLNAER6NGNLKYNMPDQXQZKDANCNFSM4ETNEGXA>\n> .\n>\n\n\n-- \nClive DaSilva CPA,CMA\nHome:  416-421-2480|Mobile: 416-560-8820\nEmail: clive.dasilva@gmail.com\nLinkedIN:  http://ca.linkedin.com/pub/clive-dasilva/3/197/b89\n", "I had the same problem when running CI pipelines on a Gitlab server. The (emulated) CPU of the runners did not provide AVX instructions.\r\n\r\nInstalling Tensorflow with Conda instead of using PyPI's wheels fixed the problem. :+1: ", "I have the same issue with Tensorflow 2.1.0. What to do?\r\n```\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              48\r\nOn-line CPU(s) list: 0-47\r\nThread(s) per core:  1\r\nCore(s) per socket:  12\r\nSocket(s):           4\r\nNUMA node(s):        8\r\nVendor ID:           AuthenticAMD\r\nCPU family:          16\r\nModel:               9\r\nModel name:          AMD Opteron(tm) Processor 6176\r\nStepping:            1\r\nCPU MHz:             800.000\r\nCPU max MHz:         2300.0000\r\nCPU min MHz:         800.0000\r\nBogoMIPS:            4599.77\r\nVirtualization:      AMD-V\r\nL1d cache:           64K\r\nL1i cache:           64K\r\nL2 cache:            512K\r\nL3 cache:            5118K\r\nNUMA node0 CPU(s):   0-5\r\nNUMA node1 CPU(s):   6-11\r\nNUMA node2 CPU(s):   12-17\r\nNUMA node3 CPU(s):   18-23\r\nNUMA node4 CPU(s):   24-29\r\nNUMA node5 CPU(s):   30-35\r\nNUMA node6 CPU(s):   36-41\r\nNUMA node7 CPU(s):   42-47\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm 3dnowext 3dnow constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm pni monitor cx16 popcnt lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt nodeid_msr hw_pstate vmmcall npt lbrv svm_lock nrip_save pausefilter\r\n```", "I managed to fix my problem by building from source using bazel. It created a whl file. Then I did pip install whl file path", "Yes, if your CPU does not support AVX (the likely cause for `Illegal instruction (core dumped)` error) then you need to compile from source. This causes the code to be generated without AVX instructions and then you can use it.\r\n\r\nFurthermore, this guarantees that the pip is built with the highest optimization level available to your platform, so you might actually see some speedup compared to using a pip built on a different platform. Focus on might.", "To install tensorflow with conda run this command : \r\nconda install -c conda-forge tensorflow\r\nit works for me.\r\n"]}, {"number": 17410, "title": "CUDNN rnn error -Failed to call ThenRnnForward", "body": "OS Platform and Distribution - Ubuntu 16.04\r\nTensorFlow installed from TensorFlow version -1.4 from \r\nBazel Version 0.6.1\r\nCUDA Version 9.0.176\r\nMachine Type -n1-standard-32 (32 vCPUs, 120 GB memory)\r\nGPU - 4 x NVIDIA Tesla P100\r\nI am using a cudnnrnnrelu like this ::\r\nwith tf.variable_scope('cudnn_rnn_stack', reuse = reuse) as scope:\r\n            rnn = tf.contrib.cudnn_rnn.CudnnRNNRelu(5,n_hidden,\"linear_input\", \"bidirectional\")\r\n            output, _ = rnn(tf.transpose(layer_1,[1,0,2]), training=True)\r\n            output_rnn_stack = tf.concat(output, 2)\r\nInitially the epochs were running fine. But I encountered this error after 2-3 epochs:\r\nInternalError (see above for traceback): Failed to call ThenRnnForward\r\n         [[Node: tower_0/cudnn_rnn_stack/cudnn_rnn_relu/CudnnRNN = CudnnRNN[T=DT_FLOAT, direc\r\ntion=\"bidirectional\", dropout=0, input_mode=\"linear_input\", is_training=true, rnn_mode=\"rnn_r\r\nelu\", seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](tower_0/cudnn_\r\nrnn_stack/transpose, tower_0/cudnn_rnn_stack/cudnn_rnn_relu/zeros, tower_0/cudnn_rnn_stack/cu\r\ndnn_rnn_relu/Const, cudnn_rnn_stack/cudnn_rnn_relu/opaque_kernel/read)]]\r\n         [[Node: Adam/update_cudnn_rnn_stack/cudnn_rnn_relu/opaque_kernel/ApplyAdam/_870 = _R\r\necv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send\r\n_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_nam\r\ne=\"edge_2200_Adam/update_cudnn_rnn_stack/cudnn_rnn_relu/opaque_kernel/ApplyAdam\", tensor_type\r\n=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\n\r\nRunning it on a distributed gpu setting in gcp instances", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Any help, yet ?filled it in the template now.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17409, "title": "TensorRT support for ARM architectures", "body": "i.e. without this modification Tensorflow under Nvidia Jetpack 3.2 is not compiling. Else condition in if-then-else block generates a path \"%s/../include\" which results in \"/usr/lib/include\" but it must be \"/usr/include/aarch64-linux-gnu\"... So this fix targets ARM architectures and with this fix, Tensorflow 1.6rc compiles fine with TensorRT support.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->", "Any chance this can go into a r1.7 patch release?"]}, {"number": 17408, "title": "Branch 187691555", "body": "", "comments": []}, {"number": 17407, "title": "ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"embedding_1/random_uniform:0\", shape=(20000, 200), dtype=float32)'", "body": "I am not sure how to fix this error:\r\nWhen I run https://github.com/laviavigdor/twitter-sentiment-analysis/issues/5 I get this error:\r\n\r\n```\r\n[jalal@goku twitter-sentiment-analysis]$  echo \"This is a sample tweet to predict on\" | python predict.py\r\nUsing TensorFlow backend.\r\n2018-03-03 22:15:52.298930: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-03-03 22:15:52.298964: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-03-03 22:15:52.298974: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-03-03 22:15:52.298981: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-03-03 22:15:52.298987: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-03-03 22:15:52.521523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6705\r\npciBusID 0000:05:00.0\r\nTotal memory: 10.92GiB\r\nFree memory: 9.92GiB\r\n2018-03-03 22:15:52.742797: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x5626add8a580 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2018-03-03 22:15:52.743474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6705\r\npciBusID 0000:06:00.0\r\nTotal memory: 10.92GiB\r\nFree memory: 10.76GiB\r\n2018-03-03 22:15:52.744161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 \r\n2018-03-03 22:15:52.744178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y \r\n2018-03-03 22:15:52.744185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y \r\n2018-03-03 22:15:52.744197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0)\r\n2018-03-03 22:15:52.744206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0)\r\n/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/preprocessing/text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\r\n  warnings.warn('The `nb_words` argument in `Tokenizer` '\r\n/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/engine/topology.py:1253: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\r\n  return cls(**config)\r\n/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/engine/topology.py:1253: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(trainable=False, name=\"embedding_1\", activity_regularizer=None, input_dtype=\"int32\", mask_zero=False, input_dim=20000, batch_input_shape=[None, 100..., output_dim=200, input_length=1000, embeddings_initializer=\"uniform\", embeddings_regularizer=None, embeddings_constraint=None)`\r\n  return cls(**config)\r\nTraceback (most recent call last):\r\n  File \"predict.py\", line 41, in <module>\r\n    main()\r\n  File \"predict.py\", line 20, in main\r\n    model = load_model('model.h5')\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/models.py\", line 239, in load_model\r\n    model = model_from_config(model_config, custom_objects=custom_objects)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/models.py\", line 313, in model_from_config\r\n    return layer_module.deserialize(config, custom_objects=custom_objects)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/layers/__init__.py\", line 55, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/utils/generic_utils.py\", line 139, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/models.py\", line 1249, in from_config\r\n    model.add(layer)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/models.py\", line 442, in add\r\n    layer(x)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/engine/topology.py\", line 576, in __call__\r\n    self.build(input_shapes[0])\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/layers/embeddings.py\", line 101, in build\r\n    dtype=self.dtype)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 87, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/engine/topology.py\", line 400, in add_weight\r\n    constraint=constraint)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 376, in variable\r\n    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 199, in __init__\r\n    expected_shape=expected_shape)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 289, in _init_from_args\r\n    initial_value, name=\"initial_value\", dtype=dtype)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 611, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 676, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 549, in _TensorTensorConversionFunction\r\n    % (dtype.name, t.dtype.name, str(t)))\r\nValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"embedding_1/random_uniform:0\", shape=(20000, 200), dtype=float32)'\r\n[jalal@goku twitter-sentiment-analysis]$ \r\n```\r\n\r\nHow should I fix it?\r\n\r\n```\r\n[jalal@goku twitter-sentiment-analysis]$ conda list | grep -i keras\r\nkeras                     2.0.9                    py36_0    conda-forge\r\n[jalal@goku twitter-sentiment-analysis]$ conda list | grep -i tensorflow\r\ntensorflow-gpu            1.3.0                         0  \r\ntensorflow-gpu-base       1.3.0           py36cuda8.0cudnn6.0_1  \r\ntensorflow-tensorboard    0.1.5                    py36_0  \r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available (in particular, all the information asked for in the issue template and in the comment above by @tensorflowbutler), and we will reopen the issue. Thanks!"]}, {"number": 17406, "title": "updating documentation", "body": "Fix for #16555 ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it! ", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 17405, "title": "No attributed named 'constant'", "body": "I used the following lines of code (from tensorflow.org website)\r\n\r\nconda create -n tensorflow pip python=3.6 \r\nactivate tensorflow\r\npip install --ignore-installed --upgrade tensorflow\r\n\r\nThe installation seems to be successful. However, I keep get an error saying TensorFlow has no attribute named 'constant' or 'InteractiveSessions' when I use tf.constant() or tf.InteractiveSessions(). I use a 64-bit Python version 3.6.3 and the OS is Windows 7 Enterprise.  \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code            No\r\nOS Platform and Distribution        Windows 7 Enterprise\r\nTensorFlow installed from             Tensorflow.org website  \r\nTensorFlow version                        Unable to find info\r\nBazel version                                  Unable to find info\r\nCUDA/cuDNN version                   Unable to find info \r\nGPU model and memory               N/A (used CPU version)\r\nExact command to reproduce       sess=tf.InteractiveSessions()   and   hello = tf.constant('Hello World') ", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 17404, "title": "Add tensor support for num_spectrogram_bins in linear_to_mel_weight_matrix", "body": "This fix tries to address the issue raised in #16553 where it was not possible to provide num_spectrogram_bins as a tensor or placeholder for linear_to_mel_weight_matrix.\r\n\r\nThe reason comes from the implementation of `_validate_arguments` which requires num_spectrogram_bins to be a python int.\r\n\r\nHowever, the validation here is not necessary as `num_spectrogram_bins` will be passed to `math_ops.linspace`, which performs the validation anyway.\r\n\r\nThe validation in `math_ops.linspace` is done in shape function and in kernel's `Compute()`.\r\n\r\nFor that it makes sense to remove the validation of `num_spectrogram_bins` in `_validate_arguments` so that the issue raised in 16553 could be addressed.\r\n\r\nThis fix adds a test case to cover the changes. Also, the error case of `num_spectrogram_bins < 0` has already been covered in the existing test case:\r\nhttps://github.com/tensorflow/tensorflow/blob/013a6c7b3112573ba4d932c8a22bfaf45f648c77/tensorflow/contrib/signal/python/kernel_tests/mel_ops_test.py#L149-L165\r\n\r\nThis fix fixes #16553.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Nagging Assignee @rryan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks @rryan for the review. The PR has been updated. Please take a look."]}, {"number": 17403, "title": "ValueError: features should be a dictionary of `Tensor`s. Given type: <class 'NoneType'>", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacBook Pro, MacOs High Sierra 10.13.3\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.6.1\r\n- **Bazel version (if compiling from source)**:n.a.\r\n- **GCC/Compiler version (if compiling from source)**: n.a.\r\n- **CUDA/cuDNN version**: n.a.\r\n- **GPU model and memory**:n.a.\r\n- **Exact command to reproduce**: \r\ngit clone https://github.com/tensorflow/models\r\ncd models/samples/core/get_started/\r\npython premade_estimator.py\r\n\r\nDarwin ZZZM30774783A 17.4.0 Darwin Kernel Version 17.4.0: Sun Dec 17 09:19:54 PST 2017; root:xnu-4570.41.2~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.13.3\r\n== are we in docker =============================================\r\nNo\r\n== compiler =====================================================\r\nApple LLVM version 9.0.0 (clang-900.0.39.2)\r\nTarget: x86_64-apple-darwin17.4.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n== uname -a =====================================================\r\nDarwin ZZZM30774783A 17.4.0 Darwin Kernel Version 17.4.0: Sun Dec 17 09:19:54 PST 2017; root:xnu-4570.41.2~1/RELEASE_X86_64 x86_64\r\n== check pips ===================================================\r\nnumpy (1.12.1)\r\nprotobuf (3.4.0)\r\ntensorflow (1.4.0)\r\ntensorflow-tensorboard (0.4.0rc2)\r\n== check for virtualenv =========================================\r\nTrue\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.4.0\r\ntf.GIT_VERSION = v1.4.0-rc1-11-g130a514\r\ntf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514\r\nSanity check: array([1], dtype=int32)\r\n/Users/xxx/tensorflow/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n== cuda libs  ===================================================\r\n\r\n\r\n### Describe the problem\r\nI am following the tutorial https://www.tensorflow.org/get_started/premade_estimators and I get the following error\r\n\r\nINFO:tensorflow:Loss for final step: 4.43589.\r\nTraceback (most recent call last):\r\n  File \"premade_estimator.py\", line 88, in <module>\r\n    tf.app.run(main)\r\n  File \"/Users/d069082/tensorflow/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"premade_estimator.py\", line 57, in main\r\n    input_fn=lambda:iris_data.eval_input_fn(test_x, test_y,\r\n  File \"/Users/d069082/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 355, in evaluate\r\n    name=name)\r\n  File \"/Users/d069082/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 810, in _evaluate_model\r\n    features, labels, model_fn_lib.ModeKeys.EVAL, self.config)\r\n  File \"/Users/d069082/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 694, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/Users/d069082/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/canned/dnn.py\", line 334, in _model_fn\r\n    config=config)\r\n  File \"/Users/d069082/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/canned/dnn.py\", line 167, in _dnn_model_fn\r\n    'Given type: {}'.format(type(features)))\r\nValueError: features should be a dictionary of `Tensor`s. Given type: <class 'NoneType'>\r\n\r\n\r\n", "comments": ["@luto65 looking at the backtracke and source a little, it seems like \"features\" in _evaluate_mode is turning out to be None. That comes from a call to _get_features_and_labels_from_input_fn. \r\n\r\nSo if you wouldn't mind a little bit of Python debugging, can you please check that the input_fun you're passing in is sane?\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing for lack of response; please reopen if you've got more information. "]}, {"number": 17402, "title": "Add missing spaces in multiline strings", "body": "Came across a missing space in a `ValueError` exception, and I run quick grep to find other instances of the same issue in the codebase.\r\n```bash\r\n> grep -Przoi \".*[a-z]{2}['\\\"]\\s+['\\\"][a-z].*\" . --include \\*.py\r\n``` \r\nIt's not a big deal, but I figured it wouldn't hurt to submit a fix. Hope it helps...", "comments": []}, {"number": 17401, "title": "tf.constant and tf.ones behave differently with unknown shape dimension", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: GTX 1050 Ti\r\n- **Exact command to reproduce**: see below\r\n\r\n### The problem\r\nThe following code works \r\n```\r\nimport tensorflow as tf\r\nbatch_size = tf.placeholder(tf.int32, shape=[])\r\nones = tf.ones((batch_size,))\r\n```\r\nwhile the following does not work\r\n```\r\nimport tensorflow as tf\r\nbatch_size = tf.placeholder(tf.int32, shape=[])\r\nones = tf.constant(1, shape=(batch_size,))\r\n\r\n>>> ValueError: setting an array element with a sequence.\r\n```\r\n\r\nTo my mind, these should be equivalent ways of creating a tensor of ones with dynamic size. This is particularly annoying if one wants to create a constant tensor with a value different from 0 or 1.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "Done.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm glad it's *possible* to do what you need to do. I agree our API is imperfect and has room for improvement. We have limited flexibility to make it easier to use, due to our [API stability promises](https://www.tensorflow.org/programmers_guide/version_compat) and API changes generally require high level approval. If you feel strongly that this should be a feature request, please provide more details on what you'd like to see happen."]}, {"number": 17400, "title": "Eager: Allowing GPU memory growth", "body": "Since we don't have session in eager mode, how can we allocate only as much GPU memory as needed in our program?\r\n\r\nOr can we set the fraction of the overall amount of memory that each visible GPU should be allocated?\r\n\r\nWhen will this feature be supported?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce"]}, {"number": 17399, "title": "Eager: Using tfe.gradients_function with the params keyword returns a function which cannot be called with lists of values", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian Stretch\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 3.6.2\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\n\r\ndef f(a, b):\r\n    return a**2 + b**3\r\n\r\npartial_grad = tfe.gradients_function(f, params=[0])\r\npartial_grad([1., 2., 3.], [1., 2., 3.])\r\n```\r\n\r\nWhen I try to use ```tfe.gradients_function``` with the keyword params, it is not possible to call the resulting function ```partial_grad```with a list of values.\r\n\r\n## Workarounds:\r\n\r\nUsing np.array([1., 2., 3.]) as input for partial_grad works.\r\n\r\n## Traceback:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-4-cb367c5f1046> in <module>()\r\n      1 partial_grad = tfe.gradients_function(f, params=[0])\r\n----> 2 partial_grad([1., 2., 3.], [1., 2., 3.])\r\n\r\n~/programs/miniconda3/envs/gym/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)\r\n    512     \"\"\"Computes the gradient of the decorated function.\"\"\"\r\n    513 \r\n--> 514     _, grad = val_and_grad_function(f, params=params)(*args, **kwds)\r\n    515     return grad\r\n    516 \r\n\r\n~/programs/miniconda3/envs/gym/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)\r\n    611       raise ValueError(\"Functions to be differentiated cannot \"\r\n    612                        \"receive keyword arguments.\")\r\n--> 613     val, vjp = make_vjp(f, params)(*args, **kwds)\r\n    614     return val, vjp(dy=dy)\r\n    615 \r\n\r\n~/programs/miniconda3/envs/gym/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)\r\n    665         sources.append(args[i])\r\n    666         tape.watch(args[i])\r\n--> 667       result = f(*args)\r\n    668       if result is None:\r\n    669         raise ValueError(\"Cannot differentiate a function that returns None; \"\r\n\r\n<ipython-input-2-159855d98f00> in f(a, b)\r\n      1 def f(a, b):\r\n----> 2     return a**2 + b**3\r\n\r\nTypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Note that this code doesn't work even without gradients_function, because the operator ** isn't defined for python lists. If you convert your arguments to tensors using tf.convert_to_tensor or tf.constant it should be ok.", "Yes, you're absolutely right.\r\nIf I use tf.pow instead, everything works fine, even with lists.\r\nThanks!"]}, {"number": 17398, "title": " Failed to load the native TensorFlow runtime.", "body": "Hello,\r\n\r\ntrying to run :\r\nimport tensorflow as tf\r\nand I obtain the following error:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\hp\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\hp\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\hp\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\hp\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["See this report, #17386.", "This error msg is different from #17386.\r\n\r\nTry to use [Process Monitor](https://docs.microsoft.com/en-us/sysinternals/downloads/procmon) to find out which DLL is missing.\r\n\r\n- add filters: Process Name is python.exe; Path ends with .dll\r\n\r\n- add highlight: Result ends with NOT FOUND", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17397, "title": "Error triggers when gradients calculated inside the tf.map_fn function", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.6.0-rc1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.2.1\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen I try to calculate jacobian using `map_fn` I get then following error: ``Could not write to TensorArray index 1 because it has already been read.``\r\n\r\nI've checked one of the similar issues, but suggested solutions didn't help to solve my problem: https://github.com/tensorflow/tensorflow/issues/13983\r\n\r\nBased on the error logs it looks to me that problem happens because I try to calculate gradient multiple times with the same set of parameters. I tried to run my script with `n_samples=1` (see code below) and I get the expected result without error. In addition, error happens with index 1, which again point that probably first iteration was finished properly and error appears when function reaches second  sample.\r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef jacobians(errors, parameters):\r\n    return tf.map_fn(\r\n        fn=lambda x, params=parameters: tf.gradients(x, params),\r\n        elems=errors,\r\n        dtype=[x.dtype for x in parameters],\r\n        back_prop=False,\r\n        name='jacobian',\r\n        parallel_iterations=1)\r\n\r\n\r\nx = tf.placeholder(tf.float32, (None, 2))\r\ny = tf.placeholder(tf.float32, (None, 1))\r\nweight = tf.Variable(np.random.random((2, 1)), dtype=tf.float32)\r\nbias = tf.Variable(np.random.random((1, 1)), dtype=tf.float32)\r\n\r\nprediction = tf.matmul(x, weight) + bias\r\nerrors = tf.square(y - prediction)\r\n\r\nn_samples = 5\r\nJs = jacobians(errors, [weight, bias])\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(Js, {\r\n        x: np.random.random((n_samples, 2)),\r\n        y: np.random.random((n_samples, 1)),\r\n    })\r\n```\r\n\r\nWhen I run this code I get the following exception\r\n\r\n```\r\n/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-03-03 11:15:16.808741: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at tensor_array_ops.cc:415 : Invalid argument: TensorArray jacobian/TensorArray_1@jacobian/while/gradients: Could not write to TensorArray index 1 because it has already been read.\r\nTraceback (most recent call last):\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1361, in _do_call\r\n    return fn(*args)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1340, in _run_fn\r\n    target_list, status, run_metadata)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray jacobian/TensorArray_1@jacobian/while/gradients: Could not write to TensorArray index 1 because it has already been read.\r\n\t [[Node: jacobian/while/gradients/jacobian/while/TensorArrayReadV3_grad/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](jacobian/while/gradients/jacobian/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3, jacobian/while/Identity, jacobian/while/gradients/Fill, jacobian/while/TensorArrayReadV3/Enter_1)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/itdxer/Downloads/jacobian_fail_example.py\", line 29, in <module>\r\n    y: np.random.random((n_samples, 1))\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1137, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\r\n    options, run_metadata)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray jacobian/TensorArray_1@jacobian/while/gradients: Could not write to TensorArray index 1 because it has already been read.\r\n\t [[Node: jacobian/while/gradients/jacobian/while/TensorArrayReadV3_grad/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](jacobian/while/gradients/jacobian/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3, jacobian/while/Identity, jacobian/while/gradients/Fill, jacobian/while/TensorArrayReadV3/Enter_1)]]\r\n\r\nCaused by op 'jacobian/while/gradients/jacobian/while/TensorArrayReadV3_grad/TensorArrayWrite/TensorArrayWriteV3', defined at:\r\n  File \"/Users/itdxer/Downloads/jacobian_fail_example.py\", line 23, in <module>\r\n    Js = jacobians(errors, [weight, bias])\r\n  File \"/Users/itdxer/Downloads/jacobian_fail_example.py\", line 11, in jacobians\r\n    parallel_iterations=1)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\", line 413, in map_fn\r\n    swap_memory=swap_memory)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3278, in while_loop\r\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3013, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2953, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\", line 403, in compute\r\n    packed_fn_values = fn(packed_values)\r\n  File \"/Users/itdxer/Downloads/jacobian_fail_example.py\", line 7, in <lambda>\r\n    fn=lambda x, params=parameters: tf.gradients(x, params),\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 611, in gradients\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 377, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 611, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py\", line 105, in _TensorArrayReadGrad\r\n    w_g = g.write(index, grad)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 118, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 879, in write\r\n    return self._implementation.write(index, value, name=name)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 118, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 278, in write\r\n    name=name)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 7385, in _tensor_array_write_v3\r\n    flow_in=flow_in, name=name)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3270, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n...which was originally created as op 'jacobian/while/TensorArrayReadV3', defined at:\r\n  File \"/Users/itdxer/Downloads/jacobian_fail_example.py\", line 23, in <module>\r\n    Js = jacobians(errors, [weight, bias])\r\n[elided 4 identical lines from previous traceback]\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2953, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\", line 402, in compute\r\n    packed_values = input_pack([elem_ta.read(i) for elem_ta in elems_ta])\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\", line 402, in <listcomp>\r\n    packed_values = input_pack([elem_ta.read(i) for elem_ta in elems_ta])\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 58, in fn\r\n    return method(self, *args, **kwargs)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 58, in fn\r\n    return method(self, *args, **kwargs)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 58, in fn\r\n    return method(self, *args, **kwargs)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 861, in read\r\n    return self._implementation.read(index, name=name)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 260, in read\r\n    name=name)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 6468, in _tensor_array_read_v3\r\n    dtype=dtype, name=name)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3270, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): TensorArray jacobian/TensorArray_1@jacobian/while/gradients: Could not write to TensorArray index 1 because it has already been read.\r\n\t [[Node: jacobian/while/gradients/jacobian/while/TensorArrayReadV3_grad/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](jacobian/while/gradients/jacobian/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3, jacobian/while/Identity, jacobian/while/gradients/Fill, jacobian/while/TensorArrayReadV3/Enter_1)]]\r\n```", "comments": ["Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It looks like you're trying to take gradients of values inside the loop w.r.t. tensors that are outside the loop, e.g. through the body arguments. Is that right?  This isn't supported. I think next version of TF will give a clearer error message.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 61 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I have the same error when trying to calculate tf.gradients() using tf.scan(). Is this problem solved in TF1.15?"]}, {"number": 17396, "title": "[WIP] MKL repos download for mac and windows + update to MKL(-DNN) 0.12", "body": "This PR implements MKL repos for Mac (tested) and Windows (not tested yet)\r\nAs well as reworks MKL.BUILD to support all 3 desktop platforms\r\n\r\n**Update 1** Unfortunately, because of bazel bug https://github.com/bazelbuild/bazel/issues/4480 you will still experience loader issues on Mac and you will be forced to install MKL globally or deal with DYLD_LIBRARY_PATH to overcome invalid rpath in generated .so files of tensorflow (similar to https://github.com/tensorflow/tensorflow/issues/6729 for CUDA libraries in dependencies).\r\n\r\n**Update 2** Bazel 0.12.0 (not yet released) fixes the issue and it builds and runs flawlessly (verified with [0.12.0rc1](https://releases.bazel.build/0.12.0/rc1/index.html) ) ", "comments": ["@gunan `mkl_util.h` changes fix a bug, which breaks even linux build. it was discussed here https://github.com/tensorflow/tensorflow/pull/17392#pullrequestreview-101268242", "Bazel 0.12.0 (not yet released) fixes the linker issue and it builds and runs flawlessly with downloaded MKL libs (verified with [0.12.0rc1](https://releases.bazel.build/0.12.0/rc1/index.html)) ", "@anton-matosov can you resolve the merge conflicts. ", "@sb2nov done", "\ud83c\udf89", "According to Update 2 Bazel 0.12.0 (not yet released) fixes the issue and it builds and runs flawlessly (verified with 0.12.0rc1 )\r\n\r\nI tried with the install version (not homebrew) of 0.12.0 bazel  using this line:\r\n\r\nbazel build --config=opt --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n\r\nand I receive this message:\r\nerror: unknown target CPU 'armv7-a' \r\n\r\nThere are messages on-line that suggest that this is a problem with 0.12.0, but 0.12.0 is needed for the MKL support.\r\nhttps://github.com/bazelbuild/bazel/issues/4794   by @FrederickGeek8   near the bottom\r\n\r\nThis is part of the verbose output:\r\n\r\nbazel-out/host/genfiles/external/bazel_tools -O3 -w -D__ARM_NEON__ '-march=armv7-a'", "bazel 0.12.0 was release couple days ago, check it out (I didn't yet, but should work)\r\nto bypass CPU 'armv7-a' issue with pre-release binary specify `--cpu=darwin` argument  (for mac build) to the bazel call. More details here https://github.com/tensorflow/tensorflow/pull/17392#issuecomment-375420834"]}]