[{"number": 486, "title": "string_input_producer has no analogue for other dtypes", "body": "It would be nice to a corresponding int_input_producer for producing a queue cycles through labels, for instance. This is a super easy fix by just adding another wrapper around _input_producer.\n", "comments": ["@josh11b: Why isn't this just `input_producer`?  It doesn't seem to be at all string specific.\n", "It seems like they wanted to make it convenient to loop through filenames for internal reasons, but for some reason didn't need it for any other file type. But it's just as easy to use _input_producer with another data type.\n", "@bschreck: `_input_producer` is a fine solution for now, but it isn't part of the public API and will go away once we implement `__all__`.  However, unless there's a reason we should probably remove the underscore and make it part of the public API.\n", "+1\n\nOn Fri, Dec 18, 2015 at 4:35 PM Geoffrey Irving notifications@github.com\nwrote:\n\n> @bschreck https://github.com/bschreck: _input_producer is a fine\n> solution for now, but it isn't part of the public API and will go away once\n> we implement __all__. However, unless there's a reason we should probably\n> remove the underscore and make it part of the public API.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/486#issuecomment-165927236\n> .\n"]}, {"number": 485, "title": "Backport for moved models.rnn.linear doesn't work", "body": "In v0.6 linear got moved into `ops.rnn_cell.linear`, but backport here `https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/linear.py` points to `tf.nn.linear`.\n\nThis break code with v0.5.\n", "comments": ["Thanks, @ebrevdo can fix this.\n\nBtw, that code was never part of the public API :(\n", "Well, since it wasn't part of the official API, this isn't really a bug. And I believe it still isn't documented, so it's still not \"ready\".  Closing for now!\n"]}, {"number": 484, "title": "Move public protobufs into a special directory", "body": "Currently all the `.proto` definitions are in `core/framework`, but not all of them are public.  For now I'm going to list the public ones in the version document, but this is obviously silly.\n", "comments": ["I think @josh11b fixed the proto visibliity issues.\n"]}, {"number": 483, "title": "Request for angle", "body": "Getting the angle of a complex number is another mathematical operation that would be useful / basic to have, along the lines of [np.angle](http://docs.scipy.org/doc/numpy/reference/generated/numpy.angle.html).\n", "comments": ["That's a good feature request.  For complex numbers, it should be called `tf.arg` or `tf.argument`.\n", "I'm a part of a group of 4 students from Drexel University who are currently working on this. Because we are not very familiar with tensorflow we are going to start by implementing tf.argument() directly as a kernel without a GPU kernel. After we get that we are going to try to implement atan, as implemented by eigen, and then making the tf.argument function a composition. Let us know if you have any tips, advice, or feedback. \n", "If you write the op in all Eigen (which seems entirely achievable), you\nshould be able to write it once for both CPU and GPU. AdjustContrast is an\nexample of that, and how the final op is registered.\n\nOn Fri, Mar 4, 2016 at 7:48 AM Ryan Young notifications@github.com wrote:\n\n> I'm a part of a group of 4 students from Drexel University who are\n> currently working on this. Because we are not very familiar with tensorflow\n> we are going to start by implementing tf.argument() directly as a kernel\n> without a GPU kernel. After we get that we are going to try to implement\n> atan, as implemented by eigen, and then making the tf.argument function a\n> composition. Let us know if you have any tips, advice, or feedback.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/483#issuecomment-192329516\n> .\n", "Thanks! We will take a look at that.\n", "Let's call it `tf.arg` rather than `tf.argument` for symmetry with `tf.abs`.\n", "Will do. We found an implementation of argument in eigen called scalar_arg_op. Should we use that implementation or proceed with registering atan and defining arg as a tf function composition?\n", "Since Eigen already has scalar_arg_op defined, we might as well use that for better performance on both CPU and GPU :)\n", "Ok, we can do that.\n", "We were able to get arctan implemented and working great with complex64 numbers. We're running into an issue with registering the scalar_arg_op with the complex64 data type. We can get arg to compile when we register it only with floats and double without errors, but when we call it in python it fails when passed a complex64 as expected. We think the error we are getting stems from: \n\n```\nno known conversion for argument 1 from \n'Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 1, 1, long int>, 16>, \nEigen::ThreadPoolDevice>::Scalar* {aka std::complex<float>*}' to 'Eigen::TensorEvaluator<const \nEigen::TensorCwiseUnaryOp<Eigen::internal::scalar_arg_op<std::complex<float> >, const \nEigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16> >, \nEigen::ThreadPoolDevice>::Scalar* {aka float*}\n```\n\ndoes this look familiar to anyone that could point us in the right direction? If not we will use arctan to implement arg.  \n", "If you present a compile error, you'll also need to present the code that doesn't comiple. It seems like you are trying to assign a complex<float> array to a float.\n", "I'm working with Ryan on this. We have it working utilizing the Eigen implementation of arg and are currently in the process of writing tests for it. We haven't been able to test it on a GPU enabled device but will likely be able to do that soon as well, should we wait until we do that to submit a pull request? Also, we were curious if we should include the code we created for implementing arctan through Eigen before we knew that Eigen had a method for computing the argument directly.\n", "Yep, let's wait for the GPU enabled device test.  As for the custom arg code, don't include it unless it is used in the final version.\n", "Status?\n", "Hi, sorry for the delay in updates. We finished implementing the feature but still have not been able to access a machine with a GPU to test if it is fully functional. If anyone would like to help us finish testing and get the code base up to a state where we can submit a pull request I'd be more than happy to add them as a collaborator or they can fork my fork.  \n", "@RyanYoung25: I have a machine with a GPU and can help you with this.\nYou could start by creating a PR that adds `tf.arg` for the CPU, ideally with a few tests to make sure that it works correctly.\n", "Any updates on this?\n", "One up. Any updates on that?  ", "(This is obviously only a temporary fix for those that find themselves at this thread before the official tf.arg is added). You can do it with existing tensorflow operations, like:\r\n\r\n    def angle(z):\r\n        \"\"\"\r\n        Returns the elementwise arctan of z, choosing the quadrant correctly.\r\n\r\n        Quadrant I: arctan(y/x)\r\n        Qaudrant II: \u03c0 + arctan(y/x) (phase of x<0, y=0 is \u03c0)\r\n        Quadrant III: -\u03c0 + arctan(y/x)\r\n        Quadrant IV: arctan(y/x)\r\n\r\n        Inputs:\r\n            z: tf.complex64 or tf.complex128 tensor\r\n        Retunrs:\r\n            Angle of z\r\n        \"\"\"\r\n        if z.dtype == tf.complex128:\r\n            dtype = tf.float64\r\n        else:\r\n            dtype = tf.float32\r\n        x = tf.real(z)\r\n        y = tf.imag(z)\r\n        xneg = tf.cast(x < 0.0, dtype)\r\n        yneg = tf.cast(y < 0.0, dtype)\r\n        ypos = tf.cast(y >= 0.0, dtype)\r\n\r\n        offset = xneg * (ypos - yneg) * np.pi\r\n\r\n        return tf.atan(y / x) + offset", "Also, I would have thought calling it `tf.angle` would be nicer than `tf.arg`, as it then mimics numpy's `np.angle`", "> Also, I would have thought calling it tf.angle would be nicer than tf.arg, as it then mimics numpy's \r\nnp.angle\r\n\r\nConformity is great, but then for API symmetry, `tf.abs` should also have an alias `tf.magnitude` that only takes complex numbers, no? It's rather NumPy that goofed by having abs(z) but not arg(z), IMO. Also, angle could refer to a lot of things not related to complex numbers, in my mind.", "Short question - is the GPU version already working? I was wondering why my model is so slow, a device_placement log gave me this:\r\n`Back_Propagate/Back_Propagate_Step/Propagate/Angle: (Angle): /job:localhost/replica:0/task:0/device:CPU:0\r\n`\r\nAll other Nodes are on GPU though. Right now I'm using TF v1.13 on Windows ", "No, the original patch 227fc9f had build failures for GPU (see also #10643). Potentially those are gone, so it's worth enabling again. \r\n\r\nSee the patch for the `#if 0`s, maybe try removing those and see whether that works."]}, {"number": 482, "title": "error in running sequence to sequence model demo using bazel", "body": "![1](https://cloud.githubusercontent.com/assets/10511526/11749467/edc39e5c-a053-11e5-90f7-739ef703e47c.png)\n![2](https://cloud.githubusercontent.com/assets/10511526/11749468/edc7d7ba-a053-11e5-8d04-0532412c76d2.png)\n\n  I am using bazel for compiling and run in tensor flow. when i try to run the sequence to sequence example i got the following error. If anyone knows help us. I also tried to execute\n using python translate.py --data_dir <path for data>. it is also showing an error. The image 1 shows the error when i try with bazel and image 2 shows the error when i try with python translate.py method.\n\nfor bazel\n\nbazel run -c opt <...>/models/rnn/translate/translate.py\n  --data_dir [your_data_directory]\n\nshowing error\n\nERROR: Cannot run target //tensorflow/models/rnn/translate:translate.py: Not executable.\nINFO: Elapsed time: 0.089s\nERROR: Build failed. Not running target.\n\npython translate.py showing error\n\nTraceback (most recent call last):\n  File \"translate.py\", line 46, in <module>\n    from tensorflow.models.rnn.translate import data_utils\nImportError: No module named translate\n", "comments": ["If using bazel:\n\nbazel run -c opt //tensorflow/models/rnn/translate:translate -- --data_dir [...]\n\nYou need a \"--\" before \"--data_dir\"\n\nIf running from python, what version of tensorflow do you have installed?  You need 0.6.0 for running translate.py directly to work.\n", "thanks. it works, i checked bazel with tensorflow 0.5.0 and python with tensorflow 0.6.0\n\nBut another problem is in both I am getting the following error after some time. please have a look in the attached image\n![13](https://cloud.githubusercontent.com/assets/10511526/11760997/b20cd746-a0d7-11e5-960e-58e681af16b4.png)\n", "That's a network error when downloading what looks like dependencies. It's\nmost likely due to a bad internet connection.\n\nOn Sat, Dec 12, 2015 at 12:24 AM vnkmr7620 notifications@github.com wrote:\n\n> thanks. it works, i checked bazel with tensorflow 0.5.0 and python with\n> tensorflow 0.6.0\n> \n> But anpther problem is in both I am getting the following error after some\n> time. please have a look in the attached image\n> [image: 13]\n> https://cloud.githubusercontent.com/assets/10511526/11760997/b20cd746-a0d7-11e5-960e-58e681af16b4.png\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/482#issuecomment-164125198\n> .\n", "i solved this error. But after downloading it started showing tokenizing and after some time again got error. please have a look on the attached image. (you can find downloaded files as well as the error\n![wmtbazelerror](https://cloud.githubusercontent.com/assets/10511526/11765697/768b05e2-a188-11e5-958b-31b4edf30860.png)\n)\n", "I am running programs on 2GB RAM machine. os type 64 bit, disk space 160 GB processor intel core 2 dueo. Is that becoz of memory prob\n", "hi all,\n           i tried with  bazel run -c opt tensorflow/models/rnn/translate:translate --local_resources 2042,.5,1.0 -- --data_dir /home/cencluster/wmtbazel/\n But it is also issuing the error. Please have a look for the error.\n![local res language modeling](https://cloud.githubusercontent.com/assets/10511526/11766651/e103d1ae-a1b7-11e5-910c-b89a76e0687d.png)\n", "Bazel is already done by the time you get the error. You can try running without bazel altogether (after building):\n\n```\nbazel-bin/tensorflow/models/rnn/translate/translate --data_dir /home/cencluster/wmtbazel/\n```\n\nBut probably, the model is probably too big to run in 2G RAM. \n", "not solved. But this time getting different error.Please have a look on the attached image\n![error in language](https://cloud.githubusercontent.com/assets/10511526/11775069/afedea00-a261-11e5-8dd8-36f4ced1dcee.png)\n", "Unfortunately, I cannot see the actual error in that screenshot. It's quite likely that bazel wasn't using all that much memory, and that you're still out of memory. You should paste the actual program output here, the screenshot doesn't contain much information.\n", "python translate.py --decode --data_dir /home/cencluster/wmt15/ --train_dir /home/cencluster/wmt15/train\n\n> who is the president of the United States?\n\nGetting the following output. But is is not correct what you have mentioned.\n![output of language modelling](https://cloud.githubusercontent.com/assets/10511526/11800855/6db52b1c-a306-11e5-9fac-f794270edac9.png)\n", "De-duping with https://github.com/tensorflow/tensorflow/issues/600, which has more information.\n"]}, {"number": 481, "title": "Error Running cifar10_train.py in Ver 0.6.0 ", "body": "The cifar-10 demo ran fine under 0.5.0 but gives errors under 0.6.0 (I rolled back to 0.5.0 and confirmed this). Also cifar10_multi_gpu_train.py works under 0.6.0 as well as 0.5.0.  The output I'm getting when running cifar10_train.py in 0.6.0 is attached. I'm running under Ubuntu 14.04.\n[error.txt](https://github.com/tensorflow/tensorflow/files/59525/error.txt)\n", "comments": ["Apologies: we've noticed this internally and fixing it.\n", "Fixed today. Will be available in next release.\n"]}, {"number": 480, "title": "2 bugs in training/input.py", "body": "In the `_dtype()` method in the training/input.py file, the following code for TypeError:\n\n```\n  raise TypeError(\"Expected types to be consistent: %s vs. %s.\" %\n                  \", \".join(x.name for x in types),\n                  \", \".join(x.name for x in other_types))\n```\n\nshould be:\n\n```\n  raise TypeError(\"Expected types to be consistent: %s vs. %s.\" %\n                  (\", \".join(x.name for x in types),\n                  \", \".join(x.name for x in other_types)))\n```\n\nAlso, when using the `tf.train.shuffle_batch_join()` method, you are suppose to pass a list of tuples of tensors, but if you pass a list of tuple and each tuple _only contains one item_, python will ignore the tuples and simply make it into a list. Thus, when the _flatten method in training/inputs.py is called, a `TypeError: 'Tensor' object is not iterable` is thrown. I suppose it's not necessarily a bug as one should know if you want to pass a tuple containing just one item, you must put a comma at the end of the tuple, but it may still may cause confusion.\n\nInstead, _flatten could be:\n\n```\ndef _flatten(tensor_list_list):\n        for elm in tensor_list_list:\n            if not isinstance(elm, tuple):\n                raise TypeError(\"tensor_list_list must contain tuples.\")\n        return [tensor for tensor_list in tensor_list_list for tensor in tensor_list]\n```\n", "comments": ["I have a fix out for the first issue: feel free to send us a PR for the second one!\n", "@vrv: If the actual bug is fixed, should we close this?  I don't think it's a TensorFlow bug that `(1)` isn't a `tuple` in Python.\n", "I think it's fixed.\n", "I just met the same issue **(2)** but about using **tf.train.batch_join()** of the TF' version(0.11.0rc1).  As _**oliverfunk**_ said, i put a comma at the end of the tuple and seems work. \r\n\r\nThanks!\r\n\r\nMy code is like as below:     \r\n\r\nexample_list  = [ \\\r\n        **(** img_preprocess.read_my_file_format(                  \\\r\n        filename_queue,                                                              \\\r\n        reader = tf.WholeFileReader(),                                        \\\r\n        decoder= img_preprocess.my_file_decoder_png,         \\\r\n        shape  = shape,                                                               \\\r\n        isWithLabel= False  **), )** for _ in range(read_threads) ];"]}, {"number": 479, "title": "undefined symbol: PyUnicode_AsUTF8String", "body": "Hi, when I try to do `import tensorflow` I receive the following error:\n\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 50, in <module>\n    from tensorflow.python.framework.framework_lib import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/framework_lib.py\", line 62, in <module>\n    from tensorflow.python.framework.ops import Graph\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 40, in <module>\n    from tensorflow.python.framework import versions\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/versions.py\", line 24, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: PyUnicode_AsUTF8String\n```\n\nOS is Ubuntu 14.04, and I get the same error with both python2.7 and python3.4.\n\nThe binaries works fine, but I'm trying to install from source cause I need support for CUDA 3.0.\n\nThanks\n", "comments": ["Is the version of Python you passed to `./configure` the same as the one you're importing tensorflow from?\n", "I thought it was but I was probably mistaken (?).\nRedoing every steps in the installation guide fixed the problem form me.\n", "We would ideally provide a better error message in this case, but I'm not sure how to do it.  By the time we'd realize the problem, linking has already taken place.\n", "I hit the same missing symbol building on Fedora 24; I'd configured with python3 so I'm wondering if something is trying to use a different version at some stage.\nThis is from the end of:\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\nERROR: /discs/more/git/tensorflow/tensorflow/contrib/session_bundle/example/BUILD:38:1: Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTraceback (most recent call last):\n  File \"/home/dg/.cache/bazel/_bazel_dg/9495d50bfa6d27aea1920e0978e97106/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/contrib/session_bundle/example/export_half_plus_two.py\", line 32, in <module>\n    import tensorflow as tf\n  File \"/home/dg/.cache/bazel/_bazel_dg/9495d50bfa6d27aea1920e0978e97106/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/home/dg/.cache/bazel/_bazel_dg/9495d50bfa6d27aea1920e0978e97106/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 48, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/home/dg/.cache/bazel/_bazel_dg/9495d50bfa6d27aea1920e0978e97106/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/home/dg/.cache/bazel/_bazel_dg/9495d50bfa6d27aea1920e0978e97106/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: /home/dg/.cache/bazel/_bazel_dg/9495d50bfa6d27aea1920e0978e97106/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: PyUnicode_AsUTF8String\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 811.088s, Critical Path: 661.59s\n", "make sure yout python-config is pointing to your python bin.  For simplicity just use python2 for building and you wont have to fix as much\n", "\u6211\u4e5f\u9047\u5230\u4e86\u540c\u6837\u4e86\u95ee\u9898\uff0c\u5728CentOS 7 \u4e0b\uff0c\u8fd9\u662f\u7531\u4e8ePython\u7f16\u8bd1\u65f6\u6ca1\u6709\u6307\u5b9a\u7f16\u7801\u5bfc\u81f4\u7684\uff0c\u8be6\u7ec6\u7684\u89e3\u51b3\u8fc7\u7a0b\u53c2\u89c1http://www.jianshu.com/p/b0b8ab2be12c\uff0c\u5f53\u7136\uff0cUbuntu\u7cfb\u7edf\u548cCentOS\u4e0d\u4e00\u6837\uff0c\u8bf7\u6ce8\u610f\u7504\u522b\u3002\u795d\u4f60\u597d\u8fd0\uff01", "Hi, I am encountering the same error. \r\nI have two Python installations, one of them works fine(2.7.6) with TF and the other(2.7.13) gets this error. I installed separaetly in each one.\r\n\r\n@girving How can I check where ./configure is pointing? \r\n", "@artuntun Look in `tensorflow/tools/python_bin_path.sh`."]}, {"number": 478, "title": "Extend tf.unsorted_segment_sum to allow 'rejecting' entries", "body": "As already discussed in #466, it would be useful to be able to mark entries which are not to be summed anywhere by assigning negative indexes (or just -1) there. @girving suggested, that this behavior should only be enabled when setting a flag (e. g. `drop_negatives`) to preserve the existing error detection behavior, which I support. \n", "comments": ["Added a PR #13055 for the fix."]}, {"number": 477, "title": "An error occurred while installing", "body": "OS\uff1aubuntu 15\npython\uff1a2.7.9\n\nerror Log\uff1a\n/usr/bin/pip run on Fri Dec 11 12:02:09 2015\ntensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.\nException information:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 122, in main\n    status = self.run(options, args)\n  File \"/usr/lib/python2.7/dist-packages/pip/commands/install.py\", line 283, in run\n    InstallRequirement.from_line(name, None))\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 168, in from_line\n    raise UnsupportedWheel(\"%s is not a supported wheel on this platform.\" % wheel.filename)\nUnsupportedWheel: tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.\n", "comments": ["What do you get from: `uname -a` ?\n\nHave you tried the same with our 0.6.0 release?\n", "Is python 3.5 supported? Having the same problem in python 3.5.1\n\n> $ pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl\n> tensorflow-0.6.0-cp34-none-linux_x86_64.whl is not a supported wheel on this platform.\n> \n> $ python --version\n> Python 3.5.1\n> \n> $ uname -a\n> ... 3.19.0-43-generic ... x86_64 GNU/Linux\n", "The Python version is encoded in the wheel filename. You can try to rename\nthe wheel (cp35), but you may have to install from source.\nOn Mon, Jan 25, 2016 at 11:57 cnsgcu notifications@github.com wrote:\n\n> Is python 3.5 supported? Having the same problem in python 3.5.1\n> \n> $ pip3 install --upgrade\n> https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl\n> tensorflow-0.6.0-cp34-none-linux_x86_64.whl is not a supported wheel on\n> this platform.\n> \n> $ python --version\n> Python 3.5.1\n> \n> $ uname -a\n> ... 3.19.0-43-generic ... x86_64 GNU/Linux\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/477#issuecomment-174640036\n> .\n", "Thanks Martin, installed using the rename trick to ...cp35... after wget ... on a Debian stretch with Python 3.5.\n\n@martinwicke what about maintaining another whl file with cp35 for tensorflow?\nIt seems to me that it will solve many upcoming annoyances... ;-)\n", "You seem to be using the 0.5.0 wheel, which has no support for Python 3.\nTry the 0.6.0 Python 3 version.\nOn Tue, Feb 2, 2016 at 07:05 Margus P\u00e4rt notifications@github.com wrote:\n\n> On ubuntu 14.04 getting the same error.\n> \n> /usr/bin/pip3 run on Tue Feb  2 10:02:00 2016\n> tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.\n> Exception information:\n> Traceback (most recent call last):\n>   File \"/usr/lib/python3/dist-packages/pip/basecommand.py\", line 122, in main\n>     status = self.run(options, args)\n>   File \"/usr/lib/python3/dist-packages/pip/commands/install.py\", line 257, in run\n>     InstallRequirement.from_line(name, None))\n>   File \"/usr/lib/python3/dist-packages/pip/req.py\", line 168, in from_line\n>     raise UnsupportedWheel(\"%s is not a supported wheel on this platform.\" % wheel.filename)\n> pip.exceptions.UnsupportedWheel: tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/477#issuecomment-178621505\n> .\n", "The key/solution is the version number in the URL. I was trying to install with pip3 using python 3.4, so I had to change the URL to: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl\n\nThis took waaay too long to figure out :-\\\n", "De-duping with #468, which is the more general issue.\n"]}, {"number": 476, "title": "Make Sure Full Functionality is Available in C/C++", "body": "Please make sure all the functionality is defined in the C++ core.\n", "comments": [":+1:\n", "/cc @saudet\n", "I am currently working on expanding the C API to add support for graph definition.  Not sure when it will be done, but it is one of our goals before 1.0.\n", "@josh11b Great! Though what about automatic differentiation?\n", "Automatic differentiation will eventually be in, though closer to 1.0.  Are you looking for C API support for use in another language or are you using C++ directly?  We consider those separate APIs, but both should have something reasonable for defining graphs (though no shape inference or automatic differentiation, yet) in the next TensorFlow release (0.10.0) -- they are both checked in at head.\n\nC++ graph-definition API update: \nhttps://github.com/tensorflow/tensorflow/commit/25ac3dabfa3af7a313eb46b03690117c85030cc2\n\nC graph-definition API was a few commits, I'm having trouble tracking them all down.\n", "Good to hear that! JavaCPP has no problems interacting with TensorFlow's C++ API directly, so using either API is fine in that case.\n", "Andy will know more about the status of automatic differentiation.\n", "Shape inference is now in C++ as is partial gradients. Work continues, but closing due to lack of recent activity. Please open a new issue referencing this one if it is still an issue. Thanks!", "@aselle Do you mean to say that the C++ interface can now compute gradients and add them to the graph as in python? I don't see anything in the C interface to access this functionality. Is there a separate ticket to track that? Is there any rough timeline on when this functionality is expected?", "@saudet, this issue is overly broad and not clear what it is asking for. Many of the things raised in this issue have been addressed. I closed this to create a more focused issue #7044.\r\n"]}, {"number": 475, "title": "Enable building with GPU support under OS X.", "body": "These changes add GPU/CUDA support when building under OS X. Cuda and CuDNN library directories and file extensions are set based on the current build platform and library versions are stored in easily modifiable variables.\n", "comments": ["@ville-k are there additional steps required for this?  I'm getting\n\n```\nInconsistent crosstool configuration; no toolchain corresponding to 'local_darwin' found for cpu 'darwin'.\n```\n", "@elbamos Thanks for trying that out! Looks like I rushed the pull request and missed a step. I was not passing the \"--config=cuda\" option to bazel and the build code that gives you the error you reported never got run.  I'll:\n1. close this pull request\n2. fix the problem\n3. submit a new pullrequest\n", "Thanks -- please let me know when you file the new one.  I'm eagerly awaiting this... \n\nThere was an exchange on r/machinelearning where the google folks asked us to file a bug report about GPUs not working with Mac OS X, so if you get this working I think there's a good chance they pull it.  Also, you might want to consider modifying the config scripts so the cuda and cudnn versions can bet set as environment variables rather than hardcoding them. \n", "Thanks @ville-k.  Btw, we don't yet accept pull requests from GitHub, only Gerrit.  See https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md for more info\n\nOnce you get something working, we'd be happy to accept this improvement!  @zheng-xq for early visibility on this change.\n", "(And if there's anything I can do to help you, pls let me know!)\n", "@elbamos: I think @zheng-xq was probably going to tackle the cudnn versions issue soon.  Not sure how he plans to do it though.\n", "Can one of the admins verify this patch?\n", "@tensorflow-jenkins  The patch does NOT work. \n", "Ok, we'll close this -- feel free to send us a new PR when it's ready!\n", "@vrv Thanks,  I'm hoping to submit a new PR soon. I have TensorFlow building and starting up on my Mac against CUDA 7.5. I still need to clean it up and debug a crash I'm getting while the GPU memory allocator is getting initialized.\n", "any news?\n", "any news??\n", "See this thread ... https://github.com/tensorflow/tensorflow/pull/664\n", "@tensorflow-jenkins"]}, {"number": 474, "title": "How to change the type of placeholder", "body": "I declared a placeholder with labels=tf.placeholder(tf.float32,shape=(5)), \nwhile when I checked the shape of 'labels' with tf.shape(labels), it returned \nTensor(\"Shape_2:0\", shape=TensorShape([Dimension(1)]), dtype=int32, device=/gpu:1) TensorShape([Dimension(5)]). \n\nObviously I set labels as tf.float32, it is actually int32. How can I resolve it. Thanks in advance. \n", "comments": ["The shape of a tensor is always integers, regardless of the dtype of the tensors.  For example, a 1-D tensor can't have 2.7 elements.\n", "If you want to see the dtype of labels, do `labels.dtype`.\n", "OK, thanks for your very quick feedback.\n"]}, {"number": 473, "title": "Occupies GPU even if it is not compatible", "body": "As the title states, even if a GPU is not CUDA compatible, Tensorflow will occupy the GPU and then proceed to ignore it.\n\nHere's a use case : \n\nCUDA_VISIBLE_DEVICES=3 python convolutional.py\n....\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:669] Ignoring gpu device (device: 0, name: Tesla K10.G2.8GB, pci bus id: 0000:45:00.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5. \n....\nFrom nvidia-dmi\n|   3  Tesla K10.G2.8GB    Off  | 0000:45:00.0     Off |                    0 |\n| N/A   34C    P0    43W / 117W |     50MiB /  3583MiB |      **0%**    E. Thread |\n.....\n|    3     26078    C   python                                          37MiB |\n+-----------------------------------------------------------------------------+\n", "comments": ["@zheng-xq, @noisychannel: Looks like this fell through the cracks.  Is it still an issue?  \n", "Closing due to lack of response.\n"]}, {"number": 472, "title": "bazel test numpy issue: RuntimeError: module compiled against API version a but this version of numpy is 7", "body": "Hello everyone,\n\n  I just wrote my own op, and finally got it to compile, but I'm having trouble running bazel test. Specifically, I get the following error:\n\n> RuntimeError: module compiled against API version a but this version of numpy is 7\n\nI am surprised as I was able to compile tensorflow with bazel, and import it without any problems. Any ideas as to what might be causing this? My test is the following:\n\n``` python\nimport tensorflow as tf\n\nclass ChainCRFTest(tf.test.TestCase):\n    def testChainCRF(self):\n        with self.test_session():\n            pre_pots = [[[1, 3, 1], [4, 1, 2], [2, 1, 1]],\n                        [[2, 2, 1], [3, 1, 1], [1, 2, 3]],\n                        [[3, 1, 1], [2, 1, 2], [1, 3, 2]],\n                        [[1, 2, 1], [4, 1, 3], [1, 1, 1]]]\n            potentials = tf.convert_to_tensor(pre_pots, tf.float32)\n            result = tf.user_ops.chain_crf(potentials)\n            print(result.eval()) # TODO\n\n# bazel test tensorflow/python:chain_crf_op_test --verbose_failures\n```\n\nAnd the full error:\n\n```\n$ cat /home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/testlogs/tensorflow/python/chain_crf_op_test/test.log\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\n-----------------------------------------------------------------------------\nRuntimeError: module compiled against API version a but this version of numpy is 7\nTraceback (most recent call last):\n  File \"/home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/chain_crf_op_test.runfiles/tensorflow/python/kernel_tests/chain_crf_op_test.py\", line 1, in <module>\n    import tensorflow as tf\n  File \"/home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/chain_crf_op_test.runfiles/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/chain_crf_op_test.runfiles/tensorflow/python/__init__.py\", line 50, in <module>\n    from tensorflow.python.framework.framework_lib import *\n  File \"/home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/chain_crf_op_test.runfiles/tensorflow/python/framework/framework_lib.py\", line 62, in <module>\n    from tensorflow.python.framework.ops import Graph\n  File \"/home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/chain_crf_op_test.runfiles/tensorflow/python/framework/ops.py\", line 40, in <module>\n    from tensorflow.python.framework import versions\n  File \"/home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/chain_crf_op_test.runfiles/tensorflow/python/framework/versions.py\", line 24, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/chain_crf_op_test.runfiles/tensorflow/python/pywrap_tensorflow.py\", line 26, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/chain_crf_op_test.runfiles/tensorflow/python/pywrap_tensorflow.py\", line 22, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: numpy.core.multiarray failed to import\n```\n\nThanks in advance,\nYacine\n", "comments": ["This looks like a numpy version issue. Maybe you have more than one version of numpy installed in various places? If you installed from source using the instructions, you should have built against numpy 1.10 (version a), but maybe your runtime environment is set up using an older version?\n", "Do the tests pass without your new op?  It seems very weird that the new op would have anything to do with this, but I haven't looked at that code path and could easily be wrong.\n", "Hello,\n\n  Thanks for the quick replies. Other tests also fail with the same error. I checked my version of numpy (using >>>import numpy; numpy.version.version :  1.10.1),  and was unable to find another (but I'm unfamiliar with how the modules are organized on that machine).\n\n  I ended up just compiling with my new op, and it works. However, if someone wants to help debug bazel-test, I'm happy to run more tests. \n\nBest,\nYacine\n", "I'm hitting this too now, though I'm not sure if it's the same issue.  If I run `python3` directly, or invoke it indirectly using `bazel run`, `import numpy` pulls in something from `/usr/local/lib/python3.4/dist-packages/numpy`.  This is also the version of numpy picked up if I compile.\n\nHowever, if I use `bazel test`, it pulls in a version of numpy from `/usr/lib/python3/dist-packages`, which in my case is incompatible.  Presumably bazel is messing with paths in an inconsistent manner, though I haven't investigated fully yet.\n", "Sounds like it's worth asking the bazel folks about. \n", "Bazel folks asked: https://github.com/bazelbuild/bazel/issues/753\n", "Closing due to inactivity / likely bazel / environment related\n", "@vrv I also get the same error message.  Should we reopen this issue?\n", "I don't think it's a TensorFlow issue -- your numpy version at runtime has to match the version that TensorFlow was compiled with.\n", "I met the same problem. But you should notice that when you install tensorflow  it use snumpy version A and when you run app of tensorflow you use numpy version 7. Just does not match. \r\nYou can check your running numpy version by: `pip freeze` or `import numpy` and `numpy.version.version` to verify this. The solution is to update your numpy to the tensorflow installing dependent numpy version."]}, {"number": 471, "title": "Build issue with bazel \"python_bin_path.sh: No such file or directory\"", "body": "I've updated my code, and following what is currently listed under https://www.tensorflow.org/versions/master/get_started/os_setup.html#source arrived at this output:\n\n``` bash\nalex@ml1:~/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nINFO: Found 1 target...\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\nINFO: Elapsed time: 0.793s, Critical Path: 0.00s\nalex@ml1:~/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\nThu Dec 10 12:13:53 PST 2015 : === Using tmpdir: /tmp/tmp.NqHERREKmr\n/tmp/tmp.NqHERREKmr ~/tensorflow\nThu Dec 10 12:13:53 PST 2015 : === Building wheel\nbazel-bin/tensorflow/tools/pip_package/build_pip_package: line 45: tensorflow/tools/python_bin_path.sh: No such file or directory\n```\n\nAny idea why the python_bin_path.sh file hasn't been generated?\n", "comments": ["Did you run ./configure  first?\n", "Sure did. Right now I'm trying updating to bazel 0.1.2 or rebooting.\n", "Hmm, @martinwicke, this might be related to your change earlier today?\n", "Probably. As a workaround while I look into this, you can edit tensorflow/tools/pip_package/build_pip_package to remove the offending line. Replace \n\n``` bash\n  source tensorflow/tools/python_bin_path.sh\n  ${PYTHON_BIN_PATH} setup.py bdist_wheel >/dev/null\n```\n\nwith\n\n``` bash\n  python setup.py bdist_wheel >/dev/null\n```\n\nThen rebuild.\n", "worst-case, we could also just do `${PYTHON_BIN_PATH:-python}`\n", "Thanks @martinwicke, that did the trick.\n", "It looks like the ./configure script puts python_bin_path.sh into the root tensorflow/tools directory, not tensorflow/tensorflow/tools.\n\nEither util/python/python_config.sh or tensorflow/tools/pip-package/build_pip_package.sh needs to change, and the script needs to be copied into the appropriate bazel-out directory.\n", "This problem still appears for me for master and bazel 0.1.2\n\n```\nFri Dec 11 09:27:04 PST 2015 : === Building wheel\nbazel-bin/tensorflow/tools/pip_package/build_pip_package: line 45: tools/python_bin_path.sh: No such file or directory\n```\n\nThe aforementioned commits are in.\n", "Confirmed. This issue still exists. \n", "Does the change in 7c5c20b2b73c40bb4fd113f26bcda73a5e5bec6d help?\n", "It does. The script was in the wrong directory when trying to source python_bin_path.sh\n", "Also fixed it for me @vrv \n", "Works indeed. Thanks.\n"]}, {"number": 470, "title": "Tensorboard does not support Python 3", "body": "[tensorboard.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/tensorboard.py) and [tensorboard_handler.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/tensorboard_handler.py) are still using `BaseHTTPServer` which was move to `http.server` in Python 3. There are some other changes with `StringIO` and `urlparse` that were quickly resolved with `2to3`.\n\nStrings also need to be encoded for the `wfile.write`s in [tensorboard_handler.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/tensorboard_handler.py).\n\nEverything seemed to work after I made these changes.\n", "comments": ["Thanks -- We'll try to update tensorboard to be python 3 compatible -- will probably be in the next binary release.\n", "nice\n", "To dankolbman,\n\nI ran into the same issue. Would you like to share your modified tensorboard.py and tensorboard_handler.py for working with python3? Thanks in advance!\n", "Yeah sure, here are the diffs. `2to3` will take care of everything except for the encoding issue on line 171 of `tensorboard_handler.py`. Just replace it with `self.wfile.write(output.encode('utf-8'))`\n\n`tensorboard.py`:\n\n``` diff\n-import http.server\n+import BaseHTTPServer\n import functools\n import os\n import socket\n-import socketserver\n+import SocketServer\n\n@@ -107,8 +107,8 @@ def ParseEventFilesFlag(flag_value):\n-class ThreadedHTTPServer(socketserver.ThreadingMixIn,\n-                         http.server.HTTPServer):\n+class ThreadedHTTPServer(SocketServer.ThreadingMixIn,\n+                         BaseHTTPServer.HTTPServer):\n```\n\n`tensorboard_handler.py`:\n\n``` diff\n-import http.server\n+import BaseHTTPServer\n-import io\n-import urllib.parse\n+import StringIO\n+import urlparse\n\n@@ -76,7 +76,7 @@ class _OutputFormat(object):\n-class TensorboardHandler(http.server.BaseHTTPRequestHandler):\n+class TensorboardHandler(BaseHTTPServer.BaseHTTPRequestHandler):\n   \"\"\"Handler class for use with BaseHTTPServer.HTTPServer.\n\n@@ -85,7 +85,7 @@ class TensorboardHandler(http.server.BaseHTTPRequestHandler):\n   def __init__(self, multiplexer, *args):\n     self._multiplexer = multiplexer\n-    http.server.BaseHTTPRequestHandler.__init__(self, *args)\n+    BaseHTTPServer.BaseHTTPRequestHandler.__init__(self, *args)\n\n@@ -140,7 +140,7 @@ class TensorboardHandler(http.server.BaseHTTPRequestHandler):\n-    out = io.StringIO()\n+    out = StringIO.StringIO()\n     f = gzip.GzipFile(fileobj=out, mode='w')\n\n@@ -168,7 +168,7 @@ class TensorboardHandler(http.server.BaseHTTPRequestHandler):\n     self.send_header('Content-Type', 'application/json')\n     self.send_header('Content-Length', len(output))\n     self.end_headers()\n-    self.wfile.write(output.encode('utf-8'))\n+    self.wfile.write(output)\n\n@@ -195,7 +195,7 @@ class TensorboardHandler(http.server.BaseHTTPRequestHandler):\n     values = self._multiplexer.Scalars(run, tag)\n\n     if query_params.get('format') == _OutputFormat.CSV:\n-      string_io = io.StringIO()\n+      string_io = StringIO.StringIO()\n       writer = csv.writer(string_io)\n\n@@ -234,7 +234,7 @@ class TensorboardHandler(http.server.BaseHTTPRequestHandler):\n     run = query_params.get('run')\n     compressed_histograms = self._multiplexer.CompressedHistograms(run, tag)\n     if query_params.get('format') == _OutputFormat.CSV:\n-      string_io = io.StringIO()\n+      string_io = StringIO.StringIO()\n       writer = csv.writer(string_io)\n\n@@ -242,7 +242,7 @@ class TensorboardHandler(http.server.BaseHTTPRequestHandler):\n       headers = ['Wall time', 'Step']\n       if compressed_histograms:\n         bucket_count = len(compressed_histograms[0].compressed_histogram_values)\n-        for i in range(bucket_count):\n+        for i in xrange(bucket_count):\n           headers += ['Edge %d basis points' % i, 'Edge %d value' % i]\n       writer.writerow(headers)\n\n@@ -366,7 +366,7 @@ class TensorboardHandler(http.server.BaseHTTPRequestHandler):\n   def do_GET(self):  # pylint: disable=invalid-name\n     \"\"\"Handler for all get requests.\"\"\"\n-    parsed_url = urllib.parse.urlparse(self.path)\n+    parsed_url = urlparse.urlparse(self.path)\n\n@@ -386,7 +386,7 @@ class TensorboardHandler(http.server.BaseHTTPRequestHandler):\n     if clean_path in handlers:\n-      query_params = urllib.parse.parse_qs(parsed_url.query)\n+      query_params = urlparse.parse_qs(parsed_url.query)\n       # parse_qs returns a list of values for each key; we're only interested in\n```\n", "Thanks! The updated script works. My 2to3 did not do a good job as this.\n", "Finally Tensorboard worked with python3 but now the issue is that the graph failed to be visual. I tried to visualize it on chrome.Can somebody please help?\nThe error is \nGraph Visualization failed:Error:Failure Parsing graph definition\n", "I run into the same error with python3 \n\nGraph Visualization failed:Error:Failure Parsing graph definition\n\nThe server side error log shows exceptions at line 92, 415, 232:\n\nException happened during processing of request from ('127.0.0.1', 53044)\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/socketserver.py\", line 617, in process_request_thread\n    self.finish_request(request, client_address)\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/socketserver.py\", line 344, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/tensorflow/tensorboard/tensorboard_handler.py\", line 95, in **init**\n    http.server.BaseHTTPRequestHandler.**init**(self, *args)\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/socketserver.py\", line 673, in **init**\n    self.handle()\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/server.py\", line 398, in handle\n    self.handle_one_request()\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/server.py\", line 386, in handle_one_request\n    method()\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/tensorflow/tensorboard/tensorboard_handler.py\", line 415, in do_GET\n    handlers[clean_path](query_params)\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/tensorflow/tensorboard/tensorboard_handler.py\", line 232, in _serve_graph\n    self._send_gzip_response(graph_pbtxt, 'text/plain')\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/tensorflow/tensorboard/tensorboard_handler.py\", line 152, in _send_gzip_response\n    f = gzip.GzipFile(fileobj=out, mode='w')\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/gzip.py\", line 220, in __init__\n    self._write_gzip_header()\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/gzip.py\", line 252, in _write_gzip_header\n    self.fileobj.write(b'\\037\\213')             # magic header\n\n## TypeError: string argument expected, got 'bytes'\n", "I ran into graph visualization errors as well. The fix is to change `f.write(content)` to `f.write(content.encode('utf-8'))` in `_send_gzip_response`\n", "@dankolbman , will you consider making a pull request with your diffs?\n", "Unfortunately, the changes here are just a hack and break compatability with python2. Some more work needs to be done to work with both python2 and 3.\n", "To nikitakit,\n\ni included your fix to the fix suggested by dankolbman but i still have the same error.\n\nType Error: string argument expected,got 'bytes'\nIf any other ideas please help!\n", "You probably have to replace StringIO with BytesIO somewhere. I've seen\nthis kind of error before here\nhttp://stackoverflow.com/questions/34416702/tensorboard-graph-visualiyation-error-using-python-3/34459035#34459035.\nIs that the same error you're getting?\n\nOn Mon, Jan 4, 2016 at 5:57 AM kibtes notifications@github.com wrote:\n\n> To nikitakit,\n> \n> i included your fix to the fix suggested by dankolbman but i still have\n> the same error.\n> \n> Type Error: string argument expected,got 'bytes'\n> If any other ideas please help!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/470#issuecomment-168683037\n> .\n", "That's right, `_send_gzip_response` should use BytesIO, not StringIO. Sorry, I didn't run a full diff and missed that.\n\n``` diff\n-    out = StringIO.StringIO()\n+    out = io.BytesIO()\n     f = gzip.GzipFile(fileobj=out, mode='w')\n-    f.write(content)\n+    f.write(content.encode('utf-8'))\n```\n", "I fix all the bugs mentioned above and there's no error on the server side now. However, besides top bar and side column, there's only \"Main Graph\" two words on the graph page. Any ideas please?\n![image](https://cloud.githubusercontent.com/assets/6175880/12097756/72574918-b358-11e5-9ee7-285a6d05deb5.png)\n\nps. what I'm doing is just import the image recognition model (google inception v3) and summary write the graph def to file trying to see how does the model look like. Thanks...\n", "Are you using chrome?\n\nI get that blank page in both firefox and safari, but chrome works. Not sure if this is because of something else missing regarding python3, or just a tensorboard bug.\n\nSee: https://github.com/tensorflow/tensorflow/issues/650\n", "@martinwicke \nyes it is the same error. I just tried to fix with the suggestion posted by @nikitakit and i have error. \nThe Error:\nException happened during processing of request from ('127.0.0.1', 62162)\nTraceback (most recent call last):\n  File \"/usr/lib/python3.4/socketserver.py\", line 617, in process_request_thread\n    self.finish_request(request, client_address)\n  File \"/usr/lib/python3.4/socketserver.py\", line 344, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"/usr/lib/python3.4/site-packages/tensorflow/tensorboard/tensorboard_handler.py\", line 86, in **init**\n    http.server.BaseHTTPRequestHandler.**init**(self,*args)#BaseHTTPServer.BaseHTTPRequestHandler.**init**(self, *args)\n  File \"/usr/lib/python3.4/socketserver.py\", line 673, in **init**\n    self.handle()\n  File \"/usr/lib/python3.4/http/server.py\", line 398, in handle\n    self.handle_one_request()\n  File \"/usr/lib/python3.4/http/server.py\", line 386, in handle_one_request\n    method()\n  File \"/usr/lib/python3.4/site-packages/tensorflow/tensorboard/tensorboard_handler.py\", line 382, in do_GET\n    handlers[clean_path](query_params)\n  File \"/usr/lib/python3.4/site-packages/tensorflow/tensorboard/tensorboard_handler.py\", line 210, in _serve_graph\n    self._send_gzip_response(graph_pbtxt, 'text/plain')\n  File \"/usr/lib/python3.4/site-packages/tensorflow/tensorboard/tensorboard_handler.py\", line 136, in _send_gzip_response\n    f = gzip.GzipFile(fileobj=out, mode='w')\n  File \"/usr/lib/python3.4/gzip.py\", line 220, in __init__\n    self._write_gzip_header()\n  File \"/usr/lib/python3.4/gzip.py\", line 267, in _write_gzip_header\n    self.fileobj.write(chr(flags.encode('latin-1')))\nAttributeError: 'int' object has no attribute 'encode'\n\nwhat do I do?\n", "@kibtes \nThe erroneous line in the traceback seems suspect. On my computer, it is `self.fileobj.write(chr(flags).encode('latin-1'))` (note the different placement of parens). And this is inside the Python installation! Did you manually modify that file, by any chance?\n", "@nikitakit \n\nThanks. The fix works. I guess i had manually modified it.\n", "@nikitakit, yes I'm using Chromium 47.0\n", "I have just submitted a PR fixing this for the master branch: #749. The same fixes can be applied to 0.6.0, and I could submit a similar PR to fix those if there's interest (I don't know if the 0.6.0 binaries receive updates or if we have to wait for the next release).\n", "Thanks for the PR! Since it's merged now, I'm going to close this issue.\n"]}, {"number": 469, "title": "bazel 0.1.2  undeclared inclusions fixes needed in BUILD files", "body": "Started out with this error\n\n```\n(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ bazel build --verbose_failures -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nINFO: Found 1 target...\nINFO: From Compiling external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.cc:\nsrc/main/tools/namespace-sandbox.c:633: execvp(argv[0], argv): No such file or directory\nERROR: /tmp/gbowyer/.cache/bazel/_bazel_gbowyer/d132132edbbae685571ab9488dabc906/external/gemmlowp/BUILD:77:1: C++ compilation of rule '@gemmlowp//:eight_bit_int_gemm' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command\n  (cd /tmp/gbowyer/.cache/bazel/_bazel_gbowyer/d132132edbbae685571ab9488dabc906/tensorflow && \\\n  exec env - \\\n    PATH=/opt/java/jdk1.8.0_31/bin:/tmp/gbowyer/dbpedia-thing/env/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda-7.0/bin:/usr/local/cuda-7.0/bin:/sbin:/usr/sbin:/tmp/gbowyer/.scripts:/sbin:/usr/sbin:/tmp/gbowyer/pkg/bin:/tmp/gbowyer/.scripts:/tmp/gbowyer/pkgs/bin \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote external/gemmlowp -iquote bazel-out/local_linux-opt/genfiles/external/gemmlowp -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.o' -MD -MF bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.d -c external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.cc -o bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.o): crosstool_wrapper_driver_is_not_gcc failed: error executing command\n  (cd /tmp/gbowyer/.cache/bazel/_bazel_gbowyer/d132132edbbae685571ab9488dabc906/tensorflow && \\\n  exec env - \\\n    PATH=/opt/java/jdk1.8.0_31/bin:/tmp/gbowyer/dbpedia-thing/env/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda-7.0/bin:/usr/local/cuda-7.0/bin:/sbin:/usr/sbin:/tmp/gbowyer/.scripts:/sbin:/usr/sbin:/tmp/gbowyer/pkg/bin:/tmp/gbowyer/.scripts:/tmp/gbowyer/pkgs/bin \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote external/gemmlowp -iquote bazel-out/local_linux-opt/genfiles/external/gemmlowp -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.o' -MD -MF bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.d -c external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.cc -o bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.o).\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 1.907s, Critical Path: 1.27s\n(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\ngcc: fatal error: no input files\ncompilation terminated.\n(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Mon_Feb_16_22:59:02_CST_2015\nCuda compilation tools, release 7.0, V7.0.27\n(env)gbowyer@compute-10-3-61-179 ~/tensorflow $\n```\n\n---\n\nSuggestion from Vijay Vasudevan is to use different bazel build params\n\n> Can you try building with --spawn_strategy=standalone ?  After talking with the bazel folks it looks like the sandbox for some reason doesn't have the crosstool wrapper in the right location, so running in standalone mode might work better.\n\n---\n\nThat yields this error\n\n```\n(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ bazel build --verbose_failures -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --spawn_strategy=standalone\nINFO: Found 1 target...\nERROR: /tmp/gbowyer/tensorflow/tensorflow/stream_executor/BUILD:5:1: undeclared inclusion(s) in rule '//tensorflow/stream_executor:stream_executor':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/stream_executor/cuda/cuda_platform_id.cc':\n  '/tmp/gbowyer/tensorflow/tensorflow/stream_executor/cuda/cuda_platform_id.h'.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 2.893s, Critical Path: 2.64s\n(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ bazel build --verbose_failures -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --spawn_strategy=standalone\nINFO: Found 1 target...\nERROR: /tmp/gbowyer/tensorflow/tensorflow/core/BUILD:272:1: undeclared inclusion(s) in rule '//tensorflow/core:gpu_kernels':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/aggregate_ops_gpu.cu.cc':\n  '/tmp/gbowyer/tensorflow/tensorflow/core/framework/tensor_types.h'.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 38.609s, Critical Path: 37.67s\n(env)gbowyer@compute-10-3-61-179 ~/tensorflow $\n```\n\n---\n\nEvgeny Shaliov notes that\n\n> nvcc --version should return 6.5.\\* but CUDA toolkit should be installed 7.0. \n> \n> I posted https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/jRkkvsB1iWA.\n> Also pay attention what gcc version you use.\n> \n> Best regards,\n> Evgeny.\n\n---\n\nVijay Vasudevan also remarks\n\n> Yikes! :(\n> \n> What version of bazel do you happen to be using? \n\nAnd asks to move this to github issues, so here we are :P\n", "comments": ["``` sh\n(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Mon_Feb_16_22:59:02_CST_2015\nCuda compilation tools, release 7.0, V7.0.27\n(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ gcc --version\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04) 4.8.4\nCopyright (C) 2013 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ bazel --version\nUnknown Bazel startup option: '--version'.\n  For more info, run 'blaze help startup_options'.\n(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ bazel version\n...............\nBuild label: head (@53f4076)\nBuild target: bazel-out/local_linux-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Wed Dec 9 16:34:56 2015 (1449678896)\nBuild timestamp: 1449678896\nBuild timestamp as int: 1449678896\n(env)gbowyer@compute-10-3-61-179 ~/tensorflow $\n```\n\nI am going to see if changing bazel and nvcc helps, I will start with bazel first as I dont relish the idea of deploying older nvcc compilers.\n", "Yeah, I think this is a bazel or tensorflow BUILD issue, not an nvcc one.\n\nCan you try using bazel 0.1.1 or bazel 0.1.2?\n\n@damienmg in case he has some ideas, since I can't reproduce this at HEAD with bazel 0.1.1\n", "I'm not sure how related this is, but I had one of the abovementioned issues, \"undeclared inclusions in rule ..stream_executor\", with another GPU (K80), and downgrading from Bazel 0.1.2 to Bazel 0.1.1 helped.\n", "The include error should have been fixed with my changes but I may have missed some targets. We don't test all tensorflow targets on ci.bazel.io (we don't have GPU). The corresponding headers should be added in the TensorFlow's build files.\n", "Thanks @damienmg, I'll try to get the fix in today.\n", "can confirm this bug, just git cloned fresh from the tree and using bazel 0.1.2, downgrading to bazel 0.1.1 fixes the problem.\n", "I can't seem to reproduce this, either on my mac or on my linux machine, both pulling tensorflow from HEAD and installing bazel fresh from bazel.io for 0.1.2.\n\nWilliam: by any chance would you have time to figure out what the necessary changes are?  I'd be happy to integrate them.\n", "Nevermind, it's the GPU only targets that are messed up.  no wonder.\n", "okay, sort of fixed in https://github.com/tensorflow/tensorflow/commit/a4cefca9f40ae6cfe366b6187d07e5199aa74895\n\n--standalone argument is still required.  Seems like it needs to be addressed by bazel team at https://github.com/bazelbuild/bazel/issues/698\n", "That fix seems to have cured it thanks\n", "There may be a similar bug which still exists. Platform arch-linux\n\n```\n./configure\nbazel build --jobs 2 --config=cuda --verbose_failures --spawn_strategy=standalone -c opt //tensorflow/tools/pip_package:build_pip_package\n```\n\nwhich leads to\n\n```\nERROR: $PARENT_DIR/src/tensorflow/tensorflow/stream_executor/BUILD:5:1: undeclared inclusion(s) in rule '//tensorflow/stream_executor:stream_executor':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/stream_executor/cuda/cuda_dnn.cc':\n  '/opt/cuda/include/cuda_runtime.h'\n  '/opt/cuda/include/host_config.h'\n  '/opt/cuda/include/builtin_types.h'\n  '/opt/cuda/include/device_types.h'\n  '/opt/cuda/include/host_defines.h'\n  '/opt/cuda/include/driver_types.h'\n  '/opt/cuda/include/surface_types.h'\n  '/opt/cuda/include/texture_types.h'\n  '/opt/cuda/include/vector_types.h'\n  '/opt/cuda/include/channel_descriptor.h'\n  '/opt/cuda/include/cuda_runtime_api.h'\n  '/opt/cuda/include/cuda_device_runtime_api.h'\n  '/opt/cuda/include/driver_functions.h'\n  '/opt/cuda/include/vector_functions.h'\n  '/opt/cuda/include/vector_functions.hpp'.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n```\n", "@lberki has a fix for this.\n", "@damienmg: is this a fix to our BUILD rules?  We might have to update our jenkins gpu pip builder to 0.1.2 I guess.\n\n@jendap: do you know which bazel we're using there?\n", "@vrv it it not a problem in tensorflow BUILD files. It is in bazel (android rules). I have added you to cc on the fix @lberki has created.\n\nIt is happening with bazel 0.1.2 release which you can download from github (and bazel.io). IT was also happening on release 0.1.3 branch a couple of days ago. But we have a fix for it now. Just release it and we are good.\n", "Actually I'm not sure about this GPU issue. We were talking with bazel team about something with android and 0.1.2 but it may be the same thing. I have tried myself 0.1.2 with android only. Not gpu yet. I will try it next week.\n", "I have a similar problem on Arch Linux, too.\nRunning\n\n```\nbazel build -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package\n```\n\nThis error occurs\n\n```\nERROR: /home/test/tensorflow/tensorflow/core/BUILD:213:1: undeclared inclusion(s) in rule '//tensorflow/core:gpu_runtime':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/common_runtime/gpu/gpu_device_factory.cc':\n  '/opt/cuda/include/cuda_runtime.h'\n  '/opt/cuda/include/host_config.h'\n  '/opt/cuda/include/builtin_types.h'\n  '/opt/cuda/include/device_types.h'\n  '/opt/cuda/include/host_defines.h'\n  '/opt/cuda/include/driver_types.h'\n  '/opt/cuda/include/surface_types.h'\n  '/opt/cuda/include/texture_types.h'\n  '/opt/cuda/include/vector_types.h'\n  '/opt/cuda/include/channel_descriptor.h'\n  '/opt/cuda/include/cuda_runtime_api.h'\n  '/opt/cuda/include/cuda_device_runtime_api.h'\n  '/opt/cuda/include/driver_functions.h'\n  '/opt/cuda/include/vector_functions.h'\n  '/opt/cuda/include/vector_functions.hpp'.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n```\n\nIs there yet any solutions?\n", "@frankyjuang, did u try downgrading bazel to 0.1.1?\n", "@wchan, downgrading to 0.1.1 works!!! Thanks a lot.\n\n@gillhofer, Arch linux 4.3.3 with CUDA 7.0, gcc/g++ 4.9.3 and bazel 0.1.1 works. You can give it a try. Btw, I have nvidia gtx 660, so setting compute capability to 3.0 with\n`$ TF_UNOFFICIAL_SETTING=1 ./configure` is needed.\n", "The include issue is due to https://github.com/bazelbuild/bazel/issues/714 (tl;dr: current workaround is to have a longer path for /opt/cuda).\n", "Note that today Bazel's release 0.1.3 should fix that error.\n", "@damienmg I tried the 0.1.3. However, this bug still exists...Any ideas?\nArch Linux with gcc 4.9.3\n\n```\n$ bazel build -c opt --config=cuda --verbose_failures --spawn_strategy=standalone //tensorflow/cc:tutorials_example_trainer\nERROR: /home/test/tensorflow/tensorflow/stream_executor/BUILD:5:1: undeclared inclusion(s) in rule '//tensorflow/stream_executor:stream_executor':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/stream_executor/cuda/cuda_rng.cc':\n  '/opt/cuda/include/cuda_runtime.h'\n  '/opt/cuda/include/host_config.h'\n  '/opt/cuda/include/builtin_types.h'\n  '/opt/cuda/include/device_types.h'\n  '/opt/cuda/include/driver_types.h'\n  '/opt/cuda/include/surface_types.h'\n  '/opt/cuda/include/texture_types.h'\n  '/opt/cuda/include/vector_types.h'\n  '/opt/cuda/include/channel_descriptor.h'\n  '/opt/cuda/include/cuda_runtime_api.h'\n  '/opt/cuda/include/host_defines.h'\n  '/opt/cuda/include/cuda_device_runtime_api.h'\n  '/opt/cuda/include/driver_functions.h'\n  '/opt/cuda/include/vector_functions.h'\n  '/opt/cuda/include/vector_functions.hpp'.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 84.234s, Critical Path: 80.07s\n```\n", "So the problem is that 0.1.3 fix the problem in our CROSSTOOL file. here the problems are in TensorFlow's cuda support.\n\nThe fix in Bazel was https://github.com/bazelbuild/bazel/commit/763f1397155fc7c12e1f1071a1bc942f91b867c4\nSo applying the same change to third_party/gpus/crosstool/CROSSTOOL should work, that is \nadding `unfiltered_cxx_flag: \"-fno-canonical-system-headers` at https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/CROSSTOOL#L110\n\nHope that helps!\n", "@damienmg ,yes it works! thanks. But I encounter another problem with `crosstool_wrapper_driver_is_not_gcc`. I had tried bazel 0.1.0, 0.1.1, 0.1.2, 0.1.3. It just can't work...\n\n```\nERROR: /home/test/tensorflow/tensorflow/python/BUILD:71:1: C++ compilation of rule '//tensorflow/python:py_func_lib' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command\n  (cd /home/test/.cache/bazel/_bazel_test/3a51eda24c560ebddf29b66b8fa0460e/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/bin:/opt/android-ndk:/opt/android-sdk/platform-tools:/opt/android-sdk/tools:/opt/cuda/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/home/test/bazel/output \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-py3-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-py3-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-5651786d5e59 -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive/eigen-eigen-5651786d5e59 -isystem third_party/py/numpy/numpy_include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/py/numpy/numpy_include -isystem util/python/python_include -isystem bazel-out/local_linux-py3-opt/genfiles/util/python/python_include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/py_func_lib/tensorflow/python/lib/core/py_func.pic.o' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/py_func_lib/tensorflow/python/lib/core/py_func.pic.d -fPIC -c tensorflow/python/lib/core/py_func.cc -o bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/py_func_lib/tensorflow/python/lib/core/py_func.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: crosstool_wrapper_driver_is_not_gcc failed: error executing command                                                                                       (cd /home/test/.cache/bazel/_bazel_test/3a51eda24c560ebddf29b66b8fa0460e/tensorflow && \\                                                                                      exec env - \\                                                                                                                                                                    INTERCEPT_LOCALLY_EXECUTABLE=1 \\                                                                                                                                              PATH=/usr/local/sbin:/usr/local/bin:/usr/bin:/opt/android-ndk:/opt/android-sdk/platform-tools:/opt/android-sdk/tools:/opt/cuda/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/home/test/bazel/output \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-py3-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-py3-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-5651786d5e59 -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive/eigen-eigen-5651786d5e59 -isystem third_party/py/numpy/numpy_include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/py/numpy/numpy_include -isystem util/python/python_include -isystem bazel-out/local_linux-py3-opt/genfiles/util/python/python_include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/py_func_lib/tensorflow/python/lib/core/py_func.pic.o' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/py_func_lib/tensorflow/python/lib/core/py_func.pic.d -fPIC -c tensorflow/python/lib/core/py_func.cc -o bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/py_func_lib/tensorflow/python/lib/core/py_func.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 170.105s, Critical Path: 121.60s\n```\n", "You should see the error from the script itself a few lines before the ERROR: line. Can you copy a bit what was before?\n", "Fixed in the above commit, we believe.\n", "i have a similar issue compiling current master with bazel 2 compute capability 3.0 and cuda 7.5 :\n\n```\nERROR: /home/pragma/tensorflow/tensorflow/core/kernels/BUILD:685:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:conv_ops':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/conv_ops.cc':\n  '/home/pragma/tensorflow/tensorflow/core/common_runtime/gpu_device_context.h'\n  '/home/pragma/tensorflow/tensorflow/core/common_runtime/device.h'\n  '/home/pragma/tensorflow/tensorflow/core/graph/graph.h'\n  '/home/pragma/tensorflow/tensorflow/core/graph/edgeset.h'\n  '/home/pragma/tensorflow/tensorflow/core/graph/types.h'.\nIn file included from external/eigen_archive/eigen-eigen-ed4c9730b545/unsupported/Eigen/CXX11/Core:35:0,\n                 from external/eigen_archive/eigen-eigen-ed4c9730b545/unsupported/Eigen/CXX11/Tensor:14,\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\n                 from ./tensorflow/core/framework/types.h:23,\n                 from ./tensorflow/core/framework/type_traits.h:22,\n                 from ./tensorflow/core/framework/allocator.h:25,\n                 from ./tensorflow/core/framework/op_kernel.h:22,\n                 from ./tensorflow/core/framework/numeric_op.h:19,\n                 from tensorflow/core/kernels/conv_ops.cc:22:\nexternal/eigen_archive/eigen-eigen-ed4c9730b545/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, long int>, 16>, const Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, long int>, 16> > >; bool Vectorizable = true]':\nexternal/eigen_archive/eigen-eigen-ed4c9730b545/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[] (size_t index) { return values[index]; }\n                                                                   ^\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 138.998s, Critical Path: 137.36s\n\n```\n", "Yep, the build is broken at the moment -- we have a fix we're trying to push through.\n", "Similar problem ... C++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed: gcc failed: error executing command \n", "FYI : I was able to build the GPU pip package using the changes listed in this install script : https://github.com/noisychannel/tensorflow_install/blob/master/tf_install.sh\n\nSpecifically, check the function fix_tf which makes changes in third_party/gpus/crosstool/CROSSTOOL, tensorflow/tensorflow.bzl and tensorflow/stream_executor/BUILD.\n"]}, {"number": 468, "title": "Generated wheel files are named incorrectly (python wheel experts, please help!)", "body": "**Note:** If you know lots about python wheels, and can answer some of the questions here, please jump in!\n\nPython wheel files encode metadata about supported architectures into their names, cf [the wheel PEP](https://www.python.org/dev/peps/pep-0427/) and [the suffix PEP](https://www.python.org/dev/peps/pep-0425/).\n\nCurrently, when we generate wheel files, we end up with either too-broad names (eg all Mac wheels are `py2-none-any`, which is wrong) or too-specific (some linux wheels are tagged `cp34-cp34m-linux-x86_64`, but we think the abi component should be `none`). This leads to issues like #467.\n\nA part of the issue here is that we're not sure what some of these tags mean -- for instance, the only explanation I've found of the `m` suffix is in [an issue on the wheel bug tracker](https://bitbucket.org/pypa/wheel/issues/61/abi-version-is-not-found) and even that doesn't clear things up.\n\nThere are really three parts to fixing this:\n- we should know what the \"right\" filename is for these wheel files.\n- we should fix our build process to generate the right output on all architectures.\n- we should consider serving our wheel files via a custom PyPA.\n\nMore details on the last one: we're making our lives harder by asking people to point to the \"right\" wheel file. `pip` knows how to take a package name (eg `tensorflow`) and a list of wheel files, and then install the right one. We can and should serve our own PyPA, so that installation instructions on **all** platforms we support would be something like\n\n```\npip install --extra_index_url=https://tensorflow.org/pypa tensorflow\n```\n\n/cc @vrv @martinwicke @keveman on the TF side.\n", "comments": ["Solution for all who wants to install from wheels on python3.5, rename \"tensorflow-0.6.0-cp34-none-linux_x86_64.whl\" to \"tensorflow-0.6.0-cp35-none-linux_x86_64.whl\".\n", "Closing since we're not getting any traction from the community on this.  Sigh.\n"]}, {"number": 467, "title": "0.6.0 whl package named incorrectly as of 12/10 9am PST", "body": "From #1:\n\n> Closing since it seems to work. Python 3 support will ship with version 0.6.0, which is coming very soon. Future Python 3 bugs should be filed as separate issues!\n\nIn a python 2 virtualenv:\n\n``` sh\n$ python --version\nPython 2.7.10\n$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl\n...\nSuccessfully installed protobuf-3.0.0a3 setuptools-18.7.1 tensorflow-0.6.0 wheel-0.26.0\n```\n\nIn a python 3 virtualenv:\n\n``` sh\n$ python --version\nPython 3.4.3\n$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp34-cp34m-linux_x86_64.whl\ntensorflow-0.6.0-cp34-cp34m-linux_x86_64.whl is not a supported wheel on this platform.\n```\n", "comments": ["Yeah, we're still testing everything out today before announcing that it's ready.\n\nJust curious, what happens if you download the wheel package and rename it tensorflow-0.6.0-cp34-none-linux_x86_64.whl ?\n", "@vrv Thanks, that worked! The python 3 installation instructions are already published in the docs so I assumed it was official.\n\n``` sh\n$ python --version\nPython 3.4.3\n$ wget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp34-cp34m-linux_x86_64.whl\n...\n$ mv tensorflow-0.6.0-cp34-cp34m-linux_x86_64.whl tensorflow-0.6.0-cp34-none-linux_x86_64.whl\n$ pip install -U tensorflow-0.6.0-cp34-none-linux_x86_64.whl\n...\nSuccessfully installed numpy-1.10.1 protobuf-3.0.0a3 setuptools-18.7.1 six-1.10.0 tensorflow-0.6.0 wheel-0.26.0\n```\n", "Yeah, they shouldn't have been pushed to the docs until they were tested fully.  We'll probably just rename the wheel files.\n", "new wheels with the updated filename are up.\n\ni'll let @martinwicke close this once the website is updated.\n", "Fixed!\n"]}, {"number": 466, "title": "tf.unsorted_segment_sum exits without error message if segment_ids contains negative number.", "body": "When running\n\n   tf.unsorted_segment_sum(data, segment_ids, num_segments, name=None)\n\nand passing a tensor containing -1 as `segment_ids`, tensorflow just exits without error message. I'm on the current master. \n\nDesired behavior in my case would be to these values are 'rejected', i. e. are not summed anywhere at all. Would it make sense to extend this method this way? And if not, can you give me an idea how to implement this behavior?\n", "comments": ["Using -1 to reject is interesting, but it turns coding errors into silent incorrect behavior so I'm leery of doing it.  For now, you could replace -1s with a positive value represent an \"ignored\" class and then drop it.\n\nIt would be reasonable to add a boolean `drop_negatives` attr to ignore negative values, defaulted to false to preserve the existing error detection behavior.\n", "Ah, but the fact that it exits without the error message is a bad bug.\n", "I'll fix the bug part now.\n", "Thanks for the fix, I'll open a new issue with my feature request.\n"]}, {"number": 465, "title": "Allow mixing values with tensors in lists that specify the shape of a Tensor.", "body": "There are many methods that expect a list or a tensor specifying a shape as argument. It is often the case that I want to pass an argument such as shape=[t1, v2] where t1 is a tensor with shape = TensorShape([]) and v2 is a value. This is currently not possible in many (maybe all) methods.\n\nMy current work around is the following.\n1- convert v2 to a tensor t2 with tf.convert_to_tensor\n2- use t1 = tf.expand_dims(0, t1) and t2 = tf.expand_dims(0, t2) to add a dimension of size 1 to the tensors\n3- pass the argument as shape=tf.concat(0, [t1, t2])\n\nAm I missing a simpler way to accomplish that? Is it a good idea to add a feature that allows passing mixture of tensors and values inside a list as argument?\n\nEdit: I just found out that the method tf.pack allows lists with tensors and values as argument. But the method tf.truncated_normal for instance, doesn't allow.\nI also realized that I can use tf.pack to replace the steps (1,2,3) above. But it still would be nice if all methods had the same behaviour.\n", "comments": ["Agreed, I've wanted this too.  There are some annoying obstacles, since in some places we automatically convert singleton tensors to lists and this feature would be backwards incompatible, but we've had some internal discussion about removing that magical (and fairly unidiomatic) list conversion.  If we did that, we'd be able to add _your_ automatic conversion, which would be great.\n", "Closing this as a duplicate of #2328.\n"]}, {"number": 464, "title": "RMSProp optimization support for sparse tensors", "body": "It seems that tf.nce_loss is not compatible with the optimizers RMSProp, ADAGRAD and Momentum. (while SGD, ADAM and FTRL works fine).\n\nWhen using rmsprop, I get this error:\n\n```\n    optimizer = tf.train.RMSPropOptimizer(learning_rate = learning_rate, decay = rms_prop_decay).minimize(nce_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 167, in minimize\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 256, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/rmsprop.py\", line 81, in _apply_sparse\n    raise NotImplementedError()\nNotImplementedError\n```\n\nWhen using adagrad or momentum, I get this error:\n\n```\n    optimizer = tf.train.MomentumOptimizer(learning_rate, learning_momentum).minimize(nce_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 167, in minimize\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 256, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/momentum.py\", line 51, in _apply_sparse\n    self._momentum_tensor, use_locking=self._use_locking).op\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/gen_training_ops.py\", line 237, in sparse_apply_momentum\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 633, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1712, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1417, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_ops.py\", line 111, in _SparseApplyMomentumShape\n    tensor_shape.TensorShape([None]).concatenate(accum_shape[1:]))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 481, in merge_with\n    self.assert_same_rank(other)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 524, in assert_same_rank\n    \"Shapes %s and %s must have the same rank\" % (self, other))\nValueError: Shapes TensorShape([Dimension(128), Dimension(11), Dimension(192)]) and TensorShape([Dimension(None), Dimension(192)]) must have the same rank\n```\n\nIs that expected?\nThe exact same code works perfectly fine with adam or sgd optimizers, so I do not think I made a mistake when constructing the graph.\n", "comments": ["Based on reading the code, I think it is expected: there is not yet support for SparseTensors with those three optimizers, since _apply_sparse() function isn't implemented.  Turning this into a feature request.\n", "OK, thank you. But since ADAM work for this, there should not be any major issue preventing to use RMSProp as well, right? (I would be quite interested in using RMSProp with nce_loss).\n\nAlso, if this is expected, I think you should try to document this. I do not think it is mentioned in the doc that only 3 out of the 6 default optimizers support sparse update. The doc of nce_loss (and of sampled softmax as well, I suppose), could also mention it.\n", "Assigning to someone who knows more about this part of the codebase\n", "After checking bug #505, it seems that there are actually two bugs going on here. One has to do with tf.reshape + Adagrad/Momentum , and the other has to do with RMSPropOptimizer not handling sparse gradient updates (which happens in cases of embeddings or sampled loss, I guess). Here is a self-contained example demonstrating the RMSProp error (just a slight modification of the one I posted for bug #505):\n\n```\nimport numpy as np\nimport tensorflow as tf\n\ndef device_for_node(n):\n    if n.type == \"MatMul\":\n        return \"/gpu:1\"\n    else:\n        return \"/cpu:0\"\n\nminibatch_size = 128\nhidden_size = 64\nembedding_size = 256\ninput_layer_size = 3\nvocab_size_input = 32\nvocab_size_output = 64\nnce_num_sampled = 16\nlearning_rate = 0.1\n\ndummy_input = np.zeros((minibatch_size, input_layer_size), dtype = np.int32)\ndummy_target = np.zeros((minibatch_size, 1), dtype = np.int32)\n\ninput_layer_flattened_size = input_layer_size * embedding_size\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n    with graph.device(device_for_node):\n        input_layer = tf.placeholder(tf.int32, shape = (minibatch_size, input_layer_size), name = \"input_layer\")       \n        ref_input = tf.placeholder(tf.int32, shape = (minibatch_size, 1), name = \"ref_input\")\n\n        # Parameters\n\n        input_embeddings = tf.Variable(tf.random_normal([vocab_size_input, embedding_size]), name = \"i_embeddings\")\n\n        Wh_i = tf.Variable(tf.random_normal((input_layer_flattened_size, hidden_size), stddev = 0.2), name = \"Wh_i\")\n        bh_i = tf.Variable(tf.random_normal((hidden_size,), stddev = 0.2), name = \"bh_i\")\n\n        Wh_o = tf.Variable(tf.random_normal((vocab_size_output, hidden_size), stddev = 0.2), name = \"Wh_o\")\n        bh_o = tf.Variable(tf.random_normal((vocab_size_output,), stddev = 0.2), name = \"bh_o\")\n\n        # Layers\n\n        i_embedded = tf.nn.embedding_lookup(input_embeddings, input_layer)\n        i_embedded_flattened = tf.reshape(i_embedded, \n                        (\n                         (minibatch_size if minibatch_size is not None else -1), \n                         input_layer_flattened_size ) \n                        )\n\n        h = tf.tanh(tf.matmul(i_embedded_flattened, Wh_i) + bh_i)\n        nce_loss = tf.reduce_mean(tf.nn.nce_loss(Wh_o, bh_o, h, ref_input, nce_num_sampled, \n                                                     num_classes = vocab_size_output, name = \"nce\"))\n        optimizer = tf.train.RMSPropOptimizer(learning_rate, decay = 0.9).minimize(nce_loss)\n\n        init_op = tf.initialize_all_variables()\n\n\nwith tf.Session(graph=graph) as session:\n\n    feed_dict = {input_layer : dummy_input, ref_input : dummy_target}\n    _, loss_val = session.run([optimizer, nce_loss], feed_dict=feed_dict)\n\n\n```\n", "I think the issue here is related to a bug in `array_grad._GatherGrad`, and I have a fix in the works. (It seems to be broken when computing gradients for an embedding lookup/gather with a >1-D indices input.)\n", "Thank you for fixing this :-) However will this also fix the RMSProp error? It seems to be of a slightly different nature...\n", "Indeed, it will only fix the issues with adagrad and momentum (and other optimizers that support sparse data using embeddings).\n", "What kind of behavior would you expect from sparse tensors in RMSProp?\nThere are two options I think:\n- Ignore the momentum terms for the embedding rows that are not present in the current batch\n- Apply momentum terms to the whole embedding (AFAIR, that's what we do for Adam)\n\nThe first might not be correct (though, not sure), the second is very slow. What I typically do now is to split my variables into two groups, and train the dense part with any optimizer I want and the rest with GradientDescent or AdaGrad.\n", "It is quite possible that I do not fully understand the issue, but I was not expecting RMSProp to be much more difficult to use than Adagrad. I will try to briefly state how I see things (and please excuse me if I write something obviously stupid).\n\nAs for the two options you gave, I take it that you are discussing the update of the running average of the squared gradient, right? \"ignore the momentum\" would consist in not updating the running average for dimensions for which the gradient is zero due to sparsity. And \"apply momentum for the whole embedding\" would be to actually update the running average of all dimensions at each iteration. \n\nThen I think there is a third option. You could keep track of the last iteration in which the running average was updated for each dimension.\n\nLet us call last(d) the iteration step at which the running average of dimension d was last updated. Then, when you want to update dimension d at iteration step i, you can update the running average of the square gradient of d by rms(d) = rms(d) \\* 0.9 ^ ( i - last(d)) \\* 0.9 + 0.1 \\* grad_i(d)^2. Then set last(d) = i.\n\nThe 0.9 ^ ( i - last(d)) part account for the modification in the running gradient since the last time we saw a non-zero gradient for dimension d (null gradient (i - last(d) times). The values of last(d) and rms(d) only need to be updated when there is a non-null gradient for dimension d. Therefore the updates should be efficient in a sparse context.\n", "You're right, the third option you suggested is probably strictly better than the second one. That's not how it's implemented in Adam, though (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/adam.py#L138)\n\nFor Adagrad, we're only updating non-zero elements and I think this is a common way to do it (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc#L381)\n", "Yes, for Adagrad it is enough to not upgrade the squared gradient sum (and keep track of the global total number of updates).\n\nWould you consider implementing this? I could probably do it myself at some point when I have time by looking at the existing code for RMSProp and Adam. But with lack of time and the annoyance of getting a Corporate Contributor Agreement, I would probably not contribute that anytime soon...\n\nIn any case, if nothing is changed, I think the docs should mention that ADAM is slower than Adagrad on sparse updates; and that RMSProp is not compatible with those.\n", "Renaming this bug since the AdaGrad and Momentum optimizers should now work.\n", "Hey what is the status of this?\n", "I'm going to mark this contributions welcome, since I don't know of anyone working on it.\n", "@fabiencro, is this still an issue? If not, or if it is not important, I will close it in a few days.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing as per previous comment: https://github.com/tensorflow/tensorflow/issues/464#issuecomment-363949559"]}, {"number": 463, "title": "No imagenet folder in the 0.6.0 wheel linked on tensorflow.org", "body": "Trying to follow the Inception-v3 tutorial, but there is  no imagenet folder in my installation.\n\n```\n(d)[nani@nande cat]$ pip show tensorflow\n\n---\nMetadata-Version: 2.0\nName: tensorflow\nVersion: 0.6.0\nSummary: TensorFlow helps the tensors flow\nHome-page: http://tensorflow.com/\nAuthor: Google Inc.\nAuthor-email: opensource@google.com\nLicense: Apache 2.0\nLocation: /home/nani/Desktop/cat/d/lib/python2.7/site-packages\nRequires: six, protobuf, wheel, numpy\n\n\n(d)[nani@nande cat]$ ls d/lib/python2.7/site-packages/tensorflow/models/image/\ncifar10  __init__.py  __init__.pyc  mnist\n```\n", "comments": ["Try to run using bazel\n", "you mean building from source? I am still building bazel.\n", "yes. try to build it from source\n", "Most of the example sources are no longer in the pip package install directory, instead, after installing the pip package, you can clone the git repository to get access to the source (at tag 0.6.0 to be safe), and then you can just do\n\ncd tensorflow/models/image/imagenet\npython classify_image.py\n", "Oh, I see. Thanks.\n"]}, {"number": 462, "title": "Building static-linked tensorflow", "body": "Have you considered distributing a static-built version of tensorflow so poor uses of old CentOS clusters  with an old glibc could use it? I understand static builds are dirty but it seems that there are many users that would benefit from this solution...\nI tried to build tensorflow using an old glibc myself but did not succeed. I guess it could be done but some knowledge of bazel and the internal structure of tensorflow required (and a lot of time!)...\n", "comments": ["Closing since I think this is covered by #527.  It would be better to fix compilation on Centos rather than ship static versions.\n"]}, {"number": 461, "title": "broken links in word2vec tutorial docs", "body": "Some of the links to word2vec_basic.py currently give a 404 -- they point to https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py . They should probably point to the version at  https://www.tensorflow.org/versions/master/tutorials/word2vec/word2vec_basic.py , right?\n", "comments": ["Yeah, we haven't pushed the new website that has these link fixes yet.  We'll probably do so soon.\n", "Pushed website, fixed!\n"]}, {"number": 460, "title": "Make a helpful python Tensor __repr__ that describes type, shape, etc.", "body": "Currently `repr(tf.placeholder(tf.float32, shape=(3,1)))` gives:\n\n```\n'<tensorflow.python.framework.ops.Tensor object at 0x7fa0e9f99b10>'\n```\n\nThis is not very informative and it's what you see as the output for interactive shell (IPython). In contrast to that, `repr(np.array([[1,2]]))` will give us nice:\n\n```\n'array([[1, 2]])'\n```\n", "comments": []}, {"number": 459, "title": "Single scalar summary point not visible in the plot", "body": "When only one event is available for a scalar summary, the plot remains empty, as shown below (here the value is 2.0):\n![snapshot2](https://cloud.githubusercontent.com/assets/331795/11701319/b1919762-9e83-11e5-8f85-fa1740c52c57.png)\nIt would be great to see one point corresponding to the value instead. The value does appear in the JSON/CSV file.\n", "comments": ["Agreed, this isn't great behavior! It's not that hard to fix so I'll try to get it for you soon :)\n", "Great, thanks!\n", "Dan fixed this a while ago.  Thanks Dan!\n"]}, {"number": 458, "title": "Visualizing scalar summaries on the graph", "body": "Would it be possible to visualize the values of scalar summaries (e.g. from a chosen or simply last event) directly on the graph? If not, what is the best way to easily inspect the values of the tensors after the most recent run?\n", "comments": ["We're planning to build closer integration between the graph visualizer and the other event types so that it would be possible to visualize data charts directly on the graph; we haven't started work on it yet, though. \nFor now, if you want to inspect values of the tensors you probably want to add a histogram summary, and you can then view the distribution of values from the tensor on the histogram page. (Ideally you would put each run in a separate directory so it's easy to filter between them using the run selector.)\nI'll post back with more info when we start building tighter graph vis/event integration. \nDoes this answer your question for now?\n", "It does, thanks. Is there an automatic way to generate the runs in separate folders?\n", "Maybe [TDB](https://github.com/ericjang/tdb) might help.\n", "That looks promising. Will have to check it out.\n", "Closing this since we aren't likely to work on it soon, and it's probably too large and involved a change for external contributors.\n"]}, {"number": 457, "title": "Wheel out of date", "body": "The wheel used for installing via pip for Tensorflow is out of date and is missing parts needed for the tutorials.\n\nI've built from source successfully, but is very difficult to do on a machine that I don't have root access to. An updated wheel would make it much easier to do a user install of Tensorflow.\n", "comments": ["Yes, we are about to build the 0.6.0 packages that should work with all the tutorials at HEAD.  Closing because we're building them now...\n", "I'm not sure if I understand. On the website there is a 0.6.0 wheel in the instructions,  but when I go to run the pretrained inception-v3 model, I don't find the imagenet directory:\n\n```\n(d)[nani@nande cat]$ pip show tensorflow\n---\nMetadata-Version: 2.0\nName: tensorflow\nVersion: 0.6.0\nSummary: TensorFlow helps the tensors flow\nHome-page: http://tensorflow.com/\nAuthor: Google Inc.\nAuthor-email: opensource@google.com\nLicense: Apache 2.0\nLocation: /home/nani/Desktop/cat/d/lib/python2.7/site-packages\nRequires: six, protobuf, wheel, numpy\n\n\n(d)[nani@nande cat]$ ls d/lib/python2.7/site-packages/tensorflow/models/image/\ncifar10  __init__.py  __init__.pyc  mnist\n```\n\nI'm going to make another issue in case this is unrelated.\n", "Can you download the wheel again? I think this is fixed by now, but I want\nto make sure.\n\nOn Thu, Dec 10, 2015 at 1:20 AM daikazoku notifications@github.com wrote:\n\n> I'm not sure if I understand. On the website there is a 0.6.0 wheel in the\n> instructions, but when I go to run the pretrained inception-v3 model, I\n> don't find the imagenet directory:\n> \n> ## (d)[nani@nande cat]$ pip show tensorflow\n> \n> Metadata-Version: 2.0\n> Name: tensorflow\n> Version: 0.6.0\n> Summary: TensorFlow helps the tensors flow\n> Home-page: http://tensorflow.com/\n> Author: Google Inc.\n> Author-email: opensource@google.com\n> License: Apache 2.0\n> Location: /home/nani/Desktop/cat/d/lib/python2.7/site-packages\n> Requires: six, protobuf, wheel, numpy\n> \n> (d)[nani@nande cat]$ ls d/lib/python2.7/site-packages/tensorflow/models/image/\n> cifar10  **init**.py  **init**.pyc  mnist\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/457#issuecomment-163541754\n> .\n", "imagenet model is still missing for me on Mac OS using virtualenv. \n", "When you pip install --upgrade, also use --no-cache-dir\n\nOn Sun, Dec 27, 2015 at 12:23 PM, Roman Manukyan notifications@github.com\nwrote:\n\n> imagenet model is still missing for me on Mac OS using virtualenv.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/457#issuecomment-167439537\n> .\n"]}]