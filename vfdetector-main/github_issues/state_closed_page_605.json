[{"number": 35517, "title": "Building with OMP support compiles, but causes runtime error \"undefined symbol: omp_get_thread_num\"", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.6 LTS (Xenial Xerus)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.15\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: from source, within a virtualenv. Bazel creates a pip wheels which is then installed.\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.12) \r\n- CUDA/cuDNN version: not used, CPU only\r\n- GPU model and memory: not used, CPU only\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am trying to build TF from source with support for `omp simd`. Compiling, creating and installing the pip wheel works fine using bazel. However, as soon as I try to import tensorflow, I get the error `ImportError: /work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: undefined symbol: omp_get_thread_num` \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nAs described in https://www.tensorflow.org/install/source, I first run `bazel clean`. Then I run `./configure` and always select the default values:\r\n\r\n```\r\n(t2t) brix@cluster:/work/tensorflow$ ./configure \r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: Waiting for server process to terminate (waited 5 seconds, waiting at most 60)\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.26.1 installed.\r\nPlease specify the location of python. [Default is /work/venv/t2t/bin/python]: \r\n\r\nFound possible Python library paths:\r\n  /work/venv/t2t/lib/python3.5/site-packages\r\nPlease input the desired Python library path to use.  Default is [/work/venv/t2t/lib/python3.5/site-packages]\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: No OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: No MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n```\r\n\r\nAfterwards, I run `bazel build -s --config=opt --copt=\"-fopenmp\" --copt=\"-lgomp\" //tensorflo\r\nw/tools/pip_package:build_pip_package`. Note that I've added `--copt=\"-fopenmp\" --copt=\"-lgomp\"` to ensure that `omp` is linked. This was suggested by the comment in this SO question: https://stackoverflow.com/questions/45667374/import-tensorlfow-failed-with-errors-undefined-symbol-omp-get-num-threads\r\n\r\nFinally, I create the pip package using `./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg` and install it.\r\n\r\nUnfortunately, when I then run `python -c \"import tensorflow\"`, I get the following stack trace:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: undefined symbol: omp_get_thread_num\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/work/venv/t2t/lib/python3.5/site-packages/tensorflow/__init__.py\", line 99, in <module>\r\n    from tensorflow_core import *\r\n  File \"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/work/venv/t2t/lib/python3.5/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/work/venv/t2t/lib/python3.5/site-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.5/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: undefined symbol: omp_get_thread_num\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n", "comments": ["omp_get_thread_num should be one of the functions in openmp.\r\nDid you install openmp before the build process?", "It seems that your problem is the same as the one I am describing [here](https://stackoverflow.com/q/60567645/2127008).", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35517\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35517\">No</a>\n"]}, {"number": 35516, "title": "request to tag the latest fully qualified commit on the master branch", "body": "It would be very helpful to know the \"latest fully qualified\" commit on the master branch (i.e. the commit for which all internal TF checks have passed). The tip of the master branch is not that (i.e. the \"latest fully qualified\") commit, because we sometimes see commits/PRs getting rolled back because they failed the internal TF checks.\r\n\r\nHaving a tag/pointer to the \"latest fully qualified\" commit would helpful to us (and I suspect othesr in a similar boat to us), in at least a couple of ways                                                                                                         \r\n\r\n1. We current have a nightly job for the ROCm Community Supported Build (CSB), which uses the tip of the master. If there was a \"bad\" commit (i.e. a commit that will, but has not yet been, rolled back due to internal check failure) since the last CSB run, then it has the potential to fail the ROCm CSB as well. When the ROCm CSB fails, we need to go through the process of triaging the failures, and ensuring that the failures were not regressions caused by ROCm.                                                 \r\n                                                                                                                                                                                                                                                                 \r\n  - If the nightly job uses the \"latest fully qualified\" commit, instead of the tip of master, then we could immediately rule out, \"bad\" upstream commits as the cause, saving us time and avoiding unnecessary ROCm CSB build failures.                             \r\n                                                                                                                                                                                                                                                                 \r\n   - In addition to the nightly job for ROCm CSB, we will soon be adding nightly jobs for running broader suite of tests for the ROCm build, and running them on the \"latest full qualified\" commit instead of the tip, would better serve our purpose (of detecting regressions in ROCm functionality)                                                                                                                                                                                                                                                      \r\n                                                                                                                                                                                                                                                                 \r\n                                                                                                                                                                                                                                                                 \r\n2. We maintain a fork on the TF repo, and periodically (weekly) sync with the `master` branch in the upstream repo. We use the tip of `master` to sync, but using the \"latest fully qualified\" commit instead, would be a better option.                         \r\n\r\nthanks", "comments": ["It is more complicated than that.\r\nSometimes, even if a commit passes all our internal checks, we may see bugs later that cause issues in production TF users, or some cases which only show up in very specific use cases. We have had roll backs up to 7 days after a commit was merged. So in my opinion, a commit we can mark that \"there will be no rollbacks at this point\" is infeasible (In my opinion).\r\n\r\nEven if we tried to write a marker that says \"this commit has passed every single TF build\" only releases so far has done that. In master branch, we run every single release test every night. However, there has never been a moment in the history of TF that master branch has passed every single nightly(release) test. ", "guess we are not necessarily looking for a commit for which every single nightly test has passed, but rather a commit that has gone through somewhat more rigorous testing then what it done in the PR checks.\r\n\r\nevery now and then we see commits which break the build / tests. They do seem to get detected and fixed fairly quickly (within hours or a day or two), but for the intervening time/commits, it would be nice to have some mechanism in place to skip those commits (for sync purposes on our end)\r\n\r\nrecent example would be this commit which broke both the ROCm and CUDA builds\r\nhttps://github.com/tensorflow/tensorflow/commit/56560f588c679c5a4744ff5c8d842266fde31467\r\n\r\nand the follow up commit to fix it.\r\nhttps://github.com/tensorflow/tensorflow/commit/adc3a1b1bec28849f62076e9b4be5c5963e5e5e7\r\n\r\nIf it helps, we can narrow down the scope of this issue to just the set of tests that are published in the TF repo. Would it be possible to tag the latest commit that has passed all the tests the in TF repo \r\n(i.e. all the tests run by the various scripts within `tensorflow/tools/ci_build[/xla]/linux/[cpu,gpu,rocm]` ... )\r\n\r\n", "Closing this as this is not feasible. ", "@lvenugopalan why close this issue....thought we were still discussing this.\r\n\r\nis my later proposal not feasible either?", "@deven-amd I closed this as per @gunan  earlier comments. Let me reopen.", "@gunan \r\n\r\ncouple of examples today, where having such a tag would have helped. \r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/35832\r\nhttps://github.com/tensorflow/tensorflow/pull/35834\r\n\r\nThough I have filed both PRs as \"ROCm\", the breakages are generic, and I would expect them to be caught by Google's CI fairly quickly", "Again, all tests under tensorflow repository is only run for all configurations once every night. What I say above is still valid for all tests under tensorflow. So from my point of view, it is equally infeasible.\r\n\r\nI guess this is in part  our general development model. We live and develop at head. there are no \"stable\" tags except for release branches/tags."]}, {"number": 35515, "title": "layers.Inputlayer does not show up in model.summary()", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nTensorflow 2.1.0 on gpu on colab\r\n\r\n**Describe the current behavior**\r\nIn model.summary(), layers.Inputlayer() does not show up as a layer.\r\n\r\n**Code to reproduce the issue**\r\n`model1 = tf.keras.models.Sequential([layers.InputLayer(input_shape=(784,)),\r\n                                     layers.Dense(1500, activation=\"relu\"),\r\n                                     layers.Dense(1000, activation=\"relu\"),\r\n                                     layers.Dense(500, activation=\"relu\"),\r\n                                     layers.Dense(1, activation=\"sigmoid\"),\r\n                                     layers.Dense(500, activation=\"relu\"),\r\n                                     layers.Dense(1000, activation=\"relu\"),\r\n                                     layers.Dense(1500, activation=\"relu\"),\r\n                                     layers.Dense(784),\r\n])`\r\n![Screenshot from 2019-12-31 22-39-50_modified](https://user-images.githubusercontent.com/43641071/71628625-d147b500-2c1e-11ea-873a-b8adf4259cfd.png)\r\n\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n", "comments": ["the Input layer does not show up explicitly in model.summary() when the Sequential API is used does not mean anything regarding the functionality of the model.\r\nrefer:-\r\n[https://stackoverflow.com/questions/54497422/the-input-layer-disappears-from-the-structure-of-a-deep-learning-model](url)", "i know it doesn't affect the functionality but it does show up in the keras functional api so i thought it would be a bug in tensorflow", "@rohanreddych \r\n\r\nI have tried in colab with TF 2.1 using TF.keras and i am not seeing ` layers.Inputlayer() `in` model1.summary()`. Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/3579f1602b4bceb0d7a526cee3316e86/untitled521.ipynb).I have tried with TF 1.14 in Keras as well ,still i am not seeing  `layers.Inputlayer()` in `model1.summary()`. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/6971f8ce63cae4b4a29904721c4fad43/untitled522.ipynb).Thanks!", "Yes it is consistent with my issue so is it a bug in tensorflow", "Input layer merely decides the input shape and is not necessarily a layer. \r\nYou may try using ```functional api``` to explicitly see it in model summary:,\r\n```python\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Dense\r\n\r\ninputs = Input(shape=(784,))\r\nx = Dense(1500, activation=\"relu\")(inputs)\r\nx = Dense(1000, activation=\"relu\")(x)\r\nprediction = Dense(784, activation='softmax')(x)\r\n\r\nmodel_functional = Model(inputs=inputs, outputs=prediction)\r\n\r\nmodel_functional.summary()\r\n```\r\nOutput:\r\n```python\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 784)]             0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 1500)              1177500   \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 1000)              1501000   \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 784)               784784    \r\n=================================================================\r\nTotal params: 3,463,284\r\nTrainable params: 3,463,284\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```", "Suppose I want to implement backprop on my own (independent if the model is created with functional or sequential API), how do I know if the input layer is included or not?! Why can't it just be implemented consistent with the functional API? Argh\r\n\r\nIn `model.get_config()[\"layers\"]` the input layer is still listed, while in `model.layers` it is not present?! In a functional model it is present in both.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nseqmodel = tf.keras.Sequential()\r\nseqmodel.add(tf.keras.Input(shape=(4)))\r\nseqmodel.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))\r\n\r\ninLayer = tf.keras.layers.Input(shape=(4))\r\ndenseLayer = tf.keras.layers.Dense(1,activation=\"sigmoid\")\r\nout = denseLayer(inLayer)\r\nfuncmodel = tf.keras.Model(inputs=inLayer, outputs=out, name=\"twolayer\")\r\n\r\nprint(\"Seq Config Num Layers: \",len(seqmodel.get_config()[\"layers\"]))\r\nprint(\"Func Config Num Layers: \",len(funcmodel.get_config()[\"layers\"]))\r\nprint()\r\nprint(\"Seq Num Layers : \",len(seqmodel.layers))\r\nprint(\"Func Num Layers: \",len(funcmodel.layers))\r\n```\r\n\r\nOutput:\r\n```\r\nSeq Config Num Layers:  2\r\nFunc Config Num Layers:  2\r\n\r\nSeq Num Layers :  1\r\nFunc Num Layers:  2\r\n```\r\n\r\nThis leads to this fix:\r\n```\r\n    layers = model.layers\r\n    numLayers = len(layers)\r\n    \r\n    if len(model.get_config()[\"layers\"]) != len(model.layers):\r\n        sequential = True\r\n        offset = 0\r\n        print(\"Sequential Model\")\r\n    else:\r\n        sequential = False\r\n        offset = 1\r\n        print(\"Functional Model\")\r\n        \r\n    for nlid in range(0,numLayers-offset): ....\r\n```\r\n", "I hope that the fact that this is not consistent with the functional API motivates to implement this.\r\n\r\nAlso related: issue #35995.", "> Input layer merely decides the input shape and is not necessarily a layer.\r\n> You may try using `functional api` to explicitly see it in model summary:,\r\n> \r\n> ```python\r\n> from tensorflow.keras.models import Model\r\n> from tensorflow.keras.layers import Input, Dense\r\n> \r\n> inputs = Input(shape=(784,))\r\n> x = Dense(1500, activation=\"relu\")(inputs)\r\n> x = Dense(1000, activation=\"relu\")(x)\r\n> prediction = Dense(784, activation='softmax')(x)\r\n> \r\n> model_functional = Model(inputs=inputs, outputs=prediction)\r\n> \r\n> model_functional.summary()\r\n> ```\r\n> \r\n> Output:\r\n> \r\n> ```python\r\n> Model: \"model\"\r\n> _________________________________________________________________\r\n> Layer (type)                 Output Shape              Param #   \r\n> =================================================================\r\n> input_1 (InputLayer)         [(None, 784)]             0         \r\n> _________________________________________________________________\r\n> dense (Dense)                (None, 1500)              1177500   \r\n> _________________________________________________________________\r\n> dense_1 (Dense)              (None, 1000)              1501000   \r\n> _________________________________________________________________\r\n> dense_2 (Dense)              (None, 784)               784784    \r\n> =================================================================\r\n> Total params: 3,463,284\r\n> Trainable params: 3,463,284\r\n> Non-trainable params: 0\r\n> _________________________________________________________________\r\n> ```\r\n\r\nWhy this kind of summary is not a default one? It would be so much more informative.", "The reason for this is we will run the model.summary() command before we fit and run the model where input will be mentioned. So basically the model.summary wont be having any idea of input dimension at its time. so in sequential you cant see the input layer in summary . hope this helps\r\n"]}, {"number": 35514, "title": "pip installation .whl for aarch64?", "body": "\r\nIs there a release version for **aarch64**?\r\nIt seems there are only version of **x86_64**, **raspberry pi(armv7)**, etc. ?\r\n", "comments": ["Hey @jiapei100 \r\n\r\nTry this- pip install tensorflow-aarch64", "@jiapei100 ,\r\nPlease try using this [command](https://pypi.org/project/tensorflow-aarch64/).Thanks!", "@jiapei100 ,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35514\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35514\">No</a>\n"]}, {"number": 35513, "title": "model.fit_generator() always uses eager execution instead of a compiled graph", "body": "**System information**\r\nOS: Debian 10, x64\r\nGPU: GeForce GTX 1060 6GB\r\nPython: 3.7.3\r\nTensorFlow: 2.0.0 installed from source (git tag v2.0.0)\r\nBazel: 0.26.1\r\nGCC: 6.5.0\r\nCUDA: 10.1\r\ncuDNN: 7.6.4\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.layers import Input, Layer\r\n\r\nBATCH_SIZE = 32\r\nN_BATCHES = 8\r\nFEATURE_SIZE = 32\r\n\r\nclass SimpleDense(Layer):\r\n    def __init__(self, units, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.units = units\r\n\r\n    def build(self, input_shape):\r\n        self.kernel = self.add_weight('kernel', shape=[input_shape[-1], self.units])\r\n\r\n    def call(self, inputs, **kwargs):\r\n        print('call() has been called with', inputs.__class__.__name__)\r\n        return tf.matmul(inputs, self.kernel)\r\n\r\ni = Input(shape=(FEATURE_SIZE,))\r\no = SimpleDense(units=FEATURE_SIZE)(i)\r\nm = Model(i, o)\r\nm.compile('sgd', 'mse')\r\n\r\nprint('*** Using fit()')\r\nxy = numpy.zeros((N_BATCHES * BATCH_SIZE, FEATURE_SIZE))\r\nm.fit(xy, xy, batch_size=BATCH_SIZE, verbose=0)\r\n\r\nprint('*** Using fit_generator()')\r\n\r\ndef gen():\r\n    zeros = numpy.zeros((BATCH_SIZE, FEATURE_SIZE))\r\n    for _ in range(N_BATCHES):\r\n        yield zeros, zeros\r\n\r\nm.fit_generator(gen(), steps_per_epoch=N_BATCHES, epochs=1, verbose=0)\r\n```\r\n\r\n**Describe the current behavior**\r\nOutput of the above code is:\r\n```\r\ncall() has been called with Tensor\r\n*** Using fit()\r\ncall() has been called with Tensor\r\ncall() has been called with Tensor\r\n*** Using fit_generator()\r\ncall() has been called with EagerTensor\r\ncall() has been called with EagerTensor\r\ncall() has been called with EagerTensor\r\ncall() has been called with EagerTensor\r\ncall() has been called with EagerTensor\r\ncall() has been called with EagerTensor\r\ncall() has been called with EagerTensor\r\ncall() has been called with EagerTensor\r\n```\r\nWhen using `fit()` the model runs as a graph, as it should. When using `fit_generator()` the model is run with eager execution, causing `call()` to be invoked on every single batch.\r\n\r\n**Describe the expected behavior**\r\n`fit_generator()` should also run the model as a graph, for increased performance.\r\n\r\n**Other observations**\r\n* Using `tf.compat.v1.disable_eager_execution()` is an obvious workaround\r\n* It's also possible to use `m.compile(..., experimental_run_tf_function=False)`, however that is very inconvenient because this flag does not persist across saving/reloading (and it's not possible to pass it again, since `compile()` is called by the loading code)\r\n* Setting `m._experimental_run_tf_function = False` is an ugly hack, but works also when reloading the model.\r\n* Setting `m.run_eagerly = False` does not help since it's already `False`.\r\n* Decorating `SimpleDense.call()` with `@tf.function` also works, but that's inconvenient to do for all layers and it's not optimal since only a part of the entire model will run as graph. Also I'm not sure how to manually wrap the entire model with `@tf.function`.", "comments": ["@jlherren \r\n\r\nModel.fit_generator is deprecated starting from tensorflow 2.1.0 which is currently is in rc2. You can find the documentation for tf-2.1.0-rc1 [here](https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/Model#fit). Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/0864e10599ddf385c85f3c60f0b2060f/untitled523.ipynb). Thanks!", "I will go for TF 2.1, thanks."]}, {"number": 35512, "title": "C++ Tensorflow is very slower than Python Tensorflow", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14 and 1.13\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): VC 14 (VS 2015)\r\n- CUDA/cuDNN version: 10.0/7.4\r\n- GPU model and memory: RTX 2080\r\n\r\n**Describe the current behavior**\r\nI exported a .pb graph file. I use it in python and it took 55 milliseconds to process. I also use it in C++ (both pre-build versions and self-build from source) it took 750 milliseconds. Everything is same!\r\n\r\n**Describe the expected behavior**\r\nboth python and C++ take 55 ms to process.\r\n\r\n \r\n**Other info / logs**\r\nI build C++ API with below command:\r\n`bazel --output_user_root=\"C:/bazel/our\" build -c opt --copt=\"-mavx\" --config=opt --config=cuda //tensorflow:libtensorflow_cc.so` then rename .so to .dll and .so.ifso to .lib\r\n\r\nin #3471 (https://github.com/tensorflow/tensorflow/issues/3471#issuecomment-234671300) mentioned to add `-c opt` flag with `--copt=-mavx`. but it didn't make any difference. Also I found tons of warning as: \r\n`cl : Command line warning D9002 : ignoring unknown option '-mavx'\r\n`\r\n", "comments": ["@concretevitamin would you please help?!", "@fmmohammadi ,\r\nThanks for reporting the issue ,Can you share a standalone code to reproduce the error reported here?Thanks!", "> @fmmohammadi ,\r\n> Thanks for reporting the issue ,Can you share a standalone code to reproduce the error reported here?Thanks!\r\n\r\nThanks for your consideration @oanush. Actually this is not a error that may crashing the program. It is a performance bug. BTW, here is a sample code:\r\n\r\nFirst include tensorflow:\r\n\r\n```\r\n#include \"tensorflow/cc/ops/const_op.h\"\r\n#include \"tensorflow/cc/ops/image_ops.h\"\r\n#include \"tensorflow/cc/ops/array_ops.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/graph.pb.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/graph/default_device.h\"\r\n#include \"tensorflow/core/graph/graph_def_builder.h\"\r\n#include \"tensorflow/core/lib/core/errors.h\"\r\n#include \"tensorflow/core/lib/core/stringpiece.h\"\r\n#include \"tensorflow/core/lib/core/threadpool.h\"\r\n#include \"tensorflow/core/lib/io/path.h\"\r\n#include \"tensorflow/core/lib/strings/stringprintf.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\n#include \"tensorflow/core/platform/init_main.h\"\r\n#include \"tensorflow/core/platform/logging.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/util/command_line_flags.h\"\r\n#include \"tensorflow/cc/ops/io_ops.h\"\r\n#include <tensorflow/core/protobuf/meta_graph.pb.h>\r\n```\r\n\r\nNext,  Load the freez graph (.pb file) and create a session:\r\n\r\n```\r\nGraphPath = \"../model.pb\";\r\ntensorflow::SessionOptions options;\r\ntensorflow::ConfigProto & config = options.config;\r\nconfig.mutable_gpu_options()->set_allow_growth(true);\r\ntensorflow::Status status;\r\nstatus = tensorflow::NewSession(options, &session);\r\nif (!status.ok()) {std::cout << \"error: \" << status.ToString() << \"\\n\";}\r\ntensorflow::GraphDef graph_def;\r\nstatus = ReadBinaryProto(tensorflow::Env::Default(), GraphPath, &graph_def);\r\nif (!status.ok()) {std::cout << \"error: \" << status.ToString() << \"\\n\";}\r\nstatus = session->Create(graph_def);\r\nif (!status.ok()) {std::cout << \"error: \" << status.ToString() << \"\\n\";}\r\n```\r\n\r\nThen, load video frames with OpenCV and copy cvMat to a Tensor:\r\n\r\n```\r\nchar VIDEO_FILE[] = \"../data/Video.asf\";\r\nVideoCapture capture;\r\ncapture.open(VIDEO_FILE);\r\nif (!capture.isOpened()) {throw \"Error when reading VIDEO_FILE\";}\r\nMat frame;\r\ncapture >> frame;\r\nif (frame.empty()) {throw \"Error when reading FRAME\";}\r\nTensor Img_ten(tensorflow::DT_FLOAT, tensorflow::TensorShape({ 1, 3, height_des, width_des }));\r\nCopyCvImage2Tensor(frame, Img_ten); //this function copy image to above tensor successfully!\r\n```\r\n\r\nFinally, run the session:\r\n\r\n```\r\n//input and output name. all is correct!\r\nstring input_layer = \"input:0\";\r\nstd::vector<string> output_layer = { \"output:0\", \"output:1\", \"output:2\",  \"output:3\", \"output:4\", \"output:5\", \"output:6\", \"output:7\", \"output:8\" };\r\nstd::vector<Tensor> net_out;\r\nStatus run_status = session->Run({ { input_layer, Img_ten } }, output_layer, {}, &net_out);\r\nif (!run_status .ok()) {std::cout << \"error: \" << run_status .ToString() << \"\\n\";}\r\n```\r\n\r\n Here, the final section (running session), with python tensorflow take about 50 to 60 milliseconds but with C++ tensorflow (which build with parameters as mentioned above) take about 700 to 720 milliseconds. about 12x slower!!!\r\n\r\n\r\n\r\nAlso, here is the python code:\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\ntf_graph = self.load_pb('../model.pb')\r\nsess = tf.Session(graph=tf_graph)\r\noutput_tensor = [\"output:0\", \"output:1\", \"output:2\",  \"output:3\", \"output:4\", \"output:5\", \"output:6\", \"output:7\", \"output:8\"]\r\ninput_tensor = tf_graph.get_tensor_by_name('input:0')\r\nret, frame = cap.read()\r\nIMAGE_tensor = copy2Tensor(frame)\r\noutput_tf = sess.run(output_tensor, feed_dict={input_tensor: IMAGE_tensor})\r\n```\r\n\r\n\r\nBoth are same version of tensorflow and run on RTX 2080. both allocate about 7GB memory of GPU but the main difference is than python API utilize about 40% of Cuda whereas C++ API utilize about 5% of Cuda in Performance tab of Task Manager (or even with nvidia-smi.exe). I think this is the cause of this performance bug. But I don't know what is the solution?! (It is worth to mention that the output of the session run is same for both python and C++ api. In fact, problem is just inference time which C++ is 12x slower than python!)\r\n\r\n\r\n\r\n", "@ymodak no idea?", "Can you attach your `.pb` file as well?", "Was this resolved later? I encountered the same problem.\r\n@ymodak\r\n@fmmohammadi", "> Was this resolved later? I encountered the same problem.\r\n> @ymodak\r\n> @fmmohammadi\r\n\r\nTry rebuild tensorflow with mkl support\r\nalso make sure that the debugger of VS is not activate during running (even in release mode)"]}, {"number": 35511, "title": "where can I find \u201chexagon_nn_skel.run\u201d ?", "body": "from \"tensorflow/lite/g3doc/performance/hexagon_delegate.md\" :\r\nRun \u201chexagon_nn_skel.run\u201d - Note: you will need to accept the license agreement. It should provide 3 different shared libraries \u201clibhexagon_nn_skel.so\u201d, \u201clibhexagon_nn_skel_v65.so\u201d, \u201clibhexagon_nn_skel_v66.so\u201d\r\n\r\nbut I can not find hexagon_nn_skel.run, where can I download  \u201clibhexagon_nn_skel.so\u201d, \u201clibhexagon_nn_skel_v65.so\u201d, \u201clibhexagon_nn_skel_v66.so\u201d  ??", "comments": ["If you read the instructions [here](https://blog.tensorflow.org/2019/12/accelerating-tensorflow-lite-on-qualcomm.html), you can see they have included a link to download that\r\n\r\nI extracted that link here for you, in case you don't want to search through the instructions: [hexagon_nn_skel.run](https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_1_10_3_1.run)", "thanks", "> thanks\r\n\r\n\r\n\r\n> If you read the instructions [here](https://blog.tensorflow.org/2019/12/accelerating-tensorflow-lite-on-qualcomm.html), you can see they have included a link to download that\r\n> \r\n> I extracted that link here for you, in case you don't want to search through the instructions: [hexagon_nn_skel.run](https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_1_10_3_1.run)\r\n\r\nHI There,\r\n\r\nWhen I'm trying to run the file it's stuck on \r\nVerifying archive integrity...  100%   All good.\r\nUncompressing Hexagon NN Shared Libs  100%  \r\n:\r\nThat's it.. I have been waiting for almost 15-20 min and no progress. \r\nCan you please help me in finding what I'm doing wrong.\r\n\r\nThanks\r\n", "> > thanks\r\n> \r\n> > If you read the instructions [here](https://blog.tensorflow.org/2019/12/accelerating-tensorflow-lite-on-qualcomm.html), you can see they have included a link to download that\r\n> > I extracted that link here for you, in case you don't want to search through the instructions: [hexagon_nn_skel.run](https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_1_10_3_1.run)\r\n> \r\n> HI There,\r\n> \r\n> When I'm trying to run the file it's stuck on\r\n> Verifying archive integrity... 100% All good.\r\n> Uncompressing Hexagon NN Shared Libs 100%\r\n> :\r\n> That's it.. I have been waiting for almost 15-20 min and no progress.\r\n> Can you please help me in finding what I'm doing wrong.\r\n> \r\n> Thanks\r\n\r\nI finally managed to make it work. For people wondering you need to type \"I ACCEPT\" without quotes after \":\" to make it extract files. Maybe \"Y/n\" option would've made more sense."]}, {"number": 35510, "title": "Added tf.strings.to_number() usage example", "body": "", "comments": ["Cool, I've changed it. Please take a look, and happy new years also :)", "Thank you and happy New year!", "Happy to contribute! :)", "Thank you!\n"]}, {"number": 35509, "title": "TFLite GPU Delegate on iOS: failed assertion `Cannot create a buffer of zero length.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iPAD OS 13.2.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPAD Pro 2018\r\n- TensorFlow installed from (source or binary): Binary (TfLiteGPUExperimental)\r\n- TensorFlow version (use command below):\r\n- Python version: NA\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: iPAD GPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen running the model on iPAD using CPU, we are able to get the output. But when doing GPU delegate, we get the error the following error:\r\n_failed assertion `Cannot create a buffer of zero length._\r\n**Describe the expected behavior**\r\nNo error\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\ndelegate = NewGpuDelegate(nullptr);\r\n      interpreter->ModifyGraphWithDelegate(delegate);\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi Folks, any update on this one. Thanks.", "Were you trying to run your own app? Which model were you using?", "@yyoon I was using 'palm_detection.tflite' model on the following web link --> https://github.com/google/mediapipe/tree/master/mediapipe/models\r\n\r\nPlease let me know if you need any additional information.", "And which exact app were you using for running that model?\r\nWas it your own written app, or did you try to modify one of the existing example apps?", "@yyoon We were trying to use tflite model on iOS using the following code.\r\n\r\n```\r\ntflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\nGpuDelegateOptions options;\r\noptions.allow_precision_loss = true;\r\noptions.wait_type = GpuDelegateOptions :: WaitType::kActive;\r\ndelegate = NewGpuDelegate(nullptr);\r\ninterpreter->ModifyGraphWithDelegate(delegate);\r\n....\r\n\r\n```\r\nThe code fails on the last  line 'interpreter->ModifyGraphWithDelegate(delegate);' above. Please let me know if you need any more information. Thanks.", "@yyoon Any update on this one. Thanks a lot for your help.", "@yyoon Any luck with this one. Thanks.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "We can close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35509\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35509\">No</a>\n"]}, {"number": 35508, "title": "Tensorflow gets stuck in sess.run() while making a prediction", "body": "I'm using an **ssd** model to make prediction on images using **tensorflow**. I'm currently on **tensorflow** **1.14.0**.\r\n\r\nI'm running the code in two different environments. I'm making a request to Cherrypy in one environment and it is working perfectly fine.\r\n\r\nIn the other environment, the request is made to **Cherrypy** through **Kafka** and the `tf.session.run()` gets stuck infinitely and doesn't return any output nor does it raise an exception.\r\n\r\n`results = self._do_run(handle, final_targets, final_fetches,\r\n                             feed_dict_tensor, options, run_metadata)`\r\n\r\nThis line is found in `session.py` package in `TensorFlow`. This is the specific point in code where the code gets stuck\r\n\r\nI have not been able to figure out a reason for this anomaly and I didn't find anything useful on the internet regarding this problem. Does sending a request through **Kafka** is causing such an abnormal behavior?\r\n\r\nAny help in this regard would be very appreciated. Thank You.\r\n\r\nThe details of my environment:\r\n\r\n> CLUTTER_IM_MODULE=xim LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:.tar=01;31:.tgz=01;31:.arc=01;31:.arj=01;31:.taz=01;31:.lha=01;31:.lz4=01;31:.lzh=01;31:.lzma=01;31:.tlz=01;31:.txz=01;31:.tzo=01;31:.t7z=01;31:.zip=01;31:.z=01;31:.Z=01;31:.dz=01;31:.gz=01;31:.lrz=01;31:.lz=01;31:.lzo=01;31:.xz=01;31:.zst=01;31:.tzst=01;31:.bz2=01;31:.bz=01;31:.tbz=01;31:.tbz2=01;31:.tz=01;31:.deb=01;31:.rpm=01;31:.jar=01;31:.war=01;31:.ear=01;31:.sar=01;31:.rar=01;31:.alz=01;31:.ace=01;31:.zoo=01;31:.cpio=01;31:.7z=01;31:.rz=01;31:.cab=01;31:.wim=01;31:.swm=01;31:.dwm=01;31:.esd=01;31:.jpg=01;35:.jpeg=01;35:.mjpg=01;35:.mjpeg=01;35:.gif=01;35:.bmp=01;35:.pbm=01;35:.pgm=01;35:.ppm=01;35:.tga=01;35:.xbm=01;35:.xpm=01;35:.tif=01;35:.tiff=01;35:.png=01;35:.svg=01;35:.svgz=01;35:.mng=01;35:.pcx=01;35:.mov=01;35:.mpg=01;35:.mpeg=01;35:.m2v=01;35:.mkv=01;35:.webm=01;35:.ogm=01;35:.mp4=01;35:.m4v=01;35:.mp4v=01;35:.vob=01;35:.qt=01;35:.nuv=01;35:.wmv=01;35:.asf=01;35:.rm=01;35:.rmvb=01;35:.flc=01;35:.avi=01;35:.fli=01;35:.flv=01;35:.gl=01;35:.dl=01;35:.xcf=01;35:.xwd=01;35:.yuv=01;35:.cgm=01;35:.emf=01;35:.ogv=01;35:.ogx=01;35:.aac=00;36:.au=00;36:.flac=00;36:.m4a=00;36:.mid=00;36:.midi=00;36:.mka=00;36:.mp3=00;36:.mpc=00;36:.ogg=00;36:.ra=00;36:.wav=00;36:.oga=00;36:.opus=00;36:.spx=00;36:.xspf=00;36: LESSCLOSE=/usr/bin/lesspipe %s %s XDG_MENU_PREFIX=gnome- LANG=en_US.UTF-8 MANAGERPID=3651 DISPLAY=:0 INVOCATION_ID=56bfbab67ced4d1496b1e75a14fb4af7 GNOME_SHELL_SESSION_MODE=ubuntu COLORTERM=truecolor ZEITGEIST_DATA_PATH=/home/pandas/.local/share/zeitgeist USERNAME=pandas XDG_VTNR=2 SSH_AUTH_SOCK=/run/user/1000/keyring/ssh XDG_SESSION_ID=2 USER=pandas DESKTOP_SESSION=ubuntu QT4_IM_MODULE=xim TEXTDOMAINDIR=/usr/share/locale/ GNOME_TERMINAL_SCREEN=/org/gnome/Terminal/screen/a438ce7f_2ce0_4925_a81f_ef228f7d5a94 JOURNAL_STREAM=9:71038 TEXTDOMAIN=im-config SSH_AGENT_PID=3796 QT_ACCESSIBILITY=1 XDG_SESSION_TYPE=x11 XDG_DATA_DIRS=/usr/share/ubuntu:/usr/local/share/:/usr/share/:/var/lib/snapd/desktop XDG_SESSION_DESKTOP=ubuntu DBUS_STARTER_ADDRESS=unix:path=/run/user/1000/bus,guid=d47913508d685ac43652dcc65e059d87 GTK_MODULES=gail:atk-bridge WINDOWPATH=2 TERM=xterm-256color SHELL=/bin/bash VTE_VERSION=5202 QT_IM_MODULE=xim XMODIFIERS=@im=ibus IM_CONFIG_PHASE=2 DBUS_STARTER_BUS_TYPE=session XDG_CURRENT_DESKTOP=ubuntu:GNOME GPG_AGENT_INFO=/run/user/1000/gnupg/S.gpg-agent:0:1 GNOME_TERMINAL_SERVICE=:1.98 XDG_SEAT=seat0 SHLVL=1 GDMSESSION=ubuntu GNOME_DESKTOP_SESSION_ID=this-is-deprecated LOGNAME=pandas DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus,guid=d47913508d685ac43652dcc65e059d87 XDG_RUNTIME_DIR=/run/user/1000 XAUTHORITY=/run/user/1000/gdm/Xauthority XDG_CONFIG_DIRS=/etc/xdg/xdg-ubuntu:/etc/xdg PATH=/home/pandas/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin SESSION_MANAGER=local/ubuntu:@/tmp/.ICE-unix/3691,unix/ubuntu:/tmp/.ICE-unix/3691 LESSOPEN=| /usr/bin/lesspipe %s GTK_IM_MODULE=ibus\r\n\r\n> ", "comments": ["@ubaid08, Just to verify, did you check whether the ssd model is running as expected before deploying it on Cherrypy. Thanks!", "@gadagashwini yes it working perfectly fine when a direct call is made to cherrypy but it causes the mentioned problem when a request is made to cherrypy through kafka.", "@ubaid08, Looks like issue is not related to Tensorflow. Can you check and confirm. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 35507, "title": "Bugfix: set correct timeline label for _Send/_Recv op.", "body": "The right parenthesis `)` is missing in both `_Send` and `_Recv` cases, which leads to the regexp match failure in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/timeline.py#L389 and confusing \"op\" and \"name\" message are generated in chrome tracing timeline.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35507) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@zhuwenxi thank you for your contribution, please sign CLA.", "@gbaned I'm pretty sure I've signed the CLA. Could you take a look at it?", "@zhuwenxi  still, I can see cla: no under the Labels. ", "@gbaned OK, it's wired... this is a screenshot of my CLA page:\r\n![image](https://user-images.githubusercontent.com/4969797/71650553-9db97780-2d51-11ea-8e5f-19376e2bda95.png)\r\n\r\nAccording to my understanding, this page shows that I've already signed the CLA. Is there anything wrong? \r\n\r\nThank you!\r\n", "Recall this PR due to CLA not ready."]}, {"number": 35506, "title": "code for libhexagon_interface.so", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.15\r\n- Python version: 3.4\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 1.2.1\r\n- GCC/Compiler version (if compiling from source): 5.4\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 1080TI, 12G\r\n\r\n**Describe the problem**\r\n@karimnosseir, saw your answers to the https://github.com/tensorflow/tensorflow/issues/35221, \r\nJust interested in where is the source code to build libhexagon_interface.so for the hexagon delegate? \r\nFound that libhexagon_nn_skel.so had the source code in https://source.codeaurora.org/quic/hexagon_nn/nnlib/, but no code for libhexagon_interface.so? Do you know if libhexagon_interface.so will be open source as well?\r\n\r\n", "comments": ["Hi,\r\n\r\nWe can't host the code because of licensing as it depends on some libraries we don't have control on.\r\n", "Got it, thanks.", "how can we get libhexagon_interface.so? I can't find this shared library from the directory of tensorflow lite. Hope someone can help me !", "@liuming-td You can get from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/hexagon_nn/BUILD#L34)\r\n\r\nexample:\r\nbazel build --config=android_arm64  -c opt //tensorflow/lite/delegates/hexagon/hexagon_nn:libhexagon_interface.so\r\n\r\nIf you need it for arm linux\r\nYou can reach directly to me through email (same as my username here) and i will share it.\r\n\r\nP.S.:\r\nThe code itself will be added to github soon, we worked with QC on resolving most of the licensing issues and will be pushed soon.", "> @liuming-td You can get from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/hexagon_nn/BUILD#L34)\r\n> \r\n> example:\r\n> bazel build --config=android_arm64 -c opt //tensorflow/lite/delegates/hexagon/hexagon_nn:libhexagon_interface.so\r\n> \r\n> If you need it for arm linux\r\n> You can reach directly to me through email (same as my username here) and i will share it.\r\n> \r\n> P.S.:\r\n> The code itself will be added to github soon, we worked with QC on resolving most of the licensing issues and will be pushed soon.\r\n\r\nThank you very much for your reply!\r\nI want to ask that what's the difference between the \"hexagon_nn_global_teardown\" function in tf lite delegate and \"hexagon_nn_teardown\" function in Hexagonnn SDK. And the same question for \"hexagon_nn_global_init\".\r\nAnd also, could you tell me something about how the \"hexagon_nn_is_device_supported\" function implemented\uff1f", "hexagon_nn_global_teardown/hexagon_nn_global_init are methods we introduced to make sure we do any initialization/destructions required before/after doing any dsp calls.\r\n\r\nhexagon_nn_teardown are responsible for destructing 1 single dsp graph (takes the graph id).\r\n\r\nhexagon_nn_is_device_supported is a method we introduced and calls during delegate creation to make sure the device is from the set of devices that we know is supported and delegate tried on it.\r\n\r\nWe maintain static list of soc ID that are supported.\r\nIt is something like\r\n - Get SoC ID\r\n - Check if SoC ID is in the static list of supported devices\r\n - If yes return true, otherwise false.\r\n\r\n\r\n\r\n", "> hexagon_nn_global_teardown/hexagon_nn_global_init are methods we introduced to make sure we do any initialization/destructions required before/after doing any dsp calls.\r\n> \r\n> hexagon_nn_teardown are responsible for destructing 1 single dsp graph (takes the graph id).\r\n> \r\n> hexagon_nn_is_device_supported is a method we introduced and calls during delegate creation to make sure the device is from the set of devices that we know is supported and delegate tried on it.\r\n> \r\n> We maintain static list of soc ID that are supported.\r\n> It is something like\r\n> \r\n> * Get SoC ID\r\n> * Check if SoC ID is in the static list of supported devices\r\n> * If yes return true, otherwise false.\r\n\r\nReally appreciate!", "@karimnosseir \r\nWhen I call close() on HexagonDelegate, the DDR frequency does not come down.\r\nI want to lower the DDR frequency by calling close() of HexagonDelegate.\r\nThe version of libhexagon_interface.so that I am using is Hexagon SDK (ver. 3.4.3).\r\nI would like to get libhexagon_interface.so which uses the new Hexagon SDK (ver.3.5.4).\r\nCould you please share the libhexagon_interface.so?\r\n\r\nPlease let me know if you have any other good solutions other than the above.", "@usuku-hideki \r\n\r\nThe code of libhexagon_interface.so is now on github [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/hexagon/hexagon_nn)\r\n\r\nFeel free to adjust it as you want per your use case.\r\n\r\nYou might want to also check the code under third_party in [this](https://github.com/tensorflow/tensorflow/blob/master/third_party/hexagon/workspace.bzl) package, as it might be different from what you find in the SDK version you're using.\r\n\r\n", "@karimnosseir \r\nThank you for your reply.\r\nI was able to build and libhexagon_interface.so which uses HexagonSDK version 3.4.3.\r\nDo you know how to make libhexagon_interface.so which uses HexagonSDK version 3.5.4?\r\nPlease let me know if you know.", "@usuku-hideki \r\nHow did you build it ? Did you update the dependent files that are required from the SDK and used blaze or something else ?\r\n\r\nThe code should compile on SDK 3.5.4 and even 4.3.0 ", "@karimnosseir \r\nI built the Heagon SDK version 3.5.4 and created libhexagon_nn_skel.so.\r\nI built the tensorflow library with bazel and created libhexagon_interface.so.\r\n\r\nNone of the files have been changed.\r\nI guess I should change some settings in the dependency files, but I don't know where to change to get the libhexagon_interface.so that uses Hexagon SDK 3.5.4.\r\nIf you know, please let me know.", "@usuku-hideki Sorry for the late reply.\r\n\r\nYou will need to update the build file and replace all dependencies with \"@hexagon_nn//\" to use your own versions.", "@karimnosseir \r\nI have a 865_rb5 device and after flash the system (ubuntu_aarch64) has `/usr/lib/libhexagon_interface.so` and `/usr/lib/rfsa/adsp/libhexagon_nn_skel.so` existed already, and I can run tflite benchmark_model with hexagon delegate successfully, why?  as you known hexagon_nn in tflite is different with which in Hexagon_SDK. \r\nI think the only way is that `/usr/lib/libhexagon_interface.so` on RB5 is built from tflite source code, is that right ?", "Maybe i found the answer:\r\nhttps://developer.qualcomm.com/forum/qdn-forums/hardware/qualcomm-robotics-rb5-development-kit/68515\r\n> If you are opting for second, you application will need two different libraries as mentioned below:\r\n    1. libhexagon_interface.so: \r\nThis library is loaded on ARM side and acts as a stub that runs on arm to send RPC calls to DSP to execute a network. \r\nThis library has to be build from tensorflow repository (using bazel if running on android, RB5 ships this library. You can find it in /usr/lib/)."]}, {"number": 35505, "title": "Improve docstrings for tf.config.optimizer.get_jit and .set_jit", "body": "", "comments": []}, {"number": 35504, "title": "AutoGraph: \"Entity could not be transformed and will be executed as-is\"", "body": "Got the following error output from Databricks; exported the notebook to the attached file; error occurs in cell 10.\r\n\r\nINFO:tensorflow:Converted call: <function read.<locals>.<lambda> at 0x7fbde97eae18>\r\n    args: (<tf.Tensor 'args_0:0' shape=() dtype=string>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Not whitelisted: <method-wrapper '__call__' of function object at 0x7fbde97eae18>: default rule\r\nINFO:tensorflow:Not whitelisted: <function read.<locals>.<lambda> at 0x7fbde97eae18>: default rule\r\nINFO:tensorflow:Entity <function read.<locals>.<lambda> at 0x7fbde97eae18> is not cached for key <code object <lambda> at 0x7fbde9888f60, file \"<command-608347>\", line 138> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7fbdddf7c0b8>, frozenset({'mean', 'var'}))\r\nINFO:tensorflow:Converting <function read.<locals>.<lambda> at 0x7fbde97eae18>\r\nINFO:tensorflow:Error transforming entity <function read.<locals>.<lambda> at 0x7fbde97eae18>\r\nTraceback (most recent call last):\r\n  File \"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/parser.py\", line 78, in parse_entity\r\n    return parse_str(source, preamble_len=len(future_features)), source\r\n  File \"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/parser.py\", line 139, in parse_str\r\n    module_node = gast.parse(src)\r\n  File \"/databricks/python/lib/python3.7/site-packages/gast/gast.py\", line 240, in parse\r\n    return ast_to_gast(_ast.parse(*args, **kwargs))\r\n  File \"/usr/lib/python3.7/ast.py\", line 35, in parse\r\n    return compile(source, filename, mode, PyCF_ONLY_AST)\r\n  File \"<unknown>\", line 4\r\n    .map(lambda x: decode(x, mean, var))\r\n    ^\r\nSyntaxError: invalid syntax\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 322, in convert\r\n    free_nonglobal_var_names)\r\n  File \"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 240, in _convert_with_cache\r\n    entity, program_ctx)\r\n  File \"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 469, in convert_entity_to_ast\r\n    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)\r\n  File \"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 630, in convert_func_to_ast\r\n    node, source = parser.parse_entity(f, future_features=future_features)\r\n  File \"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/parser.py\", line 118, in parse_entity\r\n    return parse_str(source, preamble_len=len(future_features)), source\r\n  File \"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/parser.py\", line 145, in parse_str\r\n    raise ValueError('expected exactly one node node, found {}'.format(nodes))\r\nValueError: expected exactly one node node, found []\r\nWARNING:tensorflow:Entity <function read.<locals>.<lambda> at 0x7fbde97eae18> could not be transformed and will be executed as-is. Please report this to the AutoGraph\r\n team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\r\n[v2_compute_ndcg_geov4.02.ipynb.zip](https://github.com/tensorflow/tensorflow/files/4011226/v2_compute_ndcg_geov4.02.ipynb.zip)", "comments": ["@mpundurs, Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Tensorflow 2.0.0 (Git v2.0.0-rc2-26-g64c3d38), installed from PyPI\r\nAWS i3.8xlarge nodes via Databricks (runtime 6.0)\r\nApache Spark 2.4.3\r\n", "In this case autograph is unable to parse the source code of the lambda at which the error occurs:\r\n\r\n```\r\n.map(lambda x: decode(x, mean, var))\r\n```\r\n\r\nThe reason is a limitation in Python, which doesn't offer enough information to completely delimit the lambda function from its surrounding code. Anyway, the workaround is straightforward - you just need to writhe the lambda in a separate line, like so:\r\n\r\n```\r\ndecode_x = lambda x: decode(x, mean, var)\r\n...\r\n.map(decode_x)\r\n```\r\n\r\nAlternatively, you might be able to use functools.partial - no lambda required:\r\n\r\n```\r\nimport functools\r\n...\r\n.map(functools.partial(decode, feat_mean=mean, feat_var=var))\r\n```\r\n", "@mpundurs,\r\nIs this still an issue? Please take a look at @mdanatg's workaround and let us know if the issue is resolved. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 35503, "title": "[ROCm]  Updating ROCm implementation to use MIOpen Immediate Mode API", "body": "This PR is to update the ROCm implementation (stream executor, conv kernels, XLA conv implementation) to use the MIOpen Immediate Mode API.\r\n\r\nThe changes in this PR, though big, are exclusive to the ROCm implementation, and should not have any impact on the rest of TF.\r\n\r\n------------------\r\n\r\n/cc @whchung @chsigg ", "comments": ["@rthadur, gentle ping\r\n\r\n", "@deven-amd sorry for the delay , waiting for internal approvals. thank you for your patience.", "@deven-amd there are some build failures , can you please check .", "@rthadur , those `Windows Bazel` and `Windows Bazel GPU` failures do not seem related to the changes in this PR.  In the previous CI run,  both the Windows builds had passed and nothing has changed in this PR since then. \r\n\r\nthanks", "@chsigg can you please merge the changes internally. Thank you"]}, {"number": 35502, "title": "GPU Support for Poisson Random Noise Generator in TensorFlow", "body": "I am interested in using Generative Adversarial Network (GAN) to generate synthetic images for algorithm training. Poisson-distributed noise is an important source of noise in images. However, it does not appear that the TensorFlow Poisson noise generator has GPU support. The following code fails to execute:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.Session() as sess:\r\n    with tf.device(\"/gpu:0\"):\r\n        test = sess.run(tf.random.poisson(1.0, [], dtype=tf.float32))\r\nprint(test)\r\n```\r\nwith the error message\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation random_poisson/RandomPoissonV2: node random_poisson/RandomPoissonV2 (defined at /home/kjmiller/anaconda3/envs/tf_clone/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748)  was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0 ]. Make sure the device specification refers to a valid device.\r\n\t [[random_poisson/RandomPoissonV2]]\r\n```\r\nWhen training my model (the code is too much to place here), I get\r\n```\r\nLookupError: No gradient defined for operation 'Generator/RandomPoissonV2' (op type: RandomPoissonV2)\r\n```\r\nI have tried overriding the gradient in the hopes that is the issue, but that did not work. I am pretty should that the second error also has to do with GPU support, as the `lam` argument input to tf.random.poisson is a tensor that lives in GPU memory.\r\n\r\nIt would be a tremendous help if TensorFlow had GPU support for Poisson-distributed noise, as this is a fundamental noise source for any image taken with a camera.", "comments": ["@kjmiller944, Which Tensorflow version are you using.\r\nI tried with Tf 2.0, i didn't receive error message.Please take a look at gist [here](https://colab.sandbox.google.com/gist/gadagashwini/f3e81f4c0436f97b01625569f6a2e74e/untitled330.ipynb). Thanks!", "@gadagashwini thank you. I just realize now that I did not correctly install my TensorFlow 2.0 GPU version. I am still getting the second error message regarding the `ValueError` but I want to try something and get back to you. Thank you for your quick reply.", "@kjmiller944, Any update!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 35501, "title": "cuDNN-based LSTM implementation not used when eager execution is disabled", "body": "**System information**\r\nOS: Debian 10, x64\r\nGPU: GeForce GTX 1060 6GB\r\nPython: 3.7.3\r\nTensorFlow: 2.0.0 installed from source (git tag v2.0.0)\r\nBazel: 0.26.1\r\nGCC: 6.5.0\r\nCUDA: 10.1\r\ncuDNN: 7.6.4\r\n\r\n**Sample code**\r\n```python\r\nimport numpy\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.layers import Input, LSTM\r\n\r\n# uncommenting this causes performance issues\r\n# tf.compat.v1.disable_eager_execution()\r\n\r\ni = Input(shape=(1024, 32))\r\no = LSTM(units=32)(i)\r\nm = Model(i, o)\r\n\r\nm.compile('sgd', 'mse')\r\nm.fit(numpy.zeros((512, 1024, 32)), numpy.zeros((512, 32)))\r\n```\r\n\r\n**Output with eager execution enabled**\r\n```\r\nTrain on 512 samples\r\n2019-12-30 17:06:25.988147: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_2026_2206' and '__inference___backward_cudnn_lstm_with_fallback_2026_2206_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_2818' both implement 'lstm_c56c9d1d-36f6-4e94-9410-ecd020c8700a' but their signatures do not match.\r\n512/512 [==============================] - 2s 3ms/sample - loss: 0.0000e+00\r\n```\r\n\r\n**Output with eager execution disabled**\r\n```\r\nWARNING:tensorflow:From /home/<user>/virtualenvs/tensorflow-2.0.0/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nTrain on 512 samples\r\n512/512 [==============================] - 8s 15ms/sample - loss: 0.0000e+00\r\n```\r\n\r\n**Discussion**\r\nNote how the runtime is very significantly slower when eager execution is disabled. It seems that the cuDNN-based implementation of LSTM is not used whenever eager execution is disabled, as is seen [on this line in the code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent_v2.py#L1066) where `self.could_use_cudnn` ends up being `False`. This seems wrong to me, as there's no reason not to use  cuDNN in that situation.\r\n\r\nThe warning about the skipped optimization is discussed in #30263 and apparently can be ignored, as per qlzh727's comment.", "comments": ["Issue is replicating on colab with Tf 2.0.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/e90360e2ec49aaa0c58a91b9a8154470/untitled332.ipynb). Thanks!", "Hi @jlherren.\r\n\r\nThanks for reporting this issue. In short, this is actually by design that when eager mode is disabled. \r\n\r\nWhen eager mode is disabled, all the code will be run with session, which is same as v1 code. Since the new LSTM implementation uses TF function, under the hood it is a op that invoke a function graph. In the v1 session runtime, the input/output of an op can't be modified between different runs, unless the session is cleared. In LSTM's case, this means the code will break if the model is first invoked with model.predict() and then model.fit(), since predict() will only create the forward path, and fit() will create backward path after that. The backward path will cause the LSTM function op to produce extra output/states for backprop, which violate the immutable input/output rule.\r\n\r\nThis is why we disable the function approach when code will be run under session mode. You can find more details in this change 0cdf2250b7276ff3cd2fad9bd44c917cbb760bd5.\r\n\r\nI am closing this issue as \"working as intended\". Feel free to reopen it if you feel otherwise.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35501\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35501\">No</a>\n", "To avoid confusing users, the documentation for GRU & LSTM should be updated to describe this limitation.\r\nhttps://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/layers/GRU\r\nhttps://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/layers/LSTM\r\n\r\nThe above pages do not list eager-mode as a requirement for using cuDNN. They only mention activation, recurrent_activation etc as if those are the only requirements."]}, {"number": 35500, "title": "A puzzling & fatal error occurred in the tf.matmal()", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\nSystem information\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nno\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04 and win 10\r\n- TensorFlow installed from (source or binary):\r\nInstall in the conda integration environment\r\nconda create -n tf2-gpu tensorflow-gpu=2.0\r\n- TensorFlow version (use command below):\r\ntf-gpu 2.0 stable  &  tf-gpu 2.0 beta\r\n- Python version:\r\n3.6\r\n- CUDA/cuDNN version:\r\nCUDA: 10.0.0130-0\r\ncuDNN: 7.6.5\r\n- GPU model and memory:\r\nGeForce GTX 850M\r\nGeForce RTX 2070S\r\n\r\nYou can collect some of this information using our environment capture\r\nscript\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\" 2. TF 2.0: python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n\r\nDescribe the current behavior & expected behavior\r\n\r\nThe following error occurs when using  tf.matmal() to compute the product of multidimensional tensors on the gpu.\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    \r\n    j = np.random.rand(10, 6, 1130, 16, 8)\r\n    k = np.random.rand(10, 6, 1130, 8, 1)\r\n    # with tf.device(\"CPU:0\"):\r\n    j = tf.cast(j, dtype=tf.float32)\r\n    k = tf.cast(k, dtype=tf.float32)\r\n    \r\n    a = tf.matmul(j, k)[9, 3]\r\n    b = tf.matmul(j[9], k[9])[3]\r\n    c = tf.matmul(j[9, 3], k[9, 3])\r\n    \r\n    print(tf.reduce_all(tf.equal(a, b)))\r\n    print(tf.reduce_all(tf.equal(b, c)))\r\n    \r\n    '''\r\n    tf.Tensor(False, shape=(), dtype=bool)  # The correct output would be True\r\n    tf.Tensor(True, shape=(), dtype=bool)\r\n    '''\r\n\r\nThis error does not occur while using the CPU.\r\n\r\n    ...\r\n    \r\n    with tf.device(\"CPU:0\"):\r\n        j = tf.cast(j, dtype=tf.float32)\r\n        k = tf.cast(k, dtype=tf.float32)\r\n    \r\n        a = tf.matmul(j, k)[9, 3]\r\n        b = tf.matmul(j[9], k[9])[3]\r\n        c = tf.matmul(j[9, 3], k[9, 3])\r\n    \r\n        print(tf.reduce_all(tf.equal(a, b)))\r\n        print(tf.reduce_all(tf.equal(b, c)))\r\n    \r\n    '''\r\n    tf.Tensor(True, shape=(), dtype=bool)\r\n    tf.Tensor(True, shape=(), dtype=bool)\r\n    '''\r\n\r\nThis error will not occur even if you reduce the size of some dimension a bit.\r\n\r\nWe make the following changes:\r\n\r\n    # j = np.random.rand(10, 6, 1130, 16, 8)\r\n    # k = np.random.rand(10, 6, 1130, 8, 1)\r\n    j = np.random.rand(10, 6, 1129, 16, 8)  # 1130 --> 1129\r\n    k = np.random.rand(10, 6, 1129, 8, 1)\r\n\r\nalso use the gpu:\r\n\r\n    j = tf.cast(j, dtype=tf.float32)\r\n    k = tf.cast(k, dtype=tf.float32)\r\n    \r\n    a = tf.matmul(j, k)[9, 3]\r\n    b = tf.matmul(j[9], k[9])[3]\r\n    c = tf.matmul(j[9, 3], k[9, 3])\r\n    \r\n    print(tf.reduce_all(tf.equal(a, b)))\r\n    print(tf.reduce_all(tf.equal(b, c)))\r\n    \r\n    '''\r\n    tf.Tensor(True, shape=(), dtype=bool)\r\n    tf.Tensor(True, shape=(), dtype=bool)\r\n    '''\r\n\r\nI tested it on different GPU and OS , but got the same error.\r\n\r\nWhen errors occur, I have compared the specific differences between the two methods ( a and b), and found that it is not the slight differences that cause the error.\r\n\r\n    print(tf.reduce_sum(a-b))\r\n    '''\r\n    tf.Tensor(-1.3804454e+38, shape=(), dtype=float32)\r\n    '''\r\n", "comments": ["Further, I found that the error should be related to the dimensional size of the matrix\r\n\r\n    offset = 0\r\n    while True:\r\n        j = np.random.rand(*(65530+offset*1 , 16, 8))\r\n        k = np.random.rand(*(65530+offset*1 , 8, 1))\r\n        # with tf.device(\"CPU:0\"):\r\n        j = tf.cast(j, dtype=tf.float32)\r\n        k = tf.cast(k, dtype=tf.float32)\r\n    \r\n        a = tf.matmul(j, k)[-1]\r\n    \r\n        b = tf.matmul(j[-1], k[-1])\r\n        print(offset)\r\n    \r\n        if not tf.reduce_all(tf.equal(a, b)).numpy():\r\n            break\r\n    \r\n        offset += 1\r\n    \r\n    print(65530+offset*1)\r\n    '''\r\n    65536  (is 2^16)\r\n    '''\r\n\r\n\r\n", "Issue is replicating with Tf 2.0.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/a221e9be6aab3be79921b03150442ab8/untitled333.ipynb). Thanks!", "thanks for your replication, this error still exists in colab.\r\n\r\nIs there any update on resolving this error.\r\nThis is important because this error is out of control unless the gpu is not used.", "@alextp Can you take a look at this?", "I just ran against tf-nightly and it's fixed there.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35500\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35500\">No</a>\n", "Well, this bug was also fixed in version 2.1"]}, {"number": 35499, "title": "can\u00b4t train image segmentation with model.fit", "body": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10.0.18362\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary): through pip \r\nTensorFlow version (use command below): 2.0\r\nPython version: 3.7\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: using CPU\r\nGPU model and memory: using CPU\r\n\r\n\r\n\r\nHi there,\r\nI'm just getting started with tensorflow and keras. So far I kind of understand how to do training on images and how to create my network. But I got some issues which I\u00b4m not sure how to solve, and could be due to some bug:\r\n\r\nBasically I want to do image segmentation. I\u00b4m first trying to use data that is already in numpy. So I have numpy array of (10,180,180,1) (so 10 cases of images with 180x180 and 1 channel) for both the images and its respective segmentations. I have a simple network defined as:\r\n\r\ninputs = tf.keras.layers.Input((180,180,1))\r\nc1 = tf.keras.layers.SeparableConv2D(15,(5,5), use_bias=True, activation='relu',kernel_initializer='he_normal',padding='same')(inputs) \r\n\r\nc2 = tf.keras.layers.SeparableConv2D(20,(3,3),use_bias=True, activation='relu',kernel_initializer='he_normal',padding='same')(c1)\r\n\r\noutputs = tf.keras.layers.Conv2D(1, (1, 1), activation='softmax')(c2)\r\n\r\nThen I try to train it using:\r\n`model.fit(training_ims,training_labels,batch_size=2,epochs=1)`\r\n\r\nThis gives this error:\r\n\r\n> Train on 10 samples\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 2, in <module>\r\n  File \"C:\\Users\\Manuel_LincBiotech\\Desktop\\LincBiotech\\python programs\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\Manuel_LincBiotech\\Desktop\\LincBiotech\\python programs\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)\r\n  File \"C:\\Users\\Manuel_LincBiotech\\Desktop\\LincBiotech\\python programs\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"C:\\Users\\Manuel_LincBiotech\\Desktop\\LincBiotech\\python programs\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"C:\\Users\\Manuel_LincBiotech\\Desktop\\LincBiotech\\python programs\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\Manuel_LincBiotech\\Desktop\\LincBiotech\\python programs\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 496, in _call\r\n    raise ValueError(\"Creating variables on a non-first call to a function\"\r\nValueError: Creating variables on a non-first call to a function decorated with tf.function.\r\n 2/10 [=====>........................] - ETA: 1s>>> training_ims\r\n\r\nI\u00b4m quite new with this so I\u00b4m not remotely sure what is happening, tried looking for something similar before but didn\u00b4t find anything.\r\n\r\nThanks in advance", "comments": ["extra note:\r\nIf I suppress the batch_size input I get this different error:\r\n`Train on 10 samples\r\n10/10 [==============================] - 0s 1ms/sample\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"C:\\Users\\Manuel_LincBiotech\\Desktop\\LincBiotech\\python programs\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\Manuel_LincBiotech\\Desktop\\LincBiotech\\python programs\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)\r\n  File \"C:\\Users\\Manuel_LincBiotech\\Desktop\\LincBiotech\\python programs\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"C:\\Users\\Manuel_LincBiotech\\Desktop\\LincBiotech\\python programs\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"C:\\Users\\Manuel_LincBiotech\\Desktop\\LincBiotech\\python programs\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\Manuel_LincBiotech\\Desktop\\LincBiotech\\python programs\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 487, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\nTypeError: 'NoneType' object is not callable\r\n`", "@manuel-lincbiotech, Please provide the complete code to reproduce the reported issue. Thanks!", "The complete code was just that:\r\ninputs = tf.keras.layers.Input((180,180,1))\r\nc1 = tf.keras.layers.SeparableConv2D(15,(5,5), use_bias=True, activation='relu',kernel_initializer='he_normal',padding='same')(inputs)\r\n\r\nc2 = tf.keras.layers.SeparableConv2D(20,(3,3),use_bias=True, activation='relu',kernel_initializer='he_normal',padding='same')(c1)\r\n\r\noutputs = tf.keras.layers.Conv2D(1, (1, 1), activation='softmax')(c2)\r\n\r\nThen I try to train it using:\r\nmodel.fit(training_ims,training_labels,batch_size=2,epochs=1)\r\n\r\n\r\nHowever I think I discovered by myself the problem. It seems I needed to specify the output differently, after changing the the output to have the number of channels as the number of labels it worked. Seems we can\u00b4t use one channel with multiple label values.", "@manuel-lincbiotech, Tried replicating the reported issue but looks like incomplete code, please take a look at colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/4cf42bafafdf0127596c3095fef725af/untitled14.ipynb) and provide more information to analyze the issue. Thanks!", "@manuel-lincbiotech, any update!", "@gadagashwini Sorry for de delay. Yes, as I commented at the last message the error was effectively coming from the fact that the training_labels were not hot-encoded, after changing that it worked just fine. Originally the labels had labels raging from 0 to 3, to make it work I just used tf.keras.utils.to_categorical(training_labels) and then it worked. Wasn\u00b4t a problem with the code, but with the way I fed the data. ", "@manuel-lincbiotech, \r\nIssue is resolved. Are you happy to close this issue. Thanks!", "Closing this, looks like issue is resolved. Please feel free to open if still issue persists.  Thanks"]}, {"number": 35498, "title": "Remove stale forward compatibility horizons in unittests", "body": "The forward compatibility horizons in the unit tests are expired, thus they can safely be removed.", "comments": ["f75c37faf347dd926cf0e999f03dbd7818a77811 implemented almost the same changes. I rebased."]}, {"number": 35497, "title": "[ROCm] Updating Dockerfile.rocm to use ROCm 3.0", "body": "/cc @whchung @chsigg @sunway513 @parallelo ", "comments": []}, {"number": 35496, "title": "why tf do not work", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please reopen filling in issue template. As it is now, it is useless as we don't know what issue you have."]}, {"number": 35495, "title": "Consuming sets of files :There are many datasets distributed as a set of files, where each file is an example.", "body": "I test  and run code from: https://www.tensorflow.org/guide/data#applying_arbitrary_python_logic_with_tfpy_func\r\nimport pickle as pi\r\nimport os\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport pathlib\r\nflowers_root = tf.keras.utils.get_file(\r\n    'flower_photos',\r\n    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\r\n    untar=True)\r\nflowers_root = pathlib.Path(flowers_root)\r\nfor item in flowers_root.glob(\"*\"):\r\n  print(item.name)\r\n#dir_training_data = '/laptrinh python/ket_qua'\r\n#flowers_root = pathlib.Path('/laptrinh python/test_file_anh/training')\r\nlist_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))\r\n\r\nfor f in list_ds.take(5):\r\n  print(f.numpy())\r\n\r\ndef process_path(file_path):\r\n  label = tf.strings.split(file_path, '/')[-2]\r\n  return tf.io.read_file(file_path), label\r\n\r\nlabeled_ds = list_ds.map(process_path)  \r\nfor image_raw, label_text in labeled_ds.take(1):\r\n  print(repr(image_raw.numpy()[:100]))\r\n  print()\r\n  print(label_text.numpy())\r\n\r\nAnd happen the error as following:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"F:\\laptrinh python\\laptrinh_python_chinh_thuc\\model_training.py\", line 33, in <module>\r\n    for image_raw, label_text in labeled_ds.take(1):\r\n\r\n  File \"C:\\Users\\Xuan Hau\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\", line 622, in __next__\r\n    return self.next()\r\n\r\n  File \"C:\\Users\\Xuan Hau\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\", line 666, in next\r\n    return self._next_internal()\r\n\r\n  File \"C:\\Users\\Xuan Hau\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\", line 651, in _next_internal\r\n    output_shapes=self._flat_output_shapes)\r\n\r\n  File \"C:\\Users\\Xuan Hau\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\", line 2673, in iterator_get_next_sync\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n\r\n  File \"<string>\", line 3, in raise_from\r\n\r\nInvalidArgumentError: {{function_node __inference_Dataset_map_process_path_106}} slice index -1 of dimension 0 out of bounds.\r\n\t [[{{node strided_slice}}]] [Op:IteratorGetNextSync]", "comments": ["@nguyenxuanhau ,\r\nI was able to execute the code without any issue's as per the tutorial, please share us the gist of colab with error being faced. Thanks!", "@nguyenxuanhau ,\r\nAny update on the issue ?thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 35494, "title": "distribut data io problem and I want to know way", "body": "hi, I find some bug. Code is\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nif __name__ == '__main__':\r\n    def Gen():\r\n        for i in range(10):\r\n            yield(i,2,3,4,5,6,7,8)\r\n\r\n    dataset = tf.data.Dataset.from_generator(Gen, output_types=(tf.float32,tf.float32,tf.int32,tf.int32,tf.int32,tf.float32,tf.int32,tf.int32),output_shapes=None,args=None)\r\n    for one_batch in dataset:\r\n        print('one batch',one_batch)\r\n\r\n    print(\"******end**********\")\r\n\r\n    num_gpu=1\r\n    devices = ['/device:GPU:{}'.format(i) for i in range(num_gpu)]\r\n    strategy = tf.distribute.MirroredStrategy(devices)\r\n\r\n    input_context = tf.distribute.InputContext(num_input_pipelines=1,\r\n            input_pipeline_id=0,\r\n            num_replicas_in_sync=1)\r\n\r\n    with strategy.scope():\r\n        def dataset_fn(input_context):\r\n            dataset = tf.data.Dataset.from_generator(Gen, output_types=(tf.float32,tf.float32,tf.int32,tf.int32,tf.int32,tf.float32,tf.int32,tf.int32),output_shapes=None,args=None)\r\n            return dataset.shard(\r\n                    input_context.num_input_pipelines, input_context.input_pipeline_id)\r\n\r\n        train_dist_dataset = strategy.experimental_distribute_datasets_from_function(dataset_fn)\r\n\r\n        for one_batch  in train_dist_dataset:\r\n            print('****one batch*******',one_batch)\r\n\r\nThe code can be run, but in distribut \"for one_batch  in train_dist_dataset:\" at the end batch will be error.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/python35/lib/python3.5/pdb.py\", line 1665, in main\r\n    pdb._runscript(mainpyfile)\r\n  File \"/usr/local/python35/lib/python3.5/pdb.py\", line 1546, in _runscript\r\n    self.run(statement)\r\n  File \"/usr/local/python35/lib/python3.5/bdb.py\", line 431, in run\r\n    exec(cmd, globals, locals)\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/search/speech/hubo/git/tf-code-acoustics/tf2.0-model/io_test.py\", line 45, in <module>\r\n    for one_batch  in train_dist_dataset:\r\n  File \"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py\", line 275, in __next__\r\n    return self.get_next()\r\n  File \"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py\", line 304, in get_next\r\n    global_has_value, replicas = _get_next_as_optional(self, self._strategy)\r\n  File \"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py\", line 200, in _get_next_as_optional\r\n    iterator._iterators[i].get_next_as_list(new_name))  # pylint: disable=protected-access\r\n  File \"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py\", line 878, in get_next_as_list\r\n    lambda: _dummy_tensor_fn(data.value_structure))\r\n  File \"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 1204, in cond\r\n    result = false_fn()\r\n  File \"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py\", line 878, in <lambda>\r\n    lambda: _dummy_tensor_fn(data.value_structure))\r\n  File \"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py\", line 801, in _dummy_tensor_fn\r\n    result.append(create_dummy_tensor(feature_shape, feature_type))\r\n  File \"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py\", line 784, in create_dummy_tensor\r\n    for dim in feature_shape.dims:\r\nTypeError: 'NoneType' object is not iterable\r\nUncaught exception. Entering post mortem debugging\r\nRunning 'cont' or 'step' will restart the program\r\n\r\nI want to know why.", "comments": ["datemoon@ I am unable to reproduce the error using the tf-nightly pip package. Which version of TF are you using? Can you share a colab where this error is produced?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35494\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35494\">No</a>\n"]}, {"number": 35493, "title": "tf.recompute_grad() throws dimension mismatched error when concatenating tensors with different number of channels ", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0 / 7.6\r\n- GPU model and memory: GTX 1080 / 8GB\r\n\r\n**Describe the current behavior**\r\n\r\nMy model is a segmentation model with DenseNet like structure. There are many concatenate operations between the encoder and decoder tensors. I want to recompute the gradients on these concatenate layers during backpropagation to limit the amount of GPU memory usage. A similar approach can be found here : https://github.com/joeyearsley/efficient_densenet_tensorflow.  I tried to use tf.recompute_grad() on a wrapper function which has a concatenate layer inside but it would raise an error when the channel dimensions of input tensors are not matched.\r\n\r\n**Describe the expected behavior**\r\nThe concatenate layer should not raise an error when concatenating inputs with different number of channels.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nhttps://colab.research.google.com/drive/1d7zSGbYmocnupUpDIJLnlXt-3vH5bi83#scrollTo=xcPT-MWZAuvK&uniqifier=1\r\n\r\n**Other info / logs**\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1609   try:\r\n-> 1610     c_op = c_api.TF_FinishOperation(op_desc)\r\n   1611   except errors.InvalidArgumentError as e:\r\n\r\nInvalidArgumentError: Dimension 3 in both shapes must be equal, but are 16 and 32. Shapes are [?,30,30,16] and [?,30,30,32].\r\n\tFrom merging shape 0 with other shapes. for 'packed_7' (op: 'Pack') with input shapes: [?,30,30,16], [?,30,30,32].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n16 frames\r\n<ipython-input-24-689917ca63e3> in <module>()\r\n----> 1 model = test_model()\r\n      2 history = model.fit(train_images, train_labels, epochs=10, \r\n      3                     validation_data=(test_images, test_labels))\r\n\r\n<ipython-input-23-caa5b725d514> in test_model()\r\n      4     x_list.append(layers.Conv2D(16, (3,3), activation='relu')(x_in))\r\n      5     x_list.append(layers.Conv2D(32, (3,3), activation='relu')(x_in))\r\n----> 6     x = efficient_concat(x_list)\r\n      7     x = layers.Flatten()(x)\r\n      8     x = layers.Dense(10, activation='softmax')(x)\r\n\r\n<ipython-input-3-fa43d4abcee2> in efficient_concat(input_list)\r\n      4         return x\r\n      5     wraper = tf.recompute_grad(wraper)\r\n----> 6     return wraper(input_list)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/custom_gradient.py in decorated(*args, **kwargs)\r\n    164     \"\"\"Decorated function with custom gradient.\"\"\"\r\n    165     if context.executing_eagerly():\r\n--> 166       return _eager_mode_decorator(f, *args, **kwargs)\r\n    167     else:\r\n    168       return _graph_mode_decorator(f, *args, **kwargs)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/custom_gradient.py in _eager_mode_decorator(f, *args, **kwargs)\r\n    333 \r\n    334   input_tensors = [ops.convert_to_tensor(x) for x\r\n--> 335                    in list(args) + list(variables)]\r\n    336   arg_count = len(args)\r\n    337   def actual_grad_fn(*result_grads):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/custom_gradient.py in <listcomp>(.0)\r\n    332   flat_result = [gen_array_ops.identity(x) for x in flat_result]\r\n    333 \r\n--> 334   input_tensors = [ops.convert_to_tensor(x) for x\r\n    335                    in list(args) + list(variables)]\r\n    336   arg_count = len(args)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)\r\n   1182   preferred_dtype = deprecation.deprecated_argument_lookup(\r\n   1183       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\r\n-> 1184   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n   1185 \r\n   1186 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)\r\n   1240       name=name,\r\n   1241       preferred_dtype=dtype_hint,\r\n-> 1242       as_ref=False)\r\n   1243 \r\n   1244 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)\r\n   1294 \r\n   1295     if ret is None:\r\n-> 1296       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1297 \r\n   1298     if ret is NotImplemented:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py in _autopacking_conversion_function(v, dtype, name, as_ref)\r\n   1276   elif dtype != inferred_dtype:\r\n   1277     v = nest.map_structure(_cast_nested_seqs_to_dtype(dtype), v)\r\n-> 1278   return _autopacking_helper(v, dtype, name or \"packed\")\r\n   1279 \r\n   1280 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py in _autopacking_helper(list_or_tuple, dtype, name)\r\n   1182     # checking.\r\n   1183     if all(ops.is_dense_tensor_like(elem) for elem in list_or_tuple):\r\n-> 1184       return gen_array_ops.pack(list_or_tuple, name=name)\r\n   1185   must_pack = False\r\n   1186   converted_elems = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_array_ops.py in pack(values, axis, name)\r\n   6302   axis = _execute.make_int(axis, \"axis\")\r\n   6303   _, _, _op = _op_def_lib._apply_op_helper(\r\n-> 6304         \"Pack\", values=values, axis=axis, name=name)\r\n   6305   _result = _op.outputs[:]\r\n   6306   _inputs_flat = _op.inputs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    791         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\r\n    792                          input_types=input_types, attrs=attr_protos,\r\n--> 793                          op_def=op_def)\r\n    794       return output_structure, op_def.is_stateful, op\r\n    795 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in create_op(***failed resolving arguments***)\r\n    546     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\r\n    547         op_type, inputs, dtypes, input_types, name, attrs, op_def,\r\n--> 548         compute_device)\r\n    549 \r\n    550   def capture(self, tensor, name=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\r\n   3427           input_types=input_types,\r\n   3428           original_op=self._default_original_op,\r\n-> 3429           op_def=op_def)\r\n   3430       self._create_op_helper(ret, compute_device=compute_device)\r\n   3431     return ret\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   1771           op_def, inputs, node_def.attr)\r\n   1772       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\r\n-> 1773                                 control_input_ops)\r\n   1774     # pylint: enable=protected-access\r\n   1775 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1611   except errors.InvalidArgumentError as e:\r\n   1612     # Convert to ValueError for backwards compatibility.\r\n-> 1613     raise ValueError(str(e))\r\n   1614 \r\n   1615   return c_op\r\n\r\nValueError: Dimension 3 in both shapes must be equal, but are 16 and 32. Shapes are [?,30,30,16] and [?,30,30,32].\r\n\tFrom merging shape 0 with other shapes. for 'packed_7' (op: 'Pack') with input shapes: [?,30,30,16], [?,30,30,32].", "comments": ["Issue replicating when tried replicating for code given in the gist of colab for `TF-2.0` and `tf-nightly.` Thanks!", "@ChunHsinWang Correct me if I am wrong but The weights of the model depend on the number of channels. Changing channels is changing weights. Changing weights is having a completely new model.", "@gowthamkpr thanks for the reply, but concatenate layer does not change the number of channels, it merges channels from the list of input tensors, so the total number of weights in the model does not change.  To my understanding, TensorFlow's concatenate layer will always allocate new memory and copy from the input tensors, and the memory consumption grows quadratically I think. What I'm trying to achieve is to use tf.recompute_grad for having concatenate layers to use a shared memory space, so the output of every concatenate layer does not allocate new memory. During the backward pass, these outputs will have to be recalculated for calculating the gradients. The following figure from this paper https://arxiv.org/abs/1707.06990 demonstrates the process. \r\n![figure3](https://user-images.githubusercontent.com/20900499/71760456-fe36e900-2ef8-11ea-8b07-a57e14e127d8.png)\r\n", "Issue replicating when tried replicating for code given in the gist of colab for TF 2.2.0-rc3.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/6baee6feddff579b09550fef5cb6bb4a/untitled782.ipynb). Thanks!", "@ChunHsinWang \r\n\r\nI am not seeing any issue with nightly version (`2.3.0-dev20200610`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/95ec843df15f1fdfc3a88992db8f7dec/untitled56.ipynb).Please, verify once and close the issue.Thanks!", "The issue seems to be fixed, so I will close it. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35493\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35493\">No</a>\n"]}, {"number": 35492, "title": "Tensorflow \"devel-gpu-py3\" docker image is stale, has not been updated for 2 months", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Docker image\r\n- TensorFlow version: 2.0\r\n- Python version: 3\r\n- Installed using virtualenv? pip? conda?: Docker\r\n- Bazel version (if compiling from source): default from the docker image\r\n- GCC/Compiler version (if compiling from source): default from the docker image\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Nvidia gt750m\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI have been trying to create the TensorFlow-GPU wheel for my legacy hardware for which I need to build Tensorflow 2.0 with CUDA compute 3.0. The docker image for building a TensorFlow-GPU wheel is stale and has not been updated for [2 months](https://hub.docker.com/layers/tensorflow/tensorflow/devel-gpu-py3/images/sha256-2c17ae6134fc0abbc8128c62d74efd4d9312a04f92483c8f31279f630d5a5800). \r\n\r\nFor the sake of reducing redundancy, I'll link the previous issue:\r\nhttps://github.com/tensorflow/tensorflow/issues/34719\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Thanks for isolating this into a new issue. Our previous expert on building in Docker departed a couple months ago, just before TensorFlow started using CUDA 10.1, and I've been trying to get the images fixed since then.\r\n\r\nWith recent commits to the dockerfiles/ directory, I believe the build issues are now resolved, and our CI should pick them up and deploy by December 31.", "Hey @angerson seems like they're still not up", "I just tested the latest-devel-gpu image from DockerHub, aka 2.1.0-devel-gpu, and ran into an error that when these Dockerfiles are updated should be addressed.  The fastest way to reproduce the error is to simply try running nvidia-smi from the command line.  When I ran it, it reports \"CUDA Version: ERR!\".  Running that from the latest-gpu image it returns \"CUDA Version: 10.1\".  Diff'ing the following two files points to an LD_LIBRARY_PATH issue including \"/usr/local/cuda/lib64/stubs\" in the devel image:\r\n- tensorflow/tensorflow/tools/dockerfiles/partials/ubuntu/devel-nvidia.partial.Dockerfile\r\n- tensorflow/tensorflow/tools/dockerfiles/partials/ubuntu/nvidia.partial.Dockerfile\r\n\r\nIf I export LD_LIBRARY_PATH in the devel image like in the non-devel image, then nvidia-smi reports the correct CUDA Version.", "Hi @angerson,\r\n\r\nI forgot to mention that I had to change \"ENV TF_NEED_TENSORRT 1\" to \"ENV TF_NEED_TENSORRT 0\" in order to successfully bazel build inside latest-devel-gpu.  Sorry, I didn't hunt down why that was necessary.\r\n\r\nOh, the above LD_LIBRARY_PATH issue didn't affect bazel build, just when I went to test `python -c \"import tensorflow as tf ; physical_devices = tf.config.list_physical_devices('GPU') ; print('Num GPUs:', len(physical_devices))\"`, which showed more details regarding the missing libraries and hinted to me to investigate LD_LIBRARY_PATH.", "@settle thanks for reporting that; you saved me some time when investigating https://github.com/tensorflow/tensorflow/issues/36974. Since this current issue is resolved (containers are now available), I'll close it in favor of the other issue to track the CUDA problem. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35492\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35492\">No</a>\n"]}, {"number": 35491, "title": "Error: Not in GZIP format", "body": "Getting below exception while following \"https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android\"\r\n\r\n\r\norg.gradle.api.InvalidUserDataException: Cannot expand TAR 'D:\\WorkSpaces\\ML_WorkSpace\\TF_Android\\app\\build\\intermediates\\mobilenet_v1_1.0_224.tgz'.\r\n\tat org.gradle.api.internal.file.archive.TarFileTree.cannotExpand(TarFileTree.java:133)\r\n\t... 56 more\r\nCaused by: org.gradle.api.resources.ResourceException: Could not read D:\\WorkSpaces\\TF_Android\\app\\build\\intermediates\\mobilenet_v1_1.0_224.tgz.\r\n\tat org.gradle.internal.resource.ResourceExceptions.readFailed(ResourceExceptions.java:36)\r\n\tat org.gradle.api.internal.file.archive.compression.GzipArchiver.read(GzipArchiver.java:64)\r\n\tat org.gradle.api.internal.file.MaybeCompressedFileResource.read(MaybeCompressedFileResource.java:55)\r\n\tat org.gradle.api.internal.file.archive.TarFileTree.visit(TarFileTree.java:78)\r\n\t... 107 more\r\nCaused by: java.util.zip.ZipException: Not in GZIP format\r\n\tat java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:165)\r\n\tat org.gradle.api.internal.file.archive.compression.GzipArchiver.read(GzipArchiver.java:61)\r\n\t... 109 more\r\n\r\n**Don't know, what could be the problem?**", "comments": ["Possibly a Windows-specific issue. @lintian06 could you take a look?", "@rajputashu Could you please let us know if this issue still persists ? If it it resolved then please feel free to move this issue to close status ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35491\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35491\">No</a>\n"]}, {"number": 35490, "title": "Why doesn't ```tf.keras.losses.binary_crossentropy``` raise error", "body": "```\r\nx = np.arange(10,dtype=np.float64).reshape(10,1)\r\n#x.shape = (10,1)\r\n\r\ny = np.arange(10,dtype=np.float64)\r\n#y.shape = (10,)\r\n\r\ntf.keras.losses.binary_crossentropy(y_true=y, y_pred=x)\r\n#this line does't raise error\r\n\r\ntf.keras.metrics.BinaryAccuracy()(y_true=y, y_pred=x)\r\n#this line neither\r\n\r\ntf.keras.metrics.Precision()(y_true=y, y_pred=x)\r\n#this line raise an error\r\n```\r\nI think ```binary_crossentropy``` and ```BinaryAccuracy```  should raise an ValueError like ```tf.keras.metrics.Precision```: \r\n```\r\nValueError: Shapes (128, 1) and (128,) are incompatible\r\n```", "comments": ["@DachuanZhao ,\r\nI was able to replicate the issue with TF-1.5 and TF-2.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/febed084ad734e809b857a89164015ae/35490.ipynb) of colab.Thanks!", "Precision metric should actually not be raising an error. Have a change out to fix this.", "This is fixed now in : https://github.com/tensorflow/tensorflow/commit/ba8a0c934147fcf2a879f349677fc11676c73835#diff-1d3c0e76cc08b7d6e2e3a6ab89965a5c", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35490\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35490\">No</a>\n"]}, {"number": 35488, "title": "`files", "body": "the function `files_io.get_matching_files` says it takes a \"filepath\", but actually it takes a glob\r\n\r\nthis means that if you save your checkpoints to a folder like `x=[abc]`, then you can't load the previous checkpoint using something like:\r\n\r\n```\r\ndef load_checkpoint(sess, checkpoint_path):\r\n  saver = tf.train.Saver(tf.global_variables())\r\n  ckpt = tf.train.get_checkpoint_state(checkpoint_path)\r\n  tf.logging.info('Loading model %s.', ckpt.model_checkpoint_path)\r\n  saver.restore(sess, ckpt.model_checkpoint_path)\r\n```\r\n\r\nwhere `checkpoint_path=\"./logs/x=[abc]\"`.", "comments": []}, {"number": 35487, "title": "How to Write Input_Signature's for function's and model's names when using tf.fucntion?", "body": "Hi,\r\n\r\nI want to decorate tf.function unto my training function\r\n```\r\n@tf.function\r\ndef trainOneSample(data, model, optimizer, lossFunc, accFunc):\r\n...\r\n```\r\nwhere data are a pair of 5D arrays with float32 and uint8 types (for input and label), model is the name of my NN model, optimiser is the name of an optimisation object, lossFunc and accFunc are names of custom functions. How can I decorate them in tf.function by Input_Signature? I cannot find out any solution in tf's tutorial or guide.", "comments": ["@yourtheron Please take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/28165) and let me know if it helps. Thanks!", "@gowthamkpr Got it. So it seems that I do not have to provide any input signature for optimizer, lossFunc, or accFunc here when defining and decorating trainOneSample(...), is that correct? \r\nA by-product question: My trainOneSample(...) function will execute my model's call(...) method, and of course, other operations such as lossFunc(...) and accFunc(...). So in order to let tf create a graph and concrete functions, shall I decorate tf.function unto all of those functions as well as provide input signatures to each one of them or decorate trainOneSample(...)  only? All of inputs to those functions are tf tensors.", "@yourtheron Look at this specific [issue](https://github.com/tensorflow/tensorflow/issues/28165) on how they used `@tf.function` This should give you clarity on how to use it in your case.", "@yourtheron Can I close this issue as it is not a bug. Thanks!", "@gowthamkpr Sorry for the late reply. Sure, thanks for your help!"]}]