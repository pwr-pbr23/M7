[{"number": 10012, "title": "Session vs DeprecatedSession", "body": "Hi,\r\n\r\nI'm wondering as to the main differences between the new sessions in the C API and the deprecated sessions. I noticed that [the Python API is using the deprecated sessions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/session.py#L562) and since I am using the new sessions, I want to make sure I understand the differences correctly. I list here the differences I believe exist and I hope that someone could verify and/or correct my thoughts:\r\n- `TF_ExtendGraph` is not necessary with the new sessions. The session will always be aware of changes in the underlying graph.\r\n- `TF_Reset` still makes sense with the new sessions and has the same functionality. If that is indeed the case, maybe it should be moved outside the deprecated session API region of the C API header file.\r\n- Most of the `TF_SessionOptions` still apply, but some of them do not make sense anymore (e.g., the `ConfigProto.graph_options.infer_shapes` option). Could someone please provide a list of which options have been deprecated?\r\n- I do not really understand the whole dealing with session handles. I believe it's not necessary to deal with them with the new session API as the partial run handle is not provided as a feed/fetch value, but as a separate argument. Is that true? If so, should I completely ignore them when dealing with the sessions? That would mean that I never need to use the ops defined in `session_ops`. Is that correct?\r\n\r\nThank you,\r\nAnthony", "comments": ["You're correct, TF_ExtendGraph isn't necessary or supported with new sessions.\r\n\r\nI'll look into the TF_SessionOptions and mark any deprecated ones as appropriate (and respond here).\r\n\r\nI think @suharshs can comment on TF_Reset and session handles.", "TF_Reset's semantics are currently very confusing. We are looking into alternatives for resource container management that solve what TF_Reset was made for in a clearer way. So, I think its advisable to keep TF_Reset only in the deprecated session currently. Let me know if you need TF_Reset in the new session api and we can discuss your use case (I'd be very interested).\r\n\r\nThe word 'handle' is a bit overused currently :)\r\n\r\npartial_run_handle: This handle corresponds to an sequence of partial run calls. So the user can call PartialRunSetup, which returns a new partial_run_handle. This is then passed by the user to PartialRun calls. (this is the 'handle' in the _do_run function in the python file you linked).\r\n\r\nfeed_handle and fetch_handles: I am not an expert on these, but IIUC these correspond to persistent tensor handles. (https://www.tensorflow.org/versions/r0.11/api_docs/python/session_ops/tensor_handle_operations_). These are experimental so it may not be a big deal if your client doesn't take care of them.\r\n\r\nDoes that make sense?", "I just looked at the TF_SessionOptions, and the following likely won't be useful to you:\r\n\r\n- `ConfigProto.placement_period` This is only used internally\r\n- `GraphOptions.infer_shape` Mostly useful in Session._extend_graph in session.py (a deprecated API concept), but also annotates the NodeDefs returned by Session.graph_def (not sure how useful this feature is)\r\n- `GraphOptions.timeline_step` Experimental and only used internally.\r\n\r\nHope that helps. Thanks for bringing this up, and please close the issue if you don't have further questions/comments. Also FYI, we're working on moving the Python API to the new session API.", "@skye That's great! Thanks for looking this up! :)\r\n\r\n@suharshs I was actually asking about TF_Reset, not because I have a usage scenario for it, but because I wanted to understand its semantics and when it might be useful. Regarding tensor handles, reading the documentation you referenced makes me think that, given that I share the same memory buffer when I feed/fetch tensors to sessions (as opposed to copying them), that is unnecessary. Does that sound reasonable?", "TF_Reset is used to clear resource [containers](https://www.tensorflow.org/api_docs/python/tf/container). This can be used to clear state in a setup that, for example, runs many graphs and doesn't want to run out of memory. Here is a detailed [comment](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/session.h#L189) that explains the current semantics, and also explains why it is quite confusing currently.\r\n\r\nMy understanding is that persistent tensors go a step further that what you suggest. They store the underlying tensors in the session state, such that they live on the underlying device memory. They don't have to be fed and fetched every time. They can be thought of \"in-place\" tensors across steps.\r\n\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this due to inactivity, please reopen if there is still something pending on this issue."]}, {"number": 10011, "title": "Cherry-pick RNN migration CLs into r1.2", "body": "", "comments": []}, {"number": 10010, "title": "[Feature]: Bazel - Building TensorFlow with Polly-enabled-LLVM.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: ('v1.0.0-1783-g4c3bb1a', '1.0.0')\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: ```bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package```\r\n\r\n#### TensorFlow build: \r\nBuilt from source. \r\n```\r\n$ git rev-parse HEAD\r\na33022c1470ce1334766b0cad38d9e91c17a2e5d\r\n```\r\n### Description:\r\nThis is a feature request. \r\nAs part of my Google Summer of Code project, I am trying to build TensorFlow with Polly-enabled LLVM. To do this, I have been trying to port Opt and Polly to bazel. I tried placing this dummy rule at the end of the [llvm.BUILD](https://github.com/tensorflow/tensorflow/blob/master/third_party/llvm/llvm.BUILD) file. \r\n\r\n```\r\ncc_library(\r\n    name = \"opt\",\r\n    srcs = glob([\r\n        \"tools/opt/*.c\",\r\n        \"tools/opt/*.cpp\",\r\n        \"tools/opt/*.h\",\r\n    ]),     \r\n    hdrs = glob([\r\n        \"tools/opt/*.h\",\r\n    ]),     \r\n    copts = [\"-Iexternal/llvm/tools/opt\",\"-Iexternal/llvm/include\"],\r\n    deps = [\r\n        \":intrinsics_gen\",\r\n    ],\r\n)\r\n```\r\n\r\nBut this does not seem to work. All the other targets defined in the file are building:\r\n\r\n```\r\ntensorflow/bazel-bin/external/llvm/_objs# ls\r\naarch64_asm_printer   arm_asm_printer   bit_reader            execution_engine  mc                 objc_arc              powerpc_info   transform_utils   x86_utils\r\naarch64_code_gen      arm_code_gen      bit_writer            global_i_sel      mc_disassembler    object                profile_data   vectorize\r\naarch64_desc          arm_desc          code_gen              inst_combine      mc_parser          orc_jit               runtime_dyld   x86_asm_printer\r\naarch64_disassembler  arm_disassembler  core                  instrumentation   nvptx_asm_printer  powerpc_asm_printer   scalar         x86_code_gen\r\naarch64_info          arm_info          debug_info_code_view  ipo               nvptx_code_gen     powerpc_code_gen      selection_dag  x86_desc\r\naarch64_utils         asm_parser        debug_info_msf        ir_reader         nvptx_desc         powerpc_desc          support        x86_disassembler\r\nanalysis              asm_printer       demangle              linker            nvptx_info         powerpc_disassembler  target         x86_info\r\n\r\n```\r\n\r\nI know that I have to define opt as a dependency in my build configuration. I can't see why this doesn't do the same.\r\n\r\nThanks.\r\n", "comments": ["@phawkins can you comment or redirect to someone who can? Thanks!", "@skye @phawkins \r\n\r\nSo I think I found it, I had to define the dependency in  ```tensorflow/compiler/xla/service/BUILD```.  \r\n\r\nBut I have a question: I am trying to integrate ```opt``` into the ```cpu_ir_emitter``` of xla. Any pointers as to where I should be putting it in the pipeline?\r\n\r\nThanks."]}, {"number": 10009, "title": "Problems with `raw_rnn` with dynamic dimensions", "body": "I am trying to implement the [pointing softmax](https://arxiv.org/abs/1603.08148) in TF 1.1 and I am using `raw_rnn` but I get an error. Since is hard to explain the whole thing, here is how to reproduce it:\r\n\r\n```bash\r\n$ git clone https://github.com/petrux/LiTeFlow.git\r\n$ cd LiTeFlow\r\n$ git checkout broken\r\n$ ./bin/py3venv.sh\r\n$ source .py3venv/bin/activate\r\n$ python3 -m unittest liteflow.tests.test_layers._TestSmoke\r\n```\r\nthe error that I have is:\r\n\r\n```bash\r\npetrux@orion:~/Projects/LiTeFlow$ python3 -m unittest liteflow.tests.test_layers._TestSmoke\r\n./usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:212: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\r\n  arg_spec = inspect.getargspec(func)\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/python/keras/backend.py:3593: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/petrux/.keras/keras.json' mode='r' encoding='UTF-8'>\r\n  _config = json.load(open(_config_path))\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/labeled_tensor/python/ops/_typecheck.py:233: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\r\n  spec = inspect.getargspec(f)\r\nelements_finished: Tensor(\"PointingDecoder_1_1/rnn/GreaterEqual:0\", shape=(?,), dtype=bool)\r\nnext_cell_input: Tensor(\"PointingDecoder_1_1/rnn/concat:0\", shape=(?, ?), dtype=float32)\r\nnext_cell_state: Tensor(\"GRUCellZeroState/zeros:0\", shape=(?, 5), dtype=float32)\r\nemit_output: Tensor(\"zeros:0\", shape=(?, ?), dtype=float32)\r\nnext_loop_state: (<tf.Tensor 'PointingDecoder_1_1/rnn/LocationSoftmax_1/softmax/truediv:0' shape=(?, ?) dtype=float32>, <tf.Tensor 'PointingDecoder_1_1/rnn/LocationSoftmax_1/Sum:0' shape=(?, 4) dtype=float32>)\r\n\r\nE\r\n======================================================================\r\nERROR: test_smoke (liteflow.tests.test_layers._TestSmoke)\r\nBuild a pointer decoder and test that it works.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1356, in zeros\r\n    shape = tensor_shape.as_shape(shape)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 800, in as_shape\r\n    return TensorShape(shape)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 436, in __init__\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 436, in <listcomp>\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 378, in as_dimension\r\n    return Dimension(value)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 33, in __init__\r\n    self._value = int(value)\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'Tensor'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/petrux/Projects/LiTeFlow/liteflow/tests/test_layers.py\", line 518, in test_smoke\r\n    output = decoder()\r\n  File \"/home/petrux/Projects/LiTeFlow/liteflow/layers.py\", line 616, in __call__\r\n    return super(PointingDecoder, self).__call__()\r\n  File \"/home/petrux/Projects/LiTeFlow/liteflow/layers.py\", line 147, in __call__\r\n    result = self._call_helper(*args, **kwargs)\r\n  File \"/home/petrux/Projects/LiTeFlow/liteflow/layers.py\", line 610, in _call_helper\r\n    outputs_ta, _, _ = tf.nn.raw_rnn(self._decoder_cell, self._loop_fn)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\", line 965, in raw_rnn\r\n    for size_i, dtype_i in zip(flat_emit_size, flat_emit_dtypes)]\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\", line 965, in <listcomp>\r\n    for size_i, dtype_i in zip(flat_emit_size, flat_emit_dtypes)]\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1359, in zeros\r\n    shape = ops.convert_to_tensor(shape, dtype=dtypes.int32, name=\"shape\")\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 639, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 704, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 905, in _autopacking_conversion_function\r\n    return _autopacking_helper(v, inferred_dtype, name or \"packed\")\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 867, in _autopacking_helper\r\n    constant_op.constant(elem, dtype=dtype, name=str(i)))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 102, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\", line 360, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.430s\r\n\r\nFAILED (errors=1)\r\n```  \r\n  \r\n**EDIT** a working implementation is [here](https://github.com/petrux/LiTeFlow/tree/pointing-decoder/liteflow).", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10008, "title": "fix a bug in generate-pc.sh", "body": "I don't know how line 30 in the script managed to make its way into the last pull request.\r\nI corrected the bug and forced the user to provide a version to generate the pkg-config file.\r\n\r\nSorry about that...", "comments": ["Can one of the admins verify this patch?", "@arrufat Thanks for the quick fix.\r\n@tensorflow-jenkins test this please"]}, {"number": 10007, "title": "Branch 155393864", "body": "", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 10006, "title": "Tensorflow cifar 10 example memory leak", "body": "### Describe the problem\r\nI am new to Tensorflow. I tried to run the cifar10 examples from here: https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10\r\n\r\nI didn't make any changes to the code, I just tried to run it on multiple GPUs. I am trying to use 6 GPUs and I am allocating 10 GB of RAM for my job but after a few minutes my jobs getting failed due to memory limit. Allocating more memory does not help, it just delays the error. I tried for up to 40GB of memory.\r\n\r\nHere is my system info:\r\n\r\n> == cat /etc/issue =============================================== Linux mmmdgx01 4.4.0-45-generic #66~14.04.1-Ubuntu SMP Wed Oct 19 15:05:38 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux DGX_OTA_VERSION=2.0.5 VERSION=\"14.04.5 LTS, Trusty Tahr\" VERSION_ID=\"14.04\"\r\n> \r\n> == are we in docker ============================================= No\r\n> \r\n> == compiler ===================================================== c++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4 Copyright (C) 2013 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n> \r\n> == uname -a ===================================================== Linux mmmdgx01 4.4.0-45-generic #66~14.04.1-Ubuntu SMP Wed Oct 19 15:05:38 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n> \r\n> == check pips =================================================== numpy (1.11.1) protobuf (3.2.0) tensorflow (1.1.0rc1)\r\n> \r\n> == check for virtualenv ========================================= False\r\n> \r\n> == tensorflow import ============================================ tf.VERSION = 1.1.0-rc1 tf.GIT_VERSION = v1.1.0-rc1-272-gf77f19b tf.COMPILER_VERSION = v1.1.0-rc1-272-gf77f19b Sanity check: array(1, dtype=int32)\r\n> \r\n> == env ========================================================== LD_LIBRARY_PATH /opt/sw/cuda/8.0/lib64/:/project/DGX/cuda/lib64/:/opt/sw/cuda/8.0/extras/CUPTI/lib64/:/project/DGX/lib DYLD_LIBRARY_PATH /project/DGX/torch/install/lib:/project/torch7new/install/lib:\r\n> \r\n> == nvidia-smi =================================================== Fri May 12 15:46:50 2017 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 375.20 Driver Version: 375.20\r\n> | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-SXM2... On | 0000:06:00.0 Off |\r\n> 0 | | N/A 34C P0 42W / 300W | 0MiB / 16308MiB | 0%\r\n> Default | +-------------------------------+----------------------+----------------------+ | 1 Tesla P100-SXM2... On | 0000:07:00.0 Off |\r\n> 0 | | N/A 32C P0 32W / 300W | 0MiB / 16308MiB | 0%\r\n> Default | +-------------------------------+----------------------+----------------------+ | 2 Tesla P100-SXM2... On | 0000:0A:00.0 Off |\r\n> 0 | | N/A 34C P0 33W / 300W | 0MiB / 16308MiB | 0%\r\n> Default | +-------------------------------+----------------------+----------------------+ | 3 Tesla P100-SXM2... On | 0000:0B:00.0 Off |\r\n> 0 | | N/A 33C P0 32W / 300W | 0MiB / 16308MiB | 0%\r\n> Default | +-------------------------------+----------------------+----------------------+ | 4 Tesla P100-SXM2... On | 0000:85:00.0 Off |\r\n> 0 | | N/A 33C P0 30W / 300W | 0MiB / 16308MiB | 0%\r\n> Default | +-------------------------------+----------------------+----------------------+ | 5 Tesla P100-SXM2... On | 0000:86:00.0 Off |\r\n> 0 | | N/A 33C P0 33W / 300W | 0MiB / 16308MiB | 0%\r\n> Default | +-------------------------------+----------------------+----------------------+ | 6 Tesla P100-SXM2... On | 0000:89:00.0 Off |\r\n> 0 | | N/A 31C P0 32W / 300W | 0MiB / 16308MiB | 0%\r\n> Default | +-------------------------------+----------------------+----------------------+ | 7 Tesla P100-SXM2... On | 0000:8A:00.0 Off |\r\n> 0 | | N/A 35C P0 32W / 300W | 0MiB / 16308MiB | 0%\r\n> Default | +-------------------------------+----------------------+----------------------+\r\n> \r\n> +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name\r\n> Usage | |=============================================================================| | No running processes found\r\n> | +-----------------------------------------------------------------------------+\r\n> \r\n> == cuda libs ===================================================\r\n> \r\n\r\n### TensorFlow version\r\n\r\n('v1.1.0-rc1-272-gf77f19b', '1.1.0-rc1')\r\n\r\n### Source code / logs\r\nHere is my job submission script:\r\n\r\n> #! /bin/bash\r\n> #SBATCH --account=AI\r\n> #SBATCH --time=167:00:00\r\n> #SBATCH --nodes=1\r\n> #SBATCH --ntasks-per-node=20\r\n> #SBATCH -J TFImgNet\r\n> #SBATCH -e tf.err\r\n> #SBATCH -o tf.log\r\n> #SBATCH --mem=10960\r\n> #SBATCH --gres=gpu:6\r\n> cpath=$(pwd)\r\n> cd ~\r\n> source .bashrc\r\n> cd $cpath\r\n> which python\r\n> python cifar10_multi_gpu_train.py --num_gpus 6\r\n\r\n### Here is the error:\r\n\r\n> 2017-05-12 15:14:07.162709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 2 3 4 5\r\n> 2017-05-12 15:14:07.162718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y Y Y Y N\r\n> 2017-05-12 15:14:07.162721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y Y Y N Y\r\n> 2017-05-12 15:14:07.162724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 2:   Y Y Y Y N N\r\n> 2017-05-12 15:14:07.162727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 3:   Y Y Y Y N N\r\n> 2017-05-12 15:14:07.162729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 4:   Y N N N Y Y\r\n> 2017-05-12 15:14:07.162732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 5:   N Y N N Y Y\r\n> 2017-05-12 15:14:07.162743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:06:00.0)\r\n> 2017-05-12 15:14:07.162747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:07:00.0)\r\n> 2017-05-12 15:14:07.162751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:0a:00.0)\r\n> 2017-05-12 15:14:07.162754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:0b:00.0)\r\n> 2017-05-12 15:14:07.162756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:4) -> (device: 4, name: Tesla P100-SXM2-16GB, pci bus id: 0000:85:00.0)\r\n> 2017-05-12 15:14:07.162759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:5) -> (device: 5, name: Tesla P100-SXM2-16GB, pci bus id: 0000:86:00.0)\r\n> slurmstepd: error: Job 1313520 exceeded memory limit (11240536 > 11223040), being killed\r\n> slurmstepd: error: Exceeded job memory limit\r\n> slurmstepd: error: *** JOB 1313520 ON mmmdgx01 CANCELLED AT 2017-05-12T15:28:58 ***\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I already asked it on stackoverflow.com:\r\n[http://stackoverflow.com/questions/43946948/tensorflow-cifar-10-example-memory-leak]()\r\n\r\nApparently it's a bug!"]}, {"number": 10005, "title": "Py_func call slows down scipy", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04.4 and MacOS 10.12.4 are affected\r\n- **TensorFlow installed from (source or binary)**: tried both, reproduces on both\r\n- **TensorFlow version (use command below)**: 1.0.1 and 1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: \r\n- **GPU model and memory**: both on Tesla K80 and on CPU version\r\n- **Exact command to reproduce**: https://nbviewer.jupyter.org/urls/dl.dropbox.com/s/z19wv70ogo15xg0/eigh%20speed%20bug.ipynb\r\n\r\nOutput of the enviromental script: https://pastebin.com/qdi8tJRV\r\n\r\n### Describe the problem\r\nCalling np.lianlg.svd via tf.py_fun forces scipy.linalg.eigh to become slower for some reason (4x slower on a server and 1.5x slower on my laptop). Here is the line that causes problems:\r\n```\r\nret = tf.py_func(np.linalg.svd, [np.random.randn(2, 300)], [tf.float64, tf.float64, tf.float64])\r\n%timeit sess.run(ret)\r\n```\r\nBefore this line scipy.linalg.eigh used CPU for 2400% and worked in 13ms, after it it uses CPU for 600% and works in 43ms.\r\n\r\n### Source code / logs\r\nhttps://nbviewer.jupyter.org/urls/dl.dropbox.com/s/z19wv70ogo15xg0/eigh%20speed%20bug.ipynb\r\n", "comments": ["I couldn't reproduce it\r\n\r\nI added the following to top of https://github.com/yaroslavvb/stuff/blob/master/svd_benchmark.py and ran it as `svd_benchmark.py eigh` and observed the same time.\r\n\r\n```\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\nret = tf.py_func(np.linalg.svd, [np.random.randn(2, 300)], [tf.float64, tf.float64, tf.float64])\r\nfor i in range(10): sess.run(ret)\r\n```\r\n\r\n`py_func` runs the function provided in the main Python interpreter, so a simpler repro would be to run `np.linalg.svd` directly rather than through py_func. \r\n\r\nAlso numpy backend may matter here, you can check with `python -c 'import numpy; numpy.__config__.show()'`", "OK, I can reproduce it. It seems some weird interaction between TensorFlow and MKL.\r\nCalling linalg.svd from inside py_func makes subsequent calls to eigh about 10% slower when you use MKL, but no effect in default BLAS\r\n\r\nhttps://github.com/yaroslavvb/stuff/blob/master/github_pyfunc_slowness.py\r\n\r\nNot sure why that would happen...perhaps the svd call in MKL ends up changing some defaults used by eigh? cc @vivek-rane since he seems to know about MKL\r\n\r\n"]}, {"number": 10004, "title": "In reader.py, line 17 throws error on python 3.5. The decode phrase should be removed, and then it works", "body": "In reader.py, line 17 throws error on python 3.5. The decode phrase should be removed, and then it works", "comments": ["Are you talking about tensorflow/contrib/saved_model/python/saved_model/reader.py? I suggest trying the saved_model functionality that's now in core TensorFlow: https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/saved_model/README.md", "I am talking about this one:\r\n\r\nhttps://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/reader.py\r\n", "Line 17 is a comment, do you mean that needs to be removed? Can you post the error you're seeing?", "Sorry, it's line 30", "Please provide details about what platform you are using  (operating system, architecture), and the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new). This seems like it might be platform-specific. We'll need to be able to repro the problems ourselves to fix.", "Hi Skye! Hope all is well.\r\n\r\nI'm talking about python 3.5, and TF 1.1\r\n\r\nThe code outputs the following error: AttributeError: 'str' object has no attribute 'decode'\r\nat the line:   return f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\r\n\r\nThat's because the string is already decoded.\r\n\r\nIf you change the line to: return f.read().replace(\"\\n\", \"<eos>\").split()   - all works well.\r\n", "@nadavb, could you prepare a pull-request that  makes this change (in a way that doesn't break python2 of course)\r\n", "That's the pull request - https://github.com/tensorflow/models/pull/1512\r\n\r\nIt asks me to sign some CLA agreement, which I will skip for now.", "Marking contributions welcome while we are waiting for the PR.", "We do not see this file in the models repo anymore , so this issue is not relevant ,closing this issue. Thank you ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/10004\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/10004\">No</a>\n"]}, {"number": 10003, "title": "[feature] sparse_multiply", "body": "Hi guys,\r\n\r\nWe'd like to ask if it's possible to add sparse_multiply and sparse_multiply_sparse as a sparse equivalent to existing tf.multiply but for SparseTensor arguments and both Tensor and SparseTensor result.\r\n\r\nThank you,\r\n\r\nJoanna", "comments": ["Thanks for the suggestion @asiakaczmar! Marking this as \"Contributions Welcome\" for now.", "Is the request for the equivalent of\r\n```python\r\nfrom functools import partial\r\nsparse_multiply = partial(tf.multiply, a_is_sparse=True)\r\n```\r\nand similarly for `sparse_multiply_sparse`?", "Thank you for replying! As far as we know tf.multiply requires both a and b to be Tensors and we would like it to accept SparseTensor (possibly as one or both arguments, we could work with either). We couldn't find a_is_sparse flag in tf.multiply documentation (nor source, but it could be hidden somewhere). Also sparse_multiply would return Tensor as result, but sparse_multiply_sparse would return SparseTensor (similarly to existing tf.sparse_reduce_sum and tf.sparse_reduce_sum_sparse).", "I would like to have this feature as well, with the caveat that sparse multiplied element-wise with dense is always sparse, and so should be returned as such.  This can make a huge difference in some use cases, for example if you wanted to multiply every row in a huge sparse matrix by the same dense vector element-wise.  If that operation returned a dense tensor there could easily be memory issues.  Scipy recently switched the result of sparse x dense element-wise to return sparse by default because a lot of people complained about this.", "I will take a crack at this feature.", "Hi, so by reading up the code, I've found out that as @asiakaczmar pointed out, there is no `a_is_sparse` option in the implementation of multiply as in [link](https://github.com/tensorflow/tensorflow/blob/cf18c6d384a96a53b448bd51a90c117af0ed7c96/tensorflow/python/ops/math_ops.py#L304). However, there is indeed the option of specifying `a_is_sparse` option in the implementation of `matmul` as in [link](https://github.com/tensorflow/tensorflow/blob/cf18c6d384a96a53b448bd51a90c117af0ed7c96/tensorflow/python/ops/math_ops.py#L1686). I might be missing something, @cfperez can you make sure if `matmul` is what you mean instead of `multiply` in your comment?\r\n\r\nAs I read through relevant sections of code related to sparse tensor multiplication, I see quite a bit of confusion surrounding what the word `sparse` actually mean both in the code and in people's comments. A comment by @concretevitamin in #3246 suggests that `a_is_sparse` or `b_is_sparse` does not specify that the input tensor is a SparseTensor; rather, it provides algorithmic hints suggesting that the input dense tensor contains large portion of zeros. Moreoever, the `sparse_matmul` function seems to follow suite on this line of thinking and in [here](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/math_ops.py#L1821), the implementation suggests that it essentially performs the same operation as `multiply` do with sparse hints turned on. That is to say, the behavior described in #3246 is expected in order to remain consistent about what the word `sparse` means in the context of tensorflow. \r\n\r\nHowever, the problem is that what `sparse` means somehow fluctuates depending on which function you are in. For instance, the documentation indicates that `sparse_transpose` and `sparse_reduce_sum` take SparseTensor as inputs, which seems to contradicts the definition of `sparse` as seen in `multiply` or `sparse_matmul`. So, in light of the splitting definition of `sparse`, I would like to ask for opinions on how would sparse-sparse matrix multiplication fit into the picture. \r\n\r\nThe most logical thing to do, in my opinion, is to implement in `math_ops.py` another function called `sparse_tensor_sparse_tensor_matmul` (the naming follows from `sparse_tensor_dense_matmul`). Also, I'm not sure if the current `sparse_matmul` function is a placeholder for future implementation or not; therefore, although it is the most intuitive to change the `sparse_matmul` function to handle sparse matrix sparse matrix multiplication, it might cause compatibility issue."]}, {"number": 10002, "title": "[feature] sparse_reduce_max and sparse_reduce_max_sparse ", "body": "Hi guys,\r\n\r\nCurrently we work on sparse tensors, and we are missing some sparse operations. Is there any way you could add sparse_reduce_max and sparse_reduce_max_sparse operations as functions equivalent to existing tf.sparse_reduce_sum and tf.sparse_reduce_sum_sparse but with max operation as reducing function?\r\n\r\nThank you,\r\n\r\nJoanna", "comments": ["FYI @concretevitamin @jhseu ", "I don't think we have immediate plans to implement these - happy to review PRs!  \r\n\r\nIn `sparse_reduce_sum_op.cc`, most logic can be factored out, and only [the aggregate function \r\n .sum() here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse_reduce_sum_op.cc#L198) needs to be changed, I think.", "Created a PR #10056 to support the feature. Please take a look.", "I think, this issue can be closed", "Yep", "Hi,\r\nIs it tricky to implement the gradient for these kernels ?\r\nCheers,\r\nMaxime"]}, {"number": 10001, "title": "Segmentation fault on quit() tensorflow-ppc64", "body": "### System information\r\n- **Fedora 24 4.7.4-200.fc24.ppc64 #1 SMP Thu Sep 22 17:40:37 UTC 2016 ppc64 ppc64 ppc64 GNU/Linux**:\r\n- **TensorFlow installed from source**\r\n- **TensorFlow version:  1.1.0-rc1**\r\n- **Bazel version: 0.4.5-2017-05-12**\r\n- **Exact command to reproduce**:\r\n\r\n1. `python`\r\n2. `import tensorflow as tf`\r\n3. `hello = tf.constant('Hello, TensorFlow!')`\r\n4. `sess = tf.Session()`\r\n5. `print(sess.run(hello))`\r\n6. `quit()`\r\n\r\n### Describe the problem\r\nEven with the simple above example, when I import tensorflow in python I get a segmentation fault at the moment of terminating python; all commands before `quit()` are executed correctly\r\n\r\n### Source code / logs\r\n`Thread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\n0x00003fffae6b8d78 in .std::_Function_handler<void (tensorflow::OpKernel*), tensorflow::DirectSession::GetOrCreateExecutors(tensorflow::thread::ThreadPool*, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*)::{lambda(tensorflow::OpKernel*)#2}>::_M_invoke(std::_Any_data const&, tensorflow::OpKernel*&&) ()\r\n   from /home/shady/dleoni/python_build/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so`\r\n\r\n", "comments": ["@npanpaliya any idea?\r\n\r\n@davideleoni90 could you try with asan perhaps? Any chance to get line numbers?", "I checked it on a system \"Linux f9b14d7ac714 4.4.0-75-generic #96-Ubuntu SMP Thu Apr 20 09:55:30 UTC 2017 ppc64le ppc64le ppc64le GNU/Linux\" with tensorflow 1.1.0 and quit() didn't give segmentation fault. Could you please try upgrading Tensorflow to 1.1.0 (final release)?", "@npanpaliya I had installed the last version from this fork\r\n\r\n[https://github.com/PPC64/tensorflow](https://github.com/PPC64/tensorflow)\r\n\r\nI have just tried with final release (1.1.0) from\r\n\r\n[https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow)\r\n\r\nbut I get the following error during the build process:\r\n\r\n`ERROR: /home/shady/dleoni/bazel_output_base_final/external/protobuf/BUILD:609:1: C++ compilation of rule '@protobuf//:python/google/protobuf/internal/_api_implementation.so' failed: gcc failed: error executing command \r\n  (cd /home/shady/dleoni/bazel_output_base_final/execroot/tensorflow_final && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/home/shady/dleoni/python_build/lib/engines:/home/shady/dleoni/openssl_build/lib:/usr/lib64/openssl:/home/shady/dleoni/python_build/lib \\\r\n    PATH=/home/shady/dleoni/bazel/output:/home/shady/dleoni/python_build/bin:/home/shady/dleoni/python_build/include/python2.7:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/shady/.local/bin:/home/shady/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/shady/dleoni/python_build/bin/python \\\r\n    PYTHON_LIB_PATH=/home/shady/dleoni/python_build/lib/python2.7/site-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++0x' '-march=native' '-D_GLIBCXX_USE_CXX11_ABI=0' -MD -MF bazel-out/local-opt/bin/external/protobuf/_objs/python/google/protobuf/internal/_api_implementation.so/external/protobuf/python/google/protobuf/internal/api_implementation.pic.d '-frandom-seed=bazel-out/local-opt/bin/external/protobuf/_objs/python/google/protobuf/internal/_api_implementation.so/external/protobuf/python/google/protobuf/internal/api_implementation.pic.o' -fPIC -iquote external/protobuf -iquote bazel-out/local-opt/genfiles/external/protobuf -iquote external/local_config_python -iquote bazel-out/local-opt/genfiles/external/local_config_python -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -isystem external/local_config_python/python_include -isystem bazel-out/local-opt/genfiles/external/local_config_python/python_include -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -DPYTHON_PROTO2_CPP_IMPL_V2 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/protobuf/python/google/protobuf/internal/api_implementation.cc -o bazel-out/local-opt/bin/external/protobuf/_objs/python/google/protobuf/internal/_api_implementation.so/external/protobuf/python/google/protobuf/internal/api_implementation.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ngcc: error: unrecognized command line option '-march=native'; did you mean '-mcpu=native'?\r\ngcc: error: unrecognized command line option '-march=native'; did you mean '-mcpu=native'?\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build`\r\n\r\n@drpngx I have never used asan: can you please give me some indication on how to debug with asan?\r\nThe version of my gcc compiler is\r\n\r\n`gcc version 6.2.1 20160916 (Red Hat 6.2.1-2) (GCC) `\r\n\r\nThank you both", "This build error is because of gcc version being used for building. We use gcc 5.4.0 and tensorflow works perfectly with that version. \r\nIf you want to try with gcc 6, then for above mentioned error, try changing \"default_cc_opt_flags\" in tensorflow/configure script to use -mcpu instead of -march, as the error suggests.", "I replaced `march` with `mcpu` and the building process went forward, but not much, and I got the following error:\r\n\r\n`ERROR: /home/shady/dleoni/tensorflow_final/tensorflow/tensorboard/components/tf_graph_common_d3v4/BUILD:26:1: Executing genrule //tensorflow/tensorboard/components/tf_graph_common_d3v4:ts failed: bash failed: error executing command \r\n  (cd /home/shady/dleoni/bazel_output_base_final/execroot/tensorflow_final && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/home/shady/dleoni/python_build/lib/engines:/home/shady/dleoni/openssl_build/lib:/usr/lib64/openssl:/home/shady/dleoni/python_build/lib \\\r\n    PATH=/home/shady/dleoni/bazel/output:/home/shady/dleoni/python_build/bin:/home/shady/dleoni/python_build/include/python2.7:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/shady/.local/bin:/home/shady/bin \\\r\n    PYTHON_BIN_PATH=/home/shady/dleoni/python_build/bin/python \\\r\n    PYTHON_LIB_PATH=/home/shady/dleoni/python_build/lib/python2.7/site-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/genfiles/external/com_microsoft_typescript/tsc.sh --inlineSourceMap --inlineSources --noResolve --declaration --outDir bazel-out/local-opt/genfiles/tensorflow/tensorboard/components/tf_graph_common_d3v4 external/com_microsoft_typescript/lib.es6.d.ts bazel-out/local-opt/genfiles/tensorflow/tensorboard/components/tf_imports_d3v4/d3.d.ts external/org_definitelytyped/lodash.d.ts external/org_definitelytyped/polymer.d.ts external/org_definitelytyped/webcomponents.js.d.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/annotation.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/colors.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/common.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/contextmenu.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/edge.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/externs.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/graph.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/hierarchy.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/layout.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/minimap.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/node.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/parser.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/proto.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/render.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/scene.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/template.ts tensorflow/tensorboard/components/tf_graph_common_d3v4/util.ts'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 126.\r\nbazel-out/host/genfiles/external/com_microsoft_typescript/tsc.sh: line 6: /home/shady/dleoni/bazel_output_base_final/execroot/tensorflow_final/external/org_nodejs/bin/node: cannot execute binary file: Exec format error\r\nbazel-out/host/genfiles/external/com_microsoft_typescript/tsc.sh: line 6: /home/shady/dleoni/bazel_output_base_final/execroot/tensorflow_final/external/org_nodejs/bin/node: Success\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 28.696s, Critical Path: 23.67s`\r\n\r\n\r\nThis is the output from `bazel version`:\r\n\r\n`Build label: 0.4.5-2017-05-12`\r\n", "@davideleoni90 , could you tell me the build steps that you are executing with configure options? Because I've never seen this error.", "@npanpaliya Here are the steps:\r\n\r\n1. `mkdir tensorflow_build`\r\n2. `./configure --prefix=/home/shady/dleoni/tensorflow_build`\r\n3. `mkdir bazel_output_base`\r\n4. `mkdir bazel_output_user_root`\r\n5. `bazel --output_base=/home/shady/dleoni/bazel_output_base --output_user_root=/home/shady/dleoni/bazel_output_user_root build --config=opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package`", "Uninstalling the pip package and recompiling with bazel issuing the following command\r\n\r\n`bazel --output_base=/home/shady/dleoni/bazel_output_base --output_user_root=/home/shady/dleoni/bazel_output_user_root build -c opt -c dbg --strip=never --verbose_failures --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nmagically solved the problem.\r\n\r\nThank you all for your support\r\n"]}, {"number": 10000, "title": "Fixed some misspells.", "body": "I found 3 misspells.", "comments": ["Issue/PR #10000 \ud83c\udf89"]}, {"number": 9999, "title": "Directories with spaces in their names don't compile due to missing quotes in shell scripts", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX Sierra, Macbook Pro 2015\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: v1.0.0-65-g4763edf-dirty 1.0.1\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nI was building tensorflow for iOS and as a part of this, tried running the `build_all_ios.sh` but it threw some errors. The error is mainly because there are parts in shell script that either attempt to create or move to a directory that is incorrectly referenced. In my case, it was because of spaces in directory names. This is true even for the 'compile_ios_protobuf.sh' script (and harder to detect because of the very verbose log). \r\n\r\nI was able to get-around this by modifying both of these shell scripts to include double quotes around \r\nlines that referenced directory paths. \r\n\r\n### Source code / logs\r\n\r\nFew examples lines here:\r\n\r\nLine 28 (build_all_ios.sh): 'cd ${SCRIPT_DIR}/../../../' (notice that this will ignore spaces in the directory path, if they exist\r\n\r\nLine 35 (compile_ios_protobuf.sh): 'mkdir -p ${LIBDIR}' (this will similarly create a directory at an unexpected path and will not complete compilation)", "comments": ["Hi @shre81, thank you for reaching out .Could you please provide any additional information that might allow us to reproduce this problem? Exact command lines would be very helpful.", "Sure. \r\n\r\nHere's what I tried to run:\r\n\r\n`sudo ./tensorflow/contrib/makefile/compile_ios_protobuf.sh `\r\n\r\nand this is the terminal log output:\r\n\r\n`/bin/sh: /Users/shreyasgopinath/Documents/Bluth: is a directory\r\nmake[2]: *** [google/protobuf/compiler/js/well_known_types_embed.cc] Error 126\r\nmake[2]: *** Waiting for unfinished jobs....`\r\n\r\nIt is attempting to look for a directory called 'Bluth' while it should be looking for 'Bluth Company'. I believe it is the space in the name that's causing the error.\r\n\r\nDoes this help? Or, will additional lines before and after this error help?\r\n\r\nThanks\r\n", "Adding quotes to shell scripts is something that's as easy to forget as it is to fix. We welcome any contributions improving this.", "@jart I have some time to fix this. Is replacing any detected whitespace with '\\\\' a good solution ? Or should I aim for something better. Please let me know.", "On second thought, this might not be a good issue to fix. It would be extremely difficult to maintain our build system(s) if we couldn't assume paths don't have spaces. Please build TensorFlow under a path that doesn't have spaces. If you find any stuff in our source tree that has spaces, let us know.", "You are having this problem due to the white space in the directory as:(machine learning)\r\ntensorboard --logdir=foo:E:\\projects\\machine learning\\Cat-and-Dog-novice\\log\r\n\r\nThis can be fixed using the \"\" for the directory as:\r\ntensorboard --logdir=foo:\"E:\\projects\\machine learning\\Cat-and-Dog-novice\\log\"\r\n\r\nHope this helps :100: "]}, {"number": 9998, "title": "How to build tensorflow for mips64el", "body": "The recommended way to build TensorFlow from source is using the Bazel open-source build system.\r\nHowerver, bazel depends `protobuf,grpc-java,osdetector` etc.  protocbuf can build  successfully for mips64el. But i  build grpc-java  failed. details see [#3012](https://github.com/grpc/grpc-java/issues/3012). i refer [#2022](https://github.com/grpc/grpc-java/issues/2202). but osdetector-gradle-plugin occur error.\r\nCould anybody give me some help for building tensorflow for mips64el arch? ", "comments": ["You could try the CMake build instead of using bazel: https://github.com/tensorflow/tensorflow/tree/r1.2/tensorflow/contrib/cmake", "ok,  thanks. I have a try.", "@skye i refer [https://github.com/tensorflow/tensorflow/tree/r1.2/tensorflow/contrib/cmake](https://github.com/tensorflow/tensorflow/tree/r1.2/tensorflow/contrib/cmake), dependencies included.\r\nHowever, i exec `tensorflow/tools/ci_build/ci_build.sh CMAKE tensorflow/tools/ci_build/builds/cmake.sh` for building tensorflow, output logs as follow,\r\n<pre>\r\n[root@localhost tensorflow]# tensorflow/tools/ci_build/ci_build.sh CMAKE tensorflow/tools/ci_build/builds/cmake.sh\r\nWORKSPACE: /home/loongson/tensorflow\r\nCI_DOCKER_EXTRA_PARAMS: \r\nCOMMAND: tensorflow/tools/ci_build/builds/cmake.sh\r\nCI_COMMAND_PREFIX: \r\nCONTAINER_TYPE: cmake\r\nBUILD_TAG: tf_ci\r\n  (docker container name will be tf_ci.cmake)\r\n\r\nBuilding container (tf_ci.cmake)...\r\nSending build context to Docker daemon 304.1 kB\r\nStep 1 : FROM ubuntu:16.04\r\nTrying to pull repository docker.io/library/ubuntu ... \r\nsha256:382452f82a8bbd34443b2c727650af46aced0f94a44463c62a9848133ecb1aa8: Pulling from docker.io/library/ubuntu\r\nb6f892c0043b: Pull complete \r\n55010f332b04: Pull complete \r\n2955fb827c94: Pull complete \r\n3deef3fcbd30: Pull complete \r\ncf9722e506aa: Pull complete \r\nDigest: sha256:382452f82a8bbd34443b2c727650af46aced0f94a44463c62a9848133ecb1aa8\r\nStatus: Downloaded newer image for docker.io/ubuntu:16.04\r\n ---> ebcd9d4fca80\r\nStep 2 : MAINTAINER Shanqing Cai <cais@google.com>\r\n ---> Running in 5539f888554c\r\n ---> b47c31a23a17\r\nRemoving intermediate container 5539f888554c\r\nStep 3 : COPY install/*.sh /install/\r\n ---> 1c16e7a094c4\r\nRemoving intermediate container 555cd53000c4\r\nStep 4 : RUN /install/install_bootstrap_deb_packages.sh\r\n ---> Running in ddd33994a3bf\r\npanic: standard_init_linux.go:175: exec user process caused \"exec format error\" [recovered]\r\n\tpanic: standard_init_linux.go:175: exec user process caused \"exec format error\"\r\n\r\ngoroutine 1 [running, locked to thread]:\r\npanic(0x1203d0060, 0xc420149ea0)\r\n\t/usr/lib/golang/src/runtime/panic.go:500 +0x4c4\r\n... ...\r\n</pre>\r\nAs we knows, docker is used for testing, but our os is fedora not ubuntu. It occur error. I try to modify `Dockerfile` or shut down the docker test. Could you give me some ideas ?", "You'll probably have to dig into the cmake files (which I'm not super familiar with), as we only test them on Windows 10 and Ubuntu 14.04. You can also try posting on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). Sorry I can't be more helpful; MIPS isn't an architecture we support or test.", "Thanks for your attention. @skye ", "@skye  i build bazel successful. Then, i build tensorflow getting next error.\r\n<pre>\r\n[xzy@localhost tensorflow]$ bazel build --copt=-march=native -c opt //tensorflow/tools/pip_package:build_pip_package\r\nExtracting Bazel installation...\r\n............................................\r\nWARNING: /home/xzy/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.\r\nWARNING: /home/xzy/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.\r\nINFO: Found 1 target...\r\nERROR: /home/xzy/.cache/bazel/_bazel_xzy/c01fa588a238f7be12b727d5cfbf1a03/external/io_bazel_rules_closure/java/io/bazel/rules/closure/webfiles/server/BUILD:48:1: \r\nerror executing shell command: 'JAR='external/local_jdk/bin/jar' OUTPUT='bazel-out/host/bin/external/io_bazel_rules_closure/java/io/bazel/rules/closure/webfiles/server/libbuild_info_java_proto_srcjar.srcjar' PROTO_COMPILER='bazel...' failed: bash failed: \r\nerror executing command /bin/bash -c ... (remaining 1 argument(s) skipped): \r\ncom.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nbazel-out/host/bin/external/io_bazel_rules_closure/closure/private/gensrcjar: line 75: bazel-out/host/genfiles/external/io_bazel_rules_closure/third_party/protobuf/protoc: cannot execute binary file: Exec format error\r\ngensrcjar: proto_compiler failed\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 507.821s, Critical Path: 30.98s\r\n\r\n\r\n[xzy@localhost tensorflow]$ protoc --version\r\nlibprotoc 3.2.0\r\n[xzy@localhost tensorflow]$ bazel version\r\nBuild label: 0.5.0-2017-06-01 (@2e9fff0)\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: thur may 1 01:57:44 2017 (1496282264)\r\nBuild timestamp: 1496282264\r\nBuild timestamp as int: 1496282264\r\n</pre>", "`vim bazel-out/host/bin/external/io_bazel_rules_closure/closure/private/gensrcjar +69`. After it,adding next  sentences can fix upwards error.\r\n<pre>\r\nexport PROTO_COMPILER = /path/to/protoc\r\nexport GRPC_JAVA_PLUGIN = /path/to/protoc-gen-grpc-java\r\n</pre>\r\n\r\nThen i get a new error `Executing genrule //tensorflow/tensorboard/components/tf_tensorboard:ts failed: bash failed: error executing command`\r\nHow should i do next?\r\n<pre>\r\nERROR: /home/xzy/tensorflow/tensorflow/tensorboard/components/tf_tensorboard/BUILD:54:1: Executing genrule //tensorflow/tensorboard/components/tf_tensorboard:ts failed: bash failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_xzy/c01fa588a238f7be12b727d5cfbf1a03/execroot/tensorflow && \\\r\n  exec env - \\\r\n    PATH=/opt/j2sdk-image/bin:/usr/local/bin/:/usr/bin:/opt/j2sdk-image/bin:/usr/local/bin/:/usr/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/home/xzy/.local/bin:/home/xzy/bin \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/lib/python2.7/site-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/genfiles/external/com_microsoft_typescript/tsc.sh --inlineSourceMap --inlineSources --noResolve --declaration --module es6 --outDir bazel-out/local-opt/genfiles/tensorflow/tensorboard/components/tf_tensorboard external/com_microsoft_typescript/lib.es6.d.ts tensorflow/tensorboard/components/tf_tensorboard/autoReloadBehavior.ts'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 126.\r\nbazel-out/host/genfiles/external/com_microsoft_typescript/tsc.sh: line 6: /root/.cache/bazel/_bazel_xzy/c01fa588a238f7be12b727d5cfbf1a03/execroot/tensorflow/external/org_nodejs/bin/node: cannot execute binary file: Exec format error\r\nbazel-out/host/genfiles/external/com_microsoft_typescript/tsc.sh: line 6: /root/.cache/bazel/_bazel_xzy/c01fa588a238f7be12b727d5cfbf1a03/execroot/tensorflow/external/org_nodejs/bin/node: Success\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 3.513s, Critical Path: 0.15s\r\n</pre>", "Modifying `tensorflow/workspace.bzl` `NODE=path/to/node` can fix it. I have built tensorflow success  and test pass. build patch see [build-tf-mips64.diff](https://github.com/xzy256/tensorflow-mips/blob/master/build-tf-mips.diff). Error and solution see [build-tf-mips-error.log]()\r\n<pre>\r\n[root@localhost tensorflow]# bazel build --copt=-march=native -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n...\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option \"-Wno-self-assign\"\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 4934.384s, Critical Path: 240.49s\r\n\r\n\r\n[root@localhost tensorflow]# bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n2017.06.07 Thir 10:59:09 CST : === Using tmpdir: /tmp/tmp.Fk3wTutAlo\r\n/home/xzy/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles /home/xzy/tensorflow\r\n/home/xzy/tensorflow\r\n/tmp/tmp.Fk3wTutAlo /home/xzy/tensorflow\r\n2017.06.07  Thir 10:59:12 CST : === Building wheel\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\n/home/xzy/tensorflow\r\n2017. 06. 07  Thir 10:59:40 CST : === Output wheel file is in: /tmp/tensorflow_pkg\r\n[root@localhost tensorflow_pkg]# pwd\r\n/tmp/tensorflow_pkg\r\n[root@localhost tensorflow_pkg]# ls\r\ntensorflow-1.2.0rc1-cp27-none-linux_mips64.whl\r\n\r\n\r\n# pip install /tmp/tensorflow_pkg/tensorflow-1.2.0rc1-cp27-none-linux_mips64.whl\r\n...\r\n    Installing /usr/lib/python2.7/site-packages/protobuf-3.3.0-py2.7-nspkg.pth\r\n  Found existing installation: six 1.7.3\r\n    Uninstalling six:\r\n      Successfully uninstalled six\r\n  Found existing installation: Werkzeug 0.9.6\r\n    Uninstalling Werkzeug:\r\n      Successfully uninstalled Werkzeug\r\nSuccessfully installed tensorflow mock bleach markdown numpy html5lib protobuf six werkzeug funcsigs pbr\r\nCleaning up...\r\n\r\n# python\r\n>>> import tensorflow as tf\r\n>>> \r\n</pre>", "Thank you for sharing how you resolved this!", "Hi @skye I want to have a look at tensorflow performance at my platform. How should i do? There are some existing benchmarks?", "Here are some benchmarks we performed along with the methodology: https://www.tensorflow.org/performance/benchmarks", "ok. Thank you. But this web page is invalid. I get `404 not found` error."]}, {"number": 9997, "title": "Find an error in mnist_softmax.py", "body": "\r\n\r\n\r\n------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution (Mac os)**:\r\n- **TensorFlow installed from (binary)**:\r\n- **TensorFlow version (master)**:\r\n\r\n\r\n\r\n\r\n### Describe the problem\r\nIn mnist_softmax.py,there is a difference between github an the web of Tensorflow(https://www.tensorflow.org/versions/r0.12/tutorials/mnist/beginners/index.html),and the code on github has error in line 57,58.\r\n\r\n### Source code / logs\r\nI replaced `cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))` with `cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))`.And it works.\r\n", "comments": ["Could you link to the file with the mistake?"]}, {"number": 9996, "title": "Convolution_transpose layer now gives an error (Tensorflow 1.0.0). ", "body": "I am implementing an architecture with conv and conv_transpose layers and this is what I am giving the convolution transpose layer: \r\n```\r\n    ('convolution_transpose', dict(num_outputs=96, kernel_size=[41, 11],\r\n                                     stride=[2, 1], padding=\"SAME\", scope='dec_block_1'))\r\n```\r\n\r\nand this is what I get \r\n\r\n\r\n```\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)\r\n    175       current_args = current_scope[key_func].copy()\r\n    176       current_args.update(kwargs)\r\n--> 177     return func(*args, **current_args)\r\n    178   _add_op(func)\r\n    179   setattr(func_with_args, '_key_op', _key_op(func))\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py in convolution2d_transpose(inputs, num_outputs, kernel_size, stride, padding, data_format, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)\r\n   1123         _scope=sc,\r\n   1124         _reuse=reuse)\r\n-> 1125     outputs = layer.apply(inputs)\r\n   1126 \r\n   1127     # Add variables to collections.\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py in apply(self, inputs, **kwargs)\r\n    301       Output tensor(s).\r\n    302     \"\"\"\r\n--> 303     return self.__call__(inputs, **kwargs)\r\n    304 \r\n    305 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, **kwargs)\r\n    267           input_shapes = [x.get_shape() for x in input_list]\r\n    268           if len(input_shapes) == 1:\r\n--> 269             self.build(input_shapes[0])\r\n    270           else:\r\n    271             self.build(input_shapes)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/convolutional.py in build(self, input_shape)\r\n   1048                                   regularizer=self.bias_regularizer,\r\n   1049                                   trainable=True,\r\n-> 1050                                   dtype=self.dtype)\r\n   1051     else:\r\n   1052       self.bias = None\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\r\n    986       collections=collections, caching_device=caching_device,\r\n    987       partitioner=partitioner, validate_shape=validate_shape,\r\n--> 988       custom_getter=custom_getter)\r\n    989 get_variable_or_local_docstring = (\r\n    990     \"\"\"%s\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\r\n    888           collections=collections, caching_device=caching_device,\r\n    889           partitioner=partitioner, validate_shape=validate_shape,\r\n--> 890           custom_getter=custom_getter)\r\n    891 \r\n    892   def _get_partitioned_variable(self,\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\r\n    339           reuse=reuse, trainable=trainable, collections=collections,\r\n    340           caching_device=caching_device, partitioner=partitioner,\r\n--> 341           validate_shape=validate_shape)\r\n    342     else:\r\n    343       return _true_getter(\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py in variable_getter(getter, name, shape, dtype, initializer, regularizer, trainable, **kwargs)\r\n    256           name, shape, initializer=initializer, regularizer=regularizer,\r\n    257           dtype=dtype, trainable=trainable,\r\n--> 258           variable_getter=functools.partial(getter, **kwargs))\r\n    259 \r\n    260     # Build (if necessary) and call the layer, inside a variable scope.\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py in _add_variable(self, name, shape, dtype, initializer, regularizer, trainable, variable_getter)\r\n    206                                initializer=initializer,\r\n    207                                dtype=dtype,\r\n--> 208                                trainable=trainable and self.trainable)\r\n    209     # TODO(sguada) fix name = variable.op.name\r\n    210     if variable in existing_variables:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py in layer_variable_getter(getter, *args, **kwargs)\r\n   1308       getter = functools.partial(current_custom_getter, getter)\r\n   1309     kwargs['rename'] = rename\r\n-> 1310     return _model_variable_getter(getter, *args, **kwargs)\r\n   1311   return layer_variable_getter\r\n   1312 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py in _model_variable_getter(getter, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, rename, **_)\r\n   1297       regularizer=regularizer, collections=collections, trainable=trainable,\r\n   1298       caching_device=caching_device, partitioner=partitioner,\r\n-> 1299       custom_getter=getter)\r\n   1300 \r\n   1301 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)\r\n    175       current_args = current_scope[key_func].copy()\r\n    176       current_args.update(kwargs)\r\n--> 177     return func(*args, **current_args)\r\n    178   _add_op(func)\r\n    179   setattr(func_with_args, '_key_op', _key_op(func))\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/variables.py in model_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter)\r\n    266                  trainable=trainable, collections=collections,\r\n    267                  caching_device=caching_device, device=device,\r\n--> 268                  partitioner=partitioner, custom_getter=custom_getter)\r\n    269   return var\r\n    270 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)\r\n    175       current_args = current_scope[key_func].copy()\r\n    176       current_args.update(kwargs)\r\n--> 177     return func(*args, **current_args)\r\n    178   _add_op(func)\r\n    179   setattr(func_with_args, '_key_op', _key_op(func))\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/variables.py in variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter)\r\n    223                   collections=collections,\r\n    224                   caching_device=caching_device,\r\n--> 225                   partitioner=partitioner)\r\n    226 \r\n    227 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\r\n    331           initializer=initializer, regularizer=regularizer, reuse=reuse,\r\n    332           trainable=trainable, collections=collections,\r\n--> 333           caching_device=caching_device, validate_shape=validate_shape)\r\n    334 \r\n    335     if custom_getter is not None:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\r\n    682         caching_device=caching_device,\r\n    683         dtype=variable_dtype,\r\n--> 684         validate_shape=validate_shape)\r\n    685     self._vars[name] = v\r\n    686     logging.vlog(1, \"Created variable %s with shape %s and init %s\", v.name,\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope)\r\n    224           name=name,\r\n    225           dtype=dtype,\r\n--> 226           expected_shape=expected_shape)\r\n    227 \r\n    228   def __str__(self):\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py in _init_from_args(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape)\r\n    301             with ops.name_scope(\"Initializer\"),  ops.device(None):\r\n    302               self._initial_value = ops.convert_to_tensor(\r\n--> 303                   initial_value(), name=\"initial_value\", dtype=dtype)\r\n    304               shape = (self._initial_value.get_shape()\r\n    305                        if validate_shape else tensor_shape.unknown_shape())\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py in <lambda>()\r\n    671       else:\r\n    672         init_val = lambda: initializer(\r\n--> 673             shape.as_list(), dtype=dtype, partition_info=partition_info)\r\n    674         variable_dtype = dtype.base_dtype\r\n    675 \r\n\r\nTypeError: __init__() got multiple values for argument 'dtype'\r\n\r\n```\r\n\r\nThe same code worked on Tensorflow 0.12. ", "comments": ["It's possible the API has changed in 1.0 (e.g. perhaps the argument ordering changed). Please see the documentation: https://www.tensorflow.org/api_docs/python/tf/contrib/layers/conv2d_transpose\r\nUsing keyword arguments may help too.", "@skye, unfortunately keyword arguments don't help. I am accessing conv_transpose in exactly the same way as I access standard conv2d layers. The conv_transpose throw the above error, though. \r\n\r\nEDIT: The error goes away if you set the weights_initializer = None and bias_initializer=None in the conv_transpose function. ", "Thank you for sharing your solution! I'm gonna keep this open as a bug as this seems like confusing behavior that can at least have a better error message.", "I'd be happy to work on this if still needed, please let me know (and any specific tips as well)!", "Hi @rohan-varma, it'd be great if you can look into this! Unfortunately I'm not very familiar with this code, so I can't give you any specific tips without digging in myself... assuming you can still repro the confusing error message yourself, it would be useful even if you can just understand and explain how the error arises (this is always a great way to learn about a new codebase too). Hopefully once you have that understanding, it won't be too hard to detect this condition earlier in the stack and return a more useful error message. Feel free to report back here with any findings or questions.", "I'm not able to reproduce the error. This is what I did.\r\n\r\n```\r\nIn [1]: import tensorflow as tf\r\n\r\nIn [2]: x = tf.placeholder(tf.float32, shape=[32, 256, 256, 3])\r\n\r\nIn [3]: tf.contrib.layers.convolution2d_transpose(x, num_outputs=96, kernel_size=[41, 11], stride=[2,1], padding='SAME', scope='dec_block1')\r\nOut[3]: <tf.Tensor 'dec_block1/Relu:0' shape=(32, 512, 256, 96) dtype=float32>\r\n```", "Can we close this?", "Yes we can close this.Thank you ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/9996\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/9996\">No</a>\n"]}, {"number": 9995, "title": "Update rnn.py", "body": "Support returning all states generated in dynamic_rnn if the states are rather 'simple'\r\nUnit test could be found [here](https://github.com/carefree0910/MachineLearning/blob/master/RNN/Test/UnitTest.py)\r\np.s. I finished this implementation under API r1.1 and I found the latest code of tensorflow.python.ops.rnn.py is quite different from that on my Windows machine with API r1.1. Since I cannot build the source code, though I've tested them under API r1.1, I haven't actually tested the committed codes in the latest environment", "comments": ["Can one of the admins verify this patch?", "In fact I was just wondering whether it is customary to return the final state only... Since the implementation is quite straightforward (Under 'simple' condition)", "@carefree0910 Can you provide context for this change?  Why does it need to be made?  This is adding to the API so will require API review before a lot of work should be done on it, and they will need more context as to why this is necessary.", "The easiest way to pass states out to the output is to write a wrapper RNNCell that calls the underlying cell, propagates its state through, but emits a tuple output: the underlying cell's output, and the state as the second part of the tuple.", "I'm closing this for now; since a less invasive user-side change can elicit the same behavior.", "Hi @ebrevdo , could you please elaborate more on the solution you provide here? I can't figure out how to implement such a thing. Thanks!", "```python\r\nclass Wrapper(tf.nn.rnn_cell.RNNCell):\r\n  def __init__(self, inner_cell):\r\n     super(self, Wrapper).__init__()\r\n     self._inner_cell = inner_cell\r\n  @property\r\n  def state_size(self):\r\n     return self._inner_cell.state_size\r\n  @property\r\n  def output_size(self):\r\n    return (self._inner_cell.state_size, self._inner_cell.output_size)\r\n  def call(self, input, state)\r\n    output, next_state = self._inner_cell(input, state)\r\n    emit_output = (next_state, output)\r\n    return emit_output, next_state\r\n```\r\n\r\nsomething like this", "Thanks! I didn't know you could pass a tuple as output too! In my case this wasn't necessary because I wrote my custom cell, but this is useful if you are using a cell already implemented in the api.\r\n ", "@ebrevdo Is your wrapper solution applicable for lstml cell too? Please check my question on [stackoverflow](https://stackoverflow.com/questions/47745027/tensorflow-how-to-obtain-intermediate-cell-states-c-from-lstmcell-using-dynam). Thanks!", "@ebrevdo I wasn't able to get your wrapper to work with current TF2.3, but I tried directly coding the general approach into my cell and it seems that it doesn't work if wrapping the cell in a [bidirectional wrapper](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional). This seems to be because the bidirectional wrapper tries to stack the outputs from the cell, and doesn't like that the shape of `next_state` doesn't match `output`. Specifically,\r\n```\r\n...\r\n   1653   try:\r\n-> 1654     c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\r\n   1655   except errors.InvalidArgumentError as e:\r\n\r\nInvalidArgumentError: Dimension 2 in both shapes must be equal, but are 128 and 640. Shapes are [?,1024,128] and [?,1024,640].\r\n\tFrom merging shape 2 with other shapes. for '{{node bidirectional/ReverseV2/tensor}} = Pack[N=4, T=DT_FLOAT, axis=0](bidirectional/backward_rnn/Identity, bidirectional/backward_rnn/Identity_1, bidirectional/backward_rnn/Identity_2, bidirectional/backward_rnn/Identity_3)' with input shapes: [?,1024,128], [?,1024,128], [?,1024,128], [?,1024,640].\r\n```\r\n\r\nI realize this thread is three years old, but is there any way to get all of the hidden states from a bidirectional RNN cell? Using `return_sequences=True` and `return_state=True` only returns the final state. I haven't been able to find another approach in my searches. It doesn't even need to be a particularly efficient approach (just trying to debug something)."]}, {"number": 9994, "title": "Make ResourceHandle private to tensorflow library", "body": "tensorflow::ResourceHandle is a protobuf object. The official protobuf document suggests:\r\n\r\n\"If your project is itself a DLL intended for use by third-party software, we recommend\r\nthat you do NOT expose protocol buffer objects in your library's public interface, and\r\nthat you statically link protocol buffers into your library\"\r\n\r\nWithout this change, it is not possible to implement a ResourceOpKernel in DLL on Windows.\r\n\r\nError message(before this change):\r\n\r\n```some.obj : error LNK2001: unresolved external symbol \"class google::protobuf::internal::ExplicitlyConstructed<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > google::protobuf::internal::fixed_address_empty_string\" (?fixed_address_empty_string@internal@protobuf@google@@3V?$ExplicitlyConstructed@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@123@A)\r\nsome.obj : error LNK2019: unresolved external symbol \"private: void __cdecl google::protobuf::Arena::AddListNode(void *,void (__cdecl*)(void *))\" (?AddListNode@Arena@protobuf@google@@AEAAXPEAXP6AX0@Z@Z) referenced in function \"public: void __cdecl google::protobuf::Arena::Own<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)\" (??$Own@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Arena@protobuf@google@@QEAAXPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)```\r\n", "comments": ["Can one of the admins verify this patch?", "Looks good to me. Adding @alextp just to have one more pair of eyes look at this.", "Hi @alextp  \r\n\r\nAssume we have a custom OP like this:\r\n\r\n\ttemplate <typename T>\r\n\tclass NumericResource : public tensorflow::ResourceBase\r\n\t{\r\n\tprivate:\r\n\t\tstd::atomic<T> value;\r\n\r\n\tpublic:\r\n\t\tNumericResource() : value(0) {}\r\n\t\tT get() const { return value; }\r\n\t\tvoid inc() { ++value; }\r\n\t\tvirtual std::string DebugString()\r\n\t\t{\r\n\t\t\tstd::ostringstream oss;\r\n\t\t\toss << value;\r\n\t\t\treturn oss.str();\r\n\t\t}\r\n\t};\r\n\ttypedef NumericResource<int> IntResource;\r\n\r\n\tclass IntVarOp : public tensorflow::ResourceOpKernel<IntResource>\r\n\t{\r\n\tpublic:\r\n\t\texplicit IntVarOp(OpKernelConstruction* context) : tensorflow::ResourceOpKernel<IntResource>(context) {}\r\n\tprivate:\r\n\t\tvirtual tensorflow::Status CreateResource(IntResource** resource)\r\n\t\t{\r\n\t\t\t*resource = new IntResource();\r\n\t\t\treturn tensorflow::Status::OK();\r\n\t\t}\r\n\t};\r\n\r\n\tREGISTER_KERNEL_BUILDER(Name(\"IntVar\").Device(\"CPU\"), IntVarOp);\r\n\r\nAnd we want to compile this OP into a \"dll\" or \u201cso\u201d. Because ResourceOpKernel  is a template class,  the code in that class is generated in user ops' dll, not tensorflow's dll. So ResourceOpKernel shouldn't call any function that internal to tensorflow, otherwise user will get a \"unresolved external symbol\" error in linking stage.\r\n\r\nBefore this change, user's dll will contain a call to MakeResourceHandle\\<T\\> function, which will generate protobuf related code, and allocate a protobuf object in user's dll, pass it to tensorflow's dll. This is dangerous because each dll may have its own protobuf memory pool. By any chance, if you allocate an object in one pool but free it in another pool, it will crash. That's why protobuf's document prohibit you expose protocol buffer objects in your library's public interface.\r\n\r\nAfter this change, the protobuf object's construction is done in tensorflow's dll (MakeResourceHandleToOutput function). ", "@alextp can you approve or provide further feedback?", "@tensorflow-jenkins test this please", "Sorry, I didn't test it under Linux. Will fix it.", "Ah, I see your issue now. Can you respond to my review comments?", "Jenkins, test this please.", "Jenkins, test this please.", "@tensorflow-jenkins test this please"]}, {"number": 9993, "title": "Add to tensor definition", "body": "Add a small snippet to the tensor definition for the sake of correctness. People always ask about how these are related to tensors as physicists and mathematicians use.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "Closing."]}, {"number": 9992, "title": "Disabling flaky tests", "body": "", "comments": []}, {"number": 9991, "title": "go install", "body": "\r\n\u6211\u5728\u5b89\u88c5\u7684\u65f6\u5019\u51fa\u73b0\u8fd9\u4e2a\u95ee\u9898\u6211\u7684\u662fmac\u73af\u5883\uff0c\u524d\u9762\u7684\u547d\u4ee4\u90fd\u662f\u6210\u529f\u7684\r\nmanmans-MBP:~ manman$ go test github.com/tensorflow/tensorflow/tensorflow/go\r\ncan't load package: package github.com/tensorflow/tensorflow/tensorflow/go: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go\" in any of:\r\n\t/usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go (from $GOROOT)\r\n\t/Users/manman/Documents/go/document/src/github.com/tensorflow/tensorflow/tensorflow/go (from $GOPATH)\r\nmanmans-MBP:~ manman$ go test github.com/tensorflow/tensorflow/tensorflow/go\r\nsignal: killed\r\nFAIL\tgithub.com/tensorflow/tensorflow/tensorflow/go\t0.003s\r\nmanmans-MBP:~ manman$ \r\n\r\n", "comments": []}, {"number": 9990, "title": "Revert links change", "body": "Revert the changes from https://github.com/tensorflow/tensorflow/pull/9882 and https://github.com/tensorflow/tensorflow/pull/9886 which were both merged into the r1.1 branch.\r\n\r\n(Note: the tensorflow.org/code links below were previously broken, which is why we changed them directly to GitHub links, but they are working again now.)", "comments": []}, {"number": 9989, "title": "add missing import from keras wrappers", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 9988, "title": "[feature] Support Lanczos method in tf.image.resize_images", "body": "Add support for a `Lanczos` (a truncated sinc) mode in `tf.image.resize_images`. Currently this mode is not offered.\r\n\r\n[Pillow supports `LANCZOS`](http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters) (aka `ANTIALIAS`) as a resampling method and stipulates this method has the [highest quality for down sampling](http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters-comparison-table) ([default for `thumbnail` for example](http://pillow.readthedocs.io/en/3.4.x/reference/Image.html#PIL.Image.Image.thumbnail)).\r\n![image](https://cloud.githubusercontent.com/assets/51059/26181858/91e9471e-3b41-11e7-8653-4fa7277e98a6.png)\r\n", "comments": ["FYI @girving @gpapan ", "Seems reasonable as contributions welcome to me.", "Hi, I want to work on this. \r\nI tried to take a look at related methods like `tf.image.resize_bilinear`.\r\n[api_docs](https://www.tensorflow.org/api_docs/python/tf/image/resize_bilinear) says the method is defined in `tensorflow/python/ops/gen_image_ops.py`, but I am unable to find gen_image_ops.py in tensorflow/python/ops.\r\nWhere is gen_image_ops.py? \r\nThanks", "The op is defined here: https://github.com/tensorflow/tensorflow/blob/fabd225532ee068d1f9ffbdc4a8795e7de99ad24/tensorflow/core/ops/image_ops.cc#L161", "Thanks. Where is it implemented?", "That is the old current set of resize ops. Lanczos has not been done.", "@shitian-ni  Hi, have you get the tensorflow/python/ops/gen_image_ops.p? or you know it work?\r\n", "+1 , would love to see this added", "This is available in the TF 2.0 alpha release.  ", "As @johnpjf mentioned, this is [supported](https://www.tensorflow.org/api_docs/python/tf/image/resize), as of TensorFlow 2.0. Thanks for the feature request! Closing now. \ud83d\udc4d "]}, {"number": 9987, "title": "Make .bazelrc import ~/.bazelrc", "body": "When running ./configure for the first time, an import ~/.bazelrc statement will be added to the generated .bazelrc if the user has one.\r\n\r\nFixes #9963\r\nSee also bazelbuild/bazel#3022", "comments": []}, {"number": 9986, "title": "Fix: config MKL build error - mkl_layout_pass.cc has missing dependency", "body": "Fixing the issue #9979 ", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9985, "title": "Execute average pooling on GPU if available", "body": "Try to execute pooling2d on GPU.", "comments": ["![try](https://cloud.githubusercontent.com/assets/23068/26179404/6614d956-3b18-11e7-846d-e9f72dda9604.jpg)\r\n", "Haha... tell that to test_session. :)", "We should make sure to cherry-pick this into master as well since right now it is only in r1.2."]}, {"number": 9984, "title": "Missing .pb.h Files While Building from Source on Windows using CMake ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10 Enterprise\r\n- **TensorFlow installed from (source or binary)**:\r\nTrying to install from source\r\n- **TensorFlow version (use command below)**:\r\n1.1\r\n- **Bazel version (if compiling from source)**:\r\nUsing cmake to compile from source\r\n- **CUDA/cuDNN version**:\r\nNot using CUDA. Trying to build CPU only version from source\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nAfter following the instructions to build TF from source on Windows using cmake as given [here](https://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake), at step 4, I am trying to build tf_tutorials_example_trainer.vcxproj using MS Visual Studio 2015 instead of MSBuild.\r\n\r\n### Describe the problem\r\nWhile trying to build tf_tutorials_example_trainer.vcxproj, I get a whole bunch of errors such as \"Cannot open include file <some/path/*.pb.h>\". After looking at the source code files, it looks like .pb.h files do not exist. However, the corresponding .proto files do exist. For example, I can see \"atr_value.proto\" under <TF source files/tensorflow/core/framework>, but not \"atr_value.pb.h\". I think for some reason protobuf is not generating the .pb.h files from .proto\r\n\r\nNote: I have also installed the TF executable following instructions given [here](https://www.tensorflow.org/install/install_windows). It installed correctly. Hence, I have all dependencies installed on my system.\r\n\r\n### Source code / logs\r\ncmake log:\r\n-- Building for: Visual Studio 14 2015\r\n-- The C compiler identification is MSVC 19.0.24215.1\r\n-- The CXX compiler identification is MSVC 19.0.24215.1\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe\r\n       -- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed\r\n-- Found PythonInterp: C:/local/Anaconda3-4.1.1-Windows-x86_64/envs/tensorflow/python.exe (found version \"3.5.3\")\r\n-- Found PythonLibs: C:/local/Anaconda3-4.1.1-Windows-x86_64/envs/tensorflow/libs/python35.lib (found version \"3.5.3\")\r\n-- Found SWIG: C:/local/swigwin-3.0.10/swig.exe (found version \"3.0.10\")\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: <TF source dir>/tensorflow/contrib/cmake/build\r\n", "comments": ["@mrry can you comment or redirect to someone who can? Thanks!", "Does it work if you use MSBuild rather than Visual Studio to build the project?", "I think you may need to build the tf_protos_cc module to generate those pb.h files.", "Had the same issue as you yesterday. In tf_core_framework.cmake they are trying to build the *pb.h from the *.proto files. I guess they forgot to check, if the protocol buffer compiler is installed. Maybe it will help, if you install in manually https://developers.google.com/protocol-buffers/docs/downloads. Please try and let me know", "ISSUE RESOLVED. It was a silly one. \r\n\r\nThe build process makes a bunch of git clones. As I was trying to build x64 Release mode in MSVS 2015, the 64-bit version of Git was being called and failed as I had 32-bit version installed on my system. I installed 64-bit Git and the .pb.h files were successfully generated. \r\n\r\nThanks for the help!", "> ISSUE RESOLVED. It was a silly one.\r\n> \r\n> The build process makes a bunch of git clones. As I was trying to build x64 Release mode in MSVS 2015, the 64-bit version of Git was being called and failed as I had 32-bit version installed on my system. I installed 64-bit Git and the .pb.h files were successfully generated.\r\n> \r\n> Thanks for the help!\r\n\r\nI can't understand how to generate dnn.pb.h file.\r\nCan you please upload dnn.pb.h file in order to \"\"copy\"\" the declaration and manually generate it?"]}, {"number": 9983, "title": "Update layers.md: fix variable name", "body": "Update reference to nonexistent variable h_conv2.", "comments": []}]