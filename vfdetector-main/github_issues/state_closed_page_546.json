[{"number": 37339, "title": "ValueError: Could not find matching function to call loaded from the SavedModel.", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution :  Windows 10\r\n- TensorFlow installed from: pip, tf 2.1.0,  cpu\r\n- Python version:  3.7.4\r\n\r\n**build a simple network**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nclass Model():\r\n    def __init__(self):\r\n        self.build_model()\r\n\r\n    def build_model(self):\r\n        input1 = layers.Input(shape=(5,))\r\n        input2 = layers.Input(shape=(5,))\r\n\r\n        out1 = layers.Dense(1)(input1)\r\n        out2 = layers.Dense(1)(input2)\r\n        out = out1 - out2\r\n        out = tf.nn.sigmoid(out)\r\n\r\n        self.model = tf.keras.Model(inputs=[input1, input2], outputs=out)\r\n\r\nmodel = Model()\r\ns1 = \"exp\\\\model\"\r\nmodel.model.save(s1)\r\nmodel2 = tf.keras.models.load_model(s1)\r\n```\r\n\r\n**Test Code**\r\n```\r\nmodel = Model()\r\ns1 = \"exp\\\\model\"\r\nmodel.model.save(s1)\r\nmodel2 = tf.keras.models.load_model(s1)\r\n```\r\n\r\n**Error Info**\r\n```\r\nTraceback (most recent call last):\r\n  File \"e:\\workspace\\work\\Study\\pytorch_demos\\demo5.py\", line 22, in <module>\r\n    model2 = tf.keras.models.load_model(s1)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\", line 150, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\load.py\", line 89, in load\r\n    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\saved_model\\load.py\", line 552, in load_internal\r\n    export_dir)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\load.py\", line 119, in __init__\r\n    self._finalize()\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\load.py\", line 157, in _finalize\r\n    created_layers={layer.name: layer for layer in node.layers})\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 1903, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 1851, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 773, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\utils.py\", line 59, in return_outputs_and_add_losses\r\n    outputs, losses = fn(inputs, *args, **kwargs)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\saved_model\\function_deserialization.py\", line 262, in restored_function_body\r\n    \"\\n\\n\".join(signature_descriptions)))\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (1 total):\r\n    * Tensor(\"inputs:0\", shape=(None, 1), dtype=float32)\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 1 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (1 total):\r\n    * [TensorSpec(shape=(None, 1), dtype=tf.float32, name='inputs/0')]\r\n  Keyword arguments: {}\r\n```\r\n\r\n**But removing the line will not cause the error** \r\n```\r\n# Correct if not using this line\r\nout = tf.nn.sigmoid(out)\r\n```\r\n\r\n### *The error is caused when the model is loaded from savedModel file. Model building process is correct.*\r\n", "comments": ["@Rogersjk \r\nCould you please share all dependencies for us to replicate the issue faced by you, please find the [gist](https://colab.sandbox.google.com/gist/Saduf2019/5f23558078f731add7d3aba4338512c1/untitled74.ipynb) of the error faced while we replicate your issue in our local.", "> @Rogersjk\r\n> Could you please share all dependencies for us to replicate the issue faced by you, please find the [gist](https://colab.sandbox.google.com/gist/Saduf2019/5f23558078f731add7d3aba4338512c1/untitled74.ipynb) of the error faced while we replicate your issue in our local.\r\n\r\nHi @Saduf2019 , the error disappears when I change the version of tensorflow from 2.1.0 to 2.0.1. But it still occurs in 2.1.0 version. Please find the error in the [gist](https://colab.research.google.com/gist/Rogersjk/9ea5803e9248f67156067cb3e2384dab/untitled74.ipynb)", "@Rogersjk Thank you for reporting this. \r\nThis is already fixed in the latest version of tensorflow (tf-nightly). Please find the gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/f9517b4ffe3eeeb9c856e985109b445c/copy-of-untitled74.ipynb) .\r\n\r\n", "Closing this issue as it has been fixed. Please add additional comments and we can open the issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37339\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37339\">No</a>\n", "Hi there ,have you fix this bugs now in tf2.1.0 version? I also got the same error, after saved model and reload model with tf.keras.models.load_model; IF not fixed,  what is the right way to export saved model with pb format ,and reload for inference?", "> Hi there , this happens due to the bugs of tf2.1.0 version. You can avoid that by upgrading to the latest tensorflow version.", "I'm seeing similar issue in 2.2.0 version. The difference is that if I don't touch anything, loaded model works.  But if I change input data by removing certain rows from df, then it reports this error.\r\nShouldn't saved model independent of input data?", "Seeing similar error in 2.4.0. Not at load time, but when calling model.predict(ds) or modle.predict_step()\r\n\r\ncode that works with model just after training fails with this exception after save() and load()", "I have the same bug in my code for Tensorflow 2.3 as installed with Conda. The same code for Tensorflow-gpu 2.2.0 installed with Conda produces an index error related to tf.gather_nd operations. ", "I have the same bug in tensorflow 2.2.0", "I have the same bug in Tensorflow 2.6.0", "I have the same bug in Tensorflow 2.6.0 as well", "have the same bug in TF 2.6.0", "I have the same bug in tensorflow 2.1.0 - GPU", "I got this error recently, and fixed it by assigning layers and other objects to `self` within the class. Say, you are creating a model like shown in the top post, try assigning each of the new layers you instantiate like so `self.layer_1 = tf.keras.layers.Dense(...`.", "I have the same issue with tensorflow-gpu 2.6.0", "having the same issue in tensorflow==2.6.0", "having the same issue in tensorflow==2.4.0", "having the same issue in tensorflow==2.4.1\r\n", "Same... tensorflow==2.6.0", "Same, tensorflow==2.7.0", "FYI I think it is a generic exception that may indicate many errors in the\ncode. I got it working with 2.3 eventually but unsure what needed to be\nchanged.\n\nOn Mon, Nov 29, 2021, 05:52 ilyeser ***@***.***> wrote:\n\n> Same, tensorflow==2.7.0\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/37339#issuecomment-981605961>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJVAVKW62YXJS65G2XERHELUONZRVANCNFSM4LCDUAUQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n"]}, {"number": 37338, "title": "ValueError: Input 0 of layer dense_1 is incompatible with the layer: expected axis -1 of input shape to have value 2 but received input with shape [None, 5]", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution :  Windows 10\r\n- TensorFlow installed from: pip, tf 2.1.0,  cpu\r\n- Python version: - 3.7.4\r\n\r\n**build a simple network**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nclass Model():\r\n    def __init__(self):\r\n        self.build_model()\r\n\r\n    def build_model(self):\r\n        input1 = layers.Input(shape=5)\r\n\r\n        net = tf.keras.models.Sequential([\r\n            layers.Dense(2),\r\n            layers.Dense(1),\r\n        ])\r\n\r\n        out1 = net(input1)\r\n        \"\"\"\r\n        out1 = layers.Dense(2)(input1)\r\n        out1 = layers.Dense(1)(out1)\r\n        \"\"\"\r\n\r\n        self.model = tf.keras.Model(inputs=input1, outputs=out1)\r\n```\r\n\r\n**Test Code**\r\n```\r\nmodel = Model()\r\ns1 = \"exp\\\\model\"\r\nmodel.model.save(s1)\r\nmodel2 = tf.keras.models.load_model(s1)\r\n```\r\n\r\n**Error Info**\r\n```\r\nTraceback (most recent call last):\r\n  File \"e:\\workspace\\work\\Study\\pytorch_demos\\demo5.py\", line 26, in <module>\r\n    model2 = tf.keras.models.load_model(s1)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\", line 150, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\load.py\", line 89, in load\r\n    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\saved_model\\load.py\", line 552, in load_internal\r\n    export_dir)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\load.py\", line 119, in __init__\r\n    self._finalize()\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\load.py\", line 151, in _finalize\r\n    node.add(layer)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py\", line 203, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 737, in __call__\r\n    self.name)\r\n  File \"C:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\input_spec.py\", line 213, in assert_input_compatibility\r\n    ' but received input with shape ' + str(shape))\r\nValueError: Input 0 of layer dense_1 is incompatible with the layer: expected axis -1 of input shape to have value 2 but received input with shape [None, 5]\r\n```\r\n\r\n**But following usage will not cause the error** \r\n```\r\n# Correct if not using tf.keras.Sequential\r\nout1 = layers.Dense(2)(input1)\r\nout1 = layers.Dense(1)(out1)\r\n```\r\n\r\n### *The error is caused when the model is loaded from savedModel file. Model building process is correct.*\r\n", "comments": ["@Rogersjk \r\nCan you try running the code in latest -tf-nightly version(`!pip install tf-nightly`). I am not seeing any issue in Nightly version. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/f642d14fddacae3d59be20b354406c37/untitled705.ipynb).Thanks!", "> @Rogersjk\r\n> Can you try running the code in latest -tf-nightly version(`!pip install tf-nightly`). I am not seeing any issue in Nightly version. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/f642d14fddacae3d59be20b354406c37/untitled705.ipynb).Thanks!\r\n\r\nThank you @ravikyram , I change the tensorflow version to 2.0.1, the error also disappears. So I guess it's the problem of tensorflow version 2.1.0", "@Rogersjk \r\n\r\nYou could use tf-nightly for now and in the next couple of months new stable version will be released. I am closing the issue for now. Please feel free to open the issue if it persists with the new version. Thanks!", "> @Rogersjk\r\n> \r\n> You could use tf-nightly for now and in the next couple of months new stable version will be released. I am closing the issue for now. Please feel free to open the issue if it persists with the new version. Thanks!\r\n\r\nThanks for your help."]}, {"number": 37337, "title": "libmkl_intel.so and libiomp5.so are not being copied for libtensorflow.so build", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.1 (v2.1 tag)\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen building the targets `//tensorflow:libtensorflow.so` and `//tensorflow:libtensorflow_cc.so` with `--config=mkl` while libtensorflow.so and libtensorflow_cc.so are available under `bazel-bin/tensorflow`, the MKLDNN libs aren't where we'd expect them in eg; `bazel-bin/external` or `bazel-bin/third_party`. Instead, they're in the `_solib_<some suffix>` folder where the suffix is environment specific which makes it hard to retrieve the MKL-DNN libs.\r\n\r\nInstead, the build should symlink these libs into a more user friendly dir inside third_party or external.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI built 2.1 //tensorflow:libtensorflow_cc.so and //tensorflow:libtensorflow.so with `--config=mkl`.\r\n", "comments": ["cc @oneraynyday", "I'm experiencing a similar problem when trying to compile TF 2.2 from source with CUDA and MKL support.\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from *source*\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.7.7\r\n- Installed using *conda*\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: CUDA 10.0, cuDNN 7.4.2.24\r\n- GPU model and memory: GeForce GTX 1080 Ti\r\n\r\n**Describe the problem**\r\n\r\nCompilation fails with the error:\r\n\r\n```\r\nERROR: /tensorflow-2.2.0/tensorflow/python/BUILD:2712:1: Executing genrule //tensorflow/python:clustering_ops_pygenrule failed (Exit 127)                                   \r\nbazel-out/host/bin/tensorflow/python/gen_clustering_ops_py_wrappers_cc: error while loading shared libraries: libiomp5.so: cannot open shared object file: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nbazel build --config=opt --config=mkl --config=noaws --config=nohdfs  --config=v2   //tensorflow/tools/pip_package:build_pip_package\r\n```", "@iamthebot Thank your suggestion! The current behavior is how bazel handles .so libs, it picks the right location for them. We can't make sure if we can change that behavior.  We will consider the idea in next improvement.", "Can anyone detail a work around?", "@astirn \r\nI guess your question is how to find the so files after building.\r\nIf it's right, please see my suggestion. else, please correct me.\r\n\r\nSuggestion:\r\n  Search the so files in the build/cache folder, like:\r\n  ```\r\nfind ~/.cache/bazel  -name *mkl*.so\r\n/home/xxx/.cache/bazel/_bazel_xxx/7bdb32f3c26b63ec1cee6edbca5723aa/external/mkl_linux/lib/libmklml_gnu.so\r\n/home/xxx/.cache/bazel/_bazel_xxx/7bdb32f3c26b63ec1cee6edbca5723aa/external/mkl_linux/lib/libmklml_intel.so\r\n```\r\n", "Thanks for the reply @NeoZhangJianyu! Actually, my question is during the build process. I get the following error when using the `--config=MKL`:\r\n\r\n...`libiomp5.so: cannot open shared object file: No such file or directory`\r\n\r\n My steps to install `libiomp5.so`:\r\n\r\n1. Download and install MKL libraries from [Intel](https://software.intel.com/content/www/us/en/develop/tools/math-kernel-library/choose-download/linux.html).\r\n2. `sudo ./install_GUI.sh` (exclude ia32 architecture and Fortran support--I have an Intel64 architecture)\r\n3. After, my path to libiomp5.so is `/opt/intel/compilers_and_libraries_2020.2.254/linux/compiler/lib/intel64_lin/libiomp5.so`\r\n4. I have tried adding this path to `LD_LIBRARY_PATH` and running `sudo ldconfig` without success.\r\n\r\nMy configuration is as follows:\r\n- Ubuntu 20.04. Despite this choice, TensorFlow's 18.04 [GPU support instructions](https://www.tensorflow.org/install/gpu) work fine--I have the versions specified therein. I am running python 3.8 virtual environments with pip-installed TF2.2 with GPU acceleration on my system.\r\n\r\n### My steps to build TensorFlow from source\r\n1. Install [appropriate Bazel version](https://www.tensorflow.org/install/source#tested_build_configurations). For TF2.2, do:\r\n\r\n        sudo apt install curl gnupg\r\n        curl -f https://bazel.build/bazel-release.pub.gpg | sudo apt-key add -\r\n        echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list\r\n        sudo apt update\r\n        sudo apt install bazel-2.0.0\r\n        sudo update-alternatives --install /usr/bin/bazel bazel /usr/bin/bazel-2.0.0 0\r\n        \r\n2. Checkout `https://github.com/tensorflow/tensorflow.git`\r\n3. Make and run a virtual environment with python 3.8 and install:\r\n\r\n        pip install -U  pip six 'numpy<1.19.0' wheel setuptools mock 'future>=0.17.1'\r\n        pip install -U  keras_applications --no-deps\r\n        pip install -U  keras_preprocessing --no-deps\r\n\r\n4. Checkout release branch: `git checkout r.2.2`.\r\n5. Run `./configure`. My settings and configuration options are below (note gcc is set to v8 with `update-alternatives`):\r\n\r\n        Extracting Bazel installation...\r\n        You have bazel 2.0.0 installed.\r\n        Please specify the location of python. [Default is /home/andrew/PycharmProjects/tensorflow/venv/bin/python]: \r\n        \r\n        \r\n        Found possible Python library paths:\r\n          /home/andrew/PycharmProjects/tensorflow/venv/lib/python3.8/site-packages\r\n        Please input the desired Python library path to use.  Default is [/home/andrew/PycharmProjects/tensorflow/venv/lib/python3.8/site-packages]\r\n        \r\n        Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\n        No OpenCL SYCL support will be enabled for TensorFlow.\r\n        \r\n        Do you wish to build TensorFlow with ROCm support? [y/N]: N\r\n        No ROCm support will be enabled for TensorFlow.\r\n        \r\n        Do you wish to build TensorFlow with CUDA support? [y/N]: y\r\n        CUDA support will be enabled for TensorFlow.\r\n        \r\n        Do you wish to build TensorFlow with TensorRT support? [y/N]: y\r\n        TensorRT support will be enabled for TensorFlow.\r\n        \r\n        Found CUDA 10.1 in:\r\n            /usr/local/cuda/lib64\r\n            /usr/local/cuda/include\r\n        Found cuDNN 7 in:\r\n            /usr/lib/x86_64-linux-gnu\r\n            /usr/include\r\n        Found TensorRT 7 in:\r\n            /usr/lib/x86_64-linux-gnu\r\n            /usr/include/x86_64-linux-gnu\r\n        \r\n        \r\n        Please specify a list of comma-separated CUDA compute capabilities you want to build with.\r\n        You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\n        Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 7.5,7.5]: \r\n        \r\n        \r\n        Do you want to use clang as CUDA compiler? [y/N]: N\r\n        nvcc will be used as CUDA compiler.\r\n        \r\n        Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n        \r\n        \r\n        Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n        \r\n        \r\n        Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\n        Not configuring the WORKSPACE for Android builds.\r\n        \r\n        Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n                --config=mkl            # Build with MKL support.\r\n                --config=monolithic     # Config for mostly static monolithic build.\r\n                --config=ngraph         # Build with Intel nGraph support.\r\n                --config=numa           # Build with NUMA support.\r\n                --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n                --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\n        Preconfigured Bazel build configs to DISABLE default on features:\r\n                --config=noaws          # Disable AWS S3 filesystem support.\r\n                --config=nogcp          # Disable GCP support.\r\n                --config=nohdfs         # Disable HDFS support.\r\n                --config=nonccl         # Disable NVIDIA NCCL support.\r\n        Configuration finished\r\n\r\n6. `bazel build --config=opt --config=cuda --config=mkl --local_ram_resources=9216 //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n\r\n\r\n", "@astirn \r\n\r\n1.\r\n\r\nThe build parameters: `--config=cuda --config=mkl ` are usually not used in same time. \r\n`--config=mkl` \u2014Support for the Intel\u00ae MKL-DNN.  --refer to https://www.tensorflow.org/install/source\r\n  \r\nMKL-DNN will enable the TF execute the ops on CPU, but CUDA will enable the TF ops on GPU.\r\nWe don't test the case bind them together. So, we don't know the result of building.\r\n\r\nMaybe you could remove one of them.\r\n\r\n\r\n2. \r\n\r\nfor the issue: `...libiomp5.so: cannot open shared object file: No such file or directory`\r\n\r\nCould you check if the $LD_LIBRARY_PATH include the real path?\r\n\r\nrun cmd:\r\n```\r\nenv | grep LD_LIBRARY_PATH\r\n```", "@NeoZhangJianyu Thanks again for your reply. The reason I wanted both CUDA and MKL is that it appears TF is using both CPU and GPU for RNNs. See the following TensorBoard profiler picture (the top portion is for the GPU): \r\n\r\n![profiler](https://user-images.githubusercontent.com/35155306/89115814-7ccd1800-d441-11ea-90e4-2bc0f6ed67b9.png)\r\n\r\nI have followed all the [requirements](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/layers/LSTM) for CuDNN acceleration, yet it appears that the CPU is still partially responsible. I dug a bit into recurrent_v2.py to try to understand why and observed that `lstm_with_backend_selection` is being utilized, which arbitrates between using `standard_lstm(...)` and `gpu_lstm(...)`, as opposed to directly using `gpu_lstm(...)`.\r\n\r\nI double checked that my virtual environment from which I ran the bazel build had the libiomp5.so path on LD_LIBRARY_PATH upon failing--I appended its path just before calling the build command. \r\n\r\n```\r\nERROR: /home/andrew/PycharmProjects/tensorflow/tensorflow/python/BUILD:2905:1: Executing genrule //tensorflow/python:stateless_random_ops_pygenrule failed (Exit 127)\r\nbazel-out/host/bin/tensorflow/python/gen_stateless_random_ops_py_wrappers_cc: error while loading shared libraries: libiomp5.so: cannot open shared object file: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 16278.002s, Critical Path: 229.50s\r\nINFO: 18808 processes: 18808 local.\r\nFAILED: Build did NOT complete successfully\r\n(venv) andrew@andrew-rbs:~/PycharmProjects/tensorflow$ env | grep LD_LIBRARY_PATH\r\nLD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/intel/compilers_and_libraries_2020.2.254/linux/compiler/lib/intel64_lin\r\n(venv) andrew@andrew-rbs:~/PycharmProjects/tensorflow$ sudo find / -iname libiomp5.so\r\n...\r\n...\r\n/opt/intel/compilers_and_libraries_2020.2.254/linux/compiler/lib/intel64_lin/libiomp5.so\r\n```\r\n", "@astirn \r\n\r\n1. Add the path of the library into /etc/ld.so.conf. Then run ldconfig to update the cache.\r\n\r\n2. According to the log of yours, the path setting of iomp should be right.\r\nCould you try following example to test the link to iomp?\r\n\r\nCreate hellow_omp.cpp\r\n```\r\n#include <omp.h>\r\n#include <iostream>\r\n\r\nint main ()\r\n{\r\n    #pragma omp parallel\r\n    {\r\n        std::cout << \"Hello World\" << std::endl;\r\n    }\r\n    return 0;\r\n}\r\n\r\n```\r\n\r\nCompile it:\r\n```\r\n/opt/intel/compilers_and_libraries/linux/bin/compilervars.sh intel64\r\n\r\nicpc -L/opt/intel/compilers_and_libraries_2020.0.166/linux/compiler/lib/intel64_lin -liomp5 -fopenmp -Wl,--as-needed hello_omp.cpp  \r\n\r\nldd a.out\r\n```", "@NeoZhangJianyu thanks for all your help. I was eventually able to get it to compile with moth MKL and CUDA. However, the only place I could acquire both `libiomp5.so` and `libmklml_intel.so` was from https://github.com/01org/mkl-dnn/releases/download/v0.9/mklml_lnx_2018.0.20170425.tgz (in case other people are wondering). I copied them both to /usr/lib and added it to the LD_LIBRARY_PATH and the build completed. Sadly, I did not get any speed gains, but at least I know how to compile from source now!", "@astirn \r\nIf possible, could you close this issue?\r\n", "I don't think I can since I am not OP.", "@astirn Sorry, it's my mistake.\r\n", "@iamthebot \r\nIs it possible to close this issue?", "@iamthebot\r\n\r\nIf your issue is fixed, could you close it?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37337\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37337\">No</a>\n"]}, {"number": 37336, "title": "Tensorflow fails in building `GRU` models in some cases.", "body": "**System information**  \r\n- Have I written custom code (as opposed to using example directory):  \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10\r\n- TensorFlow backend (yes / no):  yes\r\n- TensorFlow version:  1.15.0\r\n- Python version:  3.6.9\r\n- CUDA/cuDNN version:  -\r\n- GPU model and memory:  -\r\n\r\n**Describe the current behavior**  \r\n\r\n**When I directly build a model with layer `GRU`on Tensorflow, I get some variable multiplication errors**, as shown below (raised in `tensorflow_core\\python\\ops\\resource_variable_ops.py line 1229`).  **The error reveals unsatisfactory implementation on Tensorflow in supporting variable multiplication.** \r\n\r\n  ```\r\n<class 'RuntimeError'>, RuntimeError('Variable *= value not supported. Use var.assign(var * value) to modify the variable or var = var * value to get a new Tensor object.',), <traceback object at 0x00000240961ED908>),\r\n  ```\r\n\r\n**Similar issues also happen in `LSTM` and `SimpleRNN`**.  For detailed parameters of `GRU`, you can refer to the following code snippet.\r\n\r\n## Key insights\r\n\r\nThe error indicates that **the variable multiplication with format `Variable *= value` is not well supported on TensorFlow. It should be extended to full mode to conduct multiplication.  This caused Tensorflow to be unable to build the model.** \r\n\r\n## Code to reproduce the issue\r\n\r\n``` python\r\nimport numpy as np\r\nimport keras.layers as L\r\nfrom keras.engine import Model, Input\r\n\r\n## Using Tensorflow as Keras backend.\r\n## Input dtype default is float32\r\n\r\n#GRU kwargs\r\nkwargs = {\r\n\t'units': 2,\r\n  \t'dropout': 0.20430343923336958,\r\n  \t'recurrent_dropout': 0.7597739154146002,\r\n  \t'implementation': 2,\r\n    'reset_after': True,\r\n    'use_bias': True,\r\n    'return_sequences': False,\r\n    'return_state': False,\r\n    'go_backwards': False,\r\n    'stateful': True,\r\n    'unroll': False\r\n}\r\n\r\n# SimpleRNN kwargs\r\n# kwargs = {\r\n#\t\t'units': 2,\r\n#  \t'dropout': 0.9030407578803185,\r\n#  \t'use_bias': True,\r\n#  \t'recurrent_dropout': 0.8988069898639027,\r\n#  \t'return_sequences': False,\r\n#  \t'return_state': False,\r\n#  \t'go_backwards': True,\r\n#  \t'stateful': True,\r\n#  \t'unroll': True\r\n#}\r\n\r\n\r\ninput = (10 * np.random.random((2,10,8)))\r\nlayer = L.recurrent.GRU(**kwargs)\r\n#layer = L.recurrent.SimpleRNN(**kwargs)\r\nx = Input(batch_shape=input.shape)\r\ny = layer(x)\r\nbk_model = Model(x, y)\r\nprint('finish')\r\n```\r\n", "comments": ["@shiningrain, I tried replicating the reported issue. I didn't receive any error. Please take a look at the gist [here](https://colab.sandbox.google.com/gist/Saduf2019/6351f70f3c49499cd34265d487664bad/untitled419.ipynb). Thanks! ", "> @shiningrain, I tried replicating the reported issue. I didn't receive an error. Please take a look at the gist [here](https://colab.sandbox.google.com/gist/Saduf2019/6351f70f3c49499cd34265d487664bad/untitled419.ipynb). Thanks!\r\n\r\nHi! @gadagashwini \r\nThank for your help!\r\nI have read your code and it did run well in colab. But when I try to run the same code on my PC, the error I described above will still arise.(Shown in the following pictures.) This really confuses me.\r\n\r\n![pic1](https://user-images.githubusercontent.com/46860123/75994629-3927be00-5f36-11ea-8df1-ff10d225b232.jpg)\r\n![pic2](https://user-images.githubusercontent.com/46860123/75995739-d2a39f80-5f37-11ea-94dd-db4da4000a88.jpg)\r\n\r\n\r\nThe environment in the 1st picture:\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Tensorflow backend (yes / no): yes\r\n- Tensorflow version\uff1a1.15.0-cpu\r\n- Keras version: 2.3.1\r\n- Python version: 3.6.9\r\n\r\nThe environment in the 2nd picture:\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu18.04\r\n- Tensorflow backend (yes / no): yes\r\n- Tensorflow version\uff1a1.14.0-GPU\r\n- Keras version: 2.3.1\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: Cuda 9.0\r\n- GPU model and memory: TITAN V\r\n\r\nHope the above information can help with this problem.\r\nThanks again for your help!", "@shiningrain, Can you try with changing import statement as \r\n```\r\nimport numpy as np\r\nimport tensorflow.keras.layers as L\r\nfrom tensorflow.keras import Model, Input\r\n```\r\nLet us know how it works. Thanks", "> @shiningrain, Can you try with changing import statement as\r\n> \r\n> ```\r\n> import numpy as np\r\n> import tensorflow.keras.layers as L\r\n> from tensorflow.keras import Model, Input\r\n> ```\r\n> \r\n> Let us know how it works. Thanks\r\n\r\nThanks for your help!\r\nI have changed the import statement and **it still raises the same error**, shown as the following picture.\r\n![pic3](https://user-images.githubusercontent.com/46860123/76070093-ef3ee680-5fce-11ea-9801-23729218dd18.jpg)\r\n\r\n\r\n", "@shiningrain Can you try updating keras version ? The colab hosts Keras Version: 2.2.5", "> @shiningrain Can you try updating keras version ? The colab hosts Keras Version: 2.2.5\r\n\r\nHello! \r\nThanks for your help!.\r\nThe Keras version led to these errors is 2.3.1, which was also shown in my last comment.\r\n![pic4](https://user-images.githubusercontent.com/46860123/76266044-306e1980-62a1-11ea-9791-e416ef4123cd.jpg)\r\n\r\nI also tried Keras version 2.2.5 in the following pictures and the problem didn't seem to disappear \r\n![pic5](https://user-images.githubusercontent.com/46860123/76266140-7e831d00-62a1-11ea-8e59-358dec4b14e2.jpg)\r\n![pic6](https://user-images.githubusercontent.com/46860123/76266144-804ce080-62a1-11ea-983c-456949be3f24.png)\r\n\r\nThank you again for your help. ", "Can you please change your imports to `tensorflow.keras` rather than `keras`?\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras.layers as L\r\nfrom tensorflow.keras import Model, Input\r\n```", "> Can you please change your imports to `tensorflow.keras` rather than `keras`?\r\n> \r\n> ```python\r\n> import numpy as np\r\n> import tensorflow as tf\r\n> import tensorflow.keras.layers as L\r\n> from tensorflow.keras import Model, Input\r\n> ```\r\n\r\nHello! Thanks for your help!\r\nYour suggestion is the same as gadagashwini 's. I have tried this replacement and got a same error. You find it in the  previous comment in this page.", "Hi @shiningrain, I wasn't able to reproduce the error as well. Even I could, we are not going to fix and create a new release for Tensorflow 1.15 since it is frozen.\r\n\r\nI would suggest u to try the latest Tensorflow stable release or nightly and see if your code works or not. Feel free to reopen the issue if you still encounter the error with latest releases.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37336\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37336\">No</a>\n"]}, {"number": 37335, "title": "Tensorflow can run and build model with the corner case `Dense(unit=0)`", "body": "**System information**  \r\n- Have I written custom code (as opposed to using example directory):  \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10\r\n- TensorFlow backend (yes / no):  yes\r\n- TensorFlow version:  1.15.0\r\n- Python version:  3.6.9\r\n- CUDA/cuDNN version:  -\r\n- GPU model and memory:  -\r\n\r\n**Describe the current behavior**  \r\nWhen I set `Dense (units = 0)`, **TensorFlow can build the model normally**. `units = 0` is an obviously  unreasonable parameter, which should be stopped before building model. But Tensorflow treats it as a normal parameter. The model saved by Tensroflow may lead to potential risk. \r\nDoes unit = 0 have any special effect on TensorFlow? I don't see the corresponding instructions in the documentation. If not,**should Tensorflow set a check for such unreasonable parameters to avoid the risks and incorrect usages in the model?** \r\n\r\n**Code to reproduce the issue**  \r\n```\r\nimport numpy as np\r\nimport keras.layers as L\r\nimport keras.backend as K\r\nfrom keras.engine import Model, Input\r\n\r\n## Using Tensorflow as Keras backend.\r\n## Input dtype default is float32\r\n\r\nkwargs={'units': 0}\r\ninput = (10 * np.random.random((1,32,32,16)))\r\nlayer = L.core.Dense(**kwargs)\r\nx = Input(batch_shape=input.shape)\r\ny = layer(x)\r\nbk_model = Model(x, y)\r\nprint('finish')\r\n```\r\n", "comments": ["@shiningrain \r\nThere is a duplicate ticket with same code/issue #37336, could you please confirm if we may close this issue as duplicate as it is been monitored there.", "> @shiningrain\r\n> There is a duplicate ticket with same code/issue #37336, could you please confirm if we may close this issue as duplicate as it is been monitored there.\r\n\r\n Hello, thank you very much for your reply. \r\n Both of this question and [issue 37336](https://github.com/tensorflow/tensorflow/issues/37336)   hope that the problem can be reproduced with the shortest possible code, thus the structure of the codes are similar. **But they reflect totally different problems**.  This issue focuses on **\u2018Tensorflow may lack a detection of improper parameters when establishing `Dense`\u2019**, while the point of [issue 37336](https://github.com/tensorflow/tensorflow/issues/37336) is **\u2018Error reported when building GRU model under certain parameters\u2019**.\r\n  These are two different problems, and the code structure is also designed to **make the problem easier to be analyzed and reproduced**.\r\n     Hope you can understand me, thank you again for your reply ", "Can you please change your imports to `tensorflow.keras` rather than `keras`?\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers, Model\r\n\r\n## Using Tensorflow as Keras backend.\r\n## Input dtype default is float32\r\n\r\nkwargs={'units': 0}\r\ninput = (10 * np.random.random((1,32,32,16)))\r\nlayer = layers.Dense(**kwargs)\r\nx = keras.Input(batch_shape=input.shape)\r\ny = layer(x)\r\nbk_model = Model(x, y)\r\nprint('finish')\r\n```", "> ```python\r\n> import numpy as np\r\n> import tensorflow as tf\r\n> from tensorflow import keras\r\n> from tensorflow.keras import layers, Model\r\n> \r\n> ## Using Tensorflow as Keras backend.\r\n> ## Input dtype default is float32\r\n> \r\n> kwargs={'units': 0}\r\n> input = (10 * np.random.random((1,32,32,16)))\r\n> layer = layers.Dense(**kwargs)\r\n> x = keras.Input(batch_shape=input.shape)\r\n> y = layer(x)\r\n> bk_model = Model(x, y)\r\n> print('finish')\r\n> ```\r\n\r\nThanks for your help!\r\nI have tried your code, and **it can still build and save this dangerous model**, shown in the following picture:\r\n![image](https://user-images.githubusercontent.com/46860123/76579839-a88a4880-6508-11ea-9db0-be4e44b22f7d.png)\r\nThank you again!", "This is fixed with TF 2.4. Thanks!\r\nPlease refer [gist](https://colab.research.google.com/gist/ymodak/b15f37fe712fa2825c96012b8b96f67b/untitled637.ipynb).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37335\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37335\">No</a>\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 37334, "title": "Tensorflow can build and even run a model with `Conv2D('Kernel_size=0' )`", "body": "**System information**  \r\n- Have I written custom code (as opposed to using example directory):  \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Win 10 & Linux Ubuntu18.04\r\n- Tensorflow backend (yes/no): yes\r\n- TensorFlow version:1.15.0(CPU)\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**  \r\nWhen I build a model with unreasonable parameters  `Conv2D(kernel_size=0)` on TensorFlow, **it can run normally and even generate/save an model** . When I use this model to predict, Tensorflow spend about 5 minutes and still can't return an output.\r\n`Conv2D(kernel_size=0)`  seems like a corner case because **in the convolution operation, it is impossible to calculate with `kernel_size=0`**\r\n\r\nDoes `kernel_size=0` have some special meaning in Tensorflow? I have not found any description about this case in documents. If no special meaning, **Should Tensorflow set a check for such unreasonable parameters to avoid the risks and incorrect usages in the model?**  \r\n\r\n**Code to reproduce the issue**  \r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nimport keras.layers as L\r\nfrom keras.models import load_model\r\nfrom keras.engine import Model, Input\r\n\r\nkwargs = {'filters': 19, 'kernel_size': 0, 'padding': 'valid', 'strides': (2, 4), 'dilation_rate': 1, 'data_format': 'channels_first'}\r\ninput = (10 * np.random.random((1,32,32,16)))\r\nlayer = L.convolutional.Conv2D(**kwargs)\r\nx = Input(batch_shape=input.shape)\r\ny = layer(x)\r\nbk_model = Model(x, y)\r\nmodel_path = os.path.join('./', 'model.h5')\r\nbk_model.save(model_path, bk_model)\r\nmodel = load_model(model_path)\r\noutput = model.predict(input)\r\nprint('finish')\r\n```", "comments": ["@shiningrain \r\nI have run the code shared by you and it executes as expected, please find[ gist here](https://colab.sandbox.google.com/gist/Saduf2019/002a441ed92036884b41ae5a9f104210/37334.ipynb)\r\nFor kernel_size related information please refer to [this link](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D#arguments_2), also please refer to this link if it [helps](https://github.com/cwehmeyer/pydpc/issues/2#issuecomment-510912684)", "> @shiningrain\r\n> I have run the code shared by you and it executes as expected, please find[ gist here](https://colab.sandbox.google.com/gist/Saduf2019/002a441ed92036884b41ae5a9f104210/37334.ipynb)\r\n> For kernel_size related information please refer to [this link](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D#arguments_2), also please refer to this link if it [helps](https://github.com/cwehmeyer/pydpc/issues/2#issuecomment-510912684)\r\n\r\nHi!  @Saduf2019 \r\nThanks for your help!\r\nHowever, I still have some problems with your reply.\r\n1. I have read your reproduced code, but the `kernel_size` in it is set to 1, which is not the problem  I want to explain. I have also tested with `kernel_size = 1` and it did work properly. Here I want to focusing on illogical inputs such as `kernel_size = 0.`\r\n2. The description of `kernel_size` in the TensorFlow document you gave just said that **the value can be an integer or an integer tuple**. **The value of 0 does conform to what the document says**, so at least the document is not precise.\r\n3. The [last link](https://github.com/cwehmeyer/pydpc/issues/2) you gave seems to have nothing to do with the convolution operation and does not reflect how TensorFlow handle with the illogical operation such as `kenel_size = 0`.\r\n\r\nTo sum up, I mean that TensorFlow can still generate models and make predictions with illogical parameters such as `kenel_size = 0`. This should be an implementation issue. Developers should consider this situation to check its value or at least set a default value to avoid such problems.\r\n\r\n**Hope to receive your response and thank you very much for your help**", "please find the [gist](https://colab.sandbox.google.com/gist/Saduf2019/04e40b481d05e6a0c2e36ef9c0887ae6/37334.ipynb) for kernel_size=0", "This has been fixed with tf-nightly version.\r\nhttps://github.com/tensorflow/tensorflow/blob/84eb083bb5328912dde064b8b0f61d28c6edbe43/tensorflow/python/keras/layers/convolutional.py#L132\r\ncommit 1e102f63964365d82d7f22402b7ba21e0e0e64fe", "The problem still exists if I use nn module or  other tensorflow clients. e.g node.js,Java, CPP", "@fsx950223 Can you please create a new issue and provide your repro example? Also refer this issue on the new thread. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37334\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37334\">No</a>\n"]}, {"number": 37333, "title": "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed. (Running Adel Wheat))", "body": "`from alinea.adel.astk_interface import AdelWheat\r\nfrom alinea.astk.Weather import sample_weather\r\n\r\n\r\nseq, weather = sample_weather()\r\nwdata = weather.get_weather(seq)\r\n\r\nadel = AdelWheat(nsect=2)\r\n\r\ng = adel.setup_canopy(100)\r\nadel.grow(g, wdata)`\r\n\r\n\r\nGetting the error:\r\n\r\n\r\n(adel) C:\\Users\\Personal\\Desktop\\OpenAlea research\\adel\\example>python test_AdelWheat.py\r\nTraceback (most recent call last):\r\n  File \"test_AdelWheat.py\", line 1, in <module>\r\n    from alinea.adel.astk_interface import AdelWheat\r\n  File \"C:\\Users\\Personal\\Desktop\\OpenAlea research\\adel\\src\\alinea\\adel\\astk_interface.py\", line 6, in <module>\r\n    from alinea.adel.AdelR import setAdel, RunAdel, genGeoAxe, checkAxeDyn, getAxeT, \\\r\n  File \"C:\\Users\\Personal\\Desktop\\OpenAlea research\\adel\\src\\alinea\\adel\\AdelR.py\", line 23, in <module>\r\n    import rpy2.robjects as robj\r\n  File \"C:\\Users\\Personal\\.conda\\envs\\adel\\lib\\site-packages\\rpy2\\robjects\\__init__.py\", line 16, in <module>\r\n    import rpy2.rinterface as rinterface\r\n  File \"C:\\Users\\Personal\\.conda\\envs\\adel\\lib\\site-packages\\rpy2\\rinterface\\__init__.py\", line 92, in <module>\r\n    from rpy2.rinterface._rinterface import (baseenv,\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.", "comments": ["Installing this package  https://github.com/openalea-incubator/adel\r\nand trying to run the above code", "@sujithgunturu \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37333\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37333\">No</a>\n", "I am not working with tensor flow. The issue is with adel wheat which is an other paclage.", "Then this should not be posted in tensorflow repository. Locking conversation as offtopic"]}, {"number": 37332, "title": "TFTRT Int8 calibrate out of memory", "body": "**System information**\r\nCentOS Linux release 7.6.1810 (Core)\r\nTITAN Xp\r\n\r\nCUDA Version: 10.0\r\nCUDNN 7\r\nTensorflow 1.15.0\r\nPython 2.7.5\r\n\r\n**Describe the current behavior**\r\nI use tftrt module to calibrate int8 model. I can convert() the model successfully, but when I do calibrate(), the memory increase crazily and use the whole memory, finally i get error:Calibration failed: Internal: Failed to build TensorRT engine\r\n\r\n**Code to reproduce the issue**\r\n```\r\nconverter = trt.TrtGraphConverter(input_graph_def=original_graph_def,\r\n                                        nodes_blacklist=preserve_nodes,\r\n                                        max_batch_size=1,\r\n                                        max_workspace_size_bytes=(1 << 30) * 4,\r\n                                        precision_mode=\"INT8\",\r\n                                        use_calibration=True)\r\n\r\nnew_graph_def = converter.convert()\r\n\r\nnew_graph_def = converter.calibrate(fetch_names=fetch_names,\r\n                                          num_runs=1,\r\n                                          feed_dict_fn=lambda: feed_dict)\r\n```\r\n\r\n**Other info / logs**\r\nCuda error in file src/implicit_gemm.cu at line 585: out of memory\r\nCuda error in file src/implicit_gemm.cu at line 648: out of memory\r\nCuda error in file src/implicit_gemm.cu at line 585: out of memory\r\nCuda error in file src/implicit_gemm.cu at line 648: out of memory\r\n2020-03-04 23:13:41.703545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-03-04 23:13:42.514483: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger ../builder/cudnnCalibrator.cpp (703) - Cuda Error in add: 2 (out of memory)\r\n2020-03-04 23:13:42.514774: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger FAILED_ALLOCATION: std::exception\r\n2020-03-04 23:13:42.514813: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger ../rtExt/cuda/customWinogradConvActRunner.cpp (330) - Cuda Error in execute: 2 (out of memory)\r\n2020-03-04 23:13:42.514843: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger FAILED_EXECUTION: std::exception\r\n2020-03-04 23:13:42.572025: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-03-04 23:13:43.019708: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:812] Starting calibration thread on device 0, Calibration Resource @ 0x7f6414007820\r\n2020-03-04 23:13:43.021884: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Half2 support requested on hardware without native FP16 support, performance will be negatively affected.\r\n2020-03-04 23:13:43.181361: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger ../rtSafe/safeContext.cpp (105) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)\r\n2020-03-04 23:13:43.182093: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger ../rtSafe/safeContext.cpp (105) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)\r\n2020-03-04 23:13:43.182185: E tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:838] Calibration failed: Internal: Failed to build TensorRT engine\r\n\r\n", "comments": ["@wangxiang2713 \r\n\r\n - Can you please a complete reproducer (something we can copy/paste)?\r\n - Is this fixed on `tf-nightly`?\r\n\r\n", "Hi\uff1a\n    Thankyou for your response, i am sorry i can't send the code because of authority. I am trying to slove this problem with my workmates, i will tell you the reason immediately if i solve this problem.\n\n\n\n\n\n\n\n\n\n\n\nAt 2020-03-07 04:53:39, \"Sanjoy Das\" <notifications@github.com> wrote:\n\n@wangxiang2713\n\nCan you please a complete reproducer (something we can copy/paste)?\nIs this fixed on tf-nightly?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or unsubscribe.", "Have you solved the problem? I have the same problem\r\n`safeContext.cpp (105) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)\r\n`", "@only-yao I had a similar issue in PyTorch where I'm using nvidia ngc image 20.03, I had to copy libcudnn.so from `/usr/lib/x86_64-linux-gnu/libcudnn.so` to `/usr/local/cuda/targets/x86_64-linux/lib` by creating a symlink", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37332\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37332\">No</a>\n", "@wangxiang2713\r\nAnyone solved this problem?"]}, {"number": 37331, "title": "Headers in /usr/local/include cause build errors on json cpp", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.15\r\n- Python version: 3.7.5\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): Apple LLVM 11.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n`jsoncpp` build is not hermetic and uses non-system headers in `/usr/local/include` leading to a build failure if a conflicting version is installed.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nMy system has version 1.9.1 of `jsoncpp` installed using homebrew. Tensorflow tries to build 1.8.4 when building from source. By default all libraries in `/usr/local/include` are on the include path. This then triggers a bug in the 1.8.4 code-base which was fixed in this [commit](https://github.com/open-source-parsers/jsoncpp/commit/83cc92161be96d7f2113a3ec9f62109d828e1eec).\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nERROR: /private/var/tmp/_bazel_cjuvekar/e56ef34a0d740df121c6d8bab70f0129/external/jsoncpp_git/BUILD.bazel:5:1: C++ compilation of rule '@jsoncpp_git//:jsoncpp' failed (Exit 1): cc_wrapper.sh failed: error executing command\r\n  (cd /private/var/tmp/_bazel_cjuvekar/e56ef34a0d740df121c6d8bab70f0129/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/Users/cjuvekar/anaconda3/envs/tf1_15_custom/bin:/Users/cjuvekar/anaconda3/condabin:/Users/cjuvekar/local/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/Users/cjuvekar/.fzf/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/external/jsoncpp_git/_objs/jsoncpp/json_value.pic.d '-frandom-seed=bazel-out/host/bin/external/jsoncpp_git/_objs/jsoncpp/json_value.pic.o' -fPIC -iquote external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -isystem external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -g0 '-march=native' -g0 '-DJSON_USE_EXCEPTION=0' -DJSON_HAS_INT64 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/jsoncpp_git/src/lib_json/json_value.cpp -o bazel-out/host/bin/external/jsoncpp_git/_objs/jsoncpp/json_value.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nIn file included from external/jsoncpp_git/src/lib_json/json_value.cpp:7:\r\nIn file included from /usr/local/include/json/assertions.h:13:\r\n/usr/local/include/json/config.h:155:9: warning: 'JSON_HAS_INT64' macro redefined [-Wmacro-redefined]\r\n#define JSON_HAS_INT64\r\n        ^\r\n<command line>:5:9: note: previous definition is here\r\n#define JSON_HAS_INT64 1\r\n        ^\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:193:1: error: use of undeclared identifier 'Exception'; did you mean 'std::exception'?\r\nException::Exception(JSONCPP_STRING const& msg)\r\n^~~~~~~~~\r\nstd::exception\r\n/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/exception:98:29: note: 'std::exception' declared here\r\nclass _LIBCPP_EXCEPTION_ABI exception\r\n                            ^\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:193:12: error: missing return type for function 'Exception'; did you mean the constructor name 'exception'?\r\nException::Exception(JSONCPP_STRING const& msg)\r\n           ^~~~~~~~~\r\n           exception\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:193:12: error: cannot define or redeclare 'exception' here because namespace 'Json' does not enclose namespace 'exception'\r\nException::Exception(JSONCPP_STRING const& msg)\r\n~~~~~~~~~~~^\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:194:10: error: use of undeclared identifier 'msg'\r\n  : msg_(msg)\r\n         ^\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:196:1: error: use of undeclared identifier 'Exception'; did you mean 'std::exception'?\r\nException::~Exception() JSONCPP_NOEXCEPT\r\n^~~~~~~~~\r\nstd::exception\r\n/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/exception:98:29: note: 'std::exception' declared here\r\nclass _LIBCPP_EXCEPTION_ABI exception\r\n                            ^\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:196:13: error: expected the class name after '~' to name a destructor\r\nException::~Exception() JSONCPP_NOEXCEPT\r\n            ^~~~~~~~~\r\n            exception\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:198:13: error: use of undeclared identifier 'Exception'; did you mean 'std::exception'?\r\nchar const* Exception::what() const JSONCPP_NOEXCEPT\r\n            ^~~~~~~~~\r\n            std::exception\r\n/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/exception:98:29: note: 'std::exception' declared here\r\nclass _LIBCPP_EXCEPTION_ABI exception\r\n                            ^\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:198:24: error: cannot define or redeclare 'what' here because namespace 'Json' does not enclose namespace 'exception'\r\nchar const* Exception::what() const JSONCPP_NOEXCEPT\r\n            ~~~~~~~~~~~^\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:202:1: error: use of undeclared identifier 'RuntimeError'; did you mean 'std::runtime_error'?\r\nRuntimeError::RuntimeError(JSONCPP_STRING const& msg)\r\n^~~~~~~~~~~~\r\nstd::runtime_error\r\n/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/stdexcept:94:29: note: 'std::runtime_error' declared here\r\nclass _LIBCPP_EXCEPTION_ABI runtime_error\r\n                            ^\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:202:15: error: missing return type for function 'RuntimeError'; did you mean the constructor name 'runtime_error'?\r\nRuntimeError::RuntimeError(JSONCPP_STRING const& msg)\r\n              ^~~~~~~~~~~~\r\n              runtime_error\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:202:15: error: cannot define or redeclare 'runtime_error' here because namespace 'Json' does not enclose namespace 'runtime_error'\r\nRuntimeError::RuntimeError(JSONCPP_STRING const& msg)\r\n~~~~~~~~~~~~~~^\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:203:15: error: use of undeclared identifier 'msg'\r\n  : Exception(msg)\r\n              ^\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:205:1: error: use of undeclared identifier 'LogicError'\r\nLogicError::LogicError(JSONCPP_STRING const& msg)\r\n^\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:208:1: error: unknown type name 'JSONCPP_NORETURN'\r\nJSONCPP_NORETURN void throwRuntimeError(JSONCPP_STRING const& msg)\r\n^\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:208:18: error: expected unqualified-id\r\nJSONCPP_NORETURN void throwRuntimeError(JSONCPP_STRING const& msg)\r\n                 ^\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:212:1: error: unknown type name 'JSONCPP_NORETURN'\r\nJSONCPP_NORETURN void throwLogicError(JSONCPP_STRING const& msg)\r\n^\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:212:18: error: expected unqualified-id\r\nJSONCPP_NORETURN void throwLogicError(JSONCPP_STRING const& msg)\r\n                 ^\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:225:8: error: no member named 'CommentInfo' in 'Json::Value'\r\nValue::CommentInfo::CommentInfo() : comment_(0)\r\n~~~~~~~^\r\nexternal/jsoncpp_git/src/lib_json/json_value.cpp:228:8: error: no member named 'CommentInfo' in 'Json::Value'\r\nValue::CommentInfo::~CommentInfo() {\r\n~~~~~~~^\r\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n1 warning and 20 errors generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 374.433s, Critical Path: 113.24s\r\nINFO: 1577 processes: 1577 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37331\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37331\">No</a>\n"]}, {"number": 37330, "title": "Improve the TFDS Getting Started Documentation ", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/datasets/overview\r\n\r\n## Description of issue (what needs changing):\r\n\r\n \ud83d\ude00I completed step 1 and  went to \u201chttps://www.tensorflow.org/datasets/overview\u201d to get started with TFDS. I launched the code lab to continue with the overview. The code lab is a great option to easily run python and tensorflow! \r\n\r\n\ud83d\ude11- I completed the first command to install tensorflow and tensorflow-datasets\r\n![image](https://user-images.githubusercontent.com/6283150/75946296-9433b500-5e51-11ea-921e-f44a6fc1573e.png)\r\n\r\nThe download ran but it was not clear which version of tensorflow was downloaded. The reason I was confused and wanted to know which version was installed is the disclaimer above states version >=1.15 is required.\r\n\ud83d\ude11- In the second command, I received an error message after running the python script.\r\n![image](https://user-images.githubusercontent.com/6283150/75946322-a44b9480-5e51-11ea-8874-51a4863093ae.png)\r\n\r\nI was not clear if this was just a warning message, or an error due to my current tensorflow version. \r\n\r\n\ud83d\ude00Step 2 was delightful!\r\n![image](https://user-images.githubusercontent.com/6283150/75946336-aca3cf80-5e51-11ea-949a-0ca03348d768.png)\r\n\r\nAdding in a disclaimer to include citations is great! \ud83d\ude11However, why is it after the download step? This seems out of place and disrupts the developer workflow. \r\n\r\nNext was step 3 to initiate eager execution.\r\n\ud83d\ude11Without a baseline on what EE is, I felt required to read the eager execution page before I could move forward. It\u2019s frustrating when a developer guide links out to other documentation, or I feel compelled to read the other pages, because it causes disruption in grasping one concept at a time. This frustration can be a \u201cdrop off\u201d point for developers trying to onboard Tensorflow.\r\n\r\n![image](https://user-images.githubusercontent.com/6283150/75946528-35227000-5e52-11ea-9312-09405b8e3043.png)\r\n\r\nEnable_V2_Behavior is the command run after asking the user to enable eager execution. Why is that? (After reading the eager execution documentation this was clear, but it took time to dig for this info).\r\n\r\n\ud83d\ude21Step 5 understanding what the tf \u201cload\u201d function does is frustrating.\r\n![image](https://user-images.githubusercontent.com/6283150/75946560-44092280-5e52-11ea-85be-ded98f6365c3.png)\r\n\r\n\ud83d\ude21I\u2019m strongly encouraged to read the official TensorFlow guide which is over 30 pages of material. I am 5 steps down this getting started guide, and then sent to another page that will reasonably take 4+ focused hours to additionally complete. This is very frustrating when I am trying to just get an overview of tensorflow datasets.\r\n\r\n\ud83d\ude00Step 5 does a great job here showing an example directly in relation to the above paragraph on versioning! I\u2019m delighted and can move on without needing to read the hyperlink. \r\n![image](https://user-images.githubusercontent.com/6283150/75946588-54b99880-5e52-11ea-8e80-6ec1f06b684e.png)\r\n\r\n\ud83d\ude11Step 7 is confusing since it states we can achieve the same output using the DatasetBuilder, but when you run the test it only outputs the ds_train variable, as opposed to building the graph. \r\n![image](https://user-images.githubusercontent.com/6283150/75946599-5be0a680-5e52-11ea-9cfd-37520abadae3.png)\r\n\r\n### What should happen?\r\n\r\nI have organized answers to the above friction points in the following groupings:\r\n\r\n**Tensorflow Installation**\r\nTo identify which version of Tensorflow I installed I ran a grep command in the code lab to output the following:\r\n\r\n![image](https://user-images.githubusercontent.com/6283150/75946716-bd087a00-5e52-11ea-95d3-c13f68c33df9.png)\r\n\r\nHaving something like this ^ output during installation will help users know what is downloaded and executed in the install command. \r\n\r\n**Eager Execution**\r\n\r\nA simple way to clarify what eager execution is to write a one sentence definition in the guide. For example:\r\n\r\n\u201cTensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later.\u201d\r\n\r\nThis way I have a quick understanding and don\u2019t feel compelled to read the linked page which is a very long document! :)\r\nI liken this to applications having a tooltip in consumer facing applications. Adding in quick non intrusive explanations to Respect the User keep your users engaged and on the same page. \r\n\r\nAdditionally, adding in the following message to define the command, \u201cenable_v2_behavior\u201d, would help clarify that Eager Execution is enabled by default tensorflow 2. \r\n\r\n![image](https://user-images.githubusercontent.com/6283150/75946725-c560b500-5e52-11ea-9229-96f8b71dc895.png)\r\n\r\n\r\n**Linking to the official guide for Tensorflow Datasets**\r\n![image](https://user-images.githubusercontent.com/6283150/75946747-d3163a80-5e52-11ea-820b-a43d13bbbe76.png)\r\n\r\nWe need to Respect the User, and provide simplicity when on boarding someone new to TFDS. They have invested time to make it down to the 5th step. If it is imperative the user get a baseline understanding of the Tensorflow API first, then we should put the disclaimer at the top of the overview to go read the guide first before continuing. \r\n\r\nIf it is not necessary, then we should summarize the API guide into 3-5 concise pillars of information that is required for the user to understand the rest of the overview. When the user completes the overview, we can encourage them to go deeper and read the rest of the guide. Similarly, an analogy is when loading a website you respect the user by building a light-weight modern site. Performant sites lazy load in images when they are needed to improve performance and minimize how much data your user needs to download, we should apply the same principles to information.  \r\n\r\n**DatasetBuilder**\r\nWhen introducing in the DatasetBuilder we should place this information right after Step 5 (calling .load), to show the two ways to load in datasets side by side. This way the user does not need to scroll back up the documentation and read before Step 6 (plotting the dataset). \r\n\r\n\r\n\r\n", "comments": ["This was semi-recently updated. Can you check again, please? https://www.tensorflow.org/datasets/overview", "Also, that doc lives here: https://github.com/tensorflow/datasets/blob/master/docs/overview.ipynb\r\nYou can send a pull request or issue to the tensorflow/datasets repo. Thanks", "@coreyching  and @lamberta , I have raised PR [#1633](https://github.com/tensorflow/datasets/pull/1633) to resolve this issue. Please check PR and also suggest changes if needed.", "Thanks again for all the valuable feedbacks. For reference, we've done a major update of the documentation introduction.\r\nhttps://www.tensorflow.org/datasets/overview\r\n\r\nI think it fixes many of the pain points highlighted here. Let us know if there are other things which should be fixed. We're also in the process of refactoring the other doc (adding a new dataset & split API).\r\n\r\nI believe this issue can be closed", "Closing this issue based on the above comments and changes to the datasets overview page. For any additional suggestions, please open a new issue."]}, {"number": 37329, "title": "Categorical Hinge Loss Doc Updated", "body": "Issue : #36807  @pavithrasv Check this", "comments": ["I updated it.", "Thanks for the approval"]}, {"number": 37328, "title": "Categorical Hinge loss doc updated", "body": "Issue: #36807 \r\n\r\n@pavithrasv  Check this.", "comments": []}, {"number": 37327, "title": "Broken Link Inside the Friction Log Google Doc", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue / ## Description of issue (what needs changing):\r\n\r\nInside the Friction Log Document: https://docs.google.com/document/d/1HVG3t-mgGZKU4iMeguTWGejbnQ54qUTXwdCFkA5xHG0/edit\r\n\r\nThere is a broken link to the \"Bug / Performance\" template:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/.github/ISSUE_TEMPLATE/00-bug-performance-issue.md\r\n\r\n### Correct links\r\n\r\nI see there are two templates - \r\n**Performance:** https://github.com/tensorflow/tensorflow/issues/new?labels=type%3Aperformance&template=80-performance-issue.md\r\n\r\n**Bug**: https://github.com/tensorflow/tensorflow/issues/new?labels=type%3Abug&template=00-bug-issue.md\r\n\r\nThe friction log should update these links in the google doc, \r\n\r\n", "comments": ["@coreyching \r\nplease share the source of this document, the link from tensorflow or github where this document is present.", "@coreyching \r\nplease confirm if the source is from [this link](\r\nhttps://github.com/tensorflow/community/blob/master/sigs/build/tensorflow-testing.md)", "@coreyching\r\nplease update on the above comment.", "@coreyching\r\nAs there is a pr to monitor your request, moving this issue to closed status"]}, {"number": 37326, "title": "[Intel MKL] Fixing build error", "body": "This is just fixing build error caused by this commit https://github.com/tensorflow/tensorflow/commit/8f7e34982dde766b3fc73c90bcdbfccc001fe8e3", "comments": ["Hi @penpornk, this is build failure fix PR. Pls take a look. Thanks.", "Thanks for quick review @penpornk!"]}, {"number": 37325, "title": "tf.ConfigProto() and tf.estimator.RunConfig  conflict", "body": "I want to use \u201ctf.ConfigProto()\u201d to set the GPU to grow dynamically, and I want to use \u201c tf.estimator.RunConfig \u201d to set the number of checkpoints.\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\nconfig = tf.estimator.RunConfig(keep_checkpoint_max = 10)\r\nwith tf.train.MonitoredTrainingSession(,,,config=config)\r\nHow can I achieve my goal with only one config\uff1f\r\n\r\n\r\nextremely grateful !\r\nThanks!", "comments": ["\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\nPlease, fill [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Thanks!\r\n\r\n\r\n", "TensorFlow version: 1.10\r\noperating system: linux, centos7\r\narchitecture: multiple GPUs on a single machine, Use parameter server in the future", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n"]}, {"number": 37324, "title": "Cannot break while iterating over tensorflow dataset created using `tf.data.Dataset.from_generator`", "body": "\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\nYes\r\n- OS Platform and Distribution (e.g.,\r\nWindows 10\r\n- TensorFlow installed from \r\nbinary\r\n- Tensorflow Version:\r\n2.1\r\n\r\n**Describe the current behavior**\r\nThe `tensorflow` dataset created from the generator function cannot be used with `break` inside the for loop. On the other hand, the datasets created without a generator do break gracefully. The error `tf.errors.CancelledError` is raised because of cancellation i.e. there is a `break` statement inside for loop.\r\n\r\n**Describe the expected behavior**\r\nThe tensorflow dataset created using from_generator should exit gracefully when we break the for loop.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nSee the code below:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef gen():\r\n    while True:\r\n        yield np.zeros(10, dtype=np.int32)\r\n\r\n\r\nds_without_gen = tf.data.Dataset.from_tensor_slices(\r\n    [np.zeros(10, dtype=np.int32)]\r\n).repeat()\r\n\r\nds_from_gen = tf.data.Dataset.from_generator(\r\n    generator=gen, output_types=np.int32\r\n)\r\n\r\n\r\n# this works normally with break statement\r\nprint(\">>>>>>>>> ds_without_gen\")\r\nfor r in ds_without_gen.as_numpy_iterator():\r\n    print(r)\r\n    break\r\n\r\n\r\n# this does work but a error is thrown by tensorflow as below\r\n#    W tensorflow/core/kernels/data/generator_dataset_op.cc:103]\r\n#    Error occurred when finalizing GeneratorDataset iterator:\r\n#    Cancelled: Operation was cancelled\r\nprint(\">>>>>>>>> ds_from_gen\")\r\nfor r in ds_from_gen.as_numpy_iterator():\r\n    print(r)\r\n    break\r\n\r\n```\r\n\r\nThe output is:\r\n```txt\r\n2020-03-04 17:39:29.472521: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-03-04 17:39:32.463827: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\r\n2020-03-04 17:39:32.464110: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-03-04 17:39:32.469124: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: NXL23343\r\n2020-03-04 17:39:32.469517: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: NXL23343\r\n2020-03-04 17:39:32.469969: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n ds_without_gen\r\n[0 0 0 0 0 0 0 0 0 0]\r\n ds_from_gen\r\n[0 0 0 0 0 0 0 0 0 0]\r\n2020-03-04 17:39:32.528149: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n```\r\n\r\n\r\n", "comments": ["@SpikingNeuron,\r\nI tried to reproduce the issue but did not face any warnings and errors. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/af1ab144bf9ba0a7ce77b7f0c8967199/37324.ipynb). Thanks!", "@amahendrakar Please try in command prompt or terminal with local python installation. The jupyter-notebook might be changing the behavior of the debugger.\r\n\r\nThis is the error I get\r\n```txt\r\nW tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n```\r\nI realized it is not an error but a warning. The message indicates it as error but the `W` tag at the start indicates that it is a warning.\r\n\r\nI assume if I ignore the warning I will be safe. \r\n\r\nIs this warning a critical thing? Should I avoid using break??", "@SpikingNeuron,\r\nRunning it on a local machine gives the same output. Please find the screenshot attached to this message.\r\n![37324 output](https://user-images.githubusercontent.com/57165142/76093385-e22df180-5fe6-11ea-8108-27e6d22d7f38.png)\r\n \r\nWarnings are generally used to alerts programmers of future deprecation or obsolete methods. In this case, unless you face errors you can continue using break as it is just a warning.", "Yes I noticed it late that it is warning as the message clearly says that it is an error but the start tag is `W`"]}, {"number": 37323, "title": "Count number of times `tf.function` is traced", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI would like to be able to check, inside a function wrapped by `tf.function`, when this function is being retraced. This is useful for testing my functions are compatible with `tf.function`, and will make use of the speed up (and not just make a new graph every time they are called).\r\n\r\nAn example, with the requested feature would be inserted in `REQUEST`:\r\n\r\n```\r\nimport my_function\r\n@pytest.mark.unit\r\n@pytest.mark.parametrize('use_tf_function', [True, False])\r\ndef test_my_function(use_tf_function):\r\n\r\n    def test(arg1, arg2):\r\n        # REQUEST: Check to make sure this is not being retraced many times.\r\n        return my_function(arg1, arg2)\r\n\r\n    # Sometimes test in Eager mode for debugging, sometimes test in graph mode.\r\n    test_func = test\r\n    if use_tf_function:\r\n        test_func = tf.function(test_func)\r\n\r\n    #####Test 1\r\n    arg1, arg2 = #some setup stuff\r\n    test_func(arg1, arg2)  # create the graph (tracing happens).\r\n    results = test_func(arg1, arg2)  # hopefully tracing does not happen a second time.\r\n    assert results\r\n\r\n    #####Test 2\r\n    arg4, arg3 = #some setup stuff\r\n    results = test_func(arg3, arg4)  # hopefully tracing does not happen again (if the inputs are tensors and the shapes do not change)\r\n    assert results\r\n```\r\n\r\n\r\n**Will this change the current api? How?**\r\nI'm not sure, maybe just add a function, maybe it already exists.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone debugging. \r\n\r\n**Any Other info.**\r\n", "comments": ["One thing I could do is check `test_func._call_counter.get_tracing_count()`, but I don't like the use of a variable with `_` starting the name.  Is there a more visible function for doing this?", "Indeed, you should avoid using private fields - they may change at any moment.\r\n\r\nThe easiest and most robust way is to use a global Python counter, like so:\r\n\r\n```\r\n>>> trace_count = 0\r\n>>> @tf.function\r\n>>> def f(x):\r\n...   global trace_count\r\n...   trace_count += 1\r\n...   # the rest of your code\r\n>>> trace_count\r\n0\r\n>>> f(tf.constant(1))\r\n>>> trace_count\r\n1\r\n>>> f(tf.constant(2))\r\n>>> trace_count\r\n1\r\n>>> f(tf.constant([1, 2]))\r\n>>> trace_count\r\n2\r\n```\r\n", "This only works for certain cases, and doesn't seem scalable to every function everywhere. I'd have to have a different global variable for each function. \r\n\r\nWhat if my function is written in one file, and I wrap it in `tf.function` in another file? I can't use a global variable as descirbed above to test the trace count in the caller code, which is where I would like to test.", "I missed that this was a feature request. Sounds reasonable to me.", "Any suggestions on how this can be implemented? I would like to give a try if we have any pointers", "A simple solution would be to just add a public method to the `Function` class, that returns `self._call_counter.get_tracing_count()`.\r\n\r\n I'm not sure of the implications of this though, as `_call_counter` is a private field.", "Fields are kept private by default, and we only expose public methods if there is a need for one. This helps keep the API clean - once a method is made public, it's extremely difficult to remove it. So for now, I recommend naming the method/property with the `experimental_` prefix to control its adoption.", "I'd like to give this a shot. Can I submit a PR for this?", "@ammirato @mdanatg Thanks. Will work on this.", "can i work on this? @ammirato ", "@ammirato, can I work on this and submit a PR", "How about a static variable in the function class?", "import my_function\r\n@pytest.mark.unit\r\n@pytest.mark.parametrize('use_tf_function', [True, False])\r\ndef test_my_function(use_tf_function):\r\n\r\n    def test(arg1, arg2):\r\n        # REQUEST: Check to make sure this is not being retraced many times.\r\n        return my_function(arg1, arg2)\r\n\r\n    # Sometimes test in Eager mode for debugging, sometimes test in graph mode.\r\n    test_func = test\r\n    if use_tf_function:\r\n        test_func = tf.function(test_func)\r\n\r\n    #####Test 1\r\n    arg1, arg2 = #some setup stuff\r\n    test_func(arg1, arg2)  # create the graph (tracing happens).\r\n    results = test_func(arg1, arg2)  # hopefully tracing does not happen a second time.\r\n    assert results\r\n\r\n    #####Test 2\r\n    arg4, arg3 = #some setup stuff\r\n    results = test_func(arg3, arg4)  # hopefully tracing does not happen again (if the inputs are tensors and the shapes do not change)\r\n    assert results\r\n#is it right....?", "Is this issue closed? or is it open for contributions? I see that the PR is closed. Thanks", "PR has been closed in unmerged state", "If I got the issue correctly you want to save the state of your graph so you can use [lru cache](https://docs.python.org/3/library/functools.html#lru_cache), I tested both inside tensorflow library and only combination of @tf.function and @lru_cache in my main.py and both worked fine but actually i was testing a simple graph with one plus, I don't know the side effects", "Is this feature still needed? I can see previous PR was closed in an unmerged state. Thanks.", "if hv mentioned the right one......", "HI I am new to the open source community ,\r\nI would like to contribute to this project,\r\ncan anyone please point me to the right direction to contribute to this project\r\n\r\n", "> HI I am new to the open source community ,\r\n> I would like to contribute to this project,\r\n> can anyone please point me to the right direction to contribute to this project\r\n\r\nI too am new... can anyone help by letting us know on how to pick issues to work on and start contributing? I mean can multiple people work on the same issue or does someone lock or book an issue to start work.", "Hi is this issue still open? I want to work on it.", "Looks like there's a PR in progress.", "I am new to this can anyone tell me from where to begin.", "This is my first issue... I am working on it...if anyone got some information related to this issue please let me know ", "is anyone working on this issue? @mihaimaruseac @ammirato ", "There is #43366", "hi  i would like to work on the issue , can you  suggest me ways to get started", "Have you fixed the issue?\r\n\r\nI think, you should try that public method to the Function class.\r\n\r\nself._`call_counter.get_tracing_count()`", "#43366 landed, so this is fixed."]}, {"number": 37322, "title": "[r2.2:Cherrypick] Export symbols with the characteristic `tf_` like `tf_git_version`.", "body": "PiperOrigin-RevId: 298637569\nChange-Id: I5a872a6e780c35b984e4c91d4e8ff133ad7d9bf0", "comments": []}, {"number": 37321, "title": "Update version numbers for TensorFlow 2.2.0-rc0", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 1 -> 2\nPatch: 0 -> 0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.1.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/security/advisory/tfsa-2020-001.md:39:2.1.0\ntensorflow/security/advisory/tfsa-2020-001.md:41:2.1.0\ntensorflow/tools/ci_build/release/ubuntu_16/custom_op/release.sh:20:2.1.0\ntensorflow/tools/pip_package/setup.py:64:2.1.0\ntensorflow/tools/pip_package/setup.py:65:2.1.0\nBinary file \ntensorflow/cc/saved_model/testdata/VarsAndArithmeticObjectGraph/saved_model.pb \nmatches\nBinary file tensorflow/cc/saved_model/testdata/CyclicModule/saved_model.pb \nmatches\ntensorflow/lite/toco/tflite/op_version_test.cc:148:2.1.0\ntensorflow/lite/toco/tflite/op_version.cc:77:2.1.0\ntensorflow/lite/toco/tflite/op_version.cc:85:2.1.0\ntensorflow/lite/toco/tflite/op_version.cc:151:2.1.0\ntensorflow/lite/toco/tflite/op_version.cc:215:2.1.0\ntensorflow/lite/micro/tools/make/templates/library.properties:2:2.1.0\ntensorflow/lite/g3doc/guide/python.md:36:2.1.0\ntensorflow/lite/g3doc/guide/python.md:44:2.1.0\ntensorflow/lite/g3doc/guide/python.md:49:2.1.0\ntensorflow/lite/g3doc/guide/python.md:54:2.1.0\ntensorflow/lite/g3doc/guide/python.md:59:2.1.0\ntensorflow/lite/g3doc/guide/python.md:64:2.1.0\ntensorflow/lite/g3doc/guide/python.md:69:2.1.0\ntensorflow/lite/g3doc/guide/python.md:74:2.1.0\ntensorflow/lite/g3doc/guide/python.md:79:2.1.0\ntensorflow/lite/g3doc/guide/python.md:84:2.1.0\ntensorflow/lite/g3doc/guide/python.md:90:2.1.0\ntensorflow/lite/g3doc/guide/python.md:95:2.1.0\ntensorflow/lite/g3doc/guide/python.md:100:2.1.0\ntensorflow/lite/g3doc/guide/python.md:106:2.1.0\ntensorflow/lite/g3doc/guide/python.md:111:2.1.0\ntensorflow/lite/g3doc/guide/python.md:116:2.1.0\ntensorflow/lite/experimental/ios/TensorFlowLiteC.podspec:3:2.1.0\ntensorflow/lite/experimental/objc/TensorFlowLiteObjC.podspec:3:2.1.0\ntensorflow/lite/experimental/swift/TensorFlowLiteSwift.podspec:3:2.1.0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.1.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/security/advisory/tfsa-2020-001.md:39:2.1.0\ntensorflow/security/advisory/tfsa-2020-001.md:41:2.1.0\ntensorflow/tools/ci_build/release/ubuntu_16/custom_op/release.sh:20:2.1.0\ntensorflow/tools/pip_package/setup.py:64:2.1.0\ntensorflow/tools/pip_package/setup.py:65:2.1.0\nBinary file \ntensorflow/cc/saved_model/testdata/VarsAndArithmeticObjectGraph/saved_model.pb \nmatches\nBinary file tensorflow/cc/saved_model/testdata/CyclicModule/saved_model.pb \nmatches\ntensorflow/lite/toco/tflite/op_version_test.cc:148:2.1.0\ntensorflow/lite/toco/tflite/op_version.cc:77:2.1.0\ntensorflow/lite/toco/tflite/op_version.cc:85:2.1.0\ntensorflow/lite/toco/tflite/op_version.cc:151:2.1.0\ntensorflow/lite/toco/tflite/op_version.cc:215:2.1.0\ntensorflow/lite/micro/tools/make/templates/library.properties:2:2.1.0\ntensorflow/lite/g3doc/guide/python.md:36:2.1.0\ntensorflow/lite/g3doc/guide/python.md:44:2.1.0\ntensorflow/lite/g3doc/guide/python.md:49:2.1.0\ntensorflow/lite/g3doc/guide/python.md:54:2.1.0\ntensorflow/lite/g3doc/guide/python.md:59:2.1.0\ntensorflow/lite/g3doc/guide/python.md:64:2.1.0\ntensorflow/lite/g3doc/guide/python.md:69:2.1.0\ntensorflow/lite/g3doc/guide/python.md:74:2.1.0\ntensorflow/lite/g3doc/guide/python.md:79:2.1.0\ntensorflow/lite/g3doc/guide/python.md:84:2.1.0\ntensorflow/lite/g3doc/guide/python.md:90:2.1.0\ntensorflow/lite/g3doc/guide/python.md:95:2.1.0\ntensorflow/lite/g3doc/guide/python.md:100:2.1.0\ntensorflow/lite/g3doc/guide/python.md:106:2.1.0\ntensorflow/lite/g3doc/guide/python.md:111:2.1.0\ntensorflow/lite/g3doc/guide/python.md:116:2.1.0\ntensorflow/lite/experimental/ios/TensorFlowLiteC.podspec:3:2.1.0\ntensorflow/lite/experimental/objc/TensorFlowLiteObjC.podspec:3:2.1.0\ntensorflow/lite/experimental/swift/TensorFlowLiteSwift.podspec:3:2.1.0\n\nNo lingering old version strings \"r2.1\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 37320, "title": "Compilation error:  reduction_ops_gpu_complex64.cu.pic.o' was not created", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version:1.5\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source):0.9\r\n- GCC/Compiler version (if compiling from source):9.0\r\n- CUDA/cuDNN version:9.2/7.2.1\r\n- GPU model and memory:GTX1070\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n```\r\nERROR: /Users/john/Downloads/tensorflow/tensorflow/core/kernels/BUILD:2709:1: output 'tensorflow/core/kernels/_objs/reduction_ops_gpu/tensorflow/core/kernels/reduction_ops_gpu_complex64.cu.pic.o' was not created\r\nERROR: /Users/john/Downloads/tensorflow/tensorflow/core/kernels/BUILD:2709:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2120.730s, Critical Path: 166.64s\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@zhangwujun666 \r\nProvide the exact sequence of commands / steps that you executed before running into the problem, could you please confirm if the tensorflow version is 1.15 or 1.5", "Hi, there, I just followed this instruction to implement the installation:\r\n\r\n> https://gist.github.com/geekcui/d0800d62e377e0103b1cc2b889ed1128\r\n\r\nand the version of Tensorflow is following:\r\n`git` clone https://github.com/tensorflow/tensorflow.git -b v1.5.0\r\nAfter I did this command to build the file:\r\n`bazel build --config=cuda --config=opt --action_env PATH --action_env LD_LIBRARY_PATH --action_env DYLD_LIBRARY_PATH //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nthe Error shows:\r\n\r\n```\r\nERROR: /Users/john/Downloads/tensorflow/tensorflow/core/kernels/BUILD:2709:1: output 'tensorflow/core/kernels/_objs/reduction_ops_gpu/tensorflow/core/kernels/reduction_ops_gpu_complex64.cu.pic.o' was not created\r\nERROR: /Users/john/Downloads/tensorflow/tensorflow/core/kernels/BUILD:2709:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2120.730s, Critical Path: 166.64s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "@zhangwujun666 \r\nI see an old issue with similar pattern as it has the similar configuration as you have mentioned, could you please downgrade your CUDA version and try. please refer to [this link](https://github.com/tensorflow/tensorflow/issues/19646) and let us know if it helps.", "Thanks!  works well for me !!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37320\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37320\">No</a>\n"]}, {"number": 37319, "title": "Cherrypicks 19 bjr", "body": "", "comments": []}, {"number": 37318, "title": "TPU software version nightly-2.x now gone and other versions don't work", "body": "**System information**\r\n\r\nHave I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Debian GNU/Linux 9.11\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: N/A\r\nTensorFlow installed from (source or\r\nbinary): binary\r\nTensorFlow version (use command below): 2.2.0.dev20200303\r\nPython version: Python 3.7.6\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from\r\nsource): N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\n\r\n**Describe the problem**\r\n\r\nAs of about 12PM EST today it seems that \"nightly-2.x\" for the TPU software version is no longer available under \"Create a Cloud TPU.\" Unfortunately, this is the only version of the software which worked for my training code, and now nothing works. \r\n\r\nI also tried version 2.1, and I get this error: \r\n\r\n```tensorflow.python.framework.errors_impl.NotFoundError: 'ParallelInterleaveDatasetV3' is neither a type of a primitive operation nor a name of a function registered in binary running on n-de18ad71-w-0. One possible root cause is the client and server binaries are not built with the same version. Please make sure the operation or function is registered in the binary running in this process. [Op:DeleteRandomSeedGenerator]```\r\n\r\nTo be clear, I'm calling .interleave() in tf.data, not parallel_interleave, but I suppose it's really running ParallelInterleaveDatasetV3 under the hood.\r\n\r\nCan you let me know what version of the TPU software we should be using to have the same functionality as the old nightly-2.x?\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37318\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37318\">No</a>\n", "You should use this version of tensorflow as shown [here](https://github.com/tensorflow/tensorflow/tree/r2.2/tensorflow/core/ops/compat/ops_history_v1). Thanks!", "@mathemakitten Did it resolve your issue?", "Hi! I wound up flipping the TPU version over to the new nightly and it seems to work, thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37318\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37318\">No</a>\n"]}, {"number": 37317, "title": "tf-nightly-gpu 1.15.0.dev20190729 is not found", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colabs\r\n- TensorFlow version: 1.15.0.dev20190729\r\n\r\n\r\n**Describe the problem**\r\n\r\nThe tf-nightly-gpu==1.15.0.dev20190729 build appears to be missing.  We are releasing our tutorials next week for the summit using this build as listed in google colabs, but we can no longer install. Errors are listed below.  \r\n\r\nExample is here: \r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech/train_speech_model.ipynb\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nThe command in the google colab is as follows: \r\n# Replace Colab's default TensorFlow install with a more recent\r\n# build that contains the operations that are needed for training\r\n!pip uninstall -y tensorflow tensorflow_estimator tensorboard\r\n!pip install -q tf-estimator-nightly==1.14.0.dev2019072901 tf-nightly-gpu==1.15.0.dev20190729\r\n\r\nOutput is: \r\nERROR: Could not find a version that satisfies the requirement tf-nightly-gpu==1.15.0.dev20190729 (from versions: 2.2.0.dev20200201, 2.2.0.dev20200202, 2.2.0.dev20200203, 2.2.0.dev20200204, 2.2.0.dev20200205, 2.2.0.dev20200206, 2.2.0.dev20200207, 2.2.0.dev20200208, 2.2.0.dev20200210, 2.2.0.dev20200211, 2.2.0.dev20200212, 2.2.0.dev20200215, 2.2.0.dev20200216, 2.2.0.dev20200217, 2.2.0.dev20200218, 2.2.0.dev20200226, 2.2.0.dev20200227, 2.2.0.dev20200228, 2.2.0.dev20200229, 2.2.0.dev20200301, 2.2.0.dev20200302, 2.2.0.dev20200303, 2.2.0.dev20200304)\r\nERROR: No matching distribution found for tf-nightly-gpu==1.15.0.dev20190729\r\n\r\n**Any other info / logs**\r\nCan we get this build restored please? We do need TensorFlow prior to 2.0 - there's a dependency deprecation in the model conversion at the end of this tutorial that has (to my knowledge) not been addressed yet and we're on somewhat of a time crunch with that summit next week. \r\n\r\nLet me know if you need any more information! Thank you! ", "comments": ["Hi @loricrotser unfortunately due to [storage limits on pypi](https://pypi.org/stats/) and our whls taking up nearly 1TB of space we were forced to remove them from pypi.\r\n\r\nHowever as a courtesy I've made the internal archive link public so you can download that particular whl [here](https://storage.googleapis.com/tensorflow-nightly/prod/tensorflow/release/ubuntu_16/gpu_py35_full/nightly_release/3/20190729-010338/github/tensorflow/pip_pkg/tf_nightly_gpu-1.15.0.dev20190729-cp35-cp35m-manylinux2010_x86_64.whl).\r\n\r\nUnfortunately I cannot make it available on pypi due to circumstances out of my control. Please see https://github.com/pypa/pypi-support/issues/224 for additional details. Hope this helps!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37317\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37317\">No</a>\n", "Any chance to make the archive whl in Python3.6 instead since Google collab hosted runtimes is running Python3.6", "For 3.6, try [this link](https://storage.googleapis.com/tensorflow-nightly/prod/tensorflow/release/ubuntu_16/gpu_py36_full/nightly_release/3/20190729-010338/github/tensorflow/pip_pkg/tf_nightly_gpu-1.15.0.dev20190729-cp36-cp36m-manylinux2010_x86_64.whl) (just changed py35 to py36 and cp35 to cp36 throughout the link)", "> For 3.6, try [this link](https://storage.googleapis.com/tensorflow-nightly/prod/tensorflow/release/ubuntu_16/gpu_py36_full/nightly_release/3/20190729-010338/github/tensorflow/pip_pkg/tf_nightly_gpu-1.15.0.dev20190729-cp36-cp36m-manylinux2010_x86_64.whl) (just changed py35 to py36 and cp35 to cp36 throughout the link)\r\n\r\n@mihaimaruseac file does not exist.  Got the following error.\r\n<Error>\r\n<Code>NoSuchKey</Code>\r\n<Message>The specified key does not exist.</Message>\r\n<Details>\r\nNo such object: tensorflow-nightly/prod/tensorflow/release/ubuntu_16/gpu_py36_full/nightly_release/3/20190729-010338/github/tensorflow/pip_pkg/tf_nightly_gpu-1.15.0.dev20190729-cp36-cp36m-manylinux2010_x86_64.whl\r\n</Details>\r\n</Error>", "Does [this link](https://storage.googleapis.com/tensorflow-nightly/prod/tensorflow/release/ubuntu_16/gpu_py36_full/nightly_release/5/20190729-010352/github/tensorflow/pip_pkg/tf_nightly_gpu-1.15.0.dev20190729-cp36-cp36m-manylinux2010_x86_64.whl) work? I forgot that the builds are indexed by time as well. Our mistake! This should work now. ", "Apologies, should have checked the link before posting after just a lexical replacement. The link @av8ramit posted seems to work here (i.e., I can download it)", "Thanks for providing links to the nightly archives! With the hint I was able to locate more archives by\r\n```\r\ngsutil ls gs://tensorflow-nightly/prod/tensorflow/release/ubuntu_16/gpu_py37_full/nightly_release/ \r\n```\r\nthis helps me a lot with debugging."]}, {"number": 37316, "title": "Can't find tensorflow 2.1.0 with python 3.7 and pip", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Unknown\r\n\r\n**Describe the problem**\r\nI have a Docker file that is being built to run on a GPU machine in AWS. When I try to pip install tensorflow 2.1.0 I get the error:\r\n\r\n> No matching distribution found for tensorflow==2.1.0\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nMy Docker file is as follows:\r\n```\r\nFROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04\r\n\r\nRUN apt -qq -y update \\\r\n\t&& apt -qq -y upgrade\r\nRUN apt -y install python3.7\r\nRUN apt -y install python3-pip\r\n\r\nRUN which python3.7\r\nRUN which pip3\r\n\r\nRUN ln -s /usr/bin/python3.7 /usr/bin/python\r\nRUN ln -s /usr/bin/pip3 /usr/bin/pip\r\nRUN python --version\r\nRUN which pip\r\n\r\nWORKDIR /usr/src/app\r\nCOPY . .\r\nRUN pip3 install tensorflow==2.1.0\r\nENTRYPOINT [\"/usr/src/app/docker-entrypoint.sh\"]\r\nCMD [\"python\", \"test.py\"]\r\n```\r\n\r\n**Any other info / logs**\r\nThe full error is:\r\n> Collecting tensorflow==2.1.0\r\n  Could not find a version that satisfies the requirement tensorflow==2.1.0 (from -r requirements.txt (line 4)) (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0rc0, 1.1.0rc1, 1.1.0rc2, 1.1.0, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.3.0rc0, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.4.0rc0, 1.4.0rc1, 1.4.0, 1.4.1, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.8.0rc0, 1.8.0rc1, 1.8.0, 1.9.0rc0, 1.9.0rc1, 1.9.0rc2, 1.9.0, 1.10.0rc0, 1.10.0rc1, 1.10.0, 1.10.1, 1.11.0rc0, 1.11.0rc1, 1.11.0rc2, 1.11.0, 1.12.0rc0, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.12.2, 1.12.3, 1.13.0rc0, 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)\r\nNo matching distribution found for tensorflow==2.1.0 (from -r requirements.txt (line 4))\r\nThe command '/bin/sh -c pip3 install --no-cache-dir -r requirements.txt' returned a non-zero code: 1", "comments": ["I am seeing a similar error in the \"tensorflow/tensorflow/lite/micro/examples/micro_speech/train_speech_model.ipynb\" example - running this in the google notebook gives me the following error: \r\n\r\nERROR: Could not find a version that satisfies the requirement tf-nightly-gpu==1.15.0.dev20190729 (from versions: 2.2.0.dev20200201, 2.2.0.dev20200202, 2.2.0.dev20200203, 2.2.0.dev20200204, 2.2.0.dev20200205, 2.2.0.dev20200206, 2.2.0.dev20200207, 2.2.0.dev20200208, 2.2.0.dev20200210, 2.2.0.dev20200211, 2.2.0.dev20200212, 2.2.0.dev20200215, 2.2.0.dev20200216, 2.2.0.dev20200217, 2.2.0.dev20200218, 2.2.0.dev20200226, 2.2.0.dev20200227, 2.2.0.dev20200228, 2.2.0.dev20200229, 2.2.0.dev20200301, 2.2.0.dev20200302, 2.2.0.dev20200303, 2.2.0.dev20200304)\r\nERROR: No matching distribution found for tf-nightly-gpu==1.15.0.dev20190729\r\n\r\nThis was working as of a week ago. \r\n\r\n", "I think I've found a workaround, adding the line `RUN python3.7 -m pip install --upgrade pip`.\r\n\r\nIs this workflow recommended or is there a better solution?\r\n\r\nWorking Docker file:\r\n```\r\nFROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04\r\n\r\nRUN apt -qq -y update \\\r\n\t&& apt -qq -y upgrade\r\nRUN apt -y install python3.7\r\nRUN apt -y install python3-pip\r\nRUN python3.7 -m pip install --upgrade pip\r\n\r\nRUN which python3.7\r\nRUN which pip3\r\n\r\nRUN ln -s /usr/bin/python3.7 /usr/bin/python\r\nRUN ln -s /usr/bin/pip3 /usr/bin/pip\r\nRUN python --version\r\nRUN which pip\r\n\r\nWORKDIR /usr/src/app\r\nCOPY . .\r\nRUN pip3 install tensorflow==2.1.0\r\nENTRYPOINT [\"/usr/src/app/docker-entrypoint.sh\"]\r\nCMD [\"python\", \"test.py\"]\r\n```", "@maurera,\r\nAs per the [documentation](https://www.tensorflow.org/install/\r\n) TensorFlow 2 packages require a pip version >19.0.\r\n\r\nPlease check [this](https://github.com/tensorflow/tensorflow/issues/26578#issuecomment-543286872) comment from a similar issue and let us know if it works. Thanks!", "`RUN python3.7 -m pip install --upgrade pip` is indeed the right solution. Can you please make a PR to add it to the dockerfiles?", "@amahendrakar - yes, this works. See my [comment](https://github.com/tensorflow/tensorflow/issues/37316#issuecomment-594874895) from yesterday", "@maurera,\r\nIs this still an issue. Please feel free to close the issue if resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37316\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37316\">No</a>\n"]}, {"number": 37315, "title": "test issue", "body": "sdfdssdfdssdsdfsdfsdfsdfdsfsdfsdfsdfsdfsdfdsfsdfsdfsdfdsfsdfsdfsdfsdfsdfdsf load_dynamic return _load(spec)\r\nCUDA/cuDNN version: 434\r\n**Provide the exact sequence", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37315\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37315\">No</a>\n"]}, {"number": 37314, "title": "Frees up Model.fit to take advantage of async eager when no batch-lev\u2026", "body": "\u2026el Callbacks are\r\n\r\npassed.\r\nFor Datasets of unknown sizes, the first epoch will still block on each batch.\r\nHowever, after the first epoch, the number of steps to run will be known, and\r\ntherefore these Datasets will be able to take advantage\r\nof async eager as well.\r\n\r\nAlso fixes issue where the \"steps\" argument was not always passed correctly to\r\nCallbacks (tested now in data_adapter_test)\r\n\r\nPiperOrigin-RevId: 298728965\r\nChange-Id: I430a6935ffc19718dab1be992f03a637d3ff4584", "comments": []}, {"number": 37313, "title": "build and install", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\nsdfdssdfdssdsdfsdfsdfsdfdsfsdfsdfsdfsdfsdfdsfsdfsdfsdfdsfsdfsdfsdfsdfsdfdsf load_dynamic return _load(spec)\r\nCUDA/cuDNN version: 434\r\n**Provide the exact sequence", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37313\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37313\">No</a>\n"]}, {"number": 37312, "title": "Update release notes for TensorFlow 2.2.0", "body": "Release notes for TF 2.2.0 updated.", "comments": ["@mihaimaruseac great. Thanks!"]}, {"number": 37310, "title": "test issue", "body": "sdfdssdfdssdsdfsdfsdfsdfdsfsdfsdfsdfsdfsdfdsfsdfsdfsdfdsfsdfsdfsdfsdfsdfdsf load_dynamic return _load(spec)\r\nCUDA/cuDNN version: 434\r\n**Provide the exact sequence", "comments": []}, {"number": 37309, "title": "sdfdssdfdssdsdfsdfsdfsdfdsfsdfsdfsdfsdfsdfdsfsdfsdfsdfdsfsdfsdfsdfsdfsdfdsf load_dynamic return _load(spec) CUDA/cuDNN version: 434 **Provide the exact sequence", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\nsdfdssdfdssdsdfsdfsdfsdfdsfsdfsdfsdfsdfsdfdsfsdfsdfsdfdsfsdfsdfsdfsdfsdfdsf load_dynamic return _load(spec)\r\nCUDA/cuDNN version: 434\r\n**Provide the exact sequence", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37309\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37309\">No</a>\n"]}]