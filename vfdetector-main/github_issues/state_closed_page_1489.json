[{"number": 8270, "title": "Explain that to restore without a graph one can use import_meta_graph", "body": "Explain that to restore without a graph one can use import_meta_graph", "comments": ["Can one of the admins verify this patch?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please."]}, {"number": 8269, "title": "1.0.1 CPU vs GPU 1.0.0 Build for Windows", "body": "The CPU build is complete for version 1.0.1 on Windows, however the GPU build is at 1.0.0.\r\n\r\nhttp://ci.tensorflow.org/job/nightly-win/DEVICE=gpu,OS=windows/", "comments": ["Perhaps I'm reading this wrong, but it looks like the last built artifact for that build is TensorFlow 1.0.1:\r\nhttp://ci.tensorflow.org/job/nightly-win/DEVICE=gpu,OS=windows/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow_gpu-1.0.1-cp35-cp35m-win_amd64.whl", "Update there now. THanks.\n\nOn Fri, Mar 10, 2017 at 11:28 AM, Derek Murray <notifications@github.com>\nwrote:\n\n> Perhaps I'm reading this wrong, but it looks like the last built artifact\n> for that build is TensorFlow 1.0.1:\n> http://ci.tensorflow.org/job/nightly-win/DEVICE=gpu,OS=\n> windows/lastSuccessfulBuild/artifact/cmake_build/tf_\n> python/dist/tensorflow_gpu-1.0.1-cp35-cp35m-win_amd64.whl\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8269#issuecomment-285714744>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABXVTShOrmBWFvGmRktV4vAfY34pC2_tks5rkXoYgaJpZM4MZMnM>\n> .\n>\n"]}, {"number": 8268, "title": "Error when  using the TensorFlow Runtime with HVX Acceleration", "body": "OS: Ubuntu 14.04 64bits\r\nAndroid Version: 6.0.1     \r\nNDK Version: android-ndk-r12b\r\nI follow the description in [**tensorflow/tree/master/tensorflow/contrib/hvx**]((https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx)),  after compiling the `so` files and the **hexagon_graph_execution**,  what's more, I also have pushed  additional network define file(`tensorflow_inception_v3_stripped_optimized_quantized.pb`) and the test image to my phone. \r\nBut when I start to run the binary file in my **Snapdragon 820 android devices**, it failed all the test cases.\r\nThe stdout log is showed below,   How can I resolve this question?\r\n \r\n\r\nRunning main() from test_main.cc\r\n[==========] Running 1 test from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from GraphTransferer\r\n[ RUN      ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime\r\nnative : hexagon_graph_execution_test.cc:294 Fuse and run inception v3 on hexagon with tf runtime\r\ntensorflow/core/kernels/hexagon/hexagon_graph_execution_test.cc:70: Failure\r\nExpected: (version) >= (1), actual: 0 vs 1\r\nnative : hexagon_graph_execution_test.cc:121 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\r\nnative : hexagon_graph_execution_test.cc:127 header size = 54\r\nnative : hexagon_graph_execution_test.cc:129 image size = 40\r\nnative : hexagon_graph_execution_test.cc:131 width = 299\r\nnative : hexagon_graph_execution_test.cc:133 height = -299\r\nnative : hexagon_graph_execution_test.cc:306 Ioading image finished.\r\nnative : hexagon_graph_execution_test.cc:313 Build fused graph\r\ncan't determine number of CPU cores: assuming 4\r\ncan't determine number of CPU cores: assuming 4\r\nnative : hexagon_graph_execution_test.cc:121 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\r\nnative : hexagon_graph_execution_test.cc:127 header size = 54\r\nnative : hexagon_graph_execution_test.cc:129 image size = 40\r\nnative : hexagon_graph_execution_test.cc:131 width = 299\r\nnative : hexagon_graph_execution_test.cc:133 height = -299\r\nnative : hexagon_graph_execution_test.cc:218 Ioading image finished.\r\nnative : hexagon_graph_execution_test.cc:223 Copy data to tensor.\r\nnative : hexagon_graph_execution_test.cc:246 Run graph\r\nInit hexagon with max attributes (Controller version = 91)\r\nFailed to disable DSP DCVS: ffffffff\r\n\r\nFailed to append const node 65538\r\nFailed to append const node 65538\r\nFailed to append const node 65539\r\nFailed to append const node 65539\r\n.\r\n.\r\n.\r\nFailed to append const node 66635\r\nFailed to append const node 66640\r\nFailed to append const node 66640\r\nnative : hexagon_control_wrapper.cc:235 Setup graph completed\r\nPrepare failed! returned 0xffffffff\r\n\r\nExecute graph!\r\nExecution failed!\r\nexecute got err: -1\r\n\r\nExecution failed\r\nFailed to read data.\r\nnative : hexagon_graph_execution_test.cc:253 Output byte size = 4032\r\nnative : hexagon_graph_execution_test.cc:254 Output shape = [1,1,1,1008]\r\nnative : graph_transfer_utils.cc:43 === Dump ranking ===\r\nnative : graph_transfer_utils.cc:46 0: 1000, dumbbell, 0\r\nnative : graph_transfer_utils.cc:46 1: 999, carbonara, 0\r\nnative : graph_transfer_utils.cc:46 2: 998, stole, 0\r\nnative : graph_transfer_utils.cc:46 3: 997, rubber eraser, 0\r\nnative : graph_transfer_utils.cc:46 4: 996, coffee mug, 0\r\nnative : graph_transfer_utils.cc:46 5: 995, flagpole, 0\r\nnative : graph_transfer_utils.cc:46 6: 994, parallel bars, 0\r\nnative : graph_transfer_utils.cc:46 7: 993, cheeseburger, 0\r\nnative : graph_transfer_utils.cc:46 8: 992, bubble, 0\r\nnative : graph_transfer_utils.cc:46 9: 991, beaker, 0\r\nFinalize hexagon\r\n[  FAILED  ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime (5573 ms)\r\n[----------] 1 test from GraphTransferer (5573 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test case ran. (5573 ms total)\r\n[  PASSED  ] 0 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime\r\n\r\n 1 FAILED TEST\r\n  YOU HAVE 3 DISABLED TESTS\r\n\r\n\r\n", "comments": ["@tensorflower-gardener ", "@satok16 - you seem to be the owner of this contrib component.  \r\n\r\n", "> tensorflow/core/kernels/hexagon/hexagon_graph_execution_test.cc:70: Failure\r\n> Expected: (version) >= (1), actual: 0 vs 1\r\nThis line indicates that TF can't call a binary for hexagon.  So, please answer following debugging questions.\r\n\r\n- What kind of Android device do you have?  Basically, your Android device should have a special configuration to bypass qualcomm signature check.  This functionality can't be enabled by software but you should use some specific hardwares like a dragon board.\r\n\r\n- What adb says?  Please attach the output from \"adb logcat\".\r\n\r\n", "I met the same issue with \r\nFailed to disable DSP DCVS: ffffffff\r\n\r\nFailed to append const node 65538\r\n.\r\n.\r\n.\r\n\r\nI tried to install the testsig library as mentioned in \r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx\r\nand it solved the issue in my case.", "Yeah, that's correct.  Actually I've added a check whether testsig is installed or not in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/samples/build_and_run_inception_hexagon.sh#L54.  So, if using this but still seeing the same problem, that might be a hardware restriction.", "@ezhuei , installing the testsig library solved this issue. \r\n@satok16  sorry for missing the **Troubleshooting** section in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx.\r\n", "Hello, @satok16 , thanks for contributing the hexagon HVX code to tensorflow.\r\n  \r\nI follow the tensorflow official quantization process, https://www.tensorflow.org/performance/quantization, to quantize the inception v3 model, however, it failed the test cases of hexagon graph execution, which is showed bellow. But it did pass the test cases if changed the model to https://storage.googleapis.com/download.tensorflow.org/models/tensorflow_inception_v3_stripped_optimized_quantized.pb.\r\n\r\n[ RUN      ] GraphTransferer.RunInceptionV3OnHexagonExampleWithHexagonWrapper\r\nnative : hexagon_graph_execution_test.cc:450 Run inception v3 on hexagon with hexagon controller\r\nnative : hexagon_graph_execution_test.cc:88 Hexagon controller version is 90\r\ncan't determine number of CPU cores: assuming 4\r\ncan't determine number of CPU cores: assuming 4\r\ntensorflow/core/kernels/hexagon/hexagon_graph_execution_test.cc:466: Failure\r\nValue of: status.ok()\r\n  Actual: false\r\nExpected: true\r\nInvalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]]\r\n\r\n[  FAILED  ] GraphTransferer.RunInceptionV3OnHexagonExampleWithHexagonWrapper (331 ms)\r\n[ RUN      ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime\r\nnative : hexagon_graph_execution_test.cc:531 Fuse and run inception v3 on hexagon with tf runtime\r\nnative : hexagon_graph_execution_test.cc:88 Hexagon controller version is 90\r\nnative : hexagon_graph_execution_test.cc:138 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\r\nnative : hexagon_graph_execution_test.cc:144 header size = 54\r\nnative : hexagon_graph_execution_test.cc:146 image size = 40\r\nnative : hexagon_graph_execution_test.cc:148 width = 299\r\nnative : hexagon_graph_execution_test.cc:150 height = -299\r\nnative : hexagon_graph_execution_test.cc:543 Ioading image finished.\r\nnative : hexagon_graph_execution_test.cc:550 Build fused graph\r\nnative : graph_transfer_utils.cc:96 Check failed: status.ok() \r\nAborted \r\n\r\nI wonder how to quantize my model so that it will meet your hexagon code. It seems tensorflow has updated the code to quantize the model.\r\n", "Hi Gekking,\r\n\"DecodeJpeg\" is not supported yet on hexagon.  We are going to add a functionality to automatically switch implementation to cpu op-kernel implementation if the hexagon kernel is not supported.  For the time being, you should manually fuse a subgraph into a remote fused graph execute op which you want to run on hexagon.  Note that the hexagon executor only supports about 20 op kernels listed [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/hexagon/hexagon_ops_definitions.cc).  So you should avoid fusing ops which are not listed there.  See a [utility](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/hexagon/graph_transfer_utils.cc#L86) to fuse a graph.", "Would you mind telling me how do you quantize the test model? As in my case, I try to train a model for mnist classification dataset, I have followed the quantization tool provided by Tensorflow, the result predicted by the quantized model runned in PC is correct, however, when runned in my Mobile phone with a Snapdragon 820 devices, it got different and wrong predict score. Thanks. @satok16 ", "The model I used is visulzed ![graph_1](https://cloud.githubusercontent.com/assets/5322311/24087189/d94fcd12-0d56-11e7-90a0-ca923dbd9717.png)\r\n\r\n![conv_1](https://cloud.githubusercontent.com/assets/5322311/24087186/d1424384-0d56-11e7-846b-bf8836192cd0.png)\r\n", "Could you share 1. your graph file and 2. a command line which you used?", "I build the quantization tools on tensorflow v0.11.0 https://github.com/tensorflow/tensorflow/tree/v0.11.0. \r\nThe graph pb file and the sh file to run the quantization process is included in \r\n[quantize_graph.zip](https://github.com/tensorflow/tensorflow/files/856868/quantize_graph.zip). \r\nThanks. \r\n", "I could able to run inception graph(tensorflow_inception_v3_stripped_optimized_quantized.pb) using the mentioned guidelines on [page](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/samples/build_and_run_inception_hexagon.sh) for Android demo TF-Classifiy application.\r\n\r\nMy target platform was OnePlus3T (which has Snapdragon 821 SOC) and i could able to achieve **inference time = 200-300ms**. I fell this is because of HVX acceleration but not quite sure and therefore i connected snapdragon's [profiler](https://developer.qualcomm.com/software/snapdragon-profiler) to check if this is indeed coming from DSP. I noticed a spike when the application starts and later very small waveforms, so there is no way to relate this to my application (at least i do not know). Is there a better way to check the performance or a quick way to know if my application is indeed using HVX. \r\n\r\nAnother input, i checked logcat output but it doesn't show any logs coming out from 'adsp'. \r\n\r\nAny help is highly appreciated. Thanks. ", "Hi @milinddeore,\r\nThe first iteration may take time to determine min and max of quantized ops.  There are several ways to check whether your app is properly calling HVX functions.  The easiest way is \r\n```\r\n  HexagonControlWrapper hexagon_control_wrapper;\r\n  const int version = hexagon_control_wrapper.GetVersion();\r\n```\r\nIf this GetVersion returns 1 or larger value, you can properly calling hvx code.  Let me know if it doesn't solve your problem.\r\n\r\n\r\n", "@satok16 Thanks for info, appreciate your help. \r\n\r\nI have a follow up question. \r\nAPI `RunInferenceByHexagonControlWrapper()` in test section, calls  \r\n```\r\n\r\n// For initialisation \r\nHexagon_control_wrapper.Init();\r\nHexagon_control_wrapper.SetUpfGraph();\r\n\r\nand similarly \r\n\r\n// For Teardown\r\nHexagon_control_wrapper.TearDownGrpah()\r\nHexagon_control_wrapper.Finalize();\r\n\r\n```\r\nCan't we do this once at the startup and shutdown respectively? The reason i am asking, I have a phone camera which is sending images in real-time for inference and for every image i shouldn't be doing this, isn't it? Please correct me if i am wrong. Thanks. ", "You are right.  The following functions should be called only once for repeated inferences in your app.\r\n```\r\nHexagon_control_wrapper.Init();\r\nHexagon_control_wrapper.SetUpfGraph();\r\nHexagon_control_wrapper.TearDownGrpah()\r\nHexagon_control_wrapper.Finalize();\r\n```\r\n", "@satok16 \r\nI had written Android application for OnePlus 3T phone (This has snapdragon 821 and i have flashed my own compiled android image out of LineageOS, that means it completely rooted and /system is write accessible) but it was crashing and hence i tried following sample test for hexagon:\r\n\r\n```\r\n$ adb shell 'LD_LIBRARY_PATH=/data/local/tmp:$LD_LIBRARY_PATH' \"/data/local/tmp/hexagon_graph_execution\"\r\nWARNING: linker: Warning: unable to normalize \"\"\r\nRunning main() from test_main.cc\r\n[==========] Running 1 test from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from GraphTransferer\r\n[ RUN      ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime\r\nnative : hexagon_graph_execution_test.cc:446 Fuse and run inception v3 on hexagon with tf runtime\r\ntensorflow/core/kernels/hexagon/hexagon_graph_execution_test.cc:71: Failure\r\nExpected: (version) >= (1), actual: 0 vs 1\r\n\r\n```\r\nThe problem here is that the version is showing '0', does that mean DSP is not accessible? \r\n\r\n**UPDATE**:\r\nVerified logcat and getting consistent the error is following:\r\n\r\n`01-13 05:58:44.477  4373  4373 D adsprpc : vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:437: dlopen error: hexagon_nn signature verify start failed for libhexagon_nn_skel.so`\r\n\r\nTo avoid this, created testsig-xxxx but still it doesn't help much. Is there a workaround for this problem? ", "Unfortunately, you can't run any hexagon binary with a test signature on a production phone even if it's rooted.  You should get a development kit like https://www.intrinsyc.com/snapdragon-embedded-development-kits/snapdragon-820-development-kit/ to develop it.", "Thanks @satok16!\r\nBut what if when someone try to go from development to production? Do we need to add libhexagon_nn_skel.so to the android stock and compile it as [signed-production](https://wiki.lineageos.org/signing_builds.html) build? Would that work on hexagon? ", "Hi @milinddeore,\r\nThe hexagon binary should be signed in a different way apart from the Android signature process.  So, if you want to run the hexagon binary on a phone, the hexagon binary should be signed by qualcomm or phone manufactures.", "hi @satok16,\r\n\r\nI also got some problems in using HVX accelaration.  \r\nhere are the tools and enviroments info:\r\nHardware: Intrinsyc\u2019s Open\u2010Q\u2122 820 Development Kit\r\nOS: Ubuntu 16.04 64bits\r\nAndroid Version: 6.0\r\nHexagon SDK 3.0\r\nNDK Version: android-ndk-r10d  as tool in Hexagon SDK\r\nI also follow the description in tensorflow/tree/master/tensorflow/contrib/hvx, running script like this 'QUALCOMM_SDK=\"/home/tff/Qualcomm/Hexagon_SDK/3.0\" NDK_ROOT=\"/home/tff/Qualcomm/Hexagon_SDK/3.0/tools/android-ndk-r10d\" ./tensorflow/contrib/makefile/samples/build_and_run_inception_hexagon.sh -p'. But it failed, what can I do to get it right?\r\n\r\nThe printed log:\r\nRunning main() from test_main.cc\r\nNote: Google Test filter = GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime\r\n[==========] Running 1 test from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from GraphTransferer\r\n[ RUN      ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime\r\nnative : hexagon_graph_execution_test.cc:446 Fuse and run inception v3 on hexagon with tf runtime\r\nnative : hexagon_graph_execution_test.cc:72 Hexagon controller version is 90\r\nnative : hexagon_graph_execution_test.cc:122 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\r\nnative : hexagon_graph_execution_test.cc:128 header size = 54\r\nnative : hexagon_graph_execution_test.cc:130 image size = 40\r\nnative : hexagon_graph_execution_test.cc:132 width = 299\r\nnative : hexagon_graph_execution_test.cc:134 height = -299\r\nnative : hexagon_graph_execution_test.cc:458 Ioading image finished.\r\nnative : hexagon_graph_execution_test.cc:465 Build fused graph\r\ncan't determine number of CPU cores: assuming 4\r\ncan't determine number of CPU cores: assuming 4\r\nnative : op_def_util.cc:332 Op PlaceholderV2 is deprecated. It will cease to work in GraphDef version 23. Placeholder now behaves the same as PlaceholderV2..\r\nnative : hexagon_graph_execution_test.cc:122 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\r\nnative : hexagon_graph_execution_test.cc:128 header size = 54\r\nnative : hexagon_graph_execution_test.cc:130 image size = 40\r\nnative : hexagon_graph_execution_test.cc:132 width = 299\r\nnative : hexagon_graph_execution_test.cc:134 height = -299\r\nnative : hexagon_graph_execution_test.cc:262 Ioading image finished.\r\nnative : hexagon_graph_execution_test.cc:170 Ioading image finished.\r\nnative : hexagon_graph_execution_test.cc:174 Copy data to tensor.\r\nnative : hexagon_graph_execution_test.cc:284 Run graph\r\nInit hexagon with max attributes (Controller version = 91)\r\nnative : hexagon_control_wrapper.cc:252 Setup graph completed\r\nPrepare failed! returned 0xffffffff\r\n\r\nExecute graph!\r\nExecution failed!\r\nexecute got err: -1\r\n\r\nExecution failed\r\nFailed to read data.\r\nnative : hexagon_graph_execution_test.cc:291 Output byte size = 4032\r\nnative : hexagon_graph_execution_test.cc:292 Output shape = [1,1008]\r\nnative : graph_transfer_utils.cc:46 === Dump ranking ===\r\nnative : graph_transfer_utils.cc:49 0: 1000, dumbbell, 0\r\nnative : graph_transfer_utils.cc:49 1: 999, carbonara, 0\r\nnative : graph_transfer_utils.cc:49 2: 998, stole, 0\r\nnative : graph_transfer_utils.cc:49 3: 997, rubber eraser, 0\r\nnative : graph_transfer_utils.cc:49 4: 996, coffee mug, 0\r\nnative : graph_transfer_utils.cc:49 5: 995, flagpole, 0\r\nnative : graph_transfer_utils.cc:49 6: 994, parallel bars, 0\r\nnative : graph_transfer_utils.cc:49 7: 993, cheeseburger, 0\r\nnative : graph_transfer_utils.cc:49 8: 992, bubble, 0\r\nnative : graph_transfer_utils.cc:49 9: 991, beaker, 0\r\nFinalize hexagon\r\n[       OK ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime (6799 ms)\r\n[----------] 1 test from GraphTransferer (6801 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test case ran. (6803 ms total)\r\n[  PASSED  ] 1 test.", "run build_and_run_inception_hexagon.sh wih '-p' needs old version tensorflow to create hexagon_graph_execution, otherwise remove '-p',totally build all the components.", "Hi @satok16, @kdxtb,\r\nI have the same problem as kdxtb has even though I didn't use -p as an option.\r\n\r\nPrepare failed! returned 0xffffffff\r\n\r\nNN Id = 872478096\r\nExecute graph!\r\nExecution failed!\r\nexecute got err: -1\r\n\r\nNN Id = 872478096\r\nExecution failed\r\nNN Id = 872478096\r\nFailed to read data.\r\n\r\nI think that I have problem creating graph from *.pb. Do you have any suggestions?", "Hi @satok16 \r\nI try to hexagon DSP for android.\r\nbuild_and_run_inception_hexagon.sh wih '-p' prebuild so files test ok.\r\nbut local build ibhexagon_nn_skel.so, this file seem to be a problem.\r\nI thank that difference between the google server prebuild version and tensorflow source base,\r\nWhen was the prebuild version created? I want to test it back.\r\nPlease let me know git commit.\r\nThanks, Moon.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing since most of this discussion seems better suited to stackoverflow. Please open a new issue if there's a specific bug or feature request."]}, {"number": 8267, "title": "TypeError: Expected int32, got list containing Tensors of type '_Message' instead.", "body": "I am using tensorflow (1.0.0) in Ubuntu 14.04. When I train a cnn model ,I got a error as follow:\r\n\r\n/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\r\n  \"This module will be removed in 0.20.\", DeprecationWarning)\r\n2017-03-10 15:03:03,616 : INFO : building model...\r\nTraceback (most recent call last):\r\n  File \"cnn.py\", line 205, in <module>\r\n    train()\r\n  File \"cnn.py\", line 202, in train\r\n    cnn_model.build_model(sentence_words, vector_length, num_classes, filter_sizes, num_filters)\r\n  File \"cnn.py\", line 61, in build_model\r\n    h_pool = tf.concat(3, pooled_outputs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1047, in concat\r\n    dtype=dtypes.int32).get_shape(\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 651, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 716, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 367, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 302, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected int32, got list containing Tensors of type '_Message' instead.\r\n\r\nHow can I resolve this question?", "comments": ["It's because of the tensorflow has updated the parameter order of concat,  you could try modifying to tf.concat(pooled_outputs, 3)", "Please see the TF1.0 migration documentation https://www.tensorflow.org/install/migration\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Thank u! I solved it. @Geekking ", "@stein2013 how did you solve it?", "@alexiskattan just do a \r\n`git checkout master` in the /models/ directory it should update to the new tensorflow apis. "]}, {"number": 8266, "title": "\"The TensorFlow library wasn't compiled to use SSE3/SSE4.1/... instructions\", \"creating context when one is currently active\", \"Peer access not supported between device ordinals 0 and 1\" when running \"Hello TensorFlow\"", "body": "\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nI am running google TensorFlow on google compute engine instance. I started with Hello Tensorflow program and I am reading some warnings.\r\n\r\n1. The TensorFlow library wasn't compiled to use SSE3/SSE4.1/SSE4.2/AVX/AVX2/FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2. successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero.\r\n3. creating context when one is currently active; existing: 0x30fd920\r\n4. Peer access not supported between device ordinals 0 and 1\r\n\r\nCan anyone tell me what caused those warnings? First thing I want to know is if my GPUs are actually working on the data are just sent to CPU only?\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\n### What other attempted solutions have you tried?\r\nI tried run the Hello TensorFlow on both instance with 4 GPUs and instance with 8GPUs.\r\n\r\n### Logs or other output that would be helpful\r\nPlease see the attachment.\r\n\r\nThank you for your help!\r\n\r\n[Terminal Saved Output.txt](https://github.com/tensorflow/tensorflow/files/833162/Terminal.Saved.Output.txt)", "comments": ["These are just warnings telling you TF can run faster on CPU if you recompile it by yourself, if you're doing mostly GPU stuff, you don't have to worry about these warnings.\r\n\r\nAt the end, you're trying to run `tf.run(...)`, and it doesn't exist, so the error seems correct. `job` isn't defined, so the other error seems correct as well.\r\n\r\nBy the logs, it seems you are using the GPUs. You can run a script that keeps calculating stuff and run `nvidia-smi` on the terminal to see the GPU usage. Alternatively, you can always manually specify where you want to run something by using `with tf.device('/gpu:0')` for example.\r\n\r\nThis kind of question would be better addressed on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow), since GitHub is mostly used for bug reporting. ;-)", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Actually it is a good feature request.  SSE4 has been available since 2007 (https://en.wikipedia.org/wiki/SSE4) and https://en.wikipedia.org/wiki/Advanced_Vector_Extensions since 2011.  So seems reasonable to have these enabled by default.", "@paramr  There were users who couldn't run official binary because of SSE inclusion -- https://github.com/tensorflow/tensorflow/issues/6809\r\n\r\nSo the idea is to have slow \"lowest common denominator\" official release, and then let users build/distribute versions for the gazillion other configurations out there. I post Xeon V3 specialized wheels here -- https://github.com/yaroslavvb/tensorflow-community-wheels", "@yaroslavvb \r\n\r\nCan we add check for this at configure time:\r\n```\r\n https://github.com/tensorflow/tensorflow/blob/master/configure\r\n\r\n```\r\n\r\nbash to test this:\r\n\r\n```bash\r\n\r\nPLATFORM=\"$(uname -s | tr 'A-Z' 'a-z')\"\r\nSSE3_SUPPORTED=false\r\n\r\nfunction is_linux() {\r\n  if [[ \"${PLATFORM}\" == \"linux\" ]]; then\r\n    true\r\n  else\r\n    false\r\n  fi\r\n}\r\n\r\nfunction is_macos() {\r\n  if [[ \"${PLATFORM}\" == \"darwin\" ]]; then\r\n    true\r\n  else\r\n    false\r\n  fi\r\n}\r\n\r\nfunction sse3_supported_macos() {\r\n    cpuinfo | grep -i 'sse3'  > /dev/null\r\n    if [ $? -eq 0 ];  then\r\n            true     \r\n    else\r\n           false \r\n    fi\r\n}\r\n\r\nfunction sse3_supported_linux() {\r\n    cat /proc/cpuinfo | grep -i 'sse3'  > /dev/null\r\n    if [ $? -eq 0 ];  then\r\n            true     \r\n    else\r\n           false \r\n    fi\r\n}\r\n\r\n\r\n\r\n\r\nif is_macos; then\r\n    if sse3_supported_macos; then\r\n        SSE3_SUPPORTED=true\r\n        echo \"sse3 supported:\" $SSE3_SUPPORTED\r\n    fi\r\nfi\r\n\r\nif is_linux; then\r\n    if sse3_supported_linux; then\r\n        SSE3_SUPPORTED=true\r\n        echo \"sse3 supported:\" $SSE3_SUPPORTED\r\n    fi\r\nfi\r\n```", "Can we have official binaries in central with at least the different presets?  And change the dependencies so that the JNI code is not implicitlly included in `tensorflow` much like how slf4j separates the API and does a lookup for the implementation."]}, {"number": 8265, "title": "Memory leak when writing to Logfile with Tensorboard summarywriter", "body": "### Issue:\r\nMemory occupied after call to Filewriter not being freed till python termination. This causes accumulation of data and subsequent filling up of RAM which is freed only when the entire script completes execution and python is terminated.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\nCUDA/cuDNN : Not installed\r\nLink to the pip package you : https://github.com/tensorflow/tensorflow/releases/tag/v1.0.0\r\nTensorflow version: 1.0.0\r\n\r\n### A small script replicating the issue\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom memory_profiler import profile\r\nimport time\r\n\r\n@profile\r\ndef _write_into_log(images):\r\n    path_logdir = os.path.join(\"./MiniExample\")\r\n    if not os.path.exists(path_logdir):\r\n        os.makedirs(path_logdir)\r\n\r\n    with tf.Graph().as_default() as g:\r\n        image = tf.placeholder(tf.float32, shape = [None, None, None, 3])\r\n\r\n        image_summary = tf.summary.image(name = \"Images\", tensor = image, max_outputs = 2000)\r\n\r\n        with tf.Session() as sess:\r\n            summary = sess.run(image_summary, feed_dict = {image : np.concatenate(images, axis = 0)})\r\n            file_writer = tf.summary.FileWriter(path_logdir, g)\r\n            file_writer.add_summary(summary)\r\n            file_writer.close()\r\n\r\n@profile\r\ndef main():\r\n    out = [np.random.random((1, 224, 224, 3)) for i in range(2000)]\r\n    _write_into_log(out)\r\n    out = None\r\n    time.sleep(10)\r\n\r\nmain()\r\n```\r\n\r\n### Logs or other output that would be helpful\r\nPython memory profiler output\r\n```\r\n$ python test.py \r\nFilename: test.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     7   2394.5 MiB      0.0 MiB   @profile\r\n     8                             def _write_into_log(images):\r\n     9   2394.5 MiB      0.0 MiB       path_logdir = os.path.join(\"./MiniExample\")\r\n    10   2394.5 MiB      0.0 MiB       if not os.path.exists(path_logdir):\r\n    11                                     os.makedirs(path_logdir)\r\n    12                             \r\n    13   2394.6 MiB      0.0 MiB       with tf.Graph().as_default() as g:\r\n    14   2399.0 MiB      4.4 MiB           image = tf.placeholder(tf.float32, shape = [None, None, None, 3])\r\n    15                             \r\n    16   2399.1 MiB      0.1 MiB           image_summary = tf.summary.image(name = \"Images\", tensor = image, max_outputs = 2000)\r\n    17                             \r\n    18   2407.9 MiB      8.9 MiB           with tf.Session() as sess:\r\n    19   2978.8 MiB    570.9 MiB               summary = sess.run(image_summary, feed_dict = {image : np.concatenate(images, axis = 0)})\r\n    20   2980.8 MiB      2.0 MiB               file_writer = tf.summary.FileWriter(path_logdir, g)\r\n    21   3269.7 MiB    288.9 MiB             file_writer.add_summary(summary)\r\n    22   3269.7 MiB      0.0 MiB               file_writer.close()\r\n\r\n\r\nFilename: test.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    24     89.6 MiB      0.0 MiB   @profile\r\n    25                             def main():\r\n    26   2394.5 MiB   2304.9 MiB       out = [np.random.random((1, 224, 224, 3)) for i in range(2000)]\r\n    27   2981.4 MiB    586.8 MiB       _write_into_log(out)\r\n    28    676.7 MiB  -2304.7 MiB       out = None\r\n    29    676.7 MiB      0.0 MiB       time.sleep(10)\r\n```\r\nAs can be seen above, the additional 586.8 MB occupied after call to the _write_into_log is never cleared.\r\n", "comments": ["We are experiencing the same issue, but with histograms.  Eventually training script takes up 24G and is killed because of OOM.", "Digging a bit more into the first case @falaktheoptimist  shared above, we found that the memory hogging is happening at two stages - first when computing the `image_summary` (using session.run) and second by the `add_summary` function of Filewriter. Below are the profiler outputs showing half of the memory(~285 MB) occupied by `sess.run` and other half being occupied by `file_writer.add_summary` call.\r\n\r\nCase-1:\r\n```\r\nFilename: test.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     7   2394.4 MiB      0.0 MiB   @profile\r\n     8                             def _write_into_log(images):\r\n     9   2394.4 MiB      0.0 MiB       with tf.Graph().as_default() as g:\r\n    10   2398.7 MiB      4.3 MiB           image = tf.placeholder(tf.float32, shape = [None, None, None, 3])\r\n    11   2398.8 MiB      0.1 MiB           image_summary = tf.summary.image(name = \"Images\", tensor = image, max_outputs = 2000)\r\n    12                             \r\n    13   2407.3 MiB      8.5 MiB           with tf.Session() as sess:\r\n    14   2691.9 MiB    284.6 MiB               sess.run(image_summary, feed_dict = {image : np.concatenate(images, axis = 0)})\r\n\r\n\r\nFilename: test.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    17     89.4 MiB      0.0 MiB   @profile\r\n    18                             def main():\r\n    19   2394.2 MiB   2304.8 MiB       out = [np.random.random((1, 224, 224, 3)) for i in range(2000)]\r\n    20   2691.9 MiB    297.7 MiB       _write_into_log(out)\r\n    21    387.2 MiB  -2304.7 MiB       out = None\r\n```\r\nCase-2:\r\n```     \r\nFilename: test.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     7   2394.2 MiB      0.0 MiB   @profile\r\n     8                             def _write_into_log(images):\r\n     9   2394.2 MiB      0.0 MiB       path_logdir = os.path.join(\"./MiniExample\")\r\n    10   2394.2 MiB      0.0 MiB       if not os.path.exists(path_logdir):\r\n    11                                     os.makedirs(path_logdir)\r\n    12                             \r\n    13   2394.2 MiB      0.1 MiB       with tf.Graph().as_default() as g:\r\n    14   2398.6 MiB      4.4 MiB           image = tf.placeholder(tf.float32, shape = [None, None, None, 3])\r\n    15                             \r\n    16   2398.7 MiB      0.1 MiB           image_summary = tf.summary.image(name = \"Images\", tensor = image, max_outputs = 2000)\r\n    17                             \r\n    18   2409.4 MiB     10.7 MiB           with tf.Session() as sess:\r\n    19   2409.7 MiB      0.3 MiB               file_writer = tf.summary.FileWriter(path_logdir, g)\r\n    20   2982.4 MiB    572.7 MiB               sss = sess.run(image_summary, feed_dict = {image : np.concatenate(images, axis = 0)})\r\n    21   3271.2 MiB    288.8 MiB               file_writer.add_summary(sss)\r\n    22   3271.2 MiB      0.0 MiB               file_writer.close()\r\n    23   3271.2 MiB      0.0 MiB               file_writer = None\r\n    24   3271.2 MiB      0.0 MiB               image_summary = None\r\n    25   2983.0 MiB   -288.2 MiB               sss = None\r\n\r\nFilename: test.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    27     89.3 MiB      0.0 MiB   @profile\r\n    28                             def main():\r\n    29   2394.2 MiB   2304.9 MiB       out = [np.random.random((1, 224, 224, 3)) for i in range(2000)]\r\n    30   2983.0 MiB    588.8 MiB       _write_into_log(out)\r\n    31    678.3 MiB  -2304.7 MiB       out = None\r\n```\r\n\r\n", "We have found the fix to this memory leak issue- it's a minor tweak. We'll send in the pull request as soon as our CLA gets approved (we're just waiting for that).", "This has been resolved with #8981 ", "@falaktheoptimist , can you explain why it will cause memory leak when queue item is not passed directly?", "The thread was not being killed till python termination and hence the reference to event (queue item) was never being freed.", "@falaktheoptimist , I get what you mean. The reference count of the last element in the queue will never decrease, because `self._queue.get()` will block forever.", "@suiyuan2009 Exactly.. Thanks to your PR- it's fixed now.."]}, {"number": 8264, "title": "Auto-Configuration Error: Cannot find cudnn.h at /usr/lib/x86_64-linux-gnu/include/cudnn.h", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n#4397, but it's for 16.04\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04\r\nInstalled version of CUDA and cuDNN: 8.0, 5.1\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\nls -l /path/to/cuda/lib/libcud*\r\n-rw-r--r-- 1 root 543K Jan 26 15:48 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root   16 Jan 26 15:51 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root   19 Jan 26 15:51 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rw-r--r-- 1 root 406K Jan 26 15:48 /usr/local/cuda/lib64/libcudart.so.8.0.61\r\n-rw-r--r-- 1 root 757K Jan 26 15:48 /usr/local/cuda/lib64/libcudart_static.a\r\n```\r\n```\r\nll /usr/lib/x86_64-linux-gnu/libcudnn*                                                                                                                                                                \r\nlrwxrwxrwx 1 root  29 Mar  1 11:58 /usr/lib/x86_64-linux-gnu/libcudnn.so -> /etc/alternatives/libcudnn_so\r\nlrwxrwxrwx 1 root  18 Nov  6 23:19 /usr/lib/x86_64-linux-gnu/libcudnn.so.5 -> libcudnn.so.5.1.10\r\n-rw-r--r-- 1 root 81M Nov  6 23:19 /usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.10\r\nlrwxrwxrwx 1 root  32 Mar  1 11:58 /usr/lib/x86_64-linux-gnu/libcudnn_static.a -> /etc/alternatives/libcudnn_stlib\r\n-rw-r--r-- 1 root 68M Nov  6 23:19 /usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a\r\n```\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`): e895d5ca395c2362df4f5c8f08b68501b41f8a98\r\n2. The output of `bazel version`: \r\n```\r\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\r\nBuild label: 0.4.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Dec 7 18:47:11 2016 (1481136431)\r\nBuild timestamp: 1481136431\r\nBuild timestamp as int: 1481136431\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI ran\r\n```\r\n./configure\r\n```\r\nIt failed with\r\n```\r\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\r\nERROR: package contains errors: tensorflow/compiler/tests.\r\nERROR: error loading package 'tensorflow/compiler/tests': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 816\r\n                _create_cuda_repository(repository_ctx)\r\n        File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 729, in _create_cuda_repository\r\n                _get_cuda_config(repository_ctx)\r\n        File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 585, in _get_cuda_config\r\n                _cudnn_version(repository_ctx, cudnn_install_base..., ...)\r\n        File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 296, in _cudnn_version\r\n                _find_cuda_define(repository_ctx, cudnn_install_base..., ...)\r\n        File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 271, in _find_cuda_define\r\n                auto_configure_fail(\"Cannot find cudnn.h at %s\" % st...))\r\n        File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 93, in auto_configure_fail\r\n                fail(\"\r\n%sAuto-Configuration Error:%s ...))\r\n\r\nAuto-Configuration Error: Cannot find cudnn.h at /usr/lib/x86_64-linux-gnu/include/cudnn.h\r\n```\r\n\r\nI noticed the cudnn.h is in /usr/include/cudnn.h somehow, while its libs are in /usr/lib/x86_64-linux-gnu/.\r\n\r\nI read cuda_configure.bzl, but didn't find an easy way to let it find both cudnn headers and libs.\r\n", "comments": ["I am unable to reproduce this error with the information you provided.\r\nWhen I sync to e895d5c, the configure script works fine for me.  \r\nAt what point is it failing?  the error messages seem to be coming from bazel?\r\n\r\nCould you please capture what responses you provided to the ./configure script and all the command lines you typed.\r\n\r\nAlso, could you please check that you installed all required dependencies as described in this page:\r\nhttps://www.tensorflow.org/install/install_sources\r\n", "sure.\r\n```\r\n \u276f ./configure                                                                                                                                                                                           Please specify the location of python. [Default is /usr/bin/python]:\r\nPlease specify optimization flags to use during compilation [Default is -march=native]:\r\nDo you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n]\r\njemalloc enabled on Linux\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]\r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/lib/x86_64-linux-gnu\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 3.0,3.5,5.2,6.0,6.1\r\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\r\nERROR: package contains errors: tensorflow/stream_executor.\r\nERROR: error loading package 'tensorflow/stream_executor': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 815\r\n                _create_cuda_repository(repository_ctx)\r\n        File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 728, in _create_cuda_repository\r\n                _get_cuda_config(repository_ctx)\r\n        File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 584, in _get_cuda_config\r\n                _cudnn_version(repository_ctx, cudnn_install_base..., ...)\r\n        File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 295, in _cudnn_version\r\n                _find_cuda_define(repository_ctx, cudnn_install_base..., ...)\r\n        File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 270, in _find_cuda_define\r\n                auto_configure_fail(\"Cannot find cudnn.h at %s\" % st...))\r\n        File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 93, in auto_configure_fail\r\n                fail(\"\r\n%sAuto-Configuration Error:%s ...))\r\n\r\nAuto-Configuration Error: Cannot find cudnn.h at /usr/lib/x86_64-linux-gnu/include/cudnn.h\r\n.\r\n```\r\n\r\nThe cudnn.h was installed in\r\n```\r\n/usr/include/cudnn.h\r\n```\r\nwhich links to \r\n```\r\nlrwxrwxrwx 1 root 26 Mar  1 11:58 /usr/include/cudnn.h -> /etc/alternatives/libcudnn\r\n```\r\nand again\r\n```\r\nlrwxrwxrwx 1 root 40 Mar  1 11:58 /etc/alternatives/libcudnn -> /usr/include/x86_64-linux-gnu/cudnn_v5.h\r\n```\r\nI read cuda_configure.bzl, but didn't find an easy way to let it find the cudnn headers.", "How did you install cuDNN?  This normally results in a directory containing subdirs for `include` and `lib` so a single path serves to locate both the library and header files.\r\n", "It's a docker using nvidia/cuda:8.0-cudnn5-devel-ubuntu14.04\n\nThanks,\n-B\n\nOn Fri, Mar 10, 2017 at 11:59 AM, Paul Barham <notifications@github.com>\nwrote:\n\n> How did you install cuDNN? This normally results in a directory containing\n> subdirs for include and lib so a single path serves to locate both the\n> library and header files.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8264#issuecomment-285769453>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA1O1c2NOwnEns9Kl9e4l_zL4esi7rxSks5rkau7gaJpZM4MY_0i>\n> .\n>\n", "Could you please take a look at the instructions [here](https://www.tensorflow.org/install/install_linux#gpu_support) for using an nvidia GPU docker container and see if there are any steps you missed?\r\n\r\n(admittedly, that page seems to mention TF0.12 so it may be slightly out of date)\r\n\r\n@gunan Do you have anything to add here?", "I think this happens when cudnn is installed using the deb package. @byzhang could you confirm how you installed cudnn?", "I'm following\nhttps://gitlab.com/nvidia/cuda/blob/ubuntu14.04/8.0/devel/cudnn5/Dockerfile\nto install the cudnn within the docker:\n```\n\nRUN echo \"deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1404/x86_64\n/\" > /etc/apt/sources.list.d/nvidia-ml.listENV CUDNN_VERSION\n5.1.10LABEL com.nvidia.cudnn.version=\"${CUDNN_VERSION}\"RUN apt-get\nupdate && apt-get install -y --no-install-recommends \\\nlibcudnn5=$CUDNN_VERSION-1+cuda8.0 \\\nlibcudnn5-dev=$CUDNN_VERSION-1+cuda8.0 && \\\n\nrm -rf /var/lib/apt/lists/*\n```\n\n\nThanks,\n-B\n\nOn Fri, Mar 10, 2017 at 6:31 PM, gunan <notifications@github.com> wrote:\n\n> I think this happens when cudnn is installed using the deb package.\n> @byzhang <https://github.com/byzhang> could you confirm how you installed\n> cudnn?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8264#issuecomment-285835099>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA1O1Rt4awKG86qQEXSIWIv-Qjxa1fhgks5rkgeJgaJpZM4MY_0i>\n> .\n>\n", "As a very quick solution, you may create symlinks to cudnn headers in `/usr/local/cuda/include` and to all cudnn libraries in `/usr/local/cuda/lib64`. \r\n@flx42 Any idea about any changes in cudnn deb packages that may cause this?\r\n", "> This normally results in a directory containing subdirs for include and lib so a single path serves to locate both the library and header files.\r\n\r\n@prb12: that's not the case with deb packages as @gunan mentioned.\r\n\r\n@gunan: I'm not sure what your question is. But the path `/usr/lib/x86_64-linux-gnu/include/cudnn.h` doesn't make sense. There is probably a problem when configuring TensorFlow.", "I thought with deb packages, in the end we had symlinks for cudnn.h and library files that provide a structure as @prb12 described. It looks like that is not the case.\r\nI think for the error message, what happened is user entered `/usr/lib/x86_64-linux-gnu/` as cudnn root directory. Our source code assumes what @prb12 described, so `/usr/lib/x86_64-linux-gnu/include/cudnn.h` is probably constructed by our code.\r\n\r\nWe will need to update our configure script to avoid this assumption.", "`/usr/include/cudnn.h` is indeed a symlink, but that's for allowing users to switch cuDNN versions on the same machine through Debian alternatives:\r\n```\r\n/usr/include/cudnn.h -> /etc/alternatives/libcudnn -> /usr/include/x86_64-linux-gnu/cudnn_v5.h\r\n```\r\ncuDNN v4 was different though, the actual header was at `/usr/include/cudnn_v4.h`", "Yes, I took a quick fix by symlinks. But it's appreciated if the configure\nscripts is more flexible.\n\nThanks,\n-B\n\nOn Sat, Mar 11, 2017 at 3:20 PM, gunan <notifications@github.com> wrote:\n\n> As a very quick solution, you may create symlinks to cudnn headers in\n> /usr/local/cuda/include and to all cudnn libraries in\n> /usr/local/cuda/lib64.\n> @flx42 <https://github.com/flx42> Any idea about any changes in cudnn deb\n> packages that may cause this?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8264#issuecomment-285908165>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA1O1dkVQeUTL3lbxx5WJ80gUwE7O6J5ks5rkywvgaJpZM4MY_0i>\n> .\n>\n", "Sorry for the inconvenience @byzhang \r\nI intend to fix the script to handle installation through deb packages soon.\r\nUntil then, I just wanted to get you unblocked as quickly as possible using symlinks.", "I get a similar issue:\r\n\r\nThis is a very frustrating process... Trying to build Java GPU support for Windows...\r\n\r\n```\r\n$ bazel build --config opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni\r\nERROR: C:/development/projects/tensorflow/tensorflow/java/BUILD:142:1: error loading package 'tensorflow/java/src/main/native': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 958\r\n                _create_cuda_repository(repository_ctx)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 846, in _create_cuda_repository\r\n                _get_cuda_config(repository_ctx)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 656, in _get_cuda_config\r\n                _cudnn_install_basedir(repository_ctx)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 211, in _cudnn_install_basedir\r\n                auto_configure_fail(\"Cannot find cudnn install path....)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 128, in auto_configure_fail\r\n                fail(\"\\n%sAuto-Configuration Error:%s...))\r\n\r\nAuto-Configuration Error: Cannot find cudnn install path.\r\n and referenced by '//tensorflow/java:libtensorflow_jni.so'.\r\nERROR: Analysis of target '//tensorflow/java:tensorflow' failed; build aborted.\r\nINFO: Elapsed time: 0.641s\r\n```\r\n", "hi @byzhang @gunan @flx42  i install the tensorflow by the source.\r\n i meet the similar error \r\nwhen i run `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package `\r\n \r\n\r\n> Cuda Configuration Error: Cannot find cudnn.h under /usr/local/lib\r\n \r\nThe cuda is `7.5`  and cudNN is `5.0.5`\r\nI install the cuDNN by\r\n\r\n> cd ~\r\nsudo tar xvf cudnn-7.5-linux-x64-v5.0.tgz\r\n cd cuda/include\r\nsudo cp *.h /usr/local/include/\r\ncd ../lib64\r\nsudo cp lib* /usr/local/lib/\r\ncd /usr/local/lib# sudo chmod +r libcudnn.so.5.0.5\r\nsudo ln -sf libcudnn.so.5.0.5 libcudnn.so.5\r\nsudo ln -sf libcudnn.so.5 libcudnn.so\r\nsudo ldconfig\r\n\r\nwhen i run `.configure`,the it show me \r\n\r\n> Do you wish to build TensorFlow with MPI support? [y/N] \r\nMPI support will not be enabled for TensorFlow\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nConfiguration finished\r\n\r\nNot the following by the link https://www.tensorflow.org/install/install_sources\r\n\r\n> [Default is: \"3.5,5.2\"]: 3.0\r\nSetting up Cuda include\r\nSetting up Cuda lib\r\nSetting up Cuda bin\r\nSetting up Cuda nvvm\r\nSetting up CUPTI include\r\nSetting up CUPTI lib64\r\nConfiguration finished\r\n\r\nDo you give me some advice? Thanks.\r\n", "@nectario for windows please file a separate issue, that will need to be addressed separately.\r\n@alyato, when you are installing cuDNN, the simplest way (AFAICR the way recommended in NVIDIA cuDNN website) is to just do this:\r\n```\r\ncd ~\r\nsudo tar xvf cudnn-7.5-linux-x64-v5.0.tgz\r\nsudo cp -R cuda /usr/local\r\nexport LB_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda/lib\r\n```\r\nIf you do the above, it should just work with configure script.\r\nThis issue is about the problems we see when we have cuDNN installed using the deb packages distributed by NVIDIA.\r\n\r\n", "Thank you. I will file a separate issue. At the moment, I am uninstalling everything as I have several versions of Python installed.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Close due to inactivity for a long time.", "The main reason is cuda version information is not stored in ```cudnn.h``` anymore in CUDA 11.X. In CUDA 11.X, the version information seems to stored in  new ```cudnn_version.h``` file. So many build tools such as cmake depends on ```cudnn.h``` for CUDA version information cannot guess CUDA version anymore."]}, {"number": 8263, "title": "Documentation incorrect for RNN tutorial?", "body": "Is it possible the documentation is incorrect on https://www.tensorflow.org/tutorials/recurrent ?\r\n\r\n```\r\nwords = tf.placeholder(tf.int32, [batch_size, num_steps])\r\nfor i in range(num_steps):\r\n    # The value of state is updated after processing each batch of words.\r\n    output, state = lstm(words[:, i], state)\r\n```\r\n\r\nIt seems the correct output should be:\r\n`    output, state = lstm(words[i, :], state)`\r\n\r\nSince you want to process the words in the same sequence correct?\r\n\r\nE.g.\r\n[[The, quick, brown]\r\n [fox, jumped, over]]\r\n\r\nwords[:,0] == [The, fox]\r\n\r\nWhereas what you want is [The, quick, brown]  == words[0,:]\r\n\r\nPlease correct me if I'm wrong, thanks.", "comments": ["This looks wrong to my admittedly untrained eye (and doesn't match https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py#L146)\r\n\r\n@xmbrst Who is the author of this tutorial?", "Looks like Rafal Jozefowicz, originally.", "Upon looking over this, I see whats being done here. They process in batches and store the running state for each batch. However, what led me to this confusion was the first example in the tutorial takes a batch of words as input, so it doesn't seem to make sense when compared to the Truncated Backprop example.  The initial example probably should be more clear on what a batch of words looks like in this example. Mainly I think many would appreciate just a bit more detail than the pseudo code provided.", "Since you have understood it, could you add annotations (comments) in a PR? ", "@martinwicke Sure. I'm not sure where the code for the tutorials lives however. (https://www.tensorflow.org/tutorials/recurrent). Any thoughts?", "The code lives here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/tutorials/recurrent.md", "@martinwicke https://github.com/tensorflow/tensorflow/pull/9746\r\nHopefully my changes make sense."]}, {"number": 8262, "title": "Set `set +e` in generate.sh (`go generate`) to output correct information", "body": "While playing with go in tensorflow, the `go generate` will not output correct\r\ninformation about missing protoc even though generate.sh contains the following:\r\n```\r\n  PATH_PROTOC=$(which protoc)\r\n  if [ ! -x \"${PATH_PROTOC}\" ]\r\n  then\r\n    echo \"Protocol buffer compiler protoc not found in PATH or in ${PROTOC}\"\r\n    echo \"Perhaps build it using:\"\r\n    echo \"bazel build --config opt @protobuf//:protoc\"\r\n    exit 1\r\n  fi\r\n  PROTOC=$PATH_PROTOC\r\n```\r\n\r\nInstread, a non-informative error is displayed:\r\n```\r\nroot@99829f070f46:/go/src/github.com/tensorflow/tensorflow# go generate github.com/tensorflow/tensorflow/tensorflow/go/op\r\n../genop/main.go:15: running \"sh\": exit status 1\r\ntensorflow/go/op/generate.go:15: running \"go\": exit status 1\r\nroot@99829f070f46:/go/src/github.com/tensorflow/tensorflow#\r\n```\r\n\r\nThe reason is that `set -e` is at the beginning of the script and it exit\r\nimmediately at `PATH_PROTOC=$(which protoc)`.\r\n\r\nThis fix sets `set +e` before `PATH_PROTOC=$(which protoc)` and restores `set -e`\r\nback, so that information about missing protoc outputed.\r\n\r\nBelow is the new output which helps in finding out the reason for the build error:\r\n```\r\nroot@99829f070f46:/go/src/github.com/tensorflow/tensorflow# go generate github.com/tensorflow/tensorflow/tensorflow/go/op\r\nProtocol buffer compiler protoc not found in PATH or in /go/src/github.com/tensorflow/tensorflow/bazel-out/host/bin/external/protobuf/protoc\r\nPerhaps build it using:\r\nbazel build --config opt @protobuf//:protoc\r\n../genop/main.go:15: running \"sh\": exit status 1\r\ntensorflow/go/op/generate.go:15: running \"go\": exit status 1\r\nroot@99829f070f46:/go/src/github.com/tensorflow/tensorflow#\r\n```", "comments": ["Can one of the admins verify this patch?", "Cool, thanks for the fix.\r\n@tensorflow-jenkins Test this please"]}, {"number": 8261, "title": "the error of using placeholder in summary.scalar", "body": "class Model(object):\r\n\r\n  def __init__(self,\r\n               images=None,\r\n               actions=None,\r\n               states=None,\r\n               sequence_length=None,\r\n               reuse_scope=None):\r\n    self.prefix = prefix = tf.placeholder(tf.string, [])\r\n    summaries.append(\r\n          tf.summary.scalar(prefix + '_recon_cost' + str(i), recon_cost))\r\n\r\n\r\n\r\nget the error of *** TypeError: expected string or bytes-like object, use the old summary_scalar is ok\r\n\r\n", "comments": ["Please see https://www.tensorflow.org/api_docs/python/tf/summary/scalar\r\nThe first argument should be a **Python** string `name` which must be known statically (you are passing a string tensor whose value can't be determined until session.run time).\r\nThe second arg should be a tensor... I can't tell from your code fragment what the type of `recon_cost` is.\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> prefix = tf.placeholder(tf.string, [])\r\n>>> print prefix\r\nTensor(\"Placeholder:0\", shape=(), dtype=string)\r\n>>> print prefix + 'some string'\r\nTensor(\"add:0\", shape=(), dtype=string)\r\n>>> t = tf.constant(7)\r\n>>> tf.summary.scalar(prefix + 'string', t)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/google/home/pbar/tensorflow.virtualenv/local/lib/python2.7/site-packages/tensorflow/python/summary/summary.py\", line 114, in scalar\r\n    name = _clean_tag(name)\r\n  File \"/usr/local/google/home/pbar/tensorflow.virtualenv/local/lib/python2.7/site-packages/tensorflow/python/summary/summary.py\", line 86, in _clean_tag\r\n    new_name = _INVALID_TAG_CHARACTERS.sub('_', name)\r\nTypeError: expected string or buffer\r\n>>> tf.summary.scalar('string', t)\r\n<tf.Tensor 'string:0' shape=() dtype=string>\r\n>>>\r\n```\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@prb12  Interesting, in 0.11 the summary_scalar function only can handle a tensor as input name, but now in 1.0,summary.scalar can only handle a python string? That means the use of placeholder as a prefix-name in summary in 0.11 can not be implemented in 1.0 version unless you open a session to get eval(), very interesting change, may I ask is there any alternative way to use placeholder for summary.scalar without the eval() in a session. As the old summary_scalar function give the use of placeholder as a tutorial.", "@dandelionmane  Perhaps you could comment on the summary API changes?  This does seem to be outside the use case for which they were designed (i.e. emitting lots of different summaries with runtime-generated names).  "]}, {"number": 8260, "title": "native Cuda kernel for tf.dynamic_stitch op (reopened PR)", "body": "I screwed up when squashing commits in #7764, so I started a clean pull request. @girving Please take a look at this, thank you. *(and I sincerely apologize for my horrible git-fu)*", "comments": ["Can one of the admins verify this patch?", "I took a look.  For incremental changes, please leave the current commit as is and push new commits on top, *without* pulling in from master.", "@girving Thank you for the review! After reading your feedback, I realized that I totally omitted the cases where there are collisions in the output indices when I wrote the code, and since collisions never happened in my own use cases, I never caught this major flaw in my mind.\r\n\r\nAt first, I thought it would be hard to let GPU mimic how current CPU implementation handles collisions, but then I realized that some rather simple modifications to my current code would achieve the same effect.\r\n\r\nI wrote the details under the relevant part of the code review, please take a look. I want to make sure the approach is right before I start modifying the code. Thank you very much!", "@girving what do you think of @MycChiu 's proposed approach in the last comment?", "Apologies for missing the notifications earlier.  Looking now.", "Oh, it looks like I already commented that the proposed adjustment didn't work.  @drpngx Do I have the flow right, or did I miss something new?", "Oh, OK thanks @girving ! @MycChiu could you propose an alternative?", "ping @MycChiu ", "Another ping for @MycChiu (it looks like they are not around lately).", "I think we should close for now.", "for those that come across this see https://github.com/tensorflow/tensorflow/pull/11940 for a merged implementation"]}, {"number": 8259, "title": "[Request]Binary package with fast c++ implementation of protobuf support for Python 3.6?", "body": "As said in [this doc page](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/install/install_linux.md), the binary package is supported for Python 3.5 and 2.7, is it also possible for Python 3.6?", "comments": ["@gunan Who would be the right person to assign this to?  (you commented on #5997 which seems similar)", "if you download the 3.5 pip package, and rename it by replacing all \"35\" with \"36\", it should just work.\r\nCould you try it out?", "It seems to work fine for now. "]}, {"number": 8258, "title": "Revert \"Fix compile warnings.\"", "body": "Reverts tensorflow/tensorflow#8206\r\n\r\nThis PR unfortunately broke the XLA build (at least internally).\r\n\r\nI understand @gunan is working on adding XLA presubmits to our Jenkins testing. I think they are maybe available now but not enabled by default? Please get gunan's help to resubmit.", "comments": ["@dandelionmane what was the error?", "The XLA build is ready, but there was a problem with it broadcasting its results to github.\r\nNow that should be fixed.\r\nHowever continuous XLA build on github seems to be OK. Maybe we are having an issue with compiler versions?", "@gunan do you want me to resubmit? On gcc 4.8, it compiles fines. Please let me know the error.", "@hawkinsp is this failure addressed by #8276 ?\r\nShould we merge this PR, or wait for yours?", "@conqer I am exploring how we can reproduce the issue on jenkins. Looks like we had a few failures with macos, I am guessing that come from clang.", "@gunan Thanks. Please let me know if I can help."]}, {"number": 8257, "title": "Fix docs for pool and convolution functions", "body": "On https://www.tensorflow.org/api_docs/python/tf/nn/pool \r\n\r\n* 2 code blocks are not highlighted correctly because of semicolons come before them \r\n* Missing new line for padding explanation \r\n\r\nOn https://www.tensorflow.org/api_docs/python/tf/nn/convolution\r\n\r\n* 1 code block is not highlighted correctly because of semicolons come right before it. ", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins Test this please"]}, {"number": 8256, "title": "Variable for Go?", "body": "Is Variable being worked on in the Go bindings?\r\n", "comments": ["@asimshankar Could you comment on this pls?\r\n", "Well i don't see a Go equivalent of python's tf.Variable. I was trying to work out a TensorFlow tutorial in Go instead of python but the absence of Variable made it impossible. I did spot some 'low-level' operators that seem related to variables, and thought maybe the Variable functionality can be built on top of that?\r\n", "You're absolutely right on both counts :) - The Go API doesn't provide the equivalent of `tf.Variable` but it does provide enough to build that functionality.\r\n\r\nI took at stab at a quick implementation, see: https://github.com/asimshankar/go-tensorflow/tree/master/variable\r\n\r\nThere are a few caveats here, such as:\r\n- This example is using an in-progress, not-yet-finalized implementation for variables using the `DT_RESOURCE` type\r\n- The Python `tf.Variable` class does a few more things, like explicitly colocating all the ops on the same device and adding the created variable to a list of variables kept in the `MetaGraphDef` (using Collections).\r\n\r\nDepending on your use case, these may not matter. \r\n\r\nI'm going to mark this as \"Contributions Welcome\" as we don't have any immediate plans to pursue this, but if anyone else wants to, we'd be happy to guide them.\r\n\r\n(FYI @jhseu)", "Thanks! I didn't expect it could be implemented in so few lines of code. I'm not a TensorFlow expert (yet), just sniffing around basically :)\r\nThe tf.device class isn't available in Go either, and regarding this functionality, i didn't even see any C library functions to handle devices?  ", "To colocate operations, you need to specify the colocation constraint when creating the operation. This is the `TF_ColocateWith` call in the C library, which should eventually make its way to the `Scope` type in Go.\r\n\r\nTo specify a particular device, it's `TF_SetDevice` in the C library, which should also eventually make its way to the `Scope` type in Go.\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 8255, "title": "Ordering the runs in TensorBoard's embeddings tab", "body": "Feature request: it would be nice if the  runs in TensorBoard's embeddings tab were ordered.\r\n\r\nAs of now it is unordered, e.g.:\r\n\r\n![image](https://cloud.githubusercontent.com/assets/15331/23777190/d3eaabfc-0500-11e7-93f1-657a9d9bd35c.png)\r\n\r\nCorresponding Stack Exchange question: [What determine the order of the runs in TensorBoard's embeddings tab?](http://stackoverflow.com/q/42684521/395857)", "comments": ["@dandelionmane Looks like an easy one?", "@dsmilkov for the embedding projector", "This issue has been migrated to https://github.com/tensorflow/tensorboard/issues/78."]}, {"number": 8254, "title": "Expose SKCompat.", "body": "Partial fix for #7287.", "comments": []}, {"number": 8253, "title": "Reoccurring issue with tensorflow-gpu install", "body": "Hi all,\r\n\r\nThere is still this reoccurring issue with the tensorflow-gpu install with the latest version from pip.\r\n\r\nWhen i try and test using the \"Hello World\" constant I get the below errors even though it works.\r\n\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots", "comments": ["Have the same problem!", "It seems this is the same problem but for the cpu version (20 days ago): https://github.com/tensorflow/tensorflow/issues/7500\r\n\r\nThe fix is to install a nightly build or wait for next release. When is it planned (for gpu)?\r\n", "Is there a nightly that fixes the problem for gpu? Please specify instruction on how to update.", "Tried the nightly build but that gets me into other issues which are supposed to be fixed. ", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!\r\n\r\nI'm guessing you are on some variant of Windows from the pathnames?", "windows 10 64-bit. tensorflow-gpu 1.0.1", "Closing as duplicate of #8229", "Have the same problem!\r\nwindows 10 64-bit. tensorflow-gpu 1.0.1. python 64bit 3.5.2", "I have the same problem. #8229 did not help either", "I was able to run all my tensorflow programs with out any problems on the cpu version, but since I installed CUDA8.0 and CuDDN5.1 none of my programs run anymore without errors:\r\n\r\n\"C:\\Program Files\\Python35\\python.exe\" D:/tobylabtop/AI/tensorflowbook/chapters/06_recurrent_neural_networks_and_natural_language_processing/01_wikipedia/tobytest.py\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 960M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.176\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.65GiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0 \r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y \r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0)\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nb'Hello, TensorFlow!'\r\n\r\nProcess finished with exit code 0\r\n", "@prb12 can this issue please be reopened? As pointed out, this seems to be a recent regression.\r\nWhen I run Win GPU nightly build 105 in a conda environment, I do not experience the above warnings as opposed to the Win GPU 1.0.0 build.\r\n\r\nHaving just created a conda environment with Win GPU 1.0.1, it re-appears. Thx.", "@tomwanzek I don't understand your comment ... you're saying the problem is fixed in the nightly build 105 (from March 7th) ... but you want to re-open the issue because the older binary package has a problem?  \r\n", "@prb12 No, the latest Win GPU v1.0.1 release seems to be the issue. In the Jenkins build history the first wheel referring to v1.0.1 would be build 108, to be precise.\r\n\r\nWhile I installed v1.0.1 from \r\n`https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.0.1-cp35-cp35m-win_amd64.whl` as per tensorflow installation guide, I am assuming that it should correspond to build 108 given the patch version numbers.\r\n\r\nThis version does exhibit the behavior previously fixed in by 105.\r\n\r\nSo this seems to be a recent regression of sorts. If need be, I could download the wheel for 108 directly from Jenkins and install to verify.\r\n\r\nHope this clarifies.", "@tomwanzek Thanks for the clarification and delving through the version numbers! \r\n\r\nMaybe the 1.0.1 release was cherry-picked in a strange way.  @gunan Do you know why this fix may have reverted?\r\n", "No worries, I have not check the nightlies between 105 and 108 to pin it down further...", "@mrry for windows @jhseu for the cherrypicks on the release 1.0.1\r\n\r\nAny idea why this resurfaced in 1.0.1?", "@tomwanzek the release is radically different from nightly builds.\r\nThe release branch base is synced to January 15 of master. Then we cherrypicked many fixes into the release branch. So there are no nightlies that have 1-1 correspondence to any of our releases.", "@gunan Ah, did not realize you cherrypicked. I assumed, the semantic version releases were also cut from `master` whenever one was ready to ship. So while the nightly build might not be spot on, I thought them to be sufficiently close to the release build. Should have been digging deeper I suppose...:smile:", "@gunan It would be good to get this into 1.0.2 if there's going to be such a release (I think a few other bug fixes might have fallen into the cracks as well). Are we planning to have one?", "we will start working on 1.1 soon, therefore I don't think a 1.0.2 release is likely.\r\nSince this issue is resolved at head, are we ok to close this issue?", "@all if we need to get rid of these messages now, what is the option?  I assume the nightly have moved on to the version that does not have the patch that suppresses the warning messages.  \r\n\r\nI ask this because I am building training classes and do screen captures.  I need to know if I can rerun them soon without the messages, or need to figure out a way of manually deleting these from my captures.", "Nightlies still have the patch. The patch was never rolled back.\r\nRather, release branch was so old that it never really had the patch.\r\n\r\n", "@gunan.  Thanks.  I will switch to the nightly.", "Want to confirm this is (still) a problem with the CPU version - TF 1.0.1 - for Windows 10.\r\n\r\nUpgraded to TF 1.0.1 (and keras 2.0.1)  on Windows 10. Using Anaconda but the TF/keras install is with native pip ie.\r\n\r\n> pip uninstall tensorflow\r\n> pip install -U tensorflow\r\n> pip uninstall keras\r\n> pip install -U --no-deps keras\r\n\r\nErrors are:\r\n\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for\r\nunknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"')\r\nfor unknown op: UpdateFertileSlots", "Correct, it is still a problem with 1.0.1.\r\nPlease see the messages above for more information.", "@gunan I'm assuming this has been cherry-picked into a release. I'm gonna close this, please reopen if it's still an issue."]}, {"number": 8252, "title": "Add missing sparse base columns definitions and change parameter name", "body": "There are couple of issues which prevent the code from tutorial to run immediately after copy-pasting:\r\n- build_estimator() is missing definitions of two sparse base columns (\"race\" and \"marital status\")\r\n- \"dense_shape\" parameter is named \"shape\" when creating SparseTensor in input_fn()", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins Test this please", "@tensorflow-jenkins Test this please"]}, {"number": 8251, "title": "pip3 install --upgrade tensorflow-gpu: No matching distribution found for tensorflow-gpu", "body": "Following the installation guide at https://www.tensorflow.org/install/install_windows\r\nInstalled:\r\n- CUDA (cuda_8.0.61_win10.exe)\r\n- cuDDN (cudnn-8.0-windows10-x64-v5.1.zip)\r\n- Python 3.6 x64 (python-3.6.0-amd64.exe)\r\n\r\nThen issued:\r\nC:\\>pip3 install --upgrade tensorflow-gpu\r\nCollecting tensorflow-gpu\r\n  Could not find a version that satisfies the requirement tensorflow-gpu (from versions: )\r\nNo matching distribution found for tensorflow-gpu\r\n\r\nSame happens with non-gpu version:\r\nC:\\>pip3 install --upgrade tensorflow\r\nCollecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System: Windows 10 x64\r\n\r\nInstalled version of CUDA and cuDNN: \r\ncudnn-8.0-windows10-x64-v5.1.zip \r\ncuDDN/bin: cudnn64_5.dll\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nJust follow the provided instructions on https://www.tensorflow.org/install/install_windows\r\n\r\n### What other attempted solutions have you tried?\r\nFirst I tried 32-bit python but found on SO that is not supported (install guide should state which python is requrired 32 or 64)\r\n\r\nI wanted specify the correct URL for tensorflow-gpu, but I don't know which is the correct one for r1.0 gpu. Like this (example):\r\npip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\r\n\r\nPlease provide correct url - on storage.googleapis.com it is not possible to browse the directory contents. I need tensorflow r1.0 x64 gpu!\r\n\r\ngot the hint from here: http://stackoverflow.com/questions/38896424/tensorflow-not-found-in-pip\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Tensorflow GPU supports up to and including Python 3.7 refer to [supported versions](https://www.tensorflow.org/install/pip#package-location)", "Indeed! Thanks - the documentation says so but I though anything newer is ok also. \r\nMy bad.\r\n\r\nNow it installs fine:\r\n\r\nC:\\>pip3 install --upgrade tensorflow-gpu\r\nCollecting tensorflow-gpu\r\n  Downloading tensorflow_gpu-1.0.1-cp35-cp35m-win_amd64.whl (43.1MB)\r\n    100% |################################| 43.1MB 28kB/s\r\nCollecting wheel>=0.26 (from tensorflow-gpu)\r\n  Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB)\r\n    100% |################################| 71kB 5.1MB/s\r\nCollecting six>=1.10.0 (from tensorflow-gpu)\r\n  Using cached six-1.10.0-py2.py3-none-any.whl\r\nCollecting protobuf>=3.1.0 (from tensorflow-gpu)\r\n  Downloading protobuf-3.2.0-py2.py3-none-any.whl (360kB)\r\n    100% |################################| 368kB 1.7MB/s\r\nCollecting numpy>=1.11.0 (from tensorflow-gpu)\r\n  Downloading numpy-1.12.0-cp35-none-win_amd64.whl (7.7MB)\r\n    100% |################################| 7.7MB 165kB/s\r\nCollecting setuptools (from protobuf>=3.1.0->tensorflow-gpu)\r\n  Downloading setuptools-34.3.1-py2.py3-none-any.whl (389kB)\r\n    100% |################################| 399kB 2.5MB/s\r\nCollecting packaging>=16.8 (from setuptools->protobuf>=3.1.0->tensorflow-gpu)\r\n  Downloading packaging-16.8-py2.py3-none-any.whl\r\nCollecting appdirs>=1.4.0 (from setuptools->protobuf>=3.1.0->tensorflow-gpu)\r\n  Downloading appdirs-1.4.3-py2.py3-none-any.whl\r\nCollecting pyparsing (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorflow-gpu)\r\n  Downloading pyparsing-2.2.0-py2.py3-none-any.whl (56kB)\r\n    100% |################################| 61kB 5.2MB/s\r\nInstalling collected packages: wheel, six, pyparsing, packaging, appdirs, setuptools, protobuf, numpy, tensorflow-gpu\r\n  Found existing installation: setuptools 28.8.0\r\n    Uninstalling setuptools-28.8.0:\r\n      Successfully uninstalled setuptools-28.8.0\r\nSuccessfully installed appdirs-1.4.3 numpy-1.12.0 packaging-16.8 protobuf-3.2.0 pyparsing-2.2.0 setuptools-34.3.1 six-1.10.0 tensorflow-gpu-1.0.1 wheel-0.29.0\r\n", "Now I'm stuck with:\r\nhttps://github.com/tensorflow/tensorflow/issues/8253\r\n\r\n:)", "Thanks it worked for me.", "Good.", "From what I can tell, my setup is almost the exact same as the problem described in the first post, but with Python 3.5.\r\n\r\n- Windows 10 (x64)\r\n- CUDA (cuda_8.0.61_win10_network.exe with the applied cuda_8.0.61.2_windows.exe patch)\r\n- CUDNN (cudnn-8.0-windows10-x64-v6.0.zip unzipped and put into the specified spots)\r\n- Python 3.5 (python-3.5.4.exe)\r\n\r\nJust had to wipe the hard drive last night, so this setup is as fresh as fresh can be. (Installed the newest drivers as of yesterday for the GPU.)\r\nRunning the suggested install command restults in:\r\n```\r\nC:\\Windows\\system32>pip3 install --upgrade tensorflow-gpu\r\nCollecting tensorflow-gpu\r\n  Could not find a version that satisfies the requirement tensorflow-gpu (from versions: )\r\nNo matching distribution found for tensorflow-gpu\r\n```\r\nAlthough searching does bring up the specified package.\r\n```\r\nC:\\Windows\\system32>pip3 search tensorflow-gpu\r\n\r\n...\r\n\r\ntensorflow-gpu (1.4.0)                       - TensorFlow helps the tensors\r\n                                               flow\r\n```\r\nAm I overly stupid and missed something glaringly obvious, or is there something more subtle going on here?\r\n\r\nAlso, running the command to get it to fetch from the wheel gives this result:\r\n```\r\nC:\\Windows\\system32>pip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-win_amd64.whl\r\ntensorflow_gpu-0.12.1-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.\r\n```", "@SpencerOs \r\nI just ran into the same problem and got it fixed.\r\nMy problem turned out to be that my python3 installed was 32bit version. \r\nFrom your 'python-3.5.4.exe' seems like you installed the 32 bit version. \r\nI suggest you try with the 64bit version. \r\nI installed 'python-3.6.3-amd64.exe' and it's successfully pip3 installing.", "@jonejkim ran into the same issue, downloading the 64 bit version solved it! Thanks!\r\n\r\nYou can find it [here](https://www.python.org/downloads/release/python-362/)", "I tried Python 3.6.x and 3.7.x (64-bits). Only Python 3.5.x works for \"tensorflow-gpu\". FYI.", "python 3.6.x 64 bit **is working** for me with\r\n`pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.1-cp36-cp36m-linux_x86_64.whl`\r\n\r\nnewer packages you might find under: https://www.tensorflow.org/install/install_linux#the_url_of_the_tensorflow_python_package\r\n", "Is there a windows version that works on python 3.6 onwards? Unable to locate an answer that works this morning that is all.", "I think you have installed 32 bit version of python. That is why it is not working. Try to install 64-bit version of python and then install tensorflow-gpu.", "@Anchal-Mittal you saved my life! :D\r\nIt was so obvious  that I didn't see it", "Try this:\r\npip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl", "How do I change my python to 3.5.x?\r\n> Tensorflow GPU only works with Python 3.5.x", "Consider to replace pip3 with pip.", "AllenYLJiang still doenst work", "Just tried this\r\n`pip install --upgrade tensorflow_gpu-1.10.1-cp36-cp36m-linux_x86_64.whl`\r\nand I get\r\n`tensorflow_gpu-1.10.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform`\r\nLooks like python 3.7 is not supported yet.", "@arcosmin is this still true? Where does tensorflow document compatible versions?", "It just doesn't work on python 3.7", "@adammenges - it seems to now support 3.6 as well. don't think there's a dedicated compatibility area. it's usually mentioned in the install instructions. if there's no package for it then it's not supported. https://www.tensorflow.org/install/pip?lang=python3#package-location", "`pip install tensorflow-gpu` does work for 3.6 but only up to 3.6.4.\r\nI was having some issues with pip on Windows 10 not finding the wheel on 3.6.5 and above. That could be also only related to the gpu version but I'm not sure. Either way, if you are having trouble with 3.6 still, try 3.6.4 specifically.", "I have python 3.7. How can i change it to python 3.5?", "@Saisuchith - depends how it has been installed. If it's been installed directly then uninstall 3.7 and install 3.5. If you installed it with Anaconda then you can just create a new environment with a different Python version. Google is your friend here.", "If you use Anaconda, use python 3.6. refer [here](http://docs.anaconda.com/anaconda/user-guide/faq/#how-do-i-get-the-latest-anaconda-with-python-3-5).\r\nJust run below.\r\n\r\n```\r\n$ conda create -n py36 python=3.6 anaconda\r\n$ conda activate py36\r\n(py36) $ pip install tensorflow-gpu\r\n```", "Thx a lot @pilhoon, it s solve my issue!!!", "**Try this**\r\npython3 -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl", "`pip install tensorflow-gpu==1.14.0` will not work for mac because it does not exists on the pypi directory as listed on the link below\r\n[https://pypi.org/project/tensorflow-gpu/1.14.0/#files](https://pypi.org/project/tensorflow-gpu/1.14.0/#files)", "There is no GPU for mac", "> Try this:\r\n> pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl\r\n\r\nworked for me"]}, {"number": 8250, "title": ".", "body": "", "comments": ["There is a section in this [page](https://www.tensorflow.org/versions/master/get_started/os_setup\r\n) which suggests that configuration flags need to be supplied at the `./configure` step.  (Search for \"Optimizing CPU Performance\")\r\n\r\n>To be compatible with as wide a range of machines as possible, TensorFlow defaults to only using SSE4 SIMD instructions. Most modern computers support more advanced instructions. So if you're building a binary that you'll only be running on your own machine, you can enable these by using -march=native for optimization options when running configure. Then you can build your optimized binaries with the following command:\r\n$ bazel build --config opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nLooking at the configure script, it seems to examine the environment variable `CC_OPT_FLAGS` and add flags to the `tools/bazel.rc` file.   (maybe you have build a highly optimized `build_pip_package` binary which then builds a slow tensorflow package?)  Perhaps worth giving this a go?\r\n\r\nI don't know off the top of my head why flags passed on the bazel command line aren't being respected. @gunan Any ideas?", "The warnings about sse4.2 or AVX are completely benign, and can be ignored. It just means that TensorFlow can run faster on your machine if you build from sources. But everything will run OK on your machine, you can safely ignore these warnings.\r\n\r\nFrom what I can see, You did not actually clone the code, but you tried to build our sources.\r\nEspecially the first bazel command you have above does not do anything at all (no build target specified).\r\nUnless you clone our github repo, run configure, build the code, then build the pip package and install the resulting pip package, bazel commands themselves will not have any effect. For that, you need to follow  our the instructions here:\r\nhttps://www.tensorflow.org/install/install_sources\r\n\r\nI will close this issue, as the user clearly did not follow the instructions to build from sources.\r\nPlease create a new issue if you run into issues with the instructions to install from sources.\r\n"]}, {"number": 8249, "title": "use sed separator compatible with busybox sed", "body": "busybox's `sed` errors with '[' as separator. This becomes an issue when\r\ntrying to run `tensorflow/tools/ci_build/builds/configured` on Alpine\r\nLinux.", "comments": ["Can one of the admins verify this patch?", "An easy way to confirm:\r\n```\r\n$ docker run -it --rm busybox /bin/sh -c \"echo abc | sed -e 's[abc[xyz[g'\"\r\nsed: unmatched '['\r\n$ docker run -it --rm busybox /bin/sh -c \"echo abc | sed -e 's|abc|xyz|g'\"\r\nxyz\r\n```", "@tensorflow-jenkins Test this please"]}, {"number": 8248, "title": "Fix: Join checked thread in random_shuffle_queue_test.", "body": "", "comments": ["closing pr, fix sent internally."]}, {"number": 8247, "title": "\"ValueError: None values not supported.\"  when using bidirectional_dynamic_rnn", "body": "Using bidirectional_dynamic_rnn throws the \"ValueError: None values not supported\" when you are not specifying the sequence lengths. The error comes from the reverse step, the operator used to reverse the input sequences requires the sequence lengths to be specified. Maybe this should be a required parameter or alternatively generate the lengths array when the default 'None' is used.\r\n\r\n(tensorflow 1.0.0)", "comments": ["@ebrevdo Can you comment on this pls?", "This is a duplicate of #5588 and was fixed in\n16a4bc203b90f0eff07df369d9d8fe6260c99aef.\n\nOn Thu, Mar 9, 2017 at 11:47 AM, Paul Barham <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Can you comment on this pls?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8247#issuecomment-285460284>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9SmqFzLgaNk8umIdkHDEBjQVbesks5rkFdagaJpZM4MYdJI>\n> .\n>\n", "@ebrevdo Thanks!  (sorry I didn't spot the duplicate).\r\nClosing."]}, {"number": 8246, "title": "TensorFlow equivalent to numpy.repeat", "body": "This is a popular question on StackOverflow:\r\nhttp://stackoverflow.com/questions/35361467/tensorflow-numpy-repeat-alternative\r\n\r\nBut note that the answer so far only works for some use cases (the one presented in the question).\r\n\r\nThe best I could come up with for a general solution uses `tf.while_loop`, which is pretty verbose (and maybe slower than necessary). I'll add a link to the implementation I wrote for `tf.contrib.training.resample_at_rate` after the next internal/github sync.", "comments": ["See https://github.com/tensorflow/tensorflow/blob/59ecde3ecfc83aeb8ec0682e4e39bf0a234bbef8/tensorflow/contrib/training/python/training/resample.py#L32 for my example using `tf.while_loop`.", "Dear @shoyer , I am currently working on this with a new op in `array_ops` and python wrapper function. I will update you soon.", "@a-lattas Great! Please replace `_repeat_range` in `resample_at_rate` (linked above) with the new op as part of your PR.", "CC @zycdragonball @sidjee @zuoxingdong", "What's the status of this update? ", "since the thread is still open, I will paste my solution here\r\n```python\r\ndef np_repeat(tensor, repeats):\r\n    \"\"\"\r\n    Args:\r\n\r\n    input: A Tensor. 1-D or higher.\r\n    repeats: A list. Number of repeat for each dimension, length must be the same as the number of dimensions in input\r\n\r\n    Returns:\r\n    \r\n    A Tensor. Has the same type as input. Has the shape of tensor.shape * repeats\r\n    \"\"\"\r\n    assert len(repeats) == tensor.ndim, \"dimension must match\"\r\n    repeated = tensor\r\n    for axis, repeat in enumerate(repeats):\r\n        repeated = np.repeat(repeated, repeat, axis = axis)\r\n    return repeated\r\n\r\ndef tf_repeat(tensor, repeats):\r\n    \"\"\"\r\n    Args:\r\n\r\n    input: A Tensor. 1-D or higher.\r\n    repeats: A list. Number of repeat for each dimension, length must be the same as the number of dimensions in input\r\n\r\n    Returns:\r\n    \r\n    A Tensor. Has the same type as input. Has the shape of tensor.shape * repeats\r\n    \"\"\"\r\n    with tf.variable_scope(\"repeat\"):\r\n        expanded_tensor = tf.expand_dims(tensor, -1)\r\n        multiples = [1] + repeats\r\n        tiled_tensor = tf.tile(expanded_tensor, multiples = multiples)\r\n        repeated_tesnor = tf.reshape(tiled_tensor, tf.shape(tensor) * repeats)\r\n    return repeated_tesnor\r\n\r\ndef repeat_test():\r\n    shape = [1,3,3,3,2]\r\n    repeat = [1,2,2,3,1]\r\n    tensor = np.random.randn(*shape)\r\n    np_repeated_tensor = np_repeat(tensor, repeat)\r\n    tf_tensor = tf.constant(tensor)\r\n    g = tf.get_default_graph()\r\n    tf_new = tf_repeat(tf_tensor, repeat)\r\n    with tf.Session(graph=g) as sess:\r\n        tf_repeated_tensor = tf_new.eval()\r\n#    tf_repeated_tensor = np.array(tf_repeated_tensor)\r\n    if np.allclose(np_repeated_tensor, tf_repeated_tensor):\r\n        print(\"tf_repeat is the same as np_repeat\")\r\n    else:\r\n        print(\"something wrong\")\r\n````", ":+1: for what seems like a basic feature", "need one!", "Added a PR #15224 for the fix.", "@qianyizhang In numpy.repeat we can assign the number of repetitions for each element, e.g. \r\n`x = np.array([[1,2],[3,4]])`\r\n`np.repeat(x, [1, 2], axis=0)=array([[1, 2],\r\n       [3, 4],\r\n       [3, 4]])`", "This looks like it was fixed bu #15224. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "#15224 was indeed a PR to add tf.repeat, but it was closed without merging. Reopening.", "@shoyer , any updates on this?", "I haven't been looking at this, but check the referenced PRs above.", "Unfortunately @qianyizhang's answer doesn't help with the following use of np.repeat:\r\n```\r\nx = np.array([0,1,2])\r\nnp.repeat(x,[3,4,5])\r\n>>> array([0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2])\r\n```\r\n\r\nAlso has there been a merged PR?", "@sachinruk have you found a good solution for that use case? I would love to hear about it. In the meantime, if your scenario permits tf.py_func you can try my simple solution below.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef tf_repeat(arr, repeats):\r\n    return tf.py_func(np.repeat, [arr, repeats], tf.float32)\r\n\r\narr = np.float32([4, 2, 3, 5])\r\nrepeats = np.int32([2, 3, 0, 2])\r\n\r\nwith tf.Session() as sess:\r\n    out = sess.run(tf_repeat(arr, repeats))\r\n    print(out)\r\n\r\n>>> [4. 4. 2. 2. 2. 5. 5.]\r\n```", "At least for 1D tensors, a combination of tf.cumsum and tf.sparse_to_dense should do the trick:\r\n\r\n    import tensorflow as tf\r\n\r\n\r\n    def tf_repeat_1D(x,repeats):\r\n        x = tf.constant(x, dtype=tf.float64)\r\n        repeats = tf.constant(repeats, dtype=tf.int32)\r\n\r\n        shape = tf.reduce_sum(repeats)\r\n        idx = tf.concat([tf.constant([0], dtype=tf.int32), tf.cumsum(repeats[:-1])], axis=0)\r\n        y = tf.sparse_to_dense(\r\n            sparse_indices = idx,\r\n            output_shape=(shape,),\r\n            sparse_values=x - tf.concat([tf.constant([0], dtype=tf.float64), x[:-1]], axis=0)\r\n        )\r\n\r\n        return tf.cumsum(y)\r\n\r\n    z1 = tf_repeat_1D([0,1,2], [3,4,5])\r\n    z2 = tf_repeat_1D([4,2,5], [1, 3, 2])\r\n\r\n    with tf.Session() as sess:\r\n        print(z1.eval())\r\n        print(z2.eval())\r\n\r\nThis prints:\r\n\r\n    [0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 2.]\r\n    [4. 2. 2. 2. 5. 5.]\r\n\r\nThe trick is to use something like `np.concatenate(([x[0]], x[1:]-x[:-1]))` (placed at the correct indices) in order to reconstruct the correct values when using cumsum.\r\n\r\nGeneralizing to higher dim is made complicated by the fact that sparse_to_dense only accepts 1d sparse_values input... so probably using another function there is better.", "~~For anyone else tired of waiting for a PR to get reviewed/accepted, I've ported [this pull request](https://github.com/tensorflow/tensorflow/pull/15224) to a [separate repo](https://github.com/jackd/tf-repeat) with some minor tweaks. Only CPU version without gradients implemented.~~\r\n\r\nJust found [this hidden implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/ragged/ragged_util.py#L113). It's not a part of the exported API, so you'll have to be a little dirty to acces it.\r\n\r\n```python\r\nfrom tensorflow.python.ops.ragged.ragged_util import repeat\r\n```", "cc @jackd Added a PR #26517 to expose repeat.", "@yongtang Thanks for your work on this. I've been playing with this implementation (and the hidden version) for a while now, but just recently benchmarked performance and... well, it leaves a lot to be desired. For modestly sized arrays it can be significantly improved in terms of both speed and memory usage by `gather`ing a `repeat`ed `range`.\r\n\r\n```python\r\ndef gather_repeat(values, repeats, axis=0):\r\n    indices = tf.repeat(tf.range(tf.shape(values)[axis]), repeats)\r\n    return tf.gather(values, indices, axis=axis)\r\n```\r\n(note I haven't tested different axes/confirmed it covers all edge cases)\r\n\r\nFor the very modestly sized arrays in the example below you can get a ~30% speed up and 75% memory reduction. Larger tensors result in converging computation time, but the memory factor remains roughly constant. From what I understand, `gather`s aren't particularly economical, so I imagine there's plenty of performance optimization to go from there (though possibly not in python). I'm happy to put the following together into a PR (or if anyone else is more comfortable with the tf PR process, feel free to take it and run with it), but if this motivates the tf team to accept an optimized kernel solution I'd prefer to skip straight to that.\r\n\r\nUsing pip-installed tf 1.15.0-dev20190821 , GTX-1070, python 3.6\r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmax_repeats = 100\r\nnrows = 1000\r\nndims = 100\r\n\r\n\r\ndef gather_repeat(values, repeats, axis=0):\r\n    indices = tf.repeat(tf.range(tf.shape(values)[axis]), repeats)\r\n    return tf.gather(values, indices, axis=axis)\r\n\r\n\r\ndef get_args():\r\n    r = np.random.RandomState(123)  # pylint: disable=no-member\r\n    repeats = (r.uniform(size=(nrows,),) * max_repeats).astype(np.int64)\r\n    values = r.uniform(size=(nrows, ndims)).astype(np.float32)\r\n    return tf.constant(values), tf.constant(repeats)\r\n\r\n\r\ndef benchmark_base():\r\n    with tf.Graph().as_default():\r\n        values, repeats = get_args()\r\n        op = tf.repeat(values, repeats, axis=0)\r\n        with tf.Session() as sess:\r\n            print('***********************')\r\n            print('base')\r\n            tf.test.Benchmark().run_op_benchmark(sess, op)\r\n\r\n\r\ndef benchmark_gather():\r\n    with tf.Graph().as_default():\r\n        values, repeats = get_args()\r\n        op = gather_repeat(values, repeats, axis=0)\r\n        with tf.Session() as sess:\r\n            print('***********************')\r\n            print('gather')\r\n            tf.test.Benchmark().run_op_benchmark(sess, op)\r\n\r\n\r\ndef ensure_same():\r\n    with tf.Graph().as_default():\r\n        values, repeats = get_args()\r\n        base = tf.repeat(values, repeats, axis=0)\r\n        gathered = gather_repeat(values, repeats, axis=0)\r\n        max_err = tf.reduce_max(tf.abs(gathered - base))\r\n        with tf.Session() as sess:\r\n            print('Max error: {}'.format(sess.run(max_err)))\r\n\r\n\r\nif __name__ == '__main__':\r\n    benchmark_base()\r\n    benchmark_gather()\r\n    ensure_same()\r\n```\r\n\r\nOutput:\r\n```txt\r\n***********************\r\nbase\r\nentry {\r\n  name: \"TensorFlowBenchmark.run_op_benchmark\"\r\n  iters: 10\r\n  wall_time: 0.002911686897277832\r\n  extras {\r\n    key: \"allocator_maximum_num_bytes_GPU_0_bfc\"\r\n    value {\r\n      double_value: 79595528.0\r\n    }\r\n  }\r\n  extras {\r\n    key: \"allocator_maximum_num_bytes_gpu_host_bfc\"\r\n    value {\r\n      double_value: 24.0\r\n    }\r\n  }\r\n}\r\n\r\n***********************\r\ngather\r\nentry {\r\n  name: \"TensorFlowBenchmark.run_op_benchmark\"\r\n  iters: 10\r\n  wall_time: 0.0021082162857055664\r\n  extras {\r\n    key: \"allocator_maximum_num_bytes_GPU_0_bfc\"\r\n    value {\r\n      double_value: 20176400.0\r\n    }\r\n  }\r\n  extras {\r\n    key: \"allocator_maximum_num_bytes_gpu_host_bfc\"\r\n    value {\r\n      double_value: 197768.0\r\n    }\r\n  }\r\n}\r\nMax error: 0.0\r\n```", "@jackd If you have the cycles to write up an optimized kernel solution for this (incl. a gradient function), I'll help make sure it gets accepted.  (I'm the original author of the pure-python version, and a faster version would help speed up several RaggedTensor operations.)", "Hi @shoyer ! Have you checked latest document on [tf.repeat ](https://www.tensorflow.org/api_docs/python/tf/repeat)yet? Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 8245, "title": "Minor but confusing mistake in doc about shape of output tensor of gather_nd", "body": "According to the docs of [`gather_nd(params, indices)`](https://www.tensorflow.org/api_docs/python/tf/gather_nd), the output tensor produced by the function has shape\r\n\r\n```\r\n[d_0, ..., d_{Q-2}, params.shape[K], ..., params.shape[P-1]].\r\n```\r\n\r\nwhere `[d_0, ..., d_{Q-2}` are the dimensions of `indices` without the last one, `P` is the rank of params and `K` is the length of the innermost dimension of `indices`. Firstly, I believe it should say `params.shape[K-1]` instead of `params.shape[K]`. But even then, I think that this formula is only true if `K < P`, i.e. for slices. For access to elements, consider the following example:\r\n\r\n```\r\n# data is shape [2, 3, 2]\r\ndata = np.reshape(np.arange(12), [2, 3, 2])\r\nx = tf.constant(data)\r\n# indices is shape [2, 3]\r\nindices = np.array([[0, 0, 0], [1, 2, 1]])\r\n# result is shape [2, ]\r\nresult = tf.gather_nd(x, indices)\r\n```\r\nAccording to the above formula, the output tensor should have shape `[2, 2]`, which it does not. I guess the correct formula for element access should be\r\n\r\n```\r\n[d_0, ..., d_{Q-2}]\r\n```\r\nThis might already be implicitly assumed with the above formula, but if so, it is not very clear. I realise that this is a super minor thing and one can easily find out the behaviour by doing some tests. However I still wanted to raise it, as I found it a bit tricky to get the hang of `gather_nd` in the first place and these subtle differences confused me even more.\r\n\r\n\r\n", "comments": []}, {"number": 8244, "title": "[Java API] Tensor.create() slow for large arrays", "body": "The current Java API's `Tensor.create(Object)` is really slow - for a batch of 128 images of size 224x224x3 it's taking around 1.5seconds. To put this into perspective `runner.run()` with that data and an InceptionV3 graph took below 1second so data prep is x1.5 of the runtime here (for a batch of 32 images it's around 0.35-0.45sec).\r\n\r\nIs this working as intended? When running the Python code (using simple `sess.run(fetches, feed_dict=feed_dict)`) with which the graph meta file was generated (TF 1.0.1) and feeding a Python array I don't see such hiccups, the speed is the same as the Java `runner.run()`.\r\n\r\nMight it be because of build flags used, maybe I'm missing some optimizations?\r\n\r\nFor now this small part is killing the whole performance, bringing it down from 130obs/sec (`runner.run()` time) to about ~45obs/sec (Tensor.create+run()).\r\n\r\nA bit of a sidenote, the performance page states:\r\n\r\n> This will result in poor performance.\r\n> sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\r\n\r\nBut currently there's no other way to feed data from the Java API, right? A queue (able to read from a file and from memory, i.e. from a Java structure) would be amazing.\r\n\r\n### Jar build command\r\n```\r\nexport CC=\"/usr/bin/gcc\"\r\nexport CXX=\"/usr/bin/g++\"\r\nexport TF_NEED_CUDA=1\r\nexport GCC_HOST_COMPILER_PATH=$CC\r\nexport BUILDFLAGS=\"--config=cuda --copt=-m64 --linkopt=-m64 --copt=-march=native\"\r\n\r\nbazel build -c opt \\\r\n  //tensorflow/java:tensorflow \\\r\n  //tensorflow/java:libtensorflow_jni \\\r\n  $BUILDFLAGS --spawn_strategy=standalone --genrule_strategy=standalone\r\n```\r\n\r\n### Environment info\r\n**OS:** Ubuntu 16.04\r\n**GPU:** GPU TITAN X (Pascal) 12GB\r\n**CPU:** Intel\u00ae Xeon\u00ae Processor E5-2630 v4 10core\r\n**GPU Drivers:** \r\nNVidia CUDA Driver Version: 375.39\r\nCUDNN 5.1.5\r\nCUDA 8\r\n**Tensorflow version:** JAR file built from current master (c25ecb53)\r\n\r\n### Example\r\n\r\n```java\r\npublic void test() {\r\n  Random r = new Random();\r\n  int imageSize = 224 * 224 * 3;\r\n  int batch = 128;\r\n  float[][] input = new float[batch][imageSize];\r\n  for(int i = 0; i < batch; i++) {\r\n    for(int j = 0; j < imageSize; j++) {\r\n      input[i][j] = r.nextFloat();\r\n    }\r\n  }\r\n\r\n  long start = System.nanoTime();\r\n  Tensor.create(input);\r\n  long end = System.nanoTime();\r\n  // Around 1.5sec\r\n  System.out.println(\"Took: \" + (end - start));\r\n}\r\n```", "comments": ["Thanks for the detailed description and the sample code, it is very much appreciated!\r\n\r\nThe `create(Object)` method call involves use of reflection to determine the shape and copy things over one array at a time, so it is pretty slow, especially as you add dimensions. The [`create(shape, FloatBuffer)`](https://goo.gl/aGbcSB) method would be an order-of-magnitude faster. For example:\r\n\r\n```java\r\npublic void test() {\r\n    Random r = new Random();\r\n    int imageSize = 224 * 224 * 3;\r\n    int batch = 128;\r\n    long[] shape = new long[] {batch, imageSize};\r\n    FloatBuffer buf = FloatBuffer.allocate(imageSize * batch);\r\n    for (int i = 0; i < imageSize * batch; ++i) {\r\n      buf.put(r.nextFloat());\r\n    }\r\n    buf.flip();\r\n\r\n    long start = System.nanoTime();\r\n    Tensor.create(shape, buf);\r\n    long end = System.nanoTime();\r\n    System.out.println(\"Took: \" + (end - start));\r\n}\r\n```\r\n\r\nThis is still slower than I'd want it to be, have to dig into that, but hopefully it is enough to satisfy your current needs (and session execution should be faster than Python).\r\n\r\nRegarding your other question: Yes, using feeds is slower than getting input from queues. While I believe we do have the primitives to enable use of queues from any language, it is admittedly not too easy (as you have to figure out what is being done in python - e.g., start threads that run the enqueue op - and duplicate that). Note that, as per the proposal in #7951 - investing into queues in other languages might not be worth while at this stage.\r\n\r\nDo let me know if using the `FloatBuffer` suffices for now (and I can close this issue, while we look into general performance improvements for the Java API).", "@asimshankar this is really great stuff, the timings improved exactly by an order of magnitude as you said, thanks! A batch of 128 takes around 0.2s and the batch of 32 takes around 0.04s. Still I think a bit slower than what Python does but definitely I can work with that, thanks!\r\n\r\nMaybe you could add a note in the Java doc about this? I was suspecting the reflection to take a long time in `Tensorflow.create(Object)` but wasn't sure if that was the main cause.", "@asimshankar, is there any way to reuse tensors of the same shape by just copying data without reallocating memory (maybe handling endiness outside)? Eventually we would like to feed directly from the non-tensorflow GPU memory (that seems recently to become possible - https://github.com/tensorflow/tensorflow/commit/a1d6179adb1ca6208281ed955860c319525edf75) but for now we would like to eliminate all unnecessary memory copying - the source float[] data is created by a multi-threaded application. Without it the inference takes less time than source data copying (even in Python).", "At the moment, no.\r\n\r\nOff the top of my head, one option would be to add an `alias(Buffer buffer, DataType dtype, long[] shape)` factory function to the `Tensor` class. If that works, a contribution will be welcome (@karllessard may be interested).\r\n\r\nNote that there is a [private `Tensor.buffer()`](https://github.com/tensorflow/tensorflow/blob/f4de7ec889311c42b3af4d5f34f7d31f56f73177/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L572) method that provides a `ByteBuffer` view of the underlying memory. I'm not inclined to make it public as that can make it easy to run into memory safety concerns (e.g., the returned `ByteBuffer` object shouldn't outlive the `Tensor` object)\r\n", "Thanks for the suggestion. I'll try everything as is first, just keeping in mind the memory copying and your recommendation, and then try to implement it (or just try to jump over and go directly to feed from the GPU memory).\r\nBTW - here is the project where we need fast feed (just converted the trained network to have stage2 fully convolutional) - https://blog.elphel.com/2018/09/neural-network-doubled-effective-baseline-of-the-stereo-camera/", "FYI, I\u2019m experimenting on eager execution of Java operations right now and I\u2019m already taking a look on how we can make a better usage of \u02cbTensor.buffer()\u02cb for that purpose. @AndreyFilippov , I can keep you posted of my progress or please do so if you plan to implement something around it, thanks!", "We are having a similar problem: We create Tensors in java using `Tensor.<String>create(...);` We are feeding a `byte[][][]` containing padded Strings (see our [stackoverflow ](https://stackoverflow.com/questions/55296464/how-can-i-feed-a-sparse-placeholder-in-a-tensorflow-model-from-java)post regarding this).\r\n\r\nWe are feeding 4 Tensors with ~7500 Strings each, which quite some time to `create`. Has anyone a hint how we can improve the creation? \r\n", "@Docjones , did you tried something like this? Note that this will pad your `refNames` not with `\\` but with zeros but I'm curious to see if that works.\r\n\r\nWarning! I haven't ran the code before myself so it might require some adjustments...\r\n\r\n```java\r\n    final String TEST_NAME = \"Max Mustermann\";\r\n\r\n    final String[] REF_NAMES = {\r\n        \"joseph haydn\",\r\n        \"max mustermann\",\r\n        \"erika musterfrau\",\r\n        \"johann sebastian bach\",\r\n        \"wolfgang amadeus mozart\"\r\n    };\r\n\r\n    SavedModelBundle model = SavedModelBundle.load(\"/home/karl/dev/projects/tf/tests/dist_model\", \"serve\");\r\n    try (Graph g = new Graph()) {\r\n      g.importGraphDef(model.metaGraphDef());\r\n      \r\n      ByteBuffer testInputData = ByteBuffer.wrap(TEST_NAME.getBytes(\"UTF-8\"));\r\n      \r\n      byte[][] refNamesBytes = new byte[REF_NAMES.length][];\r\n      int refNameBytesMaxLength = 0;\r\n\r\n      // Convert the ref names into bytes and keep track of the longest sequence length\r\n      for (int i = 0; i < REF_NAMES.length; ++i) {\r\n        byte[] refNameBytes = REF_NAMES[i].getBytes(\"UTF-8\");\r\n        if (refNameBytesMaxLength < refNameBytes.length) {\r\n          refNameBytesMaxLength = refNameBytes.length;\r\n        }\r\n        refNamesBytes[i] = refNameBytes;\r\n      }\r\n\r\n      // Serialize ref names to the buffer\r\n      ByteBuffer refInputData = ByteBuffer.allocate(REF_NAMES.length * refNameBytesMaxLength);\r\n      for (byte[] refNameBytes : refNamesBytes) {\r\n        refInputData.put(refNameBytes);\r\n        refInputData.position(refInputData.position() + (refNameBytesMaxLength - refNameBytes.length));\r\n      }\r\n\r\n      try (Tensor<String> testInput = Tensor.create(String.class, new long[0], testInputData);\r\n          Tensor<String> refInput = Tensor.create(String.class, new long[] { REF_NAMES.length }, refInputData )) {\r\n        List<Tensor<?>> outputs = model.session().runner()\r\n            .feed(\"test_name\", testInput)\r\n            .feed(\"ref_names\", refInput)\r\n            .fetch(\"min_idx\")\r\n            .run();\r\n        System.out.println(\"Nearest distance: \" + outputs.get(0).longValue());\r\n      }\r\n```\r\n", "@karllessard - thanks for your help! We adapted your example to our (much bigger code) and unfortunately, we receive the following exception upon calling `model.session.runner.feed(...)`;\r\n\r\n`Caused by: java.lang.IllegalArgumentException: Malformed TF_STRING tensor; too short to hold number of elements`\r\n\r\nMost likely, the created ByteBuffer is different (not only in padding with `0` instead of `/`) from our self created byte[][][]. We can't see, what we have done wrong...", "Mmh, just an idea like this, maybe you should `ByteBuffer.rewind()` both buffers before feeding them to the session? The TF client doesn't do it so it means that the buffer were positioned at their end, which would explain your error... well, \"my\" error :)", "Hi all, any update on Docjones's problem? We are also working on this issue as we want to use the same idea, i.e. FloatBuffer API, to improve the performance of `Tensor<String>... Tensor.create(...)`. However, I just look into [another similar post](https://github.com/tensorflow/tensorflow/issues/24331) , and find that @asimshankar suggest to use the API without buffer, which means we can't make that improvement in the same way in this case?", "I\u2019ve dug a bit further and I saw that TF_String buffers are exceptional because they expect some extra bytes at the beginning of each elements, something called an \u201coffset\u201d.\r\n\r\nI\u2019ll continue to investigate when I\u2019ll have a few minutes but it is something you might also want to look at if you\u2019re curious to find before.\r\n\r\n", "Ok, yeah it looks like the way to encode a TF string in a buffer is way more complex that I first thought :-/ Basically, it follows as structure like this (for String array `[A, B, C]`):\r\n\r\n`[offset A][offset B][offset C][length A][data A][length B][data B][length C][data C]`\r\n\r\nAfter a few tests, I think the following code should do the job, I'm curious that you give it a try in your projects and if 1) it works and 2) it is faster, I can push it as a utility in the repo. Also note that there is no padding in the following algorithm, could be added if needed:\r\n\r\n```java\r\n  private static ByteBuffer stringArrayToBuffer(String[] values) throws IOException {\r\n    long offsets[] = new long[values.length];\r\n    byte[][] data = new byte[values.length][];\r\n    int dataSize = 0;\r\n\r\n    // Convert strings to encoded bytes and calculate required data size, including a varint for each of them\r\n    for (int i = 0; i < values.length; ++i) {\r\n      byte[] byteValue = values[i].getBytes(\"UTF-8\");\r\n      data[i] = byteValue;\r\n      int length = byteValue.length + varintLength(byteValue.length);\r\n      dataSize += length;\r\n      if (i < values.length - 1) {\r\n        offsets[i + 1] = offsets[i] + length;\r\n      }\r\n    }\r\n\r\n    // Important: buffer must follow native byte order\r\n    ByteBuffer buffer = ByteBuffer.allocate(dataSize + (offsets.length * 8)).order(ByteOrder.nativeOrder());\r\n\r\n    // First, write offsets to each elements in the buffer\r\n    for (int i = 0; i < offsets.length; ++i) {\r\n      buffer.putLong(offsets[i]);\r\n    }\r\n    \r\n    // Second, write strings bytes, each preceded by its length encoded as a varint\r\n    for (int i = 0; i < data.length; ++i) {\r\n      encodeVarint(buffer, data[i].length);\r\n      buffer.put(data[i]);\r\n    }\r\n\r\n    return (ByteBuffer)buffer.rewind();\r\n  }\r\n  \r\n  private static void encodeVarint(ByteBuffer buffer, int value) {\r\n      int v = value;\r\n      while (v >= 0x80) {\r\n        buffer.put((byte)((v & 0x7F) | 0x80));\r\n        v >>= 7;\r\n      }\r\n      buffer.put((byte)v);\r\n  }\r\n\r\n  private static int varintLength(int length) {\r\n    int len = 1;\r\n    while (length >= 0x80) {\r\n      length >>= 7;\r\n      ++len;\r\n    }\r\n    return len;\r\n  }\r\n```", "Thanks a lot @karllessard :D ! I will have a try and get back to you once we have a result. ", "Extraordinary, @karllessard. We will try that out - give us a bit. We are just launching a new service at the beginning next week and need to shift the focus a bit.", "@karllessard instead of using String[], I use org.tensorflow.example, how can I use Tensor<String> inputBatch = Tensor.create(String.class, shap, ByteBuffer) to improve response ? \r\nI just use org.tensorflow.example as one string, like that [offset A][length A][data A], it give me the same error java.lang.IllegalArgumentException: Malformed TF_STRING tensor; too short to hold number of elements", "@dirkzhang , not sure I understand correctly, what/where is `org.tensorflow.example` exactly?\r\n\r\nNow the `String[]` here is not the data for a rank-1 String tensor but a flat view of all strings of your tensor, no matter its shape, like we do with other datatypes (Int, Floats...). The values are fed in the buffer starting iterating from dim N-1 towards dim 0.\r\n\r\nFor example, to represent a 3D matrix of strings like this one:\r\n\r\n{ { {\"000\", \"001\", \"002\" }, { \"010\", \"011\", \"012\" } }, { { \"100\", \"101\", \"102\" }, { \"110\", \"111\", \"112\" } } }\r\n\r\nthe string array passed in parameter would be\r\n\r\n{ \"000\", \"001\", \"002\", \"010\", \"011\", \"012\", \"100\", \"101\", \"102\", \"110\", \"111\", \"112\" }\r\n\r\nand the shape [ 2, 2, 3 ]\r\n\r\nDoes this answer your question?", "Hi @karllessard , me and @dirkzhang are working on this stuff, and sorry to make that confusion. \r\nThe org.tensorflow.example.Example refers to the tf.Example which fundamentally a {\"feature key\": tf.train.Feature} mapping.\r\n\r\nActually, what we are doing here is that we use the tf.Example (i.e. org.tensorflow.example.Example in Java API) as the input of the prediction inference. It is kind of a Map instead of a List, and may look like this if we decode the tf.Example message:\r\n`features {\r\n  feature {\r\n    key: \"feature0\"\r\n    value {\r\n      int64_list {\r\n        value: 0\r\n        value: 1\r\n        value: 2\r\n      }\r\n    }\r\n  }\r\n  feature {\r\n    key: \"feature1\"\r\n    value {\r\n      int64_list {\r\n        value: 4\r\n        value: 5\r\n      }\r\n    }\r\n  }\r\n  feature {\r\n    key: \"feature2\"\r\n    value {\r\n      bytes_list {\r\n        value: \"goat\"\r\n      }\r\n    }\r\n  }\r\n  feature {\r\n    key: \"feature3\"\r\n    value {\r\n      float_list {\r\n        value: 0.332442\r\n        value: 0.1123344\r\n      }\r\n    }\r\n  }\r\n}` \r\n\r\nFor now, we use `Tensors.create(new byte[][]{example.toByteArray()})` to create the Tensor<String> and obviously we can't simply use the length of the byteArray for the new 'create()' API. Also, representing example as only one string is not working as that @dirkzhang just did. \r\n\r\nSo if I understand your point correctly, we need to figure out how many strings are in this example object. Is that right? Do you have any idea about this? ", "Oh, the proto! Sorry for the confusion as well,\r\n\r\nBut before going any further with this approach, did you tried to add a [ParseExample](https://static.javadoc.io/org.tensorflow/libtensorflow/1.13.1/org/tensorflow/op/core/ParseExample.html) node to your graph instead (or the equivalent in Python)? I think that would be a proper way to extract features from an example protobuf. unless you need to do some preprocessing.\r\n\r\n", "You are right, we need to do some preprocessing before we build those features and then add them to the Example object.  \r\n\r\nIt looks like:\r\n```\r\ndo some feature preprocessing;\r\n\r\nFeatures.Builder featuresBuilder = Features.newBuilder();\r\nfeaturesBuilder.putFeature(\"feature0\", createLongListFeature(preprocessed_feature_list0));\r\nfeaturesBuilder.putFeature(\"feature1\", createLongListFeature(preprocessed_feature_list1));\r\nfeaturesBuilder.putFeature(\"feature2\", createFloatListFeature(preprocessed_feature_list2));\r\n...\r\n\r\nExample example = Example.newBuilder().setFeatures(featuresBuilder.build()).build();\r\n\r\nTensor<String> instance = Tensors.create(new byte[][]{example.toByteArray()});\r\n```\r\n", "Ok, so you want to create a tensor which is a batch of `Example` protobuf serialized as strings?\r\n\r\nThe code above should work, where each element of the `String[]` is a serialized example. The error you receive make me think that there might be a mismatch with the length of this array and the shape you are providing. I guess it should be either `new long[] { batchSize }` or `new long[] { batchSize, 1 }`, depending on your model, where `batchSize` is equal to `String[].length`.\r\n\r\nFor example:\r\n`Tensor.create(String.class, new long[] { 1 }, stringArrayToBuffer(new String[] { example.toByteString().toString(\"UTF-8\") }))` \r\n\r\nIs it what you tried? I guess we can continue debugging using the developers mailing list(developers@tensorflow.org), I don't know if you are subscribed to it.\r\n\r\np.s. Also, if you did your own method to serialize the data in the buffer, don't forget to `rewind()` your buffer after you put the data in it!", "> so you want to create a tensor which is a batch of Example protobuf serialized as strings?\r\n\r\nExactly. And we might do something wrong... well, I will have a quick try. \r\n\r\nFor the developers mailing list, will my email automatically  be assigned to you?", "> For the developers mailing list, will my email automatically be assigned to you?\r\n\r\nI'll reply to you but your message will gain more visibility so we might get some input/help from other devs as well.", "emm, I just sent an email to developers@tensorflow.org and got an automatic response from tensorflow.org admins:\r\n\r\n> We're writing to let you know that the group you tried to contact (developers) may not exist, or you may not have permission to post messages to the group. A few more details on why you weren't able to post\r\n\r\nplease let me know if you can find that email.", "For those who were following the thread, I dropped a demo (draft) of how an example.proto could be serialized in a `ByteBuffer` and parsed in a graph written in Java [here](https://github.com/karllessard/models/tree/master/samples/languages/java/mnist/src/main/java/org/tensorflow/model/sample/buffer)", "Hi @karllessard, the link you shared that has the example of ByteBuffer is a broken link now. Could it be possible to re-share that? Many thanks.", "@maziyarpanahi : sure, it's now here: https://github.com/karllessard/tensorflow-models/tree/master/samples/languages/java/suite/src/main/java/org/tensorflow/model/sample/buffer", "By the way, all this is currently being addressed in the [new official Java repository for TensorFlow](https://github.com/tensorflow/java), there is a important project called \"Tensor NIO\" that I'm working on which will allow users to directly access tensor memory from Java and to read/write its data in an N-dimensional space. That will include what is found in the `Buffers` class of the previous example.\r\n\r\nIf anyone is interested to know more about it, just let me know."]}, {"number": 8243, "title": "[Time Consuming] TF C++ Session->Run - Images for Real-time Inference", "body": "[Tensorflow (TF) on CPU]\r\nI am using the skeleton code provided for C++ TF inference [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc] in order to run a frozen model I have created in Python. This model is an FC NN with two hidden layers.\r\n\r\nIn my current project's code, I run the inference for each single image (8x8 pixels). For each sample, each Session->Run call takes about 0.02 seconds, which is expensive in my application. However, when I send a batch of 1560 samples, the Session->Run call takes about 0.03 seconds. \r\n\r\nAre these time measurements normal for the Session->Run Call? From the C++ end, should I send my frozen model batches of images in one Tensor?  From the Python end, are there optimisation tricks to alleviate that bottleneck? Is there a way to concurrently do Session-Run calls in C++?\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/1439\r\nhttp://stackoverflow.com/questions/39070708/why-sometimes-tensorflow-runs-slower-and-slower-with-the-process-of-training\r\n\r\n### Environment info\r\nOperating System: Linux\r\nInstalled version of CUDA and cuDNN: N/A\r\n\r\n### What other attempted solutions have you tried?\r\n1. I installed TF using the optimised instruction set for the CPU, but it does not seem to give me the huge time saving mentioned here [http://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions]\r\n2. Unified the session for the Graph I created.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@prb12  I just posted my question on StackOverflow:\r\nhttp://stackoverflow.com/questions/42701532/time-consuming-tensorflow-c-session-run-images-for-real-time-inference\r\n\r\nIf you have any suggestions, I'd appreciate it if you can write them over there. Thanks!"]}, {"number": 8242, "title": "Operations used for inference are dropped by optimize_for_inference", "body": "Inspired by the [TensorFlow for Poets](https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/), I have been exporting models optimized for inference with the `freeze_graph` and `optimize_for_inference`. I have run into an issue where some of the nodes required for inference get dropped by `optimize_for_inference`. The most critical one being the output node being dropped, even though it was explicitly given to `freeze_graph` and `optimize_for_inference` (through the `output_node_name`/`output_names`).\r\n\r\nI think that might be related to the output node being a `tf.identity` (to give an explicit name to the result of a `tf.layers` for example).\r\n\r\n### Minimal working example\r\nHere is a piece of code to create a very simple model, running on TensorFlow v.1.0.1.\r\n```python\r\nimport tensorflow as tf\r\n\r\nl_input = tf.placeholder(tf.float32, shape=(None, 2), name='input')\r\nl_dense = tf.layers.dense(l_input, units=1, activation=None)\r\nl_output = tf.identity(l_dense, name='output')\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    saver = tf.train.Saver(tf.global_variables())\r\n    \r\n    # Save GraphDef\r\n    tf.train.write_graph(sess.graph_def, '.', 'graph.pb')\r\n    # Save Checkpoint\r\n    saver.save(sess, 'model.ckpt', write_meta_graph=False)\r\n```\r\nI am exporting the model using the `freeze_graph` and `optimize_for_inference` tools, inspired by the [TensorFlow for Poets post](https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/).\r\n```\r\nbazel-bin/tensorflow/python/tools/freeze_graph --input_graph graph.pb --input_checkpoint model.ckpt --output_graph graph_frozen.pb --output_node_name=output\r\n```\r\n```\r\nbazel-bin/tensorflow/python/tools/optimize_for_inference --input graph_frozen.pb --output graph_optimized.pb --input_names=input --output_names=output\r\n```\r\nI am using Python to load both of these models (`graph_frozen.pb` and `graph_optimized.pb`). The model defined by `graph_frozen.pb` works as expected, but the model defined by `graph_optimized.pb` is missing some operations (`import/dense/BiasAdd` and `import/output`).\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Data\r\nx = np.random.rand(3, 2)\r\n\r\n# Frozen Graph\r\nwith tf.gfile.GFile('graph_frozen.pb', 'rb') as f:\r\n    graph_def_frozen = tf.GraphDef()\r\n    graph_def_frozen.ParseFromString(f.read())\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    l_output, = tf.import_graph_def(graph_def_frozen,\r\n        return_elements=['output:0'], \r\n        name='import'\r\n    )\r\n    print('Operations in Frozen Graph:')\r\n    print([op.name for op in graph.get_operations()])\r\n    # >>> [u'import/input', u'import/dense/kernel',\r\n    #      u'import/dense/kernel/read', u'import/dense/bias',\r\n    #      u'import/dense/bias/read', u'import/dense/MatMul',\r\n    #      u'import/dense/BiasAdd', u'import/output']\r\n\r\n    l_input = graph.get_tensor_by_name('import/input:0')\r\n\r\n    with tf.Session(graph=graph) as sess:\r\n        sess.run(l_output, feed_dict={l_input: x})\r\n\r\n# Optimized Graph\r\nwith tf.gfile.GFile('graph_optimized.pb', 'rb') as f:\r\n    graph_def_optimized = tf.GraphDef()\r\n    graph_def_optimized.ParseFromString(f.read())\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    # Using `return_elements=['output:0']` raises a ValueError\r\n    # ValueError: Requested return_element 'output:0' not found in graph_def.\r\n    tf.import_graph_def(graph_def_optimized, name='import')\r\n    print('Operations in Optimized Graph:')\r\n    print([op.name for op in graph.get_operations()])\r\n    # >>> [u'import/input', u'import/dense/kernel',\r\n    #      u'import/dense/bias', u'import/dense/MatMul']\r\n\r\n    l_input = graph.get_tensor_by_name('import/input:0')\r\n\r\n    # Raises a KeyError\r\n    # KeyError: \"The name 'import/output:0' refers to a Tensor which does\r\n    # not exist. The operation, 'import/output', does not exist in the graph.\"\r\n    l_output = graph.get_tensor_by_name('import/output:0')\r\n    \r\n    with tf.Session(graph=graph) as sess:\r\n        sess.run(l_output, feed_dict={l_input: x})\r\n```\r\n\r\n### Environment info\r\n - Operating System: OSX 10.11.1\r\n - TensorFlow installed through pip (`tensorflow-1.0.1-cp27-cp27m-macosx_10_11_x86_64.whl`)\r\n - TensorFlow v.1.0.1\r\n - To export the models, I built `freeze_graph` and `optimize_for_inference` with bazel (version 0.4.3-homebrew) in 100552f943c78cbf90aad521f9981df9b5e3c738\r\n", "comments": ["I temporarily patched this issue in this special case by replacing the `tf.identity` node with\r\n```python\r\nl_output = tf.multiply(l_dense, 1., name='output')\r\n```\r\nBut I think `optimize_for_inference` should not drop operations that are actually used for inference, especially the output node.", "Just ran into this issue myself, but I feel like the bigger issue is that using `tf.identity` to name tensors is bad practice, and there should be some other way of re-naming important tensors that you want to be able to fetch easily later on. (especially when you're using a library to help build layers and don't have control of your tensor names when they're created)\r\n\r\nAlso I'd imagine that using `tf.multiply` is a bad temporary solution, since it might add additional runtime if you have a large output (e.g. for segmentation) and possibly change the output if you quantize the model later on. The better solution would be to inspect what the actual name of your output tensor is and use that instead.", "I'm not sure of the underlying issue here, but I'm hoping that the new graph transform approach to removing unused nodes might be more robust?\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms/#optimizing-for-deployment", "Since there's been no activity on this one for several weeks, I'm closing this for now. Please reopen with more information if this is incorrect.", "I've ran into the same issue with tensorflow 1.2.1. Optimized the default tiny-yolo-voc model from [darkflow](https://github.com/thtrieu/darkflow) and the output node disappeared. ", "Definitely a bug in both strip_unused and optimize_for_inference tools. Use transform_graph fixed this! See my answer at https://stackoverflow.com/questions/48212068/error-using-model-after-using-optimize-for-inference-py-on-frozen-graph/48638586#48638586 for detail."]}, {"number": 8241, "title": "[feature] Logging Support from Custom Op", "body": "Repost from [SO](http://stackoverflow.com/questions/42684403/logging-from-custom-tensorflow-op-issue-with-vlog)\r\n\r\nProvide method and documentation for how to log from within a custom op. built in ops using `VLOG` to log, however that does not work for custom ops. I have defined my own op and in it I use `VLOG` however when I try to run the Op I get:\r\n```\r\nNotFoundError: dlopen(/Users/alex/git/tensorflow/bazel-bin/tensorflow/core/user_ops/my_op.so, 6): Symbol not found: __ZN10tensorflow8internal10LogMessage12MinVLogLevelEv\r\n  Referenced from: /Users/alex/git/tensorflow/bazel-bin/tensorflow/core/user_ops/my_op.so\r\n  Expected in: flat namespace\r\n in /Users/alex/git/tensorflow/bazel-bin/tensorflow/core/user_ops/my_op.so\r\n```\r\nIdeally there would be some suggestion or note in the docs about how to handle logging.\r\n\r\n/CC @yaroslavvb ", "comments": ["This is a symbol called `_tensorflow::internal::LogMessage::MinVLogLevel()`\r\nLooking at [tf_exported_symbols.lds](https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/tf_exported_symbols.lds), it's supposed to include everything matching `*tensorflow*`. However, looking at `nm -g _pywrap_tensorflow.so`, it doesn't seem to be present, any idea why it's filtered out? @keveman @vrv \r\n\r\n```\r\nexport so=`python -c \"import tensorflow as tf; import os.path; print(os.path.dirname(tf.sysconfig.get_include())+'/python/_pywrap_tensorflow.so')\"`\r\nnm -g $so | grep MinVLog\r\n\r\n```\r\n\r\n", "Given that it sounds like this should this be reclassified as a bug instead of a feature?", "@cancan101 is this still an issue for you? I am not super inclined to fix this if it's not going to be heavily used.", "@keveman we use VLOG and LOG in OpKernels all the time; shouldn't users be able to do the same?", "@vrv Perhaps. @yaroslavvb, with TF 1.0, that symbol does seem to be present in `pywrap_tensorflow.so`:\r\n\r\n    $ export so=`python -c \"import tensorflow as tf; import os.path; print(os.path.dirname(tf.sysconfig.get_include())+'/python/_pywrap_tensorflow.so')\"`; nm -g $so | grep MinVLog\r\n    00000000023b9e70 T _ZN10tensorflow8internal10LogMessage12MinVLogLevelEv\r\n\r\n@cancan101 This problem [seems](http://stackoverflow.com/questions/35006614/what-does-symbol-not-found-expected-in-flat-namespace-actually-mean) to be coming from mixing `libc++` and `libstdc++`. Since you are on OSX, you most probably are compiling your `my_op.so` with `libc++` and `clang`. One option is to try compiling with `-stdlib=libstdc++`.", "I no longer have issues on a clean check of on master and building just using `bazel build --config opt //tensorflow/core/user_ops:zero_out.so`.\r\n\r\nThe log statements are correctly sent to stdout."]}]