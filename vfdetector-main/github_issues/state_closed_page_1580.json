[{"number": 5514, "title": "AttributeError: 'module' object has no attribute 'global_variables_initializer'", "body": "when I was trying a multi gpu training using `python cifar10_multi_gpu_train.py --num_gpus=2`, I got an error:\r\n```\r\nusr@linux:~/tensorflow_source/tensorflow/tensorflow/models/image/cifar10$ python cifar10_multi_gpu_train.py --num_gpus=2\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\r\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\r\nTraceback (most recent call last):\r\n  File \"cifar10_multi_gpu_train.py\", line 280, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"cifar10_multi_gpu_train.py\", line 276, in main\r\n    train()\r\n  File \"cifar10_multi_gpu_train.py\", line 229, in train\r\n    init = tf.global_variables_initializer()\r\nAttributeError: 'module' object has no attribute 'global_variables_initializer'\r\n```\r\nSeemingly there's no `global_variables_initializer` of tf or in other module?\r\n", "comments": ["This was introduced lately. Consider upgrading tf or use `tf.initialize_all_variables` .\r\n\r\n_(fixed typo)_", "@schmiflo thanks, I will try it\n", "@schmiflo you mean `tf.initialize_all_variables`\nthere is a typo.\n", "@liusida I changed `global_variables_initializer` into `initialize_all_variables`, and it did work, I haven't updated my tf to the latest version, maybe it has already had `global_variables_initializer`\n", "It works! Thank you!\n I changed global_variables_initializer into initialize_all_variables.\n", "@zakizhou yes, i think you are right.\n\nhttps://github.com/tensorflow/tensorflow/blob/20c3d37ecc9bef0e106002b9d01914efd548e66b/tensorflow/python/ops/variables.py#L1170\n\n```\n@deprecated(\"2017-03-02\", \"Use `tf.global_variables_initializer` instead.\")\ndef initialize_all_variables():\n  \"\"\"See `tf.global_variables_initializer`.\"\"\"\n  return global_variables_initializer()\n```\n\nThis change was made just 12 days ago\n\nhttps://github.com/tensorflow/tensorflow/commit/4cbdead95f22de74bcbc72a68c9a38d465202db9#diff-ae1a8f7b66539f000615a4ab7e4b2151\n", "@schmiflo Thankyou for your solution. However I got error by just blindly copy and paste the code. There is typo in `tf.intialize_all_variables`. It should be `tf.initialize_all_variables`.  Could you fix it to prevent confusion?  ", "I found that TensorFlow v0.12 accepts the global_variables_initializer command", "It would be nice if you update your tutorial here https://www.tensorflow.org/tutorials/mnist/beginners/ to use the new method.", "I'm no expert by any means but I will try to do that over the next weekend. Hopefully I won't mess anything up :)\nBinay \n\n    On Monday, January 9, 2017 7:05 PM, Konstantin Solomatov <notifications@github.com> wrote:\n \n\n It would be nice if you update your tutorial here https://www.tensorflow.org/tutorials/mnist/beginners/ to use the new method.\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.  \n\n   ", "For folks that needs a complete answer: user the code below as is:\r\n`sess.run(tf.initialize_all_variables())`", "I upgraded tensorflow but global_variables_initializer() still cannot work.", "> @liusida I changed `global_variables_initializer` into `initialize_all_variables`, and it did work, I haven't updated my tf to the latest version, maybe it has already had `global_variables_initializer`\r\n\r\nHello, how can I update tf to the last version in Kaggle? `tf.initialize_all_variables` didn't work for me...", "I'm having the same issue, nothing works.\r\n>>> init = tf.global_variables_initializer()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'global_variables_initializer'\r\n>>> init = tf.initialize_all_variables()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'initialize_all_variables'\r\nI installed Tensorflow using pip and got 2.2.0.", "@EricBuist if u are using tensorflow version 2.x u can try this:\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()", "**I solved this by using tf.compat.v1.global_variables_initializer()**\r\n"]}, {"number": 5513, "title": "Simplify Android integration", "body": "The current process of building an Android app that uses TensorFlow is unwieldy at best. The only available documentation is baked into the sole example app, which itself is embedded in the TensorFlow build system. Having prebuilt, drop-in Android libraries (or even the ability to build these libraries from source) would, I believe, significantly simplify the process.\r\n\r\nI added my own BUILD definitions to build libraries I can easily drop into an Android Studio project (having experience with blaze made this much more tractable):\r\n```\r\n# Inside //tensorflow/contrib/android/BUILD\r\n\r\n# Build the JAR.\r\n# Ends up in //my-app/libs/libandroid_tensorflow_inference_java.jar\r\nandroid_library(\r\n    name = \"android_tensorflow_inference_java\",\r\n    srcs = [\":android_tensorflow_inference_java_srcs\"],\r\n)\r\n\r\n# Build the SO.\r\n# bazel build //tensorflow/contrib/android:libtensorflow.so \\\r\n#   --crosstool_top=//external:android/crosstool \\\r\n#   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n#   --cpu=armeabi-v7a\r\n# Ends up in //my-app/src/main/jniLibs/armeabi-v7a/libtensorflow.so\r\nLINKER_SCRIPT = \"//tensorflow/contrib/android:jni/version_script.lds\"\r\n\r\ncc_binary(\r\n    name = \"libtensorflow.so\",\r\n    srcs = [],\r\n    copts = tf_copts(),\r\n    linkopts = [\r\n        \"-landroid\",\r\n        \"-ljnigraphics\",\r\n        \"-llog\",\r\n        \"-lm\",\r\n        \"-z defs\",\r\n        \"-s\",\r\n        \"-Wl,--version-script\",  # This line must be directly followed by LINKER_SCRIPT.\r\n        LINKER_SCRIPT,\r\n    ],\r\n    linkshared = 1,\r\n    linkstatic = 1,\r\n    deps = [\r\n        \":android_tensorflow_inference_jni\",\r\n        \"//tensorflow/core:android_tensorflow_lib\",\r\n        LINKER_SCRIPT,\r\n    ],\r\n)\r\n```\r\n\r\nMoreover, if the Android example could rely on these libraries and therefore exist as a \"standalone\" project buildable by Android Studio I think the barrier to entry for TensorFlow on Android would be materially lower.", "comments": ["Thanks for the suggestion, implemented in 341da02072ee1c1093a5dca6ad45a1488306193d. \n\nAndroid apps may now either either add tensorflow/contrib/android:libtensorflow_inference.so to their native libs list or continue to just build the src filegroup into another .so (as the demo does). The loadLibrary call in TensorFlowInferenceInterface.java has a bit of smarts in it to automatically figure out if it needs to load libtensorflow_inference.so or not.\n\nIn either case tensorflow/contrib/android:android_tensorflow_inference_java should be used as a dep rather than including the src filegroup directly.\n"]}, {"number": 5512, "title": "Re-entering an existing scope with variable_scope produces inconsistent results", "body": "Consider the following example:\r\n\r\n```python\r\nwith tf.variable_scope(name_or_scope='alpha'):\r\n    with tf.variable_scope(name_or_scope='beta'):\r\n        with tf.variable_scope(name_or_scope=tf.get_variable_scope()):\r\n            a_var = tf.get_variable(shape=(42,), name='my_var') # An arbitrary var\r\n            an_op = tf.reduce_sum(tf.zeros((42,)), name='my_op') # An arbitrary op\r\n\r\nprint('a_var is named {}'.format(a_var.name))\r\nprint('an_op is named {}'.format(an_op.name))\r\n```\r\n\r\nThis produces the following output:\r\n\r\n```\r\na_var is named alpha/beta/my_var:0\r\nan_op is named alpha/beta/beta/my_op:0\r\n```\r\nNote that re-entering the existing scope appears to correctly name the variable, but introduces a redundant prefix for ops. Is this expected behavior, or a bug?\r\n\r\nTested on `0.11.0rc1`", "comments": ["@lukaszkaiser Is this expected behavior?  I'm not sure what the scope semantics are these days.\n", "This is expected but not nice. Names of ops do not carry semantics (meaning that they can be freely permuted and the graph will execute exactly in the same way), while names of variables do. I agree that the above is a bit inconvenient, but not a bug. Still, we might consider improving this.\n", "Let's close for now.\n"]}, {"number": 5511, "title": "Build a static library", "body": "Is there a way to ask the build system bazel to build a static library that one can link a program written using the C++ API against? My apologies for this simple question, but I am very unfamiliar with this build system. I try to add some code to tensorflow/tensorflow/BUILD\r\n\r\n```\r\ncc_binary(\r\n    name = \"libtensorflow_gpu.a\",\r\n    linkstatic = 1,\r\n    linkshared = 0,\r\n    deps = [\r\n        \"//tensorflow/c:c_api\",\r\n        \"//tensorflow/core:tensorflow\",\r\n    ],\r\n)\r\n```\r\n\r\nbut it failed, it will show these log\r\n\r\n\r\n>ERROR: /gruntdata/jefby/alicpp/external/tensorflowserving/tensorflowserving-50129ec0b407e5778282ee8d7610a9494cfcf6b2/tensorflow/tensorflow/BUILD:209:1: Linking of rule '//tensorflow:libtensorflow_gpu.a' failed: gcc failed: error executing command /usr/bin/gcc -o bazel-out/local-opt/bin/tensorflow/libtensorflow_gpu.a -pthread -Wl,-no-as-needed -B/usr/bin -B/usr/bin -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n/usr/lib/gcc/x86_64-unknown-linux-gnu/4.9.2/../../../../lib64/crt1.o: In function `_start':\r\n(.text+0x20): undefined reference to `main'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow:libtensorflow_gpu.a failed to build\r\n\r\n\r\n\r\n", "comments": ["It's looking for `main`. Try `cc_library`.\n", "@drpngx Thanks for your reply. Now compiling no error, but I can't find  file libtensorflow_gpu.a which it generates.\n", "What's your exact build command?\n", "@aselle bazel build -c opt //tensorflow:libtensorflow_gpu.a\n", "@jefby Where did you look?  It should be in `bazel-bin` (or one of the similar directories).\n", "@aselle Why did you remove awaiting response?\n", "Alas, it is not currently possible to get bazel to build a single `.a` file that you can link against the C++ API. For details see https://github.com/bazelbuild/bazel/issues/1920 \n\nTill that is resolved, the alternatives are either:\n- Build the shared library (`bazel build -c opt //tensorflow:libtensorflow.so`) and link against and run with that available, OR\n- Figure out a way to get the static library built using the Makefile (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile)\n\nOur hope is that the bazel team will be able to figure out a solution and we can then add those targets.\n\nClosing this issue since I believe the original query has been answered. Feel free to re-open or ping if you think closing it was a mistake. Thanks!\n", "I managed to build tensorflow statically.\r\n- add `tensorflow/c/c_api.cc \\` above [that](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/Makefile#L534) line\r\n- add `#define __ANDROID__ 1` in `tensorflow/c/c_api.cc` on the top to skip files that's missing header files\r\n- run ./tensorflow/contrib/makefile/build_all_linux.sh\r\n\r\nworks for me on Ubuntu Artful and MacOS High Sierra.\r\n  ", "@asimshankar WDYT of the provided sample in https://github.com/bazelbuild/bazel/issues/1920#issuecomment-509144086?"]}, {"number": 5510, "title": "Removed unnecessary classproperty decorator", "body": "\u2026ad of logging", "comments": ["@terrytangyuan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ilblackdragon, @tensorflower-gardener and @keveman to be potential reviewers.\n", "Thanks! Could you check again? I don't think the new errors are related to this PR\n", "Sure, removed.\n", "Seems like this fails - #5877. I'll revery this change."]}, {"number": 5509, "title": "Remove deprecated copy of checkpoints lib", "body": "This has been deprecated for a while.", "comments": ["@terrytangyuan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ilblackdragon and @tensorflower-gardener to be potential reviewers.\n", "@benoitsteiner Mind merging this? Errors are not from this PR. Thanks!\n"]}, {"number": 5508, "title": "Branch 138702302", "body": "", "comments": ["@benoitsteiner, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @samjabrahams and @keveman to be potential reviewers.\n", "Jenkins, test this please.\n"]}, {"number": 5507, "title": "nvcc warning: option '--relaxed-constexpr' has been deprecated", "body": "# Description\r\n\r\nBuild with CUDNn 5, CUDA 8.0, tensorflow 0.11.0 rc1.\r\n\r\n```\r\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nGet a lot ```Too many warnings: nvcc warning option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.```\r\n\r\n![image](https://cloud.githubusercontent.com/assets/3538629/20159915/ae552718-a71e-11e6-9f94-043d7f382e17.png)\r\n\r\nIn the console, this happens a lot, any way to turn it off?", "comments": ["This was fixed and then reverted; not sure why (perhaps to support CUDA 7.0?).\n\nhttps://github.com/tensorflow/tensorflow/pull/5256\n", "@gunan, do you have any thoughts about this?\n", "Yes, we still need to support CUDA 7.0 for a bit longer.  I'm hoping we can drop support for 7.0 soon though.\n", "Yes, I made the change, but it broke all our cuda 7 builds.\nOnce we can drop our cuda7 builds, we will roll forward with the fix.\nFor now, we cannot really do anything about this.\n"]}, {"number": 5506, "title": "Remove stray text from index.md", "body": "Will replace with an include in an internal change, but for now just removing.", "comments": ["@xmbrst, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @vrv to be potential reviewers.\n"]}, {"number": 5505, "title": "Xmbrst patch 1", "body": "Will replace with an include in an internal change, but for now just removing.", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "I did author these commits.\n\nOn Wed, Nov 9, 2016 at 6:03 PM, googlebot notifications@github.com wrote:\n\n> We found a Contributor License Agreement for you (the sender of this pull\n> request) and all commit authors, but as best as we can tell these commits\n> were authored by someone else. If that's the case, please add them to this\n> pull request and have them confirm that they're okay with these commits\n> being contributed to Google. If we're mistaken and you did author these\n> commits, just reply here to confirm.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5505#issuecomment-259552181,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AO-8A5voMAXJk5fhCliXqxiH4_jdTAirks5q8lFGgaJpZM4KuFdt\n> .\n", "Probably want to send a PR from an up to date branch.\n"]}, {"number": 5504, "title": "Android arm64-v8a (armv8) native libs build error", "body": "Hello,\r\n\r\nI am encountering an error trying to build Tensorflow for arm64-v8a. *All other architectures build successfully*.\r\n\r\nOperating system: OS X 10.11\r\nTensorflow repo commit: v0.11.0rc2 (tagged release)\r\n\r\nFrom the Tensorflow repository root directory, I run:\r\n\r\n```\r\nbazel build -c opt //tensorflow/examples/android:tensorflow_native_libs \\\r\n--crosstool_top=//external:android/crosstool \\\r\n--host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n--verbose_failures \\\r\n--cpu=arm64-v8a\r\n```\r\n\r\nThe error emitted is:\r\n\r\n```ERROR: /Users/kevin/work/mobile-sdk/android/sdk/tensorflow/tensorflow/examples/android/BUILD:14:1: Linking of rule '//tensorflow/examples/android:libtensorflow_demo.so' failed: aarch64-linux-android-gcc failed: error executing command\r\n  (cd /private/var/tmp/_bazel_kevin/47b9700ecb360f2f1d6606e4f92873e1/execroot/tensorflow && \\\r\n  exec env - \\\r\n  external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/darwin-x86_64/bin/aarch64-linux-android-gcc -shared -o bazel-out/aarch64-linux-android-4.9-gnu-libstdcpp-opt/bin/tensorflow/examples/android/libtensorflow_demo.so -Wl,-whole-archive bazel-out/aarch64-linux-android-4.9-gnu-libstdcpp-opt/bin/tensorflow/examples/android/_objs/libtensorflow_demo.so/tensorflow/examples/android/jni/imageutils_jni.o bazel-out/aarch64-linux-android-4.9-gnu-libstdcpp-opt/bin/tensorflow/examples/android/_objs/libtensorflow_demo.so/tensorflow/examples/android/jni/rgb2yuv.o bazel-out/aarch64-linux-android-4.9-gnu-libstdcpp-opt/bin/tensorflow/examples/android/_objs/libtensorflow_demo.so/tensorflow/examples/android/jni/yuv2rgb.o bazel-out/aarch64-linux-android-4.9-gnu-libstdcpp-opt/bin/tensorflow/contrib/android/libandroid_tensorflow_inference_jni.lo bazel-out/aarch64-linux-android-4.9-gnu-libstdcpp-opt/bin/tensorflow/core/libandroid_tensorflow_lib.lo bazel-out/aarch64-linux-android-4.9-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/libandroid_tensorflow_kernels.lo bazel-out/aarch64-linux-android-4.9-gnu-libstdcpp-opt/bin/tensorflow/core/libandroid_tensorflow_lib_lite.lo bazel-out/aarch64-linux-android-4.9-gnu-libstdcpp-opt/bin/tensorflow/core/libprotos_all_cc.a bazel-out/aarch64-linux-android-4.9-gnu-libstdcpp-opt/bin/external/protobuf/libprotobuf.a bazel-out/aarch64-linux-android-4.9-gnu-libstdcpp-opt/bin/external/protobuf/libprotobuf_lite.a -Wl,-no-whole-archive external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/arm64-v8a/libgnustl_static.a external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/arm64-v8a/libsupc++.a -landroid -ljnigraphics -llog -lm -z defs -s '-Wl,--icf=all' -Wl,--version-script tensorflow/contrib/android/jni/version_script.lds -lz -static-libgcc -no-canonical-prefixes '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nexternal/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/darwin-x86_64/bin/../lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/bin/ld: unrecognized option '--icf=all'\r\nexternal/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/darwin-x86_64/bin/../lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/bin/ld: use the --help option for usage information\r\n```\r\n\r\nAs stated, I have built several other architectures successfully with this method (changing the `cpu` flag to `armeabi-v7a`, `x86`, `x86_64` all build successfully).\r\n\r\nThanks for any help! Let me know if I can provide any more info.", "comments": ["If you remove `\"-Wl,--icf=all\",  # Identical Code Folding` from the linkopts of libtensorflow_demo.so, does it build successfully?\n", "@andrewharp Yes, that works. I commented out that line in `tensorflow/examples/android/BUILD` and it builds successfully. \n\nIs there any reason why that option fails only on arm64, and if it's not easily-fixed, is there a way to make it so that that option is only used on non-arm64 architectures? I'm building Tensorflow for arm, arm64, x86, and x64 for a project I'm working on, and it'd be nice not to have to edit between builds so I can automate the process.\n\nAlternatively, can the option be omitted for all architectures? I'm not entirely sure what the ramifications of doing that would be.\n\nThanks!\n", "@kevinmost \nI don't think we need the config setting for the demo, so I'll remove it to make things simpler.\n\nIn general, though, you can use the various if_android*() methods defined in tensorflow.bzl to customize your srcs/deps/flags appropriately for different target platforms.\n", "`-Wl,--icf=all` was removed from libtensorflow_demo.so in https://github.com/tensorflow/tensorflow/commit/1a08f5e79266a2e83106ab7f9e17c3787da57703, so this should no longer be an issue."]}, {"number": 5503, "title": "Reformat markdown.", "body": "     Change: 138541907\r\n\r\n     (cherry picked)", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @LiamHe and @tiagonj to be potential reviewers.\n"]}, {"number": 5502, "title": "[Minor] ExtractImagePatches grad fails when batch size is unknown", "body": "Mentioned in\r\n#5501\r\n#3672\r\nSolution was by @mohamedadaly\r\n", "comments": ["@liuyipei, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @vrv and @benoitsteiner to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Can one of the admins verify this patch?\n", "I signed it!\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Can one of the admins verify this patch?\n", "@benoitsteiner \nI changed `channels` as per your request;  sadly, there is a distinction for the others. \n`rows_out`, `rows_in`, and etc are actually used as scalar underneath, where they do scalar-ish things like serving as for-loop constraints, getting passed into `range`, and etc.  So, a significant rewrite would be required if I wanted to let `rows_in` to be a tensor.\n", "Jenkins, test this please.\n"]}, {"number": 5501, "title": "ExtractImagePatches Gradient doesn't work if batch size is unknown", "body": "See #3672 comment by @mohamedadaly\r\nHis fix is to add \r\n```\r\n  batch_size = array_ops.shape(op.inputs[0])[0]\r\n```\r\nafter this:\r\n```\r\n  batch_size, rows_in, cols_in, channels = [\r\n    dim.value for dim in op.inputs[0].get_shape()\r\n  ]\r\n```\r\nin `tensorflow/python/ops/array_grad.py`", "comments": []}, {"number": 5500, "title": "tf.case unexpected behaviour with tf.placeholder in predicate ", "body": "I am using a `tf.case` to distinguish between `N` different cases to return exactly one of `N` output tensors. Crucially, I am using a `tf.placeholder` in the predicates. The toy example below exemplifies my code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nN = 6\r\nlength = tf.placeholder(dtype=tf.int32, shape={})\r\n\r\ntensors =[tf.constant(i * 100) for i in range(N)]\r\ndefault = lambda: tf.constant(-2)\r\npredicates = [(tf.equal(length, tf.constant(i, dtype=tf.int32)), lambda: tensors[i]) for i in range(N)]\r\n\r\nmycase = tf.case(predicates, default, exclusive=True)\r\n\r\nwith tf.Session() as session:\r\n  session.run(tf.initialize_all_variables())\r\n\r\n  for l in range(N):\r\n    out = session.run(mycase, feed_dict = {length : l})\r\n    print(\"output %d\\t(length = %d)\" % (out,l))\r\n  ```\r\nAs output I would expect \r\n```\r\noutput 0\t(length = 0)\r\noutput 100\t(length = 1)\r\noutput 200\t(length = 2)\r\noutput 300\t(length = 3)\r\noutput 400\t(length = 4)\r\noutput 500\t(length = 5)\r\n```\r\nHowever, I get\r\n```\r\noutput 500\t(length = 0)\r\noutput 500\t(length = 1)\r\noutput 500\t(length = 2)\r\noutput 500\t(length = 3)\r\noutput 500\t(length = 4)\r\noutput 500\t(length = 5)\r\n\r\n```\r\n\r\nComparing a constant to a placeholder usually does not cause problems. Am I stretching something too far by emulating such dynamic behaviour using `case` plus `palceholder`?\r\n\r\n \r\nSetup: Ubuntu 14.04, tensorflow 0.11.0rc2 from the provided tensorflow-0.11.0rc2-cp34-cp34m-linux_x86_64.whl\r\n\r\n\r\n", "comments": ["As far as I tested, I think it is not because of placeholder, even feeding a constant is not working.\r\ntf.case always returns the last element in the list.", "I misunderstood python's binding within lambda expressions. Should be \r\n```\r\npredicates = [(tf.equal(length, tf.constant(i, dtype=tf.int32)), lambda i=i: tensors[i]) for i in range(N)]\r\n```\r\nCan be closed..."]}, {"number": 5499, "title": "Branch 138642972", "body": "", "comments": ["@benoitsteiner, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @sherrym to be potential reviewers.\n", "It looks like internal commits were squashed into a single change.\n"]}, {"number": 5498, "title": "Change HTTP source locations away from Sourceforge", "body": "### Problem\r\n\r\nThere are a bunch of sources downloaded over HTTP during the build process that depend on Sourceforge (libjpeg, giflib, boost, etc.). Unfortunately, Sourceforge presents a download delay at download initiation time, often suffers timeouts, and the download speeds are really slow. This significantly impacts build times.\r\n\r\n### Solution\r\n\r\nThe sources could migrate to a subproject under the TensorFlow Github org, or to another Github repo that could host them, or to a CDN. Whatever the solution may be, this could help prevent timeouts and significantly improve the build speeds.\r\n\r\nNote: I could issue a PR but the question is what location for these sources the TensorFlow team would find acceptable as an alternative to SourceForge.", "comments": ["@gunan already changed a bunch of them away from sourceforge. Which ones are you thinking of?\n", "https://github.com/tensorflow/tensorflow/blob/master/WORKSPACE\nIf you are building from source at head, there are no more sourceforge links remaining.\nIf you are referring to old release branches, I think it's OK to leave them.\n\nIs there anything we are missing?\n", "@drpngx @gunan Ah ok, that must be a very recent development, I was working off of a quite recent but not totally up to date fork. Sorry for a false alarm.\n"]}, {"number": 5497, "title": "Bugfix in gradient code for tf.image.resize_nearest_neighbor", "body": "This PR prevents errors when computing gradients that involve `tf.image.resize_nearest_neighbor` and tensors with partially defined shapes.", "comments": ["@uschmidt83, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @tensorflower-gardener and @keveman to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Jenkins, test this please.\n"]}, {"number": 5496, "title": "tensorflow ./configure error  Unrecognized option: --action_env=PATH", "body": " ./configure \r\n\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] y\r\nHadoop File System support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] n\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with GPU support? [y/N] y\r\nGPU support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/local/cuda-8.0/\r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5.0.5\r\nPlease specify the location where cuDNN 5.0.5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda-8.0/]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 3.5,6.1\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=115\r\nINFO: Reading options for 'clean' from /home/fang/tensorflow/tools/bazel.rc:\r\n  Inherited 'build' options: --force_python=py2 --host_force_python=py2 --python2_path=/usr/bin/python --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --define PYTHON_BIN_PATH=/usr/bin/python --spawn_strategy=standalone --genrule_strategy=standalone\r\n**INFO: Reading options for 'clean' from /etc/bazel.bazelrc:\r\n  Inherited 'build' options: --action_env=PATH --action_env=LD_LIBRARY_PATH --action_env=TMPDIR --test_env=PATH --test_env=LD_LIBRARY_PATH\r\nUnrecognized option: --action_env=PATH**\r\n\r\nAny clue what happened? Thanks.\r\n", "comments": ["Problem solved by using bazel 0.4.* instead of 0.3\r\n", "@neufang I have the same problem. Do you mean 0.4.*?", "Problem still exists.\r\nTried with tensorflow source https://github.com/tensorflow/tensorflow, commit 27a98083a6c16f263d668271889863596efbeb84\r\n\r\nOS: Ubuntu 16.04\r\ngcc 5.4.0\r\nbazel info >> bazel is already the newest version (0.4.4)\r\nCuda compilation tools, release 8.0, V8.0.61\r\n\r\noutput of ./configure\r\n\r\ngopi@gp:~/tensorflow$ ./configure \r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] Y\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] N\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] N\r\nNo XLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] N\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \r\nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1.10\r\nPlease specify the location where cuDNN 5.1.10 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/lib/x86_64-linux-gnu\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 6.1\r\n.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=97\r\nINFO: Reading options for 'clean' from /home/gopi/tensorflow/tools/bazel.rc:\r\n  Inherited 'build' options: --force_python=py2 --host_force_python=py2 --python2_path=/usr/bin/python --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --define PYTHON_BIN_PATH=/usr/bin/python --spawn_strategy=standalone --genrule_strategy=standalone -c opt\r\nINFO: Reading options for 'clean' from /etc/bazel.bazelrc:\r\n  Inherited 'build' options: --action_env=PATH --action_env=LD_LIBRARY_PATH --action_env=TMPDIR --test_env=PATH --test_env=LD_LIBRARY_PATH\r\nUnrecognized option: --action_env=PATH\r\n", "Upgrading bazel to 0.4.* works for me. ", "yes, mentioned the detailed solution here\r\nhttps://github.com/tensorflow/tensorflow/issues/7979#issuecomment-283559640"]}, {"number": 5495, "title": "Modifying the convolution operation", "body": "Hi,\r\n\r\nI wanted to test a modification to the convolutional operation. Which files should I change in the source code to implement and test my modifications? I have seen some [low level .cc code](https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/core/kernels/conv_grad_ops.cc) which seems to implement convolutions. However, I'm not sure about exactly which files I should modify to implement my modification. \r\n\r\nAlso is there a cleaner way (as in, a high-level pythonic way) to  test modifications to the convolutional operation without losing out on the speed of Tensorflow?\r\n\r\nThanks! ", "comments": ["Hi @sahiliitm ,\n\nI'd start here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc.\n\nYou can verify your changes by running\n\nbazel test -c opt //tensorflow/python/kernel_tests:conv_ops_test\n\nSherry\n", "For future reference, this question is more appropriately asked on StackOverflow. thanks!\n", "@aselle Could you provide a link to the stackoverflow page you mentioned?"]}, {"number": 5494, "title": "Refactor cuda_configure to better support for different CUDA installations", "body": "This change implements cuda and cudnn version autodetection via running\r\nnvcc --version and reading cudnn.h. As a result, TF_CUDA_VERSION and\r\nTF_CUDNN_VERSION no longer have to be set. If TF_CUDA_VERSION and\r\nTF_CUDNN_VERSION are set but do not match the detected versions,\r\ncuda_configure will fail.\r\n\r\nAfter the cuda and cudnn versions are detected, cuda_configure then\r\nattempts to find the needed cuda and cudnn libraries in a set of known\r\ninstallation directories.", "comments": ["@davidzchen, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ville-k, @tensorflower-gardener and @meteorcloudy to be potential reviewers.\n", "FYI there is still a bit of manual testing I am doing, but I'm opening this to run it through the CI tests.\n\n+cc @damienmg\n", "Are the windows cmake tests broken currently? Most of the previous runs seem to be failing: https://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/\n", "Rebased and resolved merge conflicts.\n\n@jart - Any more feedback on this before it can be merged?\n", "@meteorcloudy - Is there a TF Windows test that builds with GPU or an instance that I can use to test this? If not, would it be possible for you to test this PR on your Windows machine? I currently do not have access to a Windows machine.\n", "@davidzchen Yes, I can test this, and everything looks correct to me except the library extension for Windows should be \".lib\" instead of \".dll\".\n", "@meteorcloudy Thanks! Fixed, rebased, and pushed. That's interesting. Out of curiosity, is that a convention that other libraries on Windows are starting to follow now?\n", "The windows failure that shows up is a flake that is already fixed.\r\nJenkins, test this please.", "@davidzchen Hi David, can you take a look at the two Windows problems I mentioned? If you are worried about no way to test the change, I can also send another PR to fix them after this is merged.", "@tensorflow-jenkins Test this please", "@meteorcloudy Sorry, I just got back from the Thanksgiving holiday. I have fixed the `nvcc.exe` issue. I am a bit unclear on the `.dll` issue. Where are the build rules that reference the `.dll`s?\r\n\r\nIf that issue is not blocking this PR, I'm fine with merging this change and you can send a separate PR for that issue since I do not currently have access to a Windows machine that I can test that on.", "@davidzchen No problem! I can send a fix later, thanks for giving these fixes! :)", "@meteorcloudy Sounds great. Thanks, Yun!\r\n\r\n@jart @gunan All remaining review comments have been addressed and tests are passing. Can we go ahead and merge this?"]}, {"number": 5493, "title": "TensorBoard giving all black events and distributions", "body": "Each time after typing \"tensorboard --logdir=logs\"\r\nonly \"Starting TensorBoard 29 on port 6006\" is shown\r\nso I need to type \"http://0.0.0.0:6006\" myself.\r\nThen the graph of neural network and histograms are well displayed, \r\nbut events and distributions are all black, like this:\r\n\r\n![capture d ecran 2016-11-09 a 11 59 42](https://cloud.githubusercontent.com/assets/18445114/20136246/08436326-a674-11e6-90d8-8d34c6a455ce.png)\r\n\r\n\r\nMac OS enivrement, python version 2.7.\r\nI'm sure that the code is good because someone else runs it correctly on his Mac.\r\nWhat should be the problem?\r\n", "comments": ["Hi, \nWhat browser do you use? It also happens on my safari and Firefox but chrome looks OK.\nI also want it fixed since I need to open chrome every time...\n", "@jiny2001 Good point ! I never think about the browser! Thanks! I always use Safari. So the Google tools work best with Google accessories... lol\n", "I have the same problem with Firefox and Ubuntu\n", "@smistad use chrome, it wil be solved", "I had problems on Ubuntu (even 2 devices), and Firefox, and Firefox on Android, but chrome helped. Thanks, great idea. (psi: even on built in Chinese android browser works fine). "]}, {"number": 5492, "title": "tf.assign does not update variable shape if tf.Variable(..., validate_shape=True) initially", "body": "Unless the variable is initially created with tf.Variable(..., validate_shape=False), updating a variable using `tf.assign(..., validate_shape=False)` does not update the variable shape. \r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: CUDA 8.0 and cuDNN 5.1\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n``` bash\r\n$ ls -l /usr/local/cuda-8.0/lib64/libcud*\r\n-rw-r--r-- 1 root root 558720 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Sep 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root     19 Sep 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root 415432 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root 775162 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudart_static.a\r\n$ ls -l /usr/local/cudnn-5.1-cuda-8.0/lib64/\r\ntotal 145608\r\nlrwxrwxrwx 1 root root       13 Oct 28 10:07 libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       17 Oct 28 10:07 libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rwxr-xr-x 1 root root 79337624 Oct 28 10:07 libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Oct 28 10:07 libcudnn_static.a\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#pip-installation\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\n```bash\r\n$ python -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\n0.11.0rc2\r\n```\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nIn the following script, if var is created with `validate_shape=True`, subsequent `tf.assign` operations don't update the shape (they remain as [1]), *but they do update the data*.\r\n\r\nHowever, if the var is created with `validate_shape=False`, subsequent `tf.assign` operations do update both data and the shape,  setting the shape to [1], [10], and [20] respectively.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndtype = np.float64\r\nshape = (10, )\r\n\r\nph = tf.placeholder(dtype=dtype)\r\nvar = tf.Variable(tf.ones(shape=(1,), dtype=dtype), validate_shape=True)\r\nop = tf.assign(var, ph, validate_shape=False)\r\n\r\ninit_op = tf.initialize_all_variables()\r\n\r\nwith tf.Session() as S:\r\n    S.run(init_op)\r\n    print S.run(tf.shape(var)) # [1]\r\n    print S.run(var)           # [1.]\r\n    S.run(op, feed_dict={ph: np.ones(shape=(10,), dtype=dtype)})\r\n    print S.run(tf.shape(var)) # [1], should be [10]\r\n    print S.run(var)           # [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\r\n\r\n    S.run(op, feed_dict={ph: np.ones(shape=(20,), dtype=dtype)})\r\n    print S.run(tf.shape(var)) # [1], should be [20]\r\n    print S.run(var)           # [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\r\n  1.  1.]\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["@sjperkins I don't quite follow.  What is going wrong?\n", "@girving Apologies, I mucked that report up and made it very confusing. I've corrected the minimal reproducible example.\n", "@mrry I'm guessing the shape optimizations are the culprit here?\n", "Looks like it. Forwarding the issue to @benoitsteiner, since he added them.\n\n(FWIW, in my opinion it should be an error to change the shape of a variable once it's created with a fully-defined shape... i.e. `tf.Variable(..., validate_shape=True)`. If I remember correctly, the shape-changing ops were grandfathered in and never reconciled with `tf.Variable`.)\n", "Hi! I'm experimenting the same mistake.", "Same error! ", "Set `validate_shape` to False when loading meta graph by adding code\r\n\r\n    if graph.node[-1].attr.get(\"validate_shape\"):\r\n        graph.node[-1].attr[\"validate_shape\"].b = False\r\n\r\nto tensorflow/python/framework/ops.py#2318\r\n\r\n    with self._lock:\r\n      graph = graph_pb2.GraphDef()\r\n      graph.versions.CopyFrom(self._graph_def_versions)\r\n      bytesize = 0\r\n      for op_id in sorted(self._nodes_by_id):\r\n        op = self._nodes_by_id[op_id]\r\n        if from_version is None or op_id > from_version:\r\n          graph.node.extend([op.node_def])\r\n          if graph.node[-1].attr.get(\"validate_shape\"):\r\n            graph.node[-1].attr[\"validate_shape\"].b = False\r\n          if op.outputs and add_shapes:\r\n            assert \"_output_shapes\" not in graph.node[-1].attr\r\n            graph.node[-1].attr[\"_output_shapes\"].list.shape.extend([\r\n                output.get_shape().as_proto() for output in op.outputs])\r\n          bytesize += op.node_def.ByteSize()\r\n          if bytesize >= (1 << 31) or bytesize < 0:\r\n            raise ValueError(\"GraphDef cannot be larger than 2GB.\")", "Hi, is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "How does this question been solved? "]}, {"number": 5491, "title": "R0.11", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@htwong39, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @gunan and @keveman to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 5490, "title": "Updating CMake requirements #5487", "body": "Issue #5487 ", "comments": ["Can one of the admins verify this patch?\n", "@Carmezim, thanks for your PR! By analyzing the history of the files in this pull request, we identified @clsung, @mrry and @guschmue to be potential reviewers.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "The Linux GPU failure was a network flake and won't be affected by this change, and the CMake tests passed, so I'm going to merge it.\n"]}, {"number": 5489, "title": "Branch 138590602", "body": "", "comments": []}, {"number": 5488, "title": "Tensoflow slim eval_image_classifier with TypeError Can not convert a dict_values into a Tensor or Operation.)", "body": "I\u2018m studying the tensoflow, and want to test the example of slim. When I command ./scripts/train_lenet_on_mnist.sh, The program run to eval_image_classifier give a Type Error, The Error information as follows:\r\n\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\r\nINFO:tensorflow:Scale of 0 disables regularizer.\r\nINFO:tensorflow:Evaluating /tmp/lenet-model/model.ckpt-20002\r\nINFO:tensorflow:Starting evaluation at 2016-11-09-02:55:57\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties:\r\nname: Quadro K5000\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.7055\r\npciBusID 0000:03:00.0\r\nTotal memory: 3.94GiB\r\nFree memory: 3.61GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K5000, pci bus id: 0000:03:00.0)\r\nINFO:tensorflow:Executing eval ops\r\nINFO:tensorflow:Executing eval_op 1/100\r\nINFO:tensorflow:Error reported to Coordinator: <class 'TypeError'>, Fetch argument **dict_values([<tf.Tensor 'accuracy/update_op:0' shape=() dtype=float32>, <tf.Tensor 'recall_at_5/update_op:0' shape=() dtype=float32>]) has invalid type <class 'dict_values'>, must be a string or Tensor. (Can not convert a dict_values into a Tensor or Operation.)\r\nTraceback (most recent call last):**\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 218, in __init__\r\n    fetch, allow_tensor=True, allow_operation=True))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2455, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2547, in _as_graph_element_locked\r\n    % (type(obj).__name__, types_str))\r\nTypeError: Can not convert a dict_values into a Tensor or Operation.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"eval_image_classifier.py\", line 191, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"eval_image_classifier.py\", line 187, in main\r\n    variables_to_restore=variables_to_restore)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/evaluation.py\", line 359, in evaluate_once\r\n    global_step=global_step)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/evaluation.py\", line 260, in evaluation\r\n    sess.run(eval_op, eval_op_feed_dict)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 717, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 902, in _run\r\n    fetch_handler = _FetchHandler(self._graph, fetches, feed_dict_string)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 358, in __init__\r\n    self._fetch_mapper = _FetchMapper.for_fetch(fetches)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 189, in for_fetch\r\n    return _ElementFetchMapper(fetches, contraction_fn)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 222, in __init__\r\n    % (fetch, type(fetch), str(e)))\r\nTypeError: Fetch argument dict_values([<tf.Tensor 'accuracy/update_op:0' shape=() dtype=float32>, <tf.Tensor 'recall_at_5/update_op:0' shape=() dtype=float32>]) has invalid type <class 'dict_values'>, must be a string or Tensor. (Can not convert a dict_values into a Tensor or Operation.)\r\n\r\nI don't know what happened to the program, I didnot revised any code, just download the code package from github, and for datadownload and train, which give correct result.Is there any help me?  I am waiting online. Thanks\r\n", "comments": ["The problem is the compatible of python2 and python3. As I used python3 for interpretation, but the the Keys From a Dictionary is different from python2 and python3. In Python 2, simply calling keys() on a dictionary object will return what you expect, however, in Python 3, keys() no longer returns a list, but a view object, so The TypeError can be avoided and compatibility can be maintained by simply converting the dict_keys object into a list which can then be indexed as normal in both Python 2 and Python 3. I edited the eval_image_classifier using eval_op=list(names_to_updates.values()).and then it can work perfectly\n", "Can you do a PR to tensorflow/models with the fix?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 5487, "title": "Update Cmake minimum requirement to v3.5", "body": "As it seems to have appeared an [issue with Cmake 3.3](https://github.com/tensorflow/tensorflow/issues/17#issuecomment-258649999) and the minimum known working Cmake version apparently is 3.5 [(see here)](https://github.com/tensorflow/tensorflow/pull/5071#issuecomment-255490169). Should the minimum requirement be updated on [CmakeLists](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/CMakeLists.txt#L2) to 3.5 and the [readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md#pre-requisites) for minimum 3.5 up to 3.6?\r\nIf those changes look relevant and are the ones to be made I could update them and submit a PR.", "comments": ["Sure please submit a PR.\n\nThanks,\nSherry\n"]}, {"number": 5486, "title": "Disable flacky test contrib/slim/learning_test.", "body": "", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @nathansilberman, @sguada and @tensorflower-gardener to be potential reviewers.\n"]}, {"number": 5485, "title": "Possible bug when instantiating multi RNN cell in PTB LSTM model", "body": "Note sure whether this is a bug, since TensorFlow API is unclear on this. Would like someone more familiar with TensorFlow to investigate.\r\n\r\nThe possible bug is in file tensorflow/models/rnn/ptb/ptb_word_lm.py on line 115. The line reads:\r\n\r\ncell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers, state_is_tuple=True)\r\n\r\nAccording to the API reference, the constructor for MultiRNNCell is:\r\n\r\ntf.nn.rnn_cell.MultiRNNCell.__init__(cells, state_is_tuple=True)\r\n\r\nwhere \r\n\r\ncells: list of RNNCells that will be composed in this order.\r\n\r\nThe following notation:\r\n\r\n[lstm_cell] * config.num_layers \r\n\r\nwill create a list of size config.num_layers, where each element in the list is a reference to the same object lstm_cell. This potentially means that if we modify one layer of \"cell\", we will also be modifying all the other layers. Is this what we want?", "comments": ["**init** is the Python constructor. I'm confused by your question.\n", "Ah I see.  Yes in this case we want the same cell for each layer.  Each layer will nevertheless receive its own parameters, because parameters are not tied to the layer instance.\n"]}]