[{"number": 16262, "title": "Cannot opened include file \"tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h\": no such file or directory", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: source \r\n- **TensorFlow version (use command below)**: current git master branch, should be v1.4.1 or v1.5.0rc1?\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: MSVC2015\r\n- **CUDA/cuDNN version**: CPU build only, gpu function is off\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: Trying a minimal build with cmake, with only snappy support and optimize for native arch turned on\r\n\r\n### Describe the problem\r\nBuild failing due to missing header files \"tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h\".\r\n\r\nEverything build succesful except for tpu project. I don't really know how to generate the pb.h file from protoc manually. I trying to fix the problem by chaning .cmake files, but not sure which one is for tpu.\r\n\r\n### Source code / logs\r\n133>D:\\MSVC-source\\tensorflow\\tensorflow\\contrib\\tpu\\ops\\tpu_embedding_ops.cc(16): fatal error C1083: Cannot open include file: 'tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h': No such file or directory\r\n\r\n", "comments": ["That file seems to be only 10 days old.  You can get it here\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tpu/proto/tpu_embedding_config.proto\r\nbut maybe you've got an inconsistent or incomplete view of the master repository.", "Yesterday I downloaded Tensorflow without a hitch as per the single line \"pip install --upgrade tensorflow\" instructions. Now I'm getting the exact same error.... and the distribution already includes that file you mention. The problem is: how is the \".h\" file generated from this \".proto\" file... if indeed that is what happens? Given that neither me nor the OP want any TPU support, how can we switch this off in the build? (my build has 240 successful projects, 2 failed projects, and the failures start with this error, then continue with roughly 570 LNK2005 errors from redefined functions)", "I get this file in my local git repository, the problem is that protoc does not generate the correct header file (pb.h) from the .proto file", "Thanks for the fix!", "@jackyko1991 hi, how did you find this pb.h file, in what directory of the local repository? could you please help me out? I face this problem as well", "The pb.h file is generated by protoc using the .cmake file. The issue is\nfixed in recent update\n\n2018\u5e741\u670824\u65e5 17:12 \u65bc \"Anguliachao\" <notifications@github.com> \u5beb\u9053\uff1a\n\n> @jackyko1991 <https://github.com/jackyko1991> hi, how did you find this\n> pb.h file, in what directory of the local repository? could you please help\n> me out? I face this problem as well\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16262#issuecomment-360066604>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALgaB2BrYSHmj5CXPCLmcBuz-zJDKRFJks5tNvPzgaJpZM4Rlq4j>\n> .\n>\n", "@jackyko1991 how did you fix it?\r\nI generate files like \"protoc --cpp_out=./ tensorflow/core/protobuf/*.proto\"\r\nafter that I got lots of error like image shows.\r\n\r\n![2018-07-03 13-34-24](https://user-images.githubusercontent.com/12708080/42200462-ee7205a2-7ec5-11e8-9ae6-f77cbca15e7d.png)\r\n\r\nafter add `-std=c++11` to `CFLAGS` in Makefile...\r\n\r\n![2018-07-03 13-36-55](https://user-images.githubusercontent.com/12708080/42200504-2ac3d666-7ec6-11e8-8b72-abfd1e592e7d.png)\r\n\r\nhow should I do?\r\n\r\nand I cant understand that why highest() and lowest() can passed, but infinity() and quiet_NaN() failed. whats the difference between them in coding layer?\r\n![2018-07-03 14-22-52](https://user-images.githubusercontent.com/12708080/42201994-9738cc56-7ecc-11e8-881c-b4629e30a46d.png)\r\n"]}, {"number": 16261, "title": "Tensorflow Debug tfdbg ValueError with combined loss functions.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes. Included\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Home x64, 1709\r\n- **TensorFlow installed from (source or binary)**: binary (pip3)\r\n- **TensorFlow version (use command below)**:  1.4.0\r\n- **Python version**:  3.5.2\r\n- **Exact command to reproduce**: See source code.\r\n\r\n### Describe the problem\r\nWhen attempting to combine two loss functions, tfdbg fails to properly grab the gradients due to a ValueError when executing run.\r\n\r\n### Source code / logs\r\n[consolelog.txt](https://github.com/tensorflow/tensorflow/files/1649424/consolelog.txt) Log of the error.\r\n[not_working.txt](https://github.com/tensorflow/tensorflow/files/1649426/not_working.txt) Code example that generates the error.\r\n[working.txt](https://github.com/tensorflow/tensorflow/files/1649427/working.txt) Code example where the two loss functions are split that does not generate the error.\r\n\r\n\r\n\r\n\r\n", "comments": ["@caisq, could you PTAL. Thanks!", "@zazabar Thanks for taking the time to report this issue and creating the example code for reproduction.\r\n\r\nHowever, I can't reproduce the issue using your `not_working.txt` on a GPU machine that I have. See screenshot of the successful `tfdbg> run` below:\r\n![tfdbg_bug_fail_bug_works](https://user-images.githubusercontent.com/16824702/35258674-141b22aa-ffcf-11e7-8f65-62bc792f6b27.png)\r\n\r\nThe `tfdbg> run` passed 5 out of the 5 times that I ran it. This was the case for the latest nightly version of tensorflow-gpu (1.5). I didn't try 1.4 though (too painful to downgrade CUDA from 9 to 8... :p )\r\n\r\nI wonder whether this has to do with GPU type or OS details.\r\nMine is a GeForce GTX 1050 Ti. My OS is Ubuntu 16.04.\r\n\r\nI haven't tried Windows, though. Maybe this is a Windows-specific issue. If you could try a Linux machine, that would be great. I can also try a Windows machine with GPU later.\r\n", "Hey @caisq ,\r\n\r\nAs you indicated, when I ran it on a Linux machine (Mint), it did not run into any problems. So perhaps this is just a Windows issue. Is there anything else that I can supply that might help fish out what the culprit is?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 16260, "title": "easier installation debugging", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 with Java API\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8 with cudnn64_6.dll\r\n- **GPU model and memory**: 1080\r\n- **Exact command to reproduce**: HelloTF.java\r\n\r\n### Describe the problem\r\nDefault error message from NativeLibrary load() method is not helpful enough.   \r\nSimple suggested improvement: please print the contents in the string variable \"frameworkResourceName\", which is the missing resource, when throwing a new UnsatisfiedLinkError exception.\r\n\r\n### Source code / logs\r\nSuggested Source Code Improvement for NativeLibrary.java:\r\n\r\n    if (jniResource == null) {\r\n      throw new UnsatisfiedLinkError(\r\n          String.format(\r\n              \"Cannot find TensorFlow native library %s for OS: %s, architecture: %s. See \"\r\n                  + \"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md\"\r\n                  + \" for possible solutions (such as building the library from source). Additional\"\r\n                  + \" information on attempts to find the native library can be obtained by adding\"\r\n                  + \" org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.\",\r\n                  frameworkResourceName,\r\n              os(), architecture()));\r\n    }\r\n", "comments": ["TensorFlow for Java installation instructions provide the pom.xml information and this works.  However, TensorFlow Java also needs the associated native OS binaries.  I am running on Windows 10.  Where are the TensorFlow OS specific DLL binaries, including binaries for CUDA usage.  For example, where is this native file supposed to be downloaded from?  Where is this installation requirement in the Java instructions?\r\norg/tensorflow/native/windows-x86_64/tensorflow_framework.dll\r\n\r\nMaking the code change per suggestion clearly shows the problem:\r\nException in thread \"main\" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library org/tensorflow/native/windows-x86_64/tensorflow_framework.dll for OS: windows, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.", "Note: DLL download from here:  www.tensorflow.org/install/install_java#install_on_windows", "@asimshankar Could you take a look?", "@wintsonpryme : I apologize, but I'm not sure I fully follow your question. But hopefully some background here will be informative.\r\n\r\nIf you're using maven and following the instructions at https://www.tensorflow.org/install/install_java#using_tensorflow_with_a_maven_project\r\nthen the native libraries are automatically be included (and extracted). This happens because the [`org.tensorflow:tensorflow`](https://search.maven.org/#artifactdetails%7Corg.tensorflow%7Ctensorflow%7C1.5.0-rc1%7Cjar) maven artifact in turns pulls in the [`org.tensorflow:libtensorflow_jni`](https://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.tensorflow%22%20AND%20a%3A%22libtensorflow_jni%22) artifact, which includes the platform specific native library, which is extracted on program run (via code in `NativeLibrary.java`). Is that not working for you? (More information on the structure of the Maven packages is in [tensorflow/java/maven/README.md](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/maven#artifact-structure))\r\n\r\nNote that for Windows, the release binaries do not yet include GPU support. So, for GPU support you'll have to build the native library from source, and then provide it to your Java program (setting `MAVEN_OPTS` if you're using maven, see [this](https://github.com/asimshankar/java-tensorflow/tree/master/gpu#using-gpus)). Though, in the future we hope to make this simpler by including the CUDA-enabled native library in the [`org.tensorflow:libtensorflow_jni_gpu`](https://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.tensorflow%22%20AND%20a%3A%22libtensorflow_jni_gpu%22) artifact.\r\n\r\nThe code change you suggested isn't necessarily accurate. The native library might be built in two shared libraries or a single one, depending on whether `--config=monolithic` was provided to bazel during the builds. For Windows release, we currently build with `--config=monolithic`, so there is no `tensorflow_framework.dll`, and instead just a single dynamic library: `tensorflow.dll`.\r\n\r\nI'm not sure if I've helped with your concern. Please do let us know, or if you have a specific follow up question, we're happy to help. Thanks.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "create a folder structure like org\\tensorflow\\native\\windows-x86\\tensorflow_jni.dll and paste that inside your class path. try running post that you will not get any error.\r\n"]}, {"number": 16259, "title": "Fix two small issues of XLA", "body": "1.In Tensorflow, conventionally, INT32 ops are regarded as shape or control\r\nops, and are registered on CPU only. XLA should follow the same rule, to\r\navoid the potential data transfer between TF CPU ops and in GPU_XLA ops,\r\nwhich result into performance degradation when XLA is turned-on.\r\n2. Any OpKernel with more than 500 inputs is excluded. This is a temp\r\nworkaround to avoid an potential issue that, the cuda drive do not accept\r\na compiled PTX kernel with parameter space larger than 4352 bytes. The data type\r\nof PTX kernel parameter is u64. An OpKernel with 500 inputs are more likely\r\nto exceed the limit.", "comments": ["Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@yangjunpro @jlebar @hawkinsp did you arrive on a consensus on whether to proceed here?", "I'm not in favor of accepting a solution like this. It gives us a false sense of security, and only sort of works around the problem, sometimes.  The workaround also has significant performance implications.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "closing since @jlebar is not in favor of this."]}, {"number": 16258, "title": " Enhance layout optimization logic to remove duplicated layout transp\u2026", "body": "Enhance layout optimization logic to remove duplicated layout transpose operation for performance improvement purpose.\r\n    For example:\r\n            a NCHW-to-NHWC transpose op followed by a NHWC-to-NCHW\r\n            transpose op, under this scenario these two ops acutally can be absorbed into\r\n            nothing, thus avoid unnecessary computation.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@yangjunpro any progress addressing the comments?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@yangjunpro friendly poing. any progress?", "It has been 28 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 16257, "title": "feature request: KL distance for Gaussian Mixture Model", "body": "I am hoping that **tf.contrib.distributions** module is expanded so that we can calculate **KL** divergence between **multivariate Gaussian Mixture Models(GMM)** ,with its paramter list such as weight, mean, covariance given as Tensor Array. Because I think there is going to be a more need for that for many applications. Thank you.\r\n\r\nWith current version, either  we can calculate KL divergence for a single gauss, or create GMM object, but not KL for GMM.\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/distributions/Mixture\r\nhttps://www.tensorflow.org/api_docs/python/tf/distributions/kl_divergence\r\n\r\nI tried as shown below, but it didn'T work.\r\n\r\n    import tensorflow as tf\r\n    print('tensorflow ',tf.__version__)  # for Python 3\r\n    import numpy as np\r\n    import matplotlib.pyplot as plt\r\n\r\n    ds = tf.contrib.distributions\r\n    kl_divergence=tf.contrib.distributions.kl_divergence\r\n\r\n    # Gaussian Mixure1\r\n    mix = 0.3# weight\r\n    bimix_gauss1 = ds.Mixture(\r\n    cat=ds.Categorical(probs=[mix, 1.-mix]),#weight\r\n    components=[\r\n       ds.Normal(loc=-1., scale=0.1),\r\n       ds.Normal(loc=+1., scale=0.5),\r\n    ])\r\n\r\n    # Gaussian Mixture2\r\n    mix = 0.4# weight\r\n    bimix_gauss2 = ds.Mixture(\r\n        cat=ds.Categorical(probs=[mix, 1.-mix]),#weight\r\n        components=[\r\n            ds.Normal(loc=-0.4, scale=0.2),\r\n            ds.Normal(loc=+1.2, scale=0.6),\r\n    ])\r\n\r\n    # KL between GM1 and GM2\r\n    kl_value=kl_divergence(\r\n        distribution_a=bimix_gauss1,\r\n        distribution_b=bimix_gauss2,\r\n        allow_nan_stats=True,\r\n        name=None\r\n    )\r\n     sess = tf.Session() # \r\n     with sess.as_default():\r\n        x = tf.linspace(-2., 3., int(1e4)).eval()\r\n        plt.plot(x, bimix_gauss1.prob(x).eval(),'r-')\r\n        plt.plot(x, bimix_gauss2.prob(x).eval(),'b-')\r\n        plt.show()\r\n\r\n        print('kl_value=',kl_value.eval())`\r\n\r\nThen I got this error... **NotImplementedError: No KL(distribution_a || distribution_b) registered for distribution_a type Mixture and distribution_b type Mixture**\r\n\r\nI know that with python sklearn without Tensorflow, we can calculate KL for GMM as shown below.\r\nhttps://stackoverflow.com/questions/48335823/tensorflow-kl-divergence-for-a-gaussian-mixure\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:1.4\r\n- **Python version**: 3\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:", "comments": ["@ebrevdo, please take a look at this feature request.\r\n", "This seems like a reasonable request. Ive added it to our feature request queue.\r\n\r\nIn the meantime, let me know if you're interested in adding it yourself.", "Variational Approximatin for  KL divergence in Gaussian Mixture is proposed by google scholar\r\nhttps://scholar.google.com/citations?user=ACcAdXgAAAAJ&hl=ja&oi=sra\r\n\r\nhttp://ieeexplore.ieee.org/abstract/document/4218101/", "Noted! Thanks! ", "It just occurred to me but the VectorDiffeomixture might be useful to you?\r\nhttps://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/distributions/VectorDiffeomixture\r\nhttps://arxiv.org/abs/1801.03080", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 115 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 130 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 145 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Distributins have moved to: https://github.com/tensorflow/probability\r\nCan you raise this issue there? (Or better, add PR which implements it! :))"]}, {"number": 16256, "title": "[tflite] make calling NNAPI work again", "body": "calling PrepareOpsAndTensors() before using NN API looks\r\n1. unnecessary\r\n2. will decrease `next_node_to_prepare_ so` that the next\r\n```\r\n   next_node_to_prepare_ == nodes_and_registration_.size()\r\n```\r\nwill fail", "comments": []}, {"number": 16255, "title": "tf.scatter_update Error ", "body": "Hi, \r\n\r\nI use tf.scatter_update to update non-trainable variables AS and AO in a code. As I found, when one uses scatter_update, the gradient misses, so that there is no gradient. \r\nBecause of that I set both AL and AO as non-trainable variables (actually they are non-trainable), called the optimizer: tf.train.AdamOptimizer(config.actor_lr0,0.9,0.999,1e-8).minimize(actor_loss), and I thought everything should be fine. \r\nHowever, I am getting error:\r\nLookupError: No gradient defined for operation 'actor/encoder/beer_game_flow_8/next_scat_j_2' (op type: ScatterUpdate). \r\nHere are the lines of the code that I update AO and AS that gives the error:\r\n\r\nself.players[k-1].AS = tf.scatter_update(self.players[k-1].AS, \r\n                    self.curTime + leadTimeIn, \r\n                    tf.add(self.players[k-1].AS[self.curTime + leadTimeIn], possible_shipment), name='next_scat_j' )                   \r\n\r\nself.players[k+1].AO = tf.scatter_update(self.players[k+1].AO, \r\n                    self.curTime + leadTime, tf.add(self.players[k+1].AO[self.curTime + leadTime], \r\n                    self.players[k].actionValue(self.curTime, self.playType))\r\n                    , name='handle_scat_j')\r\n\r\nSince both AS and AO are non-trainable, I do not need their gradient, and AS and AO are the only variable in this op. So, I was wondering why TensorFlow want to obtain the gradient, since there is no trainable variable here? \r\nIs it something that you can fix it, or is there any reason behind this behavior? \r\n\r\nBTW, I use python 2.7 with tf 1.4.0 on Debian 8.7 with a K80 with 12GB of memory. \r\n\r\nThanks, \r\nAfshin\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi, \r\nAs I mentioned in my question, I use python 2.7 with tf 1.4.0 on Debian 8.7 with a K80 with 12GB of memory. I am using CUDA 8 and cuDNN 6. I installed it through pip and did not install bazel. \r\nThe whole code is more than 1000 lines and it is not on a public git yet. \r\nIf you could not check the error, I'll try to implement a small code to reproduce the error. \r\nPlease let me know. \r\nAfshin\r\n", "The error is telling you that ScatterUpdate does not have a gradient defined.  The rationale behind this is discussed in #2770\r\n\r\nClosing this out.  As discussed in #2770, follow-ups should go to stackoverflow.  Thanks!"]}, {"number": 16254, "title": "adding placeholder_with_default in order to feed both via dataset and placeholders produces error on GPU", "body": "\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:1.4.0\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**:0.9.0\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**:9.1.85\r\n- **GPU model and memory**:Titan V, 12G\r\n- **Exact command to reproduce**: See below\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nThe issue is when you want to train a classifier that takes both placeholder and dataset via queue to feed input. The reason one may want to do that is to run inference via placeholders. \r\n\r\nI added the following line under define the model section of train_image_classifier.py of slim library\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\n_images = tf.placeholder_with_default(image, shape=[...], name='input)\r\n I get an error of the following kind when running on GPU:\r\nCannot assign a device for operation 'input': Could not satisfy explicit device specification '/device:GPU:0' \r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Thanks @poxvoculii I will post there as well but my impression was that this is a bug. Can you verify what I've got is an expected behavior?", "From your posting I can't tell exactly what you're trying to do.  The error message suggests that you're attempting to assign a non-GPU operation to a GPU.", "I was finally been able to achieve my goal through a different approach. But I found a bug in the current slim build. It seems like the existing bazel deployment to export and freeze a graph is not compatible with the latest slim release as it generates an error about lack of dilation in node def. ", "Not sure what can I do, maybe @mahyaar can explain the solution."]}, {"number": 16253, "title": "Introducing TensorRT operator", "body": "This PR introduces a new op that wraps around an highly optimized TensorRT engine and provides a seamless integration between TensorRT and TensorFlow.\r\n- Add a TRTEngineOp that encapsulates a TensorRT executable.\r\n- Add CreateInferenceGraph to contract a TensorRT-compilable subgraph to a TRTEngineOp.\r\n- Update BUILD files to include new contrib package\r\n- Add tensorflow.contrib.tensorrt python package to expose API to python\r\n", "comments": ["@samikama \r\nThanks for the nice work.  \r\nActually our team are working on the same task of integrating TensorRT with TensorFlow with the same design idea even with some similar function/variable name:). it looks that NVIDIA team run a little bit faster.\r\n\r\nMay I know have you made any benchmark with current implementation? Since with this integration TensorRT actually could leverage existing TF ops for its functionality completeness which is actually a headache for traditional TensorRT execution solution. \r\nBut there may be some potential performance overhead between the switch of TensorRT of TF(around several hundred us to 1ms observed in our scenarios), also if there are multiple switches between TensorRT and TF,  we need to be more careful about its overhead(considering TRT->TF->TRT execution plan).\r\n\r\nSo any performance number sharing will be really helpful.\r\n\r\nIf the work is solid enough, we would like to contribute more enhancement based on this PR and apply it into our production environment and I think this will be a win-win for community, NV and Alibaba.", "@yangjunpro \r\nThanks,\r\nWe have some preliminary measurements(@jjsjann123) for ourselves to see whether everything is working or not and we see a nice improvement. But we didn't go through performance in detail yet so we expect some more improvement soon. We can try to do some more measurements and get back to you.", "We've committed 76f6938bafeb81a4ca41b8dac2b9c83e1286fa95 for the build config and I'm merging the conflicts. Some difference between that commit and this PR:\r\n\r\n1. We need to use macro GOOGLE_TENSORRT to guard all c++ files, see [tensorrt_test.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/tensorrt_test.cc#L20) for an example.\r\n2. Bazel macro 'if_trt' now becomes 'if_tensorrt'\r\n3. The 'tensorrt' lib is renamed to 'nv_infer'\r\n\r\nMeanwhile I'll try to pull in this PR locally to make sure it works, thanks.", "I merged this PR locally with tensorflow master branch, and tried to build it with configure \"build tensorflow with tensorRT support: [y/N] y\"\r\nWhile building tensorflow from source, I got this error msg: \r\nTensorRT library version detected from /mnt/evanlu.lyf/tensorRT/TensorRT-3.0.0/include/NvInfer.h (4           //!<.0          //!<.0          //!<) does not match TF_TENSORRT_VERSION (4.0.0). To fix this rerun configure again.\r\nIs this PR is currently only internal for NV developers using tensorRT 4.0 version ?\r\n\r\nI just wondering, if it is possible to use tensorRT 3.0 instead? since v3.0 is released, then I could test this new feature locally.", "@evanluyifan - Thanks for the feedback.  No, this is not only for internal NV development.  The libnvinfer version numbering is simply done independently of the main TensorRT version number.  By any chance do you have the TRT 3.0.0 RC version instead of the more recent TRT 3.0.1 GA release?", "@evanluyifan the PR head is also failing on me after the master merge. As @cliffwoolley said, we are using the public TensorRT GA.\r\nIf you want to use it, try checkout commit 9384314 ", "Hi @aaroey,\r\n\r\n76f6938 seems to break our builds as well. I believe there could be some improvements to it. Does it work for you internally?\r\n\r\nThanks,\r\nSami\r\n", "@samikama I'm working on the fix.", "@aaroey Thanks!\r\nI pushed some updates addressing the review comments. Will test it with the merge once I saw your fix :)", "Hi @aaroey,\r\n\r\nI pushed some changes that fixes the build  for me.  Do these work for you as well?", "@samikama Thanks, I also pushed some fixes. It looks like your changes conflict with mine. Would you sync to head and retry?", "Hi @aaroey,\r\n\r\nI don't see any changes since 8e03944 . It was failing for me.", "@samikama Let me fix the problem in logger. It looks like the problem is in merging: https://github.com/tensorflow/tensorflow/pull/16253/files/ae740a67bdc01b991ead6ac047c774bff4d7bc8f..fe4260f71479374aee8340b8dacdbec8102d18c3#diff-39a0a00c611f917b0c3bbc776ddd6e01R31", "@samikama please check again.", "@aaroey,\r\nDone, sorry for the missed merge issue.\r\n", "@aaroey \r\nJust pushed another commit, removing the debug logs and replacing them with vlogs", "Build works for me.\r\nBut cannot import tensorrt in python\r\n```\r\n\r\nimport tensorflow.contrib.tensorrt\r\n2018-01-29 16:29:45.638178: F tensorflow/core/framework/op.cc:54] Non-OK-status: RegisterAlreadyLocked(op_data_factory) status: Already exists: Op with name _Arg\r\nAborted (core dumped)\r\n\r\n```", "@aaroey, @wujingyue \r\nIs there anything else left?", "It looks like, core/ops/function_ops.cc seems to be included in python bindings we create which cause static constructors to be included in our .so which in turn is causing our import to cause an abort due to multiple registration of \"_Arg\" system op, as observed by @jjsjann123 . At least this is my understanding. This file wasn't included before, @aaroey do you know what could have caused this change?", "@cliffwoolley  @jjsjann123   Thansk for your response,  have you fixed the issue on building with master branch ? \r\nI' currently using tensorRT 3.0 RC release.  Do I need to upgrade this to 3.0.1 ?\r\nAnd since this PR is based on the Public GA, why there is a TRT version check? which indicates we need tensorRT version >= 4.0 ?\r\n\r\nBTW, I think what this PR doing is adding tensorRT support into tensorflow, is there any schedule for this?\r\n\r\nPersonally I do agree that tensorflow with tensorRT native support is a great idea. On the other hand, we also have some guys working the XLA side. I just wondering, for the inference part, especially the CNN networks (I know TRT currently could only fully support CNN networks), what would be the future of \"XLA optimaztion\" vs \"tensorRT support\"?  ", "@evanluyifan - Please do update to TensorRT 3.0 GA, available from https://developer.nvidia.com/tensorrt .  The most recent build posted there is numbered 3.0.2.\r\n\r\nAgain you don't need a \"4.0\" version of TensorRT - there's no such thing.  TensorRT 3 will include libnvinfer4 out of the box.\r\n\r\nAnd yes this PR does provide integration of TRT into TF.  We have several future steps in mind to incrementally improve this integration and better leverage internal TF mechanisms; this is just the first step.  I cannot comment on a timeline, however.\r\n\r\nThanks,\r\nCliff", "@evanluyifan I think this should work with any trt version, if it doesn't, please let me know and provide a reliable repro case. There is still some issues with the build, while we're working on the fix,  this PR should be in within a few days.", "Looks like some of the later changes over-write benoitsteiner's fix on the grappler thing:\r\n\r\n```\r\nTurn the op_performance_data proto lib into a header only library by \u2026\r\n\u2026default\r\n\r\nPiperOrigin-RevId: 182621348\r\n\r\n```\r\n\r\nNot sure if applying it would break other TF stuff though.", "@samikama @jjsjann123 I'm working on fixing the build dependencies, and hopefully it would solve the problem of double linking. Just FYI.", "@samikama @jjsjann123 the import error is caused by grappler now:\r\n\r\n$ bazel query 'somepath(//tensorflow/contrib/tensorrt:init_py, //tensorflow/core:functional_ops_op_lib)'\r\n//tensorflow/contrib/tensorrt:init_py\r\n//tensorflow/contrib/tensorrt:trt_convert_py\r\n//tensorflow/contrib/tensorrt:wrap_conversion\r\n//tensorflow/contrib/tensorrt:_wrap_conversion.so\r\n//tensorflow/contrib/tensorrt:trt_conversion\r\n//tensorflow/core/grappler/costs:graph_properties\r\n//tensorflow/core:core_cpu_base\r\n//tensorflow/core:functional_ops_op_lib\r\n\r\nWould you help to fix that?", "Verified that the patch from Benoitsteiner is indeed in the code (making op_performance_data header only). \r\n\r\nDo remember hearing from him about the fix and another patch that is supposed to fix the dependency on grappler/clusters:single_machine.\r\nMaybe the second patch broke the first one ? Should we ping Benoitsteiner, unless @samikama had a fix for this.", "If the fix is going to take some time, I could revert the code back and use grappler from python side. That would get rid of the dependency. @aaroey ", "@jjsjann123 I fixed it and testing it locally after merging with changes from @aaroey. Will push it shortly\r\n", "@zheng-xq @aaroey ,\r\nI changed logging statements to VLOG but it seems VLOG logic is inverted. In order to see WARNING messages, INFO messages are also printed, to see FATALs all levels are displayed. \r\n`\r\n#define VLOG_IS_ON(lvl) \\\r\n  ((lvl) <= ::tensorflow::internal::LogMessage::MinVLogLevel())\r\n#endif\r\n`\r\nThis is counter-intuitive for me, also inverted logic wrt LOG. See below,\r\n`\r\nLogMessage::~LogMessage() {\r\n  // Read the min log level once during the first call to logging.\r\n  static int64 min_log_level = MinLogLevelFromEnv();\r\n  if (TF_PREDICT_TRUE(severity_ >= min_log_level)) GenerateLogMessage();\r\n}\r\n`\r\nIs this on purpose or is this a bug?\r\n\r\nShould I change it? If this is on purpose we need to invert VLOG severities in the code.  \r\n\r\nThanks,\r\nSami\r\n", "Hi @cliffwoolley , I have a simple question, does this change including the tensorRT optimizer? normally tensorRT save the optimized graph as a plan, while in this PR, I think the engine plan is stored in the cache, right?\r\nSo, could you show me where we do the optimize job and create a plan for trt_engine_op to load?", "@samikama That is intended. For VLOG(<level>), higher number of <level> indicates a chattier message. We should not treat <level> the same as INFO/WRANING/etc for LOG, they're different things. Similarly, for LOG(<severity>), don't treat <severity> as a number.", "Just sharing the information. Build is still broken for me.\r\n\r\nOpKernel registration seems to have problem. Inference sess run complains about No OpKernel was registered for 'TRTEngineOp'. Sami is looking at it now.", "Hi,\r\n\r\nI just pushed a new commit that fixes the issue with the missing op kernel. It looks like new defines left out the implementation and registration of the op during the compilation due to missing defines in compiler command line. I also added a small test script to validate the functionality of the workflow.\r\n\r\n@aaroey  and @wujingyue, if you are happy with the changes, I think we can continue to the next step.\r\n\r\nThanks,\r\nSami\r\n", "@samikama Thanks for the fix! And sorry about the missing tf_copts(), I guess I would have found the problem earlier if I built it with python 2.7, but I was building with python 3.5 and met that import error mentioned above.\r\n\r\nNow I can build, import and run your testing scripts with python 2.7, but still fail with python 3.5 (have you tried that?). I think we can go ahead and merge with upstream first, and I'll add more comments after that.", "@aaroey Oops! I forgot that module init functions changed signature between py2 and py3, I fixed that now and testing. Give me a few minutes to do a clean build and I will push it shortly", "Hi @aaroey,\r\n\r\nI pushed the updated script. You should be able to import the module from py3 now. However there is a swig problem that prevents use of the module. TF swig bindings seem somehow inconsistent while passing between py->c++->py layers in py3. It looks like std::strings require byte objects for py3->c++ layer on the other hand strings are converted to unicode string when going back from c++ to py3\r\n\r\nI will try to see how it is resolved elsewhere in TF code, but I think as you suggested we can proceed as it is right now.\r\n", "Hi @aaroey,\r\n\r\nI just pushed changes to make conversion to work both on py3 and py2 so we are in a good shape. If I look EDIT(below) some tests seem to be waiting. How does it work? Is somebody from your side need to trigger it?\r\n", "@samikama I've triggered some of the internal tests, it'll take some time and there will be more later.", "@aaroey, Thanks triggering tests. We applied the requested changes but since they are outstanding, builds are on hold again. If you are happy with the changes, could you please enable internal tests again so that if there are any problems with the internal checks we can fix them as soon as possible.", "Sorry @aaroey, it looks like kokoro is removing force-run once the run is started. I pushed some fixes for FAILs in sanity tests but they are not run again. Is there any other flag to have them automatically run or do you need to add force-run every time?", "@samikama Yes I think it requires manual triggering.", "@aaroey it looks like failures are not really related to this or. What do we do in this case", "Hi @samikama , is this PR ready for use or not? Or is there any previous commit that could be built correctly with tf master branch, so that I could use for testing?\r\nAnd may I ask, does this tensorRT-support going to be merged into tf 1.6?", "@samikama I will try to fix the test today.", "Failed to build for me with the following error:\r\n\r\n```\r\nERROR: /home/dmikushin/.cache/bazel/_bazel_dmikushin/2e6790ed2400dd10dbcc3c8a7729de12/external/nccl_archive/BUILD:33:1: error while parsing .d file: /home/dmikushin/.cache/bazel/_bazel_dmikushin/2e6790ed2400dd10dbcc3c8a7729de12/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/nccl_archive/_objs/nccl/external/nccl_archive/src/core.cu.pic.d (No such file or directory)\r\nnvcc fatal   : Unknown option 'iquote'\r\nnvcc fatal   : Unknown option 'iquote'\r\n```\r\n\r\nConfigured to compile with nvcc/gcc. I guess -iquote comes from clang\r\n", "Still the same problem:\r\n\r\n```\r\nERROR: /home/dmikushin/.cache/bazel/_bazel_dmikushin/2e6790ed2400dd10dbcc3c8a7729de12/external/nccl_archive/BUILD:33:1: error while parsing .d file: /home/dmikushin/.cache/bazel/_bazel_dmikushin/2e6790ed2400dd10dbcc3c8a7729de12/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/nccl_archive/_objs/nccl/external/nccl_archive/src/libwrap.cu.pic.d (No such file or directory)\r\nnvcc fatal   : Unknown option 'iquote'\r\nnvcc fatal   : Unknown option 'iquote'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 111.376s, Critical Path: 19.96s\r\nFAILED: Build did NOT complete successfully\r\n```", "Hi @dmikushin ,\r\n\r\nI am not sure the error you report is related with this PR. You might want to report it upstream. It is an external package not used or modified in this PR.", "@samikama This error appears only when I try to compile your PR tensorrt branch.", "@dmikushin, could you please tell me which master commit you tried and what is your platform compiler etc?\r\n", "@aaroey, please let us know if we can help with anything in going forward", "@samikama I'm pulling in this PR internally to make sure it works, there seems to be some swig problem but will let you know if I cannot figure it out.", "@aaroey, please do. Swig interfacing is a nightmare and I wasted hours. Let me know if I can investigate as well, I am happy to look into it.", "@samikama This is 37bcd5af97c822e99f617813269351447c79f161 on Ubuntu 17.04, CUDA 9. The build of 0464602ee99ff2a10336f2bea12440167e2d8a70 was previously successful. The '-iquote' problem occurs on random package, actually:\r\n\r\n```\r\nERROR: /home/marcusmae/apc/tensorflow/tensorflow/contrib/rnn/BUILD:218:1: error while parsing .d file: /home/marcusmae/.cache/bazel/_bazel_marcusmae/c5131a6e5cab73016b76ace3066ceaf4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/rnn/_objs/python/ops/_lstm_ops_gpu/tensorflow/contrib/rnn/kernels/lstm_ops_gpu.cu.pic.d (No such file or directory)\r\nnvcc fatal   : Unknown option 'iquote'\r\nnvcc fatal   : Unknown option 'iquote'\r\n```\r\n", "@samikama @jjsjann123  I've made some changes to make sure the code is compatible internally, please help to make sure they work from your side. Thanks.", "@aaroey unfortunately, I am getting a core dump after the changes. I am investigating.", "@aaroey last batch of changes seem to be causing a lot of trouble, would it be possible to revert them? If needed we can replace std::string in interface layers with tensorflow::string and push that change to test you internally.", "@samikama I just pushed a fix, the test script works well on building the trt op for both python2 and 3, but then it detect a mem leak as:\r\n```\r\n2018-02-07 09:22:29.444379: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:1650] OK finished op building\r\nTraceback (most recent call last):\r\n  File \".../tensorflow/contrib/tensorrt/test/test_tftrt.py\", line 67, in <module>\r\n    inpDims[0])  # Get optimized graph\r\n  File \".../tensorflow/contrib/tensorrt/python/trt_convert.py\", line 72, in CreateInferenceGraph\r\n    output_graph_def_string = to_bytes(out[1])\r\n  File \".../tensorflow/contrib/tensorrt/python/trt_convert.py\", line 49, in py3bytes\r\n    return inp.encode('utf-8', errors='surrogateescape')\r\nAttributeError: 'SwigPyObject' object has no attribute 'encode'\r\nswig/python detected a memory leak of type 'string *', no destructor found.\r\nswig/python detected a memory leak of type 'string *', no destructor found.\r\n```\r\nFor python 2 the error is like:\r\n```\r\n2018-02-07 09:44:28.066669: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:1650] OK finished op building\r\nTraceback (most recent call last):\r\n  File \".../tensorflow/contrib/tensorrt/test/test_tftrt.py\", line 67, in <module>\r\n    inpDims[0])  # Get optimized graph\r\n  File \".../tensorflow/contrib/tensorrt/python/trt_convert.py\", line 74, in CreateInferenceGraph\r\n    if len(status) < 2:\r\nTypeError: object of type 'SwigPyObject' has no len()\r\nswig/python detected a memory leak of type 'string *', no destructor found.\r\nswig/python detected a memory leak of type 'string *', no destructor found.\r\n```\r\nAny ideas?", "@aaroey unfortunately latest commit is not working for me due to swig bindings. What you see is also indicating that swig bindings are incomplete and possibly has lifetime issues.", "@aaroey,\r\n\r\nIf you are happy, could we start build tests? Meanwhile you can test for internal compliance.\r\n\r\nThanks,\r\nSami\r\n", "@samikama Sure, I'm building it internally. There is still some internal isssues and I'm trying to fixing them.", "@aaroey is there a way to test us the changes for your internals? Testing and modifying the code for internal and external independently leading to a lot of problems. We have other PRs waiting in the pipeline for this to be merged.\r\n", "@samikama Currently it's broken internally but not externally, and unfortunately I think we need to fix the internal issue first. My apologies about the errors from the last few commits, and for some of them like the issues of swig and convert_nodes.cc I was not able to provide correct solutions. Thanks so much for fixing them, and I'll make sure new changes work both internally and externally before I push them.", "@aaroey, errors are not a problem. Such things happen. We are a bit on tight schedule and holding  patches/features for next PRs. This PR has diverged enough that it may take some time to merge and validate further PRs and I am worried that we might be in a tough situation soon. We will try to provide minimalistic (O(100)LOC) PRs next time but we can't proceed without this one. Please let us know if there is anything that we can help. I am open to phone meeting if it helps.", "@samikama Understood. For python 3 I got the following problem now:\r\n```\r\n2018-02-08 22:35:58.512579: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:1590] OK finished op building\r\nTraceback (most recent call last):\r\n  File \".../tensorflow/contrib/tensorrt/test/test_tftrt.py\", line 67, in <module>\r\n    inpDims[0])  # Get optimized graph\r\n  File \".../tensorflow/contrib/tensorrt/python/trt_convert.py\", line 72, in CreateInferenceGraph\r\n    output_graph_def_string = to_bytes(out[1])\r\n  File \".../tensorflow/contrib/tensorrt/python/trt_convert.py\", line 49, in py3bytes\r\n    return inp.encode('utf-8', errors='surrogateescape')\r\nAttributeError: 'bytes' object has no attribute 'encode'\r\n```\r\nFor python 2 when I tried to `import tensorflow.contrib.tensorrt` it says libnvinfer not found, like:\r\n```\r\n>>> import tensorflow.contrib.tensorrt\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \".../tensorflow/contrib/tensorrt/__init__.py\", line 19, in <module>\r\n    from tensorflow.contrib.tensorrt.python import *\r\n  File \".../tensorflow/contrib/tensorrt/python/__init__.py\", line 6, in <module>\r\n    from tensorflow.contrib.tensorrt.python.ops import trt_engine_op\r\n  File \".../tensorflow/contrib/tensorrt/python/ops/trt_engine_op.py\", line 31, in <module>\r\n    resource_loader.get_path_to_datafile(\"_trt_engine_op.so\"))\r\n  File \".../tensorflow/contrib/util/loader.py\", line 55, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \".../tensorflow/python/framework/load_library.py\", line 58, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)\r\n  File \".../tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: libnvinfer.so.4: cannot open shared object file: No such file or directory\r\n```", "I need to fix #1. For #2 you need tensorrt installation in your LD_LIBRARY_PATH.", "@samikama Thanks. 2) works now.\r\n\r\nI would like to share with you current status of internal build. `tf_gen_op_wrapper_py(name = \"trt_engine_op\", xxx)` has building errors:\r\n```\r\n.../tensorflow/contrib/tensorrt/gen_trt_engine_op_py_wrappers_cc: error while loading shared libraries: libnvinfer.so.4: cannot open shared object file: No such file or directory\r\n```\r\nIt'll take me some time to figure out why this works externally but not internally. This also cause failure of another two `py_library` named `trt_ops_py` and `init_py`. All other targets in contrib/tensorrt work well. So I'm wondering if we can split the failing targets out from this PR and submit the healthy ones first, and send another PR for those targets. In this way you can send future PRs in parallel (assuming you don't depend on the failing targets), and at least we can make some progress.\r\n\r\nWhat do you think?", "@aaroey, package will not be functional without mentioned targets. The error message you get suggests that the tensorrt is not installed in your build system or glue package in your internal repo is not working as it should. Bazel uses soname to link against concrete library rather than softlink. Could you please verify soname of the library you have is libnvinfer.so.4 and genrule correctly adds this. Alternative could be modifying the genrule of the local_repository generated by tensorrt_configure correctly links all of them into build area?", "Thanks @samikama for the suggestions. The package is indeed installed internally and shown in the xx.runfiles directory in bazel-out/, and does have the correct soname. The problem is internally the genrule doesn't provide the same bash environment (more specifically LD_LIBRARY_PATH) as provided by bazel externally, so all cuda shared libraries are not accessible by the gen_trt_engine_op_py_wrappers_cc binary. I think it must be a problem of either the build config or  the internal bazel config, I'll need to figure it out.", "@aaroey, LD_LIBRARY_PATH is only relevant when running not linking. you can use -s flag to bazel to see exact commandline executed and check whether the paths are constructed correctly or not.", "@aaroey my tests are failing again. Perhaps we should sit together and fix these issues in front of a computer, talking.", "@samikama sure, set up a meeting, please join at your convenience.", "@samikama I've figured out the problem of gen_trt_engine_op_py_wrappers_cc. The failure was caused by some internal config, I was able to make it pass under very specific config. So the problem was not related to TF nor this PR, and is not a blocker of the merge.\r\n\r\nBut we still have swig problems, and we can discuss when we meet.", "Very interesting PR.  I have a few questions..\r\n\r\n1) Will this support int8 quantization?\r\n2) What is the difference in strategy between this and the UFF parser / converter Nvidia makes?  This doesn't seem to reuse that project so TensorFlow conversion code will diverge / have differences in behavior between tf.contrib.tensorrt and uff. The ops that this supports is very limited are there more in the works? The UFF parser supports these:\r\n```\r\n    \"Add\",\r\n    \"AvgPool\",\r\n    \"BiasAdd\",\r\n    \"ConcatV2\",\r\n    \"Const\",\r\n    \"Conv2D\",\r\n    \"Conv2DBackpropInput\",\r\n    \"DepthwiseConv2dNative\",\r\n    \"Div\",\r\n    \"FusedBatchNorm\",\r\n    \"MatMul\",\r\n    \"Maximum\",\r\n    \"MaxPool\",\r\n    \"Minimum\",\r\n    \"Mul\",\r\n    \"Pack\",\r\n    \"Placeholder\",\r\n    \"Relu\",\r\n    \"Reshape\",\r\n    \"Shape\",\r\n    \"Sigmoid\",\r\n    \"Softmax\",\r\n    \"StridedSlice\",\r\n    \"Sub\",\r\n    \"Tanh\",\r\n    \"Transpose\",\r\n    # https://devtalk.nvidia.com/default/topic/1026657/jetson-tx2/unary-layer-in-tensorrt-3-0-rc/post/5234281/\r\n    # Unary ops don't currently work.\r\n    # \"Negative\",\r\n    # \"Abs\",\r\n    # \"Sqrt\",\r\n    # \"Rsqrt\",\r\n    # \"Pow\",\r\n    # \"Exp\"\r\n    # \"Log\"\r\n    # Ops that use reduce are currently buggy\r\n    # \"Sum\",\r\n    # \"Prod\",\r\n    # \"Min\",\r\n    # \"Max\",\r\n    # \"Mean\",\r\n```", "> But there may be some potential performance overhead between the switch of TensorRT of TF(around several hundred us to 1ms observed in our scenarios)\r\n\r\n@yangjunpro We are interested in integrating TensorRT with TensorFlow. May I know all the elapsed time of your scenarios, or how much percentage of switch overhead in whole elapsed time? Thanks very much!", "@luotao1 @yangjunpro Besides the native integration of TensorRT with TF, we're also working on integrating TensorRT to official models and other places, and \r\n you can get some numbers [here](https://github.com/tensorflow/models/tree/master/research/tensorrt). Just FYI.", "@aaroey Thanks very much!"]}, {"number": 16252, "title": "Branch 182576952", "body": "", "comments": []}, {"number": 16251, "title": "Tensorflow Lite (tf-nightly) toco error for python3", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS High Sierra 10.13.2\r\n- **TensorFlow installed from (source or binary)**: yes\r\n- **TensorFlow version (use command below)**:  1.6.0-dev20180119\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:-\r\n- **GPU model and memory**: - \r\n- **Exact command to reproduce**: toco --help\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI'm following the current tensorflow codelab https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/index.html?index=..%2F..%2Findex#3 however it seems that toco doesn't run under a python 3.6 env according to the output of the terminal\r\n\r\n### Source code / logs\r\ncommand input/output :\r\n\r\n```\r\n\u2514\u2500[$] <git:(master)> toco --help\r\n2018-01-19 22:15:19.449190: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing Curl library\r\nTraceback (most recent call last):\r\n  File \"/Users/macuser/Projects/Tensorflow/bin/toco\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/macuser/Projects/Tensorflow/lib/python3.6/site-packages/tensorflow/contrib/lite/toco/python/toco_wrapper.py\", line 35, in main\r\n    os.execvp(binary, sys.argv)\r\n  File \"/Users/macuser/Projects/Tensorflow/bin/../lib/python3.6/os.py\", line 559, in execvp\r\n    _execvpe(file, args)\r\n  File \"/Users/macuser/Projects/Tensorflow/bin/../lib/python3.6/os.py\", line 583, in _execvpe\r\n    exec_func(file, *argrest)\r\nPermissionError: [Errno 13] Permission denied\r\n\r\n```\r\n\r\nusing sudo also doesn't help. There is also a stackoverflow issue logged here: \r\nhttps://stackoverflow.com/questions/43322964/permission-denied-when-installing-tensorflow\r\n\r\n\r\n", "comments": ["Did you tried `sudo chmod a+rw /path/to/folder/that/you/want/to/write/to\r\n` ?\r\nAlso, the stack overflow link which you pointed out also addresses variety of cases where some files might be locked by anaconda or may be trying specific versions for anaconda and python.\r\n\r\nAlso,please review guidelines before raising an issue !\r\n", "Could you give us the output  of running\r\n```\r\nls /Users/macuser/Projects/Tensorflow/lib/python3.6/site-packages/tensorflow/aux-bin/toco\r\n```\r\nAnd then\r\n```\r\nchmod a+rx /Users/macuser/Projects/Tensorflow/lib/python3.6/site-packages/tensorflow/aux-bin/toco\r\n```\r\nand try again.", "@aselle  Thanks - this worked..\r\n\r\n```\r\nls -al /Users/macuser/Projects/Tensorflow/lib/python3.6/site-packages/tensorflow/aux-bin/toco\r\n-rw-r--r--  1 macuser  staff  1413296 Jan 19 22:15 /Users/macuser/Projects/Tensorflow/lib/python3.6/site-packages/tensorflow/aux-bin/toco\r\n```\r\n\r\ni ran the above permissions command and this worked. \r\n\r\n@printdhruv - thanks - however i'm not using anaconda. Majority of the users from SO were addressing windows issues so was a bit lost there. ", "@pulse1989 I misunderstood the env. as anaconda env. I am glad you found a fix!"]}, {"number": 16250, "title": "Branch 182559534", "body": "", "comments": []}, {"number": 16249, "title": "Branch 182554969", "body": "", "comments": ["@yifeif seems reasonable so I guess we're good "]}, {"number": 16248, "title": "[bug?] Error in `python': malloc(): memory corruption", "body": "Error in `python': malloc(): memory corruption: 0x00000000723f9040\r\n\r\nStrangely encountered this error when the training was going, it happened after a certain number of iterations (~7000 iterations with image batch size of 1 using coco dataset).  I have also attached the memory map that showed up after the backtrace.\r\n\r\n```\r\n*** Error in `python': malloc(): memory corruption: 0x00000000723f9040 ***\r\n======= Backtrace: =========\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f9d5715a7e5]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x8213e)[0x7f9d5716513e]\r\n/lib/x86_64-linux-gnu/libc.so.6(__libc_malloc+0x54)[0x7f9d57167184]\r\n/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x1ec51)[0x7f9d560f0c51]\r\n/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x842c8)[0x7f9d561562c8]\r\n/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x844a4)[0x7f9d561564a4]\r\n/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x84920)[0x7f9d56156920]\r\n/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x85105)[0x7f9d56157105]\r\n/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x11b16d)[0x7f9d561ed16d]\r\n/home/user/tools/../data/coco/PythonAPI/pycocotools/_mask.so(+0x9406)[0x7f9d1f276406]\r\n/home/user/tools/../data/coco/PythonAPI/pycocotools/_mask.so(+0x1c05f)[0x7f9d1f28905f]\r\npython(PyEval_EvalFrameEx+0x615e)[0x4ca15e]\r\npython(PyEval_EvalFrameEx+0x5d8f)[0x4c9d8f]\r\npython(PyEval_EvalCodeEx+0x255)[0x4c2765]\r\npython(PyEval_EvalFrameEx+0x68d1)[0x4ca8d1]\r\npython(PyEval_EvalCodeEx+0x255)[0x4c2765]\r\npython(PyEval_EvalFrameEx+0x68d1)[0x4ca8d1]\r\npython(PyEval_EvalCodeEx+0x255)[0x4c2765]\r\npython(PyEval_EvalFrameEx+0x68d1)[0x4ca8d1]\r\npython(PyEval_EvalCodeEx+0x255)[0x4c2765]\r\npython(PyEval_EvalFrameEx+0x68d1)[0x4ca8d1]\r\npython(PyEval_EvalCodeEx+0x255)[0x4c2765]\r\npython(PyEval_EvalFrameEx+0x6099)[0x4ca099]\r\npython(PyEval_EvalCodeEx+0x255)[0x4c2765]\r\npython(PyEval_EvalCode+0x19)[0x4c2509]\r\npython[0x4f1def]\r\npython(PyRun_FileExFlags+0x82)[0x4ec652]\r\npython(PyRun_SimpleFileExFlags+0x191)[0x4eae31]\r\npython(Py_Main+0x68a)[0x49e14a]\r\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7f9d57103830]\r\npython(_start+0x29)[0x49d9d9]\r\n```\r\n\r\n\r\n```\r\nSystem Information\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\nCompiler:\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\n\r\nPIPs:\r\nmsgpack-numpy (0.4.2)\r\nprotobuf (3.5.0.post1)\r\ntensorflow-gpu (1.4.1)\r\ntensorflow-tensorboard (0.4.0rc3)\r\n\r\nTensorFlow:\r\ntf.VERSION = 1.4.0\r\ntf.GIT_VERSION = v1.4.0-rc1-11-g130a514\r\ntf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514\r\nSanity check: array([1], dtype=int32)\r\n\r\nEnv:\r\nLD_LIBRARY_PATH /usr/lib/x86_64-linux-gnu:/usr/local/lib:/usr/local/cuda/lib64:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\nGPU:\r\nTITAN Xp\r\n\r\nCUDA Lib:\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n```\r\n[memory map.txt](https://github.com/tensorflow/tensorflow/files/1647515/memory.map.txt)\r\n\r\n", "comments": ["Thanks for the report, though I suppose this could be due to causes outside of TensorFlow, such as an error in numpy or python or a transient hardware failure.   I don't think we'll take any action on this unless it's reproducible and very likely due to TF.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Closing due to inactivity."]}, {"number": 16247, "title": "[doc][feature request] Graphical representation of operations", "body": "It would be cool if the documentation of TF operations would contain graphical examples.\r\n\r\nE.g. the \"tf.dynamic_partition\" operation contains such a visualization:\r\n![image](https://user-images.githubusercontent.com/1200058/35163918-880ec02e-fd48-11e7-944f-7d3aac7cadc5.png)\r\n\r\nThis would be especially helpful to understand the different slicing/joining operations.", "comments": ["I agree this would be really cool. Often it is hard to do for higher dimensional tensors. @wolffg, has anyone looked intothis?  It would be especially useful for things like conv as well (though there are sources of those).", "We are always on the hunt for contributions to the API docs!  If you have images you want to contribute, make a PR with the images, and we'll pull them into our image hosting system.\r\n\r\nAgreed that many of these ops (such as space-to-depth and depth-to-space) could use more visuals, plus probably more code.  Restating it so the task has bounds, we'd say that all the ops in this section should have images:\r\n\r\nhttps://www.tensorflow.org/api_guides/python/array_ops#Slicing_and_Joining\r\n\r\nwhich is ~30 ops that don't have images.  I don't think I can close this with a quick change, so I'm setting this to \"contributions welcome\" and we'll track this suggestion internally as part of our ongoing API docs work. ", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you."]}, {"number": 16246, "title": "Failed to build error: mismatched argument pack lenghts...", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 4.14.13-1-ARCH\r\n- **TensorFlow installed from (source or binary)**: git\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: 0.9.0-1\r\n- **GCC/Compiler version (if compiling from source)**: 6.4.1\r\n- **CUDA/cuDNN version**:  9.1.85-1 / 7.0.5-2\r\n- **Exact command to reproduce**:\r\n./configure\r\nbazel build --config=opt --config=cuda --jobs 12 //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n### Describe the problem\r\nfailed to build\r\n\r\n### Source code / logs\r\n\r\n> /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'\r\n>        return __and_<is_convertible<_UElements&&, _Elements>...>::value;\r\n>                                                                  ^~~~~\r\n> /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>}; bool <anonymous> = true; _Elements = {tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece}]' not a return-statement\r\n>      }\r\n>  ^\r\n> ERROR: /home/user/dev/git/tensorflow/tensorflow/core/kernels/BUILD:1884:1: output 'tensorflow/core/kernels/_objs/list_kernels_gpu/tensorflow/core/kernels/list_kernels.cu.pic.o' was not created\r\n> ERROR: /home/user/dev/git/tensorflow/tensorflow/core/kernels/BUILD:1884:1: not all outputs were created or valid\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> Use --verbose_failures to see the command lines of failed build steps.\r\n> INFO: Elapsed time: 29.727s, Critical Path: 28.35s\r\n> FAILED: Build did NOT complete successfully\r\n> ", "comments": ["Seeing the same failure with exactly the same system parameters. Attempted to roll back to commit d75c3474f93b87ccbfeb910550b5dcdb8913681b but still fails.", "Same behaviour with same system parameters.", "Seems to be a common issue with CUDA and some versions of GCC.\r\nhttps://github.com/marian-nmt/marian-dev/pull/151\r\nhttps://github.com/ComputationalRadiationPhysics/alpaka/issues/426\r\nhttps://github.com/pytorch/pytorch/issues/3963\r\nWhich is weird because I managed to compile Tensorflow from source with CUDA 9.1/CUDnn 7.0 support on 2018-01-05.\r\nI tried compiling without CUDA support and Tensorflow does compile.  Next I tried compiling with gcc-5 however this failed with the following error:\r\n> /usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11300): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\nNext I tried to compile with gcc 7.2.1 resulting in the following error:\r\n> #error -- unsupported GNU version! gcc versions later than 6 are not supported!\r\n", "The same error was mentioned in #10220 .\r\n\r\nUnfortunately that's leading to the following trace:\r\n```2018-01-22 19:54:37.586137: E tensorflow/core/common_runtime/direct_session.cc:168] Internal: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1509, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 628, in __init__\r\n    self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.", "nvidia will fix the gcc header bug but fix won't be available till next CUDA release, whenever that might be.\r\nhttps://devtalk.nvidia.com/default/topic/1028112/cuda-setup-and-installation/nvcc-bug-related-to-gcc-6-lt-tuple-gt-header-/post/5234434/#5234434\r\nStill need to find a workaround on arch.  Somehow downgrading gcc-6 might work but can't figure out what version to use.", "As far as I could test, it doesn't compile with gcc 6.4.0. However, it compiles with gcc 6.3.0 (more specifically, the debian version 6.3.0-18).", "I have just managed to compile tensorflow using gcc 6.4.1 with CUDA/CUDnn support (first said it couldn't find CUDnn but kept defaults a second time and then it found it) on Arch.\r\nI guess someone has been fixing things?  I'll close for now as the original issue seems to be fixed.\r\n\r\nNVIDIA: 387.34-21\r\nCUDA: 9.1.85-1\r\nCUDnn: 7.0.5-2\r\ngcc6: 6.4.1-5", "Thanks @pfc!\r\n\r\nWould you be able to provide a fix?\r\n\r\nCC @gunan ", "I have not made any code changes to get it to compile.  I did update gcc from 6.4.1-4 to 6.4.1-5 and pulled latest code from repo.  One or a combination of theses must have fixed the problem."]}, {"number": 16245, "title": "Branch 182511847", "body": "", "comments": ["@gunan there seems a lot of overlap between this and https://github.com/tensorflow/tensorflow/pull/16233\r\n\r\n", "Also I somehow don't have permissions to merge anymore as well. "]}, {"number": 16244, "title": "Benchmarking GPU ops in Tensorflow Graphs", "body": "I tried using the tool for benchmarking Tensorflow Graphs at\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark\r\n\r\nSeems it only gives the memory profile for RAM per ops. How can I get the GPU memory and utilization profile per ops using this tool?\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 16243, "title": "fix typo", "body": "fix typo", "comments": []}, {"number": 16242, "title": "fix typo", "body": "fix typo", "comments": []}, {"number": 16241, "title": "Fix a buildscript error which prevents macro being used by other workspaces", "body": "Converting to label and back to string will prepend current workspace,\r\ne.g: `//tensorflow` -> `@org_tensorflow//tensorflow`. Projects who\r\nintegrate TensorFlow from another Bazel workspace, and use macros like\r\n`tflite_copts()` need the prefix.\r\n\r\nFor more info please take a look at: https://github.com/bazelbuild/bazel/issues/1551", "comments": []}, {"number": 16240, "title": "graph_metrics.py does not work well", "body": "I want to use function in graph_metrics.py, I run corresponding test file graph_metrics_test.py, but I get the following assertion error for \"weight_parameters\" metric\r\n```\r\n\r\nline 32, in testGraphMetrics\r\n    self.assertEqual(expected[statistic_type], current_stats.value)\r\nAssertionError: 100 != None\r\n```\r\nhas anyone encounter this problem?", "comments": ["The only \"graph_metrics_test.py\" I see is here\r\n\r\nhttps://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/python/tools/graph_metrics_test.py\r\n\r\nwhich is not part of officially supported TensorFlow.  It also appears to be old.", "That is right! I find another way to solve my problem! Thanks anyway, I will close this issue"]}, {"number": 16239, "title": "Not supported for GpuManagedAllocator", "body": "GpuManagedAllocator was early supported at tensorflow/core/common_runtime/gpu/gpu_managed_allocator.cc.\r\nBut it seems there is no choice for users to use it according to the source code?\r\nThe GpuManagedAllocator can help to enlarge virtual GPU memory to fit huge training models, so it is very important to support them.\r\nSo what is the plan next about this feature?", "comments": ["Do you have a concrete model that doesn't work well with our allocator that the ManagedAllocator works better on.  The code comments that the speed is worse with managedallocator since it migrates dirty pages. Perhaps that means your model wouldn't work as well with it anyways.\r\n\r\n@zheng-xq, do you have any quick way to enable this mode for general running so @ghostplant could check if that helps their situation.", "@aselle It is really necessary. :)\r\n\r\nI will provide some facts of deep learning models and why current tensorflow memory management is not suitable.\r\n\r\n## 1. Tensorflow model migration problem:\r\n\r\nWhatever MNIST_CNN, Alexnet, Inception, Resnet. etc, all nerual networks have a fixed structure so their GPU memory occupation can be precisely calculated if `batch_size` is given. So users expect a way that Tensorflow can provide something like `per_process_gpu_memory_quota_mb` in place of `per_process_gpu_memory_fraction`, because the latter makes the model migration very inconvenient. We expect a way like this:\r\n\r\n```sh\r\n(Assume MNIST_CNN model costs only 400MB GPU memory)\r\nconfig.gpu_options.per_process_gpu_memory_quota_mb=400\r\n```\r\n\r\nHowever, currently the source code can be only like this:\r\n```sh\r\nconfig.gpu_options.per_process_gpu_memory_fraction=0.2\r\n```\r\nWhen the above code is executed using different GPUs (e.g. 16GB, 1GB), 16GB-GPU can succeed but 1GB-GPU will report BFC out-of-memory which is really annoying...\r\n\r\n\r\n## 2. Huge model (inceptionV4, etc) training on Tensorflow problem:\r\n\r\nVGG_VP and Inception models are known as huge deep learning models, whose convolutional filters are too large so they usually costs extremely-large GPU memory, e.g. at least 8GB memory is required by VGG_VP.\r\n\r\nAssume we have a GPU memory of 2GB, and a host memory of 16GB. Based on the current Tensorflow management, VGG_VP and Inception CANNOT efficiently run on this GPU since Tensorflow only allocate full 2GB GPU memory which is way not enough.  If we use GpuManagedAllocator, it can allow to allocate up to 2GB + 16GB memory in total, so the VGG_VP and Inception models can be well fit in this GPU, and the training performance is very near to all-data-on-GPU-memory (85-95% according to our experiment).\r\n\r\nSo we expect something like:\r\n```sh\r\nconfig.gpu_options.allocator_type='GpuManaged'\r\n```\r\n\r\nIn a word, I think BFC out-of-memory is really a high-frequent problem that reduces the user experience when using Tensorflow to train HUGE model, e.g. we even have no way to train a huge model on a small-memory GPU, and above 2 suggestions are very helpful and supporting GpuManagedAllocator is a must to fix all of them in the first place.\r\n", "If you are interested in supporting this feature, maybe I can help on some of the implementing. :)", "The current managed allocator is only used for internal testing purpose, and not exposed to real models. Adding @chsigg.\r\n\r\nThe down side of using cudaMallocManaged directly is that it forces a device/host synchronization on a fine granularity. That's why TF currently use a BFC suballocator.\r\n\r\nThat being said, it is worth trying to use cudaMallocManaged as the underlying TF allocator.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/4806cb0646bd21f713722bd97c0d0262c575f7e0/tensorflow/stream_executor/cuda/cuda_driver.cc#L934\r\n\r\nWe will have to test it on real models to see if there are a positive effect.", "I think BFC can still be used by default, while it also allows users to select `allocator_type='GpuManaged'`.\r\nBesides, `config.gpu_options.per_process_gpu_memory_quota_mb=8192` is also needed because if your GPU has only 2GB memory, while you want to train an InceptionV3 model which requires 8GB tensor memory, Tensorflow will currently only allocate 2GB by default, so a larger memory allocation is needed which is okay by GpuManagedAllocator.", "I'd like to see some measurements before enabling this feature.\r\n\r\nIn your example, it is unclear whether it is better to ask for 8GB of memory on a 2G GPU and having the Cuda system software swapping around. Or having the model use a smaller batch size. Many models have a batch size lower limit, mainly because of the batchnorm, but it is often much smaller than the regular setting.\r\n\r\nThe alternative is to swap memory explicitly in your model, such as set \"swap_memory=True\" in tf.while_loop. When the framework knows your execution schedule, it has more knowledge to make good choice on when to swap the memory before the execution. ", "@zheng-xq \r\nI did an experiment as follows:\r\n\r\nGPU-Version: Nvidia Tesla-P100, with 16GB GMem\r\n\r\n\r\n==> model-A: Tensorflow-MNIST_CNN -- 28x28x1\r\n1) batch_size=32\r\nGMem-Current-TF: 49.070s (Real GMem Cost: 16000MB)\r\nGMem-Managed-TF: 52.110s (Real GMem Cost: 600MB)\r\n\r\n\r\n==> model-B: Keras-Resnet20 (using TF backend) -- 32x32x3\r\n1) batch_size=32\r\nGMem-Current-TF: 328.38s (Real GMem Cost: 16000MB)\r\nGMem-Managed-TF: 330.676s (Real GMem Cost: 500MB)\r\n\r\n2) batch_size=512\r\nGMem-Current-TF: 1m11.385s (Real GMem Cost: 16000MB)\r\nGMem-Managed-TF: 1m12.837s (Real GMem Cost: 2295MB)\r\n\r\n3) batch_size=4096\r\nGMem-Current-TF: 1m6.825s (Real GMem Cost: 16000MB)\r\nGMem-Managed-TF: 1m9.154s (Real GMem Cost: 13000MB)\r\n\r\n4) batch_size=8192\r\nGMem-Current-TF: FATAL-ERROR. (Cannot support due to Out-of-BFC allocator)\r\nGMem-Managed-TF: 6m59.930s (Real GMem Cost: 16000MB)\r\n\r\n\r\n==> model-C: Keras-VGG16 (using TF backend) -- 224x224x3\r\n1) batch_size=32\r\nGMem-Current-TF: 109.448s (Real GMem Cost: 16000MB)\r\nGMem-Managed-TF: 111.907s (Real GMem Cost: 8000MB)\r\n\r\n2) batch_size=128\r\nGMem-Current-TF: 301.289s (Real GMem Cost: 16000MB)\r\nGMem-Managed-TF: 302.360s (Real GMem Cost: 14000MB)\r\n\r\n3) batch_size=256\r\nGMem-Current-TF: FATAL-ERROR. (Cannot support due to Out-of-BFC allocator)\r\nGMem-Managed-TF: 548.711s (Real GMem Cost: 16000MB)\r\n", "@ghostplant, thanks for the experiments, they look interesting.\r\n\r\nSince users can control batch size, what is really important is whether they can get a higher throughput using more memory.\r\n\r\nWith your model-B Resnet20 result, at batch_size=8192, the throughput is 19.5 images/sec; at batch_size=4096, it is 61.31. So I don't think it is worthwhile to go for more managed memory.\r\n\r\nWith your model-C VGG16, the results is quite interesting: at batch_size=256, you get 0.46 images/sec; while at batch_size=128, you get 0.42 images/sec. So in this case, managed memory helps.\r\n\r\nCould you share the patch for your experiment? I'll ask @chsigg to double check that. If everything looks good, I think this is a valuable contribution. Thanks!", "@zheng-xq \r\n\r\nOn linux, we can just replace its underlay symbol link of cuMemAlloc_v2, which doesn't require to recompile Tensorflow project, but it is just for temporary experiment.\r\n\r\n\r\nFile `main.c`\r\n```sh\r\n/*\r\nCompiling and Run Example:\r\n\r\n1) gcc main.c -o replace.so -I/usr/local/cuda/include -L/usr/local/cuda/lib -shared -fPIC -lcudart -lcuda -ldl\r\n\r\n2) LD_PRELOAD=`pwd`/replace.so python3 mnist_cnn.py   # to run keras examples with replaced API\r\n\r\n3) nvidia-smi | grep -i def   # to see memory cost\r\n*/\r\n\r\n#include <stdio.h>\r\n#include <assert.h>\r\n#include <cuda.h>\r\n#include <cuda_runtime.h>\r\n#include <dlfcn.h>\r\n\r\nCUresult cuMemAlloc_v2(CUdeviceptr *dptr, size_t bytes){\r\n  printf(\"cuMemAlloc_v2 => bytes = %zd\\n\", bytes);\r\n  static void *hd = NULL;\r\n  if (!hd)\r\n    assert((hd = dlopen(\"/usr/local/cuda/lib64/libcudart.so.8.0\", RTLD_LAZY)) != NULL);\r\n  return ((int(*)(void**, size_t, int))dlsym(hd, \"cudaMallocManaged\"))((void**)dptr, bytes, cudaMemAttachGlobal);\r\n}\r\n\r\nCUresult cuMemFree_v2(CUdeviceptr dptr) {\r\n  assert(0);\r\n  return CUDA_SUCCESS;\r\n}\r\n```", "Is `config.gpu_options.per_process_gpu_memory_quota_mb=xxx` going to be supported? I think this will help to describe the basic memory usage of a model and makes the same model to conveniently execute among different kinds of GPU.", "Thanks for the experiments! This looks convincing enough. I'll ask @chsigg to look into this. Thanks!", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @chsigg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @chsigg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is this already supported now by some config.flags?", "Sorry for the delay in the follow-up.\r\n\r\nI ran various experiments with some of the models in tf_cnn_benchmarks to observe model throughput using different batch sizes with cuMemAlloc and cuMemAllocManaged. It seems that cuMemAllocManaged does not really help increase the peak throughput observed across different batch sizes for a model. So, it should be possible to increase throughput for vgg16 model by adjusting the batch size without using batch size of 8192 and managed memory. By the way, the reported runtime for vgg16 model seems far too low for it to be the per-step duration. It should be possible to get much higher throughput for vgg16 using P100. Is that duration for N steps or something else? \r\n\r\nNow going back to the two points you mentioned in your initial comments:\r\n\r\n**Huge model training**: As zheng-xq mentioned earlier, memory requirement for huge models can be decreased by decreasing the batch size. Using higher batch size by using managed memory does not seem to be giving a higher throughput. On the flip side, this can significantly slow down the training. This can be observed in the runtime you reported for Resnet20 model with batch size of 8192 which runs 6x times slower compared to batch size of 4096. I believe the slower model will be much more difficult to debug compared to the BFC out-of-memory failure.\r\n\r\n**Model migration problem**: I believe, it is worth considering the absolute memory limits support along with specifying it as a fraction of the GPU memory. Just to further understand your use-case, could you provide more details about your use-case that will benefit with such an option? For example, do you generally run multiple models concurrently on a single GPU? If that is the case, then the scheduler will still need information about the total memory available in a particular GPU to decide how many models can safely run at the same time. If that is true then it should be possible to get memory requirement as a fraction of GPU memory based on the absolute limit and total memory in the GPU. However, I believe your use-case is different.\r\n\r\nThanks for reporting this and also sharing the experiment results.", "Hi @smit-hinsu , the source code of VGG16 is a keras-based implementation and executed with Tensorflow backend, and all each the time I posted stand for the real time spent to finish 1 epoch training, this is related to the scale of input data so absolute value is meaningless here.\r\n\r\n**Huge model training** I just want a friendly way to get over BFC-overflow problem when using tensorflow, because the overflow is slow, not catch-able by python runtime and it is even hard to kill. Using GpuManagedAlloc is a suitable way to avoid such bad status for huge model.\r\n\r\n**Model migration problem** User sensitive to absolute memory is definitely needed because they want to not only evaluate the resource usage for their defined models, but also know a model is runnable on a certain GPU from a hybrid cluster. However, considering the absolute memory is related to both model itself and also the batch_size, this value might not be a global constant one, so I recently think it would be better to be able to pre-evaluate the memory occupation before sess.run, something like:\r\n\r\n```sh\r\nmemory, .. = sess.evalutate_resources(target, ...)\r\nsess.run(target, ...)\r\n```\r\nThe value of memory can be a map of different devices, e.g:\r\n```sh\r\nmemory = {\r\n    '/gpu:0': 6000000000,\r\n    '/gpu:1': 0\r\n}\r\n```\r\n\r\nThus, user can judge whether their model can fit into an unknown GPU, and also able to adjust a suitable batch_size completed before real session run.\r\n\r\n```sh\r\nfree_memory = cuda.getMemFree(..)\r\nbatch_size = 256\r\nwhile True:\r\n    target = make_model(.., batch_size)\r\n    if sess.evaluate_memory(target) > free_memory:\r\n        batch_size /= 2\r\n    else:\r\n        break\r\n# Safe to execute a model\r\nsess.run(target)\r\n```\r\n", "@smit-hinsu  I'm interested in this feature, too. In my case, I'm working on medical images of very large dimensions. For CT image, it could be 512*512*512 uint16. For pathology image, it could be larger 20000*20000*3 uint8. I can certainly do some cropping and resizing, but there's a limit. Even then, the image is way too big for a model to fit in a gpu. For me, it's the question of whether I can do it at all. I know there will be significant decrease in performance but without this feature, I can only give up on some tasks. \r\n\r\nI'd really appreciate this feature to be activated. I'm wondering if there's any way I can try it?", "@joe-of-all-trades This feature is only supported by NV cards that support unified memory. Typically, Tesla P100 supports this feature, while Tesla K40c and all GTX cards doesn't.", "@joe-of-all-trades So for all normal NV cards, the basic method is still to reduce the batch_size when training models.", "@ghostplant  I'm using p40 so I believe think it should work ?  If I were to try, should I just follow the steps you listed in one of your previous comments that \"we can just replace its underlay symbol link of cuMemAlloc_v2\" ? Can you elaborate in a little more details ?  I don't quite understand how you did it.  ", "@joe-of-all-trades Yes, actually this only works on Linux. The procedure is compile `main.c` to `main.out` and then `export LD_PRELOAD=/where/is/your/main.out`. Finally, execute your tensorflow application within this session.", "Nagging Assignee @smit-hinsu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Update: I am currently working on a change that introduces an option to make use of the unified memory supported by Pascal and newer GPUs. I will update this issue once that is available for use. After that, I will internally propose adding an option to specify memory limit as an absolute limit.\r\n\r\nRegarding the \"BFC-overflow\" problem, that feels like a separate issue to me and is unexpected. Could you help file a different issue describing a way to reproduce the problem? Ideally, high memory usage should fail gracefully regardless of the unified memory support.\r\n\r\nThanks for adding more details about the use-case.", "Sound great! By using unified memory, users should be allowed to specify memory limit larger than GPU device memory.", "The change is currently in review.\r\n\r\nDid you consider using MaxBytesInUse method that reports the peak memory usage of a device to find if a particular model and batch_size pair are runnable on an unknown GPU?\r\nSee https://www.tensorflow.org/api_docs/python/tf/contrib/memory_stats/MaxBytesInUse", "Hi, I am wondering if unified memory may help on inference side..?\r\nI am currently using cpu-inference for object detection since my images are very high resolution, so any help coming by UM would be great.", "@smit-hinsu Is `MaxBytesInUse` to report memory usage when training or before training?\r\nNot until `sess.run(training)` is launched does the engine started to occupy some GPU memory. If the session is already triggered which just results in OOM, it would be too late and useless to know `MaxBytesInUse` then.", "@ontheway16 \r\nI think unified memory will just allow you to do training/inference using large models without caring about GPU OOM. But when the model is huge to completely fit into whole device memory, it is also likely to slow down.\r\nHowever, if not using unified memory, such huge models are even not able to run on TF.", "@smit-hinsu Can you explain how the in-review change is going to support? e.g. Allow users to set fixed memory allocation quota? Allow users to enable `cudaMallocManaged` instead of `cuMemAlloc_v2`?", "@ghostplant I am comparing infererence performance under unified memory vs cpu only conditions, after completion the training in gpu mode. I think there will be some speed gain against cpu only inference.", "Regarding MaxBytesInUse, it would only be useful to find the peak memory usage in case when it does not run out of memory.\r\n\r\nThe in-review change supports use of cuda unified memory. Should be available at HEAD in one or two days.\r\n\r\nIt does not support absolute memory limit and it is still configured using the per_process_gpu_memory_fraction field. It would be possible to set greater than 1.0 value after the change. Absolute memory limit can be converted to the fraction using the BytesLimit method.\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/memory_stats/BytesLimit\r\n\r\nHope this helps!", "Following commit adds support for the unified memory. Sorry for the long delay.\r\nhttps://github.com/tensorflow/tensorflow/commit/b1139814f91c5216eb5ff229ee7e1982e5f4e888\r\n\r\nClosing this issue. Let me know if this helps you or you have any follow-up questions.", "Hi, \r\nI was wondering, in a system with 2 GPUs with NVLINK interconnect (45GB/s, Peer-to-Peer working), `config.gpu_options.per_process_gpu_memory_fraction = 1.5` would use 0.5 of the second GPU instead of the host memory. \r\n\r\nIf not, how can this be done? \r\nThanks for the information. ", "No, it would not automatically use the second GPU memory. \r\n\r\nTake a look at Mesh TensorFlow to see if it supports your use-case. \r\nhttps://github.com/tensorflow/mesh", "Thanks. Mesh-tensorflow can be a solution, but it won't be as a plugin replacement for keras based models. \r\n\r\nIs it possible to share the GPU memory instead of host memory, if they already have UVA capability (at least with some config)? This way, GPU memory will be used first. "]}, {"number": 16238, "title": "//tensorflow/contrib/gan:losses_impl_test fails with AssertionError ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04  s390x\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: v1.4.1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: No GPU\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: bazel test -c opt //tensorflow/contrib/gan:losses_impl_test\r\n\r\n### Describe the problem\r\nOne of the sub-test `test_stable_global_norm_unchanged` fails on s390x with \r\n`AssertionError: 110.709068 != 110.709084 +/- 0.000010`\r\n\r\nSeems like a minor difference, so I tried changing the tolerance slightly as below:\r\n```\r\n-        self.assertNear(gnorm_np, precond_gnorm_np, 1e-5)\r\n+        self.assertNear(gnorm_np, precond_gnorm_np, 2e-5)\r\n```\r\nwith this the test is passing.\r\n\r\nIs it ok to create a PR with this change? Could you please share your thoughts on this.\r\n\r\n### Source code / logs\r\n```\r\n.......................F..................................................................................\r\n======================================================================\r\nFAIL: test_stable_global_norm_unchanged (__main__.CombineAdversarialLossTest)\r\nTest that preconditioning doesn't change global norm value.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/gan/losses_impl_test.runfiles/org_tensorflow/tensorflow/contrib/gan/python/losses/python/losses_impl_test.py\", line 602, in test_stable_global_norm_unchanged\r\n    self.assertNear(gnorm_np, precond_gnorm_np, 1e-5)\r\n  File \"/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/gan/losses_impl_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 879, in assertNear\r\n    if msg is not None else \"\"))\r\nAssertionError: 110.709068 != 110.709084 +/- 0.000010\r\n\r\n----------------------------------------------------------------------\r\nRan 106 tests in 9.119s\r\n\r\nFAILED (failures=1)\r\n```\r\n\r\n", "comments": ["@yifeif: any reason loosening this bound would not be acceptable?", "redirect to @joel-shor who owns tf.contrib.gan. Joel do you mind taking a look? Thanks!"]}, {"number": 16237, "title": "support preconditioner for `conjugated_gradient()` in `linear_equations.py`", "body": "1. support preconditioner for `conjugated_gradient()` in `tensorflow\\tensorflow\\contrib\\solvers\\python\\ops\\linear_equations.py`\r\n2. add identity_operator() in `util.py` as default preconditioner\r\n3. edit unit test files(`util_test.py`, `linear_equations_test.py`) to validate preconditioner", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "I've signed CLA using my Gmail, but this commit was made with another email, both emails in my Github email lists, is that OK? if not , how could I fix it?", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 16236, "title": "Branch 182474037", "body": "", "comments": ["@frankchn feel free to merge once you review it. ", "hmm some how I don't have access to merging this?\r\n\r\n```\r\nThis pull request can be automatically merged by project collaborators\r\nOnly those with write access to this repository can merge pull requests.\r\n```\r\n", "Looks like a lot of changes here are duplicated with #16233 ?\r\nI will close this one.\r\n\r\nWe can investigate access issues tomorrow."]}, {"number": 16235, "title": "Feature Request: Make NDLSTM use state_is_tuple=True", "body": "I have successfully used [NDLSTM](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ndlstm) (specifically lstm2d.separable_lstm) in my own project but whenever I use it, I enocunter this warning: `\"Using a concatenated state is slower and will soon be deprecated. use state_is_tuple=true.\"`\r\n\r\nThe warning is caused by  `ndlstm_base_dynamic` in lstm1d.py. Specifically, this line: `lstm_cell = rnn_cell.BasicLSTMCell(noutput, state_is_tuple=False)`\r\n\r\nI modified the code such that the deprecation warning won't appear:\r\n\r\n```\r\nwith variable_scope.variable_scope(scope, \"SeqLstm\", [inputs]):\r\n    lstm_cell = rnn_cell.BasicLSTMCell(noutput)\r\n    if reverse:\r\n      inputs = array_ops.reverse_v2(inputs, [0])\r\n    outputs, _ = rnn.dynamic_rnn(\r\n        lstm_cell, inputs, time_major=True, dtype=inputs.dtype)\r\n    if reverse:\r\n      outputs = array_ops.reverse_v2(outputs, [0])\r\n    return outputs\r\n```\r\n\r\nBefore I make any pull requests, is there a reason why the `state_is_tuple` argument is set to `False` in the code?", "comments": ["I have also created a few sample uses:\r\n\r\n```\r\nclass MultidimensionalRNNTest(tf.test.TestCase):\r\n    def setUp(self):\r\n        self.num_classes = 26\r\n        self.num_features = 32\r\n        self.time_steps = 64\r\n        self.batch_size = 1 # Can't be dynamic, apparently.\r\n        self.num_channels = 1\r\n        self.num_filters = 16\r\n        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features, self.num_channels])\r\n        self.labels = tf.sparse_placeholder(tf.int32)\r\n\r\n    def test_simple_mdrnn(self):\r\n        net = lstm2d.separable_lstm(self.input_layer, self.num_filters)\r\n\r\n    def test_image_to_sequence(self):\r\n        net = lstm2d.separable_lstm(self.input_layer, self.num_filters)\r\n        net = lstm2d.images_to_sequence(net)\r\n\r\n    def test_convert_to_ctc_dims(self):\r\n        net = lstm2d.separable_lstm(self.input_layer, self.num_filters)\r\n        net = lstm2d.images_to_sequence(net)\r\n\r\n        net = tf.reshape(inputs, [-1, self.num_filters])\r\n\r\n         W = tf.Variable(tf.truncated_normal([self.num_filters,\r\n                                     self.num_classes],\r\n                                    stddev=0.1, dtype=tf.float32), name='W')\r\n         b = tf.Variable(tf.constant(0., dtype=tf.float32, shape=[self.num_classes], name='b'))\r\n\r\n         net = tf.matmul(net, W) + b\r\n         net = tf.reshape(net, [self.batch_size, -1, self.num_classes])\r\n\r\n         net = tf.transpose(net, (1, 0, 2))\r\n\r\n         loss = tf.nn.ctc_loss(inputs=net, labels=self.labels, sequence_length=[2])\r\n\r\n    def test_stack_ndlstms(self):\r\n         net = lstm2d.separable_lstm(self.input_layer, self.num_filters)\r\n         net = lstm2d.separable_lstm(net, self.num_filters)\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.test.main()\r\n```", "@ebrevdo, it seems like the authors of NDLSTM are no longer available, do you have any thoughts on this?", "NDLSTM isn't supported as the authors are gone; unfortunately I have time to neither add features, nor to review external contributions.  @drpngx do you want to look at this one?", "I already made a pull-request: https://github.com/tensorflow/tensorflow/pull/16402", "Merged."]}, {"number": 16234, "title": "The recognized result is not correct when converting the frozen graph to tflite for android device use ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: mac High Sierra\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.5.0\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**:0.9.0\r\n- **GCC/Compiler version (if compiling from source)**:c++/4.2.1\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nThe detailed system information, you can check the url:\r\nhttps://drive.google.com/file/d/19oKikJ0PcGHx9daauub28IYb8J3hA-rw/view?usp=sharing\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nHi, I covert the frozen graph:mobilenet_v1_224 to tflite, and put it in the tflitecamerademo app, but the regonization result is not correct. If I use the tflite file which download from https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip\r\n\r\nThe regonization result is correct, I don't know what steps is not correct when I covert the frozeon graph to tflite file, could you help me to review what steps is the wrong?\r\n\r\nI put the frozen graph, coverting tflite file and the regonized picture in the https://drive.google.com/drive/folders/12h9O2AtcnDuQZXogAdmexRSjxIQVc1Ej?usp=sharing\r\n\r\nThe correct result should be \"malamute\", but I use the my coverting tflite file, the result is \"shower curtain\"\r\n\r\nI use the command to do the covert\r\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco -- '--input_file=/tmp/mobilenet_frozen_graph.pb' '--output_file=/tmp/mobilenet_quant_20180117.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=QUANTIZED_UINT8' '--inference_input_type=QUANTIZED_UINT8' '--input_shapes=1,224,224,3' '--input_arrays=input' '--output_arrays=MobilenetV1/Predictions/Reshape_1' '--mean_values=128' '--std_values=128' '--default_ranges_min=0' '--default_ranges_max=6'\r\n\r\nThanks\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Your frozen graph has not been through quantized training so it loses a fair bit of accuracy if you simply convert it to uint8. ", "Hello andrehentz:\r\nWhat can I do the quantized training? Is there a example to refer?\r\nAnother question is I use the floating training model which download from \"https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_float_2017_11_08.zip\", and I modify the demo code to support the floating model. The regonization result also is wrong, could you also help to check this problem?\r\n\r\nThanks", "Hello, I get the floating version of tflitecamerademo from \"https://github.com/googlecodelabs/tensorflow-for-poets-2\", I found the difference with my modified version is the image data will do the below operation\r\nimgData[0][i][j][0] = ((((val >> 16) & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\nimgData[0][i][j][1] = ((((val >> 8) & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\nimgData[0][i][j][2] = ((((val) & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\n\r\nBut I don't know why the RGB value need to minus IMAGE_MEAN(128) and divide IMAGE_STD(128) before assign, if I don't do this, the classfication result will not correct.\r\n\r\nCould you help me to explain this concept?\r\n\r\nThanks", "This is a general requirement for image models, they need their input to be converted from pixel's RGB values that range from 0 to 255 integers, to -1.0 to 1.0 floats. The way this is done is by subtracting 128 and dividing by 128. Closing this as working as designed, please reopen with more information if you think this is incorrect."]}, {"number": 16233, "title": "Branch 182456096", "body": "", "comments": []}]