[{"number": 37399, "title": "InvalidArgumentError: Cannot assign a device for operation conv2d_1/kernel/IsInitialized/VarIsInitializedOp: node conv2d_1/kernel/IsInitialized/VarIsInitializedOp (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748)  was explicitly assigned to /job:worker/replica:0/task:0/device:TPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0 ]. Make sure the device specification refers to a valid device. \t [[conv2d_1/kernel/IsInitialized/VarIsInitializedOp]]", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@raniaCht \r\n\r\nIt looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue.\r\nplease share tensorflow version,code to replicate your issue.\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@raniaCht \r\nplease update as per the above comment", "@raniaCht\r\nAutomatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37399\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37399\">No</a>\n", "I'm sorry, I didn't see your comments, and for the example I gave it\nup,thank you very much for your help\n\nLe lun. 16 mars 2020 \u00e0 10:05, Saduf2019 <notifications@github.com> a \u00e9crit :\n\n> @raniaCht <https://github.com/raniaCht>\n> please as per the above comment\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/37399#issuecomment-599421767>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJXBKMMONIZ4YC2GH4DZ3BTRHXTVBANCNFSM4LDGRS7Q>\n> .\n>\n"]}, {"number": 37398, "title": "added workaround for issue #37365", "body": "Created a function \"remove_bias\" to remove bias present in Conv2D layers in ResNet architectures in keras.applications, using existing weights.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37398) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37398) for more info**.\n\n<!-- ok -->"]}, {"number": 37397, "title": "tf.ragged.map_flat_values with tf.argmax on ragged axis produces wrong operation", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or\r\nbinary): Pip\r\n - TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: Python 3.7.4\r\n- Bazel\r\nversion (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from\r\nsource): N/A\r\n- CUDA/cuDNN version: V10.1.243\r\n- GPU model and memory: GeForce RTX 2080 8G\r\n\r\n**Describe the current behavior**\r\nUsing a RaggedTensor flat mapped into tf.argmax and the flattened ragged dimension as the axis to argmax against, the wrong shape/operation results.  [batch, None, N] using map_flat_values(tf.argmax, <tensor>, -2) results in a [batch, None] tensor.\r\n\r\n**Describe the expected behavior**\r\nI'd expect to result in a [batch, N] tensor.\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\nimport tensorflow as tf\r\nfoo = tf.ragged.constant([[[0,1],[2,3]],[[4,5]],[[6,7]],[[8,9]]], dtype=tf.int64, ragged_rank=1)\r\n\r\n# Mapped into the ragged axis (should produce a [4, 2])\r\nbar = tf.ragged.map_flat_values(tf.argmax, foo, -2)\r\nbar\r\n<tf.RaggedTensor [[4, 4], [], [], []]>\r\nbar.shape\r\nTensorShape([4, None])\r\n\r\n# Mapped into the non-ragged axis\r\nbar = tf.ragged.map_flat_values(tf.argmax, foo, -1)\r\nbar\r\n<tf.RaggedTensor [[1, 1], [1], [1], [1]]>\r\n# Notice the same shape...\r\nbar.shape\r\nTensorShape([4, None])\r\n```\r\nWhat I expect is something like this:\r\n```\r\nimport tensorflow as tf\r\nfoo = tf.ragged.constant([[[0,1],[2,3]],[[4,5]],[[6,7]],[[8,9]]], dtype=tf.int64, ragged_rank=1)\r\nrow_splits = foo.row_splits\r\nold_row=row_splits[0]\r\nbar = []\r\nfor row in row_splits[1:]:\r\n    bar.append(tf.argmax(foo.values[old_row:row, ...],-2))\r\n    old_row=row\r\nbar = tf.stack(bar)\r\nbar\r\n<tf.Tensor: shape=(4, 2), dtype=int64, numpy=\r\narray([[1, 1],\r\n       [0, 0],\r\n       [0, 0],\r\n       [0, 0]])>\r\nbar.shape\r\nTensorShape([4, 2])\r\n```\r\n", "comments": ["I have tried on colab with TF version 2.1.0 beta,2.2.0-dev20200308 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/1344668f02641b79a34e585d3e1b7f2c/untitled713.ipynb). Thanks!", "I'm also hitting this bug in TF 2.2. Are there updates on this?\r\n\r\nSome temporary workaround for argmax in ragged tensors that does not involve manually looping the ragged dimension or relying on eager mode would be much appreciated.", "I'm using tf.argmax(ragged_tensor.to_tensor(np.NINF), axis) as an alternative for now.", "`tf.ragged.map_flat_values(tf.argmax, foo, -2)` **should** raise an exception, since you're constructing an invalid RaggedTensor.  In particular, this is equivalent to the following, which *does* raise an exception:\r\n\r\n```\r\n>>>foo.with_flat_values(tf.argmax(foo.flat_values, -2))\r\nValueError: Shapes (5,) and (2,) are incompatible\r\n```\r\n\r\nI'll look into adding a shape check that will make map_flat_values fail here.\r\n\r\nIn general, `map_flat_values` should only be used on operations that operate independently on rows of a ragged tensor -- in particular, the number of rows that the function returns needs to match the number of rows in the original version.  This is trivially true for elementwise operations.  Operations that do aggregation within rows are ok too.  But operations that do aggregation ***across*** rows are not.  In your case, `foo.flat_values` is a 2D tensor, so doing argmax with axis=-2 reduces across those values.  I.e., this is what's effectively happening in your code:\r\n\r\n```\r\nnew_values = tf.argmax(foo.flat_values, -2)  # = [4, 4]\r\nbar = tf.RaggedTensor.from_row_lengths(new_values, foo.row_lengths())\r\n```\r\n\r\nThis is invalid, since the row_lengths indicates that we should have 5 values, but we only have 2.\r\n\r\nYour work-around of using to_tensor seems reasonable.  I think it would also be possible to put together a solution based on `tf.map_fn`, but it would probably be more complex and slower.", "Closing this issue, since the behavior is expected.  I will add a static shape check that should give a better error message.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37397\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37397\">No</a>\n"]}, {"number": 37396, "title": "Update api_def_Acos.pbtxt", "body": "I added description for this api_def function", "comments": ["Still needs examples", "@mihaimaruseac now ?", "This has been rolled back as it breaks several tests. Probably we shouldn't have added Python code in the global API definitions. Let's see how #37396, #37604 and #37605 progress and then we can do another try."]}, {"number": 37395, "title": "Puts a ckeck on kernel_size=0 and raises an error if kernel_size=0 in conv2D", "body": "fixes issue #37334", "comments": ["I suggest adding check in op kernel instead of python client.", "Ok I am working on that\r\n", "@fsx950223 Can I know why are you suggesting to add the check in the op kernel?", "@ghosalsattam Other clients will face same problem.", "@fsx950223 Am I supposed to change the python/ops/nn_ops.py file?", "> @fsx950223 Am I supposed to change the python/ops/nn_ops.py file?\r\n\r\nSorry, I'm not a Tensorflow mentor. You should ask details from Googlers.", "@tanzhenyu Please suggest the changes that should be made.", "Also can you make a unit test, and remove the yaml?", "@tanzhenyu  I have tried to make the changes you suggested. Please check. ", "@tanzhenyu please suggest any more changes/bugs regarding the PR.", "@tanzhenyu I have done all the changes you suggested.", "@tanzhenyu please give a review on the changes made.", "Done!!!\n\nOn Thu, 19 Mar 2020 at 19:21, tanzhenyu <notifications@github.com> wrote:\n\n> *@tanzhenyu* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/keras/layers/convolutional.py\n> <https://github.com/tensorflow/tensorflow/pull/37395#discussion_r395039479>\n> :\n>\n> > +          activity_regularizer=regularizers.get(activity_regularizer),\n> +          kernel_constraint=constraints.get(kernel_constraint),\n> +          bias_constraint=constraints.get(bias_constraint),\n> +          **kwargs)\n> +\n> +\n> +\n> +  def checkProperKernel(self,kernel_size):\n> +        if(isinstance(kernel_size,tuple)):\n> +    \t    for i in kernel_size:\n> +    \t\t    if(i==0):\n> +    \t\t\t    raise ValueError(\"Kernel dimension cannot be zero\")\n> +        else:\n> +    \t    if(kernel_size==0):\n> +    \t\t    raise ValueError(\"Kernel size cannot be zero\")\n> +        return 0\n>\n> this is a very weird return value....the previous implementation looks\n> mostly good, simply just check without returning anything\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/37395#pullrequestreview-377737064>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJMBF2AZZETKGSPC5L6LC2LRIIPMZANCNFSM4LDC2VVA>\n> .\n>\n", "I had implemented the function on base class Conv and ran the unit test\nwith different parameter values. It was running correctly(No indentation\nerror was detected). For all the conv layers, I checked both kernel_size=0\nand kernel_size non-zero. It was working as expected(raising error when\nkernel_size=0 was encountered).\n\nOn Fri, 20 Mar 2020 at 06:14, tanzhenyu <notifications@github.com> wrote:\n\n> *@tanzhenyu* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/keras/layers/convolutional.py\n> <https://github.com/tensorflow/tensorflow/pull/37395#discussion_r395393899>\n> :\n>\n> > -        kernel_size=kernel_size,\n> -        strides=strides,\n> -        padding=padding,\n> -        data_format=data_format,\n> -        dilation_rate=dilation_rate,\n> -        activation=activations.get(activation),\n> -        use_bias=use_bias,\n> -        kernel_initializer=initializers.get(kernel_initializer),\n> -        bias_initializer=initializers.get(bias_initializer),\n> -        kernel_regularizer=regularizers.get(kernel_regularizer),\n> -        bias_regularizer=regularizers.get(bias_regularizer),\n> -        activity_regularizer=regularizers.get(activity_regularizer),\n> -        kernel_constraint=constraints.get(kernel_constraint),\n> -        bias_constraint=constraints.get(bias_constraint),\n> -        **kwargs)\n> +          rank=1,\n>\n> this indent doesn't seem correct.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/37395#pullrequestreview-378182874>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJMBF2HOSKUEQTB52X345N3RIK35XANCNFSM4LDC2VVA>\n> .\n>\n", "@tanzhenyu is the problem solved?", "@tanzhenyu Please give a review on the changes.", "@tanzhenyu @rthadur ,Please provide a review.", "Closing this since it has been fixed [here](https://github.com/tensorflow/tensorflow/commit/1e102f63964365d82d7f22402b7ba21e0e0e64fe)."]}, {"number": 37394, "title": "Fix a bug which may cause segmentation fault", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37394) for more info**.\n\n<!-- need_sender_cla -->", "@PeterRK please CLA and add a test case if possible.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37394) for more info**.\n\n<!-- need_author_cla -->", "@PeterRK you have to sign with the same email you make the commits"]}, {"number": 37393, "title": "tf.image.ssim_multiscale broke in tensorflow 2.1.0-rc2", "body": "**System information** \r\nPython 3.7.6 on Windows 10, x64. \r\n\r\nUsing tensorflow 2.1.0-rc2.\r\n\r\nGPU Hardware: pciBusID: 0000:01:00.0 name: TITAN X (Pascal) computeCapability: 6.1\r\ncoreClock: 1.531GHz coreCount: 28 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 447.48GiB/s\r\n\r\n**Describe the current behavior**\r\n\r\nCode should print the word 'done'\r\n\r\n**Describe the expected behavior**\r\n\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\nimport tensorflow as tf\r\ntf.test.gpu_device_name()\r\nprint(tf.__version__)\r\n\r\n# Build model\r\nimg_input = tf.keras.layers.Input(shape=(128, 128, 1))\r\nimg_output = tf.keras.layers.Convolution2D(1, 1)(img_input) \r\nmodel = tf.keras.models.Model(img_input, img_output)\r\n\r\n# Add reconstruction loss \r\n# Toggle between the next 2 lines of code to see that ssim_multiscale does not work but simple MSE does.\r\nloss = tf.reduce_mean(tf.image.ssim_multiscale(img_input, img_output, 1.0))  # This loss does not\r\n#loss = tf.reduce_mean((img_input - img_output)**2)  # This loss works\r\nmodel.add_loss(loss)\r\n\r\nmodel.compile(optimizer = tf.keras.optimizers.RMSprop(lr=1e-4), loss = None)  \r\nmodel.summary()\r\n\r\n# The error Iget when using the ssim_multiscale loss is:\r\n#tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n    \r\nprint('done')\r\n```\r\n\r\n**Other info / logs** \r\n\r\nThis problem is present in 1.15.0 and 2.1.0.  This bug is not present in in 1.13.1.\r\n\r\nI have tried several image metrics in tf.image including ssim and psnr and they all result in the same error.\r\n", "comments": ["Was able to reproduce the issue with TF 2.1 and TF-nightly. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/a27e55132ee756c965af0069155a326d/37393.ipynb). Thanks!", "@amahendrakar Thank you.  @ymodak Please let me know what you need form my end to help!", "I was able to verify using the gist that psnr is also affected.", "Can you test with 2.2.0-rc0?", "Sure can. It results in the same error.  See the stack trace below.\r\n\r\n```\r\n2.2.0-rc0\r\n\r\n---------------------------------------------------------------------------\r\n\r\nOperatorNotAllowedInGraphError            Traceback (most recent call last)\r\n\r\n<ipython-input-6-a09a88deacaf> in <module>()\r\n     10 # Add reconstruction loss\r\n     11 # Toggle between the next 2 lines of code to see that ssim_multiscale does not work but simple MSE does.\r\n---> 12 loss = tf.reduce_mean(tf.image.ssim_multiscale(img_input, img_output, 1.0))  # This loss does not\r\n     13 #loss = tf.reduce_mean((img_input - img_output)**2)  # This loss works\r\n     14 model.add_loss(loss)\r\n\r\n6 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _disallow_in_graph_mode(self, task)\r\n    535     raise errors.OperatorNotAllowedInGraphError(\r\n    536         \"{} is not allowed in Graph execution. Use Eager execution or decorate\"\r\n--> 537         \" this function with @tf.function.\".format(task))\r\n    538 \r\n    539   def _disallow_bool_casting(self):\r\n\r\nOperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n```\r\n\r\n", "The gist is here: https://colab.research.google.com/gist/amahendrakar/a27e55132ee756c965af0069155a326d/37393.ipynb#scrollTo=WvVY2Xh_omcU\r\n\r\nI changed the first line to !pip install tensorflow==2.2.0-rc0", "@mihaimaruseac Should I try with 2.2.rc1?", "Let's try, just in case a cherry-pick fixed the issue. But I don't recall any work being done to fix this in the past 2 days", "@mihaimaruseac Just tried with 2.2rc1 but I still get the same error. Same for psnr() function also.", "@mihaimaruseac  I think I narrowed the problem down.  It appears to be coming from a sanity check on the vector.  I think if this can be cleaned up for tensorflow 2.x, everything should work.  ", "@mihaimaruseac Can you confirm about the my post on the vector check? ", "@mihaimaruseac   I'm completely stuck and cant figure this out. I am just going to use someone else's ssim function until this gets fixed.  ", "@isaacgerg Sorry, I was not able to reproduce due to having too much on my plate these few days. I'll raise this issue internally", "I see you say you narrowed the problem down. Do you want to make a PR please?", "See also #28274", "@mihaimaruseac I thought I narrowed down the problem but now I'm stuck and can't get any further.  I looked at 28274 and as far as I can tell, this is a different issue than that.  If you can have someone from the tf team look at this, I think they can likely fix it quick.  Don't forget that if you fix msssim, you can apply the same patch to psnr also.", "@isaacgerg whats the fix for this? do you have a link to the PR or commit hash?", "@perfinion I don't know how to fix it.  That's why I posted the issue on github.", "@mihaimaruseac Is there any update on when this will be fixed? Thank you.", "@tomerk Any progress on this issue?", "Hi @isaacgerg , we've been working on a rework of the implementation of Keras's functional api construction. As one side effect fixes several categories of issues with automatically turning non-keras tf api symbols into keras lambda layers. (I believe it should fix tf.image.ssim_multiscale)\r\n\r\nThis implementation rework is targeted for **TF 2.4**, not TF 2.3\r\nThat said you can start experimenting with it in the tf-nightlies by doing:\r\n\r\n```\r\nfrom tensorflow.python.keras.engine import keras_tensor\r\nkeras_tensor.enable_keras_tensors()\r\n```\r\nKeep in mind this is still experimental and not exposed in TF's API. It's a fully internal flag subject to change. But, feel free to start reporting issues you run into with it.\r\n\r\n---------------------\r\n\r\nIf you need a stable workaround ASAP that works already, you should be able to just put `tf.image.ssim_multiscale` in a Keras Lambda layer or custom layer. Then call that layer instead of calling the `ssim_multiscale` directly.", "Thanks for the quick reply @tomerk.  This is good news!  I will try working with the nightlies.  Any issues I find, I'll try to help debug.  Thank you again.", "Could someone elaborate on what @tomerk meant by \"If you need a stable workaround ASAP that works already, you should be able to just put tf.image.ssim_multiscale in a Keras Lambda layer or custom layer. Then call that layer instead of calling the ssim_multiscale directly.\"\r\n\r\nI am not experienced in tensorflow and keras.", "@Sajal-1 You can make & use custom layers in Keras, e.g. following this guide: https://www.tensorflow.org/guide/keras/custom_layers_and_models\r\n\r\nOr use an arbitrary python lambda as a layer in Keras using this API:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda?hl=en", "This issue should now be fixed in the nightlies, as we've enabled the Functional API refactoring I mentioned above.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37393\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37393\">No</a>\n", "@tomerk   Many thanks for fixing this issue.  This fixes quite a bit of other bugs I had in my tensorflow codes!", "Glad to hear it, thanks @isaacgerg! Please don't hesitate to report any other such issues you run into."]}, {"number": 37392, "title": "Checking if Kernel_size=0 in conv2d and reports error accordingly.", "body": "Fixes #37334.", "comments": []}, {"number": 37391, "title": "NFC - minor spelling tweaks under lite/python directory", "body": "This PR addresses minor spelling tweaks under `tensorflow/lite/python` directory.\r\nfollow-on of #35286", "comments": []}, {"number": 37390, "title": "More elaborate logging implementation (implementing TFLogSink s, etc.)", "body": "**System information**\r\n- TensorFlow version: master\r\n- Are you willing to contribute it: Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, logging seems to directly write to `stderr`, if the log messages level fits the current log level.\r\nhttps://github.com/tensorflow/tensorflow/blob/efff893c709b585da9771cb3a2997321efef36ae/tensorflow/core/platform/default/logging.cc#L259\r\nThis is a pity, since the message is lost for other code (I'm currently going thru some hoops to fetch them from `stderr` but the solution is problematic), and in particular, it does not adhere to unified logging approaches, with Python users probably expecting their log message to end up in Python's logging infrastructure. Furthermore, many people seem to search for ways to suppress them, and setting the log level via env vars seems to be not the least confusing approach for many (cf. https://github.com/tensorflow/tensorflow/issues/1258 https://github.com/tensorflow/tensorflow/issues/566).\r\n\r\nTaking a look at the source, it seems that conceptionally, more logging infrastructure is planned, but  has not yet been implemented:\r\nhttps://github.com/tensorflow/tensorflow/blob/47df0000dc3af299cef581811c2ad58f3eda700b/tensorflow/core/platform/default/logging.cc#L40\r\n\r\nTherefore the steps seem to be: Implement and use the `TFLogSink` infrastructure, by default adding a logger writing to `stderr`. Implementing a `TFLogSink` which can pass log messages to Python, loaded and added by the Python code, replacing the `stderr` logging. Adding API to to enumerate current log sinks.\r\n\r\nIs anyone working on this? In some comments it seems that Google-internal TF might use more elaborate logging infrastructure, is any of this due to be added to open source TF? What are the opinions of the TF team on the issue?\r\n\r\n**Will this change the current api? How?**\r\nNo. It will add additional possibilities, but not break existing code.\r\n**Who will benefit with this feature?**\r\nPeople who want finer grained control of logging.\r\n**Any Other info.**\r\n\u2013", "comments": ["We're not actively working on this right now but would love it if you could contribute a PR here. Will be more than happy to review / support.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I'm currently somewhat busy with other projects, but I'll try to take a look in this. (Which shouldn't prevent anyone from doing the same!) I'm not very familiar with TensorFlow's inner C++ structure hence I'll need to do some exploring first. I think this issue should definitely be left open so others can track the progress.", "Solved by #41733"]}, {"number": 37389, "title": "NFC - minor spelling tweaks under lite/testing directory", "body": "This PR addresses minor spelling tweaks under `tensorflow/lite/testing` directory.\r\nfollow-on of #35286", "comments": ["cc @mihaimaruseac @gbaned"]}, {"number": 37388, "title": "NFC - minor spelling tweaks under lite/lib_package directory", "body": "This PR addresses minor spelling tweaks under `tensorflow/lite/lib_package` directory.\r\nfollow-on of #35286", "comments": ["cc @mihaimaruseac @gbaned", "Thank you"]}, {"number": 37387, "title": "NFC - minor spelling tweaks under lite/g3doc directory ", "body": "This PR addresses minor spelling tweaks under 'tensorflow/lite/g3doc' directory.\r\nfollow-on of #35286", "comments": ["cc @mihaimaruseac @gbaned ", "Thank you"]}, {"number": 37386, "title": "NFC - minor spelling tweaks under lite/c directory", "body": "This PR addresses minor spelling tweaks under 'tensorflow/lite/c' directory.\r\nfollow-on of #35286", "comments": ["cc @mihaimaruseac @gbaned "]}, {"number": 37385, "title": "Fix for check_pydot when Graphviz is not installed", "body": "Currently, if `pydot` is installed, but `Graphviz` is not, `check_pydot` will raise a `pydot.InvocationException` instead of returning False. This PR should fix this issue.\r\n\r\nShould probably add a test to confirm the issue, but I wasn't entirely sure how best to go about it, so holding off on it for now.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37385) for more info**.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37385) for more info**.\n\n<!-- ok -->", "Dug into `pydot`a bit and found this: https://github.com/pydot/pydot-ng/blob/8817b2141008c4b74507efb1b7711da7caa77684/test/test_pydot.py#L197-L204\r\n\r\nMight want to do a test based on the patching pattern found there to confirm this."]}, {"number": 37384, "title": "Add Python 3.8 to classifiers", "body": "According to https://github.com/tensorflow/tensorflow/issues/33374#issuecomment-595341808 nightly and TF 2.2 support Python 3.8.", "comments": ["> If we're doing this, let's also remove Py2 and Py3.4 from here\r\n\r\nDone"]}, {"number": 37383, "title": "[tflite] NNAPI Pow op allows scalar input", "body": "resolve the `NN API returned error ANEURALNETWORKS_BAD_DATA`  problem in https://github.com/tensorflow/tensorflow/issues/36645", "comments": []}, {"number": 37382, "title": "Inconsistent device placement when using nondifferentiable_batch_function and GPU", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): colab.research.google.com\r\n- TensorFlow installed from (source or\r\nbinary): - 2.1.0 (latest)\r\n- Python version: Google colab python 3\r\n- CUDA/cuDNN version: Google colab GPU\r\n\r\nI'm trying to test the possibility of using `nondifferentiable_batch_function` for inference. Specifically, I need to collect batches and execute them on GPU for better throughput. I'm aware that there are some other approaches (e.g. using tensorflow serving batching) but using a batching function within a graph could be beneficial for my current setup. While experimenting with different approaches I found out that device placement for operations wrapped by batch function could be highly inconsistent. I have managed to execute operations on GPU by wrapping concrete function into batch function and then save the model but other approaches didn't work:\r\n```\r\nsignature = (tf.TensorSpec(shape=[None, 3], dtype=tf.float32, name=\"x\"),)\r\n\r\n@tf.function(input_signature=signature)\r\ndef run(x):\r\n  with tf.device(\"/GPU:0\"):\r\n        return tf.nn.relu(tf.matmul(x, tf.random.uniform((3, 1))))\r\n\r\nconcrete_run = run.get_concrete_function()\r\n\r\n@tf.nondifferentiable_batch_function(1, 6, 10)\r\ndef batch_run(x):\r\n    return concrete_run(x)\r\n\r\nclass Model(tf.Module):\r\n    @tf.function(input_signature=signature)\r\n    def run(self, x):\r\n        return batch_run(x)\r\n\r\nmodel = Model()\r\nx = tf.random.uniform((3, 3))\r\n\r\n# MatMul and Relu are executed on CPU\r\nmodel.run(x)\r\n\r\nconcrete_model_run = model.run.get_concrete_function()\r\n\r\n# MatMul and Relu are executed on CPU\r\nconcrete_model_run(x)\r\n\r\ntf.saved_model.save(model, \"model\", signatures={ \"run\": model.run })\r\nmodel = tf.saved_model.load(\"model\")\r\n\r\n# MatMul and Relu are executed on GPU\r\nmodel.signatures[\"run\"](x=x)\r\n```\r\nPlease find full test there https://colab.research.google.com/drive/1ZcIbnixdcQ2OSabEAlcf5SYYdkoRzYtu\r\n\r\nI expected that the use of nondifferentiable_batch_function would be consistent despite the approach. A particular problem that I face right now is that saved model (with GPU placed ops) when loaded by tensorflow serving still executes operations on CPU and I believe that issues are connected.\r\n", "comments": ["@dtim1985 \r\nI have run the code shared by you but face a different error, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/670869059455d9c71e903810064ac2fe/37382.ipynb)", "@Saduf2019 I clarified tf version number 2.X -> 2.1.0 - which is the current colab latest version. \r\nI don't have any errors even with tf-nightly when running the gist you privded and error itself `load() missing 2 required positional arguments: 'tags' and 'export_dir'` looks pretty odd as there is only one required parameter - and it is `export_dir`.\r\nUnfortunately, I can't confirm the original issue for tf-nightly as tensorboard profiler does not work there and I can't easily check whether code is executed on GPU.", "@rohan100jain cold you help triage this?", "@damirtm \r\nCould you please try on the latest version and let us know if this is still an issue.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37382\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37382\">No</a>\n"]}, {"number": 37381, "title": "xla_lhlo.neg and xla_lhlo.abs support integer arguments", "body": "Do self transformation based on current mlir standard dialect \r\n", "comments": ["@joker-eph  plz take a look ^_^ ", "@qqsun8819  Can you please check joker-eph's comments and resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'll update this PR according to what @joker-eph comment", "@joker-eph please take a look again ^_^", "@joker-eph pr updated according to you comments, take a look again ^_^"]}, {"number": 37380, "title": "Model customization in iOS", "body": "First of all, thank you all for providing great example for model customization in Android https://github.com/tensorflow/examples/tree/master/lite/examples/model_personalization\r\nIt works great and I want to use it right away in my company! \r\nI have a question about example or support for iOS. Should I hold my breath that it will be available in near future or should I look for some workarounds?\r\n", "comments": ["@PawelFaron Is this still an issue ?\r\nPlease refer [this](https://www.tensorflow.org/lite/guide/ios) link and let us know if it helps?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37380\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37380\">No</a>\n"]}, {"number": 37379, "title": "matplotlib.mlab has been removed, update code please", "body": "## URL(s) with the issue:\r\n\r\na link to the documentation entry\r\nhttps://tensorflow.google.cn/tutorials/estimator/boosted_trees_model_understanding\r\n\r\n## Description of issue (what needs changing):\r\nA bug I found in \"TensorFlow > Learn > TensorFlow Core > Tutorials > Gradient Boosted Trees: Model understanding\" as below:\r\n\r\nmatplotlib.mlab has been removed @ version 3.1.0 \"https://matplotlib.org/3.1.0/api/api_changes.html\"\r\nbut the tutorials still uses _griddata_ class for plotting, i can not get the result follow this.\r\n\r\nso would you mind adjust this for correct code? i am a beginner for tensorflow and python, so fix it by myself is difficult for me ,thanks \r\n\r\n", "comments": ["@dynamor2019 Thanks for filing this issue. Yes, we need to update the tutorial. We need to use `griddata` from `scipy`. So need to import like this `from scipy.interpolate import griddata`. Thanks!", "Thanks for the report. That file lives over here in the tensorflow/docs repo: https://github.com/tensorflow/docs/blob/master/site/en/tutorials/estimator/boosted_trees_model_understanding.ipynb\r\nPlease send a pull request \ud83d\ude00", "+ @crawles", "Can I make a PR for this Issue ? @lamberta  @MarkDaoust ", "I am closing the issue as the related PR got merged. Please feel free to reopen if this was not resolved. Thanks!"]}, {"number": 37378, "title": "Add \"AddV2\" to tensorflow v1 flops calculation", "body": "", "comments": ["Why? It's strange to add a V2 operation to a V1 collection", "Because addv2 is not involved in flops profile. IDK if there is a v2 collection or not. But if there is, I did not find it.", "Is there a usecase where this is needed? What are you trying to do? I'm trying to understand why this needs to be done, and it seems we have an [XY problem](http://xyproblem.info/) here", "# A problem I encountered:\r\n\r\nWhen a serialized protobuf neural network (frozen pb, saved model) contains \"AddV2\" instead of \"Add\" operator, the flops of this operator (\"AddV2\") obtained from using `tf.profiler.ProfileOptionBuilder.float_operation` shows 0.\r\n\r\n# What I want:\r\n\r\nI want the flops from \"AddV2\" operator being counted when using `tf.profiler`.\r\n\r\n# What I found:\r\n\r\n\"AddV2\" is not registered in flops calculation.\r\n\r\n# What I did:\r\n\r\nRegister \"AddV2\" to flops calculation. ", "This breaks internal tests and has been automatically rolled back.", "Are there any methods to check which test is broken?"]}, {"number": 37377, "title": "[XLA] follow-up on GPU-deterministic reductions", "body": "GPU-deterministic `tf.nn.bias_add` (enabled with `TF_DETERMINISTIC_OPS`) and its testing was introduced via [PR 31465](https://github.com/tensorflow/tensorflow/pull/31465). Operation on XLA:GPU was found to be non-deterministic at that time.\r\n\r\nIn response to [a conversation](https://github.com/tensorflow/tensorflow/pull/34887#discussion_r356259683) on [PR 34887](https://github.com/tensorflow/tensorflow/pull/34887) (Add info about `TF_DETERMINISTIC_OPS` to version 2.1 release notes), @cheshire committed a [change](https://github.com/tensorflow/tensorflow/commit/e31955d9fb34ae7273354dc2347ba99eea8c5280) that implemented deterministic reduction functionality when using XLA:GPU. He then committed [another change](https://github.com/tensorflow/tensorflow/commit/8b7a3db0b6e09415b5640be4986fb4d7c6e5209a) that caused this functionality to be enabled by `TF_DETERMINISTIC_OPS`. Both of these changes are in the `r2.2` branch.\r\n\r\nThis current pull-request is a [follow-up](https://github.com/tensorflow/tensorflow/pull/34887#discussion_r382293594) to that conversation. It does two things:\r\n\r\n1. Enable the deterministic testing for `tf.nn.bias_add` to run on the XLA:GPU.\r\n2. Modify the way that `tensorflow/compiler/xla/service/gpu/gpu_compiler.cc` listens to `TF_DETERMINISTIC_OPS` so that it's the same as all other uses (it caches the value).\r\n\r\nIt would be ideal to cherry-pick these changes into the `r2.2` branch so that they can accompany the rest of this feature implementation.", "comments": []}, {"number": 37376, "title": "Hexagon Delegate not working with quantized EfficientNet Lite0", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Intrinsyc SD820 dev board\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 1.2.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nThe Hexagon DSP Delegate is working well on some Google hosted quantized TFLite models (e.g. MobileNets), using the benchmark_model CLI.  However, when using benchmark_model on the newly released EfficientNet Lite models from the URL below, the DSP Delegate fails to engage (0 nodes delegated):\r\n\r\nhttps://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/lite/README.md\r\n\r\nExample benchmark run:\r\n```\r\n$ adb shell benchmark_model \\\r\n   --use_hexagon=true \\\r\n   --input_layer=images \\\r\n   --input_layer_shape=1,224,224,3 \\\r\n   --graph=/sdcard/efficientnet-lite0-int8.tflite\r\n             \r\nSTARTING!\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nMax runs duration (seconds): [150]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [/sdcard/efficientnet-lite0-int8.tflite]\r\nInput layers: [images]\r\nInput shapes: [1,224,224,3]\r\nInput value ranges: []\r\nUse legacy nnapi : [0]\r\nAllow fp16 : [0]\r\nRequire full delegation : [0]\r\nEnable op profiling: [0]\r\nMax profiling buffer entries: [1024]\r\nUse gpu : [0]\r\nAllow lower precision in gpu : [1]\r\nUse Hexagon : [1]\r\nHexagon lib path : [/data/local/tmp]\r\nHexagon Profiling : [0]\r\nUse nnapi : [0]\r\nLoaded model efficientnet-lite0-int8.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nremote_handle_control available and used\r\nINFO: Created TensorFlow Lite delegate for Hexagon.\r\nINFO: Hexagon delegate: 0 nodes delegated out of 64 nodes.\r\n\r\nApplied Hexagon delegate.\r\nThe input model file size (MB): 5.42276\r\nInitialized session in 93.33ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=9 first=69875 curr=58811 min=58605 max=69875 avg=60623.8 std=3476\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=50 first=60688 curr=58755 min=58527 max=61817 avg=59263.9 std=711\r\n\r\nAverage inference timings in us: Warmup: 60623.8, Init: 93330, Inference: 59263.9\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nPeak memory footprint (MB): init=2.44922 overall=9.28125\r\n```\r\nAt first I thought it was due to the (new?) \"Quantize\" node at the beginning of the EfficientNet Lite0 network, but pointing \"--input_layer\" to the next node doesn't help.\r\n\r\n![image](https://user-images.githubusercontent.com/61851501/76033543-54bca580-5ef1-11ea-96df-8f8e826a6181.png)\r\n\r\n\r\nIs this a DSP Delegate bug, or an issue with post-training quantization vs quantization-aware training?   If so, when can we expect TF2 quantization-aware training and/or DSP Delegate compatible EfficientNet Lite models?\r\n\r\nThanks\r\n\r\n**Describe the expected behavior**\r\n\r\nMajority of the 64 nodes in the EfficientNet Lite0 quantized model above should have been delegated to the DSP, and the inference time should have been much faster.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Is this model quantized using post-training quantization ? If yes, then it is not yet supported - we are working on it.", "Thank you for your quick reply Karim.\r\n\r\nAccording to the referenced page, it does look like those EfficientNet Lite models were post-training quantized:\r\n\r\n> Each checkpoint all contains FP tflite and post-training quantized INT8 tflite files. If you use these models or checkpoints, you can cite this efficientnet paper.\r\n\r\nCan you point me to pages showing how to do TF2 quantization-aware training myself?  Thanks!", "See the bottom of this page\r\nhttps://www.tensorflow.org/lite/convert/quantization\r\n\"During training: Quantizing models for integer-only execution.\"\r\n\r\nYou will also need to set \r\nconverter.experimental_new_converter=False\r\n\r\nLet me know how it goes.\r\n\r\nThanks", "Hey @holokai-ai did you manage to get this to work?\r\n\r\n> \r\n> \r\n> Thank you for your quick reply Karim.\r\n> \r\n> According to the referenced page, it does look like those EfficientNet Lite models were post-training quantized:\r\n> \r\n> > Each checkpoint all contains FP tflite and post-training quantized INT8 tflite files. If you use these models or checkpoints, you can cite this efficientnet paper.\r\n> \r\n> Can you point me to pages showing how to do TF2 quantization-aware training myself? Thanks!\r\n\r\n", "Hi @holokai-ai , @msalvaris \r\n\r\nQuick update:\r\nWe started rolling some changes for int8 (post training quantization) support. If you tried at HEAD most of efficient-net should be accelerated.\r\nMore ops to be supported soon, also more optimizations.\r\nPlease give it a try and give us some feedback.\r\n\r\nFor the Quantization aware training,\r\ncan you explain the issues you are facing.\r\n\r\nThanks", "No issues in particular so far. Just wondered if there was an example to work off since the efficientnet repo doesn't have one AFAIK.", "Thank you for the update on post-training quantization Karim.   Will give\nit a try this week.\n\nI got side tracked with another task, but I'm still struggling with TF2\nQuantization Aware Training\n<https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html>\non my own TF2 Keras model.   One of the problems is BatchNormalization,\nwhich is used a lot in various models.   As a minimal example, if I change\nthe Keras model below slightly in the QAT example notebook to add\nBatchNormalization\n(with or without fusing), the floating point training goes fine:\n\nhttps://www.tensorflow.org/model_optimization/guide/quantization/training_example\n\nmodel = keras.Sequential([\n  keras.layers.InputLayer(input_shape=(28, 28)),\n  keras.layers.Reshape(target_shape=(28, 28, 1)),\n  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), strides=(2, 2)),\n  keras.layers.BatchNormalization(fused=True),\n  keras.layers.ReLU(max_value=6.0),\n  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n  keras.layers.Flatten(),\n  keras.layers.Dropout(0.2),\n  keras.layers.Dense(10, activation=tf.nn.softmax)\n])\n\n\nBut the subsequent quantization aware training / fine tuning crashes:\n\nTraceback (most recent call last):\n  File \"./quantization_aware_training_in_keras_example.py\", line 133, in\n<module>\n    q_aware_model = quantize_model(model)\n  File\n\"/home/holokai/.local/lib/python3.6/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py\",\nline 138, in quantize_model\n    return quantize_apply(annotated_model)\n  File\n\"/home/holokai/.local/lib/python3.6/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py\",\nline 403, in quantize_apply\n    unwrapped_model, layer_quantize_map)\n  File\n\"/home/holokai/.local/lib/python3.6/site-packages/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantize_layout_transform.py\",\nline 67, in apply\n    layer_quantize_map.keys(), layer_quantize_map).transform()\n  File\n\"/home/holokai/.local/lib/python3.6/site-packages/tensorflow_model_optimization/python/core/quantization/keras/graph_transformations/model_transformer.py\",\nline 552, in transform\n    custom_objects)\n  File\n\"/home/holokai/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py\",\nline 399, in from_config\n    model.add(layer)\n  File\n\"/home/holokai/.local/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\",\nline 456, in _method_wrapper\n    result = method(self, *args, **kwargs)\n  File\n\"/home/holokai/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py\",\nline 213, in add\n    output_tensor = layer(self.outputs[0])\n  File\n\"/home/holokai/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\",\nline 886, in __call__\n    self.name)\n  File\n\"/home/holokai/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_spec.py\",\nline 180, in assert_input_compatibility\n    str(x.shape.as_list()))\nValueError: Input 0 of layer conv2d is incompatible with the layer:\nexpected ndim=4, found ndim=2. Full shape received: [None, 196]\n\n\nSome papers talk about quantizing batch normalization, but I haven't seen a\nsimple example that spells out how to implement it in Keras:\n\n   - https://arxiv.org/pdf/1712.05877.pdf\n   - https://arxiv.org/pdf/1806.08342.pdf\n\nI've tried upgrading to tensorflow2.2.0rc3 as well as tf-nightly to no\navail.   I'm using tensorflow-model-optimization 0.3.0.\n\nThank you for any pointers\n\nOn Mon, Apr 13, 2020 at 11:05 AM Karim Nosseir <notifications@github.com>\nwrote:\n\n> Hi @holokai-ai <https://github.com/holokai-ai> , @msalvaris\n> <https://github.com/msalvaris>\n>\n> Quick update:\n> We started rolling some changes for int8 (post training quantization)\n> support. If you tried at HEAD most of efficient-net should be accelerated.\n> More ops to be supported soon, also more optimizations.\n> Please give it a try and give us some feedback.\n>\n> For the Quantization aware training,\n> can you explain the issues you are facing.\n>\n> Thanks\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/37376#issuecomment-613018680>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AOX4O3OVMTAG5BYCGSYTVB3RMNH65ANCNFSM4LCUGW6Q>\n> .\n>\n", "Thanks again for the tip that the QCOM DSP Delegate now supports post-training quantization.\r\n\r\nI recompiled benchmark_model from HEAD, and now the post-training quantized EfficientNet Lite0 model runs on DSP!\r\n\r\n```\r\n$ adb shell benchmark_model \\\r\n     --use_hexagon=true \\\r\n     --input_layer=images \\\r\n     --input_layer_shape=1,224,224,3 \\\r\n     --graph=/data/local/tmp/efficientnet-lite0-int8.tflite\r\n\r\nSTARTING!\r\nDuplicate flags: num_threads\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nMax runs duration (seconds): [150]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [/data/local/tmp/efficientnet-lite0-int8.tflite]\r\nInput layers: [images]\r\nInput shapes: [1,224,224,3]\r\nInput value ranges: []\r\nInput layer values files: []\r\nUse legacy nnapi : [0]\r\nAllow fp16 : [0]\r\nRequire full delegation : [0]\r\nEnable op profiling: [1]\r\nMax profiling buffer entries: [1024]\r\nCSV File to export profiling data to: []\r\nEnable platform-wide tracing: [0]\r\n#threads used for CPU inference: [1]\r\nMax number of delegated partitions : [0]\r\nExternal delegate path : []\r\nExternal delegate options : []\r\nUse gpu : [0]\r\nAllow lower precision in gpu : [1]\r\nUse Hexagon : [1]\r\nHexagon lib path : [/data/local/tmp]\r\nHexagon Profiling : [0]\r\nUse nnapi : [0]\r\nUse xnnpack : [0]\r\nLoaded model /data/local/tmp/efficientnet-lite0-int8.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for Hexagon.\r\nINFO: Hexagon delegate: 61 nodes delegated out of 64 nodes with 2 partitions.\r\n\r\nloaded libadsprpc.so\r\nApplied Hexagon delegate, and the model graph will be partially executed w/ the delegate.\r\nThe input model file size (MB): 5.42276\r\nInitialized session in 402.08ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=36 first=17729 curr=13808 min=13748 max=17729 avg=13940.5 std=641\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=72 first=13889 curr=13840 min=13746 max=13946 avg=13847.2 std=44\r\n\r\nInference timings in us: Init: 402080, First inference: 17729, Warmup (avg): 13940.5, Inference (avg): 13847.2\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nPeak memory footprint (MB): init=13.0703 overall=13.0703\r\nProfiling Info for Benchmark Initialization:\r\n============================== Run Order ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t ModifyGraphWithDelegate\t            0.000\t  147.622\t  147.622\t 69.490%\t 69.490%\t 11108.000\t        1\tModifyGraphWithDelegate/0\r\n\t         AllocateTensors\t          115.380\t   64.782\t   32.407\t 30.510%\t100.000%\t     0.000\t        2\tAllocateTensors/0\r\n\r\n============================== Top by Computation Time ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t ModifyGraphWithDelegate\t            0.000\t  147.622\t  147.622\t 69.490%\t 69.490%\t 11108.000\t        1\tModifyGraphWithDelegate/0\r\n\t         AllocateTensors\t          115.380\t   64.782\t   32.407\t 30.510%\t100.000%\t     0.000\t        2\tAllocateTensors/0\r\n\r\nNumber of nodes executed: 2\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t ModifyGraphWithDelegate\t        1\t   147.622\t    69.490%\t    69.490%\t 11108.000\t        1\r\n\t         AllocateTensors\t        1\t    64.815\t    30.510%\t   100.000%\t     0.000\t        2\r\n\r\nTimings (microseconds): count=1 curr=212437\r\nMemory (bytes): count=0\r\n2 nodes observed\r\n\r\n\r\n\r\nOperator-wise Profiling Info for Regular Benchmark Runs:\r\n============================== Run Order ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t                QUANTIZE\t            0.000\t    0.158\t    0.157\t  1.134%\t  1.134%\t     0.000\t        1\t[images_int8]:0\r\n\t   TfLiteHexagonDelegate\t            0.158\t   13.088\t   13.066\t 94.409%\t 95.543%\t     0.000\t        1\t[efficientnet-lite0/model/head/AvgPool]:64\r\n\t                 RESHAPE\t           13.226\t    0.004\t    0.002\t  0.016%\t 95.560%\t     0.000\t        1\t[efficientnet-lite0/model/head/Squeeze1]:60\r\n\t   TfLiteHexagonDelegate\t           13.229\t    0.626\t    0.610\t  4.409%\t 99.969%\t     0.000\t        1\t[Softmax_int8]:65\r\n\t                QUANTIZE\t           13.840\t    0.004\t    0.004\t  0.031%\t100.000%\t     0.000\t        1\t[Softmax]:63\r\n\r\n============================== Top by Computation Time ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t   TfLiteHexagonDelegate\t            0.158\t   13.088\t   13.066\t 94.409%\t 94.409%\t     0.000\t        1\t[efficientnet-lite0/model/head/AvgPool]:64\r\n\t   TfLiteHexagonDelegate\t           13.229\t    0.626\t    0.610\t  4.409%\t 98.819%\t     0.000\t        1\t[Softmax_int8]:65\r\n\t                QUANTIZE\t            0.000\t    0.158\t    0.157\t  1.134%\t 99.953%\t     0.000\t        1\t[images_int8]:0\r\n\t                QUANTIZE\t           13.840\t    0.004\t    0.004\t  0.031%\t 99.984%\t     0.000\t        1\t[Softmax]:63\r\n\t                 RESHAPE\t           13.226\t    0.004\t    0.002\t  0.016%\t100.000%\t     0.000\t        1\t[efficientnet-lite0/model/head/Squeeze1]:60\r\n\r\nNumber of nodes executed: 5\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t   TfLiteHexagonDelegate\t        2\t    13.676\t    98.829%\t    98.829%\t     0.000\t        2\r\n\t                QUANTIZE\t        2\t     0.160\t     1.156%\t    99.986%\t     0.000\t        2\r\n\t                 RESHAPE\t        1\t     0.002\t     0.014%\t   100.000%\t     0.000\t        1\r\n\r\nTimings (microseconds): count=72 first=13880 curr=13834 min=13740 max=13938 avg=13840.2 std=44\r\nMemory (bytes): count=0\r\n5 nodes observed\r\n```", "@nutsiepully Can you check the crash from the QAT.\r\n\r\n@holokai-ai Glad it is working, BTW reshape is added now, if you retried you should have 1 hexagon graph, and should be slight faster.\r\nWe are working on optimizing few parts too, so stay tuned :)\r\n\r\nThanks", "@karimnosseir Sure enough, after I recompiled from ac271534, RESHAPE is folded in and there's only one DSP Delegate graph now.\r\n\r\nWill post-training quantization support for Hexagon Delegate be part of TF 2.2.0 ?   Right now I don't see the relevant commits in r2.2.   The last commit in the Hexagon Delegate for r2.2 is from Feb 27.\r\n\r\nThanks!\r\n\r\n```\r\n$ adb shell benchmark_model \\\r\n     --use_hexagon=true \\\r\n     --input_layer=images \\\r\n     --input_layer_shape=1,224,224,3 \\\r\n     --graph=/data/local/tmp/efficientnet-lite0-int8.tflite\r\n...\r\n...\r\nOperator-wise Profiling Info for Regular Benchmark Runs:\r\n============================== Run Order ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t                QUANTIZE\t            0.000\t    0.165\t    0.164\t  1.197%\t  1.197%\t     0.000\t        1\t[images_int8]:0\r\n\t   TfLiteHexagonDelegate\t            0.165\t   13.573\t   13.509\t 98.753%\t 99.949%\t     0.000\t        1\t[Softmax_int8]:64\r\n\t                QUANTIZE\t           13.676\t    0.007\t    0.007\t  0.051%\t100.000%\t     0.000\t        1\t[Softmax]:63\r\n\r\n============================== Top by Computation Time ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t   TfLiteHexagonDelegate\t            0.165\t   13.573\t   13.509\t 98.753%\t 98.753%\t     0.000\t        1\t[Softmax_int8]:64\r\n\t                QUANTIZE\t            0.000\t    0.165\t    0.164\t  1.197%\t 99.949%\t     0.000\t        1\t[images_int8]:0\r\n\t                QUANTIZE\t           13.676\t    0.007\t    0.007\t  0.051%\t100.000%\t     0.000\t        1\t[Softmax]:63\r\n\r\nNumber of nodes executed: 3\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t   TfLiteHexagonDelegate\t        1\t    13.509\t    98.764%\t    98.764%\t     0.000\t        1\r\n\t                QUANTIZE\t        2\t     0.169\t     1.236%\t   100.000%\t     0.000\t        2\r\n\r\nTimings (microseconds): count=73 first=13745 curr=13616 min=13528 max=13861 avg=13679.8 std=86\r\nMemory (bytes): count=0\r\n3 nodes observed\r\n```\r\n", "@karimnosseir I am unable to do quantization aware training for EfficientNet. If I tried  using the recent Keras API but it doesn't work as there is only support for Sequential and Functional models not custom it seems. \r\nIf I try to use tf.contrib.quantize it seems to skip quantizing quite a few ops.\r\n\r\nPlease advise as to what would be the best way to carry out quantization aware training of EfficienetNet model.\r\n", "@nutsiepully @msalvaris  I'm guilty of starting the TF2 QAT question here, but perhaps we should move it to the TFMOT project?   Someone just posted a similar question here:\r\n\r\nhttps://github.com/tensorflow/model-optimization/issues/363", "@karimnosseir I've been able to run all of Google's post-training quantized EfficientNet Lite models on the Snapdragon 820 DSP using the Hexagon Delegate, except for EfficientNet Lite3.   Running that INT8 model causes a segfault, even though it appears it successfully delegated most of the nodes.  Lite0, Lite1, Lite2 and Lite4 run fine on the DSP Delegate, but not Lite3.  The EfficientNet Lite3 FP32 model runs find on the GPU Delegate.\r\n\r\n```\r\n$ adb shell benchmark_model \\\r\n    --use_hexagon=true \\\r\n    --input_layer=images \\\r\n    --input_layer_shape=1,280,280,3 \\\r\n    --graph=/data/local/tmp/efficientnet-lite3-int8.tflite\r\n\r\nSTARTING!\r\nDuplicate flags: num_threads\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nMax runs duration (seconds): [150]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [efficientnet-lite3-int8.tflite]\r\nInput layers: [images]\r\nInput shapes: [1,280,280,3]\r\nInput value ranges: []\r\nInput layer values files: []\r\nUse legacy nnapi : [0]\r\nAllow fp16 : [0]\r\nRequire full delegation : [0]\r\nEnable op profiling: [1]\r\nMax profiling buffer entries: [1024]\r\nCSV File to export profiling data to: []\r\nEnable platform-wide tracing: [0]\r\n#threads used for CPU inference: [1]\r\nMax number of delegated partitions : [0]\r\nExternal delegate path : []\r\nExternal delegate options : []\r\nUse gpu : [0]\r\nAllow lower precision in gpu : [1]\r\nUse Hexagon : [1]\r\nHexagon lib path : [/data/local/tmp]\r\nHexagon Profiling : [0]\r\nUse nnapi : [0]\r\nUse xnnpack : [0]\r\nLoaded model efficientnet-lite3-int8.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nloaded libadsprpc.so\r\nINFO: Created TensorFlow Lite delegate for Hexagon.\r\nINFO: Hexagon delegate: 94 nodes delegated out of 96 nodes with 1 partitions.\r\n\r\nApplied Hexagon delegate, and the model graph will be partially executed w/ the delegate.\r\nThe input model file size (MB): 9.58559\r\nInitialized session in 510.615ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\nSegmentation fault \r\n\r\n```", "@karimnosseir Now I'm trying my custom model based on EfficientNet Lite0.  Its input image shape is 224,224,1 (grayscale).   The original FP32 trained model runs fine on SD820 CPU as well as GPU (using GPU Delegate).  Next, I used post-training quantization described below to generate an INT8 model:\r\n\r\nhttps://www.tensorflow.org/lite/performance/post_training_integer_quant\r\n\r\nI've tried the TFLiteConverter flags DEFAULT as well as OPTIMIZE_FOR_SIZE and gave it some sample images; results are the same.  The resulting INT8 model is about 4x smaller than the FP32 model (as expected) and runs fine on SD820 CPU, but crashes when I try to run on DSP, even though the majority of nodes were successfully delegated.  Excerpt below.\r\n\r\nWhat am I doing wrong?   Thanks for any pointers.\r\n\r\n```\r\n$ adb shell benchmark_model \\\r\n  --use_hexagon=true \\\r\n  --input_layer=images \\\r\n  --input_layer_shape=1,224,224,1 \\\r\n  --graph=/data/local/tmp/int8.tflite\r\n\r\nSTARTING!\r\nDuplicate flags: num_threads\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nMax runs duration (seconds): [150]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [/data/local/tmp/int8.tflite]\r\nInput layers: [images]\r\nInput shapes: [1,224,224,1]\r\nInput value ranges: []\r\nInput layer values files: []\r\nUse legacy nnapi : [0]\r\nAllow fp16 : [0]\r\nRequire full delegation : [0]\r\nEnable op profiling: [1]\r\nMax profiling buffer entries: [1024]\r\nCSV File to export profiling data to: []\r\nEnable platform-wide tracing: [0]\r\n#threads used for CPU inference: [1]\r\nMax number of delegated partitions : [0]\r\nExternal delegate path : []\r\nExternal delegate options : []\r\nUse gpu : [0]\r\nAllow lower precision in gpu : [1]\r\nUse Hexagon : [1]\r\nHexagon lib path : [/data/local/tmp]\r\nHexagon Profiling : [0]\r\nUse nnapi : [0]\r\nUse xnnpack : [0]\r\nLoaded model /data/local/tmp/int8.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for Hexagon.\r\nINFO: Hexagon delegate: 59 nodes delegated out of 64 nodes with 2 partitions.\r\n\r\nloaded libadsprpc.so\r\nApplied Hexagon delegate, and the model graph will be partially executed w/ the delegate.\r\nThe input model file size (MB): 3.69867\r\nInitialized session in 357.232ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\n----------------\r\nTimestamp: Sun Jan  4 02:40:23 1970\r\n\r\n\r\nLog\r\nhexagon/ops/src/op_requantize.c:437:Requantize_8to8: Out range too small compared to in range!\r\nhexagon/src/execute.c:142:execute() failed on node id=af err=-1\r\nhexagon/src/interface.c:1174:fail in execute_inner()\r\n\r\n----------------\r\nERROR: Failed: Failed to execute graph.. STATE: FAILED_TO_EXECUTE_GRAPH\r\nERROR: Node number 64 (TfLiteHexagonDelegate) failed to invoke.\r\n\r\n----------------\r\nTimestamp: Sun Jan  4 02:40:23 1970\r\n\r\n\r\nLog\r\nhexagon/ops/src/op_requantize.c:437:Requantize_8to8: Out range too small compared to in range!\r\nhexagon/src/execute.c:142:execute() failed on node id=af err=-1\r\nhexagon/src/interface.c:1174:fail in execute_inner()\r\n\r\n----------------\r\nERROR: Failed: Failed to execute graph.. STATE: FAILED_TO_EXECUTE_GRAPH\r\nERROR: Node number 64 (TfLiteHexagonDelegate) failed to invoke.\r\n\r\n----------------\r\n<Same sets of error log lines above repeated many times>\r\n...\r\n...\r\n\r\n\r\ncount=24 first=24384 curr=21268 min=20516 max=24384 avg=21053.6 std=722\r\n\r\nBenchmarking failed.\r\n\r\n```", "@msalvaris Can you try the instructions i sent earlier for the uint8 per tensor, not the new QAT API.\r\n\r\n@holokai-ai \r\n* For the 2.2: It is finalized, so we can't add more changes. Also, currently AARs are only available in nightlies. We are working on stable releases for the AARs.\r\n\r\n* For the efficientnet lite 3\r\nI tried this https://tfhub.dev/tensorflow/lite-model/efficientnet/lite3/int8/1\r\nand works fine.\r\nCan you point me which one you tried ?\r\n\r\n* For the requantize issue:\r\nWe saw a similar issue before, it happens when you have input min/max is too big compared to output min/max. We solved by using 32 bit MULs instead. \r\nWith efficientNet i think there was a similar case with one DepthwiseConv range are too big compared -128/127, and you can tune by tweaking your model.\r\nDo you mind sharing the model privately, i am happy to have a look, my guess will be the same issue, and you can tweak the model to avoid the quantization creating this big ranges.\r\n\r\nThanks", "> For the 2.2: It is finalized, so we can't add more changes. Also, currently AARs are only available in nightlies. We are working on stable releases for the AARs.\r\n\r\nOK thank you for the clarification!\r\n\r\n-----------------------------------------------------------------------------------------------------------\r\n> For the efficientnet lite 3\r\n> I tried this https://tfhub.dev/tensorflow/lite-model/efficientnet/lite3/int8/1\r\n> and works fine.\r\n> Can you point me which one you tried ?\r\n\r\nHere's the EfficientNet Lite3 version I used, because TFHub version does not yet allow for fine tuning under TF2 for some reason:\r\n\r\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite\r\nhttps://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/lite/efficientnet-lite3.tar.gz\r\n\r\n```\r\n$ sha1sum efficientnet-lite3-int8.tflite \r\nf0b94fc6725398bdd775ada491c8457215aa9dd4  efficientnet-lite3-int8.tflite\r\n```\r\n\r\nThis INT8 model works fine on SD820 SOC CPU:\r\n\r\n```\r\n$ adb shell benchmark_model \\\r\n    --input_layer=images \\\r\n    --input_layer_shape=1,280,280,3 \\\r\n    --graph=/data/local/tmp/efficientnet-lite3-int8.tflite\r\n...\r\n...\r\nNumber of nodes executed: 96\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t                 CONV_2D\t       49\t   194.236\t    80.390%\t    80.390%\t     0.000\t       49\r\n\t       DEPTHWISE_CONV_2D\t       24\t    44.783\t    18.535%\t    98.924%\t     0.000\t       24\r\n\t                     ADD\t       17\t     1.752\t     0.725%\t    99.649%\t     0.000\t       17\r\n\t         FULLY_CONNECTED\t        1\t     0.513\t     0.212%\t    99.862%\t     0.000\t        1\r\n\t                QUANTIZE\t        2\t     0.256\t     0.106%\t    99.968%\t     0.000\t        2\r\n\t         AVERAGE_POOL_2D\t        1\t     0.066\t     0.027%\t    99.995%\t     0.000\t        1\r\n\t                 SOFTMAX\t        1\t     0.010\t     0.004%\t    99.999%\t     0.000\t        1\r\n\t                 RESHAPE\t        1\t     0.002\t     0.001%\t   100.000%\t     0.000\t        1\r\n\r\nTimings (microseconds): count=50 first=191312 curr=487173 min=190214 max=712336 avg=241662 std=113426\r\nMemory (bytes): count=0\r\n96 nodes observed\r\n```\r\n\r\nBut segfaults on the SD820 SOC DSP:\r\n```\r\n$ adb shell benchmark_model \\\r\n    --use_hexagon=true \\\r\n    --input_layer=images \\\r\n    --input_layer_shape=1,280,280,3 \\\r\n    --graph=/data/local/tmp/efficientnet-lite3-int8.tflite\r\n...\r\n...\r\nLoaded model /data/local/tmp/efficientnet-lite3-int8.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for Hexagon.\r\nINFO: Hexagon delegate: 94 nodes delegated out of 96 nodes with 1 partitions.\r\n\r\nloaded libadsprpc.so\r\nApplied Hexagon delegate, and the model graph will be partially executed w/ the delegate.\r\nThe input model file size (MB): 9.58559\r\nInitialized session in 504.5ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\nSegmentation fault \r\n```\r\nI don't think it's a resource issue (e.g. OOM) because the larger efficientnet-lite4-int8.tflite model runs fine on DSP.\r\n\r\n\r\n------------------------------------------------------------------------------------------------------\r\n\r\n> For the requantize issue:\r\n> We saw a similar issue before, it happens when you have input min/max is too big compared to output min/max. We solved by using 32 bit MULs instead.\r\n> With efficientNet i think there was a similar case with one DepthwiseConv range are too big compared -128/127, and you can tune by tweaking your model.\r\n> Do you mind sharing the model privately, i am happy to have a look, my guess will be the same issue, and you can tweak the model to avoid the quantization creating this big ranges.\r\n\r\nOK will do.  Thank you!\r\n\r\n\r\n", "@holokai-ai I don't have device with 820, but i have Pixel 1 which has 821 and both should use the Hexagon v60. \r\nI tried it and it works fine.\r\nAlso tried it on Samsung S9 which has 845 \r\nCan you share the logcat of the run \r\n\r\nDump from the run below:\r\n\r\nINFO: Initialized TensorFlow Lite runtime.\r\nFunc remote_handle64_open not available on this device (NULL).\r\nFunc remote_handle64_invoke not available on this device (NULL).\r\nFunc remote_handle64_close not available on this device (NULL).\r\nFunc remote_handle_control not available on this device (NULL).\r\nINFO: Created TensorFlow Lite delegate for Hexagon.\r\nINFO: Hexagon delegate: 96 nodes delegated out of 96 nodes with 1 partitions.\r\n\r\nloaded libadsprpc.so\r\nApplied Hexagon delegate, and the model graph will be completely executed w/ the delegate.\r\nThe input model file size (MB): 9.58559\r\nInitialized session in 1182.01ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=12 first=105922 curr=35738 min=35322 max=105922 avg=41632.8 std=19406\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=50 first=35693 curr=35259 min=35211 max=37337 avg=35585.2 std=401\r\n\r\nInference timings in us: Init: 1182010, First inference: 105922, Warmup (avg): 41632.8, Inference (avg): 35585.2", "@karimnosseir Thanks for taking the time to look into this issue.  I mentioned a SD820 dev board, but it actually has a 821 SOC (aka APQ8096pro).  Perhaps it's the difference between Android 9 and 10?  We're unfortunately stuck at Android 9.   SOM vendor says they can't upgrade the SOM to Android 10 due to QCOM, even though essentially the same SOC is on the Pixel 1 running Android 10.\r\n\r\nI'm wondering if that's the reason why your target hardware delegates all 96 nodes, but my hardware only delegates 94 nodes?\r\n\r\n```\r\n$ adb shell grep ^Hard /proc/cpuinfo\r\nHardware\t: Qualcomm Technologies, Inc APQ8096pro\r\n\r\n$ adb shell getprop ro.build.fingerprint\r\nAndroid/msm8996/msm8996:9/OpenQ820_P_v5.0-PKQ1.181007.001/12:userdebug/test-keys\r\n\r\n$ adb shell sha1sum /data/local/tmp/lib*.so\r\n46425e69ef93b4479ac0caabfcfa3e556444ac89  /data/local/tmp/libhexagon_interface.so\r\nf9809aebb6fa4d52d81e6bb2036120849face179  /data/local/tmp/libhexagon_nn_skel.so\r\ncc8ce9a338eb30c42f9b23647fc43bf8c0bddf0d  /data/local/tmp/libhexagon_nn_skel_v65.so\r\nc7ef0ae4234df8f29bf7eca0e82d5e499c23cfbc  /data/local/tmp/libhexagon_nn_skel_v66.so\r\n```\r\n\r\nLogcat excerpt taken immediately after segfault:\r\n```\r\n--------- beginning of main\r\n01-04 22:28:20.668   990  1015 I chatty  : uid=1021(gps) NDK expire 2 lines\r\n01-04 22:28:34.690   990  1015 W ServiceManagement: Waited one second for android.frameworks.sensorservice@1.0::ISensorManager/default. Waiting another...\r\n01-04 22:28:35.517 14024 14024 I tflite  : Initialized TensorFlow Lite runtime.\r\n01-04 22:28:35.601 14024 14024 V benchmark_model: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1859: Successfully created user PD on domain 0 (attrs 0x0)\r\n01-04 22:28:35.605 14024 14027 V benchmark_model: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:270: rpc latency thread start\r\n01-04 22:28:35.690   990  1015 W ServiceManagement: Waited one second for android.frameworks.sensorservice@1.0::ISensorManager/default. Waiting another...\r\n01-04 22:28:35.776 14024 14024 I tflite  : Created TensorFlow Lite delegate for Hexagon.\r\n01-04 22:28:35.778 14024 14024 I tflite  : Hexagon delegate: 94 nodes delegated out of 96 nodes with 1 partitions.\r\n--------- beginning of crash\r\n01-04 22:28:36.032 14024 14024 F libc    : Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0 in tid 14024 (benchmark_model), pid 14024 (benchmark_model)\r\n01-04 22:28:36.060 14030 14030 I crash_dump64: obtaining output fd from tombstoned, type: kDebuggerdTombstone\r\n01-04 22:28:36.061  1047  1047 I /system/bin/tombstoned: received crash request for pid 14024\r\n01-04 22:28:36.061 14030 14030 I crash_dump64: performing dump of process 14024 (target tid = 14024)\r\n01-04 22:28:36.062 14030 14030 F DEBUG   : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n01-04 22:28:36.063 14030 14030 F DEBUG   : Build fingerprint: 'Android/msm8996/msm8996:9/OpenQ820_P_v5.0-PKQ1.181007.001/12:userdebug/test-keys'\r\n01-04 22:28:36.063 14030 14030 F DEBUG   : Revision: '0'\r\n01-04 22:28:36.063 14030 14030 F DEBUG   : ABI: 'arm64'\r\n01-04 22:28:36.063 14030 14030 F DEBUG   : pid: 14024, tid: 14024, name: benchmark_model  >>> benchmark_model <<<\r\n01-04 22:28:36.063 14030 14030 F DEBUG   : signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0\r\n01-04 22:28:36.063 14030 14030 F DEBUG   : Cause: null pointer dereference\r\n01-04 22:28:36.063 14030 14030 F DEBUG   :     x0  0000000000000118  x1  0000000000000118  x2  0000000000000003  x3  0000000000000118\r\n01-04 22:28:36.063 14030 14030 F DEBUG   :     x4  0000000000000070  x5  0000000020000000  x6  ffffffffc0000001  x7  000000007fffffff\r\n01-04 22:28:36.063 14030 14030 F DEBUG   :     x8  00000077f61d3bc8  x9  00000077f4b5f780  x10 0000000000000002  x11 00000000000396c0\r\n01-04 22:28:36.063 14030 14030 F DEBUG   :     x12 0000000000000082  x13 0000000000000000  x14 0000000000000000  x15 00000000000396b0\r\n01-04 22:28:36.063 14030 14030 F DEBUG   :     x16 00000000fffffffe  x17 0000000000000001  x18 00000077f6200080  x19 00000077f606d028\r\n01-04 22:28:36.063 14030 14030 F DEBUG   :     x20 00000000000000ff  x21 00000077f61c3580  x22 0000000000000000  x23 00000077f60b7500\r\n01-04 22:28:36.063 14030 14030 F DEBUG   :     x24 0000000000000000  x25 00000077f61c3590  x26 00000077f75ca5e0  x27 00000077f60b7500\r\n01-04 22:28:36.063 14030 14030 F DEBUG   :     x28 0000000040000000  x29 0000007fe7b3d870\r\n01-04 22:28:36.063 14030 14030 F DEBUG   :     sp  0000007fe7b3d7d0  lr  00000056f01833d4  pc  00000056f030a778\r\n01-04 22:28:36.064 14030 14030 F DEBUG   : \r\n01-04 22:28:36.064 14030 14030 F DEBUG   : backtrace:\r\n01-04 22:28:36.064 14030 14030 F DEBUG   :     #00 pc 000000000031f778  /system/xbin/benchmark_model\r\n01-04 22:28:36.064 14030 14030 F DEBUG   :     #01 pc 00000000001983d0  /system/xbin/benchmark_model\r\n01-04 22:28:36.064 14030 14030 F DEBUG   :     #02 pc 000000000019b984  /system/xbin/benchmark_model\r\n01-04 22:28:36.064 14030 14030 F DEBUG   :     #03 pc 00000000000540a8  /system/xbin/benchmark_model\r\n01-04 22:28:36.064 14030 14030 F DEBUG   :     #04 pc 0000000000054a84  /system/xbin/benchmark_model\r\n01-04 22:28:36.064 14030 14030 F DEBUG   :     #05 pc 000000000005463c  /system/xbin/benchmark_model\r\n01-04 22:28:36.064 14030 14030 F DEBUG   :     #06 pc 0000000000040a78  /system/xbin/benchmark_model\r\n01-04 22:28:36.065 14030 14030 F DEBUG   :     #07 pc 00000000000d4e54  /system/lib64/libc.so (__libc_init+88)\r\n01-04 22:28:36.117 14030 14030 E crash_dump64: unable to connect to activity manager: Connection refused\r\n01-04 22:28:36.118  1047  1047 E /system/bin/tombstoned: Tombstone written to: /data/tombstones/tombstone_48\r\n01-04 22:28:36.690   990  1015 W ServiceManagement: Waited one second for android.frameworks.sensorservice@1.0::ISensorManager/default. Waiting another...\r\n--------- beginning of system\r\n\r\n```\r\n\r\nI'm at a loss to explain why only Lite3 causes a segfault ...\r\n\r\nThank you\r\n", "@karimnosseir I tried doing the quantization aware training that is mentioned in the link you shared. It basically links here https://github.com/tensorflow/tensorflow/tree/r1.15/tensorflow/contrib/quantize\r\n\r\nI added the tf.contrib.quantize.create_training_graph and tf.contrib.quantize.create_eval_graph\r\nI am using these within the estimator API and version 15.2 of TF\r\n\r\nDuring training I get \r\n``\r\nafter efficientnet-lite0/model/blocks_13/Add_1          \r\n[2020-04-21 08:03:38,183][tensorflow][INFO] - Inserting fake quant op activation_Add_quant after efficientnet-lite0/model/blocks_14/Add_1                \r\n[2020-04-21 08:03:38,215][tensorflow][INFO] - Skipping quant after efficientnet-lite0/model/stem/conv2d/add_fold                             \r\n[2020-04-21 08:03:38,215][tensorflow][INFO] - Skipping quant after efficientnet-lite0/model/blocks_0/depthwise_conv2d/add_fold              [2020-04-21 08:03:38,216][tensorflow][INFO] - Skipping quant after efficientnet-lite0/model/blocks_1/conv2d/add_fold                             \r\n[2020-04-21 08:03:38,216][tensorflow][INFO] - Skipping quant after efficientnet-lite0/model/blocks_1/depthwise_conv2d/add_fold                      [2020-04-21 08:03:38,217][tensorflow][INFO] - Skipping quant after efficientnet-lite0/model/blocks_2/conv2d/add_fold          \r\n[2020-04-21 08:03:38,217][tensorflow][INFO] - Skipping quant after efficientnet-lite0/model/blocks_2/depthwise_conv2d/add_fold\r\n[2020-04-21 08:03:38,218][tensorflow][INFO] - Skipping quant after efficientnet-lite0/model/blocks_3/conv2d/add_fold \r\n``\r\nBut it does seem to work.\r\n\r\nIn the validation though it seems to fail with\r\n``\r\n[2020-04-21 08:09:49,644][tensorflow][INFO] - Restoring parameters from output/model.ckpt-200                                                    \r\n2020-04-21 08:09:50.325225: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key efficientnet\r\n-lite0/model/blocks_10/post_activation_bypass_quant/max not found in checkpoint   \r\n``\r\n\r\nNot sure whether this is simply an error in the quantization or something else.", "@karimnosseir I believe I've fixed the issue of post-training quantization generating errors.   I had inadvertently converted a model that I was using for TF2 QAT, where I had commented out BatchNormalization (because TF2 QAT is apparently not able to handle BatchNormalization yet).   This caused low accuracy even after many epochs, which I suspect made the weights/activations to grow large.\r\n\r\nWhen I put the BatchNormalization back into the model, things converged much more quickly, with higher accuracy.   I was able to quantize that model using post-training quantization without the errors.  The resulting INT8 model ran fine on the SD820 DSP.\r\n\r\nSorry for the false alarm on this post-training quantization issue.\r\n\r\nStill hoping to eventually do TF2 QAT for better accuracy, but that's not really a DSP Delegate issue.   Cheers.\r\n\r\n@nutsiepully Have you been able to look into the TF2 QAT issue?  Thank you.\r\n", "@holokai-ai Finally reproduced the crash and have a fix, will be merged by tomorrow hopefully.\r\nI will update you when it is merged, so you can retry. Sorry for the trouble.\r\n\r\nFor the number of nodes difference, it's because we added support for Quantize op recently (i was using HEAD and i assume you were using older branch).\r\n\r\nGlad to know it is working now.\r\nThanks\r\n\r\n@msalvaris\r\nI am confused with your state now,\r\nif you want to run a quantized model which was quantized using post training quantization then it is supported now, no need to try to use the old uint8 quantization.\r\nJust use nightly, and the int8 versions of efficientnet and other int8 models should work.\r\n\r\nIf you're having issues with some new model using new QAT API, it will be good to summarize so @nutsiepully can help you.\r\n\r\nThanks", "@karimnosseir Awesome!   Thank you very much for the quick fix on the EfficientNet Lite3 DSP Delegate crash issue.  Cheers!", "@karimnosseir \r\nSorry for the confusion. I have been able to use post-training quantization which is great but two issues remain:\r\n1) The drop in accuracy is unacceptable for my use case so wanted to use quantization aware training to try and improve matters\r\n2) The delegate I am using currently supports uint8 and per tensor quantization, the new spec is int8 and per axis quantization for ops like convolution etc.\r\n\r\nIt seems I can't use efficientnet with the old quantization aware training and can't use the new keras based one as it only supports sequence and functional models.\r\n\r\nIf any of the above is incorrect please let me know\r\n", "@msalvaris for # 1 @nutsiepully can help you. For # 2 please try to use nightly the delegate now supports per channel quantization.\r\n\r\n@holokai-ai  Should be fixed now, please retry.\r\n\r\nThanks", "@karimnosseir Yes, I recompiled benchmark_model with the latest fix and INT8 EfficientNet Lite3 no longer causes a segfault.   Thank you for the quick fix!", "Thanks @karimnosseir!\r\n@nutsiepully Any recommendations?", "I am going to close this issue, since the main part about EfficientNet is now resolved.\r\nFor the QAT issue #38939 looks like tracking it.\r\n\r\nPlease feel free to reopen or create new issue if you are having problems.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37376\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37376\">No</a>\n"]}, {"number": 37375, "title": "r2.2 cherry-pick request: Remove --config=mkl_open_source_only ", "body": "Remove --config=mkl_open_source_only because it has compilation errors and has not been used.\r\n\r\nPiperOrigin-RevId: 299177856\r\nChange-Id: Ib581b41f7bc51ff27aba992656eee8fd7a5d6bc5", "comments": []}, {"number": 37374, "title": "Undo changes to the input spec when RNN.unroll is True.", "body": "The changes from fc7116d that made the input spec more stringent do not apply to all unrolled recurrent layers. I'll have to revisit b/148491963 later.\r\n\r\nPiperOrigin-RevId: 298492355\r\nChange-Id: I442ad2a23576cae8fa2fad02a27a199f13b159c1", "comments": []}, {"number": 37373, "title": "[r2.2:Cherrypick] Eliminate tf.where call in updating running mean and average for batch norm in Keras. This unlocks the improvements promised by fusing updates into the CPU and GPU kernels.", "body": "Results for ResNet50 /w batch size 32 in eager mode on GTX 1080:\n\nBefore:  85 images/s (fp32),   81 images/s (fp16)\nAfter:  101 images/s (fp32),   99 images/s (fp16)\nPiperOrigin-RevId: 298927568\nChange-Id: I2941bff5d19c7fdccad78bbb9c5df2fdcd2fc36a", "comments": []}, {"number": 37372, "title": "Undo changes to the input spec when RNN.unroll is True.", "body": "The changes from fc7116d that made the input spec more stringent do not apply to all unrolled recurrent layers. I'll have to revisit b/148491963 later.\r\n\r\nPiperOrigin-RevId: 298492355\r\nChange-Id: I442ad2a23576cae8fa2fad02a27a199f13b159c1", "comments": ["The import/copybara test is still there so I'll create a completely new branch"]}, {"number": 37371, "title": "[Intel MKL] Fixing //tensorflow/core:__tensorflow_core_graph_mkl_related_tests", "body": "This is fixing //tensorflow/core:__tensorflow_core_graph_mkl_related_tests unit test which was failing because of registering duplicate ops in the global op registry.", "comments": []}, {"number": 37370, "title": "Element/Coefficient wise Mul  _MklMul", "body": "Occasionally  see that for element/coefficient wise multiplication ( e.g.  scalar*tensor or tensor*tensor) that multiple types of operators are used  ( e.g. Mul and _MklMul) .  My questions are as follows,\r\n\r\n1.  Is there any difference between Mul and _MklMul ? \r\n2. Why  for some Ops Mul is picked and for others _MklMul is picked ? \r\n\r\nFiles referring to Ops/Kernels  are as follows:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/ops/math_ops.cc\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/kernels/cwise_op_mul_1.cc\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/kernels/mkl_cwise_ops_common.cc\r\n", "comments": ["@whatdhack \r\n\r\nSorry, there is no track for a long time!\r\nDo you still need the support for this issue?", "@whatdhack \r\n\r\nDo you still need to support for this issue?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37370\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37370\">No</a>\n"]}]