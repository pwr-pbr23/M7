[{"number": 27401, "title": "longer latency after post-training quantization on NVIDIA Jetson TX2", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nI modified the tutorial below to convert my customized models to print inference latency.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 16.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNVIDIA Jetson TX2\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version (use command below):\r\n1.12.0\r\n- Python version:\r\n3.5.2\r\n- CUDA/cuDNN version:\r\nV9.0.252\r\n- GPU model and memory:\r\nGPU model: NVIDIA Pascal\u2122, 256 CUDA cores\r\nmemory: 8 GB 128 bit LPDDR4\r\n\r\n**Describe the problem**\r\nI modified the tutorial below to convert three customized models to print inference latency.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_quant.ipynb\r\n\r\nIn the 3-layer and 9-layer model,, the inference time of tflite model is shorter than the original one.\r\nHowever, in resnet50 model, the inference time of tflite model is NOT shorter than the original one.\r\nI summarized the time table and graph below.\r\n![\u5716\u7247](https://user-images.githubusercontent.com/40556694/55369939-36089e80-552b-11e9-925f-67ec4908b4f4.png)\r\n\r\nI read the closed issue #23759:longer latency after post-training quantization. In that issue, the explanation of the longer latency is because the speed-up of integer arithmetic require special/optimized instructions/kernels, while such optimizations are not done on desktop CPU.\r\nIs this explanation applicable to NVIDIA Jetson TX2?\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code\r\n[tflite.zip](https://github.com/tensorflow/tensorflow/files/3031972/tflite.zip)\r\n that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@dannykuo25 Did you understand the results? I have the same problem with other models like Yolo Tiny, for example. I know that the tiny YOLO is not optimized, but where could I understand this optimization?\r\n\r\nDid you have any progress about this problem?", "@thiagoalmedeiros Hi there! Unfortunately I find it hard to understand the integer arithmetic optimization. So I turned to study other compression topics like pruning and sparsifying regularizer on activation functions. \r\n", "TFLite only provides optimized integer kernels on mobile CPU (Arm NEON isa, to be more specific). There's no guarantee on other platforms.  "]}, {"number": 27400, "title": "tensorflow op ExtractImagePatches is not supported", "body": "Hello\r\n\r\nI am trying to convert yolov2-voc converted from darknet to TF using dark flow.\r\nI am getting an error: \"ValueError: tensorflow op ExtractImagePatches is not supported\"\r\nCurrently I am using pre-build 1.9.0. Looking around seems like I need to build it from scratch, but it's not clear what I need to do on windows to get it to work.\r\nIt was mentioned to add it to tensorflow/core/kernels/BUILD, but it's not clear where exactly in it, and if I need to modify any other files.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n", "Never mind. Miss understood some things."]}, {"number": 27399, "title": "Pull support for latest Bazel version", "body": "This makes the 2.0 branch match the current maximum Bazel version settings on `master`. It should help resolve https://github.com/tensorflow/tensorflow/issues/26553, where the latest `devel` images don't support `r2.0`.\r\n\r\nI'm not sure what the minimum version really is, though.", "comments": ["@goldiegadde is it okay for us to merge PR on 2.0 branch?", "> @goldiegadde is it okay for us to merge PR on 2.0 branch?\r\n\r\nHi Yifei, yes I think it is okay to merge.", "Thanks @goldiegadde!"]}, {"number": 27398, "title": "Confusion about how bucketized feature columns work", "body": "I had some confusion about how bucketized feature columns represent input to the model. According to the [blog post on feature columns](https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html), when we bucketize a feature like `year` this puts each value in buckets based on the defined boundaries, and creates a binary vector, turning on each bucket based on the input value, but the example in the [documentation](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column) shows the output as a single integer. I'm confused as to how the input is to the model when using a bucketized column..\r\n**System information**\r\n- TensorFlow version:\r\n1.4.0\r\n- Doc Link:\r\nhttps://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column", "comments": ["@ssubraveti This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!", "\r\n**System information**\r\n- TensorFlow version:\r\n1.4.0\r\n- Doc Link:\r\nhttps://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column\r\nhttps://www.tensorflow.org/guide/feature_columns\r\n\r\n**Describe the documentation issue**\r\nSo the reason I think this is an issue is because the blog post and the documentation for `bucketized_feature_column` seem to say different things about how the preprocessing is done. The blog post mentions this example of a feature like year, and the illustration shows that depending on the boundaries(for the example in question, the boundaries for the bucketized_column are 1960, 1980 and 2000), and if the year value `<1960` the input to the model is represented as `[1,0,0,0]`,\r\nand if the value is `>=1960`, but `<1980`, the input to the model is `[0,1,0,0]`, and if the value is `>=1980` but `<2000`, the input to the model is represented as `[0,0,1,0]`, and if year `>=2000`, the input to the model is represented as `[0,0,0,1]`. But the documentation for `bucketized_column`, says the following:\r\n\r\n```\r\nboundaries = [0, 10, 100]\r\ninput tensor = [[-5, 10000]\r\n                [150,   10]\r\n                [5,    100]]\r\n\r\noutput = [[0, 3]\r\n          [3, 2]\r\n          [1, 3]]\r\n\r\n```\r\nMaybe I'm misunderstanding this, but does this mean that the indices of the buckets are input to the model, instead of the one-hot representation described by the blog post?-", "@ssubraveti The article came out longback (~1.5 years) and there were lot of changes. The doc link you are referring corresponds to TF1.13 where as the blog corresponds to TF1.4.\r\n\r\nPlease post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n", "@jvishnuvardhan thanks for the reply. I did post the question on stackoverflow as well, but this does seem to be an issue with documentation.\r\n\r\nI did actually look at the documentation for 1.4 as well, and the documentation there(https://github.com/tensorflow/docs/blob/r1.4/site/en/api_docs/api_docs/python/tf/feature_column/bucketized_column.md) has the same mismatch with what the blog post says. I just want to understand if I can expect the output of bucketized_column to be a one hot encoded vector of features, or if the feature is just converted to the bucket index. If I'm to believe the documentation on this, the model input is going to be a feature that is the index of the bucket it belongs to, and does this mean that there is a mistake in the article?", "Closing this because it seems like the model input is a one-hot encoded vector for a bucketized feature based on the dimensions of the weight matrix for the first hidden layer. I still think this should be mentioned explicitly in the documentation."]}, {"number": 27397, "title": "Shrink README.md logo", "body": "This got really big all of a sudden.", "comments": ["@drpngx Unless I'm making a big mistake, this change shrinks the enormous logo (which is live at https://github.com/tensorflow/tensorflow) to a more manageable small-ish logo with text underneath.", "OK, let's try it out", "Much better, thanks!"]}, {"number": 27396, "title": "[Intel MKL] BatchMatMulV2 op support", "body": "This PR uses Eigen's BatchMatMulV2 op for MKL backend until this\r\nop is supported in MKL.", "comments": ["pinging @penpornk for review"]}, {"number": 27395, "title": "labe_image bazel build failed! ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version: 1.12\r\n- Python version:2.7\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):0.24.0\r\n- GCC/Compiler version (if compiling from source): gcc  5.4.0 20160609\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\n\r\nTo build label_image for android ARMv8:\r\n\r\n```\r\n> bazel build --config android_arm64 --config monolithic --cxxopt=-std=c++11 \\\r\n  //tensorflow/lite/examples/label_image:label_image\r\n```\r\n\r\nerror infor:\r\n```\r\nERROR: /home/apuser/deeplearning/tensorflow/tensorflow-master/tensorflow/lite/kernels/internal/BUILD:496:1: no such package '@com_google_absl//absl/base': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/2c8421e1c6cef0da9e8a20b01c15256ec9ec116d.tar.gz, https://github.com/abseil/abseil-cpp/archive/2c8421e1c6cef0da9e8a20b01c15256ec9ec116d.tar.gz] to /home/apuser/.cache/bazel/_bazel_apuser/4972b2c1cfd6d41e3f1f5f386fde1d1a/external/com_google_absl/2c8421e1c6cef0da9e8a20b01c15256ec9ec116d.tar.gz: All mirrors are down: [Connection reset, GET returned 404 Not Found] and referenced by '//tensorflow/lite/kernels/internal:tensor_utils'\r\nERROR: Analysis of target '//tensorflow/lite/examples/label_image:label_image' failed; build aborted: no such package '@com_google_absl//absl/base': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/2c8421e1c6cef0da9e8a20b01c15256ec9ec116d.tar.gz, https://github.com/abseil/abseil-cpp/archive/2c8421e1c6cef0da9e8a20b01c15256ec9ec116d.tar.gz] to /home/apuser/.cache/bazel/_bazel_apuser/4972b2c1cfd6d41e3f1f5f386fde1d1a/external/com_google_absl/2c8421e1c6cef0da9e8a20b01c15256ec9ec116d.tar.gz: All mirrors are down: [Connection reset, GET returned 404 Not Found]\r\n\r\n```", "comments": ["as shown in the error message you posted, \r\n```\r\nError downloading [http://mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/2c8421e1c6cef0da9e8a20b01c15256ec9ec116d.tar.gz, https://github.com/abseil/abseil-cpp/archive/2c8421e1c6cef0da9e8a20b01c15256ec9ec116d.tar.gz] to ...\r\n```\r\n\r\nMostly, it's problem of your internet connection, e.g., behind a firewall or proxy", "Thanks,  the reason is that the version of bazel is too new."]}, {"number": 27394, "title": "fix tf_shared_library_deps on macOS", "body": "On macos, //tensorflow/go:test failed to build because\r\ntf_shared_library_deps was ambiguous on with the framework shared\r\nobject. Add macos_with_framework_shared_object explicit.\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>\r\n\r\nFixes build failure caused after: https://github.com/tensorflow/tensorflow/pull/27080", "comments": []}, {"number": 27393, "title": "TFTRT: Add converter for ArgMin and ArgMax", "body": "Also adds unit tests for ArgMin/ArgMax.\r\n\r\nDisables INT32 for all ops using TopK because it is unsupported by TRT.", "comments": []}, {"number": 27392, "title": "Runtime error when using ExponentialMovingAverage with MirroredStrategy (TF 1.13.1)", "body": "I'm trying to use ExponentialMovingAverage with MirroredStrategy training the model on several GPUs, but getting an error:\r\n```\r\nRuntimeError: Tried to create variable dense/kernel/replica_1/ExponentialMovingAverage/ with mismatching name on device 1\r\n```\r\n\r\nThis error can also be reproduced on a CPU machine by specifying the number of GPUs in the \"tf.contrib.distribute.MirroredStrategy\" more or equal to 2.\r\n\r\nThe model works fine if I'm training it on a single GPU (by specifying the number of GPUs to 1).\r\n\r\nI'm using TF v1.13.1. Here is a simplified code:\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef input_fn():\r\n    features = tf.data.Dataset.from_tensors([1., 2., 3.])\r\n    labels = tf.data.Dataset.from_tensors(1.)\r\n    dataset = tf.data.Dataset.zip((features, labels)).repeat(10).batch(1)\r\n    return dataset\r\n\r\n\r\ndef model_fn(features, labels, mode, params):\r\n    logits = tf.layers.dense(features, 1, activation=tf.nn.relu)\r\n    logits = tf.reshape(logits, (-1,))\r\n    loss = tf.losses.mean_squared_error(logits, labels)\r\n\r\n    ema = tf.train.ExponentialMovingAverage(0.999)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\r\n        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\r\n\r\n        # apply moving averages\r\n        with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\r\n            ema_update_op = ema.apply(tf.trainable_variables())\r\n\r\n        with tf.control_dependencies([train_op]):\r\n            train_op = tf.group(ema_update_op)\r\n\r\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\r\n\r\n    raise NotImplementedError\r\n\r\n\r\ndef train(model_dir):\r\n    distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\r\n\r\n    estimator = tf.estimator.Estimator(\r\n        model_fn=model_fn,\r\n        config=tf.estimator.RunConfig(\r\n            train_distribute=distribution,\r\n            model_dir=model_dir,\r\n            log_step_count_steps=1,\r\n        ),\r\n    )\r\n\r\n    estimator.train(input_fn=input_fn)\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    train('training/test1')\r\n```\r\n", "comments": ["I have a similar problem.\r\n`RuntimeError: Tried to create variable mnasnet-a1/mnas_net_model/mnasnet/mnas_blocks_4/se/conv2d_11/kernel/replica_1/ExponentialMovingAverage/ with mismatching name on device 1`\r\nI don't understand why replica_1 appears in the middle of the variable name.", "This seems to be a problem caused by using both `tf.contrib.distribute.MirroredStrategy` and `tf.train.ExponentialMovingAverage`.When `ema.apply(tf.trainable_variables())` is executed, the replica variable has been generated.Finally generated a variable name like kernel/replica_1/ExponentialMovingAverage.\r\nMaybe this is not a bug, but it needs a way to solve this problem.Can someone help solve this problem?", "@rootkitchao For now, I solved this problem by using a deprecated functionality: [tf.contrib.estimator.replicate_model_fn](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/replicate_model_fn) with [tf.contrib.estimator.TowerOptimizer](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/TowerOptimizer). Check out [this article](https://joe-antognini.github.io/machine-learning/multi-gpu-tf) to see how to use them together.", "> @rootkitchao For now, I solved this problem by using a deprecated functionality: tf.contrib.estimator.replicate_model_fn with tf.contrib.estimator.TowerOptimizer. Check out this article to see how to use them together.\r\n\r\nThanks for your reply.I solved this problem temporarily by modifying shared_variable_creator:\r\n```python\r\n  def reuse_variable(next_creator, *args, **kwargs):\r\n    \"\"\"Re-use existing variable from store with same name (in order).\"\"\"\r\n    del next_creator, args\r\n    name = kwargs.get(\"name\")\r\n    canonical_name = _canonicalize_variable_name(name)\r\n    replica_index = canonical_name.find('replica/')\r\n    if replica_index != -1:\r\n      canonical_name = canonical_name[0:replica_index] + canonical_name[replica_index+8:]\r\n    try:\r\n      variable_index = variable_scope_access_index.get(canonical_name, 0)\r\n      v = shared_variable_store[canonical_name][variable_index]\r\n      # TODO(priyag): Make this variable re-use more robust by adding checks\r\n      # that the requested shape and dtype match the existing variable.\r\n      variable_scope_access_index[canonical_name] = variable_index + 1\r\n      return v\r\n    except (KeyError, IndexError):\r\n      raise RuntimeError(\r\n          \"Tried to create variable {} with mismatching name on device {}\".\r\n          format(name, device_id))\r\n\r\n  if device_id == 0:\r\n    return create_new_variable\r\n  else:\r\n    return reuse_variable\r\n```\r\nI found that MirroredStrategy always generates some wrong variable names on GPU1.Removing 'replica/' from the variable name can temporarily solve this problem.But I think there is a conflict when MirroredStrategy and ExponentialMovingAverage are used together.Ema.apply(tf.trainable_variables()) adds variables from each GPU to the list.Need a way to just manipulate variables on the current GPU.", "@guptapriya Will this issue be fixed? Thank you.", "@rootkitchao we will definitely look into the bug, but I don't have an ETA yet unfortunately. Thanks. ", "@guptapriya hi, any updates on this issue?", "any updates?", "Hi, sorry we don't have any updates yet. I will open this up for contributions as well. ", "I have found an ugly workaround. Just define your `ExponentialMovingAverage` **outside** of the training function and set the `num_updates` **inside** the training function. Here is an working example:\r\n\r\n```python\r\ndef input_fn():\r\n  features = tf.data.Dataset.from_tensors([[1.]]).repeat(100)\r\n  labels = tf.data.Dataset.from_tensors(1.).repeat(100)\r\n  return tf.data.Dataset.zip((features, labels))\r\n\r\nema = tf.train.ExponentialMovingAverage(decay=0.9999)\r\n\r\ndef model_fn(features, labels, mode):\r\n  regularizer = tf.contrib.layers.l2_regularizer(0.001)\r\n  layer = tf.compat.v1.layers.Dense(1, kernel_regularizer=regularizer)\r\n  logits = layer(features)\r\n\r\n  loss = tf.compat.v1.losses.mean_squared_error(labels, tf.reshape(logits, []))\r\n\r\n  global_step = tf.compat.v1.train.get_or_create_global_step()\r\n\r\n  # NOTE Here we set the global step for the ema\r\n  ema._num_updates = global_step\r\n\r\n  train_op = tf.compat.v1.train.GradientDescentOptimizer(0.2).minimize(loss, global_step=global_step)\r\n\r\n  ema_vars = tf.compat.v1.trainable_variables()\r\n\r\n  with tf.control_dependencies([train_op]):\r\n    train_op = ema.apply(ema_vars)\r\n\r\n  return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\ndistribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\r\nconfig = tf.estimator.RunConfig(log_step_count_steps=10, train_distribute=distribution)\r\nclassifier = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\nclassifier.train(input_fn=input_fn)\r\n```\r\n\r\nThis workaround will only work for mirrored strategies **not** for replicated. The issue is in the [`ExponentialMovingAverage::apply`](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/training/moving_averages.py#L422) method. This method will be called for every mirror and will create a new average-variable for every trainable variable. Because variables in the `MirroredStrategy` will be shared as `MirroredVariable` this will fail for the second mirror, because this  a `MirroredVariable` was already created in the context of the first mirror.\r\n\r\nCreating a global `ExponentialMovingAverage` instance will overcome this problem, because he store the averages for already visited variables in a local dict [`_averages`](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/training/moving_averages.py#L430).\r\n\r\n", "This was fixed in https://github.com/tensorflow/tensorflow/commit/3a22ef3c8a466123d60c1a5d10d62e4d2d0f92d2#diff-4617aef7d483dc28665b640fbe23c18d. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27392\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27392\">No</a>\n"]}, {"number": 27391, "title": "Tests for InterleaveDatasetOp", "body": "This PR adds tests for `InterleaveDatasetOp`.\r\n\r\ncc: @jsimsa ", "comments": ["There are three test failures, but the detailed logs are not accessible. Could you help check if these failures are related?", "The failure in `Linux GPU` seems to be unrelated. The logs for `MacOS Python2 and CC`, `Ubuntu Python3 PIP`, and `Windows Bazel GPU` are still unavailable. ", "I will rerun the tests internally and let you know what I see."]}, {"number": 27390, "title": "Reading a tensor from file in python which was saved using C++", "body": "I'm having trouble reading/interpreting a tensor from a file using read_file() operation in Python. The tensor was saved using Save() ops in tensorflow CC API. I can read the underlying byte encoded tensor, but find no methods to convert that back to float(original content of the tensor). I have tried decode_raw() in python but it complaints about of not being of proper multiple of 4. \r\nThe above tensor comprises of a 3 channel nd array with float contents. Would appreciate any help in deciphering the content\r\nMore precisely would really appreciate on some details on how Save() ops in tensorflow CC API works?", "comments": ["@dibya-pati This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!", "@muddham Thanks for the reply. I believe its more to do on documentation of an API . Its not a bug,But there are no documentation on how the tensor is encoded before writing to the disk , hence there is no way to understand how to decode it from the file using a different method other than Restore() in CC API. Thanks for the suggestion, I would definitely put a stackoverflow question But I would really appreciate if you could point out to me if there are any existing documentation of CC Save() API", "@dibya-pati Could you provide a simple code to reproduce your issue? Thanks!", "Hi, Its not an issue, but documentation request i.e. I can't figure out from the documentation how the tensors are encoded before getting written to disk using Save() in TF CC API, So, I don't know how to read the written tensor file in python. Sorry, I couldn't get some time write a snippet to demonstrate this. But , if someone just could point to some example or any material that shows how its encoded, that will be it. I'll try to put a code snippet to better explain the problem", "@dibya-pati please confirm if the issue still persist", "Hi @dibya-pati, \r\n\r\nPersonally, if I had to get a tensor-value from a cpp program to a python program I'd try:\r\n[serialize-tensor](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/serialize-tensor) and [parse-tensor](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/parse-tensor) first, since these have clear python implementations: [serialize_tensor](https://www.tensorflow.org/api_docs/python/tf/io/serialize_tensor), and [parse_tensor](https://www.tensorflow.org/api_docs/python/tf/io/parse_tensor).\r\n\r\nI'm no expert on the internals of tf-cpp or checkpoints, but I think `ops::Save`, and `ops:SaveV2` are part of the checkpoint system.\r\n\r\n\r\n", "I'm still waiting for some API documentation on Save() ops in CC Tf. If you have some documentation please point me to it.", "@dibya-pati,\r\nSorry for the delayed response. Can you please refer to the documentation of [Save](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/save) and [SaveV2](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/save-v2) and let us know if this is what your are looking for? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 27389, "title": "Add cuda implementation of roll_op  #23590", "body": "@alextp, I tested my changes for tensorflow 1.12, since our cluster only supports cuda9 right now.\r\n", "comments": ["@penpornk @chsigg do either of you have the cycles to review this?", "@alextp The soonest I can get to this PR is a few weeks from now. (I'm on vacation this week.)", "I'm removing my review request since @chsigg has already started a review and we also have @reedwm here.", "@chsigg, @alextp hi, it has been a while, how is the review going?", "Hi @chsigg, @alextp, I made some further changes to the code, use GpuLaunchKernel and let cuda roll support more data type, can you run the internal checks? Thanks", "@chsigg, I am not sure how the failed check (control_flow_ops_test_gpu) relates to my PR, it seems to me that it has nothing to do with my roll_op implementation.   ", "@chsigg, thanks for your review. I was wondering what the current situation is, is @reedwm going review this PR one more time or it will merge soon?   ", "I don't plan on reviewing this, since @chsigg already reviewed it.", "@reedwm, thanks for the reply."]}, {"number": 27388, "title": "[Intel MKL] Fixing Windows build with MKL-DNN support", "body": "This PR is making few changes to code that MSVC compiler did not like. ", "comments": []}, {"number": 27387, "title": "remove the dequantize op test until the op is enabled", "body": "", "comments": []}, {"number": 27386, "title": "Tensorflow 2.0: Optimizer.minimize ('Adam' object has no attribute 'minimize')", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave\r\n- TensorFlow installed from (source or binary): latest 2.0.0-alpha0 via pycharm\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 3.7\r\n\r\n\r\nFor my Reinforcement Learning application, I need to be able to apply custom gradients / minimize changing loss function. According to documentation [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/Optimizer], it should be possible with Optimizer.minimize() function. However, my pip-installed version appears not to have this feature at all.\r\n\r\nhttps://stackoverflow.com/questions/55459087/tensorflow-2-0-optimizer-minimize-adam-object-has-no-attribute-minimize\r\n\r\nMy code:\r\n\r\n```\r\nfrom tensorflow.python.keras.optimizers import Adam, SGD\r\nprint(tf.version.VERSION)\r\noptim = Adam()\r\noptim.minimize(loss, var_list=network.weights)\r\n```\r\noutput:\r\n\r\n```\r\n2.0.0-alpha0\r\nTraceback (most recent call last):\r\n  File \"/Users/ikkamens/Library/Preferences/PyCharmCE2018.3/scratches/testo.py\", line 18, in <module>\r\n    optim.minimize(loss, var_list=network.weights)\r\nAttributeError: 'Adam' object has no attribute 'minimize'\r\n```", "comments": ["@ikamensh I think instead of \r\n```\r\nfrom tensorflow.python.keras.optimizers import Adam, SGD\r\n```\r\n, you should use `tf.keras.optimizers`.\r\n\r\nThe `tf.keras.optimizers` is the one exposed in 2.0. \r\n\r\n`tensorflow.python.keras.optimizers` is internal and is not routed to 2.0 API.", "Right, the source of my confusion was that tf.keras is prompting an IDE warning:\r\n`Cannot find reference 'keras' in '__init__.py'`\r\n\r\nActually running the code works. Should `__init__.py` be modified to remove this confusion?", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "I noticed your [Stackoverflow post](https://stackoverflow.com/questions/55459087/tensorflow-2-0-optimizer-minimize-adam-object-has-no-attribute-minimize). That is the right place for support type question. Thanks!", "Do you agree that __init__.py should be modified to allow IDEs to discover these right paths? \r\n\r\n> Right, the source of my confusion was that tf.keras is prompting an IDE warning:\r\n> Cannot find reference 'keras' in '__init__.py'\r\n> \r\n> Actually running the code works. Should __init__.py be modified to remove this confusion?", "Closing this as the stackoverflow is correct place for this issue. Thanks!"]}, {"number": 27385, "title": "Adding tensor names for TFLite Java", "body": "Add the commands to get the input and output names for TFLite java\r\n- Closes #27360", "comments": ["We should probably just add the name as a field on the Tensor class in Tensor.java, letting you do something like:\r\n```\r\ngetInputTensor(0).name()\r\n```", "The merge was a bit premature, expect a rollback shortly.", "@jdduke that makes sense, but I saw there was more infrastructure already in place for `getInputTensorNames` (which the python tflite interpreter also has), so maybe it should have the names in the `Tensor` and `Interpreter`.", "Yeah, we're trying to streamline the Interpreter interface, particularly for properties that are Tensor-specific."]}, {"number": 27384, "title": "How to use estimator with graph creation routines in model_fn using placeholders?", "body": "It seems like tf.estimator.Estimator is worth using even for optimization of custom graphs. I am falling down a pattern of creating graph constructors using placeholders for the input. The particluar graph chunk will appear in multiples places in the large graph with the same (shared) weights but different inputs. \r\n\r\nIt is not clear from the estimator documentation what the \"right\" pattern inside the model_fn is. I have a feeling running a session with a feed_dict in there is wrong. \r\n\r\nAre there any more examples that handle something like this? ", "comments": ["I seem to have got something working by simply avoiding using placeholders and using a build function that takes data (variables/pandas/numpy) as args and uses a tf.variable_scope inside to try to share weights."]}, {"number": 27383, "title": "transpose() can be very slow on CPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 / WSL\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary (MKL) (pip install intel-tensorflow)\r\n- TensorFlow version (use command below): b'v1.13.1-0-g6612da8' 1.13.1\r\n- Python version: 3.6.8 (Anaconda)\r\n\r\n**Describe the current behavior**\r\nOn CPU, `tf.transpose()` can be 10 times slower than `np.tranpose(...).copy()`, depending on the dimensions of the tensor. In my example case, the speed can be fixed by sandwiching the `tf.transpose` in reshapes that reduce the dimensionality for the transpose.\r\n\r\n**Describe the expected behavior**\r\n`tf.tranpose()` should be fast without the need to resort to reshaping tricks (which will not always apply anyway). Note also that some transposes are implicit (performed, say, by `tf.tensordot()`).\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport time\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\ntf.enable_v2_behavior()\r\n\r\nd = 2\r\nN = 22\r\n# NOTE: TF starts getting a lot slower than numpy at N=9, it seems. \r\n# Try d=5, N=9  vs  d=7, N=8.\r\n\r\n# Create a N-dimensional ndarray\r\npsi = np.random.randn(*[d for n in range(N)]).astype(np.float64)\r\n\r\npsi_np = psi.copy()\r\nfor _ in range(5):\r\n    # numpy's transpose is a metadata operation.\r\n    # copy() actually carries out the reordering of the data in memory.\r\n    t0 = time.time()\r\n    psi_np = np.transpose(psi_np, (*list(range(2,N)), 0, 1)).copy()\r\n    print(time.time() - t0)  # around 0.07s on my system\r\n\r\npsi_tf = tf.convert_to_tensor(psi)\r\n\r\n@tf.contrib.eager.defun  # make this a graph, just in case\r\ndef f(p):\r\n    return tf.transpose(p, (*list(range(2,N)), 0, 1))\r\n\r\nfor _ in range(5):\r\n    t0 = time.time()\r\n    psi_tf = f(psi_tf)\r\n    print(time.time() - t0)  # around 0.5s on my system (almost 10 times slower!)\r\n\r\n# check results are the same\r\nprint(np.linalg.norm(psi_tf.numpy().ravel() - psi_np.ravel()))  # should print 0.0\r\n\r\n# Note that the speed depends on the shape!\r\n# We now do an equivalent tranpose, but sandwiched \r\n# by reshapes to reduce the number of dims.\r\n\r\npsi_tf = tf.convert_to_tensor(psi)\r\n\r\n@tf.contrib.eager.defun\r\ndef f2(p):\r\n    # Rephrase the high-dimensional transpose as a matrix transpose.\r\n    pr = tf.reshape(p, (d**2, d**(N-2)))\r\n    prt = tf.transpose(pr, (1,0))\r\n    return tf.reshape(prt, p.shape)\r\n\r\nfor _ in range(5):\r\n    t0 = time.time()\r\n    psi_tf = f2(psi_tf)\r\n    print(time.time() - t0)  # around 0.008s on my system\r\n\r\n# check results are the same\r\nprint(np.linalg.norm(psi_tf.numpy().ravel() - psi_np.ravel()))  # should print 0.0\r\n```\r\n\r\n**Other info / logs**\r\nCompared to #15697, I am actually timing the equivalent op in numpy (transpose then copy).\r\nAlso, this happens on MKL builds as well as non-MKL builds of  TF 1.13.1.", "comments": ["@tatianashp can you triage?", "Related question: Does the graph optimizer currently reorder transpose, reshape sequences? If I do a transpose on a large-dimensional tensor, then reshape it to a smaller one (but with dimensions such that the equivalent transpose can still be performed), will the optimizer reorder so that the transpose is done on the lower-dimensional tensor?", "same issue, any update?", "@rmlarsen, @ezhulenev  - do you know if there is an easy fix here?", "i've same problem. Just use the transpose from numpy instead", "@heizie - what version of TensorFlow do you see the issue on?", "> @heizie - what version of TensorFlow do you see the issue on?\r\n\r\ntf 2.2, i think this should running only single core. when i run the tf.transpose, one core of all is running like on fire. if i use np.transpose, all cores are working", "I think numpy.transpose() just reorders the shape of the array as the user can see it from a high level perspective. It does not reorder the data in memory. I came to this end based on the strides of the transposed array before and after operation. The strides are just switched as do the shape dimensions but the data are never touched! I'm I correct? can someone confirm this?", "TF transpose almost always reorders the array in memory\n\nOn Mon, Oct 5, 2020 at 7:35 AM Andreas <notifications@github.com> wrote:\n\n> I think numpy.transpose() just reorders the shape of the array as the user\n> can see it from a high level perspective. It does not reorder the data in\n> memory. I came to this end based on the strides of the transposed array\n> before and after operation. The strides are just switched as do the shape\n> dimensions but the data are never touched! I'm I correct? can someone\n> confirm this?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27383#issuecomment-703673452>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRL65DC5F7CQ7ALECLTSJHKTPANCNFSM4HCXPI2Q>\n> .\n>\n\n\n-- \n - Alex\n", "You can get numpy to actually reorder the data by doing a copy after the transpose (this is what I did in my tests above).", "> You can get numpy to actually reorder the data by doing a copy after the transpose (this is what I did in my tests above).\r\n\r\n@amilsted  \r\nOkay I didn't got it from your first comment that the two actions:\r\n`B = np.transpose(A, ...).copy()`\r\nand\r\n`C = np.transpose(A, ...)`\r\nare implemented differently under the hood.\r\n\r\nNow it's clear,\r\nthanks for replying back!", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27383\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27383\">No</a>\n"]}, {"number": 27382, "title": "Read full message in ReadBinaryProto().", "body": "Add call to coded_stream.ConsumedEntireMessage() in ReadBinaryProto() to\r\nensure that full message is read, and that ReadBinaryProto() does not\r\nreport success on a partial read.\r\n\r\nCloses #27375.", "comments": ["I'm not super familiar with the message reading proto APIs, but this seems like a sensible change.  It also appears to be mentioned in the documentation as well.  \r\n\r\nIs there a test one could write that would have highlighted the problem, or is it difficult to trigger the partial read in a test?", "I am not a good reviewer for this change. Please redirect.", "I encountered a problem in code that would first try to read a file as a binary proto, and would fall back on reading it as a text proto. It would sometimes accept a text proto as binary, which made it not fall back on reading it as a text proto and resulted in garbage. With this change, I don't have that problem. I suppose I could try to distill a minimal test case out of that scenario, if you think it is worth it. I don't know how easy it would be.\r\n\r\nI would typically use MessageLite::ParseFromString() to parse from binary, which ends up calling this code:\r\nhttps://github.com/protocolbuffers/protobuf/blob/master/src/google/protobuf/message_lite.cc#L164\r\n\r\nReadBinaryProto() ought to look the same as that. The documentation for MessageLite::ParseFromCodedStream() says a call to ConsumedEntireMessage() is expected to follow.", "I guess it's fine to just use the API as specified and skip the test, given that there may not be an easy way to trigger this behavior in a test that we could write.", "Added changes to MemmappedFileSystem. It seems like a bug in the implementation, which resulted in files having padded length. This made it look like the proto file had not been fully read, resulting in a read error with the added call to ConsumedEntireMessage() in ReadBinaryProto()."]}, {"number": 27379, "title": "Docs load really slow ", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\nSystem information\r\n\r\n    chrome\r\n    Internet speed 120+ Mbps\r\n    TensorFlow version: all\r\n    Doc Link: all pages\r\n\r\nIt takes a really long time to load the docs. Navigating to another page can take 10+ seconds to load a new page.", "comments": ["@JaspervDalen It is really strange. \r\n1. Do you have any Firewall? It never took more than a second or two even with low internet speeds (6 Mbps download). \r\n2. Can you check with other browsers (IE, Firefox, Safari etc...)\r\n3. typically how much it takes to open other websites (ex. news.google.com)\r\n\r\nPlease provide as many details as possible to find root-cause of the issue. Thanks!", "@jvishnuvardhan now that I check it at home (I mostly use the tensorflow docs at work) it is a lot faster (about 3 seconds). But it is across machines (my colleagues also have the same issues) and across browsers. We haven't noticed any issues witha ny other websites. I will look tomorrow for a firewall or something. ", "@JaspervDalen Thanks for quick reply. Could you mention location (Europe, Asia, ...) from where you are accessing and also mention whether you notice any specific pages in [TF website](https://www.tensorflow.org/) that are taking more time than others. thanks!", "Pages with iframes can take longer to load (like the home page).\r\nI've also noticed pages with lots of code snippets can take longer to fill in, but should start rendering quickly.\r\nBut, as Vishnuvardhan said, any specific pages and your region would be helpful it it's not an internal configuration issue.", "@lamberta, @jvishnuvardhan  It is really all pages, I can switch between all pages of the docs and all load in about 10 seconds. Today I looked at https://www.tensorflow.org/api_docs/python/tf/map_fn, https://www.tensorflow.org/api_docs/python/tf/meshgrid both really slow and https://www.tensorflow.org/api_docs/python/tf/range all took about 10 seconds. I work in Nijmegen (the netherlands), however my home is also in nijmegen (but another provider). ", "@lamberta @jvishnuvardhan Ok after testing, it is mainly firefox 66.0.2 with ublock origin as add blocker (no other add-ons). In chrome it is a lot quicker (but still slow with about 2/3 seconds per loading of a page)", "@JaspervDalen Last time you mentioned it took 10 seconds per page, and now it is taking 2/3 seconds per page. What did you change? Just curious. Thanks!", "@jvishnuvardhan chrome is 3 second Firefox stil 10. When i tested it before with chrome it also took a long time. I think it was because i use a lot of tabs. However Firefox still has the problem", "Can't reproduce but will keep an eye on site performance. Tracking an internal ticket b/133423244 for another issue."]}, {"number": 27378, "title": "tensorflow-gpu(1.13.1) tf.gradients()", "body": "I want to know where I'm wrong.\r\n\r\n\r\n\r\n![\u6355\u83b7](https://user-images.githubusercontent.com/42964873/55328415-e3919880-54be-11e9-99e9-c6d61e445c53.PNG)\r\n![\u6355\u83b71](https://user-images.githubusercontent.com/42964873/55328494-0d4abf80-54bf-11e9-8beb-2734bc09f706.PNG)\r\n\r\n", "comments": ["Your variables are of integer types (because you initialize them with integers) and mathematically there is no gradient defined on integer types so you cannot evaluate them.\r\n\r\nThis is expected.", "@ppwwyyxx Thank you very much!  The problem is solved.", "Closing this issue since its resolved. Thanks!"]}, {"number": 27377, "title": "Docs load really slow", "body": "**System information**\r\n- chrome\r\n- Internet speed 120+ Mbps\r\n- TensorFlow version: all\r\n- Doc Link: all pages\r\n\r\nIt takes a really long time to load the docs. Navigating to another page can take 10+ seconds to load a new page.\r\n\r\n\r\n", "comments": []}, {"number": 27376, "title": "# From tensorflow/models/research/ issue", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hello, I have issue in the following steps:\r\n# From tensorflow/models/research/\r\nprotoc object_detection/protos/*.proto --python_out=.\r\n\r\nwhen I go to my MAC and change directory to tensorflow/models/research/ and enter. protoc object_detection/protos/*.proto --python_out=.  - I SEE OUT 'IS A DIRECTORY\"\r\nPLEASE SEE BELOW.  I am stuck at the step and can not move forward - please advise what I am doing wrong?  thanks.\r\n\r\n/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/models/research protoc object_detection/protos/*.proto --python_out=.\r\n-bash: /Applications/anaconda2/lib/python2.7/site-packages/tensorflow/models/research: is a directory\r\n\r\n\r\n", "I thought the problem is due to this :  make\r\ncp -r pycocotools <path_to_tensorflow>/models/research/\r\nso I tried\r\nmake cp -r pycocotools /Applications/anaconda2/lib/python2.7/site-packages/tensorflow/models/research/\r\nmake: *** No rule to make target `cp'.  Stop.\r\n\r\nit is still not working.\r\n", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "This issue is more suitable on TF Models repo. Please post it on TF Models from [here](https://github.com/tensorflow/models/issues/new). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27376\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27376\">No</a>\n", "Hello, after a lot of try I passed that stage, but still having issue with tensor flow.  below is my error, I am using MAC and python 2.7.  Can you please tell me why this error happens?\r\n\r\nLast login: Wed Apr  3 20:38:52 on console\r\n$ cd /Applications/anaconda2/lib/python2.7/site-packages/tensorflow/models/research \r\n$ export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\r\n$ python object_detection/builders/model_builder_test.py\r\nTraceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_test.py\", line 23, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/models/research/object_detection/builders/model_builder.py\", line 22, in <module>\r\n    from object_detection.builders import box_predictor_builder\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/models/research/object_detection/builders/box_predictor_builder.py\", line 20, in <module>\r\n    from object_detection.predictors import convolutional_box_predictor\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/models/research/object_detection/predictors/convolutional_box_predictor.py\", line 22, in <module>\r\n    slim = tf.contrib.slim\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/lazy_loader.py\", line 61, in __getattr__\r\n    module = self._load()\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/lazy_loader.py\", line 44, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/Applications/anaconda2/lib/python2.7/importlib/__init__.py\", line 37, in import_module\r\n    __import__(name)\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/__init__.py\", line 41, in <module>\r\n    from tensorflow.contrib import distributions\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/distributions/__init__.py\", line 44, in <module>\r\n    from tensorflow.contrib.distributions.python.ops.estimator import *\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/distributions/python/ops/estimator.py\", line 21, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators.head import _compute_weighted_loss\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/__init__.py\", line 93, in <module>\r\n    from tensorflow.contrib.learn.python.learn import *\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/__init__.py\", line 28, in <module>\r\n    from tensorflow.contrib.learn.python.learn import *\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/__init__.py\", line 30, in <module>\r\n    from tensorflow.contrib.learn.python.learn import estimators\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/__init__.py\", line 302, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py\", line 34, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 36, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators import estimator\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 52, in <module>\r\n    from tensorflow.contrib.learn.python.learn.learn_io import data_feeder\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/__init__.py\", line 26, in <module>\r\n    from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/dask_io.py\", line 33, in <module>\r\n    import dask.dataframe as dd\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/dask/dataframe/__init__.py\", line 4, in <module>\r\n    from .core import (DataFrame, Series, Index, _Frame, map_partitions,\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/dask/dataframe/core.py\", line 19, in <module>\r\n    from .. import array as da\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/dask/array/__init__.py\", line 9, in <module>\r\n    from .routines import (take, choose, argwhere, where, coarsen, insert,\r\n  File \"/Applications/anaconda2/lib/python2.7/site-packages/dask/array/routines.py\", line 256, in <module>\r\n    @wraps(np.matmul)\r\n  File \"/Applications/anaconda2/lib/python2.7/functools.py\", line 33, in update_wrapper\r\n    setattr(wrapper, attr, getattr(wrapped, attr))\r\nAttributeError: 'numpy.ufunc' object has no attribute '__module__'\r\n$ \r\n", "Hello, the issue is not closed.\n\nHello, after a lot of try I passed that stage, but still having issue with\ntensor flow. below is my error, I am using MAC and python 2.7. Can you\nplease tell me why this error happens?\n\nLast login: Wed Apr 3 20:38:52 on console\n$ cd\n/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/models/research\n$ export PYTHONPATH=$PYTHONPATH:pwd:pwd/slim\n$ python object_detection/builders/model_builder_test.py\nTraceback (most recent call last):\nFile \"object_detection/builders/model_builder_test.py\", line 23, in\nfrom object_detection.builders import model_builder\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/models/research/object_detection/builders/model_builder.py\",\nline 22, in\nfrom object_detection.builders import box_predictor_builder\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/models/research/object_detection/builders/box_predictor_builder.py\",\nline 20, in\nfrom object_detection.predictors import convolutional_box_predictor\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/models/research/object_detection/predictors/convolutional_box_predictor.py\",\nline 22, in\nslim = tf.contrib.slim\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/lazy_loader.py\",\nline 61, in getattr\nmodule = self._load()\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/lazy_loader.py\",\nline 44, in _load\nmodule = importlib.import_module(self.name)\nFile \"/Applications/anaconda2/lib/python2.7/importlib/init.py\", line 37, in\nimport_module\nimport(name)\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/init.py\",\nline 41, in\nfrom tensorflow.contrib import distributions\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/distributions/\ninit.py\", line 44, in\nfrom tensorflow.contrib.distributions.python.ops.estimator import *\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/distributions/python/ops/estimator.py\",\nline 21, in\nfrom tensorflow.contrib.learn.python.learn.estimators.head import\n_compute_weighted_loss\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/\ninit.py\", line 93, in\nfrom tensorflow.contrib.learn.python.learn import *\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/\ninit.py\", line 28, in\nfrom tensorflow.contrib.learn.python.learn import *\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/\ninit.py\", line 30, in\nfrom tensorflow.contrib.learn.python.learn import estimators\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/\ninit.py\", line 302, in\nfrom tensorflow.contrib.learn.python.learn.estimators.dnn import\nDNNClassifier\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py\",\nline 34, in\nfrom tensorflow.contrib.learn.python.learn.estimators import\ndnn_linear_combined\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\",\nline 36, in\nfrom tensorflow.contrib.learn.python.learn.estimators import estimator\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\",\nline 52, in\nfrom tensorflow.contrib.learn.python.learn.learn_io import data_feeder\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/\ninit.py\", line 26, in\nfrom tensorflow.contrib.learn.python.learn.learn_io.dask_io import\nextract_dask_data\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/dask_io.py\",\nline 33, in\nimport dask.dataframe as dd\nFile \"/Applications/anaconda2/lib/python2.7/site-packages/dask/dataframe/\ninit.py\", line 4, in\nfrom .core import (DataFrame, Series, Index, _Frame, map_partitions,\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/dask/dataframe/core.py\",\nline 19, in\nfrom .. import array as da\nFile \"/Applications/anaconda2/lib/python2.7/site-packages/dask/array/init.py\",\nline 9, in\nfrom .routines import (take, choose, argwhere, where, coarsen, insert,\nFile\n\"/Applications/anaconda2/lib/python2.7/site-packages/dask/array/routines.py\",\nline 256, in\n@wraps <https://github.com/wraps>(np.matmul)\nFile \"/Applications/anaconda2/lib/python2.7/functools.py\", line 33, in\nupdate_wrapper\nsetattr(wrapper, attr, getattr(wrapped, attr))\nAttributeError: 'numpy.ufunc' object has no attribute 'module'\n\nOn Wed, Apr 3, 2019 at 5:15 PM ymodak <notifications@github.com> wrote:\n\n> Closed #27376 <https://github.com/tensorflow/tensorflow/issues/27376>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27376#event-2251132815>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AuWqumFL6Ef9Da2HFFcISxnDqz6CdZjmks5vdRnigaJpZM4cVjwb>\n> .\n>\n"]}, {"number": 27375, "title": "ReadBinaryProto in tensorflow/core/platform/env.cc may report success on partial input", "body": "The code is calling proto->ParseFromCodedStream(&coded_stream), without also calling coded_stream.ConsumedEntireMessage(). This can result in ReadBinaryProto reporting success after reading just part of the input stream, returning a partially instantiated proto from data that didn't actually encode the desired proto in binary format.", "comments": []}, {"number": 27374, "title": "\"Import\" exported model for training", "body": "Is it possible to \"import\" an exported model (i.e. **not** from a checkpoint) to continue training? If not: what are the essential parts?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 27373, "title": "Added switch support to tflite.", "body": "This is one of the Issue #27241.", "comments": ["Hi @amitsrivastava78, I'll let @miaout17 respond with more details, but in short, we are actively working on more general control flow support which will obiate the need for this kind of change.\r\n\r\nLet's discuss this more offline, in particular, whenever you plan on landing new features/operators/etc..., it would be better to let us know in advance before you start working on said feature/operator. That is, design/detailed discussion should happen before the pull request.", "@jdduke thanks for the comments, i will wait for @miaout17 to respond, Yes i think it would be great if we can discuss in advance the new operator/features detail design, do you have any suggestions as to where we can discuss this ? \r\n\r\nRegards\r\nAmit", "Can one of the admins verify this patch?", "See https://github.com/tensorflow/tensorflow/issues/28485 for the generalized control flow proposal. Feedback welcome."]}, {"number": 27372, "title": "TF Lite zeros_like_test death test case added", "body": "Unsupported validation test case added for zeros_like operator.", "comments": []}, {"number": 27371, "title": "TfLite unpack_test test case refactor", "body": "Refactor existing test case to simplified version to support additional test case in future.", "comments": []}, {"number": 27370, "title": "TfLite tile_test test case refactor", "body": "Refactor existing test case to simplified version to support additional test case in future.", "comments": []}]