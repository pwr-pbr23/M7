[{"number": 5423, "title": "'module' object has no attribute 'Default' [Pip Installation, Ubuntu 14.04, cuda 8.0, cudnn 5.1]", "body": "I just use **Pip Installation** method to install tensorflow 0.11 on ubuntu 14.04. And I installed cuda8.0, cudnn 5.1. The installation went well, but when I tried to import tensorflow in python, the following error occurred.\r\n```bash\r\n>>> import tensorflow\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 23, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 53, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py\", line 9, in <module>\r\n    from google.protobuf import symbol_database as _symbol_database\r\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/symbol_database.py\", line 164, in <module>\r\n    _DEFAULT = SymbolDatabase(pool=descriptor_pool.Default())\r\nAttributeError: 'module' object has no attribute 'Default'\r\n```\r\nHow to solve this problem? Thanks.", "comments": ["Looks like protobufs is having trouble loading. Make sure you are running pb3... i.e.\n`pip install --upgrade protobu`.\n", "@aselle Thanks for your reply. And I have removed the original protobuf 2 and installed the latest protobuf 3.1.0 from source. But I still get the same error. What might be wrong?\n", "@aselle  And I used this whl:\n\n``` bash\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\n```\n", "I have solved my problem. It turns out that even though I  upgraded to the latest protobuf 3.1.0, there are still some remaining protobuf 3.0.0 files in my system. More specifically, protobuf 3.1.0 was installed to /usr/local/lib/python2.7/dist-packages, and the original protobuf 3.0.0 was installed in /usr/lib/python2.7/dist-packages. After I manually deleted those files, I can import tensorflow.\n", "Hello Guys, Thank you, it solved my issue with tensorflow 0.11 and my own compilation with gpu.\r\n", "@CTTC  I have also solved my problem by your method, thanks!"]}, {"number": 5422, "title": "Why my TF is much slower than Theano?", "body": "Using keras,TF cost 10s for each epoch,and Theano only cost 2s!\r\nI'm using Ubuntu16.04 sever,GTX1060,CUDA8.0,cudnn5.1\r\n~$ python\r\nPython 2.7.12 (default, Jul  1 2016, 15:12:24) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\n>>> import tensorflow as tf\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\n>>> tf\r\n<module 'tensorflow' from '/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.pyc'>\r\n", "comments": ["Maybe the issue in Keras in that case?\n", "@QuantumLiu, could you provide a reproducible full example. There are many possible performance pitfalls in using TensorFlow. A layer like Keras can only make those pitfalls even more plentiful. We definitely are interested in always improving our performance and helping users understand how to get great performance, but without any details, there is almost nothing we can do to help. Thanks.\n", "@fchollet, ccing you for future posts.\n", "I've opened an issue in the Keras repo where I experienced the same with a MNIST example. \nhttps://github.com/fchollet/keras/issues/4287\nIt seems that using Keras + Theano without cnmem makes the performance just as \"slow\" as Keras + Tensorflow.\nAdditionally, the GPU Utilization is at 10-25% without cnmem in theano, and 20% in tensorflow all the time. With cnmem on, the utilization is around 80%, and becomes around 5 times faster per epoch.\n\nI will try to reproduce this tomorrow without Keras and just Tensorflow.\n", "Alright, please post when you have more information. Thanks!\n", "MLP MNIST with native TF:\n\n```\nMean epoch time: 1.82s\n~20-45% gpu util\n```\n\nKeras with TF backend and native TF code:\n\n```\nMean epoch time: 1.94s\n~20-45% gpu util\n```\n\nKeras with TF backend and Keras sequential model:\n\n```\nMean epoch time: 3.22s\n~30% gpu util\n```\n\nKeras with Theano backend and Keras sequential model **without cnmem**:\n\n```\nMean epoch time: 4.86s\n~20% gpu util\n```\n\nKeras with Theano backend and Keras sequential model **with cnmem 90%**:\n\n```\nMean epoch time: 1.45s\n~80% gpu util\n```\n\nOn a Tesla K20, using the same dataset (55000, 784)\n\nLooks like Keras' sequential model is just slowing things down, and if you use Tensorflow's native code within Keras, the performance decrease is negligible. Also cnmem results in a huge performance boost.\n", "@aselle  thank you!\n`from keras.models import Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Input, Merge\nfrom keras.layers import Convolution2D, MaxPooling2D,AveragePooling2D\nfrom keras.optimizers import SGD\n\ninput=Input(shape=(1, 50, 125))\nconv1=Convolution2D(32, 5, 5, border_mode='valid')(input)\npool1=MaxPooling2D(pool_size=(2, 2))(conv1)\nrelu1=Activation('relu')(pool1)\n\nconv2=Convolution2D(32, 5, 5, border_mode='valid')(relu1)\npool2=AveragePooling2D(pool_size=(2, 2))(conv2)\nrelu2=Activation('relu')(pool2)\n\nconv3=Convolution2D(32, 3, 3, border_mode='valid')(relu2)\npool3=AveragePooling2D(pool_size=(2, 2))(conv3)\nrelu3=Activation('relu')(pool3)\n\nflatten=Flatten()(relu3)\nfc1=Dense(512)(flatten)\n\nfc21=Dense(23,activation='softmax')(fc1)\nfc22=Dense(23,activation='softmax')(fc1)\nfc23=Dense(23,activation='softmax')(fc1)\nfc24=Dense(23,activation='softmax')(fc1)\n#fc2=Merge(layers=[fc21,fc22,fc23,fc24],mode='concat')\nfc2=Merge(mode='concat')([fc21,fc22,fc23,fc24])\nOCRnet=Model(input=input,output=fc2)\n\nsgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\nOCRnet.compile(loss='mean_squared_error',optimizer=sgd)\n\nimport numpy as np\nX_train=np.ones((10000,1,50,125),'float32')\n\nY_train=np.ones((10000,92),'float32')\n\nOCRnet.fit(X_train, Y_train, batch_size=32, nb_epoch=100,validation_split=0.1,verbose=2)\n`\n", "@QuantumLiu try defining your model using Tensorflow tensors as described in the first section here: https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html\n\nThis is likely an issue with Keras, not Tensorflow\n", "Please try this @QuantumLiu , and let us know if this solves the issue.\n", "@QuantumLiu generally TensorFlow has been optimized for large models/large data, and it takes some work to make it efficient for tiny models/tiny data. In the case of MNIST, the issue the common source of bottlenecks is the data transfer between Python runtime and TensorFlow runtime. There are at least 2 copies incurred -- copying from Python to TensorFlow CPU, and then internally, copying data from CPU to GPU. Since these copies are blocking, things are not efficient, and you can improve things by pre-loading data in parallel\u00a0using queues/input pipelines. So the thing to check is how `fit` is implemented in Keras and how efficient it is with data transfers. One caveat for pre-loading, it is inefficient for tiny datasets/models like MNIST because Python can not switch threads [fast enough](http://stackoverflow.com/questions/39840323/benchmark-of-howto-reading-data/39842628#39842628)\n", "Closing due to lack of activity.", "Same thing here. On my particular model with 35M parameters, keras with tf backend and sequential model takes around 33 s. per epoch. With theano backend and cumem at 80%, each epoch takes 16 s.\r\n\r\nMy images are also bigger than MNIST they're 80*80."]}, {"number": 5421, "title": "fix windows gpu build", "body": "", "comments": ["@guschmue, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener to be a potential reviewer.\n", "Can one of the admins verify this patch?\n", "This fix helps me! Thank you.\n", "@tensorflow-jenkins test this please\n", "Wonder if a comment makes sense for the T(2) thing. It is very common in this file and was only missing in the 2 places I changed.\n\nI looked at the CI failure: the file must be read only for some reason. I remember seeing this 2 month back but not since. Never found out the root cause.\n22:41:30          C:/tf_jenkins/home/workspace/tensorflow-pull-requests-windows-cmake/tensorflow/contrib/cmake/build/tensorflow/core/lib/core/error_codes.pb.cc: Permission denied\n22:41:30     19>C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \n", "@tensorflow-jenkins test this please.\n"]}, {"number": 5420, "title": "fixed the equation of the sequence increase", "body": "", "comments": ["@XuesongYang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener to be a potential reviewer.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@tensorflow-jenkins test this please.\n", "@XuesongYang, can you please sign the CLA? Thanks.\n", "Hi guys,\n\nI just signed it. Thanks.\n\nBest\n\nXuesong\n\nOn Sat, Nov 5, 2016 at 6:07 PM, googlebot notifications@github.com wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n> \n> \ud83d\udcdd _Please visit https://cla.developers.google.com/\n> https://cla.developers.google.com/ to sign._\n> \n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> \n> ## verify. Thanks.\n> - If you've already signed a CLA, it's possible we don't have your\n>   GitHub username or you're using a different email address. Check your\n>   existing CLA data https://cla.developers.google.com/clas and verify\n>   that your email is set on your git commits\n>   https://help.github.com/articles/setting-your-email-in-git/.\n> - If you signed the CLA as a corporation, please let us know the\n>   company's name.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5420#issuecomment-258648228,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABkgTZWzlkfoyB1eo1HhkkPl5-KXtjTIks5q7Qw7gaJpZM4KqaWC\n> .\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "PR merged. Thank you, @XuesongYang !\n"]}, {"number": 5419, "title": "Broken build on gcc 6.2.1", "body": "ERROR: /XXX/tensorflow/core/kernels/BUILD:2632:1: C++ compilation of rule '//tensorflow/core/kernels:quantized_ops' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-canonical-system-headers ... (remaining 125 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/core/kernels/quantize_op.cc: In instantiation of 'void tensorflow::QuantizeV2Op<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = Eigen::QInt16]':\r\ntensorflow/core/kernels/quantize_op.cc:166:1:   required from here\r\ntensorflow/core/kernels/quantize_op.cc:116:33: error: no matching function for call to 'std::function<float(float)>::function(<unresolved overloaded function type>)'\r\n                 .unaryExpr(std::function<float(float)>(round))\r\n                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/framework/op.h:19:0,\r\n                 from tensorflow/core/kernels/quantize_op.cc:20:\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:2115:7: note: candidate: template<class _Functor, class, class> std::function<_Res(_ArgTypes ...)>::function(_Functor)\r\n       function<_Res(_ArgTypes...)>::\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:2115:7: note:   template argument deduction/substitution failed:\r\ntensorflow/core/kernels/quantize_op.cc:116:33: note:   couldn't deduce template parameter '_Functor'\r\n                 .unaryExpr(std::function<float(float)>(round))\r\n                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/framework/op.h:19:0,\r\n                 from tensorflow/core/kernels/quantize_op.cc:20:\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1897:7: note: candidate: std::function<_Res(_ArgTypes ...)>::function(std::function<_Res(_ArgTypes ...)>&&) [with _Res = float; _ArgTypes = {float}]\r\n       function(function&& __x) : _Function_base()\r\n       ^~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1897:7: note:   no known conversion for argument 1 from '<unresolved overloaded function type>' to 'std::function<float(float)>&&'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:2101:5: note: candidate: std::function<_Res(_ArgTypes ...)>::function(const std::function<_Res(_ArgTypes ...)>&) [with _Res = float; _ArgTypes = {float}]\r\n     function<_Res(_ArgTypes...)>::\r\n     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:2101:5: note:   no known conversion for argument 1 from '<unresolved overloaded function type>' to 'const std::function<float(float)>&'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1877:7: note: candidate: std::function<_Res(_ArgTypes ...)>::function(std::nullptr_t) [with _Res = float; _ArgTypes = {float}; std::nullptr_t = std::nullptr_t]\r\n       function(nullptr_t) noexcept\r\n       ^~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1877:7: note:   no known conversion for argument 1 from '<unresolved overloaded function type>' to 'std::nullptr_t'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1870:7: note: candidate: std::function<_Res(_ArgTypes ...)>::function() [with _Res = float; _ArgTypes = {float}]\r\n       function() noexcept\r\n       ^~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1870:7: note:   candidate expects 0 arguments, 1 provided\r\ntensorflow/core/kernels/quantize_op.cc: In instantiation of 'void tensorflow::QuantizeV2Op<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = Eigen::QUInt16]':\r\ntensorflow/core/kernels/quantize_op.cc:166:1:   required from here\r\ntensorflow/core/kernels/quantize_op.cc:116:33: error: no matching function for call to 'std::function<float(float)>::function(<unresolved overloaded function type>)'\r\n                 .unaryExpr(std::function<float(float)>(round))\r\n                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/framework/op.h:19:0,\r\n                 from tensorflow/core/kernels/quantize_op.cc:20:\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:2115:7: note: candidate: template<class _Functor, class, class> std::function<_Res(_ArgTypes ...)>::function(_Functor)\r\n       function<_Res(_ArgTypes...)>::\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:2115:7: note:   template argument deduction/substitution failed:\r\ntensorflow/core/kernels/quantize_op.cc:116:33: note:   couldn't deduce template parameter '_Functor'\r\n                 .unaryExpr(std::function<float(float)>(round))\r\n                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/framework/op.h:19:0,\r\n                 from tensorflow/core/kernels/quantize_op.cc:20:\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1897:7: note: candidate: std::function<_Res(_ArgTypes ...)>::function(std::function<_Res(_ArgTypes ...)>&&) [with _Res = float; _ArgTypes = {float}]\r\n       function(function&& __x) : _Function_base()\r\n       ^~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1897:7: note:   no known conversion for argument 1 from '<unresolved overloaded function type>' to 'std::function<float(float)>&&'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:2101:5: note: candidate: std::function<_Res(_ArgTypes ...)>::function(const std::function<_Res(_ArgTypes ...)>&) [with _Res = float; _ArgTypes = {float}]\r\n     function<_Res(_ArgTypes...)>::\r\n     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:2101:5: note:   no known conversion for argument 1 from '<unresolved overloaded function type>' to 'const std::function<float(float)>&'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1877:7: note: candidate: std::function<_Res(_ArgTypes ...)>::function(std::nullptr_t) [with _Res = float; _ArgTypes = {float}; std::nullptr_t = std::nullptr_t]\r\n       function(nullptr_t) noexcept\r\n       ^~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1877:7: note:   no known conversion for argument 1 from '<unresolved overloaded function type>' to 'std::nullptr_t'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1870:7: note: candidate: std::function<_Res(_ArgTypes ...)>::function() [with _Res = float; _ArgTypes = {float}]\r\n       function() noexcept\r\n       ^~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1870:7: note:   candidate expects 0 arguments, 1 provided\r\ntensorflow/core/kernels/quantize_op.cc: In instantiation of 'void tensorflow::QuantizeV2Op<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = Eigen::QInt8]':\r\ntensorflow/core/kernels/quantize_op.cc:166:1:   required from here\r\ntensorflow/core/kernels/quantize_op.cc:116:33: error: no matching function for call to 'std::function<float(float)>::function(<unresolved overloaded function type>)'\r\n                 .unaryExpr(std::function<float(float)>(round))\r\n                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/framework/op.h:19:0,\r\n                 from tensorflow/core/kernels/quantize_op.cc:20:\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:2115:7: note: candidate: template<class _Functor, class, class> std::function<_Res(_ArgTypes ...)>::function(_Functor)\r\n       function<_Res(_ArgTypes...)>::\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:2115:7: note:   template argument deduction/substitution failed:\r\ntensorflow/core/kernels/quantize_op.cc:116:33: note:   couldn't deduce template parameter '_Functor'\r\n                 .unaryExpr(std::function<float(float)>(round))\r\n                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/framework/op.h:19:0,\r\n                 from tensorflow/core/kernels/quantize_op.cc:20:\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1897:7: note: candidate: std::function<_Res(_ArgTypes ...)>::function(std::function<_Res(_ArgTypes ...)>&&) [with _Res = float; _ArgTypes = {float}]\r\n       function(function&& __x) : _Function_base()\r\n       ^~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1897:7: note:   no known conversion for argument 1 from '<unresolved overloaded function type>' to 'std::function<float(float)>&&'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:2101:5: note: candidate: std::function<_Res(_ArgTypes ...)>::function(const std::function<_Res(_ArgTypes ...)>&) [with _Res = float; _ArgTypes = {float}]\r\n     function<_Res(_ArgTypes...)>::\r\n     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:2101:5: note:   no known conversion for argument 1 from '<unresolved overloaded function type>' to 'const std::function<float(float)>&'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1877:7: note: candidate: std::function<_Res(_ArgTypes ...)>::function(std::nullptr_t) [with _Res = float; _ArgTypes = {float}; std::nullptr_t = std::nullptr_t]\r\n       function(nullptr_t) noexcept\r\n       ^~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1877:7: note:   no known conversion for argument 1 from '<unresolved overloaded function type>' to 'std::nullptr_t'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1870:7: note: candidate: std::function<_Res(_ArgTypes ...)>::function() [with _Res = float; _ArgTypes = {float}]\r\n       function() noexcept\r\n       ^~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1870:7: note:   candidate expects 0 arguments, 1 provided\r\ntensorflow/core/kernels/quantize_op.cc: In instantiation of 'void tensorflow::QuantizeV2Op<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = Eigen::QUInt8]':\r\ntensorflow/core/kernels/quantize_op.cc:166:1:   required from here\r\ntensorflow/core/kernels/quantize_op.cc:116:33: error: no matching function for call to 'std::function<float(float)>::function(<unresolved overloaded function type>)'\r\n                 .unaryExpr(std::function<float(float)>(round))\r\n                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/framework/op.h:19:0,\r\n                 from tensorflow/core/kernels/quantize_op.cc:20:\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:2115:7: note: candidate: template<class _Functor, class, class> std::function<_Res(_ArgTypes ...)>::function(_Functor)\r\n       function<_Res(_ArgTypes...)>::\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:2115:7: note:   template argument deduction/substitution failed:\r\ntensorflow/core/kernels/quantize_op.cc:116:33: note:   couldn't deduce template parameter '_Functor'\r\n                 .unaryExpr(std::function<float(float)>(round))\r\n                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/framework/op.h:19:0,\r\n                 from tensorflow/core/kernels/quantize_op.cc:20:\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1897:7: note: candidate: std::function<_Res(_ArgTypes ...)>::function(std::function<_Res(_ArgTypes ...)>&&) [with _Res = float; _ArgTypes = {float}]\r\n       function(function&& __x) : _Function_base()\r\n       ^~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1897:7: note:   no known conversion for argument 1 from '<unresolved overloaded function type>' to 'std::function<float(float)>&&'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:2101:5: note: candidate: std::function<_Res(_ArgTypes ...)>::function(const std::function<_Res(_ArgTypes ...)>&) [with _Res = float; _ArgTypes = {float}]\r\n     function<_Res(_ArgTypes...)>::\r\n     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:2101:5: note:   no known conversion for argument 1 from '<unresolved overloaded function type>' to 'const std::function<float(float)>&'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1877:7: note: candidate: std::function<_Res(_ArgTypes ...)>::function(std::nullptr_t) [with _Res = float; _ArgTypes = {float}; std::nullptr_t = std::nullptr_t]\r\n       function(nullptr_t) noexcept\r\n       ^~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1877:7: note:   no known conversion for argument 1 from '<unresolved overloaded function type>' to 'std::nullptr_t'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1870:7: note: candidate: std::function<_Res(_ArgTypes ...)>::function() [with _Res = float; _ArgTypes = {float}]\r\n       function() noexcept\r\n       ^~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/../../../../include/c++/6.2.1/functional:1870:7: note:   candidate expects 0 arguments, 1 provided", "comments": ["Also found by me when compiling using gcc 6.2.0\n", "@benoitsteiner, it looks like an Eigen related compilation problem.\n", "Replacing the problematic line (tensorflow/core/kernels/quantize_op.cc:116) with\n`.unaryExpr([] (float r) -> float { return round(r); })`\nworks.\n", "What happens when you pass &::round? Or &std::round?\n", "I rewrote the code to avoid calling std::round in https://github.com/tensorflow/tensorflow/commit/fb60813c2f14249f316c3e535dcb994e75a6c73c.\n\nUnfortunately I don't have gcc 6.2 installed so I couldn't verify that this solves the problem. Can you try and let me know if it works for you ?\n", "@benoitsteiner It works with gcc-6.2.0.\n", "Thanks, closing this issue\n"]}, {"number": 5418, "title": "Pylint `disable` fix", "body": "I have added `# pylint: enable=wildcard-import` wherever I found an unclosed `disable`, but only for the `wildcard-import`. There are many more for other parameter, including the custom definitions (`g-...` parameters).", "comments": ["Can one of the admins verify this patch?\n", "@zafartahirov, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @vrv and @tensorflower-gardener to be potential reviewers.\n", "@zafartahirov, can you please squash your six commits into a single one? Thanks.\n", "@caisq I thought it is automatically squashed once it is merged, according to comment by @ilblackdragon [here](https://github.com/tensorflow/tensorflow/pull/5315#pullrequestreview-6716529)\nI can squash it if it is the requirement\n", "@caisq we can squash for our users, it's better for PR analysis to have them not squash commits.\n", "@vrv done -- Please, note that reenabling the `pylint enable=...` might cause the pylint checks to fail if the `disable` line was suppressing the fail before\n", "@tensorflow-jenkins test this please\n\n(testing pylint)\n", "There is a merge conflict for `tensorflow/contrib/framework/python/framework/__init__.py`, I am not sure why it is a conflict -- it is a single line addition in the end of the file\n", "I'm not sure why it's a conflict, but if you rebase your changes on HEAD it should go away.\n", "(after you rebase I can kick off tests and merge.  Thanks for these cleanups!)\n", "Done, I think the conflicts are resolved -- not sure what caused them. There are some parameters left that are not closed -- I was only explicitly searching for `wildcard-import` ones.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 5417, "title": "1% GPU usage and slow training times after unknown DSO update?", "body": "Tensorflow with CUDA was working fine. Cross-entropy loss was going to zero (CIFAR-10 and a very simple CNN), but training error stayed around the same as random chance. I mention this in case it may be related to my issue.\r\n\r\nIn between runs, not 5 minutes after it was just working, I mysteriously got this error message about my video driver: \r\n\r\n`kernel version 361.42.0 does not match DSO version 367.57.0`\r\n\r\nI never updated anything, so that's strange. I update my video driver using `apt-get` and the Ubuntu ppa repo, then restart. I didn't check what version the video driver was before the update but I assume it was 361.42?\r\n\r\nThe error message is gone, but **training is now an order of magnitude slower with ~1% GPU usage and ~5% CPU usage**. `nvidia-smi` indicates 90% memory usage, which means the model is in memory, but what before took ~0.4 seconds per batch is now taking 4+ seconds per batch.\r\n\r\nI tried re-installing CUDA and cuDNN from official sources but no change.\r\nCUDA bandwidth test and deviceQuery results normal.\r\n\r\nUbuntu 16.04\r\nTensorflow 0.10.0 (built from source w/ CC 3.5 and 5.0)\r\nnvidia Quadro K2200\r\nCUDA 8.0\r\ncuDNN 5.1.5\r\nPython 2.7", "comments": ["Did you upgrade your CUDA driver? It looks like it isn't new enough.\n", "@aselle I did re-install the latest version of CUDA, 8.0.44 verified through `nvcc --version`.\n", "this looks to be a configuration issue, this can be closed.\n"]}, {"number": 5416, "title": "Possible Bug - Varying session run times when parallelized across GPUs on one node", "body": "Hello tensorflow team!\r\n\r\nI have been trying to parallelize my 3D convolution network across 2 GPUs on the same node (using data parallelism), and have found that some of my session runs took significantly longer than others.  Upon closer investigation using your timeline feature, I found that sometimes GPU1 would wait a certain amount of time before starting to run its convolutions even as GPU0 was happily chugging along. In the most extreme case, execution would happen essentially sequentially, doubling my runtime!  \r\n\r\nI am attaching screenshots of my timelines from three sessions to show you what I mean...\r\n\r\nIn the first one, everything is fine, it is executing both towers in parallel:\r\n![run55](https://cloud.githubusercontent.com/assets/177576/20033120/cc17df8c-a356-11e6-8ef3-55cfdeb9d244.png)\r\nIn the second, I see the \"sequential\" behaviour:\r\n![run56](https://cloud.githubusercontent.com/assets/177576/20033121/cd4d9e96-a356-11e6-8ef1-091edd981fdf.png)\r\nIn the last one, I see something in between:\r\n![run30](https://cloud.githubusercontent.com/assets/177576/20033136/2b7f68b4-a357-11e6-9eb5-92f9a30a5fa3.png)\r\n\r\nI know this happens even when I am not logging metadata because I still see a large spread of runtime distributions without logging on (see parallel-nometadata.txt at the bottom).  Also, dumping the GraphDef proto at each step tells me that the ops are correctly assigned to the different GPUs (I have attached those, as well as my timeline files, at the bottom).\r\n\r\nI actually briefly talked about this with @mrry in person a little over a week ago, and we did not find anything obviously wrong at first glance.  Any help you could provide would be much appreciated!  \r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nI have not found any related Github or StackOverflow threads.  I have been using the Timeline profiling feature as described [here](https://github.com/tensorflow/tensorflow/issues/1824).\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04.5 LTS (running in a [singularity](http://singularity.lbl.gov) container on a CentOS 6.7 host). \r\n\r\nInstalled version of CUDA and cuDNN: \r\nI am using CUDA 8.0 with NVIDIA driver 367.48, and cuDNN v5.1 . \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\nlibOpenCL.so\r\nlibOpenCL.so.1\r\nlibOpenCL.so.1.0\r\nlibOpenCL.so.1.0.0\r\nlibcublas.so\r\nlibcublas.so.8.0\r\nlibcublas.so.8.0.45\r\nlibcublas_device.a\r\nlibcublas_static.a\r\nlibcudadevrt.a\r\nlibcudart.so\r\nlibcudart.so.8.0\r\nlibcudart.so.8.0.44\r\nlibcudart_static.a\r\nlibcudnn.so\r\nlibcudnn.so.5\r\nlibcudnn.so.5.1.5\r\nlibcudnn_static.a\r\nlibcufft.so\r\nlibcufft.so.8.0\r\nlibcufft.so.8.0.44\r\nlibcufft_static.a\r\nlibcufftw.so\r\nlibcufftw.so.8.0\r\nlibcufftw.so.8.0.44\r\nlibcufftw_static.a\r\nlibcuinj64.so\r\nlibcuinj64.so.8.0\r\nlibcuinj64.so.8.0.44\r\nlibculibos.a\r\nlibcurand.so\r\nlibcurand.so.8.0\r\nlibcurand.so.8.0.44\r\nlibcurand_static.a\r\nlibcusolver.so\r\nlibcusolver.so.8.0\r\nlibcusolver.so.8.0.44\r\nlibcusolver_static.a\r\nlibcusparse.so\r\nlibcusparse.so.8.0\r\nlibcusparse.so.8.0.44\r\nlibcusparse_static.a\r\nlibnppc.so\r\nlibnppc.so.8.0\r\nlibnppc.so.8.0.44\r\nlibnppc_static.a\r\nlibnppi.so\r\nlibnppi.so.8.0\r\nlibnppi.so.8.0.44\r\nlibnppi_static.a\r\nlibnppial.so\r\nlibnppial.so.8.0\r\nlibnppial.so.8.0.44\r\nlibnppicc.so\r\nlibnppicc.so.8.0\r\nlibnppicc.so.8.0.44\r\nlibnppicom.so\r\nlibnppicom.so.8.0\r\nlibnppicom.so.8.0.44\r\nlibnppidei.so\r\nlibnppidei.so.8.0\r\nlibnppidei.so.8.0.44\r\nlibnppif.so\r\nlibnppif.so.8.0\r\nlibnppif.so.8.0.44\r\nlibnppig.so\r\nlibnppig.so.8.0\r\nlibnppig.so.8.0.44\r\nlibnppim.so\r\nlibnppim.so.8.0\r\nlibnppim.so.8.0.44\r\nlibnppist.so\r\nlibnppist.so.8.0\r\nlibnppist.so.8.0.44\r\nlibnppisu.so\r\nlibnppisu.so.8.0\r\nlibnppisu.so.8.0.44\r\nlibnppitc.so\r\nlibnppitc.so.8.0\r\nlibnppitc.so.8.0.44\r\nlibnpps.so\r\nlibnpps.so.8.0\r\nlibnpps.so.8.0.44\r\nlibnpps_static.a\r\nlibnvToolsExt.so\r\nlibnvToolsExt.so.1\r\nlibnvToolsExt.so.1.0.0\r\nlibnvblas.so\r\nlibnvblas.so.8.0\r\nlibnvblas.so.8.0.44\r\nlibnvgraph.so\r\nlibnvgraph.so.8.0\r\nlibnvgraph.so.8.0.44\r\nlibnvgraph_static.a\r\nlibnvrtc-builtins.so\r\nlibnvrtc-builtins.so.8.0\r\nlibnvrtc-builtins.so.8.0.44\r\nlibnvrtc.so\r\nlibnvrtc.so.8.0\r\nlibnvrtc.so.8.0.44\r\nstubs\r\n```\r\n\r\nI installed tensorflow using the 0.11.0rc2-gpu tag on docker hub.\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nHere is the code I used to generate all the data I am sharing.\r\n\r\n```\r\nimport os\r\nimport time\r\nimport timeit\r\n\r\nimport google\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ngrid_size = 9\r\nchannel_size = 4\r\nbatch_size = 2**9\r\nnum_convs = 8\r\nlog_metadata = True\r\ntowers = 2\r\nnum_train_batches = 100\r\n\r\n\r\ndef define_model(towers):\r\n    tower_outs = []\r\n    for i in range(0, towers):\r\n        with tf.device('/gpu:{:}'.format(i)):\r\n            with tf.name_scope('TOWER_{:}'.format(i)):\r\n                tower_outs.append(define_tower(i))\r\n                tf.get_variable_scope().reuse_variables()\r\n    return tower_outs\r\n\r\n\r\ndef define_tower(gpu_num):\r\n    x = tf.placeholder(\r\n        tf.float32,\r\n        shape=[None, grid_size, grid_size, grid_size, channel_size],\r\n        name='grid')\r\n\r\n    num_inputs = channel_size\r\n    for i in range(num_convs):\r\n        with tf.variable_scope(\"conv{:d}\".format(i)):\r\n            weights = tf.get_variable(\r\n                \"weights\",\r\n                [3, 3, 3, num_inputs, 32])\r\n            x = tf.nn.conv3d(x, weights, [1, 1, 1, 1, 1], 'SAME')\r\n            num_inputs = 32\r\n\r\n    return x\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    # Define model.\r\n    tower_outs = define_model(towers)\r\n\r\n    # Create inputs.\r\n    feed_dict = {}\r\n    for i in range(towers):\r\n        feed_dict['TOWER_{:}/grid:0'.format(i)] = np.random.rand(\r\n            batch_size, grid_size, grid_size, grid_size, channel_size)\r\n\r\n    # Prepare for logging.\r\n    if log_metadata:\r\n        run_options = tf.RunOptions(\r\n            trace_level=tf.RunOptions.FULL_TRACE,\r\n            output_partition_graphs=True)\r\n        run_metadata = tf.RunMetadata()\r\n        kwargs = {'options': run_options,\r\n                  'run_metadata': run_metadata}\r\n\r\n        curr_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\r\n        if log_metadata:\r\n            out_dir = curr_time\r\n            os.mkdir(out_dir)\r\n    else:\r\n        kwargs = {}\r\n\r\n    # Run model.\r\n    train_learning_times = []\r\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\r\n        tf.initialize_all_variables().run()\r\n        for i in range(num_train_batches):\r\n            learning_time_start = timeit.default_timer()\r\n            _ = sess.run(tower_outs, feed_dict=feed_dict, **kwargs)\r\n            train_learning_times.append(timeit.default_timer() -\r\n                                        learning_time_start)\r\n\r\n            if log_metadata:\r\n                from tensorflow.python.client import timeline\r\n                tl = timeline.Timeline(run_metadata.step_stats)\r\n                ctf = tl.generate_chrome_trace_format()\r\n                with open('{}/timeline_parallel_{}.json'\r\n                          .format(curr_time, i), 'w') as f:\r\n                    f.write(ctf)\r\n                with open('{}/run_metadata_{}.txt'\r\n                          .format(curr_time, i), 'w') as f:\r\n                    f.write(\r\n                        google.protobuf.text_format.MessageToString(\r\n                            run_metadata))\r\n\r\n    # Print stats\r\n    print 'Histogram counts: {}'.format(\r\n        np.histogram(train_learning_times[2:])[0])\r\n    print 'Histogram edges: {}'.format(\r\n        np.histogram(train_learning_times[2:])[1])\r\n    print 'Median time for learning: {:5.2f}'.format(\r\n        np.median(train_learning_times))\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nNo other solutions tried.\r\n\r\n\r\n\r\n\r\n### Logs or other output that would be helpful\r\nDump of GraphDef and timeline json for each of 100 runs.\r\n\r\n[2016-11-05-18-52-32.zip](https://github.com/tensorflow/tensorflow/files/573462/2016-11-05-18-52-32.zip)\r\n\r\nOutput of code with log_metadata set to True:\r\n[parallel.txt](https://github.com/tensorflow/tensorflow/files/573464/parallel.txt)\r\n\r\nOutput of code with log_metadata set to False:\r\n[parallel-nometadata.txt](https://github.com/tensorflow/tensorflow/files/573465/parallel-nometadata.txt)", "comments": ["Thanks for making such a detailed bug report, @raphtown!\n\nThe first thing I noticed when looking at the timelines is that all of the GPU kernel profiling events _appear_ to be running on `/gpu:0`... even if they've been issued by the TensorFlow device `/gpu:1`. However, this seems to be a red herring, because there's a [`TODO` comment in `gpu_tracer.cc` that reveals that these prefixes are inaccurate with multiple devices](https://github.com/tensorflow/tensorflow/blob/a6b33c30926b7a11a1ec0a18d01e6f8852561376/tensorflow/core/common_runtime/gpu/gpu_tracer.cc#L561). @prb12, do you have a sense of how easy it would be to plumb the appropriate information through here? (I'm guessing that we need to add some reverse map from device contexts (or some other object that we get in a CUPTI callback) back to TensorFlow devices....)\n\nThe variability in runtime appears to be caused by (usually) `/gpu:1` issuing its first kernel a long time after `/gpu:0` starts issuing kernels... and sometimes `/gpu:1` doesn't get started until `/gpu:0` is finished (and has sync'd its stream). This suggests to me that there's some mutex (in the stream executor? the GPU driver?) being held, or a single worker thread is running a blocking operation and prevent the other device from issuing kernels. I'm not sure how likely either of these scenarios is... @zheng-xq, do you have any ideas?\n", "@mrry and I had a look at some of these traces this morning.  There definitely seems to be a problem here, and I'm not sure when it crept in.\n\nFirst, @mrry is correct that the gpu tracer in the open source tree is hardwired to report `/gpu:0` - this should be a simple fix, which can probably be done at the same time that GPU tracing is enabled for the distributed runtime.\n\nGetting back to the bug - there does appear to be some unexpected and undesirable serialization of op dispatch on the GPU devices here (e.g. the dispatch of the Conv3D ops on different GPU devices do not appear to run concurrently!).  I'm not 100% sure what is going on, but I do know that there were some changes to the threadpool usage for GPU op dispatch a while ago (to deliberately avoid using multiple threads to enqueue ops on the same GPU stream).  \n\nAlso, in the build you are using, the `_SINK` NoOp (which is the last op to run on each device) calls `StreamExecutor::SynchronizeAllActivity` to ensure that all work is done before turning off the GPU tracer.  Looking at the implementation, this does actually appear to a) synchronize the CUDA Context and b) calls `BlockOnThreadExecutor(background_threads_.get());` on an internal threadpool.\n\nIn later builds the position of this call to SyncAll() had been moved slightly - but I don't think this would affect the behaviour (it would make the Timeline look slightly different - in that the _SINK NoOp would not appear to take as long)\n\nIt might be worth trying to explicitly increase the `inter-op-parallalelism`, (in your `ConfigProto`) but I suspect the problem is more subtle than this....\n\nWhat concerns me is that one of your traces has **all** the ops on GPU1 executing after all the ops on GPU0 have completed.  In this case there is actually a dependency of the first GPU1 Conv3D on a 'read' of a Variable which is assigned to GPU0.  This will actually require a device to device DMA, and involves multiple GPU streams and some CUDA Event synchronization.  None of this is currently visible in the Timeline (and in fact I don't even see a MEMCPYD2D!)   This worries me a little (and I really think that we really need to change the instrumentation for Send/Recv ops so that they are more visible in the Timeline!)\n", "> Thanks for making such a detailed bug report, @raphtown!  \n\n+1     It's always a pleasure to have a clear bug report with simple repro code.\n", "Hello @mrry, @prb12, and thank you for taking a look into this!\n\nVarying the inter_op_parallelism_threads field in the ConfigProto doesn't seem to do much, except when I set it very high (e.g. 1000) and then more of the runs execute sequentially.\n\nMaybe this will help you debug the issue, but it seems like the number of CPU cores available has a direct impact on if the GPUs execute sequentially or in parallel (I am using the SLURM allocation system which lets me specify how many CPUs to give).  The results I gave in my original bug report were with 2 CPUs available total.  If I increase this amount to 4 CPUs, then the issue disappears.  If I decrease the amount to 1 CPU, then it seems that all runs execute sequentially (worsening the problem).\n\nLastly, I also tested this with 0.9.0, which displayed similar issues.\n", "Also, yes, I would definitely love to have better instrumentation of the Send/Recv ops as well as GPU tracing for distributed tensorflow (which is my next step once I get the single-node case working)!\n", "> Maybe this will help you debug the issue, but it seems like the number of CPU cores available has a direct impact on if the GPUs execute sequentially or in parallel (I am using the SLURM allocation system which lets me specify how many CPUs to give). The results I gave in my original bug report were with 2 CPUs available total. If I increase this amount to 4 CPUs, then the issue disappears. If I decrease the amount to 1 CPU, then it seems that all runs execute sequentially (worsening the problem).\n\nI'm glad you mentioned this.  We were looking at your traces earlier today and very bemused as to why the threadpool dispatching GPU ops appeared to be behaving as though there was only 1-2 threads!  (somebody apparently removed the log message which tells us how many worker threads are configured in each threadpool - but it defaults to the number of \"physical\" cores.)\n\nI can totally understand how this problem occurs with only a couple of threads.  Your workload is very sensitive to the order of op dispatch (due to the way that GPU to GPU memcpys are currently implemented).  If there are only a tiny number of threads available then the executor for one GPU device manages to get all of its ops enqueued on the GPU stream before the other even gets a look in.  The D2D memcpys actually include a stream synchronization - so the destination device of the transfer ends up waiting for all of the previously dispatched ops on the other GPU.  \n\nThe exact behavior is non-deterministic, but it takes very little time to enqueue a handful of ops on GPU0, and if all threadpool workers are busy doing anything else for even a millisecond then it's game over for GPU1! \n\nI think we probably need to improve the D2D transfer code path, but for now I would recommend using at least 4 or 8 cores in a multi-gpu setting (and probably more!)\n", "Hello @prb12, I think I see what you are saying.  When we enqueue GPU1's read of a convolution layer's weights (in this case, from GPU0), it must wait for all of GPU0's previously enqueued ops to finish.  \n\nI can see even in my \"parallel\" cases (such as the first screenshot above) that some of the later GPU1 reads are delayed and wait for GPU0 to finish a couple conv layers, but this is not an issue because GPU1 has enough computation to do in the mean time.\n\nThe nodes I am using have 16 CPUs for 8 GPUs, so I assume this problem will not completely go away but will most likely be minimized for now.\n\nThank you for being so responsive!\n", "> The nodes I am using have 16 CPUs for 8 GPUs, so I assume this problem will not completely go away but will most likely be minimized for now.\n\nYes - you are right.  \n\nThe only way to get nice parallelism is if all of the Variable read and Send ops on GPU 0 and the Recv ops on GPU1 issue before any of the Conv ops start getting enqueued on GPU0.  This is a bit of a lottery currently!    (Alternatively, if you keep the Variables on CPU then you probably won't have this issue, but will incur the cost of a separate DMA to each GPU per step)\n\n@zheng-xq and I talked about a possible workaround this morning.... but it would require recording a CUDA event for each of the Send ops on the source GPU so that the dest GPU could wait for the appropriate point in the execution stream.  Previously we've considered this too expensive.\n", "It does indeed seem like a tricky problem to get the reads all synced up properly.  In any case, after running some tests, it appears the problem is minimized for my 16 CPU / 8 GPU setup.  Assuming there is no further follow-up needed on the tensorflow team's end, I would be fine with closing this issue for now.\n\nThanks for the help :)\n", "Do you see any difference if you keep the variables in CPU?\n\nOn Saturday, November 12, 2016, raphtown notifications@github.com wrote:\n\n> It does indeed seem like a tricky problem to get the reads all synced up\n> properly. In any case, after running some tests, it appears the problem is\n> minimized for my 16 CPU / 8 GPU setup. Assuming there is no further\n> follow-up needed on the tensorflow team's end, I would be fine with closing\n> this issue for now.\n> \n> Thanks for the help :)\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5416#issuecomment-260166949,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABr0fNLPkiaX67Ck3_qUmN2ar_x4Reevks5q9padgaJpZM4KqXs2\n> .\n\n## \n\nSergio\n", "Is there an issue tracking the bug in the tracing code that incorrect reports other GPUs under `/gpu:0`? this can be quite misleading for people looking at profiling results  \r\n\r\non the `master` branch this looks to be a problem still https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_tracer.cc#L566 \r\n\r\n", "@mortada I am closing this based on the original bug.  For the incorrect reports of GPUs issue please open another github issue.  That will make it easier to track and restart the thread.  Thank you."]}, {"number": 5415, "title": "UnsupportedWheel: tensorflow-0.11.0rc2-cp27-cp27mu-linux_x86_64.whl", "body": "Hi all,\r\n\r\nI'm updating my tensorflow from r0.10 to r0.11 or the master, but after I run the cmd:\r\n```\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n```\r\nI just got a `tensorflow-0.11.0rc2-cp27-cp27mu-linux_x86_64.whl` instead of `tensorflow-0.11.0rc2-py2-none-any.whl`. And this file always caused an `UnsupportedWheel` error when I run pip install. \r\n\r\nThis is the pip.log:\r\n```\r\n------------------------------------------------------------\r\n/usr/bin/pip run on Sat Nov  5 16:59:34 2016\r\ntensorflow-0.11.0rc2-cp27-cp27mu-linux_x86_64.whl is not a supported wheel on this platform.\r\nException information:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 122, in main\r\n    status = self.run(options, args)\r\n  File \"/usr/lib/python2.7/dist-packages/pip/commands/install.py\", line 257, in run\r\n    InstallRequirement.from_line(name, None))\r\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 168, in from_line\r\n    raise UnsupportedWheel(\"%s is not a supported wheel on this platform.\" % wheel.filename)\r\nUnsupportedWheel: tensorflow-0.11.0rc2-cp27-cp27mu-linux_x86_64.whl is not a supported wheel on this platform.\r\n```\r\n\r\nOperating system: Ubuntu 14.04\r\nKernel release: 4.4.0-45-generic\r\nCuda toolkit: 8.0.44\r\ncuDNN: 5.1.5\r\nGPU: GTX 1080\r\nbazel version: 0.4.0\r\ngcc and g++: 4.8.5\r\npython: 2.7.6\r\n\r\nI wonder what  `cp27-cp27mu` means and why I can't get an supported .whl file?\r\n ", "comments": ["Same problem with the downloadable on page: https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html\n\nUbuntu 16.04, kernel 4.4.0-45, CUDA 8.0.44, gcc 5.4.0, python 2.7.12 : \n$ sudo -H pip install --upgrade tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl\ntensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.\n$\n\nBut \"parallel\" python 3.5 file works in my python3 setup:\n$ sudo -H pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc2-cp35-cp35m-linux_x86_64.whl\n...\nSuccessfully installed numpy-1.11.2 protobuf-3.0.0 setuptools-28.8.0 tensorflow-0.11.0rc2\n$\n", "Renaming the wheel to `tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl` should address the problem.\nCan you give it a try?\n", "@aselle What's with the tag fight?\n", "@caisq yup that fixes it for me", "The tag fight was due to a bug in github that marked the \"awaiting response\" as coming from the original poster rather than the person that did it (me).", "I'm closing this issue for now. For future reference:\r\nThere is a script that takes care of the pip wheel renaming automatically:\r\n\r\nE.g.,\r\ntensorflow/tools/ci_build/ci_build.sh CPU tensorflow/tools/ci_build/builds/pip.sh CPU\r\ntensorflow/tools/ci_build/ci_build.sh GPU tensorflow/tools/ci_build/builds/pip.sh GPU\r\n\r\nFor more details see:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build"]}, {"number": 5414, "title": "how to run distributed version on mutiple CPUs but not GPUs. ", "body": "when i run scripts like follows:\r\nCUDA_VISIBLE_DEVICES='' python distribute.py --ps_hosts=192.168.1.100:2222 --worker_hosts=192.168.1.100:2224,192.168.1.100:2225 --job_name=ps --task_index=0\r\n\r\nCUDA_VISIBLE_DEVICES='' python distribute.py --ps_hosts=192.168.1.100:2222 --worker_hosts=192.168.1.100:2224,192.168.1.100:2225 --job_name=worker --task_index=0\r\n\r\nCUDA_VISIBLE_DEVICES='' python distribute.py --ps_hosts=192.168.1.100:2222 --worker_hosts=192.168.1.100:2224,192.168.1.100:2225 --job_name=worker --task_index=1 \r\n\r\nterminal will not start , I am confused. when I change one of the CUDA_VISIBLE_DEVICES=''  to CUDA_VISIBLE_DEVICES=0,  it works .  But I need just CPUs without a GPU", "comments": ["Please resubmit your issue, filling in the required information from github template. In particular, provide any error messages and details about your environment.\n"]}, {"number": 5413, "title": "StudentT.cdf() bug", "body": "With the TF0.11 rc2 (also with rc0) I get the following error while trying to evaluate the CDF of the StudentT distribution:\r\n```\r\n----> 1 a.cdf(5.0)\r\n\r\n/home/ilm/.local/lib/python2.7/site-packages/tensorflow/contrib/distributions/python/ops/distribution.pyc in cdf(self, value, name)\r\n    496         values of type `self.dtype`.\r\n    497     \"\"\"\r\n--> 498     self._check_hasattr(self._cdf)\r\n    499     with self._name_scope(name, values=[value]):\r\n    500       value = ops.convert_to_tensor(value, name=\"value\")\r\n\r\nAttributeError: 'StudentT' object has no attribute '_cdf'\r\n\r\n```\r\nIs the CDF of StudentT supported? According to the docs it should:\r\nhttps://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.distributions.html#StudentT\r\n\r\nMWE:\r\n```\r\nimport tensorflow as tf\r\na = tf.contrib.distributions.StudentT(5.0, 0.0, 1.0)\r\na.cdf(5.0)\r\n```", "comments": ["It's not implemented as of nightly.\n\n```\n>>> import tensorflow as tf\n>>> a = tf.contrib.distributions.StudentT(5.0, 0.0, 1.0)\n>>> a.cdf(5.0)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/distribution.py\", line 706, in cdf\n    raise original_exception\nNotImplementedError: cdf is not implemented\n```\n\nContributions are welcome.  From @brianwa84's original checkin:\n\n```\nAdds Student's t distribution to tf.contrib.distributions\nLeaving out the cdf for now, as it requires incomplete beta, not available in eigen at the moment.\n```\n", "Ok thanks, I was confused because the documentation says otherwise.\n\nIt seems Eigen now implements the (regularized) incomplete beta, both for cpu and gpu:\nhttps://eigen.tuxfamily.org/dox-devel/group__CoeffwiseMathFunctions.html\nhttps://bitbucket.org/eigen/eigen/pull-requests/190/add-ternaryfunctors-and-the-betainc/diff\n\nNot sure if it is in stable Eigen yet, have to check.\n", "On Mon, Nov 7, 2016 at 1:00 PM, icouckuy notifications@github.com wrote:\n\n> Ok thanks, I was confused because the documentation says otherwise.\n\nHi there!  Sorry for the confusion and troubles!\n\nWe regard base class distribution as being somewhat like an abstract base\nclass which lists the properties a distribution _might_ have.  In the\nlatest release (and indeed for some time now) a call to an unsupported\nfunction should trigger NotImplementedError or an exception indicating\nsomething along the lines of \"cannot be implemented.\"\n\n> It seems Eigen now implements the (regularized) incomplete beta, both for\n> cpu and gpu:\n> https://eigen.tuxfamily.org/dox-devel/group__CoeffwiseMathFunctions.html\n> https://bitbucket.org/eigen/eigen/pull-requests/190/add-\n> ternaryfunctors-and-the-betainc/diff\n> \n> Not sure if it is in stable Eigen yet, have to check.\n> \n> I opened a bug to implement student's t cdf now that betainc is supported.\n> We should be able to get this in reasonably quickly.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5413#issuecomment-258960948,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABFZtkY_5Q4Zlwnl8_-SeX4Gn__TXYg0ks5q75FqgaJpZM4KqQH4\n> .\n", "Has this been assigned to anyone? I want to start contributing to tensorflow and am looking for a bug to work on.\n", "Go for it, instructions here:\nhttps://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md\n\nBrian Patton | Software Engineer | bjp@google.com | +1 (360) 903-9003\n\nOn Tue, Nov 8, 2016 at 6:36 PM, Finbarr Timbers notifications@github.com\nwrote:\n\n> Has this been assigned to anyone? I want to start contributing to\n> tensorflow and am looking for a bug to work on.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5413#issuecomment-259293843,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AVJZI32PPECgGnxsDThNjITFZbfKmgWiks5q8QeIgaJpZM4KqQH4\n> .\n", "Sounds good. On it. \n", "Looks like this is just closed today."]}, {"number": 5412, "title": "Request: Functionality for obtaining gradients that are internal to a while_loop", "body": "I'd like to manipulate the gradients that are internal to a while loop. The actual purpose is to view / manipulate recurrent-neural-network behavior, but here is a simplified example to get the idea across:\r\n\r\n```\r\nt_0 = tf.constant(0)\r\nx_0 = tf.constant(1.0)\r\nsize = 3\r\nx_ta = tf.TensorArray(tf.float32, size=size)\r\n\r\ndef cond(t, x_prev, x_ta):\r\n    return tf.less(t, size)\r\n\r\ndef body(t, x_prev, x_ta):\r\n    x = 2*x_prev\r\n    x_ta = x_ta.write(t, x)\r\n    return t + 1, x, x_ta\r\n\r\n_, _, x_ta = tf.while_loop(cond, body, [t_0, x_0, x_ta])\r\nx = x_ta.pack()\r\n\r\nloss = x[size-1]\r\n\r\ndloss_dx, = tf.gradients(loss, x)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.initialize_all_variables())\r\n    print(sess.run(dloss_dx))\r\n```\r\n\r\nIn this example, `x[t] = 2*x[t-1]` for all `t`, so we'd like `dloss_dx` to be `[4., 2., 1.]`. We instead get `[ 0.  0.  1.]`. I don't think this is a bug: we're actually taking the derivative with respect to the pack operation, which `x[2]` doesn't actually depend on.\r\n\r\nPlease correct me if I'm wrong, but I think it's currently impossible to obtain `[4., 2., 1.]` in the general case. (One hack here would be to compute gradients within the `while_loop` and then form a cumulative product corresponding to the chain rule, but this won't work with vector-valued `x[t]`'s because there's no way to get the Jacobians).\r\n\r\nBut the gradients with respect to each of these time steps must live somewhere. Can we add a property to the `TensorArray` class to be able to obtain these?", "comments": ["I am not sure I understand this.  In your example, you seem to be doing tf.gradients(x[size-1], x) for some tensor x. So it should have nothing to do with the while loop or TensorArray?\n", "Yes, that's my point really: In order to access the `TensorArray` intermediate states, I need to use `pack` or `gather`. This creates a new `Tensor` which isn't on the route in the graph between the intermediate states and the final `loss`, and therefore if we differentiate with respect to this new node, we'll get a bunch of zeros.\r\n\r\nBut I need the intermediate gradients, as shown in this simple example that uses an explicit loop instead of `tf.while_loop`:\r\n\r\n```\r\nx = [tf.constant(1.0)]\r\nfor _ in xrange(2):\r\n    x.append(2*x[-1])\r\n\r\nloss = x[-1]\r\ndloss_dx = tf.gradients(loss, x)\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(dloss_dx))\r\n```\r\n\r\nThis will print `[4., 2., 1.]`.", "Ping. Note that most of the `TensorArray` code was written by @ebrevdo. @ebrevdo would you be willing to share some insight on this?\r\n\r\nMy best guess here is that we need something like an `unpack` for a `TensorArray`, which would return the internal `Tensor`s that we need, so the code would be something like\r\n\r\n```\r\nlist_of_tensors = x_ta.unpack()\r\ndloss_dx = tf.gradients(loss, list_of_tensors)\r\n```", "Ping. I'd really like to know if this is feasible, which isn't apparent from looking at the `TensorArray` source.", "Friendly ping", "Friendly ping @yuanbyu @ebrevdo @girving ", "Closing due to lack of activity.  Apologies that this did not come to a conclusion.  If the issue still exists or you still have a question please open a new issue here or on stackoverflow."]}, {"number": 5411, "title": "Fix windows build.", "body": "Add resource_variable_ops to cmake", "comments": ["@snnn, thanks for your PR! By analyzing the history of the files in this pull request, we identified @danmane, @ageron and @tensorflower-gardener to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 5410, "title": "DONOTMERGE Create a windows batch script to run jenkins windows builds.", "body": "", "comments": ["@guschmue @mrry PTAL\n", "http://ci.tensorflow.org/job/tensorflow-pr-win-cmake-pytest/7/\n\nHere is the run with this script.\nLet's see what happens.\n", "yes, windows build is broken in a couple of ways. The last known good is 832f5fa943dc1f621c29c3194651eb4f688f5fcb.\n\nThe most important one (I think that's the one you are seeing) is that 'import tensorflow' can't find the module\ntensorflow.python.ops.gen_resource_variable_ops. Something was added and I guess that we need to add it to the cmake pip build. I'll take a look today.\nThe other issue is that gpu builds are broken: parameterized_truncated_normal_op_gpu.cu.cc (which is new) needs 2 changes to compile on windows. I have a fix for this here https://github.com/guschmue/tensorflow/commits/winbuildfix. \nWith that Release for gpu builds again. \nI have not sent a PR yet because RelWithDebugInfo complains about some issue with the pdb for the python wrapper: no permission or disk space but neither is true.\nI think something is corrupt in the pdb (like some symbol is too long or so). Have seen this before when I was trying too turn on the normal handling for constexpr in eigen (which is different for windows).\nWill try to get a handle on this too but tensorflow.python.ops.gen_resource_variable_ops comes first.\n", "@guschmue I just merged a PR that should fix the resource ops.\n", "Yes, that fixed it! Thanks!\nPython tests looking good too:\n100% tests passed, 0 tests failed out of 141\n", "Rebased on top of master again.\nAlso applied the recommended changes.\nRunning a new build with this script.\n", "OK, here is my test result\nhttp://ci.tensorflow.org/job/tensorflow-pr-win-cmake-pytest/11/console\n\npip install exits successfully, however ctest runs cannot find tensorflow.\nI think we need a virtualenv on windows expert to help us.\n", "without virtenv my jenkins running the tests fine. I can switch this to a virtenv using the same script that you have - will let you know the outcome.\n", "found out why its not working in the virtual env:\nin tf_tests.cmake we use:\n     add_test(NAME ${sourcefile} COMMAND ${PYTHON_EXECUTABLE} ${sourcefile})\n\nbut that PYTHON_EXECUTABLE is not in the virtual env that you install the pip in. \nThere are 2 options to fix it:\n1. we call cmake and set PYTHON_EXECUTABLE to the python in the virtual env\n2. we use 'python' in tf_tests.cmake, than it will take python from $PATH which would be the virtual env\n", "In the script, I set PYTHON_EXECUTABLE to python:\n(-DPYTHON_EXECUTABLE=\"python\")\nWould that not use the python in virtual environment?\n\nOn Sat, Nov 5, 2016 at 8:44 PM, guschmue notifications@github.com wrote:\n\n> found out why its not working in the virtual env:\n> in tf_tests.cmake we use:\n> add_test(NAME ${sourcefile} COMMAND ${PYTHON_EXECUTABLE} ${sourcefile})\n> \n> but that PYTHON_EXECUTABLE is not in the virtual env that you install the\n> pip in.\n> There are 2 options to fix it:\n> 1. we call cmake and set PYTHON_EXECUTABLE to the python in the virtual env\n> 2. we use 'python' in tf_tests.cmake, than it will take python from $PATH\n> which would be the virtual env\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5410#issuecomment-258658517,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AHlCOTNmjJmPF0rn66kFwSSu-ayTgRvHks5q7U0dgaJpZM4KqM2Y\n> .\n", "cmake will resolve the PYTHON_EXECUTABLE  variable at cmake time. I know there is a -D in ctest but its for some scripting/dashboard mode (I have never used this) \n", "Thanks for the pointer.\nI will then try to precreate the virtualenv.\n\nOn Sat, Nov 5, 2016 at 9:57 PM, guschmue notifications@github.com wrote:\n\n> cmake will resolve the PYTHON_EXECUTABLE variable at cmake time. I know\n> there is a -D in ctest but its for some scripting/dashboard mode (I have\n> never used this)\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5410#issuecomment-258660691,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AHlCOR3LgIALMhh3CZV9iCLn1NcjAIvQks5q7V5RgaJpZM4KqM2Y\n> .\n", "OK, looks like virtualenv ends up causing more and more issues:\nhttp://ci.tensorflow.org/job/tensorflow-pr-win-cmake-pytest/12/console\n\nThe error message:\n\n```\n00:14:17.678 ImportError: Traceback (most recent call last):\n00:14:17.678   File \"C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-pytest\\cmake_build\\test_venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\n00:14:17.678     from tensorflow.python import pywrap_tensorflow\n00:14:17.678   File \"C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-pytest\\cmake_build\\test_venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\n00:14:17.678     _pywrap_tensorflow = swig_import_helper()\n00:14:17.678   File \"C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-pytest\\cmake_build\\test_venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 24, in swig_import_helper\n00:14:17.678     _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n00:14:17.678   File \"C:\\Program Files\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n00:14:17.678     return load_dynamic(name, filename, file)\n00:14:17.678   File \"C:\\Program Files\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n00:14:17.678     return _load(spec)\n00:14:17.678 ImportError: DLL load failed: The specified module could not be found.\n```\n\nAny ideas?\n", "Here is what I use which is working with a virtual env. The only trick is that you need to use always the same python.exe. (the variables that are not defined in the script are set globally in Jenkins, things like \n%SWIG_EXECUTABLE% ...).\nHope that helps.\n\ncall \"C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\amd64\\vcvars64.bat\"\nset PreferredToolArchitecture=x64\nset rel=Release\nset PYTHON_EXECUTABLE=C:\\Anaconda3\\envs\\pyrel\\python.exe\n\ncd %WORKSPACE%\\tensorflow\\contrib\\cmake\nmkdir build\ncd build\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=%rel% -DPYTHON_EXECUTABLE=%PYTHON_EXECUTABLE% -DPYTHON_LIBRARIES=%PYTHON_LIBRARIES% -DSWIG_EXECUTABLE=%SWIG_EXECUTABLE% -Dtensorflow_BUILD_PYTHON_TESTS=ON\n\nMSBuild /p:Configuration=%rel% tf_python_build_pip_package.vcxproj\n\n%PYTHON_EXECUTABLE% -m pip install --upgrade %WORKSPACE%\\tensorflow\\contrib\\cmake\\build\\tf_python\\dist\\tensorflow-0.11.0rc2_cmake_experimental-py3-none-any.whl\nctest -C %rel%\n", "I will close this now and keep working on it.\n"]}, {"number": 5409, "title": "TF_OperationOutputType returns values not contained in TF_DataType", "body": "For example, if TF_OperationOutputType is used on a Variable node of \r\ntype Double, the function returns value 102, which corresponds to DT_DOUBLE_REF\r\nin protobuf DataType enum (types.proto). However, value 102 is not defined in \r\nTF_DataType enum (c_api.h).\r\n\r\nThis inconsistency can be easily avoided by using integers instead of TF_DataType enum,\r\nhowever it should be probably fixed at some point.  ", "comments": ["Could you please take this bug on @josh11b. Thanks!\n", "@juraj-bicikl : Could you elaborate a bit on your use of `TF_OperationOutputType`? Specifically, I'm wondering whether your use of the C API needs to care about reference types.\n\n@alextp is in the process of changing how variables are used without references (using the `DT_RESOURCE` type) and perhaps it makes sense for `TF_OperationOutputType` to always return the type (so `DT_DOUBLE` and not `DT_DOUBLE_REF`) so that the notion of references remains internal to the runtime.\n", "So far, I have been using TF_OperationOutputType in the context of building a tensorflow \ngraph. The function is useful when the type of the next node in the graph has to match \nthe type of one of the previous nodes. The function enables me to figure out the type \nthat I have to assign to the new node. \n\nFor example, when implementing Adam optimizer, I have to define\ntwo new variables (per each variable over which we optimize the cost) to keep track\nof the mean and variance of the gradient. To create those \"Variable\" nodes, I have to know\nthe type, and for that I use TF_OperationOutputType. \n\nI haven't yet encountered a situation where it was important to me to distinguish \nbetween DT_FLOAT and DT_FLOAT_REF, or DT_DOUBLE and DT_DOUBLE_REF. \nCurrently I am not aware of the case where this could be useful.\n", "Thanks. I'll ask around to see what folks think about hiding the reference type and having `TF_OperationOutputType` return the non-reference type always and circle back.\n", "This issue is quite old and hasn't had recent activity. If it is still not working in the latest version of TensorFlow, and please create a new bug. Thank you."]}, {"number": 5408, "title": "[feature request] GPU Kernel for SparseSegmentMean/SparseSegmentMeanGrad", "body": "I'm using embedding_lookup_sparse op (with 'mean' combiner) to do dictionary learning -- sparse lookup is needed because each object is associated with variable length of word list. But it seems there is no gpu kernel implemented for SparseSegmentMean/SparseSegmentMeanGrad. This operation bottlenecks the entire speed.\r\n\r\nWondering if there are plans to implement gpu kernel for these two ops? I believe they are of general interests to people working on NLP, attributes learning as well.\r\n\r\nThanks!\r\n", "comments": ["Could you speak to this. AFAIK, most of our sparse operations are not super amenable to running on the GPU.\n", "While the Sparse.\\* ops aren't easily amenable to GPU implementations, the Unsorted.\\* versions _are_ and do have GPU implementations.  We just happen to use the Sparse versions for our embedding lookups.  Changing these over to the Unsorted lookups is rather trivial in the python code _but_ this would be a major change that could affect many, many downstream users.  Making that change requires careful performance analysis for many representative clients both on the training and inference side of things.\n", "Thanks!  @ebrevdo . To understand what you say, in my case for example, do you mean I can change my embedding_lookup_sparse to embedding_lookup + (unsorted)Segment_Mean to do the work with GPU support? If not, could you please elaborate a bit on the 'change' you mentioned?\n\nThank you!\n", "yes; that's what i mean.  use the unsorted version, which has a GPU kernel implementation.\n", "@ebrevdo  Thanks! I'll do that.\nOne question though, I checked and could see unsorted_segment_sum is supported by GPU.\nhttps://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/core/kernels/segment_reduction_ops_gpu.cu.cc\nBut I do not see its gradient op implemented (searching through core/kernel/ directory)? I guess I don't know enough about these kernel implementation yet. But in general, what should I check to know if some op kernel are implemented?  (I'm partly referring to #2502)\n", "Let us know how it works for you.\n\nOn Tue, Nov 8, 2016 at 12:34 AM, Kuan Liu notifications@github.com wrote:\n\n> Reopened #5408 https://github.com/tensorflow/tensorflow/issues/5408.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5408#event-851136523,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtimz1yorw9OwnbPQx-YdVNHf6siSHtks5q8DQkgaJpZM4KqH0s\n> .\n", "Closing due to lack of activity.\n"]}, {"number": 5407, "title": "tf.contrib.metrics.streaming_precision doesn't accept predictions and labels of dtype tf.bool", "body": "According to documentation and comments in code, \"tf.contrib.metrics.streaming_precision\" should accept predictions and labels of boolean type, but it doesn't seem to be true.\r\n\r\nTo reproduce, modify testAllCorrect procedure in [this file](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/metrics/python/ops/metric_ops_test.py) by adding dtype=tf.bool to tf.constant, as below:\r\n```\r\n  def testAllCorrect(self):\r\n    inputs = np.random.randint(0, 2, size=(100, 1))\r\n\r\n    predictions = tf.constant(inputs, dtype=tf.bool)\r\n    labels = tf.constant(inputs, dtype=tf.bool)\r\n    precision, update_op = metrics.streaming_precision(\r\n        predictions, labels)\r\n```\r\n\r\nThe test will fail:\r\n```\r\ntensorflow/contrib/metrics/python/ops % python ./metric_ops_test.py\r\n======================================================================\r\nERROR: testAllCorrect (__main__.StreamingPrecisionTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"./metric_ops_test.py\", line 656, in testAllCorrect\r\n    predictions, labels)\r\n  File \"/home/sergeii/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/framework/python/framework/deprecation.py\", line 218, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/sergeii/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py\", line 572, in streaming_precision\r\n    updates_collections=None, name=None)\r\n  File \"/home/sergeii/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py\", line 215, in _streaming_true_positives\r\n    is_true_positive = math_ops.logical_and(math_ops.equal(labels, 1),\r\n  File \"/home/sergeii/tools/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 726, in equal\r\n    result = _op_def_lib.apply_op(\"Equal\", x=x, y=y, name=name)\r\n  File \"/home/sergeii/tools/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 490, in apply_op\r\n    preferred_dtype=default_dtype)\r\n  File \"/home/sergeii/tools/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 657, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/sergeii/tools/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 180, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/sergeii/tools/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 163, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\r\n  File \"/home/sergeii/tools/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 353, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"/home/sergeii/tools/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 290, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected bool, got 1 of type 'int' instead.\r\n````\r\n\r\nThis happens in _streaming_true_positives function in [this file](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/metrics/python/ops/metric_ops.py), when executing ```math_ops.equal(labels, 1)```\r\n\r\nCan be repeated simply as:\r\n```\r\nimport numpy as np\r\nfrom tensorflow.python.ops import math_ops\r\nimport tensorflow as tf\r\n\r\nlabel = tf.constant(np.array([1, 0, 1]), dtype=tf.bool)\r\nmath_ops.equal(label, 1)\r\n```\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04.1\r\n\r\n```\r\n%ls -l /usr/local/cuda/lib64/libcud*  \r\n-rw-r--r-- 1 root root    558720 Oct 20 14:53 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root        16 Oct 20 14:53 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root        19 Oct 20 14:53 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root    415432 Oct 20 14:53 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root    775162 Oct 20 14:53 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 1000 users       13 Jul 26 22:55 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 1000 users       17 Jul 26 22:55 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rwxrwxr-x 1 1000 users 79337624 Jul 26 22:53 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-rw-r-- 1 1000 users 69756172 Jul 26 22:53 /usr/local/cuda/lib64/libcudnn_static.a\r\n```\r\n\r\n```\r\npython -c \"import tensorflow; print(tensorflow.__version__)\"\r\nVersion: 0.11.0rc2\r\n```\r\n", "comments": ["The links to files are both broken for me. \n", "Updated the links, please check again\n", "To fix this, we'll need to replace the 1 with `True` if the input is a bool.  Contributions welcome!\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 5406, "title": "TF_GraphImportGraphDef breaks down while importing graphs with control flow ops", "body": "I've encountered issues when trying to load graphs from python which contain\r\ndropout or batch norm. In the following example:\r\n```python\r\n    x = tf.placeholder(tf.float64, [None, 3])\r\n\r\n    w = tf.Variable(tf.random_normal([3, 3], dtype=tf.float64))\r\n    h = tf.matmul(x, w)\r\n\r\n    is_training = tf.Variable(True, trainable=False)\r\n    h_dropout = tf.contrib.layers.dropout(h, keep_prob=0.5, is_training=is_training)\r\n```\r\nTF_GraphImportGraphDef breaks down and I get the following message:\r\n\r\n*SIGSEGV (0xb) at pc=0x000000013539dcac, pid=23288, tid=0x0000000000006a1b\r\nC  [libtensorflow.dylib+0x14cac]  tensorflow::shape_inference::InferenceContext::Rank(tensorflow::shape_inference::ShapeHandle) const+0x1c*\r\n\r\nThe same happens if I replace dropout operation with batch norm:\r\n```python\r\ntf.contrib.layers.batch_norm(h, center=True, scale=True, is_training=is_training)\r\n```\r\n\r\nIt seems to me this problem is related to the presence of control flow ops.", "comments": ["It is very difficult to help you when you do not provide basic information about your environment. What OS? What Version of TensorFlow? Also, you should create a simple test example that can be reproduced. Thanks!\n", "I am using Mac OS X 10.11.6, and Python 3.5.1 with tensorflow 0.11.0rc0 version.\nI compiled the following C++ code on top of 0.11.0rc0 version (commit 2dea76c).\n\nPython code where I create a graph and serialize it as GraphDef\n\n``` python\nimport tensorflow as tf\nimport numpy as np\nimport sys\n\ngraph = tf.get_default_graph()\n\nx = tf.placeholder(tf.float64, [None, 3])\nw = tf.Variable(tf.random_normal([3, 3], dtype=tf.float64))\nh = tf.matmul(x, w)\n\nis_training = tf.Variable(True, trainable=False)\nh_dropout = tf.contrib.layers.dropout(h, keep_prob=0.5, is_training=is_training)\n\nsys.stdout.buffer.write(graph.as_graph_def().SerializeToString())\n```\n\nC++ code where I import the graph:\n\n``` C++\n#include <cstdio>\n#include <cstdlib>\n#include <cstring>\n#include <fstream>\n#include <string>\n#include <iostream>\n#include \"tensorflow/c/c_api.h\"\n#include \"tensorflow/core/framework/graph.pb.h\"\n#include \"tensorflow/core/framework/graph_def_util.h\"\n\nint main(int argc, char* argv[]) {\n\n    // loading graph proto\n    tensorflow::GraphDef graph_def;\n    std::ifstream f_in(argv[1]);\n    bool ctrl = graph_def.ParseFromIstream(&f_in);\n    f_in.close();\n    if (ctrl) std::cout << \"deserialization successful\\n\\n\";\n\n    std::cout << \"Graph contents:\\n\";\n    std::cout << tensorflow::SummarizeGraphDef(graph_def) << \"\\n\";\n\n    // loading graph_def as TF_Graph\n    TF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();\n    TF_Status* status = TF_NewStatus();\n    TF_Graph* tf_graph = TF_NewGraph();\n    std::string gd_as_string = std::string();\n    graph_def.SerializeToString(&gd_as_string);\n    TF_Buffer* gd_as_buffer = TF_NewBufferFromString(gd_as_string.c_str(), gd_as_string.length());\n    TF_GraphImportGraphDef(tf_graph, gd_as_buffer, opts, status);\n\n    if (TF_GetCode(status) == 0)\n      std::cout << \"graph_def succesfully loaded into tf_graph\\n\";\n    else\n      std::cout << TF_Message(status) << '\\n';\n\n    TF_DeleteImportGraphDefOptions(opts);\n    TF_DeleteStatus(status);\n    TF_DeleteGraph(tf_graph);\n\n    return 0;\n}\n```\n\nWhen I run the C++ program and try to load GraphDef generated by the python code (which \nincludes dropout), I get:\n\n_Segmentation fault: 11_\n\nIf I do not include the dropout layer, everything works fine.\n", "The segfault was fixed in https://github.com/tensorflow/tensorflow/commit/7ea2f7d2689d9686c93650bf5bcc4c8ba459377d\nwhich unfortunately isn't part of the 0.11 release, but will be part of 0.12.\n\nI'm going to go ahead and close this for now since the problem mentioned in the issue is fixed at head. @juraj-bicikl - hopefully this is okay and you don't _require_ this in 0.11.\n\n( @cwhipkey  FYI)\n", "Thanks a lot!\n"]}, {"number": 5405, "title": "Ignore tools/git/gen and fix a bash error in python_config.sh", "body": "https://github.com/tensorflow/tensorflow/commit/b4798133adfe8fcd9d138ddb86d5b3aa0c4901e9 only works if the new feature is used; otherwise it prints an obscure bash error.", "comments": ["@girving, thanks for your PR! By analyzing the history of the files in this pull request, we identified @andreas-eberle, @vrv and @meteorcloudy to be potential reviewers.\n", "python 3 CI failure is known flaky test.  If the CPU test passes ,I'll merge.\n"]}, {"number": 5404, "title": "VARIABLES Collection name is deprecated, please use GLOBAL_VARIABLES instead", "body": "I pulled the latest HEAD a few mins ago and ran my code.  I started to get the listed error:\r\n\r\n**WARNING:tensorflow:VARIABLES Collection name is deprecated, please use GLOBAL_VARIABLES instead.**\r\n**VARIABLES will be removed after 2017-03-02**\r\n\r\nI didn't fully debug all the causes but seems like there is errors thrown when the following lines are hit:\r\n\r\n> from tensorflow.contrib.session_bundle import exporter\r\n\r\nif I remove that line, it seems to move to another line building the graph, not sure if it is a \"get_variables()\" operation or something.  I don't use VARIABLES anywhere in  my code.  Given the \"saver\" below causing a similar issue, it is possible my \"histogram_summary\" or \"scalar_summary\" ops may have triggered this as well while building the graph.\r\n\r\nThe same error is printed on each iteration of \"save()\" with the saver, this really messes up my logging to the screen:\r\n> save_path = saverRun.save(session, saveto)\r\n\r\nSeems like something within the Saver and possibly the \"variables\" might need updating.", "comments": ["Assigning @ilblackdragon  since this is your CL. It looks like tensorflow.contrib is still touching the old all_variables instead of the new global_variables. Is it possible we could do a more replacement of everything within the tensorflow repository?\n", "@martinwicke, do we have a policy on updating contrib when we do deprecations? It is definitely less supported, of course.\n", "All the tensorflow internal usages were updated on Friday (https://github.com/tensorflow/tensorflow/commit/4cbdead95f22de74bcbc72a68c9a38d465202db9, https://github.com/tensorflow/tensorflow/commit/cbd3cacfb73bbea912b9d01c2540187684f751a7). There is a code in flight for printing location where deprecated function was called from.\n", "Note also that this is only a warning, your code should continue working as before. \n\nIn general, code is contrib can and will change at will. We're trying to be nice about it, but sometimes it's not possible. Not that in this instance, nothing should have been broken.\n", "Thanks Martin and @ilblackdragon  and @martinwicke for the quick response.\n"]}, {"number": 5403, "title": "Update version string to 0.11.0.", "body": "", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @gunan, @keveman and @vrv to be potential reviewers.\n"]}, {"number": 5402, "title": "Update index.md", "body": "We at Hewlett Packard Labs have created a plugin for TensorFlow, the Operator Vectorization Library, to more easily create custom operators in pure python and also automatically merge those operators when possible. We would appreciate a link to our project on the TensorFlow page. \r\n\r\nWhite paper is here: http://www.labs.hpe.com/techreports/2016/HPE-2016-94.html", "comments": ["@kbrems, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @martinwicke to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Great, thank you! \n\n@keveman FYI\n"]}, {"number": 5401, "title": "Creating and fitting a Trainable leaks file descriptors ", "body": "Creating a Trainable and calling fit() on it leaks file descriptors.\r\n\r\n### Environment info\r\nOperating System: Mac OS X\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nNone\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\n0.11.0rc0\r\n\r\nIf installed from source, provide \r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow\r\nfrom tensorflow.contrib import learn, layers\r\n\r\ndef input_fn():\r\n    return (\r\n        {'a': tensorflow.constant([1.1, 2.2, 3.3], dtype=tensorflow.float32)}, \r\n        tensorflow.constant([5.0, 8.0, 12.5], dtype=tensorflow.float32))\r\n\r\nwhile True:\r\n    linear_regressor = learn.LinearRegressor(\r\n        feature_columns=[layers.real_valued_column('a')])\r\n    linear_regressor.fit(input_fn=input_fn, steps=1)\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n\r\nTraceback (most recent call last):\r\n  File \"tensor_bug.py\", line 13, in <module>\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 333, in fit\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 708, in _train_model\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 285, in _monitored_train\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitored_session.py\", line 368, in run\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitored_session.py\", line 521, in run\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitored_session.py\", line 488, in run\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitored_session.py\", line 612, in run\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitored_session.py\", line 634, in _call_hook_before_run\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/basic_session_run_hooks.py\", line 180, in before_run\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/training/training_util.py\", line 76, in write_graph\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\", line 211, in write_string_to_file\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\", line 89, in write\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\", line 81, in _prewrite_check\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/errors.py\", line 463, in raise_exception_on_not_ok_status\r\ntensorflow.python.framework.errors.ResourceExhaustedError: /var/folders/6c/3pzbp8jj3zd0lkhsszmvyd5h001cct/T/tmpzr7juea4/graph.pbtxt\r\n\r\nDuring the run \"lsof\" is indicating that the number of file descriptors used is increased on every loop iteration.", "comments": ["@martinwicke, @ilblackdragon , could you take a look.\n", "@ispirmustafa Maybe a summarywriter or similar object is not closing a file properly?\n", "Leak can be anywhere in the code since this example is running all Estimator in an infinite loop.\nI'll look into it. I'll update the thread when I find the reason/fix.\n"]}, {"number": 5400, "title": "Branch 138201323", "body": "", "comments": ["@vrv, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @langmore and @ilblackdragon to be potential reviewers.\n"]}, {"number": 5399, "title": "Seg faults when build with Hadoop file system support", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNo\r\n### Environment info\r\nOperating System: CentOS with kernel 3.10.0-123.el7.x86_64\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): 8.0\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`) c80ef0cd19a3ac9ad0657f617b4e7328f6a48ac3\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\ntensorflow_serving/example/mnist_export.py\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nWhen run tensorflow build with HDFS support (HDFS is configured and loaded, but not used), python crash with segment faults, here is the backtrace:\r\n```\r\n(gdb) bt\r\n#0  update_refs (containers=<optimized out>) at /usr/src/debug/Python-2.7.5/Modules/gcmodule.c:400\r\n#1  collect (generation=2) at /usr/src/debug/Python-2.7.5/Modules/gcmodule.c:998\r\n#2  0x00007fb2a3102227 in collect_generations () at /usr/src/debug/Python-2.7.5/Modules/gcmodule.c:1124\r\n#3  _PyObject_GC_Malloc (basicsize=3) at /usr/src/debug/Python-2.7.5/Modules/gcmodule.c:1585\r\n#4  0x00007fb2a310226d in _PyObject_GC_New (tp=tp@entry=0x7fb2a338a220 <_PyWeakref_RefType>) at /usr/src/debug/Python-2.7.5/Modules/gcmodule.c:1595\r\n#5  0x00007fb2a309a456 in new_weakref (ob=ob@entry=<TensorProto at remote 0x6fe0cf8>, callback=callback@entry=0x0) at /usr/src/debug/Python-2.7.5/Objects/weakrefobject.c:36\r\n#6  0x00007fb2a309d5f3 in PyWeakref_NewProxy (ob=<TensorProto at remote 0x6fe0cf8>, callback=<optimized out>) at /usr/src/debug/Python-2.7.5/Objects/weakrefobject.c:840\r\n#7  0x00007fb2a31158c7 in weakref_proxy (self=<optimized out>, args=<optimized out>) at /usr/src/debug/Python-2.7.5/Modules/_weakref.c:73\r\n#8  0x00007fb2a30d2b94 in call_function (oparg=<optimized out>, pp_stack=0x7fffd07e3c50) at /usr/src/debug/Python-2.7.5/Python/ceval.c:4098\r\n#9  PyEval_EvalFrameEx (\r\n    f=f@entry=Frame 0x31c0bc0, for file /home/heliangliang/tf/lib/python2.7/site-packages/google/protobuf/internal/python_message.py, line 1393, in __init__ (self=<_Listener at remote 0x6fdced0>, parent_message=<TensorProto at remote 0x6fe0cf8>), throwflag=throwflag@entry=0) at /usr/src/debug/Python-2.7.5/Python/ceval.c:2740\r\n#10 0x00007fb2a30d41ad in PyEval_EvalCodeEx (co=<optimized out>, globals=<optimized out>, locals=locals@entry=0x0, args=args@entry=0x5510ba8, argcount=2, kws=kws@entry=0x0, kwcount=kwcount@entry=0, \r\n    defs=defs@entry=0x0, defcount=defcount@entry=0, closure=0x0) at /usr/src/debug/Python-2.7.5/Python/ceval.c:3330\r\n#11 0x00007fb2a3061098 in function_call (func=<function at remote 0x305be60>, arg=(<_Listener at remote 0x6fdced0>, <TensorProto at remote 0x6fe0cf8>), kw=0x0)\r\n    at /usr/src/debug/Python-2.7.5/Objects/funcobject.c:526\r\n#12 0x00007fb2a303c073 in PyObject_Call (func=func@entry=<function at remote 0x305be60>, arg=arg@entry=(<_Listener at remote 0x6fdced0>, <TensorProto at remote 0x6fe0cf8>), kw=kw@entry=0x0)\r\n    at /usr/src/debug/Python-2.7.5/Objects/abstract.c:2529\r\n#13 0x00007fb2a304b085 in instancemethod_call (func=<function at remote 0x305be60>, arg=(<_Listener at remote 0x6fdced0>, <TensorProto at remote 0x6fe0cf8>), kw=0x0)\r\n    at /usr/src/debug/Python-2.7.5/Objects/classobject.c:2602\r\n#14 0x00007fb2a303c073 in PyObject_Call (func=func@entry=<instancemethod at remote 0x5516820>, arg=arg@entry=(<TensorProto at remote 0x6fe0cf8>,), kw=kw@entry=0x0)\r\n    at /usr/src/debug/Python-2.7.5/Objects/abstract.c:2529\r\n#15 0x00007fb2a3093167 in slot_tp_init (self=<optimized out>, args=(<TensorProto at remote 0x6fe0cf8>,), kwds=0x0) at /usr/src/debug/Python-2.7.5/Objects/typeobject.c:5692\r\n#16 0x00007fb2a3091e7f in type_call (type=<optimized out>, args=(<TensorProto at remote 0x6fe0cf8>,), kwds=0x0) at /usr/src/debug/Python-2.7.5/Objects/typeobject.c:745\r\n#17 0x00007fb2a303c073 in PyObject_Call (func=func@entry=<type at remote 0x2ef68b0>, arg=arg@entry=(<TensorProto at remote 0x6fe0cf8>,), kw=kw@entry=0x0) at /usr/src/debug/Python-2.7.5/Objects/abstract.c:2529\r\n#18 0x00007fb2a30d034c in do_call (nk=<optimized out>, na=1, pp_stack=0x7fffd07e4230, func=<type at remote 0x2ef68b0>) at /usr/src/debug/Python-2.7.5/Python/ceval.c:4316\r\n#19 call_function (oparg=<optimized out>, pp_stack=0x7fffd07e4230) at /usr/src/debug/Python-2.7.5/Python/ceval.c:4121\r\n#20 PyEval_EvalFrameEx (\r\n    f=f@entry=Frame 0x6c355c0, for file /home/heliangliang/tf/lib/python2.7/site-packages/google/protobuf/internal/python_message.py, line 489, in init (self=<TensorProto at remote 0x6fe0cf8>, kwargs={}), \r\n    throwflag=throwflag@entry=0) at /usr/src/debug/Python-2.7.5/Python/ceval.c:2740\r\n#21 0x00007fb2a30d41ad in PyEval_EvalCodeEx (co=<optimized out>, globals=<optimized out>, locals=locals@entry=0x0, args=args@entry=0x57ef6a8, argcount=1, kws=kws@entry=0x0, kwcount=kwcount@entry=0, \r\n    defs=defs@entry=0x0, defcount=defcount@entry=0, closure=(<cell at remote 0x31858d8>, <cell at remote 0x3185910>)) at /usr/src/debug/Python-2.7.5/Python/ceval.c:3330\r\n#22 0x00007fb2a3061098 in function_call (func=<function at remote 0x318c500>, arg=(<TensorProto at remote 0x6fe0cf8>,), kw=0x0) at /usr/src/debug/Python-2.7.5/Objects/funcobject.c:526\r\n#23 0x00007fb2a303c073 in PyObject_Call (func=func@entry=<function at remote 0x318c500>, arg=arg@entry=(<TensorProto at remote 0x6fe0cf8>,), kw=kw@entry=0x0)\r\n    at /usr/src/debug/Python-2.7.5/Objects/abstract.c:2529\r\n#24 0x00007fb2a304b085 in instancemethod_call (func=<function at remote 0x318c500>, arg=(<TensorProto at remote 0x6fe0cf8>,), kw=0x0) at /usr/src/debug/Python-2.7.5/Objects/classobject.c:2602\r\n#25 0x00007fb2a303c073 in PyObject_Call (func=func@entry=<instancemethod at remote 0x557f410>, arg=arg@entry=(), kw=kw@entry=0x0) at /usr/src/debug/Python-2.7.5/Objects/abstract.c:2529\r\n#26 0x00007fb2a3093167 in slot_tp_init (self=<optimized out>, args=(), kwds=0x0) at /usr/src/debug/Python-2.7.5/Objects/typeobject.c:5692\r\n#27 0x00007fb2a3091e7f in type_call (type=<optimized out>, args=(), kwds=0x0) at /usr/src/debug/Python-2.7.5/Objects/typeobject.c:745\r\n#28 0x00007fb2a303c073 in PyObject_Call (\r\n    func=func@entry=<GeneratedProtocolMessageType(__module__='tensorflow.core.framework.tensor_pb2', TENSOR_CONTENT_FIELD_NUMBER=4, TENSOR_SHAPE_FIELD_NUMBER=2, _UpdateOneofState=<function at remote 0x318fc80>, MergeFromString=<function at remote 0x318f938>, dtype=<property at remote 0x3183af8>, __str__=<function at remote 0x318f5f0>, SerializeToString=<function at remote 0x318f7d0>, _SetListener=<function at remote 0x305bc80>, _oneofs=<member_descriptor at remote 0x2fd2128>, _cached_byte_size_dirty=<member_descriptor at remote 0x2fd2998>, string_val=<property at remote 0x3183db8>, _unknown_fields=<member_descriptor at remote 0x2fd2c68>, HasField=<function at remote 0x318f488>, _Modified=<function at remote 0x318fc08>, _extensions_by_name={}, __weakref__=<getset_descriptor at remote 0x2fd2b90>, DTYPE_FIELD_NUMBER=1, __init__=<function at remote 0x318c500>, double_val=<property at remote 0x3183d08>, scomplex_val=<property at remote 0x3183e10>, SCOMPLEX_VAL_FIELD_NUMBER=9, half_val=<property at remote 0x3183c58>, int_val=<property...(truncated), arg=arg@entry=(), kw=kw@entry=0x0) at /usr/src/debug/Python-2.7.5/Objects/abstract.c:2529\r\n#29 0x00007fb2a30d034c in do_call (nk=<optimized out>, na=0, pp_stack=0x7fffd07e4810, \r\n    func=<GeneratedProtocolMessageType(__module__='tensorflow.core.framework.tensor_pb2', TENSOR_CONTENT_FIELD_NUMBER=4, TENSOR_SHAPE_FIELD_NUMBER=2, _UpdateOneofState=<function at remote 0x318fc80>, MergeFromString=<function at remote 0x318f938>, dtype=<property at remote 0x3183af8>, __str__=<function at remote 0x318f5f0>, SerializeToString=<function at remote 0x318f7d0>, _SetListener=<function at remote 0x305bc80>, _oneofs=<member_descriptor at remote 0x2fd2128>, _cached_byte_size_dirty=<member_descriptor at remote 0x2fd2998>, string_val=<property at remote 0x3183db8>, _unknown_fields=<member_descriptor at remote 0x2fd2c68>, HasField=<function at remote 0x318f488>, _Modified=<function at remote 0x318fc08>, _extensions_by_name={}, __weakref__=<getset_descriptor at remote 0x2fd2b90>, DTYPE_FIELD_NUMBER=1, __init__=<function at remote 0x318c500>, double_val=<property at remote 0x3183d08>, scomplex_val=<property at remote 0x3183e10>, SCOMPLEX_VAL_FIELD_NUMBER=9, half_val=<property at remote 0x3183c58>, int_val=<property...(truncated))\r\n    at /usr/src/debug/Python-2.7.5/Python/ceval.c:4316\r\n#30 call_function (oparg=<optimized out>, pp_stack=0x7fffd07e4810) at /usr/src/debug/Python-2.7.5/Python/ceval.c:4121\r\n#31 PyEval_EvalFrameEx (\r\n    f=f@entry=Frame 0x35b42f0, for file /home/heliangliang/tf/lib/python2.7/site-packages/google/protobuf/internal/python_message.py, line 432, in MakeSubMessageDefault (message=<AttrValue at remote 0x6fe0c80>), throwflag=throwflag@entry=0) at /usr/src/debug/Python-2.7.5/Python/ceval.c:2740\r\n#32 0x00007fb2a30d41ad in PyEval_EvalCodeEx (co=<optimized out>, globals=<optimized out>, locals=locals@entry=0x0, args=<optimized out>, argcount=argcount@entry=1, kws=0x72e07a8, kwcount=0, defs=0x0, \r\n    defcount=0, closure=closure@entry=(<cell at remote 0x3196d70>, <cell at remote 0x3196da8>)) at /usr/src/debug/Python-2.7.5/Python/ceval.c:3330\r\n```\r\n", "comments": ["Please look at the issue submission template. If you want further help you will need to provide that information. Thank you.\n", "Closing due to lack of activity.\n"]}, {"number": 5398, "title": "Add Kerberos support for HDFS file system", "body": "This PR addresses https://github.com/tensorflow/tensorflow/issues/5369.\r\n\r\nIt also fixes an issue in DeleteDir.", "comments": ["@llhe, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jhseu and @yuefengz to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Jenkins, test this please\n", "> This PR addresses #5369.\r\n> \r\n> It also fixes an issue in DeleteDir.\r\n\r\nHello, ask how to read a picture from HDFS using the tensorflow related API and use other libraries for preprocessing."]}, {"number": 5397, "title": "Add environment variable to configure usage of default python library path", "body": "Some changes to `util/python_config.sh` since release version r0.10 introduced the problem that the user is asked to choose the python library path during `./configure`. The problem with this is that automated scripts / builds have no option to choose the default python library path. \r\n\r\nIn PR #5135, an environment variable has been introduced to configure the python library path via an environment variable to get around this problem. However, it leaves the script using `./configure` with the task to find a correct python library path. In many scenarios this cannot be hard coded and determining it with code means to replicate the logic that is already in the script.\r\n\r\nTherefore this PR adds an additional environment variable named `USE_DEFAULT_PYTHON_LIB_PATH`. Like the name says, if this variable is set to `1`, the default python library path is used without user interaction. ", "comments": ["@andreas-eberle, thanks for your PR! By analyzing the history of the files in this pull request, we identified @villasv, @vrv and @meteorcloudy to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "It is technically less general, but if hardcoding is really not always possible (I wonder when) I think this is good too. In the end, I can't imagine why someone would want to automate this but _not_ want to use the default lib path... so I see no problem here.\n", "@tensorflow-jenkins test this please\n", "not sure where the failure is coming from.\nJenkins, test this please.\n"]}, {"number": 5396, "title": "tf.train.range_input_producer(epoch_size, shuffle=True) does not terminate", "body": "Hey guys,\r\n\r\nWhile working on language modeling using PTB dataset  [(as shown in TensorFlow tutorial)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py) I had an issue with `ptb_producer `[(see on StackOverflow)](http://stackoverflow.com/questions/40406469/tf-nn-embedding-lookup-does-nothing-blocking-the-execution-but-no-associated-cp).\r\n\r\nLong story short, it turns out that `ptb_producer` never terminates.\r\n\r\nTo debug, I tried to 'manually' run `ptb_producer` with the code below.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nlearning_rate = 1.0\r\nnum_layers = 2\r\nnum_steps = 20\r\nhidden_size = 2\r\nepochs = 50\r\nbatch_size = 20\r\nvocab_size = 10000\r\n\r\n# Loading data\r\nimport tensorflow.models.rnn.ptb.reader as reader\r\n\r\nptb_data_path = '<path>/tensorflow/ptb_data/simple-examples/data/'\r\n(train, valid, test, _) = reader.ptb_raw_data(ptb_data_path)\r\n\r\nraw_data = train\r\nname = None\r\n\r\n# Exact content of PTBProducer\r\nwith tf.name_scope(name, \"PTBProducer\", [raw_data, batch_size, num_steps]):\r\n    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\r\n\r\n    data_len = tf.size(raw_data)\r\n    batch_len = data_len // batch_size\r\n    data = tf.reshape(raw_data[0 : batch_size * batch_len],\r\n                      [batch_size, batch_len])\r\n\r\n    epoch_size = (batch_len - 1) // num_steps\r\n    assertion = tf.assert_positive(\r\n        epoch_size,\r\n        message=\"epoch_size == 0, decrease batch_size or num_steps\")\r\n    with tf.control_dependencies([assertion]):\r\n        epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\r\n\r\n    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\r\n    x = tf.slice(data, [0, i * num_steps], [batch_size, num_steps])\r\n    y = tf.slice(data, [0, i * num_steps + 1], [batch_size, num_steps])\r\n```\r\n\r\nAnd then running to find out where thing is getting into troubles\r\n`sess = tf.Session()` then `sess.run(epoch_size)` gives `2323`\r\n\r\n\r\n\r\nRunning `sess.run(tf.train.range_input_producer(epoch_size, shuffle=True).dequeue())` just waits (it does not terminate nor induce CPU load, i.e. same problem than when I run the whole code).\r\n\r\nIn fact, even `queue = tf.train.range_input_producer(epoch_size, shuffle=True)` (which can be intermediate step for previous syntax) does nothing. There is the problem line, still don't know why\r\n\r\nI am not sure how this issue could be related to be, thus it might be a bug.\r\n\r\n# Config Information\r\n- `tf.__version__` = '0.11.0rc2'\r\n- python 3.5.2\r\n- Ubuntu 16.04 \r\n", "comments": ["To be clear. Are you running the unmodified tutorial, or are you writing your own code and having problems? If it is the former, then it is a bug, and a github issue is appropriate. Otherwise, please wait for a response on stackoverflow.\n", "My point here is that I am using [reader.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py) from a custom script (i.e. not [ptb_word_lm.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py) as suggested in the tuto). \n\nMy issue in fact only concerns [reader.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py) since I'm having trouble using `tensorflow.models.rnn.ptb.ptb_word_lm.ptb_producer()`. My tensorflow is up to date with master. \n\nI think I may have missunderstood something. I would expect `inputs_ = sess.run(inputs)` to run correctly with following \"model\":\n\n```\nptb_data_path = '<some_path>/ptb_data/simple-examples/data/'\n(train, valid, test, _) = reader.ptb_raw_data(ptb_data_path)\nc = ptb_word_lm.SmallConfig()\ninputs, targets = reader.ptb_producer(train, c.batch_size, c.num_steps, name=None)\n```\n\nIt instead \"sleeps\" (does not terminate, no (C/G)PU load\n\nEDIT:\nMoreover, the code below does not terminate either:\n\n```\nimport tensorflow as tf\n\nepoch_size = 20\ni = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n```\n\n(hints: thats what involved in [reader.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py) line 115.\n", "You probably need to start queue runners.  Please see the documentation at\n\nhttps://www.tensorflow.org/versions/r0.11/how_tos/threading_and_queues/index.html\n", "**Edit:**\nEven simpler, I don't get why this code fails (does not terminate nor load cpu/gpu blablabla)\n\n```\nimport tensorflow as tf\n\n# [0, 1, 2, 3, 4 ,...]\nx = tf.range(1, 10, name=\"x\")\nl = tf.placeholder(tf.int32)\n# A queue that outputs 0,1,2,3,...\nrange_q = tf.train.range_input_producer(limit=l, shuffle=False)\nslice_end = range_q.dequeue()\n\ny = tf.slice(x, [0], [slice_end], name=\"y\")\n\nwith tf.Session() as sess:\n    print(sess.run(y, feed_dict={l: 5}))\n```\n\n---\n\nI am still having trouble with this ptb_producer. My point now is just not to pass data on graph building (i.e. not in the PTBModel object creation) but on execution through placeholder.\n\nThus I modified PTBInput the following way:\n\n```\nclass PTBInput(object):\n  \"\"\"The input data.\"\"\"\n\n  def __init__(self, config, data=None, name=None):\n    self.data = data #tf.placeholder(tf.int32, shape=[None], name=\"data\")\n    datalen = tf.size(self.data)\n\n    self.batch_size = batch_size = config.batch_size\n    self.num_steps = num_steps = config.num_steps\n    self.epoch_size = ((datalen // batch_size) - 1) // num_steps\n    self.input_data, self.targets = reader.producer(\n        self.data, batch_size, num_steps, name=name)\n```\n\nand I try to run:\n\n```\nfrom ptb_word_lm import PTBModel, SmallConfig\nimport reader\n\nconfig = SmallConfig() \ndata_path='./data'\ntrain_data, valid_data, test_data, word_to_id = reader.ptb_raw_data(data_path)\nwith tf.Graph().as_default():\n   initializer = tf.random_uniform_initializer(-config.init_scale,config.init_scale)\n    with tf.name_scope(\"Train\"):\n        with tf.variable_scope(\"Model\", reuse=False, initializer=initializer):\n            m = Model(is_training=True, config=config)\n\n    with tf.Session()  as session:\n        m.assign_lr(session, 1.0)\n        state = session.run(m.initial_state)\n        vals = session.run([m.cost, m.final_state], feed_dict={m.initial_state: state, m.data: train_data})\n```\n\nWhich should run one training epoch. \nThis code is minimalistic but still does not work. As always, no CPU nor GPU load, just waiting...\n\nThe only difference is that 'data' is not an array anymore but a Tensorf (placeholder), don't see which this induces errors.\n", "Please read the documentation on queues I linked above.  You need to start queue runners.\n", "If you just use code of `with tf.Session() as sess:,` you must open the thread explicitly with `threads = tf.train.start_queue_runners()`. But in ptb_word_lm.py, It uses the codes like this `sv = tf.train.Supervisor()  with sv.managed_session() as sess:`, the Supervisor() function contains something which starts the thread implicitly", "I really confused ! \r\n `  x = tf.slice(data, [0, i * num_steps], [batch_size, num_steps])\r\n    y = tf.slice(data, [0, i * num_steps + 1], [batch_size, num_steps])`\r\nDo x  and y use the same value of i ??????????????????????", "For anyone else struggling with this, please read the documentation as mentioned by girving. For the lazy ones:\r\n\r\n```\r\ninit = tf.global_variables_initializer()\r\nsess.run(init)\r\nthreads = tf.train.start_queue_runners()\r\nprint(sess.run(name_of_output_tensor))\r\n```", "Can you help me understand this problem.I am also having the same problem, I am checking the function values with Interactive session as\r\n\r\n```\r\ndef ptb_producer(raw_data, batch_size, num_steps, name=None):\r\n  session = tf.InteractiveSession()\r\n  with tf.name_scope(name, \"PTBProducer\", [raw_data, batch_size, num_steps]):\r\n    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\r\n    data_len = tf.size(raw_data)\r\n    print(\"Data Len\", data_len.eval())\r\n    batch_len = data_len // batch_size\r\n    print(\"Batch Len\", batch_len.eval())\r\n    data = tf.reshape(raw_data[0 : batch_size * batch_len],\r\n                      [batch_size, batch_len])\r\n    print(\"Data\", data.eval())\r\n    a = tf.shape(data)\r\n    print(\"Data Size\", a.eval())\r\n    epoch_size = (batch_len - 1) // num_steps\r\n    print(\"Epoch_Size\", epoch_size.eval())\r\n    assertion = tf.assert_positive(\r\n        epoch_size,\r\n        message=\"epoch_size == 0, decrease batch_size or num_steps\")\r\n    with tf.control_dependencies([assertion]):\r\n      epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\r\n    # session.run(tf.global_variables_initializer())\r\n    threads = tf.train.start_queue_runners()\r\n    # session.run(threads)\r\n    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\r\n    print(\"i\", i.eval())\r\n    x = tf.strided_slice(data, [0, i * num_steps],\r\n                         [batch_size, (i + 1) * num_steps])\r\n    print(\"x\", x.eval())\r\n    x.set_shape([batch_size, num_steps])\r\n    print(\"x\", x.eval())\r\n    y = tf.strided_slice(data, [0, i * num_steps + 1],\r\n                         [batch_size, (i + 1) * num_steps + 1])\r\n    print(\"y\", y.eval())\r\n    y.set_shape([batch_size, num_steps])\r\n    print(\"y\", y.eval())\r\n    return x, y\r\n\r\nraw_data = ptb_raw_data(\"DataPath\")\r\nptb_producer(raw_data[0], batch_size=25, num_steps=7)\r\n```\r\n\r\nBut I could not get any output at print(\"i\", i.eval()) and the process also kept running"]}, {"number": 5395, "title": "Update examples/tutorials/mnist/mnist_softmax.py", "body": "The comment is wrong. tf.softmax(y) is predicted probability distribution while y_ is the label. Check more about cross-entropy(http://colah.github.io/posts/2015-09-Visual-Information/#cross-entropy)", "comments": ["Can one of the admins verify this patch?\n", "@littlekfc, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @willdzeng to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks for the PR @littlekfc.  I think the existing formulation is correct.  See http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression#Cost_Function for some more details.\n\nIn this case, y_ is a matrix of [batch_size, 10] where each row is a one-hot vector indicating the class labels 0-9.  Each one-hot vector is the indicator variable vector for the sample.\n\nSo when computing the log-likelihood, we apply the softmax function on the logits, and then take a log of the output of the softmax, and then multiply that (batch-wise) with the one-hot vector indicating the label.\n\nAs far as I can tell, this equation is correct, though I admit it can be a bit confusing.  Definitely let me know if we're wrong after looking into this more!\n"]}, {"number": 5394, "title": "Distributed training hangs", "body": "As mentioned in #4651, during training with syncreplicasoptimizer, tensorflow must run into hanging(after thousands steps). This does not happen with simple network setup. \r\nBoth workers and pses are still \"alive\", no errors/exceptions are reported. The saver thread keeps working.\r\n\r\nEnvironment info\r\nthe cluster is set on 2 servers, one ps and one worker on each. And each worker works on 4 gpu cards.\r\n\r\nOperating System:\r\nCentos (customized)\r\n\r\nInstalled version of CUDA and cuDNN: \r\nCUDA 7.5\r\ncnDNN 5.1\r\n\r\ninstalled from source,  \r\nbuild on branch r0.11, and r0.10\r\n\r\ntrace log after hanging\r\n[ps0_pstack.txt](https://github.com/tensorflow/tensorflow/files/570595/ps0_pstack.txt)\r\n[ps0_strace.txt](https://github.com/tensorflow/tensorflow/files/570597/ps0_strace.txt)\r\n[ps1_pstack.txt](https://github.com/tensorflow/tensorflow/files/570598/ps1_pstack.txt)\r\n[ps1_strace.txt](https://github.com/tensorflow/tensorflow/files/570602/ps1_strace.txt)\r\n[worker0_pstack.txt](https://github.com/tensorflow/tensorflow/files/570601/worker0_pstack.txt)\r\n[worker0_strace.txt](https://github.com/tensorflow/tensorflow/files/570600/worker0_strace.txt)\r\n[worker1_pstack.txt](https://github.com/tensorflow/tensorflow/files/570596/worker1_pstack.txt)\r\n[worker1_strace.txt](https://github.com/tensorflow/tensorflow/files/570599/worker1_strace.txt)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["If you need the setup to reproduce this issue, please message me your email address.\n", "I'm sorry, but I'm closing this for now for these reasons.\n- Providing reproduction instructions was why #4651 was closed, and you still have not provided them. This type of problem cannot be solved by simple inspection of the stack trace (@mrry, you don't have any magical solution do you).\n- Providing them for email removes visibility to your issue which is at odds with the idea of community support.\n- CentOS is an unsupported platform. @jart's example of someone getting help was on an Ubuntu platform. \n", "See #5394\n", "From @mrry: Maybe the queue runner on the chief wasn't started, which is required for synchronous updates. No way to verify without the code.\n", "The problem does not happen any more on current version of RDMA. Seems like an issue on GRPC.", "Maybe it's an instance of the bug fixed by https://github.com/grpc/grpc/pull/12648? We should get an updated ref to gRPC soon, which might fix this."]}]