[{"number": 24039, "title": "Reserve because we know the size before, helps save reallocation cost.", "body": "Reserve vector in advance to save reallocation cost.", "comments": ["Nagging Reviewer @suharshs: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@suharshs this should be a pretty simple merge, please let me know if you need anything from me.", "Is there anything else needed from my side?", "> Is there anything else needed from my side?\r\n\r\nWe are looking into this. Will let you know if we need your help again. Thanks !"]}, {"number": 24038, "title": "Fix mkl_softmax integration error when the input tensor is mkl format", "body": "Bugs of master:\r\nThe method to get input_dims is wrong when input is mkl tensor.\r\nThe layout_type is wrong when input is mkl tensor.\r\n\r\nThis PR fix these two bugs", "comments": ["Please resolve the merge conflict. Thanks.", "We submit another the same PR, and close this."]}, {"number": 24037, "title": "Fix deprecated use of `sparse_to_dense`.", "body": "Calling `sparse_to_dense` gives a deprecation warning that asks users to use `sparse.to_dense`:\r\nhttps://github.com/tensorflow/tensorflow/blob/71f40f044450736cd6acd29e92ffbfc0e571ee14/tensorflow/python/ops/sparse_ops.py#L952-L955\r\n\r\nHowever, `sparse.to_dense` calls `sparse_to_dense`, which again produces the deprecation warning.", "comments": ["Which version is that? 1.13.1? How can I check?"]}, {"number": 24036, "title": "lite: promote precision of quantization multiplier (r1.12 branch)", "body": "The original implementation loss precision when calculating\r\n`input_product_scale`. This patch fixes it.\r\n\r\nTest: bazel run tensorflow/contrib/lite/kernels:kernel_util_test", "comments": ["This PR addresses https://github.com/tensorflow/tensorflow/issues/23800.\r\n\r\nThere are other precision losing of the *scale related multiply* in TFLite quantization implementation elsewhere, but let's discuss about this first.", "Btw, this `real_multiplier` precision impacts the `quantized_multiplier` which eventually impacts the fixed point output of Conv. ", "Btw, this issue exists on nearly all branches. I think it's more convenient for you guys to merge them internally. ", "@jackwish  Can you please run clang-format on your code.", "@hgadig format done. Sorry for missing it before. I put the formatting changes in a new commit to avoid confusion of the code change. Please let me know if you guys prefer them in one commit.", "@suharshs  @liyunlu0618  Could you please review and approve this ?", "Hi guys, seems no activity for a period?", "Apologies, i had thought I had already approved this.", "> Apologies, i had thought I had already approved this.\r\n\r\nThanks. The newly added clang-format commit removes the *approved* tag...", "@jackwish  Request you to rebase as I see some failures. ", "Seems no update to branch `r1.12` recently.\r\n\r\nI looked into the `Ubuntu CC \u2014 Internal CI build failed` which is marked with **Required** above, and found error message below.\r\n```\r\nERROR: Skipping '//tensorflow/lite/...': no targets found beneath 'tensorflow/lite'\r\n```\r\n\r\nIs this building process targeting `master` branch? For `r1.12` branch, the TFLite target should be `//tensorflow/contrib/lite`, which is moved as `//tensorflow/lite` since `r1.13`. (This is why I have this PR and #25302 for `master` branch)", "closing this PR, as this changes are not pushed in to master branch", "@jackwish I guess your change has to be made in the master branch, didn't realize it was on already created branch before. Could you do that instead?", "> @jackwish I guess your change has to be made in the master branch, didn't realize it was on already created branch before. Could you do that instead?\r\n\r\n@suharshs There is a master branch patch #25302 which is pending merge. So, the public repo accepts patch to master branch only?", "Great, thanks! Yes, that's right. Each branch corresponds to a particular release cutoff, so new changes only go to master and get included in future releases."]}, {"number": 24035, "title": "how to take out the output of a layer", "body": "I am using the code from here:\r\nhttps://www.tensorflow.org/tutorials/estimators/cnn\r\nhow to take out the output of pool2_flat?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 24034, "title": "the performance of tensorflow distributed ", "body": "I have trained a speech recognition network using tensorflow both on multi-gpu and distributed. there is some question that i have been puzzled a long time. that's my [code,](https://github.com/dingevin/distributed-training) when i'm training i faced two questions:\r\n1. **the accuracy of distributed not equal to multi-gpu**. in multi-gpu version, i use 2 gpu and training 8000 steps, the accuracy can reach 32%; however the accuracy only can reach 28% at distributed version(i have 2 ps 2 worker, and each worker use 2 gpu), each worker training 4000 steps, and the learning rate was same. it seems the parameters not share.\r\nmulti-gpu: (when training 4000 steps\uff0c loss is reduce to about 5.0)\r\n```\r\nINFO:tensorflow:Step #4015: lr: 0.000100, time 2018-11-28 16:35:16, accuracy 17.12786%, cross entropy 5.230135(147.0 examples/sec; 0.218 sec/batch)\r\nINFO:tensorflow:Step #4016: lr: 0.000100, time 2018-11-28 16:35:17, accuracy 16.93807%, cross entropy 5.064457(151.4 examples/sec; 0.211 sec/batch)\r\nINFO:tensorflow:Step #4017: lr: 0.000100, time 2018-11-28 16:35:17, accuracy 18.51658%, cross entropy 5.146353(149.4 examples/sec; 0.214 sec/batch)\r\nINFO:tensorflow:Step #4018: lr: 0.000100, time 2018-11-28 16:35:17, accuracy 18.98760%, cross entropy 4.905624(145.8 examples/sec; 0.219 sec/batch)\r\nINFO:tensorflow:Step #4019: lr: 0.000100, time 2018-11-28 16:35:17, accuracy 19.00991%, cross entropy 4.929277(139.1 examples/sec; 0.230 sec/batch)\r\nINFO:tensorflow:Step #4020: lr: 0.000100, time 2018-11-28 16:35:17, accuracy 15.84382%, cross entropy 5.140810(139.6 examples/sec; 0.229 sec/batch)\r\nINFO:tensorflow:Step #4021: lr: 0.000100, time 2018-11-28 16:35:18, accuracy 20.37208%, cross entropy 4.693164(143.1 examples/sec; 0.224 sec/batch)\r\n```\r\ndistributed ( training 2000 steps\uff0c the loss was reduce to 5.5\uff0c equal to 2000 steps at multi-gpu )\r\n```\r\nINFO:tensorflow:Step #2018: learning_rate:0.0001, accuracy 17.56266%, cross entropy 5.382931(15.6 examples/sec; 2.050 sec/batch)\r\nINFO:tensorflow:Step #2019: learning_rate:0.0001, accuracy 16.61609%, cross entropy 5.386977(21.3 examples/sec; 1.505 sec/batch)\r\nINFO:tensorflow:Step #2020: learning_rate:0.0001, accuracy 11.97795%, cross entropy 6.026215(29.3 examples/sec; 1.093 sec/batch)\r\nINFO:tensorflow:Step #2021: learning_rate:0.0001, accuracy 14.76162%, cross entropy 5.657806(32.7 examples/sec; 0.977 sec/batch)\r\nINFO:tensorflow:Step #2022: learning_rate:0.0001, accuracy 15.25826%, cross entropy 5.588447(29.4 examples/sec; 1.087 sec/batch)\r\nINFO:tensorflow:Step #2023: learning_rate:0.0001, accuracy 13.63419%, cross entropy 5.882462(28.7 examples/sec; 1.117 sec/batch)\r\nINFO:tensorflow:Step #2024: learning_rate:0.0001, accuracy 15.66866%, cross entropy 5.585463(37.7 examples/sec; 0.849 sec/batch)\r\n```\r\n2. **the speed of distributed version was so slow**. the bandwidth is 125Mb/s, is not the bottleneck.\r\ni execute the command as follow on every worker:\r\n```\r\nCUDA_VISIBLE_DEVICES='' nohup python distributed_train.py --ps_hosts=gpu42-hca:2220, gpu47-hca:2221 --worker_hosts=gpu42-hca:3330,gpu47-hca:3331 --job_name=ps --task_index=0 &\r\nCUDA_VISIBLE_DEVICES='0,1' nohup python distributed_train.py --ps_hosts=gpu42-hca:2220,gpu47-hca:2221 --worker_hosts=gpu42-hca:3330,gpu47-hca:3331 --job_name=worker --task_index=0 --gpu_num=2 --learning_rate=0.0001--how_many_training_steps=4000 --save_step_interval=5000 &\r\n```\r\nand another worker:\r\n```\r\nCUDA_VISIBLE_DEVICES='' nohup python distributed_train.py --ps_hosts=gpu42-hca:2220, gpu47-hca:2221 --worker_hosts=gpu42-hca:3330,gpu47-hca:3331 --job_name=ps --task_index=1 &\r\nCUDA_VISIBLE_DEVICES='0,1' nohup python distributed_train.py --ps_hosts=gpu42-hca:2220,gpu47-hca:2221 --worker_hosts=gpu42-hca:3330,gpu47-hca:3331 --job_name=worker --task_index=1 --gpu_num=2 --learning_rate=0.0001--how_many_training_steps=4000 --save_step_interval=5000 &\r\n```\r\n\r\nwas my code wrong? or i misunderstand distributed? can somebody give me some advice. thanks.\r\n\r\n", "comments": ["__the accuracy of distributed not equal to multi-gpu__: usually the case. Considering your batch size doubles if you use 2x number of GPU. You need to adjust your learning rate accordingly to reach the same convergence level.\r\n\r\n__bandwidth is 125Mb/s, is not the bottleneck__: seems too rough. What's is your model size and how long does each step take? You may use networking bandwidth monitoring tools like `jnettop` or `collectl` to see your actual network traffic.\r\n\r\nPS: we usually use at least 10Gbps network for distributed DL, which is 10x than that of your environment.", "@byronyi  according to your advice, i have read **Linear Scaling Rule**  and adjust my learning rate. thanks. as for the training speed, i will to see the actual network traffic.", "I will suggest you to keep the batch size and number of training steps same in both cases before you trying to track down the issue to distributed TF.\r\n\r\nFor example, if you use batch size 128 per GPU in 2 GPU case (256 in total) then you could try 64 per GPU in 4 GPU (256 in total) case and keep your lr and number of epoch the same. \r\n\r\nLinear scaling is fine, but it doesn\u2019t mean you get exactly the same results.", "Closing this as it is in \"awaiting response\" status for more than 7 days. Feel free to add your comments and we will reopen(if required)."]}, {"number": 24033, "title": "[TF1.12][graph_transforms] The INT8 performance test of graph_transforms for inceptionv3 model, GPU utilization extremely low.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.15\r\n- GCC/Compiler version (if compiling from source): 4.9.3\r\n- CUDA/cuDNN version: 9.0/7.0.5\r\n- GPU model and memory: TitanXP\r\n\r\n**Describe the current behavior**\r\nI prepared an inception v3 model and try to quantify it by the tool \"graph_transforms\".\r\nThis is my conversion command:\r\n`bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=frozen_graph.pb --out_graph=out_graph.pb --inputs='Placeholder:0' --outputs='InceptionV3/Logits/SpatialSqueeze:0' --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"16,299,299,3\") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes strip_unused_nodes sort_by_execution_order'`\r\n\r\nThen I evaluated the accuracy loss by test dataset which is consists of ~80k images. The result shows the accuracy decreased from 70.1% to 64.03%.\r\n\r\nThen I try to evaluate the GPU performance by repeatedly run a batch of images, and the GPU utilization is extremely low and the speed is very slow. \r\n```\r\n$ nvidia-smi dmon -i 0\r\n# gpu   pwr  temp    sm   mem   enc   dec  mclk  pclk\r\n# Idx     W     C     %     %     %     %   MHz   MHz\r\n    0    59    35     2     0     0     0  5508  1430\r\n    0    59    35     0     0     0     0  5508  1430\r\n    0    59    36     2     0     0     0  5508  1430\r\n    0    59    36     0     0     0     0  5508  1430\r\n    0    59    36     2     0     0     0  5508  1430\r\n    0    59    36     2     0     0     0  5508  1430\r\n    0    59    36     0     0     0     0  5508  1430\r\n    0    59    36     3     0     0     0  5508  1430\r\n    0    59    36     0     0     0     0  5508  1430\r\n    0    59    36     2     0     0     0  5508  1430\r\n    0    59    36     0     0     0     0  5508  1430\r\n    0    60    36     2     0     0     0  5508  1430\r\n    0    59    36     4     0     0     0  5508  1430\r\n    0    59    36     0     0     0     0  5508  1430\r\n    0    60    36     3     0     0     0  5508  1430\r\n    0    59    37     0     0     0     0  5508  1430\r\n    0    59    37     2     0     0     0  5508  1430\r\n    0    59    37     2     0     0     0  5508  1430\r\n    0    59    36     2     0     0     0  5508  1430\r\n    0    59    37     3     0     0     0  5508  1430\r\n    0    59    37     0     0     0     0  5508  1430\r\n    0    59    37     3     0     0     0  5508  1430\r\n    0    59    37     0     0     0     0  5508  1430\r\n    0    59    37     2     0     0     0  5508  1430\r\n    0    59    37     4     0     0     0  5508  1430\r\n```\r\n\r\nThere shall exists some bugs in this tool, which is related to the GPU utilization.\r\n\r\nAny idea will be welcome.", "comments": ["Below is the GPU utilization info when running on original frozen model.\r\n```\r\n$ nvidia-smi dmon -i 0\r\n# gpu   pwr  temp    sm   mem   enc   dec  mclk  pclk\r\n# Idx     W     C     %     %     %     %   MHz   MHz\r\n    0   237    63    82    32     0     0  5508  1822\r\n    0   204    63    85    34     0     0  5508  1809\r\n    0   258    64    81    31     0     0  5508  1809\r\n    0   213    64    83    33     0     0  5508  1847\r\n    0   263    64    85    34     0     0  5508  1797\r\n    0   277    65    85    35     0     0  5508  1822\r\n    0   179    65    83    33     0     0  5508  1835\r\n    0   233    65    84    34     0     0  5508  1835\r\n    0   217    66    83    33     0     0  5508  1809\r\n    0   262    66    85    33     0     0  5508  1797\r\n    0   205    66    84    33     0     0  5508  1797\r\n    0   250    67    84    33     0     0  5508  1809\r\n    0   236    67    83    32     0     0  5508  1771\r\n    0   208    67    83    32     0     0  5508  1771\r\n    0   230    67    85    33     0     0  5508  1771\r\n    0   250    67    84    33     0     0  5508  1797\r\n    0   208    68    83    32     0     0  5508  1784\r\n    0   213    68    83    32     0     0  5508  1784\r\n    0   249    68    83    32     0     0  5508  1771\r\n    0   255    69    85    34     0     0  5508  1797\r\n    0   240    69    82    32     0     0  5508  1809\r\n    0   172    69    83    32     0     0  5508  1771\r\n    0   254    69    85    33     0     0  5508  1784\r\n    0   184    69    82    31     0     0  5508  1771\r\n    0   260    70    85    33     0     0  5508  1809\r\n    0   190    70    85    34     0     0  5508  1809\r\n\r\n```\r\n", "Is this issue still relevant? There has been no activity since early December."]}, {"number": 24032, "title": "TFTRT and `tf.keras.application.ResNet50` INT8 and FP32 not working", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n**Ubuntu 18.04**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **tf-nightly (1.13.0-dev20181127).**\r\n- Python version: **3.5**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source):  **N/A**\r\n CUDA/cuDNN version: **9.0 / 7.3**\r\n- GPU model and memory: **GTX 1080 Ti (11GB)**\r\n\r\n**Describe the current behavior**\r\n\r\nConversion to FP32 using TensorRT fails on `tf.keras.applications.InceptionResNetV2`, `tf.keras.applications.InceptionV3` with:\r\n\r\n```\r\nEngine my_trt_op_0 creation for segment 0, composed of 2049 nodes failed:\r\nInvalid argument: Validation failed for TensorRTInputPH_0 and input slot 0:\r\nInput tensor with shape [?,?,?,3] has an unknown non-batch dimemension at dim 1.\r\n```\r\n\r\nWith `tf.keras.applications.VGG19`, conversion to FP32 fails with:\r\n\r\n```\r\npython: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion `d.nbDims >= 3' failed.\r\n``` \r\n\r\nFor `tf.keras.applications.ResNet50`, conversion to FP32 succeed but with INT8, the error is similar to `VGG19` with FP32:\r\n\r\n```\r\npython: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion `d.nbDims >= 3' failed.```\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nGiven that it is possible to convert TensorFlow models such as Inception to TensorRT as evidenced by https://github.com/NVIDIA-AI-IOT/tf_trt_models, the conversion should similarly work for models in `tf.keras.applications.*`.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.tensorrt as trt\r\nimport utils\r\nimport time\r\n\r\nfrom imutils import paths\r\nfrom tensorflow.python.client import session as csess\r\nfrom tensorflow.python.framework import importer as importer\r\nfrom tensorflow.python.framework import ops as ops\r\nfrom tensorflow.core.protobuf import config_pb2 as cpb2\r\nfrom tensorflow.python.platform import gfile\r\n\r\n\r\nprint(tf.__version__)\r\n\r\ngpu_memory = 10000000000\r\ntrt_frac = 0.5\r\n\r\n# Create a session first...\r\nsess = tf.Session(config=tf.ConfigProto(\r\n    gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=trt_frac)))\r\n\r\ntf.keras.backend.set_session(sess)\r\n\r\ninput_tensor = tf.placeholder(tf.float32, shape=(None, 224, 224, 3), name='image_tensor')\r\n\r\ntf.keras.backend.set_learning_phase(0)\r\n\r\n# NOTE: ResNet50 doesn't work with INT8\r\n# python: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion `d.nbDims >= 3' failed.\r\nmodel = tf.keras.applications.ResNet50(weights='imagenet')\r\n\r\n# NOTE: MobileNetV2 doesn't work with INT8\r\n# python: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion `d.nbDims >= 3' failed.\r\n# model = tf.keras.applications.MobileNetV2(weights='imagenet')\r\n\r\n# NOTE: InceptionResNetV2 and InceptionV3 doesn't work:\r\n# Engine my_trt_op_0 creation for segment 0, composed of 2049 nodes failed:\r\n# Invalid argument: Validation failed for TensorRTInputPH_0 and input slot 0:\r\n# Input tensor with shape [?,?,?,3] has an unknown non-batch dimemension at dim 1.\r\n# model = tf.keras.applications.InceptionResNetV2(weights='imagenet')\r\n# model = tf.keras.applications.InceptionV3(weights='imagenet')\r\n\r\n# NOTE: VGG19 doesn't work\r\n# python: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion `d.nbDims >= 3' failed.\r\n# model = tf.keras.applications.VGG19(weights='imagenet')\r\n\r\nmodel_input = model.input.name.strip(':0')\r\nmodel_output = model.output.name.strip(':0')\r\n\r\nprint(model_input, model_output)\r\n\r\ngraph = tf.get_default_graph().as_graph_def()\r\n\r\n# freeze graph and remove nodes used for training\r\nfrozen_graph = tf.graph_util.convert_variables_to_constants(sess, graph, [model_output])\r\nfrozen_graph = tf.graph_util.remove_training_nodes(frozen_graph)\r\n\r\nnodes = [node.name for node in frozen_graph.node]\r\nprint(nodes)\r\n\r\n# Make this the default graph now\r\ntf.import_graph_def(frozen_graph, name='')\r\n\r\nmax_workspace_size_bytes = int(trt_frac * gpu_memory)\r\n\r\nprecision_mode = 'INT8' # FP32 | INT8\r\n\r\ntrt_graph = trt.create_inference_graph(\r\n    input_graph_def=frozen_graph,\r\n    outputs=[model_output],\r\n    max_batch_size=1,\r\n    max_workspace_size_bytes=max_workspace_size_bytes,\r\n    precision_mode=precision_mode,\r\n    minimum_segment_size=3,\r\n    maximum_cached_engines=3,\r\n    is_dynamic_op=False\r\n)\r\n\r\ntrt_engine_opts = len([1 for n in trt_graph.node if str(n.op) == 'TRTEngineOp'])\r\nprint('TRT Engine Op: {}'.format(trt_engine_opts))\r\nassert trt_engine_opts > 0, 'No TRT Engine Ops!'\r\n\r\nif precision_mode == 'INT8':\r\n    with gfile.GFile(\"frozen/frozen_graph_INT8CALIB.pb\", 'wb') as f:\r\n        f.write(trt_graph.SerializeToString())\r\nelse:\r\n    with gfile.GFile(\"frozen/frozen_graph_FP32.pb\", 'wb') as f:\r\n        f.write(trt_graph.SerializeToString())\r\n\r\n\r\nif precision_mode == 'INT8':\r\n    gpu_options = cpb2.GPUOptions(per_process_gpu_memory_fraction=trt_frac)\r\n    sess_config = cpb2.ConfigProto(gpu_options=gpu_options)\r\n\r\n    ops.reset_default_graph()\r\n    g1 = ops.Graph()\r\n    with g1.as_default():\r\n        inp, out = importer.import_graph_def(\r\n            graph_def=trt_graph,\r\n            return_elements=[model_input, model_output]\r\n        )\r\n        inp = inp.outputs[0]\r\n        out = out.outputs[0]\r\n    with csess.Session(\r\n            config=sess_config, graph=g1) as sess:\r\n        for file_name in list(paths.list_images('samples'))[0:10]:\r\n            input = utils.read_tensor_from_image_file(file_name,\r\n                                                      input_height=224,\r\n                                                      input_width=224)\r\n            start_time = time.time()\r\n            val = sess.run(out, {inp: input})\r\n            stop_time = time.time()\r\n            print('Inference time is {}'.format(stop_time - start_time))\r\n            print('Running {}'.format(file_name))\r\n\r\n    trt_graph = trt.calib_graph_to_infer_graph(trt_graph)\r\n\r\n    with gfile.GFile(\"frozen/frozen_graph_INT8.pb\", 'wb') as f:\r\n        f.write(trt_graph.SerializeToString())\r\n\r\n    ops.reset_default_graph()\r\n    g2 = ops.Graph()\r\n    with g2.as_default():\r\n        # Run the inference\r\n\r\n        inp, out = importer.import_graph_def(\r\n            graph_def=trt_graph,\r\n            return_elements=[model_input, model_output]\r\n        )\r\n\r\n        inp = inp.outputs[0]\r\n        out = out.outputs[0]\r\n\r\n    with csess.Session(\r\n            config=sess_config, graph=g2) as sess:\r\n        for file_name in list(paths.list_images('samples'))[0:10]:\r\n            input = utils.read_tensor_from_image_file(file_name,\r\n                                                      input_height=224,\r\n                                                      input_width=224)\r\n            start_time = time.time()\r\n            val = sess.run(out, {inp: input})\r\n            stop_time = time.time()\r\n            print('Inference time is {}'.format(stop_time - start_time))\r\n            print('Running {}'.format(file_name))\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI have also tried messing around with every combination of `max_batch_size`, `minimum_segment_size`, `maximum_cached_engines` but no joy.\r\n\r\n", "comments": ["Hi, thanks for reporting the issue. \r\n\r\n* `Input tensor with shape [?,?,?,3] has an unknown non-batch dimemension at dim 1.`\r\nCould you try with is_dynamic_op=True for create_inference_graph? TF-TRT requires all shapes to be fully defined unless in dynamic mode. You could also have the shapes be explicitly defined by doing the following:\r\n```\r\ninput_tensor = Input(shape=(224, 224, 3))  # this assumes K.image_data_format() == 'channels_last'\r\nmodel = InceptionV3(input_tensor=input_tensor)\r\n```\r\n\r\n*  `Assertion `d.nbDims >= 3' failed.`\r\nWhich version of TensorRT are you using? The message you are seeing is a bug in TRT 4.0 with INT8, which was fixed in TRT 5.0.", "Closing this. Please reopen if @trevor-m 's suggestions don't work."]}, {"number": 24031, "title": "ImportError: module could not be found - Windows 10", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10.0.17134.407\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.0.130 / 7.4.1.5 for Win 10\r\n- GPU model and memory: Quadro M520\r\n\r\n\r\n\r\n**Describe the problem**\r\ntensorflow fails on import with \"module could not be found\".  Other issue says to open a new issue due to differing config (https://github.com/tensorflow/tensorflow/issues/22794)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. python\r\n2. import tensorflow as tf\r\n\r\n\r\n\r\n\r\n**Any other info / logs**\r\nPython 3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\marktayl\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\marktayl\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\marktayl\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\marktayl\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\marktayl\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\marktayl\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\marktayl\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\marktayl\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\marktayl\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\marktayl\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\marktayl\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\marktayl\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\marktayl\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Duplicate of #23726"]}, {"number": 24030, "title": "Windows Bazel build fails when using TF_SYSTEM_LIBS", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 Pro\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.11\r\n- Python version: 3.5.5\r\n- Installed using: conda\r\n- Bazel version: 0.15.1\r\n- GCC/Compiler version: Visual Studio 14.0\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: NVIDIA GeForce GTX 960\r\n\r\n**Describe the problem**\r\n\r\nI can build  Tensorflow 1.11 normally using Bazel on Windows, but if I set any TF_SYSTEM_LIBS, then Bazel can't find the necessary headers.\r\n\r\nVia setting some options and messing with bazel's `copt` commandline argument and the CC_OPT_FLAGS environment variable, I was once able to get bazel to find zlib.h, but now I can't figure out what I did to make that happen. When that happened, I got a different error, something about zlib.h being an undeclared dependency.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nsteps that I am running in Git Bash:\r\n```\r\nexport PYTHON_BIN_PATH=C:/Users/nstier/AppData/Local/Continuum/anaconda3/python.exe\r\nexport PYTHON_LIB_PATH=C:/Users/nstier/AppData/Local/Continuum/anaconda3/Lib\r\n\r\nexport TF_CUDA_VERSION=9.0\r\nexport TF_CUDNN_VERSION=7\r\nexport CUDA_TOOLKIT_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\"\r\nexport CUDNN_INSTALL_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\"\r\n\r\nexport GCC_HOST_COMPILER_PATH=/usr/bin/gcc\r\nexport TF_CUDA_CLANG=0\r\n\r\n# zlib/ is symlinked into the tensorflow directory to avoid the error \"include path references outside of execution root....\" \r\nexport CC_OPT_FLAGS=\"/arch:AVX /I/c/Users/nstier/tensorflow/ExternalPackages/zlib-1.2.11-t1-VC15.8.1/include\"\r\n\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n\r\nexport TF_NEED_CUDA=1\r\nexport TF_NEED_JEMALLOC=1\r\nexport TF_ENABLE_XLA=0\r\nexport TF_NEED_OPENCL=0\r\nexport TF_NEED_OPENCL_SYCL=0\r\nexport TF_NEED_TENSORRT=0\r\nexport TF_NEED_NGRAPH=0\r\n\r\nexport TF_NEED_GCP=0\r\nexport TF_NEED_AWS=0\r\n\r\nexport TF_NEED_KAFKA=0\r\nexport TF_NEED_HDFS=0\r\nexport TF_NEED_GDR=0\r\nexport TF_NEED_VERBS=0\r\nexport TF_NEED_MPI=0\r\n\r\nexport TF_SET_ANDROID_WORKSPACE=0\r\nexport TMP=/tmp\r\nexport TF_SYSTEM_LIBS=zlib_archive\r\nexport TF_OVERRIDE_EIGEN_STRONG_INLINE=1\r\n\r\nbazel build \\\r\n  --config=opt \\\r\n  --config=cuda \\\r\n  --config=monolithic \\\r\n  --define=no_tensorflow_py_deps=true \\\r\n  --copt=/I/c/Users/nstier/tensorflow/ExternalPackages/zlib-1.2.11-t1-VC15.8.1/include \\\r\n  --cxxopt=/I/c/Users/nstier/tensorflow/ExternalPackages/zlib-1.2.11-t1-VC15.8.1/include \\\r\n   //tensorflow:libtensorflow_cc.so \\\r\n  //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nbuild log:\r\n<pre>\r\nStarting local Bazel server and connecting to it...\r\n.............\r\nWARNING: The following configs were expanded more than once: [cuda, monolithic]. For repeatable flags, repeats are counted twice anday lead to unexpected behavior.\r\nLoading:\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/tools/pip_package ... (2 packages)\r\nDEBUG: C:/users/nstier/_bazel_nstier/t2qf7f76/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\nDEBUG: C:/users/nstier/_bazel_nstier/t2qf7f76/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/nstier/_bazel_nstier/t2qf7f76/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\nDEBUG: C:/users/nstier/_bazel_nstier/t2qf7f76/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\nDEBUG: C:/users/nstier/_bazel_nstier/t2qf7f76/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/nstier/_bazel_nstier/t2qf7f76/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\nAnalyzing: 2 targets (2 packages loaded)\r\nAnalyzing: 2 targets (150 packages loaded)\r\nWARNING: C:/users/nstier/tensorflow/tensorflow/core/BUILD:2464:1: in includes attribute of cc_library rule //tensorflow/core:framewo_internal_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its paage 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the eor might have been caused by the macro implementation in C:/users/nstier/tensorflow/tensorflow/tensorflow.bzl:1373:20\r\nWARNING: C:/users/nstier/tensorflow/tensorflow/core/BUILD:2549:1: in includes attribute of cc_library rule //tensorflow/core:framewo_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'teorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error mighhave been caused by the macro implementation in C:/users/nstier/tensorflow/tensorflow/tensorflow.bzl:1373:20\r\nWARNING: C:/users/nstier/_bazel_nstier/t2qf7f76/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopbplease do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on  appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused  the macro implementation in C:/users/nstier/_bazel_nstier/t2qf7f76/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: C:/users/nstier/_bazel_nstier/t2qf7f76/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopbplease do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on  appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused  the macro implementation in C:/users/nstier/_bazel_nstier/t2qf7f76/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: C:/users/nstier/_bazel_nstier/t2qf7f76/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopbplease do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on  appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused  the macro implementation in C:/users/nstier/_bazel_nstier/t2qf7f76/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: C:/users/nstier/tensorflow/tensorflow/core/BUILD:2563:1: in includes attribute of cc_library rule //tensorflow/core:stream_ecutor_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its packa 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the erromight have been caused by the macro implementation in C:/users/nstier/tensorflow/tensorflow/tensorflow.bzl:1373:20\r\nWARNING: C:/users/nstier/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. itch to SavedModel immediately.\r\nWARNING: C:/users/nstier/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switcho SavedModel immediately.\r\nWARNING: C:/users/nstier/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/conib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated rget '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https:/ithub.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will bremoved by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: C:/users/nstier/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rul//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/teseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': Tensorow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in .contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.dtributions` to `tfp.distributions`.\r\nWARNING: C:/users/nstier/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library ru //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseri/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distbutions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecatedopies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all uge of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: C:/users/nstier/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_p target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py'TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaing in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.ctrib.distributions` to `tfp.distributions`.\r\nWARNING: C:/users/nstier/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distribions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated coes remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usagof `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: C:/users/nstier/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tenrflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributio has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distrutions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` totfp.distributions`.\r\nINFO: Analysed 2 targets (297 packages loaded).\r\nINFO: Found 2 targets...\r\nBuilding: no action\r\n[1 / 8] [-----] BazelWorkspaceStatusAction stable-status.txt\r\n<b>ERROR: C:/users/nstier/_bazel_nstier/t2qf7f76/external/grpc/BUILD:1443:1: C++ compilation of rule '@grpc//:grpc_transport_chttp2' faed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/nstier/_bazel_nstier/t2qf7f76/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\ilude\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\l\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Proam Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Programiles (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/nstier/AppData/Local/Continuum/anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/nstier/AppData/Local/Continuum/anaconda3/Lib\r\n    SET TEMP=C:\\Users\\nstier\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_CUDA_VERSION=9.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_SYSTEM_LIBS=zlib_archive\r\n    SET TMP=C:\\Users\\nstier\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /DRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351wd4291 /wd4250 /wd4996 /Iexternal/grpc /Ibazel-out/x64_windows-opt/genfiles/external/grpc /Iexternal/bazel_tools /Ibazel-out/x64_winws-opt/genfiles/external/bazel_tools /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Iexternal/gc/include /Ibazel-out/x64_windows-opt/genfiles/external/grpc/include /Ibazel-out/x64_windows-opt/bin/external/grpc/include /DGRPC_AR=0 /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX /I/c/Users/nstier/tensorflow/ExternalPackages/zlib-1.2.11-t1-VC15.8.1/include /I/c/Uss/nstier/tensorflow/ExternalPackages/zlib-1.2.11-t1-VC15.8.1/include /I/c/Users/nstier/tensorflow/ExternalPackages/zlib-1.2.11-t1-VC.8.1/include /Fobazel-out/x64_windows-opt/bin/external/grpc/_objs/grpc_transport_chttp2/hpack_parser.o /c external/grpc/src/core/extransport/chttp2/transport/hpack_parser.cc\r\nexternal/grpc\\src/core/lib/compression/stream_compression.h(27): fatal error C1083: Cannot open include file: 'zlib.h': No such file or directory </b>\r\nINFO: Elapsed time: 33.850s, Critical Path: 3.71s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n</pre>\r\nAny ideas?\r\n\r\nThis line from the logs looks interesting: \"set INCLUDE=...\". Not sure how to get the directories that I want into that list. And if I manage to do that, I might end up with the \"undeclared dependency error.\"\r\n", "comments": ["I do not think we put in any work into TF_SYSTEM_LIBS to work with windows.\r\n@angersson @perfinion to confirm.", "Yeah, the whole syslibs stuff is completely disabled on windows. I don't have any windows machines to test on but i'd be up for reviewing if you feel like making it work. You can can start library-by-library, dont have to do all at once.\r\nFirst thing you'd need to do would be look in `third_party/syslibs/syslib_configure` and in `third_party/repo.bzl` and remove the checks that disables it on windows.", "oh my gold, i can't find out which file run this command, the error is because my windows system have not this path \"C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\" after i uninstall the  newer version and now my system is \"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.10240.0\" version which give me this error when bazel build tensorflowlib_cc.so\r\n\r\nD:\\tf3\\tensorflow-r1.12>bazel build --config=opt --config=cuda //tensorflow:libtensorflow_cc.so\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nWARNING: C:/users/swls/_bazel_swls/5dz6uozl/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/swls/_bazel_swls/5dz6uozl/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: C:/users/swls/_bazel_swls/5dz6uozl/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/swls/_bazel_swls/5dz6uozl/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: C:/users/swls/_bazel_swls/5dz6uozl/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/swls/_bazel_swls/5dz6uozl/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nINFO: Analysed target //tensorflow:libtensorflow_cc.so (1 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/grpc/BUILD:388:1: C++ compilation of rule '@grpc//:grpc_plugin_support' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/swls/_bazel_swls/5dz6uozl/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    **SET INCLUDE**=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Python35/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Python35/lib/site-packages\r\n    SET TEMP=C:\\Users\\swls\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\swls\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/grpc /Ibazel-out/x64_windows-opt/genfiles/external/grpc /Ibazel-out/x64_windows-opt/bin/external/grpc /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/grpc/include /Ibazel-out/x64_windows-opt/genfiles/external/grpc/include /Ibazel-out/x64_windows-opt/bin/external/grpc/include /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /DGRPC_ARES=0 /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX2 /Fobazel-out/x64_windows-opt/bin/external/grpc/_objs/grpc_plugin_support/cpp_generator.o /c external/grpc/src/compiler/cpp_generator.cc\r\n\r\nC:\\users\\swls\\_bazel_swls\\5dz6uozl\\execroot\\org_tensorflow>set arg0=C:\\users\\swls\\_bazel_swls\\5dz6uozl\\execroot\\org_tensorflow\\external\\local_config_cuda\\crosstool\\windows\\msvc_wrapper_for_nvcc.bat\r\n\r\nC:\\users\\swls\\_bazel_swls\\5dz6uozl\\execroot\\org_tensorflow>for %F in (\"C:\\users\\swls\\_bazel_swls\\5dz6uozl\\execroot\\org_tensorflow\\external\\local_config_cuda\\crosstool\\windows\\msvc_wrapper_for_nvcc.bat\") do set DRIVER_BIN=%~dpF\r\n\r\nC:\\users\\swls\\_bazel_swls\\5dz6uozl\\execroot\\org_tensorflow>set DRIVER_BIN=C:\\Users\\swls\\_bazel_swls\\5dz6uozl\\execroot\\org_tensorflow\\external\\local_config_cuda\\crosstool\\windows\\\r\n\r\nC:\\users\\swls\\_bazel_swls\\5dz6uozl\\execroot\\org_tensorflow>\"C:/Python35/python.exe\" -B \"C:\\Users\\swls\\_bazel_swls\\5dz6uozl\\execroot\\org_tensorflow\\external\\local_config_cuda\\crosstool\\windows\\\\msvc_wrapper_for_nvcc.py\" /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/grpc /Ibazel-out/x64_windows-opt/genfiles/external/grpc /Ibazel-out/x64_windows-opt/bin/external/grpc /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/grpc/include /Ibazel-out/x64_windows-opt/genfiles/external/grpc/include /Ibazel-out/x64_windows-opt/bin/external/grpc/include /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /DGRPC_ARES=0 /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX2 /Fobazel-out/x64_windows-opt/bin/external/grpc/_objs/grpc_plugin_support/cpp_generator.o /c external/grpc/src/compiler/cpp_generator.cc\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\crtdefs.h(10): fatal error C1083: Cannot open include file: 'corecrt.h': No such file or directory\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nINFO: Elapsed time: 1.548s, Critical Path: 0.27s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n\r\nabove we can see SET INCLUDE command whcih include the error path \"C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared\",my windows only have \"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.10240.0\\shared\" ,so how can i find the code which run this \"SET ***=***\" because i want to modify it to correctly,please me, thanks\r\n@noahstier @gunan  @perfinion ", "@gunan  can you please share the timeline that we are able to build [tensorlfowlib 2.0 ](https://github.com/tensorflow/tensorflow/issues/35405)more reliably?", "I will close the issue. The last two comments are unrelated to the original issue, so please file new issues for them.\r\n\r\nThere may already be issues for these, but please keep in mind that I have 80+ issues assigned to me, and I am just back from holiday break.\r\nSo it may take some time for me to get to all the issues.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24030\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24030\">No</a>\n"]}, {"number": 24029, "title": "Inference Accuracy Regression for Resnet50 and Densenet", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NA\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Centos 7.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): master branch, commit 0c1eb886 onward\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 6.3.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nInference accuracy for Resnet50 and Densenet has dropped to zero on CPU. This commit https://github.com/tensorflow/tensorflow/commit/0c1eb8861624d6d17c797b70d25330711df5eb2f was found to be causing the regression.\r\n\r\n**Describe the expected behavior**\r\nResnet50 inference accuracy should look something close to (Top1, Top5) = (0.7315, 0.9109).\r\n", "comments": ["Hello @petermattson  , can you please check this resnet50 accuracy issue ? Thanks.", "Hi mysymp, I'm not right person for this one.", "@mahmoud-abuzaina Could you check and let us know whether the issue persists with latest TensorFlow2.0? Could you share a code to reproduce the bug? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 24028, "title": "TensorFlow estimator train_and_evaluate loss is None after step 0 and model does not train", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 14.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\n'1.4.0'\r\n- Python version:\r\nPython 2.7.12\r\n- Bazel version (if compiling from source):\r\n- CUDA/cuDNN version:\r\nUsing CPU\r\n**Describe the current behavior**\r\nI've built a tensorflow custom estimator using Keras layers, and it worked fine initially when I used `train_and_evaluate`, but I'm seeing now that when I am using `train_and_evaluate`, it just checkpoints at step 0, the loss being `None` and moves to the evaluate phase. I'm not sure why this is happening, and any suggestions about what to look for would be great\r\n**Describe the expected behavior**\r\nNormal behavior as expected from the `train_and_evaluate` function\r\n**Code to reproduce the issue**\r\n```\r\n# I've wrapped the estimator train_and_evaluate function inside this function:\r\ndef train_and_evaluate():    \r\n\r\n#     classifier = tf.estimator.Estimator(model_fn = dnn_custom_estimator_v2,\r\n#                                         config = run_config,\r\n#                                         params=hparams)\r\n\r\n    classifier = tf.estimator.Estimator(model_fn = dnn_custom_estimator_v2,\r\n                                     params=hparams,\r\n                                     config=run_config)\r\n\r\n    train_spec = tf.estimator.TrainSpec(\r\n        input_fn = lambda: csv_input_fn(\r\n            TRAIN_DATA_FILES_PATTERN,\r\n            mode = tf.estimator.ModeKeys.TRAIN,\r\n            num_epochs=1000\r\n        ),\r\n        max_steps=6000,\r\n        hooks=None\r\n    )\r\n\r\n    eval_spec = tf.estimator.EvalSpec(\r\n        input_fn = lambda: csv_input_fn(\r\n            TEST_DATA_FILES_PATTERN,\r\n            mode=tf.estimator.ModeKeys.EVAL,\r\n            num_epochs=1  \r\n        ),\r\n        exporters=[tf.estimator.LatestExporter(\r\n            name=\"predict\", # the name of the folder in which the model will be exported to under export\r\n            serving_input_receiver_fn=json_serving_input_fn,\r\n            exports_to_keep=1,\r\n            as_text=False)],\r\n        steps=None\r\n    )\r\n    \r\n    tf.estimator.train_and_evaluate(\r\n        estimator=classifier,\r\n        train_spec=train_spec, \r\n        eval_spec=eval_spec\r\n    )\r\n```\r\n```\r\n# The estimator function:\r\nhe_init = tf.keras.initializers.he_normal()\r\n\r\ndef build_dense_layer(X, n_units=32, activation=tf.keras.activations.relu, initialization=he_init,\r\n                          batch_normalization=False, kernel_regularizer=None, training=False, name=None):\r\n    layer = tf.keras.layers.Dense(n_units,\r\n                              activation=activation,\r\n                              kernel_initializer = he_init,\r\n                              kernel_regularizer = kernel_regularizer,\r\n                              name=name)(X)\r\n    if batch_normalization:\r\n        bn = tf.keras.layers.BatchNormalization(momentum=0.90)\r\n        layer = bn(layer, training=training)\r\n\r\n    return layer\r\n\r\n\r\n\r\ndef dnn_custom_estimator_v2(features, labels, mode, params):\r\n    in_training = mode == tf.estimator.ModeKeys.TRAIN\r\n    # Returns the feature columns after transforming them\r\n    continuos_feature_cols, dense_vector_feature_cols, embedding_feature_cols, weight_col = get_feature_columns()\r\n    \r\n    ### Build Vector Network\r\n    vec_features = {}\r\n    for feature_name in [some col names]:\r\n        vec_features[feature_name] = features[feature_name]\r\n\r\n    vec_net_ip = tf.feature_column.input_layer(vec_features, dense_vector_feature_cols)\r\n    ## Build embedding Network\r\n    embedding_features = {}\r\n    for feature_name in [Column names]:\r\n        embedding_features[feature_name] = features[feature_name]\r\n    weight_col = features['sample_weight']\r\n    embedding_net_ip = tf.feature_column.input_layer(embedding_features, feature_columns = embedding_feature_cols)\r\n    ## Continous feature Network\r\n    continous_cols = [Some column names]\r\n\r\n    continous_features = {}\r\n    for feature_name in continous_cols:\r\n        if feature_name != 'sample_weight':\r\n            continous_features[feature_name] = features[feature_name]\r\n\r\n    continous_net_ip = tf.feature_column.input_layer(continous_features, feature_columns = continuos_feature_cols)\r\n\r\n    ## Merge continous, embedding and vec layers together\r\n    merged_layer = tf.keras.layers.concatenate([vec_net_ip, embedding_net_ip, continous_net_ip])\r\n\r\n    ## OP deep dense layer\r\n    output_hidden_1 = build_dense_layer(merged_layer,\r\n                                        n_units=128, training=in_training, \r\n                                        batch_normalization = False, \r\n                                        activation = tf.keras.activations.relu, name = 'output_hidden_1')\r\n    output_hidden_2 = build_dense_layer(output_hidden_1, n_units=64, \r\n                        training=in_training, batch_normalization = False,\r\n                        activation = tf.keras.activations.relu, \r\n                        name = 'output_hidden_2')\r\n    \r\n    output_hidden_3 = build_dense_layer(output_hidden_2, n_units=32, \r\n                                    training=in_training, \r\n                                    batch_normalization = False,\r\n                                activation = tf.keras.activations.relu, name = 'output_hidden_3')\r\n    output_layer_size = len(TARGET_LABELS)\r\n\r\n    logits = build_dense_layer(output_hidden_3,\r\n                             n_units=output_layer_size, \r\n                            activation=None, name='prob_output')\r\n    output = tf.squeeze(logits)\r\n\r\n    # Provide an estimator spec for `ModeKeys.PREDICT`.\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        probabilities = tf.nn.softmax(logits)\r\n        predicted_indices = tf.argmax(probabilities, 1)\r\n        # Convert predicted_indices back into strings\r\n        predictions = {\r\n            'classes': tf.gather(TARGET_LABELS, predicted_indices),\r\n            'scores': probabilities\r\n        }\r\n        export_outputs = {\r\n            'prediction': tf.estimator.export.PredictOutput(predictions)\r\n        }\r\n\r\n        # Provide an estimator spec for `ModeKeys.PREDICT` modes.\r\n        return tf.estimator.EstimatorSpec(mode,\r\n                                          predictions=predictions,\r\n                                          export_outputs=export_outputs)\r\n\r\n    # Calculate loss using softmax cross entropy\r\n    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n            logits=logits, labels=labels)\r\n    \r\n    loss = tf.reduce_mean(weight_col*losses)\r\n    \r\n    tf.summary.scalar('loss', loss)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        # Create Optimiser\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.9, beta2=0.999,  epsilon=1e-8)\r\n\r\n        # Create training operation\r\n        train_op = optimizer.minimize(\r\n            loss=loss, global_step=tf.train.get_global_step())\r\n\r\n        # Provide an estimator spec for `ModeKeys.TRAIN` modes.\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          train_op=train_op)\r\n\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        probabilities = tf.nn.softmax(logits)\r\n        predicted_indices = tf.argmax(probabilities, 1)\r\n\r\n        # Return accuracy and area under ROC curve metrics\r\n        labels_one_hot = tf.one_hot(\r\n            labels,\r\n            depth=len(TARGET_LABELS),\r\n            on_value=True,\r\n            off_value=False,\r\n            dtype=tf.bool\r\n        )\r\n\r\n        eval_metric_ops = {\r\n            'accuracy': tf.metrics.accuracy(labels, predicted_indices),\r\n            'auroc': tf.metrics.auc(labels_one_hot, probabilities),\r\n            'precision': tf.metrics.precision(labels, predicted_indices),\r\n            'recall': tf.metrics.recall(labels, predicted_indices)\r\n        }\r\n\r\n        # Provide an estimator spec for `ModeKeys.EVAL` modes.\r\n        return tf.estimator.EstimatorSpec(mode,\r\n                                          loss=loss,\r\n                                          eval_metric_ops=eval_metric_ops)\r\n```\r\n\r\n\r\n\r\n**Other info / logs**\r\nThe output of `train_and_evaluate` as defined above is this:\r\n```\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': 19830610, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f86f040dc50>, '_model_dir': ' /whereever/model/needs/to/be/saved/model_v2', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}\r\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\r\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\r\n\r\n* Output from  data input_fn:\r\n================\r\nInput file(s):  /whereever/train/is/data/train_equal_downsample.csv\r\nBatch size: 1024\r\nEpoch Count: 1000\r\nMode: train\r\nThread Count: 32\r\nShuffle: True\r\n================\r\n\r\n('target_dtype', <tf.Tensor 'DecodeCSV:9' shape=(?,) dtype=int32>)\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Saving checkpoints for 0 into  /whereever/model/needs/to/be/saved//model.ckpt.\r\nINFO:tensorflow:Loss for final step: None.\r\n\r\n* data input_fn:\r\n================\r\nInput file(s):  /whereever/test/is/test_v2.csv\r\nBatch size: 1024\r\nEpoch Count: 1\r\nMode: eval\r\nThread Count: 32\r\nShuffle: False\r\n================\r\n\r\n('target_dtype', <tf.Tensor 'DecodeCSV:9' shape=(?,) dtype=int32>)\r\nINFO:tensorflow:Starting evaluation at 2018-11-28-21:52:28\r\nINFO:tensorflow:Restoring parameters from /wherever/model/is/model_v2/model.ckpt-0\r\nINFO:tensorflow:Finished evaluation at 2018-11-28-22:14:58\r\nINFO:tensorflow:Saving dict for global step 0: accuracy = 0.91067266, auroc = 0.94267476, global_step = 0, loss = 0.5579337, precision = 0.14235616, recall = 0.002937618\r\nINFO:tensorflow:Restoring parameters from /wherever/model/is/model_v2/model.ckpt-0\r\nINFO:tensorflow:Assets added to graph.\r\nINFO:tensorflow:No assets to write.\r\nINFO:tensorflow:SavedModel written to: /wherever/model/is/export/predict/temp-1543443300/saved_model.pb\r\n\r\n* data input_fn:\r\n================\r\nInput file(s): /wherever/this/file/is/train_equal_downsample.csv\r\nBatch size: 1024\r\nEpoch Count: 1000\r\nMode: train\r\nThread Count: 32\r\nShuffle: True\r\n================\r\n\r\n('target_dtype', <tf.Tensor 'DecodeCSV:9' shape=(?,) dtype=int32>)\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Restoring parameters from /whereever/model/needs/to/be/saved/model.ckpt-0\r\nINFO:tensorflow:Saving checkpoints for 0 into  /whereever/model/needs/to/be/saved//model.ckpt.\r\nINFO:tensorflow:Loss for final step: None.\r\nWARNING:tensorflow:No new checkpoint ready for evaluation. Skip the current evaluation pass as evaluation results are expected to be same for the same checkpoint.\r\n\r\n\r\nRuntimeErrorTraceback (most recent call last)\r\n<ipython-input-52-858f040bdf6d> in <module>()\r\n      6 if(not os.path.exists(output_dir)):\r\n      7     os.makedirs(output_dir)\r\n----> 8 train_and_evaluate()\r\n\r\n<ipython-input-51-6feac2ec48d2> in train_and_evaluate()\r\n     36         estimator=classifier,\r\n     37         train_spec=train_spec,\r\n---> 38         eval_spec=eval_spec\r\n     39     )\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.pyc in train_and_evaluate(estimator, train_spec, eval_spec)\r\n    428       config.task_type != run_config_lib.TaskType.EVALUATOR):\r\n    429     logging.info('Running training and evaluation locally (non-distributed).')\r\n--> 430     executor.run_local()\r\n    431     return\r\n    432 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.pyc in run_local(self)\r\n    618       if not metrics:\r\n    619         #  This is unexpected. Training should always end with a new checkpoint.\r\n--> 620         raise RuntimeError('There was no new checkpoint after the training.')\r\n    621 \r\n    622       if _should_stop_local_train(metrics[ops.GraphKeys.GLOBAL_STEP]):\r\n\r\nRuntimeError: There was no new checkpoint after the training.\r\n\r\n\r\n```", "comments": ["Could this be caused by an improperly formatted csv files?", "@ssubraveti did you solve this problem?  I met the same case when running TF with big data and it's quite annoying", "@ssubraveti @Zhiqiang-Ye How did you solve this problem? I also met this problem.", "I'm not sure... I rewrote the estimator from scratch because I wasn't able to find a reason for the bug, and it worked after that \ud83d\ude05", "I found the problem.\r\nYou set max steps as max_steps = 6000, yet it is not same as **steps** from tf.estimator.train(... steps,max_steps). As stated in documentation of train_and_evaluate \r\n[https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate](url)\r\n> Stop condition: In order to support both distributed and non-distributed configuration reliably, the only supported stop condition for model training is train_spec.max_steps. If train_spec.max_steps is None, the model is trained forever. Use with care if the model stop condition is different. For example, assume that the model is expected to be trained with one epoch of training data, and the training input_fn is configured to throw OutOfRangeError after going through one epoch, which stops the Estimator.train. For a three-training-worker distributed configuration, each training worker is likely to go through the whole epoch independently. So, the model will be trained with three epochs of training data instead of one epoch.\r\n\r\n\r\n\r\nwhich means that once you reach max_steps=6000, training is not resumed anymore and train_and_evaluate returns (None,None) .\r\nTry to remove max_steps and see if this help. Your input_fn has to raise OutOfRangeError once iteration through data set is done. It looks to me that train_and_evaluate does not support control over number of training steps. ", "> \r\n> \r\n> @ssubraveti @Zhiqiang-Ye How did you solve this problem? I also met this problem.\r\n\r\nHi, I had this problem when my program could not read data from HDFS. As long as it found any data to read, this problem is gone.\r\nMy colleague had this problem using old file queue for input. The mysterious solution is to set training max steps to forever....", "@agniszczotka @Zhiqiang-Ye @ssubraveti \r\n\r\nHi guys, i know super late to resume this convo, but: Same issue here (training loss is None and \"perhaps input is empty or misspecified)!\r\n\r\nTF version: 1.15.1\r\n\r\n```\r\ntrain_spec = tf.estimator.TrainSpec(\r\n    input_fn=lambda: read_train(data_folder, params),\r\n    max_steps=None)\r\n```\r\nmax_steps was 1400000 before and i tried now to set it to None, but it didnt work for me. My input pipeline looks like this:\r\n```\r\nclvf = CLVFeatures(ignore_crosses=True)\r\n\r\ndef parse_csv(csv_row):\r\n  columns = tf.decode_csv(csv_row, record_defaults = clvf.get_all_defaults())\r\n  features = dict(zip(clvf.get_all_names(), columns))\r\n  \r\n  for column_name in clvf.get_unused():\r\n    features.pop(column_name)\r\n\r\n  target = features.pop(clvf.get_target_name())\r\n\r\n  return features, target\r\n\r\n#@tf.function\r\ndef dataset_input_fn(data_folder, prefix=None, mode=None, params=None, count=None):\r\n  shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\r\n\r\n  filenames = tf.matching_files('{}{}*.csv'.format(data_folder, prefix))\r\n  dataset = tf.data.TextLineDataset(filenames)#skip(1)\r\n  dataset = dataset.map(parse_csv)\r\n  if shuffle:\r\n    dataset = dataset.shuffle(buffer_size=params.buffer_size)\r\n  dataset = dataset.repeat(count=count)\r\n  dataset = dataset.batch(params.batch_size)\r\n\r\n  iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)#tf.compat.v1.data.make_one_shot_iterator(dataset)/#tf.compat.v1.data.make_initializable_iterator(dataset)\r\n  \r\n  features, target = iterator.get_next()\r\n\r\n  return features, target\r\n\r\ndef read_train(data_folder, params):\r\n  return dataset_input_fn(\r\n      data_folder=data_folder,\r\n      prefix='train',\r\n      params=params,\r\n      mode=tf.estimator.ModeKeys.TRAIN)\r\n\r\n\r\ndef read_eval(data_folder, params):\r\n  return dataset_input_fn(data_folder=data_folder,\r\n                          prefix='eval',\r\n                          params=params)\r\n\r\n\r\ndef read_test(data_folder, params):\r\n  return dataset_input_fn(data_folder=data_folder,\r\n                          prefix='test',\r\n                          params=params,\r\n                          count=1)\r\n```\r\n\r\nI would appreciate some help, so much. This is for school haha thank you!"]}, {"number": 24027, "title": "Duplicated variable names with inner tf.keras.layers.Dense layers in eager mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL7 3.10.0-862.11.6.el7.x86_64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 1.11.0\r\n- Python version: Python 3.6.6 :: Anaconda, Inc.\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n\r\nWhen creating a custom `tf.keras.Model` with custom `build()` function, the variable name scopes seem not to propagate correctly in `tf.keras.Dense` layers.\r\n\r\n**Describe the expected behavior**\r\n\r\nVariables of inner `tf.keras.Dense` layers should have different names, such as:\r\n`dense/kernel:0`\r\n`dense/bias:0`\r\n`dense_1/kernel:0`\r\n`dense_1/bias:0`\r\n\r\nRight now they have the same name:\r\n`kernel:0`\r\n`bias:0`\r\n`kernel:0`\r\n`bias:0`\r\n\r\nThis is problematic for saving / loading parameters.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport os\r\n\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\n\r\ntf.enable_eager_execution()\r\ntf.executing_eagerly()\r\n\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__(name='mymodel')\r\n        \r\n        self.f = tf.keras.layers.Dense(units=10)\r\n        self.g = tf.keras.layers.Dense(units=10)\r\n\r\n#         Workaround\r\n#         self.f = tf.keras.Sequential([tf.keras.layers.Dense(units=10)])\r\n#         self.g = tf.keras.Sequential([tf.keras.layers.Dense(units=10)])\r\n\r\n    def build(self, input_shapes):\r\n        self.f.build(input_shapes[0])\r\n        self.g.build(input_shapes[1])\r\n        self.built = True\r\n    \r\n    def call(self, x, y):\r\n        return self.f(x) + self.g(y)\r\n\r\n\r\nmodel = MyModel()\r\nmodel.build([(None, 5), (None, 3)])\r\n\r\nfor v in model.variables:\r\n    print(v.name)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["How are you saving and loading? It won't matter for `Model.save_weights` or `tf.train.Checkpoint`, which are the APIs we're moving toward.", "I'm using tf.contrib.eager.Saver", "Thanks for the reminder. I'll add a warning and a note in the documentation. Can you try `Checkpoint`? This MNIST model is a reasonable usage example: https://github.com/tensorflow/models/blob/5324fc6628522eabbff74f2d12f0cc69d3432a2c/official/mnist/mnist_eager.py#L149", "Saver has been deprecated. Check out the [2.0 checkpointing guide](https://www.tensorflow.org/alpha/guide/checkpoints) for an API which does not depend on variable names."]}, {"number": 24026, "title": "Abs operator not implemented?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora release 28\r\n- TensorFlow installed from (source or binary): binary (conda)\r\n- TensorFlow version (or github SHA if from source): '1.13.0-dev20181127'\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, DEPTHWISE_CONV_2D, L2_NORMALIZATION, MUL, RELU, SQUEEZE, SUB. Here is a list of operators for which you will need custom implementations: Abs.\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@utkarsh-VRL  Please refer [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tf_ops_compatibility.md) which gives information about supported ops by TF Lite.\r\nAlso #21526 which tracks TFLite ops to be added in future. Thanks !"]}, {"number": 24025, "title": "Does XLA GPU ptxas recompile on each session.run()?", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.12\r\n- Doc Link: https://www.tensorflow.org/xla/tfcompile\r\n\r\n**Describe the documentation issue**\r\n\r\nI'm seeing undocumented behavior where NVPTXCompiler::CompilePtxOrGetCachedResult(const string& ptx) is being called with new ptx code on each invocation of session.run().\r\n\r\n1. Is it intended behavior that XLA JIT is invoked with each session.run() call? I'm currently calling session.run() multiple times by invoking q.dequeue() repeatedly.\r\n2. Currently it takes longer to compile the ptxas code than to actually execute the rest of the code.\r\n3. Should we restructure our code so we minimize the number of session.run() calls to make JIT behave nicely?\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["To add more information, it looks like putting the dequeue op inside a tf.while_loop() still triggers XLA recompiles. ", "XLA should invoke ptxas only once per unique compilation unit (\"HloModule\") that it sees.  In other words, there is caching.\r\n\r\nWithout seeing your code, what I suspect is that the input shapes are dynamic, so every time we invoke session.run, it's a different HloModule.  See \"caveats\" section in https://medium.com/tensorflow/pushing-the-limits-of-gpu-performance-with-xla-53559db8e473 for details about this.\r\n\r\nYou should be able to verify that this is what's happening by setting some flags in the environment variable for debugging.  Full set of flags is in xla.proto.  You could try `XLA_FLAGS= --xla_generate_hlo_text_to=/tmp/whatever` -- you will probably see that each of the modules XLA is compiling is slightly different.", "Thanks that's what I suspected as well. I have a custom op that does something like a dynamic partition to create 4 blocks of (?A, 256), (?B, 256), (?C, 256), (?D, 256) where each block 0 <= ?X < 1024. I had hoped that the *worst* case scenario is that it would generate 1024*4=4096 custom ptx blocks for all four possible values of ?A, ?B, ?C, ?D, but the ?X shapes cascades to all downstream operations resulting in lots of Hlomodules. \r\n\r\nPS. I've verified that feeding in the exact same identical batch (resulting in the same ?A, ?B, ?C, ?D) no longer triggers the recompile. \r\n\r\nIs there a way to avoid optimizing over the 0th dimension? I presume we see the same issue in general when we try to train with widely varying batch sizes.", "> Is there a way to avoid optimizing over the 0th dimension? I presume we see the same issue in general when we try to train with widely varying batch sizes.\r\n\r\nThe requirement that all of the dimensions be known upfront is pretty fundamental to XLA.  We actually *are* looking at relaxing this, but it's a really big change and we're in the very beginnings of it right now.\r\n\r\nFor your purposes what I'd usually recommend is bucketing the 0'th dimension, if that's possible.  That is, instead of generating all possible shapes, pad out to e.g. powers of two.  Dunno if that's possible in your situation.", "Thanks - I'll see what I can do to work around the issue. Looking forward to relaxing the constraint. But ofr now, closing this issue."]}, {"number": 24024, "title": "AttributeError: 'IndexedSlices' object has no attribute '_copy'", "body": "Hello,\r\nThis is puzzling, as this only happens with `tensorflow-gpu` version 1.12.0 -- `tensorflow` version 1.12.0 works flawlessly (both on GNU/Linux and OSX).\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./bin/moe-cli.py\", line 476, in <module>\r\n    main(sys.argv[1:])\r\n  File \"./bin/moe-cli.py\", line 372, in main\r\n    gradients = tape.gradient(loss, trainable_variables)\r\n  File \"/home/testing/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 901, in gradient\r\n    output_gradients=output_gradients)\r\n  File \"/home/testing/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\", line 64, in imperative_grad\r\n    output_gradients)\r\n  File \"/home/testing/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 858, in grad_fun\r\n    return [dresult._copy(device_name=self_device)]\r\nAttributeError: 'IndexedSlices' object has no attribute '_copy'\r\n```\r\n", "comments": ["Since the problematic line seems to be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L852 (`dresult` might be `IndexedSlices`, which does not implement `_copy`), I have fixed the TF source code as follows, and the issue disappeared:\r\n\r\n```\r\n  def _copy(self, ctx=None, device_name=None):\r\n    \"\"\"Copies tensor to dest device.\"\"\"\r\n    new_tensor = self._copy_nograd(ctx, device_name)\r\n    # Record the copy on tape and define backprop copy as well.\r\n    if context.executing_eagerly():\r\n      self_device = self.device\r\n      def grad_fun(dresult):\r\n        #\u00a0return [dresult._copy(device_name=self_device)]\r\n        return [dresult] if isinstance(dresult, IndexedSlices) else [dresult._copy(device_name=self_device)]\r\n      tape.record_operation(\"_copy\", [new_tensor], [self], grad_fun)\r\n    return new_tensor\r\n```", "Closing this issue since its resolved. Thanks!", "@ymodak it is not really resolved - I just hacked that \"fix\" into my tensorflow installation, but the problem persists", "Can you please fill the information asked by the template and we can reopen your issue? Thanks!\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "Having the same issue:\r\n* CUDA 10.0 \r\n* Ubuntu 16.04\r\n* GPU Nvidia TITAN\r\n* Tensorflow version: 2.0.0-dev20190225\r\n\r\nThe fix mentioned above seems to work.\r\n", "I got the same issue when I enabled the eager execution and called `tf.gather()` on a custom variable. I found it was okay when I used the keras Embedding layer that basically calls the same `gather()` function on a low level. I did the debug in TensorFlow code and then found the reason. There is a piece of code in the keras `embeddings.py`:\r\n\r\n```\r\ndef build(self, input_shape):\r\n    # Note: most sparse optimizers do not have GPU kernels defined. When\r\n    # building graphs, the placement algorithm is able to place variables on CPU\r\n    # since it knows all kernels using the variable only exist on CPU.\r\n    # When eager execution is enabled, the placement decision has to be made\r\n    # right now. Checking for the presence of GPUs to avoid complicating the\r\n    # TPU codepaths which can handle sparse optimizers.\r\n    if context.executing_eagerly() and context.context().num_gpus():\r\n      with ops.device('cpu:0'):\r\n        self.embeddings = self.add_weight(\r\n            shape=(self.input_dim, self.output_dim),\r\n            initializer=self.embeddings_initializer,\r\n            name='embeddings',\r\n            regularizer=self.embeddings_regularizer,\r\n            constraint=self.embeddings_constraint)\r\n    else:\r\n      self.embeddings = self.add_weight(\r\n          shape=(self.input_dim, self.output_dim),\r\n          initializer=self.embeddings_initializer,\r\n          name='embeddings',\r\n          regularizer=self.embeddings_regularizer,\r\n          constraint=self.embeddings_constraint)\r\n    self.built = True\r\n```\r\n\r\nAnd my custom variable was defined on GPU by default.\r\n\r\nTo fix it, try to define your variable on CPU if you are going to call `tf.gather()` or other sparse ops on it.", "@netpaladinx @meyerjo you may be interested in this pull request of mine: https://github.com/tensorflow/tensorflow/pull/27986"]}, {"number": 24023, "title": "ValueError: tf.enable_eager_execution must be called at program startup.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNA\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n1.12.0\r\n- Python version:\r\n3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n9/7\r\n- GPU model and memory:\r\nNVIDIA GTX1080, 8GB\r\n\r\n**Describe the current behavior**\r\nEager execution isn't working on a jupyter notebook with GPU. Have tried restarting the kernel multiple times. I get a ValueError: tf.enable_eager_execution must be called at program startup.\r\n\r\n**Describe the expected behavior**\r\nEager Execution should work. Why isn't it working?\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n`import tensorflow as tf\r\n`tf.enable_eager_execution()\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-e43ec884c055> in <module>()\r\n      5 import numpy as np\r\n      6 import tensorflow as tf\r\n----> 7 tf.enable_eager_execution()\r\n      8 from keras import layers\r\n      9 from keras import models\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\DLCdependencies\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in enable_eager_execution(config, device_policy, execution_mode)\r\n   5421         device_policy=device_policy,\r\n   5422         execution_mode=execution_mode,\r\n-> 5423         server_def=None)\r\n   5424 \r\n   5425 \r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\DLCdependencies\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in enable_eager_execution_internal(config, device_policy, execution_mode, server_def)\r\n   5465     if graph_mode_has_been_used:\r\n   5466       raise ValueError(\r\n-> 5467           \"tf.enable_eager_execution must be called at program startup.\")\r\n   5468   context.default_execution_mode = context.EAGER_MODE\r\n   5469   # pylint: disable=protected-access\r\n\r\nValueError: tf.enable_eager_execution must be called at program startup.", "comments": ["@Umar-Ayub  Can you please try in the [colab](https://colab.sandbox.google.com/drive/1rCTY1pv9jRG1_SxsOCNlZZRiEYZP5NF5) and let us know if you see the same error ?", "Eager Execution is working fine. I was importing a python module which had keras models inside it and that's why the code was failing since placeholders were being defined in that module. ", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 24022, "title": "Offset when rotating image and points by the same random angle", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04, Os X 10.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): conda and pip\r\n- TensorFlow version (use command below): 1.8\r\n- Python version: 2.7, 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI have an image and 3 points. I want to rotate the image and the points together. To this end, I rotate the image by some angle a and the points by the same angle. When a is fixed to a python scalar (say pi/3), the rotation works fine (cf. image below, the blue dots are on the dark squares).\r\n\r\n![rotation](https://user-images.githubusercontent.com/16196950/49168292-d5304600-f33f-11e8-87c5-c1b73b84a053.jpg)\r\n\r\n\r\nWhen the angle is randomly chosen with `angle = tf.random_uniform([])`, there is an offset between the rotated image and the rotated points.\r\n\r\n![rotation](https://user-images.githubusercontent.com/16196950/49168588-7c14e200-f340-11e8-8a21-9060bea077bf.jpg)\r\n\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\n# create toy image\r\nsquare = np.zeros((1, 800, 800, 3))\r\nsquare[:, 100:400, 100:400] = 1\r\nsquare[:, 140:180, 140:180] = 0\r\nsquare[:, 240:280, 240:280] = 0\r\nsquare[:, 280:320, 280:320] = 0\r\nkp = np.array([[160, 160], [260, 260], [300, 300]])\r\nkp = np.expand_dims(kp, axis=0)\r\n\r\ndef _rotate(image, keypoints, angle, keypoints_num):\r\n    image = tf.contrib.image.rotate(image, angle)\r\n    cos, sin = tf.cos(angle), tf.sin(angle)\r\n    x0, y0  = .5, .5\r\n    rot_mat = tf.Variable([[cos, -sin], [sin, cos]], trainable=False)\r\n    keypoints -= (x0, y0)\r\n    keypoints = tf.reshape(keypoints, shape=[-1, 2])\r\n    keypoints = tf.matmul(keypoints, rot_mat)\r\n    keypoints = tf.reshape(keypoints, shape=[-1, keypoints_num, 2])\r\n    keypoints += (x0, y0)\r\n    return image, keypoints\r\n\r\n\r\nimage = tf.placeholder(tf.float32, [None, 800, 800, 3])\r\nkeypoints = tf.placeholder(tf.float32, [None, 3, 2])\r\n\r\nangle = np.pi / 3 # fix angle, works fine\r\n#angle = tf.random_uniform([]) # random angle, does not work\r\nimage_r, keypoints_r = _rotate(image, keypoints / 800, angle, 3)\r\nkeypoints_r *= 800\r\n\r\nsess = tf.Session()\r\nsess.run(tf.initialize_all_variables())\r\n\r\nimr, kr = sess.run([image_r, keypoints_r], feed_dict={image: square, keypoints:kp})\r\n\r\n# displaying output\r\nplt.imshow(imr[0])\r\nplt.scatter(*zip(*kr[0]))\r\nplt.savefig('rotation.jpg')\r\n```", "comments": []}, {"number": 24021, "title": "INTEL MKL: Fixing a Bug in Quantized Convolution", "body": "This PR fixes a bug in MKL quantized convolution. It also includes clang-format changes.", "comments": []}, {"number": 24020, "title": "[ppc64le] Getting error installing a custom tensorflow wheel on Power8 with NVIDIA P100 with anaconda3", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLSB Version:\t:core-4.1-noarch:core-4.1-ppc64le\r\nDistributor ID:\tRedHatEnterpriseServer\r\nDescription:\tRed Hat Enterprise Linux Server release 7.4 (Maipo)\r\nRelease:\t7.4\r\nCodename:\tMaipo\r\n\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.8\r\n- Python version: 3.6.5\r\n- Installed using virtualenv? pip? conda?: conda (anaconda3 5.2)\r\n- Bazel version (if compiling from source): 0.10.0\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: 8.0/ 7.0.5\r\n- GPU model and memory: NVIDIA Tesla P100/ 16 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nGetting the following error while building a tensorflow wheel on power8 system.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nCUDA_VER=8.0\r\nCUDNN_VER=7.0.5\r\nNCCL_VER=2.3.7-1\r\nBAZEL_VER=0.10.0\r\nTF_VER=1.8.0\r\nCONDA_VER=5.2.0\r\nPY_VER=3.6\r\n\r\n#module load cuda/8.0.61-1\r\nCUDA_DIR=$(pwd)/8.0.61-1\r\n\r\n#install pre-built anaconda\r\nwget https://repo.anaconda.com/archive/Anaconda3-$CONDA_VER-Linux-ppc64le.sh -O anaconda3.sh\r\nbash ./anaconda3.sh -b -p anaconda3\r\nexport PATH=$(pwd)/anaconda3/bin:$PATH\r\npip install --upgrade pip\r\npip install keras_applications==1.0.6 --no-deps\r\npip install keras_preprocessing==1.0.4 --no-deps\r\n#install bazel \r\nwget https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VER/bazel-$BAZEL_VER-dist.zip\r\nunzip bazel-$BAZEL_VER-dist.zip -d bazel\r\ncd bazel\r\n./compile.sh\r\nexport PATH=$(pwd)/output:$PATH\r\ncd ../\r\n\r\n#workaround for nccl2 \r\ngit clone https://github.com/NVIDIA/nccl\r\ncd nccl\r\ngit checkout v$NCCL_VER\r\nmake -j160 src.build CUDA_HOME=$CUDA_DIR\r\nmake pkg.txz.build CUDA_HOME=$CUDA_DIR\r\ntar -xf  build/pkg/txz/* -C ..\r\ncd ..\r\nln -s nccl_$NCCL_VER* nccl2\r\ncd nccl_$NCCL_VER*\r\nln -s LICENSE.txt NCCL-SLA.txt\r\ncd ..\r\n\r\nCUDNN_DIR=$CUDA_DIR\r\n\r\n#setup env var\r\nexport PYTHON_BIN_PATH=$(pwd)/anaconda3/bin/python3\r\nexport PYTHON_LIB_PATH=$(pwd)/anaconda3/lib/python$PY_VER/site-packages\r\nexport TF_NEED_MKL=0\r\nexport CC_OPT_FLAGS=\"-march=native\"\r\nexport TF_NEED_JEMALLOC=0\r\nexport TF_NEED_GCP=0\r\nexport TF_NEED_HDFS=0\r\nexport TF_ENABLE_XLA=0\r\nexport TF_NEED_OPENCL=0\r\nexport TF_NEED_CUDA=1\r\nexport TF_CUDA_CLANG=0\r\nexport TF_CUDA_VERSION=$CUDA_VER\r\nexport CUDA_TOOLKIT_PATH=$CUDA_DIR\r\nexport TF_CUDNN_VERSION=$CUDNN_VER\r\nexport CUDNN_INSTALL_PATH=$CUDNN_DIR\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=\"6.0\"\r\nexport TF_NEED_VERBS=0\r\nexport TF_NEED_AWS=0\r\nexport TF_NEED_NGRAPH=0\r\nexport TF_NEED_S3=0\r\nexport TF_NEED_GDR=0\r\nexport TF_NEED_OPENCL_SYCL=0\r\nexport GCC_HOST_COMPILER_PATH=/usr/bin/gcc\r\nexport TF_NEED_MPI=0\r\nexport TF_NEED_KAFKA=0\r\nexport TF_NEED_ROCM=0\r\nexport TF_NEED_IGNITE=0\r\nexport TF_NEED_TENSORRT=0\r\nexport TF_SET_ANDROID_WORKSPACE=0\r\nexport TF_NCCL_VERSION=$(echo $NCCL_VER | cut -d. -f1,2)\r\nexport NCCL_INSTALL_PATH=\"$(pwd)/nccl2\"\r\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:$CUDA_DIR/lib64/stubs\r\n\r\n#build tensorflow\r\ngit clone --recursive https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\ngit checkout v$TF_VER\r\n./configure\r\nbazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package ../tensorflow_pkg\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nERROR: /autofs/nccs-svm1_home1/shubhankar/summitdev/tensorflow/tensorflow/core/kernels/BUILD:1201:1: output 'tensorflow/core/kernels/_objs/gather_functor_gpu/tensorflow/core/kernels/gather_functor_gpu.cu.pic.o' was not created\r\nINFO: From Compiling tensorflow/core/kernels/quantization_utils.cc:\r\nIn file included from tensorflow/core/kernels/quantization_utils.cc:16:0:\r\n./tensorflow/core/kernels/quantization_utils.h:35:0: warning: \"GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\" redefined [enabled by default]\r\n #define GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\n ^\r\n<command-line>:0:0: note: this is the location of the previous definition\r\nIn file included from tensorflow/core/kernels/quantization_utils.cc:16:0:\r\n./tensorflow/core/kernels/quantization_utils.h: In function 'void tensorflow::RequantizeManyInNewRangeReference(const qint32*, tensorflow::int64, float, float, float, float, tensorflow::quint8*)':\r\n./tensorflow/core/kernels/quantization_utils.h:269:34: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (size_t index = 0; index < count; ++index) {\r\n                                  ^\r\nERROR: /autofs/nccs-svm1_home1/shubhankar/summitdev/tensorflow/tensorflow/core/kernels/BUILD:1201:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 221.932s, Critical Path: 161.23s\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["@wdirons   PTAL as this is related to power8", "@ghltshubh, can you attach the full build log as a file. I don't think I'm seeing the real error in the output above.  I do know for TensorFlow 1.8 this fix had to be cherry-picked in. https://github.com/tensorflow/tensorflow/commit/8f8a3c5151a674b3496691af49c3aa063841f292", "I don't think it is related to the failure, but should use:\r\n\r\n`export CC_OPT_FLAGS='-mcpu=power8 -mtune=power8'`\r\n\r\nfor the CC_OPT_FLAGS", "Another note on the keras add-ons. Since you are using Anaconda, instead of pip you can just:\r\n`conda install  keras-applications`\r\n`conda install keras-preprocessing`", "@wdirons using cherry-pick 8f8a3c5 changes the error to \r\n```\r\nERROR: /autofs/nccs-svm1_home1/shubhankar/summitdev/tensorflow/third_party/py/numpy/BUILD:11:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/summitdev/tensorflow/third_party/py/python_configure.bzl\", line 291\r\n\t\t_create_local_python_repository(repository_ctx)\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/summitdev/tensorflow/third_party/py/python_configure.bzl\", line 255, in _create_local_python_repository\r\n\t\t_get_numpy_include(repository_ctx, python_bin)\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/summitdev/tensorflow/third_party/py/python_configure.bzl\", line 239, in _get_numpy_include\r\n\t\t_execute(repository_ctx, [python_bin, \"-c\",...\"], <2 more arguments>)\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/summitdev/tensorflow/third_party/py/python_configure.bzl\", line 54, in _execute\r\n\t\t_fail(\"\\n\".join([error_msg.strip() if ... \"\"]))\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/summitdev/tensorflow/third_party/py/python_configure.bzl\", line 27, in _fail\r\n\t\tfail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: Problem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'numpy'\r\nIs numpy installed?\r\n and referenced by '//third_party/py/numpy:headers'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed\r\nINFO: Elapsed time: 84.756s\r\nFAILED: Build did NOT complete successfully (116 packages loaded)\r\n```\r\n\r\nEverything above was normal build log though. ", "@jayfurmanek conda install keras-applications doesn't work. \r\n```\r\nPackagesNotFoundError: The following packages are not available from current channels:\r\n\r\n  - keras_applications\r\n\r\nCurrent channels:\r\n\r\n  - https://repo.anaconda.com/pkgs/main/linux-ppc64le\r\n  - https://repo.anaconda.com/pkgs/main/noarch\r\n  - https://repo.anaconda.com/pkgs/free/linux-ppc64le\r\n  - https://repo.anaconda.com/pkgs/free/noarch\r\n  - https://repo.anaconda.com/pkgs/r/linux-ppc64le\r\n  - https://repo.anaconda.com/pkgs/r/noarch\r\n  - https://repo.anaconda.com/pkgs/pro/linux-ppc64le\r\n  - https://repo.anaconda.com/pkgs/pro/noarch\r\n  - https://conda.anaconda.org/conda-forge/linux-ppc64le\r\n  - https://conda.anaconda.org/conda-forge/noarch\r\n\r\nTo search for alternate channels that may provide the conda package you're\r\nlooking for, navigate to\r\n\r\n    https://anaconda.org\r\n\r\nand use the search bar at the top of the page.\r\n\r\n```\r\n\r\n@wdirons here is the full build log you asked for(I used bazel 0.11.1 but that doesn't matter the error is still sort of the same):\r\nhttps://gist.githubusercontent.com/ghltshubh/68784ff142390c4c52bc1061f62e1533/raw/a57efc6adb6c88fde4cbfeaad8d9543be75421fe/gistfile1.txt", "look for `keras-applications` and `keras-preprocessing`  (dash instead of underscore)", "Installing those from Anaconda will pull in `numpy` as well which should get you past that other issue.\r\n\r\nI just did a couple builds on my P8/P100 box, similar to what you did above. I did see some cuda kernel errors when building against cuda8.0. I tried again against cuda9.2 and it built fine (assuming you have @wdirons patch, of course)\r\n\r\nI'd recommend upgrading your cuda if you can. We really didn't get ppc64le builds fixed upstream until the cuda9.x timeframe.", "I don't have cuda 9.2 on power8 but cuda 9.0 instead. I will try with 9.0 and hope it works.", "@wdirons @jayfurmanek why am I getting\r\n```\r\n-bash-4.2$ git checkout v1.8.0    \r\nChecking out files: 100% (11488/11488), done.\r\nNote: checking out 'v1.8.0'.\r\n\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\n\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n\r\n  git checkout -b new_branch_name\r\n\r\nHEAD is now at 93bc2e2... Merge pull request #18928 from tensorflow/release-patch-4-1\r\n-bash-4.2$ git cherry-pick 8f8a3c5\r\nfatal: bad revision '8f8a3c5'\r\n\r\n```", "I can't explain why git cherry-pick is failing. From this page https://help.github.com/articles/commit-exists-on-github-but-not-in-my-local-clone/, the only possible reason I think would be: \r\nSomeone force pushed over the commit.\r\n\r\ntry this instead:\r\n```\r\nwget https://github.com/tensorflow/tensorflow/commit/8f8a3c5.patch\r\npatch -p1 third_party/png.BUILD 8f8a3c5.patch\r\n```", "@wdirons Nothing works as of now. I only have the following CUDA versions available not sure if they will help:\r\n\r\n- 8.0.35  \r\n- 8.0.44  \r\n- 8.0.54  \r\n- 8.0.61-1  \r\n- 9.0.69\r\n\r\nI have tried 8.0.61-1 with the patch doesn't help. I have tried 9.0.69 that also doesn't help just has a different error output\r\n\r\n```\r\nERROR: /autofs/nccs-svm1_home1/shubhankar/summitdev/tensorflow/third_party/py/numpy/BUILD:11:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/summitdev/tensorflow/third_party/py/python_configure.bzl\", line 291\r\n\t\t_create_local_python_repository(repository_ctx)\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/summitdev/tensorflow/third_party/py/python_configure.bzl\", line 255, in _create_local_python_repository\r\n\t\t_get_numpy_include(repository_ctx, python_bin)\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/summitdev/tensorflow/third_party/py/python_configure.bzl\", line 239, in _get_numpy_include\r\n\t\t_execute(repository_ctx, [python_bin, \"-c\",...\"], <2 more arguments>)\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/summitdev/tensorflow/third_party/py/python_configure.bzl\", line 54, in _execute\r\n\t\t_fail(\"\\n\".join([error_msg.strip() if ... \"\"]))\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/summitdev/tensorflow/third_party/py/python_configure.bzl\", line 27, in _fail\r\n\t\tfail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: Problem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'numpy'\r\nIs numpy installed?\r\n and referenced by '//third_party/py/numpy:headers'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed\r\nINFO: Elapsed time: 86.170s\r\nFAILED: Build did NOT complete successfully (108 packages loaded)\r\n```\r\n", "@wdirons I was able to install tf1.8 git checkout 397f04a with cuda 8.0.61-1 and it works fine on single node but I need to install horovod as well in order to run it on cluster but I am having issues building horovod from source because tf is missing some sub-directories such as /compiler inside \r\n/anaconda3/lib/python3.6/site- packages/tensorflow/include/tensorflow/ which I was able to copy from tf git repo and build horovod from source but I am not sure if that is it or there are more missing sub-directories. ", "@ghltshubh To help others, can you post what you did to get past your error.\r\n\r\nI did go back and build tf 1.8 against cuda 9.0.176 and had no issues. Was you problem just an environment issue? (didn't have numpy installed?)", "I have access to CUDA 9.0.69 only. There must have been some changes in the later version 9.0.176 which you used. I just followed the standard procedure that I mentioned in the issue above just use CUDA 8.0.61-1 and tf 1.8 397f04a and it will build fine."]}, {"number": 24019, "title": "Fix broken path in lite docs", "body": "This fix fixes broken link in lite docs\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang Can you please sign the CLA to move ahead with this PR? Thanks!", "Thanks @ymodak, The PR has been updated."]}, {"number": 24018, "title": "Improve shape function of tf.sparse_concat when inputs are fully defined", "body": "\r\nThis fix tries to address the issue raised in #21964 where the shape function of tf.sparse_concat always returns `TensorShape([Dimension(None), Dimension(None)])` even if the inputs are fully defined.\r\n\r\nThis fix addresses the issue by finding the shape if the inputs are fully defined.\r\n\r\nThis fix fixes #21964.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": ["Nagging Reviewer @ebrevdo: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied."]}, {"number": 24017, "title": "[Intel MKL] Updating README.md with links to Intel(R) Optimized TensorFlow 1.12", "body": "", "comments": ["Nagging Reviewer @drpngx: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 24016, "title": "Changes to Makefile for tf-lite", "body": "creating a pull-request by results from discussion at:\r\nhttps://github.com/tensorflow/tensorflow/issues/23926", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "I've signed the CLA\r\n", "I've double-checked e-mails, all is fine. CLA is signed.", "> I've double-checked e-mails, all is fine. CLA is signed.\r\n\r\nSorry to say but we cannot proceed until CLA reflects YES. Could you please open a new PR(which may resolve the CLA issue) and try. Closing this."]}, {"number": 24015, "title": "Fixed bug in Makefile for tf-lite", "body": "Creating a pull-request by results from discussion at:\r\nhttps://github.com/tensorflow/tensorflow/issues/23926#issuecomment-442211084", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I've signed the CLA", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "I've double-checked and signed all needed CLA docs.", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 24014, "title": "Fix typo: s/dimemension/dimension/", "body": "", "comments": []}, {"number": 24013, "title": "MirroredStrategy not work with estimator", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Docker\r\n- TensorFlow version (use command below): 1.12.0-rc0\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: V9.0.176\r\n- GPU model and memory: TITAN Xp\r\n\r\n**Describe the current behavior**\r\nMy code looks like\r\n```python\r\ndist_strategy = tf.contrib.distribute.MirroredStrategy()\r\n\r\nrun_config = tf.estimator.RunConfig(\r\n    train_distribute=dist_strategy,\r\n    log_step_count_steps=10)\r\n\r\nestimator = tf.estimator.Estimator(\r\n    model_fn,\r\n    model_dir=params.ckpt_dir,\r\n    params=params,\r\n    config=run_config)\r\n\r\nestimator.train(train_input_fn, max_steps=steps)\r\n```\r\n\r\n**Describe the expected behavior**\r\nSuccessfully train.\r\n\r\n**Other info / logs**\r\n```log\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 109, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"main.py\", line 75, in main\r\n    estimator.train(train_input_fn, max_steps=steps)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1181, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1287, in _train_model_distributed\r\n    grouped_estimator_spec = self._train_distribution.call_for_each_tower(\r\nAttributeError: 'MirroredStrategy' object has no attribute 'call_for_each_tower'\r\n```\r\n", "comments": ["I got the same issue when run NCF in tensorflow-models, and I has already know that is caused by mismatching API between tensorflow and tensorflow-estimator.\r\n\r\nTF deleted this API call_for_each_tower and used call_for_each_replica instead, but the release package of tensorflow-estimator is too old(release on Sep, 12, 2018) that still used the old API.\r\nTensorflow-estimator has already changed this API in their master, but they didn't release any new package and I didn't find somewhere to submit the issue, so I just answered this issue and hope someone can help to release a new estimator.", "@Zantares @JayYip can we use git to clone the tensorflow-estimator repository (https://github.com/tensorflow/estimator#developing) and build the tensorflow-estimator master branch to walk around this issue\uff1f", "> @Zantares @JayYip can we use git to clone the tensorflow-estimator repository (https://github.com/tensorflow/estimator#developing) and build the tensorflow-estimator master branch to walk around this issue\uff1f\r\n\r\nYes, it' OK if build a locally estimator from the newest master branch. The problem is the release package is too old so that if anyone uses pip install will occur this error. ", "Both tensorflow and tensorflow-estimator has 1.13.0rc0 now (both are pre-release). \r\nAnother option is to use the nightly version for both. ", "Close since it's fixed in latest build. "]}, {"number": 24012, "title": "java.lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present in the APK.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:1.6\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): - \r\n- CUDA/cuDNN version: - \r\n- GPU model and memory: - \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n \r\nbazel build -c opt //tensorflow/examples/android:libtensorflow_demo.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hello, can you please try bazel 0.9.0  ?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24011, "title": "[INTEL MKL]Optimize CropAndResizeGradImage Op.", "body": "Optimize CropAndResizeGradImage Op, please review. After optimized, the performance could raise about  20.6% compared to original version. We use parallization(eigen thread pool) to optimize.", "comments": ["@qlzh727 I use newer clang-format 3.9 to check the errors not finding any ones, and I couldn't click into detail link for clang-format errors. Please help me get to know more about it.", "let me rerun the build, the previous one seems to be failed due to build system issue.", "Any updates? @qlzh727 @rmlarsen ", "Any updates? @qlzh727 @rmlarsen ", "Any updates? @qlzh727 @rmlarsen", "Any updates? @qlzh727 @rmlarsen @agramesh1 ", "Any updates? @qlzh727 @rmlarsen", "Any updates? @qlzh727 @rmlarsen", "@rmlarsen is on his holiday until next week. Adding @shpeisman for review.", "@qlzh727 Thank you!", "Any updates? @qlzh727 @tatianashp ", "Ping @tatianashp for review. Thanks.", "@qlzh727 Thank you~", "Any updates? @qlzh727 @tatianashp", "Any updates? @qlzh727 @tatianashp", "Sorry for the very late reply, I have pinged @tatianashp and she will probably reply very soon.", "Thank you @qlzh727 ", "Nagging Reviewer @rmlarsen, @tatianashp: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 15 days with no activity and the `awaiting review` label has been applied.", "Any updates?", "Sorry for the very late reply, I will ping reviewers again today for this PR. In the meantime, can you resolve the merge conflicts?", "@penpornk Your idea is good. For the 'two lambda functions', do you mean by the implementation in my 'change function splitting method' commit? Please review my implementation ~", "Quoting myself to add:\r\n> For example, defining `resize_fn`\r\n> \r\n> ```c++\r\n> auto resize_fn = (method_name == \"bilinear\")\r\n>     ? <lambda function for \"bilinear\">\r\n>     : <lambda function for \"nearest\">;\r\n> ```\r\nSince each lambda takes multiple lines, you might want to define `resize_fn` this way instead.\r\n```c++\r\nauto resize_fn = <lambda_function_for_bilinear>;\r\nif (method_name == \"nearest\") {\r\n  resize_fn = <lambda_function_for_nearest>;\r\n}\r\n```\r\n", "@penpornk Thank you! Changes done. ", "@penpornk No problem! Changes done.", "Hi @penpornk @ezhulenev \r\n\r\n@pandaoxin will leave his job this week and start a new career. From now on, I will take over this PR until it's merged.\r\n\r\nIs there anything I should do to make the ownership transfer of this PR more smoothly? For example,  re-check CLA? If so, please let me know.\r\n\r\nThank you!", "@pandaoxin Sorry for my late response. I wish you all the best on your new journey!\r\n\r\n@wenxizhu Since you have committed to the repo before, I believe your CLA is fine. But googlebot will probably complain that there is more than one users contributing to the PR. And both @pandaoxin and you will have to post that you are okay with your commits being used in this PR.", "@penpornk I'm sorry @pandaoxin already left his job yesterday, so he won't be able to login his account (it's a company account) to post that comment. Is there any way we can workaround this?", "@wenxizhu Sorry for the inconvenience! Let's wait until googlebot complains and I'll ask someone. At worst we'll just open a new PR.", "@penpornk Changes applied. Any updates?", "@peppornk For the \"additions count\" problem, I already gave my best guess. But since I'm not the author of that code, I also don't know where my guess is correct. Your opinions?", "@penpornk Any updates?", "@penpornk Changes applied for cost estimation update. Please see my commit for details.", "@penpornk You're welcome. And thank you for reivew this PR!", "@penpornk Hi, changes made for GPU build. Let's see if it works.", "@penpornk Fixed now.", "@wenxizhu Thank you for the quick fix! Let's see if it passes now.", "The PR is merged in https://github.com/tensorflow/tensorflow/commit/ddf3966936b65186b46cafe52181e0c2af480fae. I'm closing this PR now. Thank you again for your contributions!"]}, {"number": 24010, "title": "Make var loading logging messages debug from info", "body": "You may load a large amount of variables from a checkpoint and printing them all to \"INFO\" is not very useful as it spams the log.  I propose downgrading these to debug so that they only show up when you are trying to \"debug\" your loading.", "comments": ["Can I get a review on this?", "Ping?", "Nagging Reviewer @case540: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 74 days with no activity and the `awaiting review` label has been applied.", "@mihaimaruseac can you please help review this PR ?"]}]