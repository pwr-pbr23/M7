[{"number": 31400, "title": "Tensorflow Speech Command model showing many false positive", "body": "I am using tensorflow speech commands project to make a model that works on 4 commands : up, down, left, right. While the true positive rate is very good, the false positive rate is also very high. How to tackle this problem?", "comments": ["@Amanpradhan Please refer to the following [page](https://www.kdnuggets.com/2016/12/4-reasons-machine-learning-model-wrong.html) to understand more about confusion matrix which will resolve the issue that you are facing. Also please post these support questions in Stack Overflow as there is a bigger community to help. Thanks!"]}, {"number": 31399, "title": "Bug in tflite_convert: when converting tf.gather operation (r1.14)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): built from r1.14 (https://github.com/tensorflow/tensorflow/tree/r1.14)\r\n- TensorFlow version (or github SHA if from source): 1.14\r\n\r\n\r\n\r\nI might found a bug in tflite_convert, and I want to report this.\r\nI am not sure whether you aware of this or not, but I will explain it for the safety.\r\n\r\nThe following converting code gives an error in making tflite model.\r\nIt complains about checking dims() during the conversion.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nexport_dir = 'test_saved_model'\r\nbuilder = tf.saved_model.builder.SavedModelBuilder(export_dir)\r\n\r\nweight = np.random.rand(32,512)\r\n\r\nx = tf.placeholder(tf.float32, shape=[None,512])\r\nW = tf.constant(weight,dtype=tf.float32, name='kernel')\r\nz = x*W\r\n\r\ngdim = tf.constant(3,dtype=tf.int32)\r\ngaxis = tf.constant(0,dtype=tf.int32)\r\nWg = tf.get_variable(\"kernel\", [32,512])\r\ny = tf.gather(Wg,gdim,gaxis)\r\nres = y\r\n\r\n\r\n\r\n\r\n\r\nwith tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        builder.add_meta_graph_and_variables(sess, [\"test\"])\r\n        builder.save(as_text=True)\r\n\r\n        converter = tf.lite.TFLiteConverter.from_session(sess, [x], [res])\r\n\r\n        # options for converter\r\n        converter.dump_graphviz_dir=\"graph_dir\" # outputs graph visualization file (.dot)\r\n        converter.dump_graphviz_video=True\r\n\r\n        tflite_model = converter.convert()\r\n\r\n```\r\n\r\nI found that it is caused by the resolve_constant_gather.cc\r\nhttps://github.com/tensorflow/tensorflow/blob/a497fb2da9258cbfaa4e443f446c5ab039ef4a32/tensorflow/lite/toco/graph_transformations/resolve_constant_gather.cc#L42\r\n\r\nIt seems that the Gather function assumes that 'coords_array' is always 1-dim array.\r\nHowever, tf.gather may accept it as 0-dim array, a scalar value, so that the checking fail if 'coords_array' became a scalar.\r\n\r\n\r\nMy solution is following modification for Gather function.\r\nIt worked for me when I personally built and installed from the modified source.\r\n\r\nI hope someone to check about this and fix the issue if necessary.\r\n\r\n```\r\n// Gathers data from axis 0.\r\ntemplate <ArrayDataType Type>\r\ninline void Gather(const Array& input_array, const Array& coords_array,\r\n                   Array* output_array) {\r\n  const Shape& input_shape = input_array.shape();\r\n  const std::vector<DataType<Type>>& input_data =\r\n      input_array.GetBuffer<Type>().data;\r\n  const Shape& coords_shape = coords_array.shape();\r\n  const std::vector<int32>& coords_data =\r\n      coords_array.GetBuffer<ArrayDataType::kInt32>().data;\r\n\r\n  const Shape& output_shape = output_array->shape();\r\n  std::vector<DataType<Type>>& output_data =\r\n      output_array->GetMutableBuffer<Type>().data;\r\n  output_data.resize(RequiredBufferSizeForShape(output_shape));\r\n\r\n  int stride = 1;\r\n  for (int i = 1; i < input_shape.dimensions_count(); ++i) {\r\n    stride *= input_shape.dims(i);\r\n  }\r\n\r\n  if (coords_shape.dimensions_count()==0){\r\n        // when coords_array is 0-dim (a constant)\r\n        CHECK_EQ(stride * 1, output_data.size());\r\n\r\n        DCHECK_GE(coords_data[0], 0);\r\n        DCHECK_LT(coords_data[0], input_shape.dims(0));\r\n        DataType<Type>* out = output_data.data();\r\n        const DataType<Type>* in = input_data.data() + coords_data[0] * stride;\r\n        memcpy(out, in, sizeof(DataType<Type>) * stride);\r\n  }\r\n  else {\r\n        // when coords_array is 1-dim array\r\n        CHECK_EQ(coords_shape.dims(0), output_array->shape().dims(0));\r\n\r\n        // Let's make sure we have enough space for all element in the memcpy()\r\n        // below, which writes 'stride' elements starting at 'i * stride'.\r\n        CHECK_EQ(stride * coords_shape.dims(0), output_data.size());\r\n\r\n        for (int i = 0; i < coords_shape.dims(0); ++i) {\r\n            DCHECK_GE(coords_data[i], 0);\r\n            DCHECK_LT(coords_data[i], input_shape.dims(0));\r\n            DataType<Type>* out = output_data.data() + i * stride;\r\n            const DataType<Type>* in = input_data.data() + coords_data[i] * stride;\r\n        }\r\n  }\r\n}\r\n\r\n```", "comments": ["hey, sorry for late reply..\r\n\r\nwe've released a new mlir-based tflite converter. you can try with the new converter to see if the issue still happens.\r\n\r\nTo use it, please download latest tf-nightly, and then set `converter.experimental_new_converter = True`.\r\nthanks!", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31399\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31399\">No</a>\n"]}, {"number": 31398, "title": "The precision difference between tensorflow and numpy for matrix multiplication", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.0\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTensorflow matrix multiplication on CPU does not reproduce same result as numpy matrix multiplication for float32. Furthermore, (X @ Y) and (Y.T @ X.T).T produces different results.\r\nIs this a normal behavior, if it is, then which version of the matrix product should be taken as correct output?\r\n\r\n**Describe the expected behavior**\r\nTheoretically (X @ Y) and (Y.T @ X.T).T should evaluate to same value and their value should be same as numpy version.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = np.arange(150, dtype=np.float32).reshape(-1, 5)/10\r\ny = np.arange(200, dtype=np.float32).reshape(-1, 5)/10\r\nX_tf = tf.Variable(x)\r\nXT_tf = tf.Variable(x.T)\r\nY_tf = tf.Variable(y)\r\nYT_tf = tf.Variable(y.T)\r\nYXT1_tf = Y_tf @ XT_tf\r\nYXT2_tf = tf.transpose(X_tf @ YT_tf)\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    YXT1, YXT2 = sess.run([YXT1_tf, YXT2_tf])\r\nyxt1 = y @ x.T\r\nyxt2 = (x @ y.T).T\r\n\r\nprint(np.max(np.abs(YXT1-YXT2)), np.max(np.abs(yxt1-yxt2)))\r\nprint(np.max(np.abs(yxt1-YXT1)), np.max(np.abs(yxt1-YXT2)))\r\n```\r\n\r\nwhich outputs\r\n0.00012207031 0.0 \r\n0.00012207031 0.00012207031\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I was able to replicate the issue with TF verison-1.13, please find the [Gist](https://colab.sandbox.google.com/drive/1HpwjLl3nOSSFk7965__kiR22NoBsjjz_#scrollTo=b0BhWDenuN-y) of collab.Thanks", "@ozanyildiz93 I cannot reproduce the issue in tf-nightly. I think this was resolved in the `tf-nightly`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/ba880d7fb9ffdd6af13068a116aeed9a/tf_31398.ipynb). Thanks!", "Thank you. I will upgrade to the latest version in that case.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31398\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31398\">No</a>\n"]}, {"number": 31397, "title": "how to transfer ckpt to pb or saved model?", "body": "as the title shows, how we do this?\r\n\r\ntensorlfow has lots of saved format, and version issues.\r\n\r\nthere is no such document,about how we can do this.\r\n\r\ni search the similar issues, you all said these question maybe suitalbe for stackoverflow.\r\n\r\nbut none of you have answered the question.\r\n\r\nmaybe this is an document issue or not.\r\n\r\nthank you very much if you can give me the instructions", "comments": ["Have you tried using ```freeze_graph.py```.\r\nhttps://github.com/tensorflow/tensorflow/blob/236a77594fec644e0676ffe9144ff047cd7142fd/tensorflow/python/tools/freeze_graph.py#L26\r\n", "thanks ,i will try.. it seems to be able to transfer ckpt to frozen_graph as pb.\r\n\r\nwhat about the solutions for transfering to saved_model?", "See TensorFlow Saved Model https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md\r\nFeel free to reopen if have any further problems. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31397\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31397\">No</a>\n"]}, {"number": 31396, "title": "use unittest assertion method", "body": "", "comments": []}, {"number": 31395, "title": "how to remove pruned weights from graph?", "body": "i have prune some weights as 0, to improve the  inference speed, i want to remove these weights from my original graph, how can i realize this?", "comments": ["@Jasperty. Heres how you remove pruned weights from the graph:\r\n\r\n` bazel build -c opt contrib/model_pruning:strip_pruning_vars\r\nbazel-bin/contrib/model_pruning/strip_pruning_vars --checkpoint_dir=/path/to/checkpoints/ --output_node_names=graph_node1,graph_node2 --output_dir=/tmp --filename=pruning_stripped.pb`\r\n\r\nAlso Please refer to the following [link](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/model_pruning), as it explains how to remove the pruning ops after training the model and let me know if it helps. Thanks!", "@Jasperty Are you still facing the same issue or did you solve it?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this issue as it has been inactive for more than 14 days. Please add additional comments and we can reopen the issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31395\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31395\">No</a>\n"]}, {"number": 31394, "title": "replace assert and remove unused import", "body": "This PR uses the assertion method from `unittest` and also remove the unused import. ", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31394) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 31393, "title": "[ROCm] Fix for the broken `--config=rocm` build", "body": "The following PR/commit breaks the `--config=rocm` build\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/93f215539f208c20136c90fda5a6809eb463d404\r\n\r\nIt introduces references to `se::cuda::RedzoneAllocator` (which is only visible in the CUDA build) within code that is common to both the ROCm and CUDA builds. This \"fix\" moves those reference to code that is visible only in the CUDA build\r\n\r\n\r\n----------------------------\r\n\r\n\r\n@tatianashp @whchung @chsigg \r\n", "comments": []}, {"number": 31392, "title": "CUDA 10.1, failure to copy cublas_v2.h to build directory (again)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 29\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 19.2\r\n- GCC/Compiler version (if compiling from source): 7.4\r\n- CUDA/cuDNN version: 10.1 / 7.5.0.56\r\n- GPU model and memory: GeForce GT 750M with 2 GB\r\n\r\n\r\n**Describe the problem**\r\n\r\nAs with the *second* issue in [#28936](https://github.com/tensorflow/tensorflow/issues/28936), I am having problems when compiling tensorflow with CUDA 10.1. It seems that libcublas_v2.h is not being copied to the build directory. That ticket was closed with a comment indicating that it was due to a bug that had been fixed -- but the bug seems to have returned.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Installed CUDA 10.1 and cuDNN 7.5.\r\n2. Created logical links from /usr/lib64 to /usr/local/cuda-10.1/targets/x86_64-linux/lib/ for libcublas.so.10.1 (This removed the *first* issue reported in [#28936](https://github.com/tensorflow/tensorflow/issues/28936).)\r\n3. Ran ./configure, accepting defaults, except 'Y' for CUDA support, 3.0 for Cuda capability (actually, this is the default, as my GPU is correctly identified)\r\n4. bazel clean --expunge\r\n5. TMP=/tmp time bazel build --config=opt --local_resources=6144,6,1.0 //tensorflow/tools/pip_package:build_pip_package\r\n\r\nThis ran for more than an hour before yielding the error:\r\n\r\n```\r\nERROR: /usr/local/src/git/tensorflow/tensorflow/core/kernels/BUILD:2809:1: C++ compilation of rule '//tensorflow/core/kernels:cholesky_op' failed (Exit 1)\r\nIn file included from tensorflow/core/kernels/cholesky_op.cc:35:0:\r\n./tensorflow/core/kernels/cuda_solvers.h:29:10: fatal error: cuda/include/cublas_v2.h: No such file or directory\r\n #include \"cuda/include/cublas_v2.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```\r\n\r\nNote that unlike the report for [#28936](https://github.com/tensorflow/tensorflow/issues/28936), the file is missing from cuda/include/, rather than third_party/gpus/cuda/include/.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["My mistake: ran the clean --expunge after, rather than before, ./configure."]}, {"number": 31391, "title": "R2.0", "body": "update the version on TF Keras", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31391) for more info**.\n\n<!-- need_sender_cla -->", "This shouldn't be merged as such. `r2.0` branch was started from `master` and only gets cherry-picks from `master`."]}, {"number": 31390, "title": "Attempt to load versioned shared objects in Java", "body": "Since TensorFlow 1.14.0, libtensorflow_framework.so has been packaged as\r\na versioned shared object. This change attempts to support that naming\r\nscheme in the Java NativeLibrary loader.\r\n\r\nIf libtensorflow_framework.so is not present and a major version number\r\ncan be determined, the loader will attempt to find\r\nlibtensorflow_framework.so.<majorVersion>, where <majorVersion> is\r\ndetermined from the manifest of the JAR from which the NativeLibrary\r\nclass was loaded. On darwin, the loader will look for\r\nlibtensorflow_framework.<majorVersion>.so.\r\n\r\nPiperOrigin-RevId: 260749010", "comments": []}, {"number": 31389, "title": "Enhance release notes related to TF_CUDNN_DETERMINISTIC", "body": "Follow-up to [PR 29667 (merged)](https://github.com/tensorflow/tensorflow/pull/29667). @mihaimaruseac please merge before v1.14.1 tag.", "comments": []}, {"number": 31388, "title": "Use incompatible_windows_native_test_wrapper instead of experimental_\u2026", "body": "\u2026windows_native_test_wrapper\r\n\r\nBased on comment in 259986109.\r\n\r\nWith `experimental_windows_native_test_wrapper`, we get all windows jobs\r\nfailing with\r\n\r\n```\r\n+ Tue, Aug  6, 2019 10:21:51 PM + bazel test --announce_rc --config=opt -k --test_output=errors --experimental_windows_native_test_wrapper --define=no_tensorflow_py_deps=true --test_lang_filters=py --test_tag_filters=-no_pip,-no_windows,-no_oss,-gpu --build_tag_filters=-no_pip,-no_windows,-no_oss,-gpu --build_tests_only --test_size_filters=small,medium --jobs=32 --test_timeout=300,450,1200,3600 --flaky_test_attempts=3 //py_test_dir/tensorflow/python/...\r\nERROR: Unrecognized option: --experimental_windows_native_test_wrapper\r\n```", "comments": []}, {"number": 31387, "title": "replace assert and remove unused import", "body": "This PR uses the assertion method from `unittest` and also remove the unused import. ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31387) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 31386, "title": "Build v2 by default", "body": "When running bazel build on r2.0 branch, we should build TensorFlow 2.0 by default.\r\n\r\nAfter this change, behavior is as follows:\r\n* If --config=v2 is passed in, generate TensorFlow 2.x API and run tests with 2.x behavior.\r\n* If --config=v1 is passed in, generate TensorFlow 1.x API and run tests with 1.x behavior.\r\n* Default is same as if --config=v2 was passed in.\r\n", "comments": ["I had to fix / disable a bunch of tests to get presubmits green. Now that everything is green, ptal."]}, {"number": 31385, "title": "r2.0-rc0 cherry-pick request: Enabling eager rewriting for MKL matmul", "body": "This PR is a performance fix for TF-MKL. Without this PR, TF-MKL cannot use MKL matmul in eager mode and will be slow.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31385) for more info**.\n\n<!-- need_author_consent -->", "@agramesh1 This is a cherry-pick for #31311. (#30402 made it into the cut.) Could you please reply \"@googlebot I consent\" to resolve the CLA problem? Thank you!", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31385) for more info**.\n\n<!-- cla_yes -->", "> A Googler has manually verified that the CLAs look good.\r\n> \r\n> (Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31385) for more info**.\r\n\r\nApproving these changes to take in the performance fix. manually added the cla because the author requesting the cherrypick is a googler. "]}, {"number": 31384, "title": "Fix buildifier check", "body": "This cherry-picks 4 commits that are on master and are needed to make buildifier check pass on the new Ubuntu16 box", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31384) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 31383, "title": "Update TF version 2.0.0-rc0 and update estimator and tensorboard nightly versions.", "body": "", "comments": []}, {"number": 31382, "title": "TF2.0 cherry-pick request: Don't copy more variant elements...", "body": "This cherry-picks commit 966a8f9aeac3ce7ce3e106a9d429f1a415792c8e\r\n\r\n```\r\nDon't copy more variant elements than allowed by tensor shape\r\nPiperOrigin-RevId: 261962798\r\n```\r\n\r\nThis is a fuzzer discovered issue which could be exploited if not patched.", "comments": []}, {"number": 31381, "title": "Tensor2tensor acceleration ops", "body": "This pull request creates a new contrib project t2t with two CUDA-optimized ops for use by tensor2tensor. These ops accelerate GPU training of tensor2tensor transformer models by ~10% using FP32 and up to 20% using FP16.\r\n\r\n[Replacement of PR#31003]\r\n\r\nIt also enables ROCm GPU compilation pathways for most reduction ops (verified to work with the latest ROCm).  ", "comments": ["I would much rather not add things to contrib. Contrib is deprecated, and will be removed soon (it will not be distributed with TensorFlow starting at 2.0). \r\n\r\nI don't know if any of these ops are more generally useful -- at first glance, they seem to be general functions, and not specifically tied to t2t. If so, we would likely accept optimized GPU kernels for ops such as L2 norm (possibly, in tensorflow/addons at first).\r\n\r\nBut contrib isn't a good place for this any more."]}, {"number": 31380, "title": "Switch  flat_map tests to use TF combinations", "body": "This PR switches `flat_map` tests to use TF combination.", "comments": ["@jsimsa Just in case you missed this PR, this is a gentle reminder to help review this PR when you get a chance. ", "Can one of the admins verify this patch?", "@jsimsa More tests are switched to use TF combination.", "@rthadur what is this PR blocked on?", "@jsimsa there were some merge conflicts , i resolved it , can you please approve again ?", "@jsimsa The comments are addressed and there is no change for the `from_sparse_tensor_slices_test.py`  in the master branch now. ", "The internal checks failed. Could you please help check the logs and paste them here?", "```\r\n======================================================================\r\nFAIL: testSkipEagerNestedStructure_test_mode_eager_tfapiversion_1 (__main__.FromTensorsTest)\r\ntestSkipEagerNestedStructure_test_mode_eager_tfapiversion_1 (__main__.FromTensorsTest)\r\ntestSkipEagerNestedStructure_test_mode_eager_tfapiversion_1(mode='eager', tf_api_version=1)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/build/work/bbfe7e7aa04b186c1d68ff6665bd9049dc07/google3/runfiles/google3/third_party/py/absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n    yield\r\n  File \"/build/work/bbfe7e7aa04b186c1d68ff6665bd9049dc07/google3/runfiles/google3/third_party/py/absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n    testMethod()\r\n  File \"/build/work/bbfe7e7aa04b186c1d68ff6665bd9049dc07/google3/runfiles/google3/third_party/py/absl/testing/parameterized.py\", line 263, in bound_param_test\r\n    test_method(self, **testcase_params)\r\n  File \"/build/work/bbfe7e7aa04b186c1d68ff6665bd9049dc07/google3/runfiles/google3/third_party/tensorflow/python/framework/test_combinations.py\", line 310, in decorated\r\n    execute_test_method()\r\n  File \"/build/work/bbfe7e7aa04b186c1d68ff6665bd9049dc07/google3/runfiles/google3/third_party/tensorflow/python/framework/test_combinations.py\", line 298, in execute_test_method\r\n    test_method(**kwargs_to_pass)\r\n  File \"/build/work/bbfe7e7aa04b186c1d68ff6665bd9049dc07/google3/runfiles/google3/third_party/tensorflow/python/data/kernel_tests/from_tensors_test.py\", line 248, in testSkipEagerNestedStructure\r\n    self.assertEqual([None, 3], w.shape.as_list())\r\nAssertionError: Lists differ: [None, 3] != [1, 3]\r\n\r\nFirst differing element 0:\r\nNone\r\n1\r\n\r\n- [None, 3]\r\n+ [1, 3]\r\n```", "@jsimsa Thanks for checking the internal failures! The failures are caused by the difference of output shapes in the graph and eager mode. The issues are addressed [here](https://github.com/tensorflow/tensorflow/pull/31380/commits/f6619348950c206667ff60e491dbe47a74c6c16c). Could you please have another look?\r\n\r\nBTW, do we need to disable `FromTensorsTest` on mac as indicated [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/kernel_tests/BUILD#L245). I can run it on my mac locally. ", "Feel free to remove the \"nomac\" exclusion if the tests pass for you, but let's do it in a separate CL.", "> Feel free to remove the \"nomac\" exclusion if the tests pass for you, but let's do it in a separate CL.\r\n\r\nSure, will do it once this PR gets merged.", "@jsimsa Sorry that I made a mistake in the last commit, which causes the method name conflicts. This commit(https://github.com/tensorflow/tensorflow/pull/31380/commits/1a6763540474873b5864d24430e26293dd6df360) changes the method name from `def testNestedStructure(...)` to `testNestedStructureV2(...)` to avoid the conflict with [here](https://github.com/tensorflow/tensorflow/pull/31380/files#diff-ee9cf39bad94aaf7df788842a74ce795L137). ", "@jsimsa Thanks for your suggestion. The last commit has been rewritten as you suggested. Please have a look at the change (https://github.com/tensorflow/tensorflow/pull/31380/commits/98aa8bbca941a7b7544ba4ae073e012226cedc4a).", "This PR has been pending for a while. Are there anything else I need to do?", "@jsimsa @rthadur This PR seems to be blocked. Could you please help check? Let me know if there is anything I can do.", "@feihugis sorry for the delay , let me try to pull this again , thank you for your patience.", "> @feihugis sorry for the delay , let me try to pull this again , thank you for your patience.\r\n\r\n@rthadur No worries! Thanks for your help!", "This has been merged internally , waiting for auto-merge to happen, thank you", "Thank you, @rthadur!"]}, {"number": 31379, "title": "Update r2.0 branch to prepare  for TF2.0.0 release", "body": "Followed Mihai's detailed instructions to update r2.0 branch up to Aug 6th to prepare for TF 2.0.0 release.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31379) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 31378, "title": "Don't copy more variant elements than allowed by tensor shape", "body": "PiperOrigin-RevId: 261962798\r\n\r\nCherry-pick of 966a8f9aeac3ce7ce3e106a9d429f1a415792c8e", "comments": []}, {"number": 31377, "title": "Quantized SSD TFLite runs much slower in GPU (GTX 1080) when compared to its original model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): NO\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10/7.5\r\n- GPU model and memory: GTX 1080 TI\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI converted the SSD into the TFLite model with quantization and ran a speed comparison between the quantized SSD and the original SSD. The time used for each image: \r\nquantized SSD - 110 ms \r\noriginal SSD - 12 ms\r\nthe quantized SSD is almost ten times slower than the original one in GPU. \r\nSome additional info:\r\nSSD is based on the mobilenet_v1, no other costumed operations. \r\nCodes used to do the conversion:\r\ntflite_convert \\\r\n  --output_file=./ssd.tflite \\\r\n  --graph_def_file=./tflite_graph.pb \\\r\n  --inference_type=FLOAT \\\r\n  --input_arrays=normalized_input_image_tensor \\\r\n  --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n  --std_dev_values=128 --mean_values=128 \\\r\n  --default_ranges_min=-6 --default_ranges_max=6 \\\r\n  --input_shapes=1,300,300,3 \\\r\n  --allow_custom_ops\r\n\r\nThe two models are in the link:\r\nhttps://drive.google.com/drive/folders/1Q5aOZRlxhDsMdiSYYSuWlUyQP82FJTdC?usp=sharing\r\n\r\nAnyone can give me some suggestion?  Thank you!\r\n\r\n", "comments": ["Some more info:\r\nwhen run the quantized ssd, although it use the GPU (the mem usage is increased to ~600M while the calculation usage is 0%, but the cpu usage is increased a lot; ) \r\nwhen run the original model, the gpu mem is also around 600M the gpu computation is 5-10%. \r\n\r\nSo it looks like the quantized model can not use GPU?", "@xmuszq Interesting. Why do you expect quantized TFLite model to be better on GTX 1080? There are some problems.\r\n1. as far as I can remember, 1080's integer computation power is weaker than floating point one\r\n2. There is no cuda GPU acceleration for TFLite. Although there is TFLite GPU delegate, which supports GL ES compute shader and Apple's Metal shader, it's not ported to desktop/server GPU (yet)\r\n3. TFLite GPU delegate  was designed for floating point. So, even if you have it ported to 1080, mostly It won't help quantized TFLite model.", "1080 has fp16 arithmetic very fast, ", "Yes, generally the quantized model is meant for running on CPU and certain hardware accelerators (e.g ones behind the NNAPI and the EdgeTPU) and not for the GPU. As Stefan said, fp16 is great on the GPUs and you may want to look at this: https://www.tensorflow.org/lite/performance/post_training_float16_quant"]}, {"number": 31376, "title": "Fix loading for tf_exec_compatible_with and tf_cuda_tests_tags", "body": "PiperOrigin-RevId: 249041071\r\n\r\nCherry picking the above commit as macos builds fail with\r\n\r\n```\r\nERROR: /Volumes/BuildData/tmpfs/src/github/tensorflow/tensorflow/examples/adding_an_op/BUILD:11:1: file '//tensorflow:tensorflow.bzl' does not contain symbol 'tf_cuda_tests_tags'\r\nERROR: /Volumes/BuildData/tmpfs/src/github/tensorflow/tensorflow/examples/adding_an_op/BUILD:13:1: file '//tensorflow:tensorflow.bzl' does not contain symbol 'tf_exec_compatible_with'\r\nERROR: /Volumes/BuildData/tmpfs/src/github/tensorflow/tensorflow/examples/adding_an_op/BUILD:134:28: Traceback (most recent call last):\r\n\tFile \"/Volumes/BuildData/tmpfs/src/github/tensorflow/tensorflow/examples/adding_an_op/BUILD\", line 130\r\n\t\tpy_test(name = \"cuda_op_test\", size = \"sma...\", <6 more arguments>)\r\n\tFile \"/Volumes/BuildData/tmpfs/src/github/tensorflow/tensorflow/examples/adding_an_op/BUILD\", line 134, in py_test\r\n\t\ttf_exec_compatible_with\r\nname 'tf_exec_compatible_with' is not defined\r\nERROR: package contains errors: tensorflow/examples/adding_an_op\r\n```", "comments": []}, {"number": 31375, "title": "Release notes for AMP and TF-TRT precision_mode", "body": "@mihaimaruseac These are NVIDIA's final tweaks for the TF 1.14.1 release notes.", "comments": []}, {"number": 31374, "title": "Jvishnuvardhan patch 2", "body": "", "comments": []}, {"number": 31373, "title": "tf.data.Dataset.map + tf.numpy_function = lost shape sadness", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I slightly modified a tf documentation code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04 LTS / macOS 14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): 1.14\r\n- TensorFlow version (use command below): conda\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: colab\r\n- GPU model and memory: colab\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n`tf.numpy_function` works are wrapping numpy / python functions with tf 1.14 in eager and normal mode, but shape is lost\r\n\r\n\r\n```python\r\n# slight modification from \"Load Images with tf.data\" https://www.tensorflow.org/tutorials/load_data/images\r\n\r\n\r\ndef preprocess_image(image, resize=[192, 192]):\r\n  image = tf.image.decode_jpeg(image, channels=3)\r\n  image = tf.image.resize(image, [192, 192])\r\n  image /= 255.0  # normalize to [0,1] range\r\n\r\n  return image\r\n\r\n\r\ndef load_and_preprocess_image(path, resize=[192, 192]):\r\n  image = tf.read_file(path)\r\n  return preprocess_image(image, resize)\r\n\r\n# dataset only contains paths, so wrap whatever value for `resize` in lambda\r\n_load_and_preprocess_image = lambda path: load_and_preprocess_image(path, [192,192])\r\n\r\n# we \"have\" numpy functionality for handling images so wrap in `tf.numpy_function`\r\ntf_load_and_preprocess_image = lambda path: tf.numpy_function(_load_and_preprocess_image, [path], tf.float32)\r\n\r\n# map\r\nimage_ds2_error_boogaloo = path_ds.map(tf_load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\r\n\r\n# no shape\r\nimage_ds2_error_boogaloo\r\n# `<DatasetV1Adapter shapes: , types: tf.float32>`\r\n\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nshape is not lost\r\n\r\nsupposedly fixed here #16052 \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nhttps://colab.research.google.com/drive/1DeGMPxb8cyHm5QLpqJ9-cB2sbF8PC3vA#scrollTo=qGiGN5rl4s2f&line=22&uniqifier=1\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I have tried on colab with TF version 1.14 [gist](https://colab.research.google.com/drive/1w9vKC09IiCJm8X-IQHDGNWxs8WpMPc8J) and nightly versions [gist](https://colab.research.google.com/drive/1Qh1ts8-ijENNopr9yd2pA8_uFBMkxMiQ) was able to reproduce the issue.Thanks!", "@jvishnuvardhan any updates?", "I was dealing with the same problem (in Tensorflow 2.0 beta) and found a workaround of manually setting the `_output_structure` attribute of the `_map_func` (`StructuredFunctionWrapper`) attribute of `MapDataset` (possible thanks to Python's lack of hidden variables) with the proper `NestedStructure` object i.e\r\n```\r\nimage_ds._map_func._output_structure = data.experimental.NestedStructure(...)\r\n```\r\n This suggests a fix of providing an optional structure tuple to `Dataset.map` that can be passed to the  `StructuredFunctionWrapper` to be used in lieu of automatic inference. Not sure how easy that is ; the `StructuredFunctionWrapper` class is fairly opaque to me at first glance.", "@SumNeuron `numpy_function` is opaque to shape inference (as it can contain arbitrary Python code), so the functionality you are asking for is not feasible. Having said that, I don't see why you need to use `numpy_function` for your example. The follow pipeline will work just fine:\r\n\r\n```\r\nfiles = tf.data.Dataset.list_files(...)\r\nimages = files.map(lambda f: tf.io.read_file(f))\r\nimages = images.map(lambda image: tf.io.decode_jpeg(image))\r\nimages = images.map(lambda image: tf.cast(tf.image.resize_images(image, (128, 128)), tf.uint8))\r\nimages = images.map(lambda image: tf.cast(image, tf.float32) / 255)\r\n```\r\n\r\nEDIT: Note that you can also restore shapes lost due to applying a `numpy_function` through `map(lambda x: tf.set_shape(x, ...)`.\r\n\r\n@deasmhumhna I would highly discourage your from relying on implementation details of private attributes as these may change between versions and your code will break.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31373\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31373\">No</a>\n", "@jsimsa the reason for the `numpy_function` inclusion, is because I provided a MWE rather than the actual code. Perhaps one would, inside of `preprocess_image`, do more advanced alterations. See [issue 31235](https://github.com/tensorflow/tensorflow/issues/31235) which includes code copy-pasted from keras's image processing which one might want to use without re writing `numpy` core functions. \r\n\r\nAs for \"as it can contain arbitrary Python code\", true, but in context of applying `numpy` code to `tf.data.Datasets` that doesn't mean it has to exclude attempting shape inference. As stated in the docs:\r\n>Given a python function func, which takes numpy arrays as its arguments and returns numpy arrays as its outputs, wrap this function as an operation in a TensorFlow graph. \r\n\r\nSince it returns arrays as outputs, and numpy arrays have shape this seems like an easy thing to do.", "The \"easy thing to do\" amounts to writing shape inference for arbitrary Python code, which is not an easy thing to do.", "I had the issue of lost shape because of .map with tf.py_function and I fixed it with the following code (thanks to https://github.com/tensorflow/tensorflow/issues/16052#issue-287936599) :\r\n\r\n```python\r\ndef set_shapes(img, label, img_shape):\r\n    img.set_shape(img_shape)\r\n    label.set_shape([])\r\n    return img, label\r\n\r\nimg_shape = images[0].shape  # images is a list of numpy.ndarray\r\nds = ds.map(lambda img, label: tf.py_function( ... ) )\r\nds = ds.map(lambda img, label: set_shapes(img, label, img_shape) )\r\n```\r\n", "this shape missing behaviour is a really annoying problem, it defeat the purpose of the py_function, and it should be fixed.", "To fix this issue, I used the following code :\r\ndef map_func(img_name, cap):\r\n <pre> img_tensor = np.load(img_name) \r\n return img_tensor, cap </pre>\r\ndataset = tf.data.Dataset.from_tensor_slices((train_video_paths, train_labels))\r\ndef shape(x,y):\r\n<pre>\r\n  x.set_shape((6000,400))\r\n  y.set_shape((48))\r\n  return x,y\r\n</pre>\r\n\r\ndataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\ndataset=dataset.map(shape)\r\n\r\ndataset = dataset.shuffle(5).batch(5)\r\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n\r\n\r\nBut it gives the following error when iterating through the dataset:\r\nInvalidArgumentError: 0-th value returned by pyfunc_7 is double, but expects float\r\n\t [[{{node PyFunc}}]]"]}, {"number": 31372, "title": "Add one include file so that compile would succeed.", "body": "Without\r\n\r\n```c\r\n#include \"tensorflow/core/common_runtime/input_colocation_exemption_registry.h\"\r\n```\r\n\r\ncompilation will crash with\r\n\r\n```\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/core/kernels/data/experimental/BUILD:360:1: C++ compilation of rule '//tensorflow/core/kernels/data/experimental:take_while_dataset_op' failed (Exit 1)\r\ntensorflow/core/kernels/data/experimental/take_while_dataset_op.cc:203:36: error: expected constructor, destructor, or type conversion before '(' token\r\n REGISTER_INPUT_COLOCATION_EXEMPTION(\"TakeWhileDataset\");\r\n                                    ^\r\ntensorflow/core/kernels/data/experimental/take_while_dataset_op.cc:204:36: error: expected constructor, destructor, or type conversion before '(' token\r\n REGISTER_INPUT_COLOCATION_EXEMPTION(\"ExperimentalTakeWhileDataset\");\r\n                                    ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1158.013s, Critical Path: 245.65s\r\nINFO: 8464 processes: 8464 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nThis follows #31341 and #31336. However, as the include was added in another change which is too complex to also be cherry-picked here, I'm doing it manually.", "comments": ["good job @mihaimaruseac ", "tensorflow/core/kernels/data/experimental/group_by_reducer_dataset_op.cc"]}, {"number": 31371, "title": "Update r2.0 branch to Aug 3rd for TF2.0.0 release", "body": "This is fast-forwarding `r2.0` branch to e7a69f18905d948733289aaf99736ae1bc71c190\r\n\r\nSteps taken:\r\n\r\n1. Clone and get a branch created to where we want to fast forward.\r\n```bash\r\nmihaimaruseac@ankh:/tmp/tf2.0$ git clone git@github.com:tensorflow/tensorflow.git\r\n...\r\nmihaimaruseac@ankh:/tmp/tf2.0$ cd tensorflow/\r\nmihaimaruseac@ankh:/tmp/tf2.0/tensorflow$ git checkout e7a69f18905d948733289aaf99736ae1bc71c190\r\n...\r\nmihaimaruseac@ankh:/tmp/tf2.0/tensorflow$ git switch -c master-where-we-want-it\r\nSwitched to a new branch 'master-where-we-want-it'\r\n```\r\n\r\n2. First part of fast-forwarding: checkout branch, merge \"master\" into it but don't resolve any conflicts:\r\n```bash\r\nmihaimaruseac@ankh:/tmp/tf2.0/tensorflow$ git checkout r2.0 \r\nBranch 'r2.0' set up to track remote branch 'r2.0' from 'origin'.\r\nSwitched to a new branch 'r2.0'\r\nmihaimaruseac@ankh:/tmp/tf2.0/tensorflow$ git merge --no-commit master-where-we-want-it \r\nRemoving tensorflow/tools/tensorflow_builder/data/__init__.py\r\nAuto-merging tensorflow/tools/pip_package/setup.py\r\nCONFLICT (content): Merge conflict in tensorflow/tools/pip_package/setup.py\r\n....\r\nAutomatic merge failed; fix conflicts and then commit the result.\r\n```\r\n\r\n3. Solve all conflicts using what's on \"master\":\r\n```bash\r\nmihaimaruseac@ankh:/tmp/tf2.0/tensorflow$ git checkout --theirs master-where-we-want-it  .\r\nUpdated 40 paths from 929b4dd112\r\n```\r\n\r\n4. Just checking status, we see that there are some files which got deleted on \"master\" but have been updated locally. Thus, we're telling git we want to remove them:\r\n```bash\r\nmihaimaruseac@ankh:/tmp/tf2.0/tensorflow$ git status -sb | grep '^UD'\r\nUD tensorflow/tools/api/golden/v1/tensorflow.data.experimental.-nested-structure.pbtxt\r\nUD tensorflow/tools/api/golden/v2/tensorflow.data.experimental.-dataset-structure.pbtxt\r\nUD tensorflow/tools/api/golden/v2/tensorflow.data.experimental.-nested-structure.pbtxt\r\nUD tensorflow/tools/api/golden/v2/tensorflow.data.experimental.-structure.pbtxt\r\nmihaimaruseac@ankh:/tmp/tf2.0/tensorflow$ git rm tensorflow/tools/api/golden/v1/tensorflow.data.experimental.-nested-structure.pbtxt tensorflow/tools/api/golden/v2/tensorflow.data.experimental.-dataset-structure.pbtxt tensorflow/tools/api/golden/v2/tensorflow.data.experimental.-nested-structure.pbtxt tensorflow/tools/api/golden/v2/tensorflow.data.experimental.-structure.pbtxt\r\nrm 'tensorflow/tools/api/golden/v1/tensorflow.data.experimental.-nested-structure.pbtxt'\r\nrm 'tensorflow/tools/api/golden/v2/tensorflow.data.experimental.-dataset-structure.pbtxt'\r\nrm 'tensorflow/tools/api/golden/v2/tensorflow.data.experimental.-nested-structure.pbtxt'\r\nrm 'tensorflow/tools/api/golden/v2/tensorflow.data.experimental.-structure.pbtxt'\r\n```\r\n\r\n5. Now commit all these changes, make a new branch, push it and then create this PR\r\n```bash\r\nmihaimaruseac@ankh:/tmp/tf2.0/tensorflow$ git commit\r\nRecorded resolution for 'RELEASE.md'.\r\nRecorded resolution for 'tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc'.\r\nRecorded resolution for 'tensorflow/core/public/version.h'.\r\nRecorded resolution for 'tensorflow/lite/python/lite_v2_test.py'.\r\nRecorded resolution for 'tensorflow/python/data/experimental/ops/scan_ops.py'.\r\nRecorded resolution for 'tensorflow/python/data/kernel_tests/dataset_test.py'.\r\nRecorded resolution for 'tensorflow/python/data/kernel_tests/optional_test.py'.\r\nRecorded resolution for 'tensorflow/python/data/ops/dataset_ops.py'.\r\nRecorded resolution for 'tensorflow/python/data/ops/optional_ops.py'.\r\nRecorded resolution for 'tensorflow/python/data/util/structure.py'.\r\nRecorded resolution for 'tensorflow/python/data/util/structure_test.py'.\r\nRecorded resolution for 'tensorflow/python/framework/indexed_slices.py'.\r\nRecorded resolution for 'tensorflow/python/framework/sparse_tensor.py'.\r\nRecorded resolution for 'tensorflow/python/framework/tensor_spec.py'.\r\nRecorded resolution for 'tensorflow/python/framework/type_spec.py'.\r\nRecorded resolution for 'tensorflow/python/keras/BUILD'.\r\nRecorded resolution for 'tensorflow/python/keras/backend.py'.\r\nRecorded resolution for 'tensorflow/python/keras/distribute/multi_worker_callback_test.py'.\r\nRecorded resolution for 'tensorflow/python/keras/engine/base_layer.py'.\r\nRecorded resolution for 'tensorflow/python/keras/engine/network.py'.\r\nRecorded resolution for 'tensorflow/python/keras/engine/training.py'.\r\nRecorded resolution for 'tensorflow/python/keras/integration_test.py'.\r\nRecorded resolution for 'tensorflow/python/keras/layers/lstm_v2_test.py'.\r\nRecorded resolution for 'tensorflow/python/keras/layers/recurrent_v2.py'.\r\nRecorded resolution for 'tensorflow/python/keras/saving/hdf5_format_test.py'.\r\nRecorded resolution for 'tensorflow/python/keras/saving/save.py'.\r\nRecorded resolution for 'tensorflow/python/keras/saving/save_test.py'.\r\nRecorded resolution for 'tensorflow/python/keras/saving/saved_model_experimental.py'.\r\nRecorded resolution for 'tensorflow/python/keras/saving/saved_model_experimental_test.py'.\r\nRecorded resolution for 'tensorflow/python/ops/ragged/ragged_tensor.py'.\r\nRecorded resolution for 'tensorflow/python/ops/tensor_array_ops.py'.\r\nRecorded resolution for 'tensorflow/python/saved_model/load.py'.\r\nRecorded resolution for 'tensorflow/python/training/tracking/base.py'.\r\nRecorded resolution for 'tensorflow/python/training/tracking/data_structures.py'.\r\nRecorded resolution for 'tensorflow/tensorflow.bzl'.\r\nRecorded resolution for 'tensorflow/tools/api/golden/v1/tensorflow.-optional-spec.pbtxt'.\r\nRecorded resolution for 'tensorflow/tools/api/golden/v1/tensorflow.data.experimental.-dataset-structure.pbtxt'.\r\nRecorded resolution for 'tensorflow/tools/api/golden/v1/tensorflow.data.experimental.-optional-structure.pbtxt'.\r\nRecorded resolution for 'tensorflow/tools/api/golden/v2/tensorflow.data.experimental.pbtxt'.\r\nRecorded resolution for 'tensorflow/tools/pip_package/setup.py'.\r\n[r2.0 564e052b01] Update r2.0 branch to Aug 3rd for TF2.0.0 release\r\nmihaimaruseac@ankh:/tmp/tf2.0/tensorflow$ git checkout -b mm-2.0\r\nSwitched to a new branch 'mm-2.0'\r\nmihaimaruseac@ankh:/tmp/tf2.0/tensorflow$ git push\r\nEnumerating objects: 3535, done.\r\nCounting objects: 100% (2218/2218), done.\r\nDelta compression using up to 56 threads\r\nCompressing objects: 100% (487/487), done.\r\nWriting objects: 100% (968/968), 381.35 KiB | 11.92 MiB/s, done.\r\nTotal 968 (delta 805), reused 611 (delta 478)\r\nremote: Resolving deltas: 100% (805/805), completed with 301 local objects.\r\nremote: \r\nremote: Create a pull request for 'mm-2.0' on GitHub by visiting:\r\nremote:      https://github.com/tensorflow/tensorflow/pull/new/mm-2.0\r\nremote: \r\nTo github.com:tensorflow/tensorflow.git\r\n * [new branch]            mm-2.0 -> mm-2.0\r\nBranch 'mm-2.0' set up to track remote branch 'mm-2.0' from 'origin'.\r\n```\r\n\r\nJust checking that this branch is the same as the target commit:\r\n\r\n```bash\r\nmihaimaruseac@ankh:/tmp/tf2.0/tensorflow$ git diff e7a69f18905d948733289aaf99736ae1bc71c190\r\n```\r\n\r\n", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31371) for more info**.\n\n<!-- need_author_consent -->", "Looking at https://github.com/tensorflow/tensorflow/compare/r2.0 and checking commits there, there are a few that need cherry-picking:\r\n\r\n1. version change: d1d77dd8c8f35f21d3488dd909551d0d4f1ebc93, 2becc5bc82205d992c17c99da93e1d14dc827ad1, 00f6608a0a73b768cd4a810601293d13e3bc722f and b43f012dec74bbb85b3d24d917ac1ca5d3f223c1\r\n1. release notes: 74a8ab67fac913c8b8595a0dc3d768f8f8b43d3e, 976d0b63c72d4fee799b61e0ca79c97e1c286340 and 8e423e3d56390671f0d954c90f4fd163ab02a9c1\r\n\r\nFor each one of them:\r\n\r\n```bash\r\n$ git cherry-pick ${commit_hash}\r\n$ git mergetool                               # if conflicts, solve them\r\n$ git cherry-pick --continue            # if conflicts\r\n```\r\n\r\nNote: I'm not sure if we need fba5ab6ad0c11e84c0a64fb7e689d790a6c1d7fc or not. In the end, we might need to check the version numbers for all of our dependencies to make sure they are good. That can come in a separate PR.", "To be redone on a different commit, closing this one"]}]