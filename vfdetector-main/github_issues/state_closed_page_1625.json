[{"number": 4160, "title": "R0.10", "body": "Upgrade to cudnn5 and point deve docker image to the correct cudnn location.\n", "comments": ["@gunan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @ebrevdo, @vrv and @martinwicke to be potential reviewers\n", "LGTM\n", "+1\n"]}, {"number": 4159, "title": "Push from internal.", "body": "", "comments": []}, {"number": 4158, "title": "Add support for NVIDIA nccl library", "body": "The nccl library supports fast GPU-GPU communications and tensorflow could leverage its use.\n\nhttps://github.com/NVIDIA/nccl\n\nI think I read somewhere that Google is implementing this internally, but would be good to know what is the plan.\n", "comments": ["We're aware of this.  The TensorFlow programming model is somewhat different from what this library addresses.  We want programmers to specify their computations at a higher level, and plan to automatically support parallel distribution including efficient data transfers as needed.\n", "So let me reframe the question. What is the plan to leverage the GPU to GPU communication capabilities of hardware like PCI switch or the DGX1 ? [nccl seems to use NVLINK and QPI primitives](https://github.com/NVIDIA/nccl/blob/75bad643bde5e252346176fe26df08c65ecbb6b6/src/libwrap.cu) and doing a github search for those same primitive names, seems the stream_executor does not support that. \n", "It is unlikely TensorFlow can use NCCL in its current form, as it doesn't fit into TensorFlow's programming and memory model. However, we could implement something similar to leverage the lower-level functionalities. Although I don't have a timeline at this point. \n", "I think this has changed recently right?", "Looks like NCCL support has been added via contrib.  @cwhipkey ", "yes, basic support for all-reductions + broadcast is checked in to contrib.", "@cwhipkey where is the support?", "tf.contrib.nccl  - see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/nccl/python/ops/nccl_ops.py"]}, {"number": 4157, "title": "add specialized size_t comparison macros to reduce number of warnings", "body": "", "comments": ["@Mistobaan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman and @josh11b to be potential reviewers\n", "Can one of the admins verify this patch?\n", "@keveman Can you please take a look?\n", "@martinwicke fixed \n", "Jenkins, test this please.\n"]}, {"number": 4156, "title": "[WIP] Embed git version into open source build (for review)", "body": "TODO(aselle): DO NOT MERGE\n", "comments": ["Handling internally from here on.\n"]}, {"number": 4155, "title": "Tensorboard: Feature and/or documentation request: Viz hyper parameter set ", "body": "(I am not sure if this is a documentation issue or a feature request.)\n\nRunning multiple studies using different hyper parameters can be difficult when more than a handful of hyper parameters are investigated over a shorter timespan. The way we done it is to show the active hyper parameter set  in an ascii string (the log directory name). \n\nWe already have 10 or so hyper parameters and showing them all in the directory name of the plot and comparing them is simply not a sustainable workflow. Naming the log directories the same as the input file (spec. the hyperparameters) is not so good either because the researcher have to open up the file and see / remind himself what the specific hyperparamteres where set to.\n\nI wonder what people suggests using in this workflow? Is there a efficient workflow for this that I just have not discovered yet, or is some new feature required?\n", "comments": ["This seems more like a stackoverflow question.  It doesn't seem like a feature that TensorFlow would plausibly provide.\n", "Yes, this is a question better suited for StackOverflow.  Please ask it there and tag it with the `tensorflow` tag.  Thanks!\n"]}, {"number": 4154, "title": "wrapper for NVIDIA's gpu inference engine?", "body": "does this help improving the throughput of the infence for tensorflow?\ncan we make wrapper for it?\n\nhttps://developer.nvidia.com/gpu-inference-engine\n", "comments": ["We are in the process of evaluating GIE among other options. \n", "What are the other options evaluated? Just would like to get some understanding of other options we have today.\n", "@gopigrip7 if you have a specific question, please open a new bug or try on stack overflow. For the original question, I think we have a fused op which is indeed faster."]}, {"number": 4153, "title": "Why I get the slow result after Quantization tools on CPU?", "body": "here is my benchmark info (The model is inception-2015-12-05):\nI tensorflow/core/util/stat_summarizer.cc:262] 50 runs, avg 2880 ms, 1960 nodes defined 1664 nodes observed\n============ Top by duration =================\n  [start]  [first]    [avg]      [%]      [cdf%]          [Op]  [Name]\n  348.471  199.520  162.116   5.629%      5.629%    QuantizedConv2D conv_4/Conv2D_eightbit_quantized_conv\n  153.190  115.819  109.304   3.796%      9.425%    QuantizedConv2D conv_2/Conv2D_eightbit_quantized_conv\n  824.866   80.364   76.637   2.661%     12.086%    QuantizedConv2D mixed_3/conv/Conv2D_eightbit_quantized_conv\n   54.955   74.988   65.929   2.289%     14.375%    QuantizedConv2D conv_1/Conv2D_eightbit_quantized_conv\n 1594.582   51.488   52.699   1.830%     16.205%    QuantizedConv2D mixed_10/tower_1/conv_1/Conv2D_eightbit_quantized_conv\n 1464.195   48.764   48.403   1.681%     17.886%    QuantizedConv2D mixed_9/tower_1/conv_1/Conv2D_eightbit_quantized_conv\n 1553.345   40.343   42.581   1.479%     19.365%    QuantizedConv2D mixed_10/tower_1/conv/Conv2D_eightbit_quantized_conv\n  588.883   36.744   39.660   1.377%     20.742%    QuantizedConv2D mixed/tower/conv_1/Conv2D_eightbit_quantized_conv\n 1553.350   45.938   38.929   1.352%     22.094%    QuantizedConv2D mixed_10/tower/conv/Conv2D_eightbit_quantized_conv\n  672.714   33.140   35.053   1.217%     23.311%    QuantizedConv2D mixed_1/tower/conv_1/Conv2D_eightbit_quantized_conv\n 1210.702   28.586   34.865   1.211%     24.521%    QuantizedConv2D mixed_7/conv/Conv2D_eightbit_quantized_conv\n  757.103   37.392   33.253   1.155%     25.676%    QuantizedConv2D mixed_2/tower/conv_1/Conv2D_eightbit_quantized_conv\n  986.100   28.534   33.197   1.153%     26.829%    QuantizedConv2D mixed_5/conv/Conv2D_eightbit_quantized_conv\n 1210.706   33.822   32.936   1.144%     27.973%    QuantizedConv2D mixed_7/tower/conv/Conv2D_eightbit_quantized_conv\n 1553.349   44.802   32.891   1.142%     29.115%    QuantizedConv2D mixed_10/conv/Conv2D_eightbit_quantized_conv\n 1210.701   35.172   32.204   1.118%     30.233%    QuantizedConv2D mixed_7/tower_1/conv/Conv2D_eightbit_quantized_conv\n 1090.376   37.017   31.968   1.110%     31.343%    QuantizedConv2D mixed_6/conv/Conv2D_eightbit_quantized_conv\n 1213.365   32.057   31.738   1.102%     32.445%    QuantizedConv2D mixed_7/tower_2/conv/Conv2D_eightbit_quantized_conv\n  849.401   44.313   31.509   1.094%     33.539%    QuantizedConv2D mixed_3/tower/conv_1/Conv2D_eightbit_quantized_conv\n  589.508   34.897   30.761   1.068%     34.607%    QuantizedConv2D mixed/tower_1/conv_1/Conv2D_eightbit_quantized_conv\n  908.076   26.733   30.611   1.063%     35.670%    QuantizedConv2D mixed_4/conv/Conv2D_eightbit_quantized_conv\n 1093.255   34.497   29.926   1.039%     36.709%    QuantizedConv2D mixed_6/tower_2/conv/Conv2D_eightbit_quantized_conv\n  988.679   27.107   29.897   1.038%     37.748%    QuantizedConv2D mixed_5/tower_2/conv/Conv2D_eightbit_quantized_conv\n 1090.377   34.881   29.526   1.025%     38.773%    QuantizedConv2D mixed_6/tower/conv/Conv2D_eightbit_quantized_conv\n  986.103   29.144   29.099   1.010%     39.783%    QuantizedConv2D mixed_5/tower/conv/Conv2D_eightbit_quantized_conv\n  911.030   32.084   28.822   1.001%     40.784%    QuantizedConv2D mixed_4/tower_2/conv/Conv2D_eightbit_quantized_conv\n  628.323   22.784   28.299   0.983%     41.767%    QuantizedConv2D mixed/tower_1/conv_2/Conv2D_eightbit_quantized_conv\n  682.759   24.791   28.184   0.979%     42.745%    QuantizedConv2D mixed_1/tower_1/conv_1/Conv2D_eightbit_quantized_conv\n 1090.373   37.104   27.889   0.968%     43.714%    QuantizedConv2D mixed_6/tower_1/conv/Conv2D_eightbit_quantized_conv\n  986.098   28.129   27.769   0.964%     44.678%    QuantizedConv2D mixed_5/tower_1/conv/Conv2D_eightbit_quantized_conv\n 1554.331   35.148   26.506   0.920%     45.599%    QuantizedConv2D mixed_10/tower_2/conv/Conv2D_eightbit_quantized_conv\n 1246.348   28.499   26.234   0.911%     46.510%    QuantizedConv2D mixed_7/tower/conv_1/Conv2D_eightbit_quantized_conv\n  908.072   32.080   26.203   0.910%     47.419%    QuantizedConv2D mixed_4/tower/conv/Conv2D_eightbit_quantized_conv\n  274.045   35.524   25.959   0.901%     48.321%    QuantizedBatchNormWithGlobalNormalization   conv_2/batchnorm_eightbit_quantized_batch_norm\n  757.224   38.495   25.621   0.890%     49.210%    QuantizedConv2D mixed_2/tower_1/conv_1/Conv2D_eightbit_quantized_conv\n 1670.984   18.898   25.575   0.888%     50.099%    QuantizedMatMul softmax/logits/MatMul_eightbit_quantized_bias_add\n  908.072   19.688   25.503   0.886%     50.984%    QuantizedConv2D mixed_4/tower_1/conv/Conv2D_eightbit_quantized_conv\n 1431.550   31.564   25.326   0.879%     51.864%    QuantizedConv2D mixed_9/tower_1/conv/Conv2D_eightbit_quantized_conv\n  798.592   22.487   25.139   0.873%     52.736%    QuantizedConv2D mixed_2/tower_1/conv_2/Conv2D_eightbit_quantized_conv\n  710.682   28.952   25.041   0.870%     53.606%    QuantizedConv2D mixed_1/tower_1/conv_2/Conv2D_eightbit_quantized_conv\n 1249.958   26.722   24.774   0.860%     54.466%    QuantizedConv2D mixed_7/tower_1/conv_1/Conv2D_eightbit_quantized_conv\n 1276.259   24.995   24.427   0.848%     55.315%    QuantizedConv2D mixed_7/tower/conv_2/Conv2D_eightbit_quantized_conv\n 1289.982   17.597   23.551   0.818%     56.132%    QuantizedConv2D mixed_7/tower_1/conv_2/Conv2D_eightbit_quantized_conv\n 1431.555   18.537   22.818   0.792%     56.925%    QuantizedConv2D mixed_9/tower/conv/Conv2D_eightbit_quantized_conv\n  824.860   21.110   22.180   0.770%     57.695%    QuantizedConv2D mixed_3/tower/conv/Conv2D_eightbit_quantized_conv\n 1646.740   22.990   21.676   0.753%     58.448%    QuantizedConv2D mixed_10/tower_1/mixed/conv_1/Conv2D_eightbit_quantized_conv\n 1646.744   22.860   21.567   0.749%     59.196%    QuantizedConv2D mixed_10/tower_1/mixed/conv/Conv2D_eightbit_quantized_conv\n 1371.468   20.181   21.385   0.743%     59.939%    QuantizedConv2D mixed_8/tower/conv_1/Conv2D_eightbit_quantized_conv\n 1372.601   23.081   20.983   0.729%     60.668%    QuantizedConv2D mixed_8/tower_1/conv_1/Conv2D_eightbit_quantized_conv\n 1043.432   26.873   20.580   0.715%     61.382%    QuantizedConv2D mixed_5/tower/conv_2/Conv2D_eightbit_quantized_conv\n 1149.722   16.327   20.291   0.705%     62.087%    QuantizedConv2D mixed_6/tower/conv_2/Conv2D_eightbit_quantized_conv\n 1600.192   25.630   19.435   0.675%     62.762%    QuantizedConv2D mixed_10/tower/mixed/conv/Conv2D_eightbit_quantized_conv\n 1017.888   21.327   19.281   0.670%     63.431%    QuantizedConv2D mixed_5/tower/conv_1/Conv2D_eightbit_quantized_conv\n 1308.978   18.376   19.117   0.664%     64.095%    QuantizedConv2D mixed_7/tower_1/conv_3/Conv2D_eightbit_quantized_conv\n 1128.559   19.422   18.884   0.656%     64.751%    QuantizedConv2D mixed_6/tower/conv_1/Conv2D_eightbit_quantized_conv\n 1513.946   38.340   18.755   0.651%     65.402%    QuantizedConv2D mixed_9/tower_1/mixed/conv/Conv2D_eightbit_quantized_conv\n 1600.189   29.234   18.674   0.648%     66.050%    QuantizedConv2D mixed_10/tower/mixed/conv_1/Conv2D_eightbit_quantized_conv\n 1397.119   19.557   18.436   0.640%     66.691%    QuantizedConv2D mixed_8/tower_1/conv_2/Conv2D_eightbit_quantized_conv\n 1015.906   20.308   18.426   0.640%     67.330%    QuantizedConv2D mixed_5/tower_1/conv_1/Conv2D_eightbit_quantized_conv\n 1135.983   14.258   18.416   0.639%     67.970%    QuantizedConv2D mixed_6/tower_1/conv_1/Conv2D_eightbit_quantized_conv\n 1450.805   29.566   18.226   0.633%     68.603%    QuantizedConv2D mixed_9/tower/mixed/conv_1/Conv2D_eightbit_quantized_conv\n 1328.743   22.393   18.113   0.629%     69.232%    QuantizedConv2D mixed_7/tower_1/conv_4/Conv2D_eightbit_quantized_conv\n 1450.809   28.226   17.926   0.622%     69.854%    QuantizedConv2D mixed_9/tower/mixed/conv/Conv2D_eightbit_quantized_conv\n  743.108   12.778   17.708   0.615%     70.469%    QuantizedConv2D mixed_2/conv/Conv2D_eightbit_quantized_conv\n  551.000   18.514   17.690   0.614%     71.083%    QuantizedBatchNormWithGlobalNormalization   conv_4/batchnorm_eightbit_quantized_batch_norm\n  743.106   11.945   17.686   0.614%     71.698%    QuantizedConv2D mixed_2/tower_1/conv/Conv2D_eightbit_quantized_conv\n 1158.867   17.448   17.553   0.610%     72.307%    QuantizedConv2D mixed_6/tower_1/conv_2/Conv2D_eightbit_quantized_conv\n 1431.553   15.412   17.497   0.608%     72.915%    QuantizedConv2D mixed_9/conv/Conv2D_eightbit_quantized_conv\n  654.795   18.570   17.457   0.606%     73.521%    QuantizedConv2D mixed_1/tower_1/conv/Conv2D_eightbit_quantized_conv\n 1513.932   16.375   17.418   0.605%     74.126%    QuantizedConv2D mixed_9/tower_1/mixed/conv_1/Conv2D_eightbit_quantized_conv\n  654.796   16.350   16.927   0.588%     74.713%    QuantizedConv2D mixed_1/conv/Conv2D_eightbit_quantized_conv\n 1037.406   21.229   16.914   0.587%     75.301%    QuantizedConv2D mixed_5/tower_1/conv_2/Conv2D_eightbit_quantized_conv\n  743.109   12.459   16.895   0.587%     75.887%    QuantizedConv2D mixed_2/tower/conv/Conv2D_eightbit_quantized_conv\n 1352.871   17.220   16.892   0.587%     76.474%    QuantizedConv2D mixed_8/tower/conv/Conv2D_eightbit_quantized_conv\n 1352.867   18.301   16.870   0.586%     77.060%    QuantizedConv2D mixed_8/tower_1/conv/Conv2D_eightbit_quantized_conv\n 1432.532   22.308   16.561   0.575%     77.635%    QuantizedConv2D mixed_9/tower_2/conv/Conv2D_eightbit_quantized_conv\n  952.475   12.333   16.346   0.568%     78.202%    QuantizedConv2D mixed_4/tower/conv_2/Conv2D_eightbit_quantized_conv\n  654.800   16.098   16.232   0.564%     78.766%    QuantizedConv2D mixed_1/tower/conv/Conv2D_eightbit_quantized_conv\n 1191.637   17.268   15.125   0.525%     79.291%    QuantizedConv2D mixed_6/tower_1/conv_4/Conv2D_eightbit_quantized_conv\n 1073.325   15.302   14.567   0.506%     79.797%    QuantizedConv2D mixed_5/tower_1/conv_4/Conv2D_eightbit_quantized_conv\n   27.923   23.420   14.489   0.503%     80.300%    QuantizedBatchNormWithGlobalNormalization   conv/batchnorm_eightbit_quantized_batch_norm\n  658.731   11.937   13.903   0.483%     80.783%    QuantizedConv2D mixed_1/tower_2/conv/Conv2D_eightbit_quantized_conv\n  757.306   25.541   13.874   0.482%     81.265%    QuantizedConv2D mixed_2/tower_2/conv/Conv2D_eightbit_quantized_conv\n 1177.505   12.944   13.679   0.475%     81.740%    QuantizedConv2D mixed_6/tower_1/conv_3/Conv2D_eightbit_quantized_conv\n  574.255   13.735   13.379   0.465%     82.204%    QuantizedConv2D mixed/conv/Conv2D_eightbit_quantized_conv\n  574.230   11.661   13.310   0.462%     82.667%    QuantizedConv2D mixed/tower_1/conv/Conv2D_eightbit_quantized_conv\n 1060.333   11.805   13.287   0.461%     83.128%    QuantizedConv2D mixed_5/tower_1/conv_3/Conv2D_eightbit_quantized_conv\n  132.755   17.705   13.125   0.456%     83.584%    QuantizedBatchNormWithGlobalNormalization   conv_1/batchnorm_eightbit_quantized_batch_norm\n  971.865   12.488   12.591   0.437%     84.021%    QuantizedConv2D mixed_4/tower_1/conv_4/Conv2D_eightbit_quantized_conv\n  574.298   12.449   12.557   0.436%     84.457%    QuantizedConv2D mixed/tower/conv/Conv2D_eightbit_quantized_conv\n  946.591   15.766   12.203   0.424%     84.881%    QuantizedConv2D mixed_4/tower_1/conv_2/Conv2D_eightbit_quantized_conv\n  941.124   10.019   12.132   0.421%     85.302%    QuantizedConv2D mixed_4/tower/conv_1/Conv2D_eightbit_quantized_conv\n  930.408   14.499   12.010   0.417%     85.719%    QuantizedConv2D mixed_4/tower_1/conv_1/Conv2D_eightbit_quantized_conv\n 1418.545   12.330   10.408   0.361%     86.080%    QuantizedConv2D mixed_8/tower_1/conv_3/Conv2D_eightbit_quantized_conv\n  319.410   10.155    9.309   0.323%     86.404%    QuantizedConv2D conv_3/Conv2D_eightbit_quantized_conv\n  963.651    7.257    8.809   0.306%     86.709%    QuantizedConv2D mixed_4/tower_1/conv_3/Conv2D_eightbit_quantized_conv\n  331.914   14.339    8.533   0.296%     87.006%    QuantizedBatchNormWithGlobalNormalization   conv_3/batchnorm_eightbit_quantized_batch_norm\n    7.234   16.748    8.518   0.296%     87.302%    QuantizedConv2D conv/Conv2D_eightbit_quantized_conv\n  578.657    7.823    8.201   0.285%     87.586%    QuantizedConv2D mixed/tower_2/conv/Conv2D_eightbit_quantized_conv\n  590.466    8.945    7.803   0.271%     87.857%    QuantizeDownAndShrinkRange  mixed/conv/batchnorm_eightbit_quantize_down\n  758.138   11.852    7.449   0.259%     88.116%    QuantizeDownAndShrinkRange  mixed_2/conv/batchnorm_eightbit_quantize_down\n  896.657    7.011    7.431   0.258%     88.374%    QuantizedConv2D mixed_3/tower/conv_2/Conv2D_eightbit_quantized_conv\n  654.795    3.927    5.455   0.189%     88.563%    QuantizedAvgPool    mixed_1/tower_2/pool_eightbit_quantized\n  743.107   14.192    5.226   0.181%     88.745%    QuantizedAvgPool    mixed_2/tower_2/pool_eightbit_quantized\n  784.526    0.270    5.046   0.175%     88.920%    QuantizeDownAndShrinkRange  mixed_2/tower_2/conv/batchnorm_eightbit_quantize_down\n  673.545    0.263    4.586   0.159%     89.079%    QuantizeDownAndShrinkRange  mixed_1/conv/batchnorm_eightbit_quantize_down\n  845.982    0.646    4.385   0.152%     89.232%    QuantizeDownAndShrinkRange  mixed_3/tower/conv/Conv2D_eightbit_quantize_down\n  673.023    0.279    4.126   0.143%     89.375%    QuantizeDownAndShrinkRange  mixed_1/tower_2/conv/batchnorm_eightbit_quantize_down\n  269.022    5.018    4.039   0.140%     89.515%    QuantizeDownAndShrinkRange  conv_2/Conv2D_eightbit_quantize_down\n  574.249    4.398    3.825   0.133%     89.648%    QuantizedAvgPool    mixed/tower_2/pool_eightbit_quantized\n 1127.402    0.210    3.734   0.130%     89.778%    QuantizeDownAndShrinkRange  mixed_6/conv/Conv2D_eightbit_quantize_down\n  309.582    4.882    3.708   0.129%     89.906%    QuantizeDownAndShrinkRange  conv_2/batchnorm_eightbit_quantize_down\n 1246.651    0.211    3.177   0.110%     90.017%    QuantizeDownAndShrinkRange  mixed_7/tower_2/conv/batchnorm_eightbit_quantize_down\n 1239.296    2.590    3.160   0.110%     90.126%    QuantizeDownAndShrinkRange  mixed_7/conv/Conv2D_eightbit_quantize_down\n 1015.791    0.184    3.154   0.110%     90.236%    QuantizeDownAndShrinkRange  mixed_5/tower_2/conv/Conv2D_eightbit_quantize_down\n  589.067    0.368    3.109   0.108%     90.344%    QuantizeDownAndShrinkRange  mixed/tower_1/conv/batchnorm_eightbit_quantize_down\n 1245.428    0.211    3.066   0.106%     90.450%    QuantizeDownAndShrinkRange  mixed_7/tower_2/conv/Conv2D_eightbit_quantize_down\n 1243.366    0.220    2.937   0.102%     90.552%    QuantizeDownAndShrinkRange  mixed_7/conv/batchnorm_eightbit_quantize_down\n  314.719    4.671    2.925   0.102%     90.654%    QuantizedMaxPool    pool_eightbit_quantized\n  548.004    2.990    2.676   0.093%     90.747%    QuantizeDownAndShrinkRange  conv_4/Conv2D_eightbit_quantize_down\n  986.100    2.571    2.643   0.092%     90.839%    QuantizedAvgPool    mixed_5/tower_2/pool_eightbit_quantized\n  624.922    2.845    2.627   0.091%     90.930%    QuantizedBatchNormWithGlobalNormalization   mixed/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm\n 1210.698    2.659    2.564   0.089%     91.019%    QuantizedAvgPool    mixed_7/tower_2/pool_eightbit_quantized\n  569.522    2.728    2.518   0.087%     91.106%    QuantizeDownAndShrinkRange  conv_4/batchnorm_eightbit_quantize_down\n 1014.642    0.200    2.483   0.086%     91.192%    QuantizeDownAndShrinkRange  mixed_5/conv/Conv2D_eightbit_quantize_down\n  908.075    2.947    2.448   0.085%     91.277%    QuantizedAvgPool    mixed_4/tower_2/pool_eightbit_quantized\n   24.010    3.897    2.405   0.084%     91.361%    QuantizeDownAndShrinkRange  conv/Conv2D_eightbit_quantize_down\n 1090.373    2.874    2.401   0.083%     91.444%    QuantizedAvgPool    mixed_6/tower_2/pool_eightbit_quantized\n  794.763    1.403    2.383   0.083%     91.527%    QuantizedBatchNormWithGlobalNormalization   mixed_2/tower/conv_1/batchnorm_eightbit_quantized_batch_norm\n 1371.251    0.178    2.374   0.082%     91.610%    QuantizeDownAndShrinkRange  mixed_8/tower/conv/batchnorm_eightbit_quantize_down\n  651.526    2.474    2.333   0.081%     91.691%    QuantizedBatchNormWithGlobalNormalization   mixed/tower_1/conv_2/batchnorm_eightbit_quantized_batch_norm\n  707.929    2.311    2.318   0.081%     91.771%    QuantizedBatchNormWithGlobalNormalization   mixed_1/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm\n 1129.115    5.158    2.287   0.079%     91.850%    QuantizeDownAndShrinkRange  mixed_6/conv/batchnorm_eightbit_quantize_down\n  939.397    0.211    2.285   0.079%     91.930%    QuantizeDownAndShrinkRange  mixed_4/conv/batchnorm_eightbit_quantize_down\n   51.354    3.404    2.267   0.079%     92.009%    QuantizeDownAndShrinkRange  conv/batchnorm_eightbit_quantize_down\n  676.107    6.540    2.243   0.078%     92.086%    QuantizeDownAndShrinkRange  mixed_1/tower_1/conv/batchnorm_eightbit_quantize_down\n  894.118    2.012    2.242   0.078%     92.164%    QuantizedBatchNormWithGlobalNormalization   mixed_3/tower/conv_1/batchnorm_eightbit_quantized_batch_norm\n  947.078    3.976    2.233   0.078%     92.242%    QuantizeDownAndShrinkRange  mixed_4/tower_2/conv/batchnorm_eightbit_quantize_down\n  796.073    2.036    2.229   0.077%     92.319%    QuantizedBatchNormWithGlobalNormalization   mixed_2/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm\n  755.892    0.239    2.216   0.077%     92.396%    QuantizeDownAndShrinkRange  mixed_2/conv/Conv2D_eightbit_quantize_down\n 1016.312    0.250    2.215   0.077%     92.473%    QuantizeDownAndShrinkRange  mixed_5/conv/batchnorm_eightbit_quantize_down\n  707.511    0.252    2.187   0.076%     92.549%    QuantizeDownAndShrinkRange  mixed_1/tower/conv_1/batchnorm_eightbit_quantize_down\n 1129.453    4.033    2.144   0.074%     92.624%    QuantizeDownAndShrinkRange  mixed_6/tower_2/conv/batchnorm_eightbit_quantize_down\n  670.963    2.054    2.113   0.073%     92.697%    QuantizedBatchNormWithGlobalNormalization   mixed_1/tower_2/conv/batchnorm_eightbit_quantized_batch_norm\n  588.324    2.136    2.103   0.073%     92.770%    QuantizedBatchNormWithGlobalNormalization   mixed/conv/batchnorm_eightbit_quantized_batch_norm\n  756.135    1.997    2.080   0.072%     92.842%    QuantizedBatchNormWithGlobalNormalization   mixed_2/conv/batchnorm_eightbit_quantized_batch_norm\n  756.893    0.281    2.072   0.072%     92.914%    QuantizeDownAndShrinkRange  mixed_2/tower_1/conv/batchnorm_eightbit_quantize_down\n  740.059    2.084    2.069   0.072%     92.986%    QuantizedBatchNormWithGlobalNormalization   mixed_1/tower_1/conv_2/batchnorm_eightbit_quantized_batch_norm\n  821.437    2.547    2.067   0.072%     93.058%    QuantizedBatchNormWithGlobalNormalization   mixed_2/tower_1/conv_2/batchnorm_eightbit_quantized_batch_norm\n 1017.169    0.408    2.062   0.072%     93.129%    QuantizeDownAndShrinkRange  mixed_5/tower_2/conv/batchnorm_eightbit_quantize_down\n  625.635    0.293    2.044   0.071%     93.200%    QuantizeDownAndShrinkRange  mixed/tower/conv_1/Conv2D_eightbit_quantize_down\n  129.959    2.789    2.007   0.070%     93.270%    QuantizeDownAndShrinkRange  conv_1/Conv2D_eightbit_quantize_down\n  905.567    1.910    1.991   0.069%     93.339%    QuantizedBatchNormWithGlobalNormalization   mixed_3/conv/batchnorm_eightbit_quantized_batch_norm\n 1301.262    0.225    1.967   0.068%     93.407%    QuantizeDownAndShrinkRange  mixed_7/tower/conv_2/Conv2D_eightbit_quantize_down\n  846.632    2.042    1.963   0.068%     93.476%    QuantizedBatchNormWithGlobalNormalization   mixed_3/tower/conv/batchnorm_eightbit_quantized_batch_norm\n  625.932    1.353    1.955   0.068%     93.543%    QuantizedBatchNormWithGlobalNormalization   mixed/tower/conv_1/batchnorm_eightbit_quantized_batch_norm\n  150.469    2.571    1.954   0.068%     93.611%    QuantizeDownAndShrinkRange  conv_1/batchnorm_eightbit_quantize_down\n  755.340    1.549    1.951   0.068%     93.679%    QuantizedBatchNormWithGlobalNormalization   mixed_2/tower_1/conv/batchnorm_eightbit_quantized_batch_norm\n  848.680    0.633    1.913   0.066%     93.745%    QuantizeDownAndShrinkRange  mixed_3/tower/conv/batchnorm_eightbit_quantize_down\n  673.656    2.444    1.904   0.066%     93.812%    QuantizedBatchNormWithGlobalNormalization   mixed_1/tower_1/conv/batchnorm_eightbit_quantized_batch_norm\n 1276.689    9.140    1.887   0.066%     93.877%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv_1/Conv2D_eightbit_quantize_down\n  586.394    2.666    1.884   0.065%     93.943%    QuantizedBatchNormWithGlobalNormalization   mixed/tower_1/conv/batchnorm_eightbit_quantized_batch_norm\n  572.418    1.798    1.769   0.061%     94.004%    QuantizedMaxPool    pool_1_eightbit_quantized\n  783.151    1.370    1.756   0.061%     94.065%    QuantizedBatchNormWithGlobalNormalization   mixed_2/tower_2/conv/batchnorm_eightbit_quantized_batch_norm\n  671.424    2.115    1.747   0.061%     94.126%    QuantizedBatchNormWithGlobalNormalization   mixed_1/conv/batchnorm_eightbit_quantized_batch_norm\n 1127.757    0.201    1.734   0.060%     94.186%    QuantizeDownAndShrinkRange  mixed_6/tower_2/conv/Conv2D_eightbit_quantize_down\n  706.134    1.372    1.714   0.060%     94.245%    QuantizedBatchNormWithGlobalNormalization   mixed_1/tower/conv_1/batchnorm_eightbit_quantized_batch_norm\n  934.819    3.298    1.690   0.059%     94.304%    QuantizeDownAndShrinkRange  mixed_4/conv/Conv2D_eightbit_quantize_down\n  945.612    1.458    1.690   0.059%     94.363%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower_2/conv/batchnorm_eightbit_quantized_batch_norm\n  586.995    1.576    1.690   0.059%     94.421%    QuantizedBatchNormWithGlobalNormalization   mixed/tower/conv/batchnorm_eightbit_quantized_batch_norm\n 1241.891    1.468    1.645   0.057%     94.479%    QuantizedBatchNormWithGlobalNormalization   mixed_7/conv/batchnorm_eightbit_quantized_batch_norm\n 1244.535    0.188    1.629   0.057%     94.535%    QuantizeDownAndShrinkRange  mixed_7/tower/conv/Conv2D_eightbit_quantize_down\n  794.502    0.257    1.624   0.056%     94.592%    QuantizeDownAndShrinkRange  mixed_2/tower/conv_1/Conv2D_eightbit_quantize_down\n 1247.636    2.254    1.475   0.051%     94.643%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv/batchnorm_eightbit_quantize_down\n  782.853    0.294    1.471   0.051%     94.694%    QuantizeDownAndShrinkRange  mixed_2/tower_2/conv/Conv2D_eightbit_quantize_down\n  796.173    0.242    1.458   0.051%     94.744%    QuantizeDownAndShrinkRange  mixed_2/tower/conv_1/batchnorm_eightbit_quantize_down\n 1014.845    1.460    1.450   0.050%     94.795%    QuantizedBatchNormWithGlobalNormalization   mixed_5/conv/batchnorm_eightbit_quantized_batch_norm\n 1245.882    0.292    1.438   0.050%     94.845%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv/Conv2D_eightbit_quantize_down\n 1245.641    1.005    1.438   0.050%     94.895%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower_2/conv/batchnorm_eightbit_quantized_batch_norm\n  329.577    2.330    1.415   0.049%     94.944%    QuantizeDownAndShrinkRange  conv_3/Conv2D_eightbit_quantize_down\n  755.816    1.034    1.409   0.049%     94.993%    QuantizedBatchNormWithGlobalNormalization   mixed_2/tower/conv/batchnorm_eightbit_quantized_batch_norm\n 1285.835    1.461    1.408   0.049%     95.042%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm\n  943.121    2.486    1.391   0.048%     95.090%    QuantizeDownAndShrinkRange  mixed_4/tower_2/conv/Conv2D_eightbit_quantize_down\n  671.166    1.264    1.389   0.048%     95.138%    QuantizedBatchNormWithGlobalNormalization   mixed_1/tower/conv/batchnorm_eightbit_quantized_batch_norm\n 1015.256    0.178    1.369   0.048%     95.186%    QuantizeDownAndShrinkRange  mixed_5/tower/conv/Conv2D_eightbit_quantize_down\n  938.122    1.264    1.356   0.047%     95.233%    QuantizedBatchNormWithGlobalNormalization   mixed_4/conv/batchnorm_eightbit_quantized_batch_norm\n 1015.978    1.184    1.350   0.047%     95.280%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower_2/conv/batchnorm_eightbit_quantized_batch_norm\n  627.290    0.280    1.347   0.047%     95.326%    QuantizeDownAndShrinkRange  mixed/tower/conv_1/batchnorm_eightbit_quantize_down\n  705.870    0.261    1.347   0.047%     95.373%    QuantizeDownAndShrinkRange  mixed_1/tower/conv_1/Conv2D_eightbit_quantize_down\n 1015.665    0.182    1.340   0.047%     95.420%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv/batchnorm_eightbit_quantize_down\n 1036.229    0.172    1.338   0.046%     95.466%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv_1/Conv2D_eightbit_quantize_down\n 1127.962    1.485    1.338   0.046%     95.513%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower_2/conv/batchnorm_eightbit_quantized_batch_norm\n 1127.616    1.493    1.329   0.046%     95.559%    QuantizedBatchNormWithGlobalNormalization   mixed_6/conv/batchnorm_eightbit_quantized_batch_norm\n 1301.492    1.461    1.324   0.046%     95.605%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower/conv_2/batchnorm_eightbit_quantized_batch_norm\n  346.260    2.064    1.319   0.046%     95.651%    QuantizeDownAndShrinkRange  conv_3/batchnorm_eightbit_quantize_down\n 1275.068    0.957    1.314   0.046%     95.696%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower/conv_1/batchnorm_eightbit_quantized_batch_norm\n  755.057    0.280    1.281   0.044%     95.741%    QuantizeDownAndShrinkRange  mixed_2/tower_1/conv/Conv2D_eightbit_quantize_down\n  673.372    0.281    1.278   0.044%     95.785%    QuantizeDownAndShrinkRange  mixed_1/tower_1/conv/Conv2D_eightbit_quantize_down\n  967.976    1.451    1.244   0.043%     95.828%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower/conv_2/batchnorm_eightbit_quantized_batch_norm\n 1244.726    1.385    1.237   0.043%     95.871%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower/conv/batchnorm_eightbit_quantized_batch_norm\n 1246.178    1.451    1.234   0.043%     95.914%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower_1/conv/batchnorm_eightbit_quantized_batch_norm\n 1176.319    0.165    1.228   0.043%     95.957%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv_2/Conv2D_eightbit_quantize_down\n  671.153    0.267    1.220   0.042%     95.999%    QuantizeDownAndShrinkRange  mixed_1/conv/Conv2D_eightbit_quantize_down\n 1166.058    0.416    1.216   0.042%     96.041%    QuantizeDownAndShrinkRange  mixed_6/tower/conv_2/Conv2D_eightbit_quantize_down\n 1166.479    1.325    1.195   0.041%     96.083%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower/conv_2/batchnorm_eightbit_quantized_batch_norm\n  927.772    1.473    1.193   0.041%     96.124%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv/Conv2D_eightbit_quantize_down\n 1070.499    0.956    1.190   0.041%     96.166%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower/conv_2/batchnorm_eightbit_quantized_batch_norm\n  586.682    1.051    1.169   0.041%     96.206%    QuantizedBatchNormWithGlobalNormalization   mixed/tower_2/conv/batchnorm_eightbit_quantized_batch_norm\n 1127.483    0.175    1.164   0.040%     96.247%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv/Conv2D_eightbit_quantize_down\n 1039.225    0.279    1.148   0.040%     96.286%    QuantizeDownAndShrinkRange  mixed_5/tower/conv_1/Conv2D_eightbit_quantize_down\n 1370.289    0.956    1.129   0.039%     96.326%    QuantizedBatchNormWithGlobalNormalization   mixed_8/tower/conv/batchnorm_eightbit_quantized_batch_norm\n  824.866    1.453    1.122   0.039%     96.365%    QuantizedMaxPool    mixed_3/pool_eightbit_quantized\n 1128.346    0.157    1.109   0.039%     96.403%    QuantizeDownAndShrinkRange  mixed_6/tower/conv/batchnorm_eightbit_quantize_down\n 1125.273    2.229    1.099   0.038%     96.441%    QuantizeDownAndShrinkRange  mixed_6/tower/conv/Conv2D_eightbit_quantize_down\n  588.578    0.230    1.098   0.038%     96.479%    QuantizeDownAndShrinkRange  mixed/tower/conv/batchnorm_eightbit_quantize_down\n  587.738    0.192    1.096   0.038%     96.517%    QuantizeDownAndShrinkRange  mixed/tower_2/conv/batchnorm_eightbit_quantize_down\n 1036.406    0.799    1.094   0.038%     96.555%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm\n 1148.374    1.069    1.091   0.038%     96.593%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower/conv_1/batchnorm_eightbit_quantized_batch_norm\n 1371.398    0.965    1.091   0.038%     96.631%    QuantizedBatchNormWithGlobalNormalization   mixed_8/tower_1/conv/batchnorm_eightbit_quantized_batch_norm\n 1287.305    2.608    1.086   0.038%     96.669%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv_1/batchnorm_eightbit_quantize_down\n  672.436    0.234    1.086   0.038%     96.707%    QuantizeDownAndShrinkRange  mixed_1/tower/conv/batchnorm_eightbit_quantize_down\n 1014.450    1.208    1.083   0.038%     96.744%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower_1/conv/batchnorm_eightbit_quantized_batch_norm\n 1395.893    0.969    1.073   0.037%     96.782%    QuantizedBatchNormWithGlobalNormalization   mixed_8/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm\n 1307.784    0.954    1.062   0.037%     96.818%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower_1/conv_2/batchnorm_eightbit_quantized_batch_norm\n 1015.437    1.117    1.052   0.037%     96.855%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower/conv/batchnorm_eightbit_quantized_batch_norm\n 1127.663    1.496    1.031   0.036%     96.891%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower_1/conv/batchnorm_eightbit_quantized_batch_norm\n  944.914    0.606    1.026   0.036%     96.926%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv_1/Conv2D_eightbit_quantize_down\n 1416.925    1.304    1.026   0.036%     96.962%    QuantizedBatchNormWithGlobalNormalization   mixed_8/tower_1/conv_2/batchnorm_eightbit_quantized_batch_norm\n 1351.363    0.955    1.024   0.036%     96.997%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower_1/conv_4/batchnorm_eightbit_quantized_batch_norm\n 1070.312    0.183    1.023   0.036%     97.033%    QuantizeDownAndShrinkRange  mixed_5/tower/conv_2/Conv2D_eightbit_quantize_down\n 1327.551    0.954    1.015   0.035%     97.068%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower_1/conv_3/batchnorm_eightbit_quantized_batch_norm\n 1039.509    1.049    1.012   0.035%     97.103%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower/conv_1/batchnorm_eightbit_quantized_batch_norm\n  670.675    0.284    1.010   0.035%     97.138%    QuantizeDownAndShrinkRange  mixed_1/tower_2/conv/Conv2D_eightbit_quantize_down\n 1127.506    0.836    1.006   0.035%     97.173%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower/conv/batchnorm_eightbit_quantized_batch_norm\n  964.815    3.157    1.006   0.035%     97.208%    QuantizeDownAndShrinkRange  mixed_4/tower/conv_2/Conv2D_eightbit_quantize_down\n  587.998    0.322    0.999   0.035%     97.243%    QuantizeDownAndShrinkRange  mixed/conv/Conv2D_eightbit_quantize_down\n 1209.109    0.959    0.997   0.035%     97.278%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower_1/conv_4/batchnorm_eightbit_quantized_batch_norm\n  929.251    0.883    0.995   0.035%     97.312%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower_1/conv/batchnorm_eightbit_quantized_batch_norm\n 1088.848    0.952    0.993   0.034%     97.347%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower_1/conv_4/batchnorm_eightbit_quantized_batch_norm\n 1391.652    0.090    0.992   0.034%     97.381%    QuantizeDownAndShrinkRange  mixed_8/tower/conv_1/Conv2D_eightbit_quantize_down\n 1246.118    0.192    0.975   0.034%     97.415%    QuantizeDownAndShrinkRange  mixed_7/tower/conv/batchnorm_eightbit_quantize_down\n  984.569    0.956    0.959   0.033%     97.448%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower_1/conv_4/batchnorm_eightbit_quantized_batch_norm\n 1157.845    0.806    0.957   0.033%     97.482%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm\n 1150.248    7.594    0.951   0.033%     97.515%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv_1/Conv2D_eightbit_quantize_down\n 1058.866    1.213    0.949   0.033%     97.547%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower_1/conv_2/batchnorm_eightbit_quantized_batch_norm\n 1274.860    0.205    0.943   0.033%     97.580%    QuantizeDownAndShrinkRange  mixed_7/tower/conv_1/Conv2D_eightbit_quantize_down\n 1040.567    2.812    0.930   0.032%     97.613%    QuantizeDownAndShrinkRange  mixed_5/tower/conv_1/batchnorm_eightbit_quantize_down\n 1148.006    0.364    0.925   0.032%     97.645%    QuantizeDownAndShrinkRange  mixed_6/tower/conv_1/Conv2D_eightbit_quantize_down\n 1129.167    6.752    0.919   0.032%     97.677%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv/batchnorm_eightbit_quantize_down\n 1014.241    0.205    0.912   0.032%     97.708%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv/Conv2D_eightbit_quantize_down\n 1176.487    0.796    0.902   0.031%     97.740%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower_1/conv_2/batchnorm_eightbit_quantized_batch_norm\n  969.433    0.211    0.894   0.031%     97.771%    QuantizeDownAndShrinkRange  mixed_4/tower/conv_2/batchnorm_eightbit_quantize_down\n 1071.460    0.184    0.863   0.030%     97.801%    QuantizeDownAndShrinkRange  mixed_5/tower/conv_2/batchnorm_eightbit_quantize_down\n  940.305    0.641    0.854   0.030%     97.830%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower/conv/batchnorm_eightbit_quantized_batch_norm\n 1190.636    0.795    0.853   0.030%     97.860%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower_1/conv_3/batchnorm_eightbit_quantized_batch_norm\n 1058.651    0.211    0.833   0.029%     97.889%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv_2/Conv2D_eightbit_quantize_down\n 1149.454    0.211    0.827   0.029%     97.917%    QuantizeDownAndShrinkRange  mixed_6/tower/conv_1/batchnorm_eightbit_quantize_down\n  951.307    0.975    0.824   0.029%     97.946%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower/conv_1/batchnorm_eightbit_quantized_batch_norm\n 1072.316    0.795    0.823   0.029%     97.975%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower_1/conv_3/batchnorm_eightbit_quantized_batch_norm\n  756.856    0.200    0.821   0.029%     98.003%    QuantizeDownAndShrinkRange  mixed_2/tower/conv/batchnorm_eightbit_quantize_down\n 1167.811    0.208    0.821   0.029%     98.032%    QuantizeDownAndShrinkRange  mixed_6/tower/conv_2/batchnorm_eightbit_quantize_down\n 1370.098    0.187    0.819   0.028%     98.060%    QuantizeDownAndShrinkRange  mixed_8/tower/conv/Conv2D_eightbit_quantize_down\n 1553.349    0.971    0.810   0.028%     98.088%    QuantizedMaxPool    mixed_10/tower_2/pool_eightbit_quantized\n 1016.560    1.268    0.810   0.028%     98.116%    QuantizeDownAndShrinkRange  mixed_5/tower/conv/batchnorm_eightbit_quantize_down\n  795.726    0.341    0.803   0.028%     98.144%    QuantizeDownAndShrinkRange  mixed_2/tower_1/conv_1/Conv2D_eightbit_quantize_down\n  951.149    0.154    0.789   0.027%     98.172%    QuantizeDownAndShrinkRange  mixed_4/tower/conv_1/Conv2D_eightbit_quantize_down\n  945.523    0.884    0.789   0.027%     98.199%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm\n  940.952    0.143    0.747   0.026%     98.225%    QuantizeDownAndShrinkRange  mixed_4/tower/conv/batchnorm_eightbit_quantize_down\n  962.594    0.873    0.730   0.025%     98.250%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower_1/conv_2/batchnorm_eightbit_quantized_batch_norm\n  952.288    0.155    0.728   0.025%     98.276%    QuantizeDownAndShrinkRange  mixed_4/tower/conv_1/batchnorm_eightbit_quantize_down\n 1276.031    0.188    0.705   0.024%     98.300%    QuantizeDownAndShrinkRange  mixed_7/tower/conv_1/batchnorm_eightbit_quantize_down\n  962.364    0.227    0.690   0.024%     98.324%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv_2/Conv2D_eightbit_quantize_down\n  586.757    0.233    0.679   0.024%     98.348%    QuantizeDownAndShrinkRange  mixed/tower/conv/Conv2D_eightbit_quantize_down\n  971.048    0.638    0.660   0.023%     98.370%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower_1/conv_3/batchnorm_eightbit_quantized_batch_norm\n 1431.550    0.976    0.658   0.023%     98.393%    QuantizedAvgPool    mixed_9/tower_2/pool_eightbit_quantized\n 1463.275    0.709    0.650   0.023%     98.416%    QuantizedBatchNormWithGlobalNormalization   mixed_9/tower_1/conv/batchnorm_eightbit_quantized_batch_norm\n  940.158    0.142    0.634   0.022%     98.438%    QuantizeDownAndShrinkRange  mixed_4/tower/conv/Conv2D_eightbit_quantize_down\n 1593.936    0.467    0.619   0.022%     98.459%    QuantizedBatchNormWithGlobalNormalization   mixed_10/tower_1/conv/batchnorm_eightbit_quantized_batch_norm\n  585.913    0.475    0.611   0.021%     98.481%    QuantizeDownAndShrinkRange  mixed/tower_1/conv/Conv2D_eightbit_quantize_down\n 1302.961    0.304    0.578   0.020%     98.501%    QuantizeDownAndShrinkRange  mixed_7/tower/conv_2/batchnorm_eightbit_quantize_down\n 1392.113    0.097    0.569   0.020%     98.520%    QuantizeDownAndShrinkRange  mixed_8/tower/conv_1/batchnorm_eightbit_quantize_down\n  903.803    0.748    0.541   0.019%     98.539%    QuantizedBatchNormWithGlobalNormalization   mixed_3/tower/conv_2/batchnorm_eightbit_quantized_batch_norm\n 1669.749    0.458    0.540   0.019%     98.558%    QuantizedBatchNormWithGlobalNormalization   mixed_10/tower_1/mixed/conv/batchnorm_eightbit_quantized_batch_norm\n  707.555    0.370    0.540   0.019%     98.577%    QuantizeDownAndShrinkRange  mixed_1/tower_1/conv_1/Conv2D_eightbit_quantize_down\n  907.481    0.346    0.529   0.018%     98.595%    QuantizeDownAndShrinkRange  mixed_3/conv/batchnorm_eightbit_quantize_down\n  930.142    0.220    0.529   0.018%     98.614%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv/batchnorm_eightbit_quantize_down\n 1629.581    0.498    0.518   0.018%     98.632%    QuantizedBatchNormWithGlobalNormalization   mixed_10/tower/mixed/conv_1/batchnorm_eightbit_quantized_batch_norm\n  624.420    0.495    0.516   0.018%     98.649%    QuantizeDownAndShrinkRange  mixed/tower_1/conv_1/Conv2D_eightbit_quantize_down\n  654.004    0.422    0.513   0.018%     98.667%    QuantizeDownAndShrinkRange  mixed/tower_1/conv_2/batchnorm_eightbit_quantize_down\n 1450.225    0.441    0.508   0.018%     98.685%    QuantizedBatchNormWithGlobalNormalization   mixed_9/tower/conv/batchnorm_eightbit_quantized_batch_norm\n 1625.984    0.441    0.504   0.018%     98.702%    QuantizedBatchNormWithGlobalNormalization   mixed_10/tower/mixed/conv/batchnorm_eightbit_quantized_batch_norm\n 1479.160    0.426    0.501   0.017%     98.720%    QuantizedBatchNormWithGlobalNormalization   mixed_9/tower/mixed/conv/batchnorm_eightbit_quantized_batch_norm\n 1552.490    0.405    0.499   0.017%     98.737%    QuantizedBatchNormWithGlobalNormalization   mixed_9/tower_1/mixed/conv/batchnorm_eightbit_quantized_batch_norm\n 1599.427    0.593    0.497   0.017%     98.754%    QuantizedBatchNormWithGlobalNormalization   mixed_10/tower/conv/batchnorm_eightbit_quantized_batch_norm\n 1158.655    0.177    0.496   0.017%     98.772%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv_1/batchnorm_eightbit_quantize_down\n 1480.531    0.649    0.495   0.017%     98.789%    QuantizedBatchNormWithGlobalNormalization   mixed_9/tower/mixed/conv_1/batchnorm_eightbit_quantized_batch_norm\n 1530.484    0.612    0.484   0.017%     98.806%    QuantizedBatchNormWithGlobalNormalization   mixed_9/tower_1/mixed/conv_1/batchnorm_eightbit_quantized_batch_norm\n 1447.088    0.558    0.479   0.017%     98.822%    QuantizedBatchNormWithGlobalNormalization   mixed_9/conv/batchnorm_eightbit_quantized_batch_norm\n 1598.267    0.370    0.475   0.016%     98.839%    QuantizedBatchNormWithGlobalNormalization   mixed_10/conv/batchnorm_eightbit_quantized_batch_norm\n  798.113    0.408    0.474   0.016%     98.855%    QuantizeDownAndShrinkRange  mixed_2/tower_1/conv_1/batchnorm_eightbit_quantize_down\n 1513.106    0.619    0.473   0.016%     98.872%    QuantizedBatchNormWithGlobalNormalization   mixed_9/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm\n 1669.868    0.474    0.468   0.016%     98.888%    QuantizedBatchNormWithGlobalNormalization   mixed_10/tower_1/mixed/conv_1/batchnorm_eightbit_quantized_batch_norm\n  893.727    0.384    0.468   0.016%     98.904%    QuantizeDownAndShrinkRange  mixed_3/tower/conv_1/Conv2D_eightbit_quantize_down\n  742.641    0.452    0.464   0.016%     98.920%    QuantizedConcat mixed_1/join_eightbit_quantized_concat\n 1037.213    0.158    0.458   0.016%     98.936%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv_1/batchnorm_eightbit_quantize_down\n 1391.744    0.366    0.455   0.016%     98.952%    QuantizedBatchNormWithGlobalNormalization   mixed_8/tower/conv_1/batchnorm_eightbit_quantized_batch_norm\n  824.421    0.428    0.454   0.016%     98.968%    QuantizedConcat mixed_2/join_eightbit_quantized_concat\n 1060.085    0.190    0.454   0.016%     98.983%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv_2/batchnorm_eightbit_quantize_down\n 1646.188    0.401    0.449   0.016%     98.999%    QuantizedBatchNormWithGlobalNormalization   mixed_10/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm\n  627.772    0.458    0.443   0.015%     99.014%    QuantizeDownAndShrinkRange  mixed/tower_1/conv_1/batchnorm_eightbit_quantize_down\n    6.239    0.976    0.429   0.015%     99.029%    QuantizeV2  conv/Conv2D_eightbit_quantize_Mul\n  651.111    0.411    0.415   0.014%     99.044%    QuantizeDownAndShrinkRange  mixed/tower_1/conv_2/Conv2D_eightbit_quantize_down\n  586.490    0.187    0.410   0.014%     99.058%    QuantizeDownAndShrinkRange  mixed/tower_2/conv/Conv2D_eightbit_quantize_down\n  946.413    0.133    0.395   0.014%     99.072%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv_1/batchnorm_eightbit_quantize_down\n    0.000    0.165    0.388   0.013%     99.085%                _SOURCE\n 1598.155    0.108    0.379   0.013%     99.098%    QuantizeDownAndShrinkRange  mixed_10/conv/Conv2D_eightbit_quantize_down\n  905.237    0.326    0.375   0.013%     99.111%    QuantizeDownAndShrinkRange  mixed_3/conv/Conv2D_eightbit_quantize_down\n  896.134    0.375    0.373   0.013%     99.124%    QuantizeDownAndShrinkRange  mixed_3/tower/conv_1/batchnorm_eightbit_quantize_down\n  710.245    0.360    0.371   0.013%     99.137%    QuantizeDownAndShrinkRange  mixed_1/tower_1/conv_1/batchnorm_eightbit_quantize_down\n  742.147    0.420    0.369   0.013%     99.150%    QuantizeDownAndShrinkRange  mixed_1/tower_1/conv_2/batchnorm_eightbit_quantize_down\n  739.647    0.407    0.369   0.013%     99.163%    QuantizeDownAndShrinkRange  mixed_1/tower_1/conv_2/Conv2D_eightbit_quantize_down\n 1307.586    0.195    0.366   0.013%     99.175%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv_2/Conv2D_eightbit_quantize_down\n 1670.654    0.283    0.364   0.013%     99.188%    QuantizedAvgPool    pool_3_eightbit_quantized\n  821.088    0.345    0.364   0.013%     99.201%    QuantizeDownAndShrinkRange  mixed_2/tower_1/conv_2/Conv2D_eightbit_quantize_down\n 1352.869    0.305    0.361   0.013%     99.213%    QuantizedMaxPool    mixed_8/pool_eightbit_quantized\n  823.993    0.354    0.356   0.012%     99.226%    QuantizeDownAndShrinkRange  mixed_2/tower_1/conv_2/batchnorm_eightbit_quantize_down\n 1352.588    0.267    0.338   0.012%     99.237%    QuantizedConcat mixed_7/join_eightbit_quantized_concat\n 1090.046    0.315    0.338   0.012%     99.249%    QuantizedConcat mixed_5/join_eightbit_quantized_concat\n 1190.454    0.179    0.337   0.012%     99.261%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv_3/Conv2D_eightbit_quantize_down\n 1210.351    0.332    0.331   0.012%     99.272%    QuantizedConcat mixed_6/join_eightbit_quantized_concat\n 1308.742    0.192    0.322   0.011%     99.283%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv_2/batchnorm_eightbit_quantize_down\n 1463.990    0.166    0.321   0.011%     99.295%    QuantizeDownAndShrinkRange  mixed_9/tower_1/conv/batchnorm_eightbit_quantize_down\n  673.305    0.085    0.317   0.011%     99.306%    QuantizedRelu   mixed_1/tower_2/conv_eightbit_quantized\n  985.777    0.308    0.316   0.011%     99.317%    QuantizedConcat mixed_4/join_eightbit_quantized_concat\n  755.577    0.234    0.304   0.011%     99.327%    QuantizeDownAndShrinkRange  mixed_2/tower/conv/Conv2D_eightbit_quantize_down\n 1589.839    0.089    0.297   0.010%     99.337%    QuantizeDownAndShrinkRange  mixed_10/tower_2/conv/batchnorm_eightbit_quantize_down\n  670.908    0.254    0.293   0.010%     99.348%    QuantizeDownAndShrinkRange  mixed_1/tower/conv/Conv2D_eightbit_quantize_down\n  654.512    0.270    0.293   0.010%     99.358%    QuantizedConcat mixed/join_eightbit_quantized_concat\n 1372.369    0.194    0.292   0.010%     99.368%    QuantizeDownAndShrinkRange  mixed_8/tower_1/conv/batchnorm_eightbit_quantize_down\n 1454.917    0.343    0.284   0.010%     99.378%    QuantizedBatchNormWithGlobalNormalization   mixed_9/tower_2/conv/batchnorm_eightbit_quantized_batch_norm\n 1589.616    0.220    0.280   0.010%     99.388%    QuantizedBatchNormWithGlobalNormalization   mixed_10/tower_2/conv/batchnorm_eightbit_quantized_batch_norm\n 1073.115    0.178    0.277   0.010%     99.397%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv_3/batchnorm_eightbit_quantize_down\n 1593.697    0.236    0.266   0.009%     99.406%    QuantizeDownAndShrinkRange  mixed_10/tower_1/conv/Conv2D_eightbit_quantize_down\n 1454.845    0.069    0.261   0.009%     99.416%    QuantizeDownAndShrinkRange  mixed_9/tower_2/conv/Conv2D_eightbit_quantize_down\n 1430.985    0.315    0.244   0.008%     99.424%    QuantizedBatchNormWithGlobalNormalization   mixed_8/tower_1/conv_3/batchnorm_eightbit_quantized_batch_norm\n 1553.082    0.245    0.224   0.008%     99.432%    QuantizedConcat mixed_9/join_eightbit_quantized_concat\n 1395.688    0.202    0.224   0.008%     99.440%    QuantizeDownAndShrinkRange  mixed_8/tower_1/conv_1/Conv2D_eightbit_quantize_down\n 1531.102    0.764    0.221   0.008%     99.447%    QuantizeDownAndShrinkRange  mixed_9/tower_1/mixed/conv_1/batchnorm_eightbit_quantize_down\n 1447.650    0.192    0.218   0.008%     99.455%    QuantizeDownAndShrinkRange  mixed_9/conv/batchnorm_eightbit_quantize_down\n 1455.264    2.639    0.217   0.008%     99.462%    QuantizeDownAndShrinkRange  mixed_9/tower_2/conv/batchnorm_eightbit_quantize_down\n 1327.362    0.186    0.209   0.007%     99.470%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv_3/Conv2D_eightbit_quantize_down\n 1396.867    0.213    0.208   0.007%     99.477%    QuantizeDownAndShrinkRange  mixed_8/tower_1/conv_1/batchnorm_eightbit_quantize_down\n 1210.073    0.213    0.208   0.007%     99.484%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv_4/batchnorm_eightbit_quantize_down\n 1352.322    0.222    0.207   0.007%     99.491%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv_4/batchnorm_eightbit_quantize_down\n 1670.479    0.166    0.204   0.007%     99.498%    QuantizedConcat mixed_10/join_eightbit_quantized_concat\n 1351.148    0.212    0.203   0.007%     99.505%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv_4/Conv2D_eightbit_quantize_down\n 1418.237    0.240    0.202   0.007%     99.512%    QuantizeDownAndShrinkRange  mixed_8/tower_1/conv_2/batchnorm_eightbit_quantize_down\n 1416.688    0.233    0.201   0.007%     99.519%    QuantizeDownAndShrinkRange  mixed_8/tower_1/conv_2/Conv2D_eightbit_quantize_down\n 1328.509    0.196    0.200   0.007%     99.526%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv_3/batchnorm_eightbit_quantize_down\n  985.531    0.208    0.199   0.007%     99.533%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv_4/batchnorm_eightbit_quantize_down\n 1089.804    0.203    0.198   0.007%     99.540%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv_4/batchnorm_eightbit_quantize_down\n 1371.173    0.220    0.198   0.007%     99.547%    QuantizeDownAndShrinkRange  mixed_8/tower_1/conv/Conv2D_eightbit_quantize_down\n 1208.909    0.198    0.197   0.007%     99.554%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv_4/Conv2D_eightbit_quantize_down\n 1088.632    0.213    0.197   0.007%     99.561%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv_4/Conv2D_eightbit_quantize_down\n  984.358    0.206    0.197   0.007%     99.567%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv_4/Conv2D_eightbit_quantize_down\n 1479.039    0.118    0.196   0.007%     99.574%    QuantizeDownAndShrinkRange  mixed_9/tower/mixed/conv/Conv2D_eightbit_quantize_down\n 1191.435    0.170    0.193   0.007%     99.581%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv_3/batchnorm_eightbit_quantize_down\n  314.482    0.231    0.191   0.007%     99.588%    QuantizedRelu   conv_2_eightbit_quantized\n 1446.973    0.111    0.185   0.006%     99.594%    QuantizeDownAndShrinkRange  mixed_9/conv/Conv2D_eightbit_quantize_down\n 1177.287    0.185    0.183   0.006%     99.600%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv_2/batchnorm_eightbit_quantize_down\n  971.691    0.145    0.183   0.006%     99.607%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv_3/batchnorm_eightbit_quantize_down\n  907.896    0.164    0.182   0.006%     99.613%    QuantizedConcat mixed_3/join_eightbit_quantized_concat\n  963.472    0.148    0.173   0.006%     99.619%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv_2/batchnorm_eightbit_quantize_down\n 1072.144    0.170    0.171   0.006%     99.625%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv_3/Conv2D_eightbit_quantize_down\n 1463.126    0.144    0.163   0.006%     99.631%    QuantizeDownAndShrinkRange  mixed_9/tower_1/conv/Conv2D_eightbit_quantize_down\n 1594.407    0.135    0.159   0.006%     99.636%    QuantizeDownAndShrinkRange  mixed_10/tower_1/conv/batchnorm_eightbit_quantize_down\n 1589.488    0.123    0.157   0.005%     99.642%    QuantizeDownAndShrinkRange  mixed_10/tower_2/conv/Conv2D_eightbit_quantize_down\n 1598.640    0.151    0.154   0.005%     99.647%    QuantizeDownAndShrinkRange  mixed_10/conv/batchnorm_eightbit_quantize_down\n 1599.293    0.129    0.154   0.005%     99.652%    QuantizeDownAndShrinkRange  mixed_10/tower/conv/Conv2D_eightbit_quantize_down\n  970.913    0.132    0.149   0.005%     99.658%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv_3/Conv2D_eightbit_quantize_down\n   54.775    0.173    0.149   0.005%     99.663%    QuantizedRelu   conv_eightbit_quantized\n 1512.964    0.138    0.143   0.005%     99.668%    QuantizeDownAndShrinkRange  mixed_9/tower_1/conv_1/Conv2D_eightbit_quantize_down\n 1629.432    0.145    0.140   0.005%     99.673%    QuantizeDownAndShrinkRange  mixed_10/tower/mixed/conv_1/Conv2D_eightbit_quantize_down\n  572.257    0.153    0.139   0.005%     99.677%    QuantizedRelu   conv_4_eightbit_quantized\n 1480.376    0.151    0.138   0.005%     99.682%    QuantizeDownAndShrinkRange  mixed_9/tower/mixed/conv_1/Conv2D_eightbit_quantize_down\n 1669.608    0.138    0.136   0.005%     99.687%    QuantizeDownAndShrinkRange  mixed_10/tower_1/mixed/conv/Conv2D_eightbit_quantize_down\n 1670.210    0.172    0.135   0.005%     99.692%    QuantizeDownAndShrinkRange  mixed_10/tower_1/mixed/conv/batchnorm_eightbit_quantize_down\n 1646.075    0.110    0.135   0.005%     99.696%    QuantizeDownAndShrinkRange  mixed_10/tower_1/conv_1/Conv2D_eightbit_quantize_down\n 1625.831    0.150    0.133   0.005%     99.701%    QuantizeDownAndShrinkRange  mixed_10/tower/mixed/conv/Conv2D_eightbit_quantize_down\n 1481.185    0.129    0.133   0.005%     99.705%    QuantizeDownAndShrinkRange  mixed_9/tower/mixed/conv_1/batchnorm_eightbit_quantize_down\n 1552.297    0.190    0.132   0.005%     99.710%    QuantizeDownAndShrinkRange  mixed_9/tower_1/mixed/conv/Conv2D_eightbit_quantize_down\n 1530.315    0.165    0.130   0.005%     99.715%    QuantizeDownAndShrinkRange  mixed_9/tower_1/mixed/conv_1/Conv2D_eightbit_quantize_down\n 1670.345    0.109    0.128   0.004%     99.719%    QuantizeDownAndShrinkRange  mixed_10/tower_1/mixed/conv_1/batchnorm_eightbit_quantize_down\n  153.046    0.140    0.128   0.004%     99.723%    QuantizedRelu   conv_1_eightbit_quantized\n 1626.428    0.112    0.128   0.004%     99.728%    QuantizeDownAndShrinkRange  mixed_10/tower/mixed/conv/batchnorm_eightbit_quantize_down\n 1450.098    0.124    0.128   0.004%     99.732%    QuantizeDownAndShrinkRange  mixed_9/tower/conv/Conv2D_eightbit_quantize_down\n 1630.082    0.111    0.127   0.004%     99.737%    QuantizeDownAndShrinkRange  mixed_10/tower/mixed/conv_1/batchnorm_eightbit_quantize_down\n 1669.735    0.129    0.127   0.004%     99.741%    QuantizeDownAndShrinkRange  mixed_10/tower_1/mixed/conv_1/Conv2D_eightbit_quantize_down\n  904.558    0.141    0.126   0.004%     99.746%    QuantizeDownAndShrinkRange  mixed_3/tower/conv_2/batchnorm_eightbit_quantize_down\n 1600.024    0.116    0.126   0.004%     99.750%    QuantizeDownAndShrinkRange  mixed_10/tower/conv/batchnorm_eightbit_quantize_down\n 1646.592    0.124    0.126   0.004%     99.754%    QuantizeDownAndShrinkRange  mixed_10/tower_1/conv_1/batchnorm_eightbit_quantize_down\n  903.675    0.123    0.125   0.004%     99.759%    QuantizeDownAndShrinkRange  mixed_3/tower/conv_2/Conv2D_eightbit_quantize_down\n 1513.729    0.164    0.124   0.004%     99.763%    QuantizeDownAndShrinkRange  mixed_9/tower_1/conv_1/batchnorm_eightbit_quantize_down\n 1479.588    0.145    0.124   0.004%     99.767%    QuantizeDownAndShrinkRange  mixed_9/tower/mixed/conv/batchnorm_eightbit_quantize_down\n 1552.898    0.161    0.123   0.004%     99.771%    QuantizeDownAndShrinkRange  mixed_9/tower_1/mixed/conv/batchnorm_eightbit_quantize_down\n 1450.669    0.106    0.122   0.004%     99.776%    QuantizeDownAndShrinkRange  mixed_9/tower/conv/batchnorm_eightbit_quantize_down\n  348.332    0.135    0.118   0.004%     99.780%    QuantizedRelu   conv_3_eightbit_quantized\n  599.414    0.090    0.091   0.003%     99.783%    QuantizedRelu   mixed/conv_eightbit_quantized\n  628.234    0.086    0.089   0.003%     99.786%    QuantizedRelu   mixed/tower_1/conv_1_eightbit_quantized\n    5.808    0.225    0.084   0.003%     99.789%           Max  conv/Conv2D_eightbit_max_Mul\n  896.518    0.134    0.083   0.003%     99.792%    QuantizedRelu   mixed_3/tower/conv_1_eightbit_quantized\n 1430.886    0.095    0.081   0.003%     99.795%    QuantizeDownAndShrinkRange  mixed_8/tower_1/conv_3/Conv2D_eightbit_quantize_down\n  654.430    0.079    0.080   0.003%     99.797%    QuantizedRelu   mixed/tower_1/conv_2_eightbit_quantized\n  798.525    0.064    0.080   0.003%     99.800%    QuantizedRelu   mixed_2/tower_1/conv_1_eightbit_quantized\n    5.848    0.383    0.079   0.003%     99.803%           Min  conv/Conv2D_eightbit_min_Mul\n  849.317    0.081    0.079   0.003%     99.806%    QuantizedRelu   mixed_3/tower/conv_eightbit_quantized\n  589.438    0.067    0.079   0.003%     99.808%    QuantizedRelu   mixed/tower_1/conv_eightbit_quantized\n 1431.410    0.125    0.078   0.003%     99.811%    QuantizedConcat mixed_8/join_eightbit_quantized_concat\n 1431.305    0.078    0.078   0.003%     99.814%    QuantizeDownAndShrinkRange  mixed_8/tower_1/conv_3/batchnorm_eightbit_quantize_down\n  710.609    0.070    0.078   0.003%     99.817%    QuantizedRelu   mixed_1/tower_1/conv_1_eightbit_quantized\n  627.572    0.046    0.078   0.003%     99.819%    QuantizedRelu   mixed/tower/conv_1_eightbit_quantized\n  769.994    0.082    0.076   0.003%     99.822%    QuantizedRelu   mixed_2/conv_eightbit_quantized\n  796.418    0.045    0.074   0.003%     99.825%    QuantizedRelu   mixed_2/tower/conv_1_eightbit_quantized\n  682.652    0.104    0.074   0.003%     99.827%    QuantizedRelu   mixed_1/tower_1/conv_eightbit_quantized\n  784.800    0.080    0.073   0.003%     99.830%    QuantizedRelu   mixed_2/tower_2/conv_eightbit_quantized\n  707.766    0.046    0.073   0.003%     99.832%    QuantizedRelu   mixed_1/tower/conv_1_eightbit_quantized\n  757.177    0.045    0.072   0.003%     99.835%    QuantizedRelu   mixed_2/tower_1/conv_eightbit_quantized\n  824.352    0.066    0.072   0.002%     99.837%    QuantizedRelu   mixed_2/tower_1/conv_2_eightbit_quantized\n  742.570    0.068    0.070   0.002%     99.840%    QuantizedRelu   mixed_1/tower_1/conv_2_eightbit_quantized\n  907.830    0.063    0.069   0.002%     99.842%    QuantizedRelu   mixed_3/conv_eightbit_quantized\n  673.812    0.079    0.069   0.002%     99.844%    QuantizedRelu   mixed_1/conv_eightbit_quantized\n  588.811    0.069    0.062   0.002%     99.847%    QuantizedRelu   mixed/tower/conv_eightbit_quantized\n  939.612    0.064    0.061   0.002%     99.849%    QuantizedRelu   mixed_4/conv_eightbit_quantized\n 1134.276    0.065    0.058   0.002%     99.851%    QuantizedRelu   mixed_6/conv_eightbit_quantized\n 1017.581    0.065    0.058   0.002%     99.853%    QuantizedRelu   mixed_5/tower_2/conv_eightbit_quantized\n 1016.566    0.061    0.058   0.002%     99.855%    QuantizedRelu   mixed_5/conv_eightbit_quantized\n  672.675    0.036    0.057   0.002%     99.857%    QuantizedRelu   mixed_1/tower/conv_eightbit_quantized\n  951.059    0.068    0.057   0.002%     99.859%    QuantizedRelu   mixed_4/tower_2/conv_eightbit_quantized\n 1243.589    0.067    0.057   0.002%     99.861%    QuantizedRelu   mixed_7/conv_eightbit_quantized\n 1246.866    0.066    0.056   0.002%     99.863%    QuantizedRelu   mixed_7/tower_2/conv_eightbit_quantized\n 1133.490    0.059    0.054   0.002%     99.864%    QuantizedRelu   mixed_6/tower_2/conv_eightbit_quantized\n 1168.023    0.034    0.053   0.002%     99.866%    QuantizedRelu   mixed_6/tower/conv_2_eightbit_quantized\n 1303.269    0.060    0.052   0.002%     99.868%    QuantizedRelu   mixed_7/tower/conv_2_eightbit_quantized\n 1249.895    0.061    0.051   0.002%     99.870%    QuantizedRelu   mixed_7/tower_1/conv_eightbit_quantized\n  969.648    0.065    0.051   0.002%     99.872%    QuantizedRelu   mixed_4/tower/conv_2_eightbit_quantized\n 1289.918    0.061    0.050   0.002%     99.873%    QuantizedRelu   mixed_7/tower_1/conv_1_eightbit_quantized\n  757.059    0.041    0.050   0.002%     99.875%    QuantizedRelu   mixed_2/tower/conv_eightbit_quantized\n 1246.313    0.033    0.049   0.002%     99.877%    QuantizedRelu   mixed_7/tower/conv_eightbit_quantized\n 1071.647    0.034    0.049   0.002%     99.878%    QuantizedRelu   mixed_5/tower/conv_2_eightbit_quantized\n 1371.432    0.033    0.049   0.002%     99.880%    QuantizedRelu   mixed_8/tower/conv_eightbit_quantized\n  587.936    0.042    0.048   0.002%     99.882%    QuantizedRelu   mixed/tower_2/conv_eightbit_quantized\n 1015.851    0.053    0.047   0.002%     99.883%    QuantizedRelu   mixed_5/tower_1/conv_eightbit_quantized\n 1017.831    0.054    0.047   0.002%     99.885%    QuantizedRelu   mixed_5/tower/conv_eightbit_quantized\n 1043.385    0.044    0.045   0.002%     99.887%    QuantizedRelu   mixed_5/tower/conv_1_eightbit_quantized\n 1276.223    0.034    0.044   0.002%     99.888%    QuantizedRelu   mixed_7/tower/conv_1_eightbit_quantized\n 1149.669    0.050    0.044   0.002%     99.890%    QuantizedRelu   mixed_6/tower/conv_1_eightbit_quantized\n 1372.566    0.033    0.043   0.001%     99.891%    QuantizedRelu   mixed_8/tower_1/conv_eightbit_quantized\n 1135.923    0.057    0.043   0.001%     99.893%    QuantizedRelu   mixed_6/tower_1/conv_eightbit_quantized\n 1397.084    0.033    0.043   0.001%     99.894%    QuantizedRelu   mixed_8/tower_1/conv_1_eightbit_quantized\n 1308.938    0.038    0.043   0.001%     99.896%    QuantizedRelu   mixed_7/tower_1/conv_2_eightbit_quantized\n 1037.374    0.029    0.041   0.001%     99.897%    QuantizedRelu   mixed_5/tower_1/conv_1_eightbit_quantized\n 1128.507    0.049    0.041   0.001%     99.899%    QuantizedRelu   mixed_6/tower/conv_eightbit_quantized\n 1158.836    0.029    0.041   0.001%     99.900%    QuantizedRelu   mixed_6/tower_1/conv_1_eightbit_quantized\n 1060.278    0.052    0.040   0.001%     99.901%    QuantizedRelu   mixed_5/tower_1/conv_2_eightbit_quantized\n 1352.549    0.036    0.040   0.001%     99.903%    QuantizedRelu   mixed_7/tower_1/conv_4_eightbit_quantized\n 1090.010    0.034    0.040   0.001%     99.904%    QuantizedRelu   mixed_5/tower_1/conv_4_eightbit_quantized\n 1328.708    0.034    0.039   0.001%     99.906%    QuantizedRelu   mixed_7/tower_1/conv_3_eightbit_quantized\n 1418.481    0.061    0.039   0.001%     99.907%    QuantizedRelu   mixed_8/tower_1/conv_2_eightbit_quantized\n 1210.292    0.056    0.038   0.001%     99.908%    QuantizedRelu   mixed_6/tower_1/conv_4_eightbit_quantized\n  930.366    0.039    0.037   0.001%     99.910%    QuantizedRelu   mixed_4/tower_1/conv_eightbit_quantized\n  952.447    0.026    0.037   0.001%     99.911%    QuantizedRelu   mixed_4/tower/conv_1_eightbit_quantized\n 1177.475    0.028    0.037   0.001%     99.912%    QuantizedRelu   mixed_6/tower_1/conv_2_eightbit_quantized\n  941.097    0.025    0.036   0.001%     99.913%    QuantizedRelu   mixed_4/tower/conv_eightbit_quantized\n  985.741    0.035    0.035   0.001%     99.915%    QuantizedRelu   mixed_4/tower_1/conv_4_eightbit_quantized\n 1191.607    0.029    0.034   0.001%     99.916%    QuantizedRelu   mixed_6/tower_1/conv_3_eightbit_quantized\n  946.550    0.038    0.034   0.001%     99.917%    QuantizedRelu   mixed_4/tower_1/conv_1_eightbit_quantized\n 1073.295    0.028    0.033   0.001%     99.918%    QuantizedRelu   mixed_5/tower_1/conv_3_eightbit_quantized\n 1689.900    0.028    0.033   0.001%     99.919%    QuantizedBiasAdd    softmax/logits_eightbit_quantized_bias_add\n 1689.945    0.028    0.033   0.001%     99.920%       Softmax  softmax\n  963.624    0.025    0.032   0.001%     99.921%    QuantizedRelu   mixed_4/tower_1/conv_2_eightbit_quantized\n 1630.197    0.038    0.030   0.001%     99.922%    QuantizedRelu   mixed_10/tower/mixed/conv_1_eightbit_quantized\n 1464.160    0.032    0.029   0.001%     99.924%    QuantizedRelu   mixed_9/tower_1/conv_eightbit_quantized\n 1594.545    0.035    0.028   0.001%     99.924%    QuantizedRelu   mixed_10/tower_1/conv_eightbit_quantized\n  971.838    0.025    0.027   0.001%     99.925%    QuantizedRelu   mixed_4/tower_1/conv_3_eightbit_quantized\n 1689.985    0.026    0.026   0.001%     99.926%                _SINK\n 1670.385    0.018    0.026   0.001%     99.927%    QuantizedRelu   mixed_10/tower_1/mixed/conv_eightbit_quantized\n  904.702    0.037    0.025   0.001%     99.928%    QuantizedRelu   mixed_3/tower/conv_2_eightbit_quantized\n 1598.794    0.017    0.025   0.001%     99.929%    QuantizedRelu   mixed_10/conv_eightbit_quantized\n 1626.543    0.019    0.025   0.001%     99.930%    QuantizedRelu   mixed_10/tower/mixed/conv_eightbit_quantized\n 1553.062    0.017    0.024   0.001%     99.931%    QuantizedRelu   mixed_9/tower_1/mixed/conv_eightbit_quantized\n 1481.317    0.030    0.024   0.001%     99.932%    QuantizedRelu   mixed_9/tower/mixed/conv_1_eightbit_quantized\n 1479.735    0.018    0.024   0.001%     99.932%    QuantizedRelu   mixed_9/tower/mixed/conv_eightbit_quantized\n 1600.144    0.037    0.024   0.001%     99.933%    QuantizedRelu   mixed_10/tower/conv_eightbit_quantized\n 1447.845    0.031    0.024   0.001%     99.934%    QuantizedRelu   mixed_9/conv_eightbit_quantized\n 1392.212    0.016    0.024   0.001%     99.935%    QuantizedRelu   mixed_8/tower/conv_1_eightbit_quantized\n 1450.780    0.020    0.024   0.001%     99.936%    QuantizedRelu   mixed_9/tower/conv_eightbit_quantized\n 1670.458    0.018    0.024   0.001%     99.937%    QuantizedRelu   mixed_10/tower_1/mixed/conv_1_eightbit_quantized\n 1531.870    0.028    0.023   0.001%     99.937%    QuantizedRelu   mixed_9/tower_1/mixed/conv_1_eightbit_quantized\n 1513.897    0.029    0.022   0.001%     99.938%    QuantizedRelu   mixed_9/tower_1/conv_1_eightbit_quantized\n 1646.719    0.017    0.022   0.001%     99.939%    QuantizedRelu   mixed_10/tower_1/conv_1_eightbit_quantized\n    4.674    0.033    0.022   0.001%     99.940%         Const  mixed_9/tower_2/conv/batchnorm/gamma_min\n    4.709    0.024    0.018   0.001%     99.940%         Const  mixed_9/tower_2/conv/batchnorm/gamma_max\n 1589.931    0.012    0.018   0.001%     99.941%    QuantizedRelu   mixed_10/tower_2/conv_eightbit_quantized\n 1457.907    0.014    0.017   0.001%     99.941%    QuantizedRelu   mixed_9/tower_2/conv_eightbit_quantized\n 1670.970    0.012    0.017   0.001%     99.942%    QuantizeV2  softmax/logits/MatMul_eightbit_quantize_pool_3/_reshape\n 1431.387    0.020    0.015   0.001%     99.943%    QuantizedRelu   mixed_8/tower_1/conv_3_eightbit_quantized\n 1689.888    0.010    0.013   0.000%     99.943%    QuantizeDownAndShrinkRange  softmax/logits/MatMul_eightbit_quantize_down\n 1670.939    0.009    0.011   0.000%     99.943%    Dequantize  pool_3\n    4.656    0.016    0.010   0.000%     99.944%         Const  mixed_9/tower_2/conv/batchnorm/gamma_quint8_const\n 1689.930    0.007    0.009   0.000%     99.944%    QuantizeDownAndShrinkRange  softmax/logits_eightbit_quantize_down\n    0.247    0.013    0.009   0.000%     99.944%         Const  mixed_10/join/concat_dim\n 1689.938    0.006    0.007   0.000%     99.945%    Dequantize  softmax/logits\n    0.441    0.008    0.007   0.000%     99.945%         Const  conv_2/batchnorm/gamma_quint8_const\n 1670.959    0.005    0.007   0.000%     99.945%           Max  softmax/logits/MatMul_eightbit_max_pool_3/_reshape\n 1670.966    0.003    0.005   0.000%     99.945%           Min  softmax/logits/MatMul_eightbit_min_pool_3/_reshape\n    2.727    0.014    0.005   0.000%     99.945%         Const  mixed_5/tower/conv/batchnorm/gamma_quint8_const\n    0.895    0.005    0.004   0.000%     99.945%         Const  mixed/tower_1/conv_1/batchnorm/gamma_quint8_const\n    4.852    0.010    0.004   0.000%     99.946%         Const  mixed_10/tower/conv/batchnorm/gamma_quint8_const\n    5.789    0.006    0.003   0.000%     99.946%       Reshape  conv/Conv2D_eightbit_reshape_Mul\n 1670.949    0.003    0.003   0.000%     99.946%       Reshape  pool_3/_reshape\n    2.164    0.007    0.003   0.000%     99.946%         Const  mixed_4/tower/conv/batchnorm/gamma_quint8_const\n 1670.954    0.002    0.003   0.000%     99.946%       Reshape  softmax/logits/MatMul_eightbit_reshape_pool_3/_reshape\n    4.225    0.003    0.002   0.000%     99.946%         Const  mixed_7/tower_2/conv/conv2d_params_quint8_const\n    0.265    0.004    0.002   0.000%     99.946%         Const  conv/conv2d_params_quint8_const\n    1.023    0.007    0.002   0.000%     99.946%         Const  mixed_1/conv/conv2d_params_quint8_const\n    0.324    0.003    0.002   0.000%     99.946%         Const  conv/batchnorm/gamma_quint8_const\n    0.664    0.005    0.002   0.000%     99.946%         Const  mixed/tower/conv/batchnorm/gamma_quint8_const\n    2.054    0.003    0.002   0.000%     99.946%         Const  mixed_4/conv/conv2d_params_quint8_const\n    1.136    0.005    0.002   0.000%     99.947%         Const  mixed_1/tower/conv/batchnorm/beta_quint8_const\n    4.734    0.004    0.002   0.000%     99.947%         Const  mixed_10/conv/conv2d_params_quint8_const\n    0.282    0.003    0.002   0.000%     99.947%         Const  conv/batchnorm/moving_mean_quint8_const\n    4.442    0.003    0.002   0.000%     99.947%         Const  mixed_8/tower_1/conv_1/conv2d_params_quint8_const\n    2.971    0.004    0.002   0.000%     99.947%         Const  mixed_5/tower_1/conv_2/conv2d_params_quint8_const\n    5.310    0.004    0.002   0.000%     99.947%         Const  softmax/logits/MatMul_eightbit_reduction_dims\n    5.227    0.003    0.002   0.000%     99.947%         Const  mixed_10/tower_2/conv/conv2d_params_quint8_const\n    0.836    0.006    0.002   0.000%     99.947%         Const  mixed/tower_1/conv_1/conv2d_params_quint8_const\n    0.592    0.003    0.002   0.000%     99.947%         Const  mixed/conv/batchnorm/moving_variance_min\n    4.791    0.003    0.002   0.000%     99.947%         Const  mixed_10/conv/batchnorm/gamma_quint8_const\n    1.536    0.003    0.002   0.000%     99.947%         Const  mixed_2/tower/conv_1/conv2d_params_quint8_const\n    1.853    0.028    0.002   0.000%     99.947%         Const  mixed_3/conv/batchnorm/beta_max\n    3.844    0.005    0.002   0.000%     99.947%         Const  mixed_7/tower/conv_1/conv2d_params_quint8_const\n    1.720    0.002    0.002   0.000%     99.947%         Const  mixed_2/tower_1/conv_2/batchnorm/moving_mean_max\n    0.559    0.003    0.002   0.000%     99.947%         Const  mixed/conv/conv2d_params_quint8_const\n    3.300    0.003    0.002   0.000%     99.948%         Const  mixed_6/tower/conv_1/conv2d_params_quint8_const\n    3.193    0.002    0.002   0.000%     99.948%         Const  mixed_6/conv/conv2d_params_quint8_const\n    1.205    0.003    0.002   0.000%     99.948%         Const  mixed_1/tower_1/conv/conv2d_params_quint8_const\n    0.329    0.004    0.002   0.000%     99.948%         Const  conv_1/conv2d_params_quint8_const\n    4.277    0.006    0.002   0.000%     99.948%         Const  mixed_8/tower/conv/conv2d_params_quint8_const\n    3.683    0.003    0.002   0.000%     99.948%         Const  mixed_6/tower_2/conv/conv2d_params_quint8_const\n    3.352    0.007    0.002   0.000%     99.948%         Const  mixed_6/tower/conv_2/conv2d_params_quint8_const\n    2.336    0.005    0.002   0.000%     99.948%         Const  mixed_4/tower_1/conv_1/conv2d_params_quint8_const\n    1.643    0.003    0.002   0.000%     99.948%         Const  mixed_2/tower_1/conv_1/conv2d_params_quint8_const\n    3.789    0.005    0.002   0.000%     99.948%         Const  mixed_7/tower/conv/conv2d_params_quint8_const\n    1.994    0.004    0.002   0.000%     99.948%         Const  mixed_3/tower/conv_2/conv2d_params_quint8_const\n    1.424    0.003    0.002   0.000%     99.948%         Const  mixed_2/conv/conv2d_params_quint8_const\n    0.451    0.003    0.002   0.000%     99.948%         Const  conv_3/conv2d_params_quint8_const\n    0.310    0.003    0.002   0.000%     99.948%         Const  conv/batchnorm/beta_quint8_const\n    5.335    0.006    0.002   0.000%     99.948%         Const  mixed_9/tower/conv/conv2d_params_quint8_const\n    5.102    0.007    0.002   0.000%     99.948%         Const  mixed_10/tower_1/mixed/conv/conv2d_params_quint8_const\n    4.796    0.006    0.002   0.000%     99.949%         Const  mixed_10/tower/conv/conv2d_params_quint8_const\n    4.133    0.003    0.002   0.000%     99.949%         Const  mixed_7/tower_1/conv_3/batchnorm/moving_mean_min\n    2.395    0.003    0.002   0.000%     99.949%         Const  mixed_4/tower_1/conv_2/conv2d_params_quint8_const\n    1.190    0.006    0.002   0.000%     99.949%         Const  mixed_1/tower/conv_1/batchnorm/beta_quint8_const\n    0.506    0.004    0.002   0.000%     99.949%         Const  conv_4/conv2d_params_quint8_const\n    0.388    0.003    0.002   0.000%     99.949%         Const  conv_2/conv2d_params_quint8_const\n    3.737    0.002    0.002   0.000%     99.949%         Const  mixed_7/conv/conv2d_params_quint8_const\n    3.463    0.002    0.002   0.000%     99.949%         Const  mixed_6/tower_1/conv_1/conv2d_params_quint8_const\n    1.883    0.003    0.002   0.000%     99.949%         Const  mixed_3/tower/conv/conv2d_params_quint8_const\n    0.920    0.004    0.002   0.000%     99.949%         Const  mixed/tower_1/conv_2/batchnorm/moving_mean_quint8_const\n    5.392    0.006    0.002   0.000%     99.949%         Const  mixed_9/tower/mixed/conv/conv2d_params_quint8_const\n    5.165    0.005    0.002   0.000%     99.949%         Const  mixed_10/tower_1/mixed/conv_1/conv2d_params_quint8_const\n    4.986    0.005    0.002   0.000%     99.949%         Const  mixed_10/tower_1/conv/conv2d_params_quint8_const\n    4.603    0.003    0.002   0.000%     99.949%         Const  mixed_9/conv/conv2d_params_quint8_const\n    3.629    0.003    0.002   0.000%     99.949%         Const  mixed_6/tower_1/conv_4/conv2d_params_quint8_const\n    3.517    0.004    0.002   0.000%     99.949%         Const  mixed_6/tower_1/conv_2/conv2d_params_quint8_const\n    2.805    0.003    0.002   0.000%     99.950%         Const  mixed_5/tower/conv_2/conv2d_params_quint8_const\n    2.621    0.003    0.002   0.000%     99.950%         Const  mixed_5/conv/conv2d_params_quint8_const\n    1.261    0.002    0.002   0.000%     99.950%         Const  mixed_1/tower_1/conv_1/conv2d_params_quint8_const\n    1.220    0.002    0.002   0.000%     99.950%         Const  mixed_1/tower_1/conv/batchnorm/moving_mean_quint8_const\n    0.691    0.005    0.002   0.000%     99.950%         Const  mixed/tower/conv_1/batchnorm/moving_mean_quint8_const\n    4.930    0.005    0.002   0.000%     99.950%         Const  mixed_10/tower/mixed/conv_1/conv2d_params_quint8_const\n    4.870    0.004    0.002   0.000%     99.950%         Const  mixed_10/tower/mixed/conv/conv2d_params_quint8_const\n    2.857    0.010    0.002   0.000%     99.950%         Const  mixed_5/tower_1/conv/conv2d_params_quint8_const\n    0.968    0.003    0.002   0.000%     99.950%         Const  mixed/tower_2/conv/conv2d_params_quint8_const\n    0.902    0.006    0.002   0.000%     99.950%         Const  mixed/tower_1/conv_2/conv2d_params_quint8_const\n    0.489    0.003    0.002   0.000%     99.950%         Const  conv_3/batchnorm/beta_quint8_const\n    0.425    0.003    0.002   0.000%     99.950%         Const  conv_2/batchnorm/moving_variance_max\n    0.402    0.003    0.002   0.000%     99.950%         Const  conv_2/batchnorm/moving_mean_quint8_const\n    4.549    0.003    0.002   0.000%     99.950%         Const  mixed_8/tower_1/conv_3/conv2d_params_quint8_const\n    4.115    0.003    0.002   0.000%     99.950%         Const  mixed_7/tower_1/conv_3/conv2d_params_quint8_const\n    4.062    0.003    0.002   0.000%     99.950%         Const  mixed_7/tower_1/conv_2/conv2d_params_quint8_const\n    3.084    0.003    0.002   0.000%     99.950%         Const  mixed_5/tower_1/conv_4/conv2d_params_quint8_const\n    2.676    0.003    0.002   0.000%     99.951%         Const  mixed_5/tower/conv/conv2d_params_quint8_const\n    2.282    0.007    0.002   0.000%     99.951%         Const  mixed_4/tower_1/conv/conv2d_params_quint8_const\n    1.817    0.003    0.002   0.000%     99.951%         Const  mixed_3/conv/batchnorm/moving_mean_quint8_const\n    1.749    0.005    0.002   0.000%     99.951%         Const  mixed_2/tower_2/conv/conv2d_params_quint8_const\n    1.694    0.005    0.002   0.000%     99.951%         Const  mixed_2/tower_1/conv_2/conv2d_params_quint8_const\n    1.082    0.006    0.002   0.000%     99.951%         Const  mixed_1/tower/conv/conv2d_params_quint8_const\n    0.851    0.003    0.002   0.000%     99.951%         Const  mixed/tower_1/conv_1/batchnorm/moving_mean_quint8_const\n    0.752    0.005    0.002   0.000%     99.951%         Const  mixed/tower_1/conv/conv2d_params_quint8_const\n    0.613    0.003    0.002   0.000%     99.951%         Const  mixed/tower/conv/conv2d_params_quint8_const\n    5.045    0.006    0.002   0.000%     99.951%         Const  mixed_10/tower_1/conv_1/conv2d_params_quint8_const\n    4.331    0.006    0.002   0.000%     99.951%         Const  mixed_8/tower/conv_1/conv2d_params_quint8_const\n    3.027    0.003    0.002   0.000%     99.951%         Const  mixed_5/tower_1/conv_3/conv2d_params_quint8_const\n    2.918    0.005    0.002   0.000%     99.951%         Const  mixed_5/tower_1/conv_1/conv2d_params_quint8_const\n    2.226    0.003    0.002   0.000%     99.951%         Const  mixed_4/tower/conv_2/conv2d_params_quint8_const\n    2.172    0.003    0.002   0.000%     99.951%         Const  mixed_4/tower/conv_1/conv2d_params_quint8_const\n    1.937    0.003    0.002   0.000%     99.951%         Const  mixed_3/tower/conv_1/conv2d_params_quint8_const\n    0.464    0.003    0.002   0.000%     99.951%         Const  conv_3/batchnorm/moving_mean_quint8_const\n    0.272    0.003    0.002   0.000%     99.952%         Const  conv/conv2d_params_min\n    5.525    0.003    0.002   0.000%     99.952%         Const  mixed_9/tower_1/conv/batchnorm/moving_mean_quint8_const\n    4.170    0.002    0.002   0.000%     99.952%         Const  mixed_7/tower_1/conv_4/conv2d_params_quint8_const\n    3.598    0.005    0.002   0.000%     99.952%         Const  mixed_6/tower_1/conv_3/batchnorm/moving_variance_quint8_const\n    3.571    0.003    0.002   0.000%     99.952%         Const  mixed_6/tower_1/conv_3/conv2d_params_quint8_const\n    3.408    0.005    0.002   0.000%     99.952%         Const  mixed_6/tower_1/conv/conv2d_params_quint8_const\n    3.246    0.003    0.002   0.000%     99.952%         Const  mixed_6/tower/conv/conv2d_params_quint8_const\n    2.816    0.007    0.002   0.000%     99.952%         Const  mixed_5/tower/conv_2/batchnorm/moving_mean_quint8_const\n    2.109    0.004    0.002   0.000%     99.952%         Const  mixed_4/tower/conv/conv2d_params_quint8_const\n    1.802    0.003    0.002   0.000%     99.952%         Const  mixed_3/conv/conv2d_params_quint8_const\n    1.370    0.003    0.002   0.000%     99.952%         Const  mixed_1/tower_2/conv/conv2d_params_quint8_const\n    1.051    0.003    0.002   0.000%     99.952%         Const  mixed_1/conv/batchnorm/moving_mean_max\n    5.680    0.003    0.002   0.000%     99.952%         Const  mixed_9/tower_1/mixed/conv_1/conv2d_params_quint8_const\n    5.623    0.004    0.002   0.000%     99.952%         Const  mixed_9/tower_1/mixed/conv/conv2d_params_quint8_const\n    5.509    0.004    0.002   0.000%     99.952%         Const  mixed_9/tower_1/conv/conv2d_params_quint8_const\n    5.449    0.003    0.002   0.000%     99.952%         Const  mixed_9/tower/mixed/conv_1/conv2d_params_quint8_const\n    3.954    0.003    0.002   0.000%     99.952%         Const  mixed_7/tower_1/conv/conv2d_params_quint8_const\n    2.508    0.003    0.002   0.000%     99.952%         Const  mixed_4/tower_1/conv_4/conv2d_params_quint8_const\n    2.449    0.003    0.002   0.000%     99.953%         Const  mixed_4/tower_1/conv_3/conv2d_params_quint8_const\n    1.287    0.005    0.002   0.000%     99.953%         Const  mixed_1/tower_1/conv_1/batchnorm/moving_variance_quint8_const\n    0.374    0.002    0.002   0.000%     99.953%         Const  conv_1/batchnorm/beta_quint8_const\n    5.566    0.003    0.002   0.000%     99.953%         Const  mixed_9/tower_1/conv_1/conv2d_params_quint8_const\n    5.123    0.003    0.002   0.000%     99.953%         Const  mixed_10/tower_1/mixed/conv/batchnorm/moving_mean_quint8_const\n    4.495    0.003    0.002   0.000%     99.953%         Const  mixed_8/tower_1/conv_2/conv2d_params_quint8_const\n    4.008    0.003    0.002   0.000%     99.953%         Const  mixed_7/tower_1/conv_1/conv2d_params_quint8_const\n    2.688    0.005    0.002   0.000%     99.953%         Const  mixed_5/tower/conv/batchnorm/moving_mean_quint8_const\n    2.562    0.003    0.002   0.000%     99.953%         Const  mixed_4/tower_2/conv/conv2d_params_quint8_const\n    2.346    0.003    0.002   0.000%     99.953%         Const  mixed_4/tower_1/conv_1/conv2d_params_max\n    2.066    0.005    0.002   0.000%     99.953%         Const  mixed_4/conv/batchnorm/moving_mean_quint8_const\n    0.711    0.004    0.002   0.000%     99.953%         Const  mixed/tower/conv_1/batchnorm/moving_variance_quint8_const\n    0.626    0.002    0.002   0.000%     99.953%         Const  mixed/tower/conv/batchnorm/moving_mean_quint8_const\n    0.319    0.003    0.002   0.000%     99.953%         Const  conv/batchnorm/beta_max\n    5.737    0.003    0.002   0.000%     99.953%         Const  mixed_9/tower_2/conv/conv2d_params_quint8_const\n    4.097    0.003    0.002   0.000%     99.953%         Const  mixed_7/tower_1/conv_2/batchnorm/moving_variance_max\n    3.748    0.006    0.002   0.000%     99.953%         Const  mixed_7/conv/batchnorm/moving_mean_quint8_const\n    3.113    0.003    0.002   0.000%     99.953%         Const  mixed_5/tower_1/conv_4/batchnorm/moving_variance_quint8_const\n    2.751    0.003    0.002   0.000%     99.954%         Const  mixed_5/tower/conv_1/conv2d_params_quint8_const\n    1.654    0.006    0.002   0.000%     99.954%         Const  mixed_2/tower_1/conv_1/batchnorm/moving_mean_quint8_const\n    1.315    0.003    0.002   0.000%     99.954%         Const  mixed_1/tower_1/conv_2/conv2d_params_quint8_const\n    1.164    0.003    0.002   0.000%     99.954%         Const  mixed_1/tower/conv_1/batchnorm/moving_mean_quint8_const\n    0.813    0.007    0.002   0.000%     99.954%         Const  mixed/tower_1/conv/batchnorm/moving_variance_max\n    0.588    0.003    0.002   0.000%     99.954%         Const  mixed/conv/batchnorm/moving_variance_quint8_const\n    0.532    0.003    0.002   0.000%     99.954%         Const  conv_4/batchnorm/moving_variance_quint8_const\n    0.477    0.003    0.002   0.000%     99.954%         Const  conv_3/batchnorm/moving_variance_quint8_const\n    5.350    0.003    0.002   0.000%     99.954%         Const  mixed_9/tower/conv/batchnorm/moving_mean_quint8_const\n    4.386    0.003    0.002   0.000%     99.954%         Const  mixed_8/tower_1/conv/conv2d_params_quint8_const\n    3.899    0.003    0.002   0.000%     99.954%         Const  mixed_7/tower/conv_2/conv2d_params_quint8_const\n    3.368    0.003    0.002   0.000%     99.954%         Const  mixed_6/tower/conv_2/batchnorm/moving_mean_quint8_const\n    2.143    0.002    0.002   0.000%     99.954%         Const  mixed_4/tower/conv/batchnorm/moving_variance_min\n    1.590    0.003    0.002   0.000%     99.954%         Const  mixed_2/tower_1/conv/conv2d_params_quint8_const\n    1.491    0.002    0.002   0.000%     99.954%         Const  mixed_2/tower/conv/batchnorm/moving_mean_quint8_const\n    1.436    0.005    0.002   0.000%     99.954%         Const  mixed_2/conv/batchnorm/moving_mean_quint8_const\n    0.980    0.006    0.002   0.000%     99.954%         Const  mixed/tower_2/conv/batchnorm/moving_mean_quint8_const\n    0.866    0.003    0.002   0.000%     99.954%         Const  mixed/tower_1/conv_1/batchnorm/moving_variance_quint8_const\n    0.416    0.003    0.002   0.000%     99.955%         Const  conv_2/batchnorm/moving_variance_quint8_const\n    0.305    0.003    0.002   0.000%     99.955%         Const  conv/batchnorm/moving_variance_max\n    5.289    0.007    0.002   0.000%     99.955%         Const  softmax/weights_quint8_const\n    4.945    0.003    0.002   0.000%     99.955%         Const  mixed_10/tower/mixed/conv_1/batchnorm/moving_mean_quint8_const\n    3.695    0.005    0.002   0.000%     99.955%         Const  mixed_6/tower_2/conv/batchnorm/moving_mean_quint8_const\n    3.258    0.005    0.002   0.000%     99.955%         Const  mixed_6/tower/conv/batchnorm/moving_mean_quint8_const\n    3.045    0.002    0.002   0.000%     99.955%         Const  mixed_5/tower_1/conv_3/batchnorm/moving_mean_quint8_const\n    2.194    0.003    0.002   0.000%     99.955%         Const  mixed_4/tower/conv_1/batchnorm/moving_mean_max\n    1.988    0.003    0.002   0.000%     99.955%         Const  mixed_3/tower/conv_1/batchnorm/beta_max\n    1.476    0.006    0.002   0.000%     99.955%         Const  mixed_2/tower/conv/conv2d_params_quint8_const\n    0.952    0.003    0.002   0.000%     99.955%         Const  mixed/tower_1/conv_2/batchnorm/beta_quint8_const\n    0.832    0.003    0.002   0.000%     99.955%         Const  mixed/tower_1/conv/batchnorm/beta_max\n    0.780    0.009    0.002   0.000%     99.955%         Const  mixed/tower_1/conv/batchnorm/moving_mean_min\n    0.772    0.005    0.002   0.000%     99.955%         Const  mixed/tower_1/conv/batchnorm/moving_mean_quint8_const\n    0.469    0.002    0.002   0.000%     99.955%         Const  conv_3/batchnorm/moving_mean_min\n    0.360    0.003    0.002   0.000%     99.955%         Const  conv_1/batchnorm/moving_variance_quint8_const\n    0.335    0.003    0.002   0.000%     99.955%         Const  conv_1/conv2d_params_min\n    5.486    0.002    0.002   0.000%     99.955%         Const  mixed_9/tower/mixed/conv_1/batchnorm/moving_variance_min\n    4.034    0.005    0.002   0.000%     99.955%         Const  mixed_7/tower_1/conv_1/batchnorm/moving_variance_quint8_const\n    3.803    0.003    0.002   0.000%     99.956%         Const  mixed_7/tower/conv/batchnorm/moving_mean_quint8_const\n    3.477    0.003    0.002   0.000%     99.956%         Const  mixed_6/tower_1/conv_1/batchnorm/moving_mean_quint8_const\n    3.139    0.003    0.002   0.000%     99.956%         Const  mixed_5/tower_2/conv/conv2d_params_quint8_const\n    2.836    0.002    0.002   0.000%     99.956%         Const  mixed_5/tower/conv_2/batchnorm/moving_variance_min\n    2.492    0.004    0.002   0.000%     99.956%         Const  mixed_4/tower_1/conv_3/batchnorm/beta_quint8_const\n    1.763    0.003    0.002   0.000%     99.956%         Const  mixed_2/tower_2/conv/batchnorm/moving_mean_quint8_const\n    1.341    0.003    0.002   0.000%     99.956%         Const  mixed_1/tower_1/conv_2/batchnorm/moving_variance_quint8_const\n    1.150    0.002    0.002   0.000%     99.956%         Const  mixed_1/tower/conv_1/conv2d_params_quint8_const\n    1.120    0.003    0.002   0.000%     99.956%         Const  mixed_1/tower/conv/batchnorm/moving_mean_max\n    0.823    0.003    0.002   0.000%     99.956%         Const  mixed/tower_1/conv/batchnorm/beta_quint8_const\n    0.651    0.003    0.002   0.000%     99.956%         Const  mixed/tower/conv/batchnorm/beta_quint8_const\n    0.519    0.003    0.002   0.000%     99.956%         Const  conv_4/batchnorm/moving_mean_quint8_const\n    0.502    0.003    0.002   0.000%     99.956%         Const  conv_3/batchnorm/gamma_quint8_const\n    0.456    0.003    0.002   0.000%     99.956%         Const  conv_3/conv2d_params_min\n    0.365    0.002    0.002   0.000%     99.956%         Const  conv_1/batchnorm/moving_variance_min\n    5.684    0.003    0.002   0.000%     99.956%         Const  mixed_9/tower_1/mixed/conv_1/conv2d_params_min\n    4.292    0.002    0.002   0.000%     99.956%         Const  mixed_8/tower/conv/batchnorm/moving_mean_quint8_const\n    4.238    0.002    0.002   0.000%     99.956%         Const  mixed_7/tower_2/conv/batchnorm/moving_mean_quint8_const\n    3.968    0.003    0.002   0.000%     99.956%         Const  mixed_7/tower_1/conv/batchnorm/moving_mean_quint8_const\n    3.832    0.003    0.002   0.000%     99.957%         Const  mixed_7/tower/conv/batchnorm/beta_quint8_const\n    3.586    0.003    0.002   0.000%     99.957%         Const  mixed_6/tower_1/conv_3/batchnorm/moving_mean_quint8_const\n    3.532    0.003    0.002   0.000%     99.957%         Const  mixed_6/tower_1/conv_2/batchnorm/moving_mean_quint8_const\n    3.423    0.002    0.002   0.000%     99.957%         Const  mixed_6/tower_1/conv/batchnorm/moving_mean_quint8_const\n    3.207    0.003    0.002   0.000%     99.957%         Const  mixed_6/conv/batchnorm/moving_mean_quint8_const\n    2.354    0.003    0.002   0.000%     99.957%         Const  mixed_4/tower_1/conv_1/batchnorm/moving_mean_min\n    2.160    0.002    0.002   0.000%     99.957%         Const  mixed_4/tower/conv/batchnorm/beta_max\n    1.668    0.003    0.002   0.000%     99.957%         Const  mixed_2/tower_1/conv_1/batchnorm/moving_variance_quint8_const\n    1.382    0.006    0.002   0.000%     99.957%         Const  mixed_1/tower_2/conv/batchnorm/moving_mean_quint8_const\n    1.209    0.005    0.002   0.000%     99.957%         Const  mixed_1/tower_1/conv/conv2d_params_min\n    1.179    0.002    0.002   0.000%     99.957%         Const  mixed_1/tower/conv_1/batchnorm/moving_variance_quint8_const\n    1.070    0.003    0.002   0.000%     99.957%         Const  mixed_1/conv/batchnorm/beta_quint8_const\n    0.991    0.003    0.002   0.000%     99.957%         Const  mixed/tower_2/conv/batchnorm/moving_mean_max\n    0.930    0.003    0.002   0.000%     99.957%         Const  mixed/tower_1/conv_2/batchnorm/moving_mean_max\n    0.799    0.004    0.002   0.000%     99.957%         Const  mixed/tower_1/conv/batchnorm/moving_variance_quint8_const\n    0.759    0.004    0.002   0.000%     99.957%         Const  mixed/tower_1/conv/conv2d_params_min\n    0.731    0.004    0.002   0.000%     99.957%         Const  mixed/tower/conv_1/batchnorm/beta_quint8_const\n    0.671    0.005    0.002   0.000%     99.957%         Const  mixed/tower/conv_1/conv2d_params_quint8_const\n    0.541    0.002    0.002   0.000%     99.957%         Const  conv_4/batchnorm/moving_variance_max\n    0.494    0.002    0.002   0.000%     99.958%         Const  conv_3/batchnorm/beta_min\n    0.346    0.003    0.002   0.000%     99.958%         Const  conv_1/batchnorm/moving_mean_quint8_const\n    5.306    0.003    0.001   0.000%     99.958%         Const  softmax/logits/MatMul_eightbit_reshape_dims\n    4.811    0.003    0.001   0.000%     99.958%         Const  mixed_10/tower/conv/batchnorm/moving_mean_quint8_const\n    3.616    0.008    0.001   0.000%     99.958%         Const  mixed_6/tower_1/conv_3/batchnorm/beta_min\n    3.612    0.002    0.001   0.000%     99.958%         Const  mixed_6/tower_1/conv_3/batchnorm/beta_quint8_const\n    3.503    0.005    0.001   0.000%     99.958%         Const  mixed_6/tower_1/conv_1/batchnorm/beta_quint8_const\n    2.059    0.002    0.001   0.000%     99.958%         Const  mixed_4/conv/conv2d_params_min\n    1.962    0.006    0.001   0.000%     99.958%         Const  mixed_3/tower/conv_1/batchnorm/moving_variance_quint8_const\n    1.330    0.003    0.001   0.000%     99.958%         Const  mixed_1/tower_1/conv_2/batchnorm/moving_mean_quint8_const\n    1.039    0.003    0.001   0.000%     99.958%         Const  mixed_1/conv/batchnorm/moving_mean_quint8_const\n    0.973    0.002    0.001   0.000%     99.958%         Const  mixed/tower_2/conv/conv2d_params_min\n    0.909    0.003    0.001   0.000%     99.958%         Const  mixed/tower_1/conv_2/conv2d_params_min\n    0.577    0.005    0.001   0.000%     99.958%         Const  mixed/conv/batchnorm/moving_mean_min\n    5.342    0.003    0.001   0.000%     99.958%         Const  mixed_9/tower/conv/conv2d_params_min\n    5.285    0.003    0.001   0.000%     99.958%         Const  pool_3/_reshape/shape\n    4.456    0.003    0.001   0.000%     99.958%         Const  mixed_8/tower_1/conv_1/batchnorm/moving_mean_quint8_const\n    3.914    0.003    0.001   0.000%     99.958%         Const  mixed_7/tower/conv_2/batchnorm/moving_mean_quint8_const\n    3.655    0.003    0.001   0.000%     99.958%         Const  mixed_6/tower_1/conv_4/batchnorm/moving_variance_quint8_const\n    3.644    0.002    0.001   0.000%     99.958%         Const  mixed_6/tower_1/conv_4/batchnorm/moving_mean_quint8_const\n    3.557    0.003    0.001   0.000%     99.959%         Const  mixed_6/tower_1/conv_2/batchnorm/beta_quint8_const\n    2.791    0.002    0.001   0.000%     99.959%         Const  mixed_5/tower/conv_1/batchnorm/beta_quint8_const\n    2.716    0.003    0.001   0.000%     99.959%         Const  mixed_5/tower/conv/batchnorm/beta_quint8_const\n    2.428    0.002    0.001   0.000%     99.959%         Const  mixed_4/tower_1/conv_2/batchnorm/moving_variance_min\n    2.271    0.003    0.001   0.000%     99.959%         Const  mixed_4/tower/conv_2/batchnorm/beta_quint8_const\n    2.241    0.004    0.001   0.000%     99.959%         Const  mixed_4/tower/conv_2/batchnorm/moving_mean_quint8_const\n    2.184    0.005    0.001   0.000%     99.959%         Const  mixed_4/tower/conv_1/batchnorm/moving_mean_quint8_const\n    2.153    0.003    0.001   0.000%     99.959%         Const  mixed_4/tower/conv/batchnorm/beta_quint8_const\n    1.825    0.005    0.001   0.000%     99.959%         Const  mixed_3/conv/batchnorm/moving_mean_max\n    1.602    0.005    0.001   0.000%     99.959%         Const  mixed_2/tower_1/conv/batchnorm/moving_mean_quint8_const\n    1.429    0.002    0.001   0.000%     99.959%         Const  mixed_2/conv/conv2d_params_min\n    1.378    0.003    0.001   0.000%     99.959%         Const  mixed_1/tower_2/conv/conv2d_params_max\n    1.275    0.003    0.001   0.000%     99.959%         Const  mixed_1/tower_1/conv_1/batchnorm/moving_mean_quint8_const\n    1.234    0.003    0.001   0.000%     99.959%         Const  mixed_1/tower_1/conv/batchnorm/moving_variance_quint8_const\n    1.097    0.003    0.001   0.000%     99.959%         Const  mixed_1/tower/conv/batchnorm/moving_mean_quint8_const\n    0.638    0.003    0.001   0.000%     99.959%         Const  mixed/tower/conv/batchnorm/moving_variance_quint8_const\n    0.596    0.003    0.001   0.000%     99.959%         Const  mixed/conv/batchnorm/moving_variance_max\n    0.545    0.002    0.001   0.000%     99.959%         Const  conv_4/batchnorm/beta_quint8_const\n    0.383    0.003    0.001   0.000%     99.959%         Const  conv_1/batchnorm/beta_max\n    5.365    0.003    0.001   0.000%     99.959%         Const  mixed_9/tower/conv/batchnorm/moving_variance_quint8_const\n    5.153    0.003    0.001   0.000%     99.960%         Const  mixed_10/tower_1/mixed/conv/batchnorm/beta_quint8_const\n    4.765    0.003    0.001   0.000%     99.960%         Const  mixed_10/conv/batchnorm/moving_variance_quint8_const\n    4.650    0.002    0.001   0.000%     99.960%         Const  mixed_9/tower_2/conv/batchnorm/beta_max\n    4.615    0.005    0.001   0.000%     99.960%         Const  mixed_9/conv/batchnorm/moving_mean_quint8_const\n    4.346    0.002    0.001   0.000%     99.960%         Const  mixed_8/tower/conv_1/batchnorm/moving_mean_quint8_const\n    4.266    0.003    0.001   0.000%     99.960%         Const  mixed_7/tower_2/conv/batchnorm/beta_quint8_const\n    4.129    0.003    0.001   0.000%     99.960%         Const  mixed_7/tower_1/conv_3/batchnorm/moving_mean_quint8_const\n    3.777    0.003    0.001   0.000%     99.960%         Const  mixed_7/conv/batchnorm/beta_quint8_const\n    3.448    0.006    0.001   0.000%     99.960%         Const  mixed_6/tower_1/conv/batchnorm/beta_quint8_const\n    3.341    0.002    0.001   0.000%     99.960%         Const  mixed_6/tower/conv_1/batchnorm/beta_quint8_const\n    2.987    0.003    0.001   0.000%     99.960%         Const  mixed_5/tower_1/conv_2/batchnorm/moving_mean_quint8_const\n    2.932    0.003    0.001   0.000%     99.960%         Const  mixed_5/tower_1/conv_1/batchnorm/moving_mean_quint8_const\n    2.764    0.003    0.001   0.000%     99.960%         Const  mixed_5/tower/conv_1/batchnorm/moving_mean_quint8_const\n    2.463    0.003    0.001   0.000%     99.960%         Const  mixed_4/tower_1/conv_3/batchnorm/moving_mean_quint8_const\n    2.424    0.003    0.001   0.000%     99.960%         Const  mixed_4/tower_1/conv_2/batchnorm/moving_variance_quint8_const\n    2.076    0.002    0.001   0.000%     99.960%         Const  mixed_4/conv/batchnorm/moving_mean_max\n    2.014    0.003    0.001   0.000%     99.960%         Const  mixed_3/tower/conv_2/batchnorm/moving_mean_quint8_const\n    1.976    0.002    0.001   0.000%     99.960%         Const  mixed_3/tower/conv_1/batchnorm/beta_quint8_const\n    1.842    0.006    0.001   0.000%     99.960%         Const  mixed_3/conv/batchnorm/beta_quint8_const\n    1.832    0.002    0.001   0.000%     99.960%         Const  mixed_3/conv/batchnorm/moving_variance_quint8_const\n    1.708    0.003    0.001   0.000%     99.961%         Const  mixed_2/tower_1/conv_2/batchnorm/moving_mean_quint8_const\n    1.450    0.002    0.001   0.000%     99.961%         Const  mixed_2/conv/batchnorm/moving_variance_quint8_const\n    1.363    0.006    0.001   0.000%     99.961%         Const  mixed_1/tower_1/conv_2/batchnorm/beta_max\n    1.324    0.005    0.001   0.000%     99.961%         Const  mixed_1/tower_1/conv_2/conv2d_params_max\n    1.251    0.004    0.001   0.000%     99.961%         Const  mixed_1/tower_1/conv/batchnorm/beta_min\n    1.154    0.005    0.001   0.000%     99.961%         Const  mixed_1/tower/conv_1/conv2d_params_min\n    1.011    0.003    0.001   0.000%     99.961%         Const  mixed/tower_2/conv/batchnorm/beta_quint8_const\n    0.939    0.008    0.001   0.000%     99.961%         Const  mixed/tower_1/conv_2/batchnorm/moving_variance_min\n    0.605    0.002    0.001   0.000%     99.961%         Const  mixed/conv/batchnorm/beta_min\n    0.473    0.002    0.001   0.000%     99.961%         Const  conv_3/batchnorm/moving_mean_max\n    0.407    0.002    0.001   0.000%     99.961%         Const  conv_2/batchnorm/moving_mean_min\n    0.296    0.003    0.001   0.000%     99.961%         Const  conv/batchnorm/moving_variance_quint8_const\n    5.583    0.002    0.001   0.000%     99.961%         Const  mixed_9/tower_1/conv_1/batchnorm/moving_mean_quint8_const\n    4.885    0.003    0.001   0.000%     99.961%         Const  mixed_10/tower/mixed/conv/batchnorm/moving_mean_quint8_const\n    4.825    0.003    0.001   0.000%     99.961%         Const  mixed_10/tower/conv/batchnorm/moving_variance_quint8_const\n    4.759    0.005    0.001   0.000%     99.961%         Const  mixed_10/conv/batchnorm/moving_mean_max\n    4.743    0.003    0.001   0.000%     99.961%         Const  mixed_10/conv/conv2d_params_min\n    4.589    0.002    0.001   0.000%     99.961%         Const  mixed_8/tower_1/conv_3/batchnorm/beta_quint8_const\n    4.563    0.003    0.001   0.000%     99.961%         Const  mixed_8/tower_1/conv_3/batchnorm/moving_mean_quint8_const\n    4.510    0.003    0.001   0.000%     99.961%         Const  mixed_8/tower_1/conv_2/batchnorm/moving_mean_quint8_const\n    4.417    0.003    0.001   0.000%     99.962%         Const  mixed_8/tower_1/conv/batchnorm/moving_variance_quint8_const\n    4.320    0.002    0.001   0.000%     99.962%         Const  mixed_8/tower/conv/batchnorm/beta_quint8_const\n    4.229    0.003    0.001   0.000%     99.962%         Const  mixed_7/tower_2/conv/conv2d_params_min\n    4.195    0.003    0.001   0.000%     99.962%         Const  mixed_7/tower_1/conv_4/batchnorm/moving_variance_quint8_const\n    4.076    0.003    0.001   0.000%     99.962%         Const  mixed_7/tower_1/conv_2/batchnorm/moving_mean_quint8_const\n    4.022    0.003    0.001   0.000%     99.962%         Const  mixed_7/tower_1/conv_1/batchnorm/moving_mean_quint8_const\n    3.940    0.005    0.001   0.000%     99.962%         Const  mixed_7/tower/conv_2/batchnorm/beta_quint8_const\n    3.872    0.003    0.001   0.000%     99.962%         Const  mixed_7/tower/conv_1/batchnorm/moving_variance_quint8_const\n    3.763    0.002    0.001   0.000%     99.962%         Const  mixed_7/conv/batchnorm/moving_variance_quint8_const\n    3.441    0.002    0.001   0.000%     99.962%         Const  mixed_6/tower_1/conv/batchnorm/moving_variance_min\n    3.360    0.003    0.001   0.000%     99.962%         Const  mixed_6/tower/conv_2/conv2d_params_min\n    3.165    0.005    0.001   0.000%     99.962%         Const  mixed_5/tower_2/conv/batchnorm/moving_variance_quint8_const\n    3.070    0.005    0.001   0.000%     99.962%         Const  mixed_5/tower_1/conv_3/batchnorm/beta_quint8_const\n    3.013    0.005    0.001   0.000%     99.962%         Const  mixed_5/tower_1/conv_2/batchnorm/beta_quint8_const\n    2.662    0.002    0.001   0.000%     99.962%         Const  mixed_5/conv/batchnorm/beta_quint8_const\n    2.567    0.002    0.001   0.000%     99.962%         Const  mixed_4/tower_2/conv/conv2d_params_min\n    2.523    0.002    0.001   0.000%     99.962%         Const  mixed_4/tower_1/conv_4/batchnorm/moving_mean_quint8_const\n    2.435    0.005    0.001   0.000%     99.962%         Const  mixed_4/tower_1/conv_2/batchnorm/beta_quint8_const\n    2.139    0.003    0.001   0.000%     99.962%         Const  mixed_4/tower/conv/batchnorm/moving_variance_quint8_const\n    1.923    0.002    0.001   0.000%     99.962%         Const  mixed_3/tower/conv/batchnorm/beta_quint8_const\n    1.912    0.002    0.001   0.000%     99.963%         Const  mixed_3/tower/conv/batchnorm/moving_variance_quint8_const\n    1.777    0.002    0.001   0.000%     99.963%         Const  mixed_2/tower_2/conv/batchnorm/moving_variance_quint8_const\n    1.576    0.003    0.001   0.000%     99.963%         Const  mixed_2/tower/conv_1/batchnorm/beta_quint8_const\n    1.305    0.005    0.001   0.000%     99.963%         Const  mixed_1/tower_1/conv_1/batchnorm/beta_min\n    1.297    0.003    0.001   0.000%     99.963%         Const  mixed_1/tower_1/conv_1/batchnorm/moving_variance_max\n    0.960    0.006    0.001   0.000%     99.963%         Const  mixed/tower_1/conv_2/batchnorm/beta_max\n    0.630    0.003    0.001   0.000%     99.963%         Const  mixed/tower/conv/batchnorm/moving_mean_min\n    0.429    0.003    0.001   0.000%     99.963%         Const  conv_2/batchnorm/beta_quint8_const\n    0.398    0.002    0.001   0.000%     99.963%         Const  conv_2/conv2d_params_max\n    0.340    0.003    0.001   0.000%     99.963%         Const  conv_1/conv2d_params_max\n    0.301    0.002    0.001   0.000%     99.963%         Const  conv/batchnorm/moving_variance_min\n    5.766    0.003    0.001   0.000%     99.963%         Const  mixed_9/tower_2/conv/batchnorm/moving_variance_quint8_const\n    5.750    0.006    0.001   0.000%     99.963%         Const  mixed_9/tower_2/conv/batchnorm/moving_mean_quint8_const\n    5.695    0.003    0.001   0.000%     99.963%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/moving_mean_quint8_const\n    5.594    0.006    0.001   0.000%     99.963%         Const  mixed_9/tower_1/conv_1/batchnorm/moving_variance_quint8_const\n    5.552    0.005    0.001   0.000%     99.963%         Const  mixed_9/tower_1/conv/batchnorm/beta_quint8_const\n    5.494    0.006    0.001   0.000%     99.963%         Const  mixed_9/tower/mixed/conv_1/batchnorm/beta_quint8_const\n    5.473    0.007    0.001   0.000%     99.963%         Const  mixed_9/tower/mixed/conv_1/batchnorm/moving_mean_max\n    5.408    0.002    0.001   0.000%     99.963%         Const  mixed_9/tower/mixed/conv/batchnorm/moving_mean_quint8_const\n    5.380    0.003    0.001   0.000%     99.963%         Const  mixed_9/tower/conv/batchnorm/beta_quint8_const\n    5.243    0.003    0.001   0.000%     99.964%         Const  mixed_10/tower_2/conv/batchnorm/moving_mean_quint8_const\n    5.034    0.002    0.001   0.000%     99.964%         Const  mixed_10/tower_1/conv/batchnorm/beta_min\n    4.902    0.002    0.001   0.000%     99.964%         Const  mixed_10/tower/mixed/conv/batchnorm/moving_variance_quint8_const\n    4.777    0.005    0.001   0.000%     99.964%         Const  mixed_10/conv/batchnorm/beta_quint8_const\n    4.751    0.002    0.001   0.000%     99.964%         Const  mixed_10/conv/batchnorm/moving_mean_quint8_const\n    4.428    0.005    0.001   0.000%     99.964%         Const  mixed_8/tower_1/conv/batchnorm/beta_quint8_const\n    4.401    0.003    0.001   0.000%     99.964%         Const  mixed_8/tower_1/conv/batchnorm/moving_mean_quint8_const\n    4.360    0.003    0.001   0.000%     99.964%         Const  mixed_8/tower/conv_1/batchnorm/moving_variance_quint8_const\n    3.983    0.002    0.001   0.000%     99.964%         Const  mixed_7/tower_1/conv/batchnorm/moving_variance_quint8_const\n    3.922    0.006    0.001   0.000%     99.964%         Const  mixed_7/tower/conv_2/batchnorm/moving_mean_max\n    3.907    0.002    0.001   0.000%     99.964%         Const  mixed_7/tower/conv_2/conv2d_params_min\n    3.741    0.002    0.001   0.000%     99.964%         Const  mixed_7/conv/conv2d_params_min\n    3.709    0.002    0.001   0.000%     99.964%         Const  mixed_6/tower_2/conv/batchnorm/moving_variance_quint8_const\n    3.633    0.003    0.001   0.000%     99.964%         Const  mixed_6/tower_1/conv_4/conv2d_params_min\n    3.444    0.003    0.001   0.000%     99.964%         Const  mixed_6/tower_1/conv/batchnorm/moving_variance_max\n    3.437    0.003    0.001   0.000%     99.964%         Const  mixed_6/tower_1/conv/batchnorm/moving_variance_quint8_const\n    3.311    0.003    0.001   0.000%     99.964%         Const  mixed_6/tower/conv_1/batchnorm/moving_mean_quint8_const\n    3.225    0.002    0.001   0.000%     99.964%         Const  mixed_6/conv/batchnorm/moving_variance_min\n    3.178    0.003    0.001   0.000%     99.964%         Const  mixed_5/tower_2/conv/batchnorm/beta_quint8_const\n    2.946    0.002    0.001   0.000%     99.964%         Const  mixed_5/tower_1/conv_1/batchnorm/moving_variance_quint8_const\n    2.925    0.002    0.001   0.000%     99.964%         Const  mixed_5/tower_1/conv_1/conv2d_params_min\n    2.892    0.003    0.001   0.000%     99.965%         Const  mixed_5/tower_1/conv/batchnorm/moving_variance_quint8_const\n    2.878    0.002    0.001   0.000%     99.965%         Const  mixed_5/tower_1/conv/batchnorm/moving_mean_quint8_const\n    2.853    0.003    0.001   0.000%     99.965%         Const  mixed_5/tower/conv_2/batchnorm/beta_max\n    2.846    0.003    0.001   0.000%     99.965%         Const  mixed_5/tower/conv_2/batchnorm/beta_quint8_const\n    2.706    0.002    0.001   0.000%     99.965%         Const  mixed_5/tower/conv/batchnorm/moving_variance_min\n    2.633    0.006    0.001   0.000%     99.965%         Const  mixed_5/conv/batchnorm/moving_mean_quint8_const\n    2.588    0.005    0.001   0.000%     99.965%         Const  mixed_4/tower_2/conv/batchnorm/moving_variance_quint8_const\n    2.453    0.005    0.001   0.000%     99.965%         Const  mixed_4/tower_1/conv_3/conv2d_params_min\n    2.147    0.005    0.001   0.000%     99.965%         Const  mixed_4/tower/conv/batchnorm/moving_variance_max\n    2.080    0.002    0.001   0.000%     99.965%         Const  mixed_4/conv/batchnorm/moving_variance_quint8_const\n    2.025    0.003    0.001   0.000%     99.965%         Const  mixed_3/tower/conv_2/batchnorm/moving_variance_quint8_const\n    1.767    0.005    0.001   0.000%     99.965%         Const  mixed_2/tower_2/conv/batchnorm/moving_mean_min\n    1.626    0.002    0.001   0.000%     99.965%         Const  mixed_2/tower_1/conv/batchnorm/moving_variance_max\n    1.551    0.002    0.001   0.000%     99.965%         Const  mixed_2/tower/conv_1/batchnorm/moving_mean_quint8_const\n    1.410    0.003    0.001   0.000%     99.965%         Const  mixed_1/tower_2/conv/batchnorm/beta_quint8_const\n    1.397    0.002    0.001   0.000%     99.965%         Const  mixed_1/tower_2/conv/batchnorm/moving_variance_quint8_const\n    1.338    0.002    0.001   0.000%     99.965%         Const  mixed_1/tower_1/conv_2/batchnorm/moving_mean_max\n    1.257    0.002    0.001   0.000%     99.965%         Const  mixed_1/tower_1/conv/batchnorm/beta_max\n    1.227    0.003    0.001   0.000%     99.965%         Const  mixed_1/tower_1/conv/batchnorm/moving_mean_max\n    1.160    0.003    0.001   0.000%     99.965%         Const  mixed_1/tower/conv_1/conv2d_params_max\n    1.031    0.003    0.001   0.000%     99.965%         Const  mixed_1/conv/conv2d_params_min\n    0.891    0.002    0.001   0.000%     99.966%         Const  mixed/tower_1/conv_1/batchnorm/beta_max\n    0.887    0.002    0.001   0.000%     99.966%         Const  mixed/tower_1/conv_1/batchnorm/beta_min\n    0.874    0.003    0.001   0.000%     99.966%         Const  mixed/tower_1/conv_1/batchnorm/moving_variance_max\n    0.862    0.003    0.001   0.000%     99.966%         Const  mixed/tower_1/conv_1/batchnorm/moving_mean_max\n    0.647    0.002    0.001   0.000%     99.966%         Const  mixed/tower/conv/batchnorm/moving_variance_max\n    0.643    0.002    0.001   0.000%     99.966%         Const  mixed/tower/conv/batchnorm/moving_variance_min\n    0.634    0.003    0.001   0.000%     99.966%         Const  mixed/tower/conv/batchnorm/moving_mean_max\n    0.601    0.002    0.001   0.000%     99.966%         Const  mixed/conv/batchnorm/beta_quint8_const\n    0.485    0.003    0.001   0.000%     99.966%         Const  conv_3/batchnorm/moving_variance_max\n    5.664    0.003    0.001   0.000%     99.966%         Const  mixed_9/tower_1/mixed/conv/batchnorm/beta_quint8_const\n    5.426    0.003    0.001   0.000%     99.966%         Const  mixed_9/tower/mixed/conv/batchnorm/moving_variance_min\n    5.270    0.005    0.001   0.000%     99.966%         Const  mixed_10/tower_2/conv/batchnorm/beta_quint8_const\n    5.235    0.003    0.001   0.000%     99.966%         Const  mixed_10/tower_2/conv/conv2d_params_min\n    5.041    0.003    0.001   0.000%     99.966%         Const  mixed_10/tower_1/conv/batchnorm/gamma_quint8_const\n    5.030    0.002    0.001   0.000%     99.966%         Const  mixed_10/tower_1/conv/batchnorm/beta_quint8_const\n    4.960    0.003    0.001   0.000%     99.966%         Const  mixed_10/tower/mixed/conv_1/batchnorm/moving_variance_quint8_const\n    4.937    0.003    0.001   0.000%     99.966%         Const  mixed_10/tower/mixed/conv_1/conv2d_params_min\n    4.829    0.002    0.001   0.000%     99.966%         Const  mixed_10/tower/conv/batchnorm/moving_variance_min\n    4.642    0.003    0.001   0.000%     99.966%         Const  mixed_9/conv/batchnorm/beta_quint8_const\n    4.567    0.003    0.001   0.000%     99.966%         Const  mixed_8/tower_1/conv_3/batchnorm/moving_mean_min\n    4.364    0.002    0.001   0.000%     99.967%         Const  mixed_8/tower/conv_1/batchnorm/moving_variance_min\n    4.156    0.002    0.001   0.000%     99.967%         Const  mixed_7/tower_1/conv_3/batchnorm/beta_quint8_const\n    4.141    0.002    0.001   0.000%     99.967%         Const  mixed_7/tower_1/conv_3/batchnorm/moving_variance_quint8_const\n    4.066    0.003    0.001   0.000%     99.967%         Const  mixed_7/tower_1/conv_2/conv2d_params_min\n    3.858    0.003    0.001   0.000%     99.967%         Const  mixed_7/tower/conv_1/batchnorm/moving_mean_quint8_const\n    3.759    0.002    0.001   0.000%     99.967%         Const  mixed_7/conv/batchnorm/moving_mean_max\n    3.672    0.003    0.001   0.000%     99.967%         Const  mixed_6/tower_1/conv_4/batchnorm/beta_min\n    3.522    0.005    0.001   0.000%     99.967%         Const  mixed_6/tower_1/conv_2/conv2d_params_min\n    3.513    0.003    0.001   0.000%     99.967%         Const  mixed_6/tower_1/conv_1/batchnorm/beta_max\n    3.326    0.003    0.001   0.000%     99.967%         Const  mixed_6/tower/conv_1/batchnorm/moving_variance_quint8_const\n    3.272    0.002    0.001   0.000%     99.967%         Const  mixed_6/tower/conv/batchnorm/moving_variance_quint8_const\n    3.219    0.002    0.001   0.000%     99.967%         Const  mixed_6/conv/batchnorm/moving_variance_quint8_const\n    3.032    0.008    0.001   0.000%     99.967%         Const  mixed_5/tower_1/conv_3/conv2d_params_min\n    2.906    0.003    0.001   0.000%     99.967%         Const  mixed_5/tower_1/conv/batchnorm/beta_quint8_const\n    2.787    0.003    0.001   0.000%     99.967%         Const  mixed_5/tower/conv_1/batchnorm/moving_variance_max\n    2.776    0.003    0.001   0.000%     99.967%         Const  mixed_5/tower/conv_1/batchnorm/moving_variance_quint8_const\n    2.769    0.002    0.001   0.000%     99.967%         Const  mixed_5/tower/conv_1/batchnorm/moving_mean_min\n    2.577    0.002    0.001   0.000%     99.967%         Const  mixed_4/tower_2/conv/batchnorm/moving_mean_quint8_const\n    2.381    0.006    0.001   0.000%     99.967%         Const  mixed_4/tower_1/conv_1/batchnorm/beta_quint8_const\n    2.350    0.003    0.001   0.000%     99.967%         Const  mixed_4/tower_1/conv_1/batchnorm/moving_mean_quint8_const\n    2.343    0.002    0.001   0.000%     99.967%         Const  mixed_4/tower_1/conv_1/conv2d_params_min\n    2.254    0.002    0.001   0.000%     99.968%         Const  mixed_4/tower/conv_2/batchnorm/moving_mean_max\n    2.230    0.003    0.001   0.000%     99.968%         Const  mixed_4/tower/conv_2/conv2d_params_min\n    2.047    0.006    0.001   0.000%     99.968%         Const  mixed_3/tower/conv_2/batchnorm/beta_max\n    2.022    0.002    0.001   0.000%     99.968%         Const  mixed_3/tower/conv_2/batchnorm/moving_mean_max\n    1.951    0.003    0.001   0.000%     99.968%         Const  mixed_3/tower/conv_1/batchnorm/moving_mean_quint8_const\n    1.933    0.003    0.001   0.000%     99.968%         Const  mixed_3/tower/conv/batchnorm/beta_max\n    1.629    0.003    0.001   0.000%     99.968%         Const  mixed_2/tower_1/conv/batchnorm/beta_quint8_const\n    1.519    0.003    0.001   0.000%     99.968%         Const  mixed_2/tower/conv/batchnorm/beta_quint8_const\n    1.301    0.002    0.001   0.000%     99.968%         Const  mixed_1/tower_1/conv_1/batchnorm/beta_quint8_const\n    1.074    0.003    0.001   0.000%     99.968%         Const  mixed_1/conv/batchnorm/beta_min\n    1.063    0.006    0.001   0.000%     99.968%         Const  mixed_1/conv/batchnorm/moving_variance_max\n    1.055    0.003    0.001   0.000%     99.968%         Const  mixed_1/conv/batchnorm/moving_variance_quint8_const\n    0.738    0.004    0.001   0.000%     99.968%         Const  mixed/tower/conv_1/batchnorm/beta_min\n    0.724    0.004    0.001   0.000%     99.968%         Const  mixed/tower/conv_1/batchnorm/moving_variance_max\n    0.524    0.002    0.001   0.000%     99.968%         Const  conv_4/batchnorm/moving_mean_min\n    0.511    0.003    0.001   0.000%     99.968%         Const  conv_4/conv2d_params_min\n    0.498    0.002    0.001   0.000%     99.968%         Const  conv_3/batchnorm/beta_max\n    0.460    0.003    0.001   0.000%     99.968%         Const  conv_3/conv2d_params_max\n    0.433    0.003    0.001   0.000%     99.968%         Const  conv_2/batchnorm/beta_min\n    0.421    0.002    0.001   0.000%     99.968%         Const  conv_2/batchnorm/moving_variance_min\n    0.393    0.003    0.001   0.000%     99.968%         Const  conv_2/conv2d_params_min\n    0.287    0.003    0.001   0.000%     99.969%         Const  conv/batchnorm/moving_mean_min\n    5.540    0.003    0.001   0.000%     99.969%         Const  mixed_9/tower_1/conv/batchnorm/moving_variance_quint8_const\n    5.138    0.003    0.001   0.000%     99.969%         Const  mixed_10/tower_1/mixed/conv/batchnorm/moving_variance_quint8_const\n    4.876    0.003    0.001   0.000%     99.969%         Const  mixed_10/tower/mixed/conv/conv2d_params_min\n    4.821    0.003    0.001   0.000%     99.969%         Const  mixed_10/tower/conv/batchnorm/moving_mean_max\n    4.787    0.003    0.001   0.000%     99.969%         Const  mixed_10/conv/batchnorm/beta_max\n    4.638    0.003    0.001   0.000%     99.969%         Const  mixed_9/conv/batchnorm/moving_variance_max\n    4.521    0.005    0.001   0.000%     99.969%         Const  mixed_8/tower_1/conv_2/batchnorm/moving_variance_quint8_const\n    4.390    0.006    0.001   0.000%     99.969%         Const  mixed_8/tower_1/conv/conv2d_params_min\n    4.048    0.003    0.001   0.000%     99.969%         Const  mixed_7/tower_1/conv_1/batchnorm/beta_quint8_const\n    4.004    0.003    0.001   0.000%     99.969%         Const  mixed_7/tower_1/conv/batchnorm/beta_max\n    3.851    0.002    0.001   0.000%     99.969%         Const  mixed_7/tower/conv_1/conv2d_params_min\n    3.539    0.003    0.001   0.000%     99.969%         Const  mixed_6/tower_1/conv_2/batchnorm/moving_mean_max\n    3.492    0.002    0.001   0.000%     99.969%         Const  mixed_6/tower_1/conv_1/batchnorm/moving_variance_quint8_const\n    3.322    0.003    0.001   0.000%     99.969%         Const  mixed_6/tower/conv_1/batchnorm/moving_mean_max\n    3.304    0.003    0.001   0.000%     99.969%         Const  mixed_6/tower/conv_1/conv2d_params_min\n    3.286    0.002    0.001   0.000%     99.969%         Const  mixed_6/tower/conv/batchnorm/beta_quint8_const\n    3.088    0.003    0.001   0.000%     99.969%         Const  mixed_5/tower_1/conv_4/conv2d_params_min\n    3.023    0.003    0.001   0.000%     99.969%         Const  mixed_5/tower_1/conv_2/batchnorm/beta_max\n    3.002    0.002    0.001   0.000%     99.969%         Const  mixed_5/tower_1/conv_2/batchnorm/moving_variance_quint8_const\n    2.942    0.002    0.001   0.000%     99.969%         Const  mixed_5/tower_1/conv_1/batchnorm/moving_mean_max\n    2.882    0.006    0.001   0.000%     99.970%         Const  mixed_5/tower_1/conv/batchnorm/moving_mean_min\n    2.798    0.005    0.001   0.000%     99.970%         Const  mixed_5/tower/conv_1/batchnorm/beta_max\n    2.773    0.002    0.001   0.000%     99.970%         Const  mixed_5/tower/conv_1/batchnorm/moving_mean_max\n    2.702    0.002    0.001   0.000%     99.970%         Const  mixed_5/tower/conv/batchnorm/moving_variance_quint8_const\n    2.698    0.003    0.001   0.000%     99.970%         Const  mixed_5/tower/conv/batchnorm/moving_mean_max\n    2.648    0.002    0.001   0.000%     99.970%         Const  mixed_5/conv/batchnorm/moving_variance_quint8_const\n    2.534    0.005    0.001   0.000%     99.970%         Const  mixed_4/tower_1/conv_4/batchnorm/moving_variance_quint8_const\n    2.513    0.002    0.001   0.000%     99.970%         Const  mixed_4/tower_1/conv_4/conv2d_params_min\n    2.410    0.003    0.001   0.000%     99.970%         Const  mixed_4/tower_1/conv_2/batchnorm/moving_mean_quint8_const\n    2.298    0.002    0.001   0.000%     99.970%         Const  mixed_4/tower_1/conv/batchnorm/moving_mean_quint8_const\n    2.265    0.005    0.001   0.000%     99.970%         Const  mixed_4/tower/conv_2/batchnorm/moving_variance_max\n    2.198    0.002    0.001   0.000%     99.970%         Const  mixed_4/tower/conv_1/batchnorm/moving_variance_quint8_const\n    2.090    0.002    0.001   0.000%     99.970%         Const  mixed_4/conv/batchnorm/moving_variance_max\n    2.000    0.004    0.001   0.000%     99.970%         Const  mixed_3/tower/conv_2/conv2d_params_min\n    1.897    0.003    0.001   0.000%     99.970%         Const  mixed_3/tower/conv/batchnorm/moving_mean_quint8_const\n    1.887    0.005    0.001   0.000%     99.970%         Const  mixed_3/tower/conv/conv2d_params_min\n    1.810    0.002    0.001   0.000%     99.970%         Const  mixed_3/conv/conv2d_params_min\n    1.798    0.002    0.001   0.000%     99.970%         Const  mixed_2/tower_2/conv/batchnorm/beta_max\n    1.712    0.007    0.001   0.000%     99.970%         Const  mixed_2/tower_1/conv_2/batchnorm/moving_mean_min\n    1.690    0.002    0.001   0.000%     99.970%         Const  mixed_2/tower_1/conv_1/batchnorm/beta_max\n    1.615    0.003    0.001   0.000%     99.970%         Const  mixed_2/tower_1/conv/batchnorm/moving_variance_quint8_const\n    1.513    0.005    0.001   0.000%     99.971%         Const  mixed_2/tower/conv/batchnorm/moving_variance_max\n    1.268    0.006    0.001   0.000%     99.971%         Const  mixed_1/tower_1/conv_1/conv2d_params_max\n    1.265    0.002    0.001   0.000%     99.971%         Const  mixed_1/tower_1/conv_1/conv2d_params_min\n    1.246    0.003    0.001   0.000%     99.971%         Const  mixed_1/tower_1/conv/batchnorm/beta_quint8_const\n    1.059    0.003    0.001   0.000%     99.971%         Const  mixed_1/conv/batchnorm/moving_variance_min\n    0.704    0.004    0.001   0.000%     99.971%         Const  mixed/tower/conv_1/batchnorm/moving_mean_max\n    0.583    0.003    0.001   0.000%     99.971%         Const  mixed/conv/batchnorm/moving_mean_max\n    0.528    0.002    0.001   0.000%     99.971%         Const  conv_4/batchnorm/moving_mean_max\n    0.351    0.003    0.001   0.000%     99.971%         Const  conv_1/batchnorm/moving_mean_min\n    0.315    0.002    0.001   0.000%     99.971%         Const  conv/batchnorm/beta_min\n    5.609    0.002    0.001   0.000%     99.971%         Const  mixed_9/tower_1/conv_1/batchnorm/beta_quint8_const\n    5.482    0.002    0.001   0.000%     99.971%         Const  mixed_9/tower/mixed/conv_1/batchnorm/moving_variance_quint8_const\n    5.388    0.003    0.001   0.000%     99.971%         Const  mixed_9/tower/conv/batchnorm/beta_max\n    5.373    0.005    0.001   0.000%     99.971%         Const  mixed_9/tower/conv/batchnorm/moving_variance_max\n    5.190    0.003    0.001   0.000%     99.971%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/moving_mean_quint8_const\n    5.011    0.003    0.001   0.000%     99.971%         Const  mixed_10/tower_1/conv/batchnorm/moving_mean_max\n    4.803    0.003    0.001   0.000%     99.971%         Const  mixed_10/tower/conv/conv2d_params_min\n    4.632    0.005    0.001   0.000%     99.971%         Const  mixed_9/conv/batchnorm/moving_variance_min\n    4.553    0.003    0.001   0.000%     99.971%         Const  mixed_8/tower_1/conv_3/conv2d_params_min\n    4.409    0.006    0.001   0.000%     99.971%         Const  mixed_8/tower_1/conv/batchnorm/moving_mean_max\n    4.182    0.005    0.001   0.000%     99.971%         Const  mixed_7/tower_1/conv_4/batchnorm/moving_mean_quint8_const\n    4.070    0.005    0.001   0.000%     99.971%         Const  mixed_7/tower_1/conv_2/conv2d_params_max\n    3.888    0.002    0.001   0.000%     99.972%         Const  mixed_7/tower/conv_1/batchnorm/beta_quint8_const\n    3.826    0.005    0.001   0.000%     99.972%         Const  mixed_7/tower/conv/batchnorm/moving_variance_max\n    3.807    0.006    0.001   0.000%     99.972%         Const  mixed_7/tower/conv/batchnorm/moving_mean_min\n    3.800    0.002    0.001   0.000%     99.972%         Const  mixed_7/tower/conv/conv2d_params_max\n    3.773    0.003    0.001   0.000%     99.972%         Const  mixed_7/conv/batchnorm/moving_variance_max\n    3.722    0.003    0.001   0.000%     99.972%         Const  mixed_6/tower_2/conv/batchnorm/beta_quint8_const\n    3.546    0.003    0.001   0.000%     99.972%         Const  mixed_6/tower_1/conv_2/batchnorm/moving_variance_quint8_const\n    3.485    0.005    0.001   0.000%     99.972%         Const  mixed_6/tower_1/conv_1/batchnorm/moving_mean_max\n    3.397    0.002    0.001   0.000%     99.972%         Const  mixed_6/tower/conv_2/batchnorm/beta_quint8_const\n    3.372    0.005    0.001   0.000%     99.972%         Const  mixed_6/tower/conv_2/batchnorm/moving_mean_min\n    3.293    0.002    0.001   0.000%     99.972%         Const  mixed_6/tower/conv/batchnorm/beta_max\n    3.251    0.002    0.001   0.000%     99.972%         Const  mixed_6/tower/conv/conv2d_params_min\n    3.157    0.003    0.001   0.000%     99.972%         Const  mixed_5/tower_2/conv/batchnorm/moving_mean_min\n    3.052    0.005    0.001   0.000%     99.972%         Const  mixed_5/tower_1/conv_3/batchnorm/moving_mean_max\n    2.900    0.005    0.001   0.000%     99.972%         Const  mixed_5/tower_1/conv/batchnorm/moving_variance_max\n    2.828    0.002    0.001   0.000%     99.972%         Const  mixed_5/tower/conv_2/batchnorm/moving_mean_max\n    2.780    0.006    0.001   0.000%     99.972%         Const  mixed_5/tower/conv_1/batchnorm/moving_variance_min\n    2.756    0.002    0.001   0.000%     99.972%         Const  mixed_5/tower/conv_1/conv2d_params_min\n    2.651    0.006    0.001   0.000%     99.972%         Const  mixed_5/conv/batchnorm/moving_variance_min\n    2.551    0.007    0.001   0.000%     99.972%         Const  mixed_4/tower_1/conv_4/batchnorm/beta_min\n    2.477    0.002    0.001   0.000%     99.972%         Const  mixed_4/tower_1/conv_3/batchnorm/moving_variance_quint8_const\n    2.157    0.002    0.001   0.000%     99.973%         Const  mixed_4/tower/conv/batchnorm/beta_min\n    2.098    0.002    0.001   0.000%     99.973%         Const  mixed_4/conv/batchnorm/beta_min\n    2.018    0.002    0.001   0.000%     99.973%         Const  mixed_3/tower/conv_2/batchnorm/moving_mean_min\n    1.742    0.002    0.001   0.000%     99.973%         Const  mixed_2/tower_1/conv_2/batchnorm/beta_min\n    1.683    0.002    0.001   0.000%     99.973%         Const  mixed_2/tower_1/conv_1/batchnorm/beta_quint8_const\n    1.527    0.003    0.001   0.000%     99.973%         Const  mixed_2/tower/conv/batchnorm/beta_min\n    1.495    0.005    0.001   0.000%     99.973%         Const  mixed_2/tower/conv/batchnorm/moving_mean_min\n    1.375    0.002    0.001   0.000%     99.973%         Const  mixed_1/tower_2/conv/conv2d_params_min\n    1.312    0.002    0.001   0.000%     99.973%         Const  mixed_1/tower_1/conv_1/batchnorm/beta_max\n    1.242    0.003    0.001   0.000%     99.973%         Const  mixed_1/tower_1/conv/batchnorm/moving_variance_max\n    1.128    0.002    0.001   0.000%     99.973%         Const  mixed_1/tower/conv/batchnorm/moving_variance_min\n    1.089    0.003    0.001   0.000%     99.973%         Const  mixed_1/tower/conv/conv2d_params_min\n    0.948    0.003    0.001   0.000%     99.973%         Const  mixed/tower_1/conv_2/batchnorm/moving_variance_max\n    0.882    0.003    0.001   0.000%     99.973%         Const  mixed/tower_1/conv_1/batchnorm/beta_quint8_const\n    0.792    0.004    0.001   0.000%     99.973%         Const  mixed/tower_1/conv/batchnorm/moving_mean_max\n    0.745    0.004    0.001   0.000%     99.973%         Const  mixed/tower/conv_1/batchnorm/beta_max\n    0.572    0.003    0.001   0.000%     99.973%         Const  mixed/conv/batchnorm/moving_mean_quint8_const\n    0.568    0.003    0.001   0.000%     99.973%         Const  mixed/conv/conv2d_params_max\n    0.549    0.002    0.001   0.000%     99.973%         Const  conv_4/batchnorm/beta_min\n    0.369    0.003    0.001   0.000%     99.973%         Const  conv_1/batchnorm/moving_variance_max\n    5.781    0.002    0.001   0.000%     99.973%         Const  mixed_9/tower_2/conv/batchnorm/beta_quint8_const\n    5.726    0.002    0.001   0.000%     99.973%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/beta_min\n    5.722    0.002    0.001   0.000%     99.974%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/beta_quint8_const\n    5.441    0.003    0.001   0.000%     99.974%         Const  mixed_9/tower/mixed/conv/batchnorm/beta_min\n    5.369    0.003    0.001   0.000%     99.974%         Const  mixed_9/tower/conv/batchnorm/moving_variance_min\n    5.142    0.003    0.001   0.000%     99.974%         Const  mixed_10/tower_1/mixed/conv/batchnorm/moving_variance_min\n    5.077    0.002    0.001   0.000%     99.974%         Const  mixed_10/tower_1/conv_1/batchnorm/moving_variance_quint8_const\n    5.001    0.002    0.001   0.000%     99.974%         Const  mixed_10/tower_1/conv/batchnorm/moving_mean_quint8_const\n    4.993    0.003    0.001   0.000%     99.974%         Const  mixed_10/tower_1/conv/conv2d_params_min\n    4.956    0.003    0.001   0.000%     99.974%         Const  mixed_10/tower/mixed/conv_1/batchnorm/moving_mean_max\n    4.918    0.003    0.001   0.000%     99.974%         Const  mixed_10/tower/mixed/conv/batchnorm/beta_quint8_const\n    4.898    0.002    0.001   0.000%     99.974%         Const  mixed_10/tower/mixed/conv/batchnorm/moving_mean_max\n    4.629    0.002    0.001   0.000%     99.974%         Const  mixed_9/conv/batchnorm/moving_variance_quint8_const\n    4.539    0.005    0.001   0.000%     99.974%         Const  mixed_8/tower_1/conv_2/batchnorm/beta_min\n    4.371    0.006    0.001   0.000%     99.974%         Const  mixed_8/tower/conv_1/batchnorm/beta_quint8_const\n    4.310    0.002    0.001   0.000%     99.974%         Const  mixed_8/tower/conv/batchnorm/moving_variance_min\n    4.274    0.002    0.001   0.000%     99.974%         Const  mixed_7/tower_2/conv/batchnorm/beta_max\n    4.211    0.002    0.001   0.000%     99.974%         Const  mixed_7/tower_1/conv_4/batchnorm/beta_quint8_const\n    4.101    0.002    0.001   0.000%     99.974%         Const  mixed_7/tower_1/conv_2/batchnorm/beta_quint8_const\n    4.087    0.005    0.001   0.000%     99.974%         Const  mixed_7/tower_1/conv_2/batchnorm/moving_variance_quint8_const\n    4.058    0.003    0.001   0.000%     99.974%         Const  mixed_7/tower_1/conv_1/batchnorm/beta_max\n    4.030    0.003    0.001   0.000%     99.974%         Const  mixed_7/tower_1/conv_1/batchnorm/moving_mean_max\n    3.994    0.002    0.001   0.000%     99.974%         Const  mixed_7/tower_1/conv/batchnorm/beta_quint8_const\n    3.990    0.003    0.001   0.000%     99.974%         Const  mixed_7/tower_1/conv/batchnorm/moving_variance_max\n    3.958    0.005    0.001   0.000%     99.975%         Const  mixed_7/tower_1/conv/conv2d_params_min\n    3.929    0.003    0.001   0.000%     99.975%         Const  mixed_7/tower/conv_2/batchnorm/moving_variance_quint8_const\n    3.880    0.003    0.001   0.000%     99.975%         Const  mixed_7/tower/conv_1/batchnorm/moving_variance_max\n    3.840    0.002    0.001   0.000%     99.975%         Const  mixed_7/tower/conv/batchnorm/beta_max\n    3.836    0.003    0.001   0.000%     99.975%         Const  mixed_7/tower/conv/batchnorm/beta_min\n    3.796    0.002    0.001   0.000%     99.975%         Const  mixed_7/tower/conv/conv2d_params_min\n    3.605    0.002    0.001   0.000%     99.975%         Const  mixed_6/tower_1/conv_3/batchnorm/moving_variance_min\n    3.579    0.006    0.001   0.000%     99.975%         Const  mixed_6/tower_1/conv_3/conv2d_params_max\n    3.568    0.002    0.001   0.000%     99.975%         Const  mixed_6/tower_1/conv_2/batchnorm/beta_max\n    3.383    0.002    0.001   0.000%     99.975%         Const  mixed_6/tower/conv_2/batchnorm/moving_variance_quint8_const\n    3.319    0.002    0.001   0.000%     99.975%         Const  mixed_6/tower/conv_1/batchnorm/moving_mean_min\n    3.276    0.005    0.001   0.000%     99.975%         Const  mixed_6/tower/conv/batchnorm/moving_variance_min\n    3.268    0.002    0.001   0.000%     99.975%         Const  mixed_6/tower/conv/batchnorm/moving_mean_max\n    3.175    0.002    0.001   0.000%     99.975%         Const  mixed_5/tower_2/conv/batchnorm/moving_variance_max\n    3.154    0.002    0.001   0.000%     99.975%         Const  mixed_5/tower_2/conv/batchnorm/moving_mean_quint8_const\n    3.143    0.002    0.001   0.000%     99.975%         Const  mixed_5/tower_2/conv/conv2d_params_min\n    3.099    0.003    0.001   0.000%     99.975%         Const  mixed_5/tower_1/conv_4/batchnorm/moving_mean_quint8_const\n    3.062    0.003    0.001   0.000%     99.975%         Const  mixed_5/tower_1/conv_3/batchnorm/moving_variance_min\n    3.006    0.002    0.001   0.000%     99.975%         Const  mixed_5/tower_1/conv_2/batchnorm/moving_variance_min\n    2.936    0.005    0.001   0.000%     99.975%         Const  mixed_5/tower_1/conv_1/batchnorm/moving_mean_min\n    2.832    0.002    0.001   0.000%     99.975%         Const  mixed_5/tower/conv_2/batchnorm/moving_variance_quint8_const\n    2.813    0.002    0.001   0.000%     99.976%         Const  mixed_5/tower/conv_2/conv2d_params_max\n    2.695    0.002    0.001   0.000%     99.976%         Const  mixed_5/tower/conv/batchnorm/moving_mean_min\n    2.680    0.003    0.001   0.000%     99.976%         Const  mixed_5/tower/conv/conv2d_params_min\n    2.625    0.003    0.001   0.000%     99.976%         Const  mixed_5/conv/conv2d_params_min\n    2.400    0.005    0.001   0.000%     99.976%         Const  mixed_4/tower_1/conv_2/conv2d_params_min\n    2.369    0.003    0.001   0.000%     99.976%         Const  mixed_4/tower_1/conv_1/batchnorm/moving_variance_quint8_const\n    2.325    0.002    0.001   0.000%     99.976%         Const  mixed_4/tower_1/conv/batchnorm/beta_quint8_const\n    2.311    0.003    0.001   0.000%     99.976%         Const  mixed_4/tower_1/conv/batchnorm/moving_variance_quint8_const\n    2.301    0.005    0.001   0.000%     99.976%         Const  mixed_4/tower_1/conv/batchnorm/moving_mean_min\n    2.290    0.003    0.001   0.000%     99.976%         Const  mixed_4/tower_1/conv/conv2d_params_min\n    2.258    0.002    0.001   0.000%     99.976%         Const  mixed_4/tower/conv_2/batchnorm/moving_variance_quint8_const\n    2.212    0.002    0.001   0.000%     99.976%         Const  mixed_4/tower/conv_1/batchnorm/beta_quint8_const\n    2.125    0.002    0.001   0.000%     99.976%         Const  mixed_4/tower/conv/batchnorm/moving_mean_quint8_const\n    2.094    0.002    0.001   0.000%     99.976%         Const  mixed_4/conv/batchnorm/beta_quint8_const\n    2.073    0.002    0.001   0.000%     99.976%         Const  mixed_4/conv/batchnorm/moving_mean_min\n    2.040    0.002    0.001   0.000%     99.976%         Const  mixed_3/tower/conv_2/batchnorm/beta_quint8_const\n    1.969    0.002    0.001   0.000%     99.976%         Const  mixed_3/tower/conv_1/batchnorm/moving_variance_min\n    1.959    0.002    0.001   0.000%     99.976%         Const  mixed_3/tower/conv_1/batchnorm/moving_mean_max\n    1.835    0.003    0.001   0.000%     99.976%         Const  mixed_3/conv/batchnorm/moving_variance_min\n    1.724    0.002    0.001   0.000%     99.976%         Const  mixed_2/tower_1/conv_2/batchnorm/moving_variance_quint8_const\n    1.701    0.002    0.001   0.000%     99.976%         Const  mixed_2/tower_1/conv_2/conv2d_params_min\n    1.562    0.002    0.001   0.000%     99.976%         Const  mixed_2/tower/conv_1/batchnorm/moving_variance_quint8_const\n    1.407    0.002    0.001   0.000%     99.977%         Const  mixed_1/tower_2/conv/batchnorm/moving_variance_max\n    1.400    0.005    0.001   0.000%     99.977%         Const  mixed_1/tower_2/conv/batchnorm/moving_variance_min\n    1.320    0.002    0.001   0.000%     99.977%         Const  mixed_1/tower_1/conv_2/conv2d_params_min\n    1.283    0.003    0.001   0.000%     99.977%         Const  mixed_1/tower_1/conv_1/batchnorm/moving_mean_max\n    1.239    0.002    0.001   0.000%     99.977%         Const  mixed_1/tower_1/conv/batchnorm/moving_variance_min\n    1.201    0.002    0.001   0.000%     99.977%         Const  mixed_1/tower/conv_1/batchnorm/beta_max\n    1.186    0.002    0.001   0.000%     99.977%         Const  mixed_1/tower/conv_1/batchnorm/moving_variance_max\n    1.172    0.005    0.001   0.000%     99.977%         Const  mixed_1/tower/conv_1/batchnorm/moving_mean_max\n    1.142    0.003    0.001   0.000%     99.977%         Const  mixed_1/tower/conv/batchnorm/beta_min\n    1.132    0.002    0.001   0.000%     99.977%         Const  mixed_1/tower/conv/batchnorm/moving_variance_max\n    1.124    0.003    0.001   0.000%     99.977%         Const  mixed_1/tower/conv/batchnorm/moving_variance_quint8_const\n    1.007    0.003    0.001   0.000%     99.977%         Const  mixed/tower_2/conv/batchnorm/moving_variance_max\n    0.999    0.003    0.001   0.000%     99.977%         Const  mixed/tower_2/conv/batchnorm/moving_variance_min\n    0.871    0.002    0.001   0.000%     99.977%         Const  mixed/tower_1/conv_1/batchnorm/moving_variance_min\n    0.765    0.004    0.001   0.000%     99.977%         Const  mixed/tower_1/conv/conv2d_params_max\n    0.655    0.003    0.001   0.000%     99.977%         Const  mixed/tower/conv/batchnorm/beta_min\n    0.553    0.002    0.001   0.000%     99.977%         Const  conv_4/batchnorm/beta_max\n    0.536    0.003    0.001   0.000%     99.977%         Const  conv_4/batchnorm/moving_variance_min\n    0.378    0.003    0.001   0.000%     99.977%         Const  conv_1/batchnorm/beta_min\n    0.356    0.002    0.001   0.000%     99.977%         Const  conv_1/batchnorm/moving_mean_max\n    5.770    0.005    0.001   0.000%     99.977%         Const  mixed_9/tower_2/conv/batchnorm/moving_variance_min\n    5.758    0.003    0.001   0.000%     99.977%         Const  mixed_9/tower_2/conv/batchnorm/moving_mean_min\n    5.742    0.003    0.001   0.000%     99.978%         Const  mixed_9/tower_2/conv/conv2d_params_min\n    5.718    0.002    0.001   0.000%     99.978%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/moving_variance_max\n    5.661    0.002    0.001   0.000%     99.978%         Const  mixed_9/tower_1/mixed/conv/batchnorm/moving_variance_max\n    5.638    0.003    0.001   0.000%     99.978%         Const  mixed_9/tower_1/mixed/conv/batchnorm/moving_mean_quint8_const\n    5.562    0.003    0.001   0.000%     99.978%         Const  mixed_9/tower_1/conv/batchnorm/beta_max\n    5.400    0.002    0.001   0.000%     99.978%         Const  mixed_9/tower/mixed/conv/conv2d_params_min\n    5.354    0.006    0.001   0.000%     99.978%         Const  mixed_9/tower/conv/batchnorm/moving_mean_min\n    5.215    0.002    0.001   0.000%     99.978%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/beta_quint8_const\n    5.127    0.006    0.001   0.000%     99.978%         Const  mixed_10/tower_1/mixed/conv/batchnorm/moving_mean_min\n    5.084    0.006    0.001   0.000%     99.978%         Const  mixed_10/tower_1/conv_1/batchnorm/moving_variance_max\n    5.060    0.004    0.001   0.000%     99.978%         Const  mixed_10/tower_1/conv_1/batchnorm/moving_mean_quint8_const\n    4.974    0.003    0.001   0.000%     99.978%         Const  mixed_10/tower/mixed/conv_1/batchnorm/beta_quint8_const\n    4.881    0.003    0.001   0.000%     99.978%         Const  mixed_10/tower/mixed/conv/conv2d_params_max\n    4.754    0.003    0.001   0.000%     99.978%         Const  mixed_10/conv/batchnorm/moving_mean_min\n    4.608    0.002    0.001   0.000%     99.978%         Const  mixed_9/conv/conv2d_params_min\n    4.517    0.003    0.001   0.000%     99.978%         Const  mixed_8/tower_1/conv_2/batchnorm/moving_mean_max\n    4.252    0.003    0.001   0.000%     99.978%         Const  mixed_7/tower_2/conv/batchnorm/moving_variance_quint8_const\n    4.199    0.007    0.001   0.000%     99.978%         Const  mixed_7/tower_1/conv_4/batchnorm/moving_variance_min\n    4.188    0.003    0.001   0.000%     99.978%         Const  mixed_7/tower_1/conv_4/batchnorm/moving_mean_min\n    4.174    0.002    0.001   0.000%     99.978%         Const  mixed_7/tower_1/conv_4/conv2d_params_min\n    4.145    0.006    0.001   0.000%     99.978%         Const  mixed_7/tower_1/conv_3/batchnorm/moving_variance_min\n    4.105    0.002    0.001   0.000%     99.978%         Const  mixed_7/tower_1/conv_2/batchnorm/beta_min\n    4.080    0.002    0.001   0.000%     99.979%         Const  mixed_7/tower_1/conv_2/batchnorm/moving_mean_min\n    4.052    0.005    0.001   0.000%     99.979%         Const  mixed_7/tower_1/conv_1/batchnorm/beta_min\n    3.876    0.003    0.001   0.000%     99.979%         Const  mixed_7/tower/conv_1/batchnorm/moving_variance_min\n    3.854    0.003    0.001   0.000%     99.979%         Const  mixed_7/tower/conv_1/conv2d_params_max\n    3.766    0.003    0.001   0.000%     99.979%         Const  mixed_7/conv/batchnorm/moving_variance_min\n    3.726    0.003    0.001   0.000%     99.979%         Const  mixed_6/tower_2/conv/batchnorm/beta_min\n    3.712    0.006    0.001   0.000%     99.979%         Const  mixed_6/tower_2/conv/batchnorm/moving_variance_min\n    3.705    0.002    0.001   0.000%     99.979%         Const  mixed_6/tower_2/conv/batchnorm/moving_mean_max\n    3.668    0.003    0.001   0.000%     99.979%         Const  mixed_6/tower_1/conv_4/batchnorm/beta_quint8_const\n    3.648    0.002    0.001   0.000%     99.979%         Const  mixed_6/tower_1/conv_4/batchnorm/moving_mean_min\n    3.590    0.003    0.001   0.000%     99.979%         Const  mixed_6/tower_1/conv_3/batchnorm/moving_mean_min\n    3.576    0.002    0.001   0.000%     99.979%         Const  mixed_6/tower_1/conv_3/conv2d_params_min\n    3.482    0.002    0.001   0.000%     99.979%         Const  mixed_6/tower_1/conv_1/batchnorm/moving_mean_min\n    3.415    0.002    0.001   0.000%     99.979%         Const  mixed_6/tower_1/conv/conv2d_params_min\n    3.289    0.003    0.001   0.000%     99.979%         Const  mixed_6/tower/conv/batchnorm/beta_min\n    3.240    0.005    0.001   0.000%     99.979%         Const  mixed_6/conv/batchnorm/beta_max\n    3.215    0.002    0.001   0.000%     99.979%         Const  mixed_6/conv/batchnorm/moving_mean_max\n    3.197    0.002    0.001   0.000%     99.979%         Const  mixed_6/conv/conv2d_params_min\n    3.041    0.002    0.001   0.000%     99.979%         Const  mixed_5/tower_1/conv_3/conv2d_params_max\n    2.979    0.003    0.001   0.000%     99.979%         Const  mixed_5/tower_1/conv_2/conv2d_params_min\n    2.896    0.003    0.001   0.000%     99.979%         Const  mixed_5/tower_1/conv/batchnorm/moving_variance_min\n    2.723    0.003    0.001   0.000%     99.979%         Const  mixed_5/tower/conv/batchnorm/beta_max\n    2.713    0.002    0.001   0.000%     99.980%         Const  mixed_5/tower/conv/batchnorm/moving_variance_max\n    2.669    0.005    0.001   0.000%     99.980%         Const  mixed_5/conv/batchnorm/beta_max\n    2.602    0.002    0.001   0.000%     99.980%         Const  mixed_4/tower_2/conv/batchnorm/beta_quint8_const\n    2.559    0.002    0.001   0.000%     99.980%         Const  mixed_4/tower_1/conv_4/batchnorm/beta_max\n    2.445    0.003    0.001   0.000%     99.980%         Const  mixed_4/tower_1/conv_2/batchnorm/beta_max\n    2.406    0.003    0.001   0.000%     99.980%         Const  mixed_4/tower_1/conv_2/conv2d_params_max\n    2.328    0.003    0.001   0.000%     99.980%         Const  mixed_4/tower_1/conv/batchnorm/beta_min\n    2.318    0.005    0.001   0.000%     99.980%         Const  mixed_4/tower_1/conv/batchnorm/moving_variance_max\n    2.202    0.005    0.001   0.000%     99.980%         Const  mixed_4/tower/conv_1/batchnorm/moving_variance_min\n    2.180    0.003    0.001   0.000%     99.980%         Const  mixed_4/tower/conv_1/conv2d_params_max\n    2.116    0.004    0.001   0.000%     99.980%         Const  mixed_4/tower/conv/conv2d_params_min\n    1.945    0.005    0.001   0.000%     99.980%         Const  mixed_3/tower/conv_1/conv2d_params_max\n    1.941    0.002    0.001   0.000%     99.980%         Const  mixed_3/tower/conv_1/conv2d_params_min\n    1.814    0.002    0.001   0.000%     99.980%         Const  mixed_3/conv/conv2d_params_max\n    1.791    0.002    0.001   0.000%     99.980%         Const  mixed_2/tower_2/conv/batchnorm/beta_quint8_const\n    1.784    0.002    0.001   0.000%     99.980%         Const  mixed_2/tower_2/conv/batchnorm/moving_variance_max\n    1.738    0.002    0.001   0.000%     99.980%         Const  mixed_2/tower_1/conv_2/batchnorm/beta_quint8_const\n    1.636    0.003    0.001   0.000%     99.980%         Const  mixed_2/tower_1/conv/batchnorm/beta_max\n    1.619    0.005    0.001   0.000%     99.980%         Const  mixed_2/tower_1/conv/batchnorm/moving_variance_min\n    1.558    0.003    0.001   0.000%     99.980%         Const  mixed_2/tower/conv_1/batchnorm/moving_mean_max\n    1.414    0.003    0.001   0.000%     99.980%         Const  mixed_1/tower_2/conv/batchnorm/beta_min\n    1.356    0.002    0.001   0.000%     99.980%         Const  mixed_1/tower_1/conv_2/batchnorm/beta_quint8_const\n    1.279    0.003    0.001   0.000%     99.980%         Const  mixed_1/tower_1/conv_1/batchnorm/moving_mean_min\n    1.182    0.003    0.001   0.000%     99.981%         Const  mixed_1/tower/conv_1/batchnorm/moving_variance_min\n    1.146    0.002    0.001   0.000%     99.981%         Const  mixed_1/tower/conv/batchnorm/beta_max\n    1.101    0.003    0.001   0.000%     99.981%         Const  mixed_1/tower/conv/batchnorm/moving_mean_min\n    1.044    0.006    0.001   0.000%     99.981%         Const  mixed_1/conv/batchnorm/moving_mean_min\n    1.035    0.003    0.001   0.000%     99.981%         Const  mixed_1/conv/conv2d_params_max\n    0.995    0.003    0.001   0.000%     99.981%         Const  mixed/tower_2/conv/batchnorm/moving_variance_quint8_const\n    0.935    0.002    0.001   0.000%     99.981%         Const  mixed/tower_1/conv_2/batchnorm/moving_variance_quint8_const\n    0.916    0.002    0.001   0.000%     99.981%         Const  mixed/tower_1/conv_2/conv2d_params_max\n    0.847    0.003    0.001   0.000%     99.981%         Const  mixed/tower_1/conv_1/conv2d_params_max\n    0.806    0.004    0.001   0.000%     99.981%         Const  mixed/tower_1/conv/batchnorm/moving_variance_min\n    0.618    0.002    0.001   0.000%     99.981%         Const  mixed/tower/conv/conv2d_params_min\n    0.515    0.003    0.001   0.000%     99.981%         Const  conv_4/conv2d_params_max\n    0.291    0.003    0.001   0.000%     99.981%         Const  conv/batchnorm/moving_mean_max\n    5.707    0.003    0.001   0.000%     99.981%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/moving_variance_quint8_const\n    5.650    0.005    0.001   0.000%     99.981%         Const  mixed_9/tower_1/mixed/conv/batchnorm/moving_variance_quint8_const\n    5.465    0.002    0.001   0.000%     99.981%         Const  mixed_9/tower/mixed/conv_1/batchnorm/moving_mean_quint8_const\n    5.437    0.003    0.001   0.000%     99.981%         Const  mixed_9/tower/mixed/conv/batchnorm/beta_quint8_const\n    5.422    0.002    0.001   0.000%     99.981%         Const  mixed_9/tower/mixed/conv/batchnorm/moving_variance_quint8_const\n    5.384    0.003    0.001   0.000%     99.981%         Const  mixed_9/tower/conv/batchnorm/beta_min\n    5.331    0.003    0.001   0.000%     99.981%         Const  mixed_9/conv/batchnorm/beta_max\n    5.297    0.003    0.001   0.000%     99.981%         Const  softmax/weights_min\n    5.280    0.003    0.001   0.000%     99.981%         Const  mixed_10/tower_2/conv/batchnorm/beta_max\n    5.202    0.003    0.001   0.000%     99.982%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/moving_variance_quint8_const\n    5.172    0.002    0.001   0.000%     99.982%         Const  mixed_10/tower_1/mixed/conv_1/conv2d_params_min\n    5.161    0.002    0.001   0.000%     99.982%         Const  mixed_10/tower_1/mixed/conv/batchnorm/beta_max\n    5.053    0.002    0.001   0.000%     99.982%         Const  mixed_10/tower_1/conv_1/conv2d_params_min\n    5.015    0.002    0.001   0.000%     99.982%         Const  mixed_10/tower_1/conv/batchnorm/moving_variance_quint8_const\n    4.964    0.002    0.001   0.000%     99.982%         Const  mixed_10/tower/mixed/conv_1/batchnorm/moving_variance_min\n    4.912    0.004    0.001   0.000%     99.982%         Const  mixed_10/tower/mixed/conv/batchnorm/moving_variance_max\n    4.840    0.003    0.001   0.000%     99.982%         Const  mixed_10/tower/conv/batchnorm/beta_quint8_const\n    4.773    0.002    0.001   0.000%     99.982%         Const  mixed_10/conv/batchnorm/moving_variance_max\n    4.769    0.003    0.001   0.000%     99.982%         Const  mixed_10/conv/batchnorm/moving_variance_min\n    4.596    0.006    0.001   0.000%     99.982%         Const  mixed_8/tower_1/conv_3/batchnorm/beta_max\n    4.500    0.002    0.001   0.000%     99.982%         Const  mixed_8/tower_1/conv_2/conv2d_params_min\n    4.485    0.005    0.001   0.000%     99.982%         Const  mixed_8/tower_1/conv_1/batchnorm/beta_min\n    4.481    0.003    0.001   0.000%     99.982%         Const  mixed_8/tower_1/conv_1/batchnorm/beta_quint8_const\n    4.474    0.002    0.001   0.000%     99.982%         Const  mixed_8/tower_1/conv_1/batchnorm/moving_variance_min\n    4.452    0.003    0.001   0.000%     99.982%         Const  mixed_8/tower_1/conv_1/conv2d_params_max\n    4.324    0.002    0.001   0.000%     99.982%         Const  mixed_8/tower/conv/batchnorm/beta_min\n    4.314    0.004    0.001   0.000%     99.982%         Const  mixed_8/tower/conv/batchnorm/moving_variance_max\n    4.284    0.003    0.001   0.000%     99.982%         Const  mixed_8/tower/conv/conv2d_params_min\n    4.152    0.002    0.001   0.000%     99.982%         Const  mixed_7/tower_1/conv_3/batchnorm/moving_variance_max\n    4.012    0.003    0.001   0.000%     99.982%         Const  mixed_7/tower_1/conv_1/conv2d_params_min\n    3.976    0.002    0.001   0.000%     99.982%         Const  mixed_7/tower_1/conv/batchnorm/moving_mean_max\n    3.950    0.003    0.001   0.000%     99.983%         Const  mixed_7/tower/conv_2/batchnorm/beta_max\n    3.892    0.002    0.001   0.000%     99.983%         Const  mixed_7/tower/conv_1/batchnorm/beta_min\n    3.862    0.003    0.001   0.000%     99.983%         Const  mixed_7/tower/conv_1/batchnorm/moving_mean_min\n    3.781    0.002    0.001   0.000%     99.983%         Const  mixed_7/conv/batchnorm/beta_min\n    3.755    0.002    0.001   0.000%     99.983%         Const  mixed_7/conv/batchnorm/moving_mean_min\n    3.665    0.002    0.001   0.000%     99.983%         Const  mixed_6/tower_1/conv_4/batchnorm/moving_variance_max\n    3.594    0.002    0.001   0.000%     99.983%         Const  mixed_6/tower_1/conv_3/batchnorm/moving_mean_max\n    3.496    0.002    0.001   0.000%     99.983%         Const  mixed_6/tower_1/conv_1/batchnorm/moving_variance_min\n    3.455    0.003    0.001   0.000%     99.983%         Const  mixed_6/tower_1/conv/batchnorm/beta_min\n    3.391    0.004    0.001   0.000%     99.983%         Const  mixed_6/tower/conv_2/batchnorm/moving_variance_max\n    3.379    0.002    0.001   0.000%     99.983%         Const  mixed_6/tower/conv_2/batchnorm/moving_mean_max\n    3.233    0.002    0.001   0.000%     99.983%         Const  mixed_6/conv/batchnorm/beta_quint8_const\n    3.182    0.005    0.001   0.000%     99.983%         Const  mixed_5/tower_2/conv/batchnorm/beta_min\n    2.949    0.003    0.001   0.000%     99.983%         Const  mixed_5/tower_1/conv_1/batchnorm/moving_variance_min\n    2.850    0.002    0.001   0.000%     99.983%         Const  mixed_5/tower/conv_2/batchnorm/beta_min\n    2.795    0.002    0.001   0.000%     99.983%         Const  mixed_5/tower/conv_1/batchnorm/beta_min\n    2.644    0.002    0.001   0.000%     99.983%         Const  mixed_5/conv/batchnorm/moving_mean_max\n    2.541    0.002    0.001   0.000%     99.983%         Const  mixed_4/tower_1/conv_4/batchnorm/moving_variance_min\n    2.363    0.004    0.001   0.000%     99.983%         Const  mixed_4/tower_1/conv_1/batchnorm/moving_mean_max\n    2.275    0.002    0.001   0.000%     99.983%         Const  mixed_4/tower/conv_2/batchnorm/beta_min\n    2.261    0.003    0.001   0.000%     99.983%         Const  mixed_4/tower/conv_2/batchnorm/moving_variance_min\n    2.250    0.003    0.001   0.000%     99.983%         Const  mixed_4/tower/conv_2/batchnorm/moving_mean_min\n    2.136    0.002    0.001   0.000%     99.983%         Const  mixed_4/tower/conv/batchnorm/moving_mean_max\n    2.132    0.002    0.001   0.000%     99.984%         Const  mixed_4/tower/conv/batchnorm/moving_mean_min\n    2.006    0.003    0.001   0.000%     99.984%         Const  mixed_3/tower/conv_2/conv2d_params_max\n    1.980    0.006    0.001   0.000%     99.984%         Const  mixed_3/tower/conv_1/batchnorm/beta_min\n    1.916    0.002    0.001   0.000%     99.984%         Const  mixed_3/tower/conv/batchnorm/moving_variance_min\n    1.773    0.003    0.001   0.000%     99.984%         Const  mixed_2/tower_2/conv/batchnorm/moving_mean_max\n    1.745    0.002    0.001   0.000%     99.984%         Const  mixed_2/tower_1/conv_2/batchnorm/beta_max\n    1.705    0.002    0.001   0.000%     99.984%         Const  mixed_2/tower_1/conv_2/conv2d_params_max\n    1.612    0.002    0.001   0.000%     99.984%         Const  mixed_2/tower_1/conv/batchnorm/moving_mean_max\n    1.583    0.006    0.001   0.000%     99.984%         Const  mixed_2/tower/conv_1/batchnorm/beta_max\n    1.573    0.002    0.001   0.000%     99.984%         Const  mixed_2/tower/conv_1/batchnorm/moving_variance_max\n    1.505    0.003    0.001   0.000%     99.984%         Const  mixed_2/tower/conv/batchnorm/moving_variance_quint8_const\n    1.443    0.002    0.001   0.000%     99.984%         Const  mixed_2/conv/batchnorm/moving_mean_min\n    1.389    0.003    0.001   0.000%     99.984%         Const  mixed_1/tower_2/conv/batchnorm/moving_mean_min\n    1.352    0.003    0.001   0.000%     99.984%         Const  mixed_1/tower_1/conv_2/batchnorm/moving_variance_max\n    1.294    0.002    0.001   0.000%     99.984%         Const  mixed_1/tower_1/conv_1/batchnorm/moving_variance_min\n    1.216    0.002    0.001   0.000%     99.984%         Const  mixed_1/tower_1/conv/conv2d_params_max\n    1.198    0.002    0.001   0.000%     99.984%         Const  mixed_1/tower/conv_1/batchnorm/beta_min\n    1.093    0.003    0.001   0.000%     99.984%         Const  mixed_1/tower/conv/conv2d_params_max\n    1.019    0.002    0.001   0.000%     99.984%         Const  mixed/tower_2/conv/batchnorm/beta_max\n    0.843    0.003    0.001   0.000%     99.984%         Const  mixed/tower_1/conv_1/conv2d_params_min\n    0.698    0.004    0.001   0.000%     99.984%         Const  mixed/tower/conv_1/batchnorm/moving_mean_min\n    0.685    0.004    0.001   0.000%     99.984%         Const  mixed/tower/conv_1/conv2d_params_max\n    0.622    0.002    0.001   0.000%     99.985%         Const  mixed/tower/conv/conv2d_params_max\n    0.437    0.003    0.001   0.000%     99.985%         Const  conv_2/batchnorm/beta_max\n    0.411    0.003    0.001   0.000%     99.985%         Const  conv_2/batchnorm/moving_mean_max\n    0.277    0.002    0.001   0.000%     99.985%         Const  conv/conv2d_params_max\n    3.427    0.002    0.001   0.000%     99.985%         Const  mixed_6/tower_1/conv/batchnorm/moving_mean_min\n    5.544    0.003    0.001   0.000%     99.985%         Const  mixed_9/tower_1/conv/batchnorm/moving_variance_min\n    5.457    0.002    0.001   0.000%     99.985%         Const  mixed_9/tower/mixed/conv_1/conv2d_params_min\n    5.412    0.002    0.001   0.000%     99.985%         Const  mixed_9/tower/mixed/conv/batchnorm/moving_mean_min\n    5.251    0.006    0.001   0.000%     99.985%         Const  mixed_10/tower_2/conv/batchnorm/moving_mean_max\n    5.185    0.003    0.001   0.000%     99.985%         Const  mixed_10/tower_1/mixed/conv_1/conv2d_params_max\n    5.146    0.006    0.001   0.000%     99.985%         Const  mixed_10/tower_1/mixed/conv/batchnorm/moving_variance_max\n    5.073    0.002    0.001   0.000%     99.985%         Const  mixed_10/tower_1/conv_1/batchnorm/moving_mean_max\n    4.949    0.006    0.001   0.000%     99.985%         Const  mixed_10/tower/mixed/conv_1/batchnorm/moving_mean_min\n    4.783    0.003    0.001   0.000%     99.985%         Const  mixed_10/conv/batchnorm/beta_min\n    4.582    0.002    0.001   0.000%     99.985%         Const  mixed_8/tower_1/conv_3/batchnorm/moving_variance_min\n    4.535    0.002    0.001   0.000%     99.985%         Const  mixed_8/tower_1/conv_2/batchnorm/beta_quint8_const\n    4.528    0.002    0.001   0.000%     99.985%         Const  mixed_8/tower_1/conv_2/batchnorm/moving_variance_min\n    4.470    0.003    0.001   0.000%     99.985%         Const  mixed_8/tower_1/conv_1/batchnorm/moving_variance_quint8_const\n    4.460    0.003    0.001   0.000%     99.985%         Const  mixed_8/tower_1/conv_1/batchnorm/moving_mean_min\n    4.338    0.002    0.001   0.000%     99.985%         Const  mixed_8/tower/conv_1/conv2d_params_min\n    4.306    0.003    0.001   0.000%     99.985%         Const  mixed_8/tower/conv/batchnorm/moving_variance_quint8_const\n    4.302    0.003    0.001   0.000%     99.985%         Const  mixed_8/tower/conv/batchnorm/moving_mean_max\n    4.260    0.005    0.001   0.000%     99.985%         Const  mixed_7/tower_2/conv/batchnorm/moving_variance_max\n    4.234    0.002    0.001   0.000%     99.986%         Const  mixed_7/tower_2/conv/conv2d_params_max\n    4.160    0.002    0.001   0.000%     99.986%         Const  mixed_7/tower_1/conv_3/batchnorm/beta_min\n    4.027    0.002    0.001   0.000%     99.986%         Const  mixed_7/tower_1/conv_1/batchnorm/moving_mean_min\n    4.001    0.002    0.001   0.000%     99.986%         Const  mixed_7/tower_1/conv/batchnorm/beta_min\n    3.937    0.002    0.001   0.000%     99.986%         Const  mixed_7/tower/conv_2/batchnorm/moving_variance_max\n    3.818    0.002    0.001   0.000%     99.986%         Const  mixed_7/tower/conv/batchnorm/moving_variance_quint8_const\n    3.702    0.002    0.001   0.000%     99.986%         Const  mixed_6/tower_2/conv/batchnorm/moving_mean_min\n    3.687    0.002    0.001   0.000%     99.986%         Const  mixed_6/tower_2/conv/conv2d_params_min\n    3.637    0.005    0.001   0.000%     99.986%         Const  mixed_6/tower_1/conv_4/conv2d_params_max\n    3.564    0.002    0.001   0.000%     99.986%         Const  mixed_6/tower_1/conv_2/batchnorm/beta_min\n    3.554    0.002    0.001   0.000%     99.986%         Const  mixed_6/tower_1/conv_2/batchnorm/moving_variance_max\n    3.467    0.005    0.001   0.000%     99.986%         Const  mixed_6/tower_1/conv_1/conv2d_params_min\n    3.334    0.005    0.001   0.000%     99.986%         Const  mixed_6/tower/conv_1/batchnorm/moving_variance_max\n    3.330    0.003    0.001   0.000%     99.986%         Const  mixed_6/tower/conv_1/batchnorm/moving_variance_min\n    3.201    0.002    0.001   0.000%     99.986%         Const  mixed_6/conv/conv2d_params_max\n    3.161    0.002    0.001   0.000%     99.986%         Const  mixed_5/tower_2/conv/batchnorm/moving_mean_max\n    3.121    0.002    0.001   0.000%     99.986%         Const  mixed_5/tower_1/conv_4/batchnorm/moving_variance_max\n    3.049    0.002    0.001   0.000%     99.986%         Const  mixed_5/tower_1/conv_3/batchnorm/moving_mean_min\n    3.009    0.002    0.001   0.000%     99.986%         Const  mixed_5/tower_1/conv_2/batchnorm/moving_variance_max\n    2.995    0.005    0.001   0.000%     99.986%         Const  mixed_5/tower_1/conv_2/batchnorm/moving_mean_max\n    2.914    0.002    0.001   0.000%     99.986%         Const  mixed_5/tower_1/conv/batchnorm/beta_max\n    2.720    0.002    0.001   0.000%     99.986%         Const  mixed_5/tower/conv/batchnorm/beta_min\n    2.605    0.003    0.001   0.000%     99.986%         Const  mixed_4/tower_2/conv/batchnorm/beta_min\n    2.594    0.003    0.001   0.000%     99.987%         Const  mixed_4/tower_2/conv/batchnorm/moving_variance_min\n    2.584    0.003    0.001   0.000%     99.987%         Const  mixed_4/tower_2/conv/batchnorm/moving_mean_max\n    2.580    0.003    0.001   0.000%     99.987%         Const  mixed_4/tower_2/conv/batchnorm/moving_mean_min\n    2.570    0.005    0.001   0.000%     99.987%         Const  mixed_4/tower_2/conv/conv2d_params_max\n    2.548    0.002    0.001   0.000%     99.987%         Const  mixed_4/tower_1/conv_4/batchnorm/beta_quint8_const\n    2.527    0.002    0.001   0.000%     99.987%         Const  mixed_4/tower_1/conv_4/batchnorm/moving_mean_min\n    2.501    0.002    0.001   0.000%     99.987%         Const  mixed_4/tower_1/conv_3/batchnorm/beta_min\n    2.121    0.002    0.001   0.000%     99.987%         Const  mixed_4/tower/conv/conv2d_params_max\n    2.062    0.002    0.001   0.000%     99.987%         Const  mixed_4/conv/conv2d_params_max\n    1.901    0.002    0.001   0.000%     99.987%         Const  mixed_3/tower/conv/batchnorm/moving_mean_min\n    1.839    0.002    0.001   0.000%     99.987%         Const  mixed_3/conv/batchnorm/moving_variance_max\n    1.821    0.002    0.001   0.000%     99.987%         Const  mixed_3/conv/batchnorm/moving_mean_min\n    1.759    0.003    0.001   0.000%     99.987%         Const  mixed_2/tower_2/conv/conv2d_params_max\n    1.731    0.005    0.001   0.000%     99.987%         Const  mixed_2/tower_1/conv_2/batchnorm/moving_variance_max\n    1.672    0.002    0.001   0.000%     99.987%         Const  mixed_2/tower_1/conv_1/batchnorm/moving_variance_min\n    1.598    0.002    0.001   0.000%     99.987%         Const  mixed_2/tower_1/conv/conv2d_params_max\n    1.462    0.002    0.001   0.000%     99.987%         Const  mixed_2/conv/batchnorm/moving_variance_max\n    1.432    0.003    0.001   0.000%     99.987%         Const  mixed_2/conv/conv2d_params_max\n    1.418    0.005    0.001   0.000%     99.987%         Const  mixed_1/tower_2/conv/batchnorm/beta_max\n    0.828    0.002    0.001   0.000%     99.987%         Const  mixed/tower_1/conv/batchnorm/beta_min\n    0.679    0.004    0.001   0.000%     99.987%         Const  mixed/tower/conv_1/conv2d_params_min\n    0.659    0.003    0.001   0.000%     99.987%         Const  mixed/tower/conv/batchnorm/beta_max\n    0.481    0.003    0.001   0.000%     99.987%         Const  conv_3/batchnorm/moving_variance_min\n    5.656    0.003    0.001   0.000%     99.988%         Const  mixed_9/tower_1/mixed/conv/batchnorm/moving_variance_min\n    5.601    0.003    0.001   0.000%     99.988%         Const  mixed_9/tower_1/conv_1/batchnorm/moving_variance_min\n    5.571    0.002    0.001   0.000%     99.988%         Const  mixed_9/tower_1/conv_1/conv2d_params_min\n    5.514    0.006    0.001   0.000%     99.988%         Const  mixed_9/tower_1/conv/conv2d_params_min\n    5.319    0.003    0.001   0.000%     99.988%         Const  softmax/biases_quint8_const\n    5.258    0.003    0.001   0.000%     99.988%         Const  mixed_10/tower_2/conv/batchnorm/moving_variance_quint8_const\n    5.247    0.003    0.001   0.000%     99.988%         Const  mixed_10/tower_2/conv/batchnorm/moving_mean_min\n    5.134    0.003    0.001   0.000%     99.988%         Const  mixed_10/tower_1/mixed/conv/batchnorm/moving_mean_max\n    5.037    0.002    0.001   0.000%     99.988%         Const  mixed_10/tower_1/conv/batchnorm/beta_max\n    4.815    0.005    0.001   0.000%     99.988%         Const  mixed_10/tower/conv/batchnorm/moving_mean_min\n    4.747    0.002    0.001   0.000%     99.988%         Const  mixed_10/conv/conv2d_params_max\n    4.622    0.002    0.001   0.000%     99.988%         Const  mixed_9/conv/batchnorm/moving_mean_min\n    4.575    0.002    0.001   0.000%     99.988%         Const  mixed_8/tower_1/conv_3/batchnorm/moving_variance_quint8_const\n    4.368    0.002    0.001   0.000%     99.988%         Const  mixed_8/tower/conv_1/batchnorm/moving_variance_max\n    4.357    0.002    0.001   0.000%     99.988%         Const  mixed_8/tower/conv_1/batchnorm/moving_mean_max\n    4.350    0.002    0.001   0.000%     99.988%         Const  mixed_8/tower/conv_1/batchnorm/moving_mean_min\n    4.327    0.002    0.001   0.000%     99.988%         Const  mixed_8/tower/conv/batchnorm/beta_max\n    4.296    0.005    0.001   0.000%     99.988%         Const  mixed_8/tower/conv/batchnorm/moving_mean_min\n    4.218    0.003    0.001   0.000%     99.988%         Const  mixed_7/tower_1/conv_4/batchnorm/beta_max\n    4.120    0.002    0.001   0.000%     99.988%         Const  mixed_7/tower_1/conv_3/conv2d_params_min\n    4.084    0.002    0.001   0.000%     99.988%         Const  mixed_7/tower_1/conv_2/batchnorm/moving_mean_max\n    3.972    0.003    0.001   0.000%     99.988%         Const  mixed_7/tower_1/conv/batchnorm/moving_mean_min\n    3.730    0.005    0.001   0.000%     99.988%         Const  mixed_6/tower_2/conv/batchnorm/beta_max\n    3.691    0.002    0.001   0.000%     99.989%         Const  mixed_6/tower_2/conv/conv2d_params_max\n    3.550    0.002    0.001   0.000%     99.989%         Const  mixed_6/tower_1/conv_2/batchnorm/moving_variance_min\n    3.536    0.002    0.001   0.000%     99.989%         Const  mixed_6/tower_1/conv_2/batchnorm/moving_mean_min\n    3.499    0.003    0.001   0.000%     99.989%         Const  mixed_6/tower_1/conv_1/batchnorm/moving_variance_max\n    3.387    0.002    0.001   0.000%     99.989%         Const  mixed_6/tower/conv_2/batchnorm/moving_variance_min\n    3.308    0.002    0.001   0.000%     99.989%         Const  mixed_6/tower/conv_1/conv2d_params_max\n    3.189    0.002    0.001   0.000%     99.989%         Const  mixed_5/tower_2/conv/batchnorm/beta_max\n    3.171    0.002    0.001   0.000%     99.989%         Const  mixed_5/tower_2/conv/batchnorm/moving_variance_min\n    3.129    0.004    0.001   0.000%     99.989%         Const  mixed_5/tower_1/conv_4/batchnorm/beta_min\n    3.107    0.002    0.001   0.000%     99.989%         Const  mixed_5/tower_1/conv_4/batchnorm/moving_mean_max\n    3.066    0.003    0.001   0.000%     99.989%         Const  mixed_5/tower_1/conv_3/batchnorm/moving_variance_max\n    3.059    0.002    0.001   0.000%     99.989%         Const  mixed_5/tower_1/conv_3/batchnorm/moving_variance_quint8_const\n    2.960    0.002    0.001   0.000%     99.989%         Const  mixed_5/tower_1/conv_1/batchnorm/beta_quint8_const\n    2.910    0.003    0.001   0.000%     99.989%         Const  mixed_5/tower_1/conv/batchnorm/beta_min\n    2.889    0.002    0.001   0.000%     99.989%         Const  mixed_5/tower_1/conv/batchnorm/moving_mean_max\n    2.869    0.004    0.001   0.000%     99.989%         Const  mixed_5/tower_1/conv/conv2d_params_min\n    2.842    0.003    0.001   0.000%     99.989%         Const  mixed_5/tower/conv_2/batchnorm/moving_variance_max\n    2.666    0.002    0.001   0.000%     99.989%         Const  mixed_5/conv/batchnorm/beta_min\n    2.629    0.002    0.001   0.000%     99.989%         Const  mixed_5/conv/conv2d_params_max\n    2.615    0.004    0.001   0.000%     99.989%         Const  mixed_4/tower_2/conv/batchnorm/beta_max\n    2.598    0.002    0.001   0.000%     99.989%         Const  mixed_4/tower_2/conv/batchnorm/moving_variance_max\n    2.505    0.002    0.001   0.000%     99.989%         Const  mixed_4/tower_1/conv_3/batchnorm/beta_max\n    2.432    0.002    0.001   0.000%     99.989%         Const  mixed_4/tower_1/conv_2/batchnorm/moving_variance_max\n    2.235    0.004    0.001   0.000%     99.990%         Const  mixed_4/tower/conv_2/conv2d_params_max\n    2.215    0.003    0.001   0.000%     99.990%         Const  mixed_4/tower/conv_1/batchnorm/beta_min\n    2.177    0.002    0.001   0.000%     99.990%         Const  mixed_4/tower/conv_1/conv2d_params_min\n    1.930    0.002    0.001   0.000%     99.990%         Const  mixed_3/tower/conv/batchnorm/beta_min\n    1.781    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower_2/conv/batchnorm/moving_variance_min\n    1.728    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower_1/conv_2/batchnorm/moving_variance_min\n    1.661    0.003    0.001   0.000%     99.990%         Const  mixed_2/tower_1/conv_1/batchnorm/moving_mean_min\n    1.651    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower_1/conv_1/conv2d_params_max\n    1.647    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower_1/conv_1/conv2d_params_min\n    1.633    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower_1/conv/batchnorm/beta_min\n    1.608    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower_1/conv/batchnorm/moving_mean_min\n    1.580    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower/conv_1/batchnorm/beta_min\n    1.569    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower/conv_1/batchnorm/moving_variance_min\n    1.544    0.004    0.001   0.000%     99.990%         Const  mixed_2/tower/conv_1/conv2d_params_max\n    1.532    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower/conv/batchnorm/beta_max\n    1.446    0.003    0.001   0.000%     99.990%         Const  mixed_2/conv/batchnorm/moving_mean_max\n    1.360    0.002    0.001   0.000%     99.990%         Const  mixed_1/tower_1/conv_2/batchnorm/beta_min\n    1.349    0.002    0.001   0.000%     99.990%         Const  mixed_1/tower_1/conv_2/batchnorm/moving_variance_min\n    1.015    0.003    0.001   0.000%     99.990%         Const  mixed/tower_2/conv/batchnorm/beta_min\n    0.977    0.002    0.001   0.000%     99.990%         Const  mixed/tower_2/conv/conv2d_params_max\n    0.718    0.004    0.001   0.000%     99.990%         Const  mixed/tower/conv_1/batchnorm/moving_variance_min\n    0.609    0.002    0.001   0.000%     99.990%         Const  mixed/conv/batchnorm/beta_max\n    5.699    0.002    0.001   0.000%     99.990%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/moving_mean_min\n    5.591    0.002    0.001   0.000%     99.990%         Const  mixed_9/tower_1/conv_1/batchnorm/moving_mean_max\n    5.533    0.005    0.001   0.000%     99.991%         Const  mixed_9/tower_1/conv/batchnorm/moving_mean_max\n    5.529    0.002    0.001   0.000%     99.991%         Const  mixed_9/tower_1/conv/batchnorm/moving_mean_min\n    5.490    0.002    0.001   0.000%     99.991%         Const  mixed_9/tower/mixed/conv_1/batchnorm/moving_variance_max\n    5.361    0.003    0.001   0.000%     99.991%         Const  mixed_9/tower/conv/batchnorm/moving_mean_max\n    5.347    0.002    0.001   0.000%     99.991%         Const  mixed_9/tower/conv/conv2d_params_max\n    5.327    0.003    0.001   0.000%     99.991%         Const  softmax/biases_max\n    5.239    0.003    0.001   0.000%     99.991%         Const  mixed_10/tower_2/conv/conv2d_params_max\n    5.219    0.003    0.001   0.000%     99.991%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/beta_min\n    5.194    0.003    0.001   0.000%     99.991%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/moving_mean_min\n    5.117    0.004    0.001   0.000%     99.991%         Const  mixed_10/tower_1/mixed/conv/conv2d_params_max\n    5.111    0.004    0.001   0.000%     99.991%         Const  mixed_10/tower_1/mixed/conv/conv2d_params_min\n    5.095    0.002    0.001   0.000%     99.991%         Const  mixed_10/tower_1/conv_1/batchnorm/beta_min\n    5.091    0.003    0.001   0.000%     99.991%         Const  mixed_10/tower_1/conv_1/batchnorm/beta_quint8_const\n    5.057    0.002    0.001   0.000%     99.991%         Const  mixed_10/tower_1/conv_1/conv2d_params_max\n    4.982    0.002    0.001   0.000%     99.991%         Const  mixed_10/tower/mixed/conv_1/batchnorm/beta_max\n    4.968    0.005    0.001   0.000%     99.991%         Const  mixed_10/tower/mixed/conv_1/batchnorm/moving_variance_max\n    4.941    0.002    0.001   0.000%     99.991%         Const  mixed_10/tower/mixed/conv_1/conv2d_params_max\n    4.922    0.003    0.001   0.000%     99.991%         Const  mixed_10/tower/mixed/conv/batchnorm/beta_min\n    4.906    0.002    0.001   0.000%     99.991%         Const  mixed_10/tower/mixed/conv/batchnorm/moving_variance_min\n    4.503    0.005    0.001   0.000%     99.991%         Const  mixed_8/tower_1/conv_2/conv2d_params_max\n    4.446    0.002    0.001   0.000%     99.991%         Const  mixed_8/tower_1/conv_1/conv2d_params_min\n    4.435    0.002    0.001   0.000%     99.991%         Const  mixed_8/tower_1/conv/batchnorm/beta_min\n    4.425    0.002    0.001   0.000%     99.991%         Const  mixed_8/tower_1/conv/batchnorm/moving_variance_max\n    4.342    0.002    0.001   0.000%     99.992%         Const  mixed_8/tower/conv_1/conv2d_params_max\n    4.207    0.002    0.001   0.000%     99.992%         Const  mixed_7/tower_1/conv_4/batchnorm/moving_variance_max\n    4.178    0.002    0.001   0.000%     99.992%         Const  mixed_7/tower_1/conv_4/conv2d_params_max\n    3.987    0.002    0.001   0.000%     99.992%         Const  mixed_7/tower_1/conv/batchnorm/moving_variance_min\n    3.933    0.002    0.001   0.000%     99.992%         Const  mixed_7/tower/conv_2/batchnorm/moving_variance_min\n    3.814    0.003    0.001   0.000%     99.992%         Const  mixed_7/tower/conv/batchnorm/moving_mean_max\n    3.745    0.002    0.001   0.000%     99.992%         Const  mixed_7/conv/conv2d_params_max\n    3.528    0.003    0.001   0.000%     99.992%         Const  mixed_6/tower_1/conv_2/conv2d_params_max\n    3.510    0.002    0.001   0.000%     99.992%         Const  mixed_6/tower_1/conv_1/batchnorm/beta_min\n    3.433    0.003    0.001   0.000%     99.992%         Const  mixed_6/tower_1/conv/batchnorm/moving_mean_max\n    3.404    0.002    0.001   0.000%     99.992%         Const  mixed_6/tower/conv_2/batchnorm/beta_max\n    3.229    0.002    0.001   0.000%     99.992%         Const  mixed_6/conv/batchnorm/moving_variance_max\n    3.147    0.005    0.001   0.000%     99.992%         Const  mixed_5/tower_2/conv/conv2d_params_max\n    3.135    0.002    0.001   0.000%     99.992%         Const  mixed_5/tower_1/conv_4/batchnorm/beta_max\n    3.076    0.003    0.001   0.000%     99.992%         Const  mixed_5/tower_1/conv_3/batchnorm/beta_min\n    2.991    0.003    0.001   0.000%     99.992%         Const  mixed_5/tower_1/conv_2/batchnorm/moving_mean_min\n    2.983    0.002    0.001   0.000%     99.992%         Const  mixed_5/tower_1/conv_2/conv2d_params_max\n    2.928    0.003    0.001   0.000%     99.992%         Const  mixed_5/tower_1/conv_1/conv2d_params_max\n    2.684    0.002    0.001   0.000%     99.992%         Const  mixed_5/tower/conv/conv2d_params_max\n    2.640    0.002    0.001   0.000%     99.992%         Const  mixed_5/conv/batchnorm/moving_mean_min\n    2.481    0.003    0.001   0.000%     99.992%         Const  mixed_4/tower_1/conv_3/batchnorm/moving_variance_min\n    2.471    0.002    0.001   0.000%     99.992%         Const  mixed_4/tower_1/conv_3/batchnorm/moving_mean_max\n    2.460    0.002    0.001   0.000%     99.992%         Const  mixed_4/tower_1/conv_3/conv2d_params_max\n    2.414    0.002    0.001   0.000%     99.992%         Const  mixed_4/tower_1/conv_2/batchnorm/moving_mean_min\n    2.392    0.002    0.001   0.000%     99.993%         Const  mixed_4/tower_1/conv_1/batchnorm/beta_max\n    2.374    0.002    0.001   0.000%     99.993%         Const  mixed_4/tower_1/conv_1/batchnorm/moving_variance_min\n    2.332    0.002    0.001   0.000%     99.993%         Const  mixed_4/tower_1/conv/batchnorm/beta_max\n    2.208    0.002    0.001   0.000%     99.993%         Const  mixed_4/tower/conv_1/batchnorm/moving_variance_max\n    2.101    0.003    0.001   0.000%     99.993%         Const  mixed_4/conv/batchnorm/beta_max\n    2.084    0.005    0.001   0.000%     99.993%         Const  mixed_4/conv/batchnorm/moving_variance_min\n    1.973    0.002    0.001   0.000%     99.993%         Const  mixed_3/tower/conv_1/batchnorm/moving_variance_max\n    1.849    0.002    0.001   0.000%     99.993%         Const  mixed_3/conv/batchnorm/beta_min\n    1.686    0.003    0.001   0.000%     99.993%         Const  mixed_2/tower_1/conv_1/batchnorm/beta_min\n    1.679    0.002    0.001   0.000%     99.993%         Const  mixed_2/tower_1/conv_1/batchnorm/moving_variance_max\n    1.665    0.002    0.001   0.000%     99.993%         Const  mixed_2/tower_1/conv_1/batchnorm/moving_mean_max\n    1.594    0.003    0.001   0.000%     99.993%         Const  mixed_2/tower_1/conv/conv2d_params_min\n    1.541    0.002    0.001   0.000%     99.993%         Const  mixed_2/tower/conv_1/conv2d_params_min\n    1.509    0.003    0.001   0.000%     99.993%         Const  mixed_2/tower/conv/batchnorm/moving_variance_min\n    1.469    0.002    0.001   0.000%     99.993%         Const  mixed_2/conv/batchnorm/beta_min\n    1.466    0.002    0.001   0.000%     99.993%         Const  mixed_2/conv/batchnorm/beta_quint8_const\n    1.393    0.002    0.001   0.000%     99.993%         Const  mixed_1/tower_2/conv/batchnorm/moving_mean_max\n    1.224    0.002    0.001   0.000%     99.993%         Const  mixed_1/tower_1/conv/batchnorm/moving_mean_min\n    1.168    0.002    0.001   0.000%     99.993%         Const  mixed_1/tower/conv_1/batchnorm/moving_mean_min\n    0.564    0.002    0.001   0.000%     99.993%         Const  mixed/conv/conv2d_params_min\n    5.669    0.005    0.001   0.000%     99.993%         Const  mixed_9/tower_1/mixed/conv/batchnorm/beta_min\n    5.612    0.006    0.001   0.000%     99.993%         Const  mixed_9/tower_1/conv_1/batchnorm/beta_min\n    5.469    0.002    0.001   0.000%     99.993%         Const  mixed_9/tower/mixed/conv_1/batchnorm/moving_mean_min\n    5.461    0.002    0.001   0.000%     99.994%         Const  mixed_9/tower/mixed/conv_1/conv2d_params_max\n    5.301    0.003    0.001   0.000%     99.994%         Const  softmax/weights_max\n    5.262    0.002    0.001   0.000%     99.994%         Const  mixed_10/tower_2/conv/batchnorm/moving_variance_min\n    5.211    0.002    0.001   0.000%     99.994%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/moving_variance_max\n    5.206    0.003    0.001   0.000%     99.994%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/moving_variance_min\n    5.080    0.003    0.001   0.000%     99.994%         Const  mixed_10/tower_1/conv_1/batchnorm/moving_variance_min\n    5.019    0.002    0.001   0.000%     99.994%         Const  mixed_10/tower_1/conv/batchnorm/moving_variance_min\n    4.844    0.003    0.001   0.000%     99.994%         Const  mixed_10/tower/conv/batchnorm/beta_min\n    4.646    0.002    0.001   0.000%     99.994%         Const  mixed_9/conv/batchnorm/beta_min\n    4.557    0.003    0.001   0.000%     99.994%         Const  mixed_8/tower_1/conv_3/conv2d_params_max\n    4.531    0.003    0.001   0.000%     99.994%         Const  mixed_8/tower_1/conv_2/batchnorm/moving_variance_max\n    4.478    0.002    0.001   0.000%     99.994%         Const  mixed_8/tower_1/conv_1/batchnorm/moving_variance_max\n    4.421    0.002    0.001   0.000%     99.994%         Const  mixed_8/tower_1/conv/batchnorm/moving_variance_min\n    4.256    0.002    0.001   0.000%     99.994%         Const  mixed_7/tower_2/conv/batchnorm/moving_variance_min\n    4.094    0.002    0.001   0.000%     99.994%         Const  mixed_7/tower_1/conv_2/batchnorm/moving_variance_min\n    4.041    0.002    0.001   0.000%     99.994%         Const  mixed_7/tower_1/conv_1/batchnorm/moving_variance_min\n    3.965    0.002    0.001   0.000%     99.994%         Const  mixed_7/tower_1/conv/conv2d_params_max\n    3.719    0.002    0.001   0.000%     99.994%         Const  mixed_6/tower_2/conv/batchnorm/moving_variance_max\n    3.608    0.002    0.001   0.000%     99.994%         Const  mixed_6/tower_1/conv_3/batchnorm/moving_variance_max\n    3.419    0.002    0.001   0.000%     99.994%         Const  mixed_6/tower_1/conv/conv2d_params_max\n    3.401    0.002    0.001   0.000%     99.994%         Const  mixed_6/tower/conv_2/batchnorm/beta_min\n    3.348    0.003    0.001   0.000%     99.994%         Const  mixed_6/tower/conv_1/batchnorm/beta_max\n    3.264    0.003    0.001   0.000%     99.994%         Const  mixed_6/tower/conv/batchnorm/moving_mean_min\n    3.236    0.002    0.001   0.000%     99.994%         Const  mixed_6/conv/batchnorm/beta_min\n    3.211    0.003    0.001   0.000%     99.995%         Const  mixed_6/conv/batchnorm/moving_mean_min\n    3.125    0.002    0.001   0.000%     99.995%         Const  mixed_5/tower_1/conv_4/batchnorm/beta_quint8_const\n    3.117    0.003    0.001   0.000%     99.995%         Const  mixed_5/tower_1/conv_4/batchnorm/moving_variance_min\n    3.080    0.002    0.001   0.000%     99.995%         Const  mixed_5/tower_1/conv_3/batchnorm/beta_max\n    2.953    0.002    0.001   0.000%     99.995%         Const  mixed_5/tower_1/conv_1/batchnorm/moving_variance_max\n    2.874    0.002    0.001   0.000%     99.995%         Const  mixed_5/tower_1/conv/conv2d_params_max\n    2.824    0.003    0.001   0.000%     99.995%         Const  mixed_5/tower/conv_2/batchnorm/moving_mean_min\n    2.658    0.003    0.001   0.000%     99.995%         Const  mixed_5/conv/batchnorm/moving_variance_max\n    2.519    0.003    0.001   0.000%     99.995%         Const  mixed_4/tower_1/conv_4/conv2d_params_max\n    2.486    0.004    0.001   0.000%     99.995%         Const  mixed_4/tower_1/conv_3/batchnorm/moving_variance_max\n    2.467    0.003    0.001   0.000%     99.995%         Const  mixed_4/tower_1/conv_3/batchnorm/moving_mean_min\n    2.388    0.002    0.001   0.000%     99.995%         Const  mixed_4/tower_1/conv_1/batchnorm/beta_min\n    2.377    0.002    0.001   0.000%     99.995%         Const  mixed_4/tower_1/conv_1/batchnorm/moving_variance_max\n    2.219    0.002    0.001   0.000%     99.995%         Const  mixed_4/tower/conv_1/batchnorm/beta_max\n    2.044    0.002    0.001   0.000%     99.995%         Const  mixed_3/tower/conv_2/batchnorm/beta_min\n    2.032    0.003    0.001   0.000%     99.995%         Const  mixed_3/tower/conv_2/batchnorm/moving_variance_min\n    1.756    0.002    0.001   0.000%     99.995%         Const  mixed_2/tower_2/conv/conv2d_params_min\n    1.502    0.002    0.001   0.000%     99.995%         Const  mixed_2/tower/conv/batchnorm/moving_mean_max\n    1.078    0.002    0.001   0.000%     99.995%         Const  mixed_1/conv/batchnorm/beta_max\n    0.957    0.002    0.001   0.000%     99.995%         Const  mixed/tower_1/conv_2/batchnorm/beta_min\n    0.925    0.003    0.001   0.000%     99.995%         Const  mixed/tower_1/conv_2/batchnorm/moving_mean_min\n    5.733    0.002    0.001   0.000%     99.995%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/beta_max\n    5.703    0.002    0.001   0.000%     99.995%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/moving_mean_max\n    5.688    0.003    0.001   0.000%     99.995%         Const  mixed_9/tower_1/mixed/conv_1/conv2d_params_max\n    5.646    0.002    0.001   0.000%     99.996%         Const  mixed_9/tower_1/mixed/conv/batchnorm/moving_mean_max\n    5.587    0.002    0.001   0.000%     99.996%         Const  mixed_9/tower_1/conv_1/batchnorm/moving_mean_min\n    5.548    0.003    0.001   0.000%     99.996%         Const  mixed_9/tower_1/conv/batchnorm/moving_variance_max\n    5.502    0.002    0.001   0.000%     99.996%         Const  mixed_9/tower/mixed/conv_1/batchnorm/beta_min\n    5.445    0.002    0.001   0.000%     99.996%         Const  mixed_9/tower/mixed/conv/batchnorm/beta_max\n    5.430    0.002    0.001   0.000%     99.996%         Const  mixed_9/tower/mixed/conv/batchnorm/moving_variance_max\n    5.418    0.003    0.001   0.000%     99.996%         Const  mixed_9/tower/mixed/conv/batchnorm/moving_mean_max\n    5.266    0.002    0.001   0.000%     99.996%         Const  mixed_10/tower_2/conv/batchnorm/moving_variance_max\n    5.098    0.003    0.001   0.000%     99.996%         Const  mixed_10/tower_1/conv_1/batchnorm/beta_max\n    5.022    0.002    0.001   0.000%     99.996%         Const  mixed_10/tower_1/conv/batchnorm/moving_variance_max\n    4.926    0.002    0.001   0.000%     99.996%         Const  mixed_10/tower/mixed/conv/batchnorm/beta_max\n    4.848    0.002    0.001   0.000%     99.996%         Const  mixed_10/tower/conv/batchnorm/beta_max\n    4.807    0.003    0.001   0.000%     99.996%         Const  mixed_10/tower/conv/conv2d_params_max\n    4.612    0.002    0.001   0.000%     99.996%         Const  mixed_9/conv/conv2d_params_max\n    4.571    0.002    0.001   0.000%     99.996%         Const  mixed_8/tower_1/conv_3/batchnorm/moving_mean_max\n    4.545    0.002    0.001   0.000%     99.996%         Const  mixed_8/tower_1/conv_2/batchnorm/beta_max\n    4.514    0.002    0.001   0.000%     99.996%         Const  mixed_8/tower_1/conv_2/batchnorm/moving_mean_min\n    4.492    0.002    0.001   0.000%     99.996%         Const  mixed_8/tower_1/conv_1/batchnorm/beta_max\n    4.464    0.002    0.001   0.000%     99.996%         Const  mixed_8/tower_1/conv_1/batchnorm/moving_mean_max\n    4.438    0.002    0.001   0.000%     99.996%         Const  mixed_8/tower_1/conv/batchnorm/beta_max\n    4.397    0.003    0.001   0.000%     99.996%         Const  mixed_8/tower_1/conv/conv2d_params_max\n    4.288    0.002    0.001   0.000%     99.996%         Const  mixed_8/tower/conv/conv2d_params_max\n    4.270    0.002    0.001   0.000%     99.996%         Const  mixed_7/tower_2/conv/batchnorm/beta_min\n    4.163    0.005    0.001   0.000%     99.996%         Const  mixed_7/tower_1/conv_3/batchnorm/beta_max\n    4.137    0.002    0.001   0.000%     99.996%         Const  mixed_7/tower_1/conv_3/batchnorm/moving_mean_max\n    4.123    0.002    0.001   0.000%     99.997%         Const  mixed_7/tower_1/conv_3/conv2d_params_max\n    4.044    0.003    0.001   0.000%     99.997%         Const  mixed_7/tower_1/conv_1/batchnorm/moving_variance_max\n    4.019    0.002    0.001   0.000%     99.997%         Const  mixed_7/tower_1/conv_1/conv2d_params_max\n    3.947    0.002    0.001   0.000%     99.997%         Const  mixed_7/tower/conv_2/batchnorm/beta_min\n    3.918    0.003    0.001   0.000%     99.997%         Const  mixed_7/tower/conv_2/batchnorm/moving_mean_min\n    3.911    0.002    0.001   0.000%     99.997%         Const  mixed_7/tower/conv_2/conv2d_params_max\n    3.895    0.003    0.001   0.000%     99.997%         Const  mixed_7/tower/conv_1/batchnorm/beta_max\n    3.679    0.002    0.001   0.000%     99.997%         Const  mixed_6/tower_1/conv_4/batchnorm/beta_max\n    3.661    0.003    0.001   0.000%     99.997%         Const  mixed_6/tower_1/conv_4/batchnorm/moving_variance_min\n    3.626    0.002    0.001   0.000%     99.997%         Const  mixed_6/tower_1/conv_3/batchnorm/beta_max\n    3.474    0.002    0.001   0.000%     99.997%         Const  mixed_6/tower_1/conv_1/conv2d_params_max\n    3.459    0.002    0.001   0.000%     99.997%         Const  mixed_6/tower_1/conv/batchnorm/beta_max\n    3.345    0.002    0.001   0.000%     99.997%         Const  mixed_6/tower/conv_1/batchnorm/beta_min\n    3.020    0.002    0.001   0.000%     99.997%         Const  mixed_5/tower_1/conv_2/batchnorm/beta_min\n    2.968    0.002    0.001   0.000%     99.997%         Const  mixed_5/tower_1/conv_1/batchnorm/beta_max\n    2.964    0.002    0.001   0.000%     99.997%         Const  mixed_5/tower_1/conv_1/batchnorm/beta_min\n    2.809    0.002    0.001   0.000%     99.997%         Const  mixed_5/tower/conv_2/conv2d_params_min\n    2.418    0.005    0.001   0.000%     99.997%         Const  mixed_4/tower_1/conv_2/batchnorm/moving_mean_max\n    2.294    0.002    0.001   0.000%     99.997%         Const  mixed_4/tower_1/conv/conv2d_params_max\n    2.279    0.002    0.001   0.000%     99.997%         Const  mixed_4/tower/conv_2/batchnorm/beta_max\n    2.036    0.002    0.001   0.000%     99.997%         Const  mixed_3/tower/conv_2/batchnorm/moving_variance_max\n    1.955    0.003    0.001   0.000%     99.997%         Const  mixed_3/tower/conv_1/batchnorm/moving_mean_min\n    1.795    0.002    0.001   0.000%     99.997%         Const  mixed_2/tower_2/conv/batchnorm/beta_min\n    1.555    0.002    0.001   0.000%     99.997%         Const  mixed_2/tower/conv_1/batchnorm/moving_mean_min\n    1.454    0.002    0.001   0.000%     99.998%         Const  mixed_2/conv/batchnorm/moving_variance_min\n    1.334    0.003    0.001   0.000%     99.998%         Const  mixed_1/tower_1/conv_2/batchnorm/moving_mean_min\n    0.856    0.002    0.001   0.000%     99.998%         Const  mixed/tower_1/conv_1/batchnorm/moving_mean_min\n    5.785    0.002    0.001   0.000%     99.998%         Const  mixed_9/tower_2/conv/batchnorm/beta_min\n    5.675    0.003    0.001   0.000%     99.998%         Const  mixed_9/tower_1/mixed/conv/batchnorm/beta_max\n    5.632    0.005    0.001   0.000%     99.998%         Const  mixed_9/tower_1/mixed/conv/conv2d_params_max\n    5.628    0.002    0.001   0.000%     99.998%         Const  mixed_9/tower_1/mixed/conv/conv2d_params_min\n    5.559    0.002    0.001   0.000%     99.998%         Const  mixed_9/tower_1/conv/batchnorm/beta_min\n    5.505    0.003    0.001   0.000%     99.998%         Const  mixed_9/tower/mixed/conv_1/batchnorm/beta_max\n    5.198    0.003    0.001   0.000%     99.998%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/moving_mean_max\n    5.066    0.005    0.001   0.000%     99.998%         Const  mixed_10/tower_1/conv_1/batchnorm/moving_mean_min\n    4.978    0.003    0.001   0.000%     99.998%         Const  mixed_10/tower/mixed/conv_1/batchnorm/beta_min\n    4.833    0.002    0.001   0.000%     99.998%         Const  mixed_10/tower/conv/batchnorm/moving_variance_max\n    4.625    0.002    0.001   0.000%     99.998%         Const  mixed_9/conv/batchnorm/moving_mean_max\n    4.382    0.002    0.001   0.000%     99.998%         Const  mixed_8/tower/conv_1/batchnorm/beta_max\n    4.378    0.003    0.001   0.000%     99.998%         Const  mixed_8/tower/conv_1/batchnorm/beta_min\n    4.249    0.002    0.001   0.000%     99.998%         Const  mixed_7/tower_2/conv/batchnorm/moving_mean_max\n    4.245    0.002    0.001   0.000%     99.998%         Const  mixed_7/tower_2/conv/batchnorm/moving_mean_min\n    4.215    0.002    0.001   0.000%     99.998%         Const  mixed_7/tower_1/conv_4/batchnorm/beta_min\n    4.192    0.002    0.001   0.000%     99.998%         Const  mixed_7/tower_1/conv_4/batchnorm/moving_mean_max\n    3.785    0.002    0.001   0.000%     99.998%         Const  mixed_7/conv/batchnorm/beta_max\n    3.651    0.003    0.001   0.000%     99.998%         Const  mixed_6/tower_1/conv_4/batchnorm/moving_mean_max\n    3.282    0.002    0.001   0.000%     99.998%         Const  mixed_6/tower/conv/batchnorm/moving_variance_max\n    3.254    0.002    0.001   0.000%     99.998%         Const  mixed_6/tower/conv/conv2d_params_max\n    3.103    0.002    0.001   0.000%     99.998%         Const  mixed_5/tower_1/conv_4/batchnorm/moving_mean_min\n    2.531    0.002    0.001   0.000%     99.999%         Const  mixed_4/tower_1/conv_4/batchnorm/moving_mean_max\n    2.442    0.002    0.001   0.000%     99.999%         Const  mixed_4/tower_1/conv_2/batchnorm/beta_min\n    2.308    0.002    0.001   0.000%     99.999%         Const  mixed_4/tower_1/conv/batchnorm/moving_mean_max\n    1.905    0.002    0.001   0.000%     99.999%         Const  mixed_3/tower/conv/batchnorm/moving_mean_max\n    1.473    0.002    0.001   0.000%     99.999%         Const  mixed_2/conv/batchnorm/beta_max\n    0.988    0.002    0.001   0.000%     99.999%         Const  mixed/tower_2/conv/batchnorm/moving_mean_min\n    5.777    0.002    0.001   0.000%     99.999%         Const  mixed_9/tower_2/conv/batchnorm/moving_variance_max\n    5.579    0.002    0.001   0.000%     99.999%         Const  mixed_9/tower_1/conv_1/conv2d_params_max\n    5.521    0.002    0.001   0.000%     99.999%         Const  mixed_9/tower_1/conv/conv2d_params_max\n    5.404    0.002    0.001   0.000%     99.999%         Const  mixed_9/tower/mixed/conv/conv2d_params_max\n    5.157    0.002    0.001   0.000%     99.999%         Const  mixed_10/tower_1/mixed/conv/batchnorm/beta_min\n    4.405    0.003    0.001   0.000%     99.999%         Const  mixed_8/tower_1/conv/batchnorm/moving_mean_min\n    4.112    0.002    0.001   0.000%     99.999%         Const  mixed_7/tower_1/conv_2/batchnorm/beta_max\n    3.822    0.002    0.001   0.000%     99.999%         Const  mixed_7/tower/conv/batchnorm/moving_variance_min\n    3.096    0.002    0.001   0.000%     99.999%         Const  mixed_5/tower_1/conv_4/conv2d_params_max\n    2.544    0.002    0.001   0.000%     99.999%         Const  mixed_4/tower_1/conv_4/batchnorm/moving_variance_max\n    1.919    0.003    0.001   0.000%     99.999%         Const  mixed_3/tower/conv/batchnorm/moving_variance_max\n    1.894    0.002    0.001   0.000%     99.999%         Const  mixed_3/tower/conv/conv2d_params_max\n    1.487    0.002    0.001   0.000%     99.999%         Const  mixed_2/tower/conv/conv2d_params_max\n    1.483    0.003    0.001   0.000%     99.999%         Const  mixed_2/tower/conv/conv2d_params_min\n    5.714    0.003    0.001   0.000%     99.999%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/moving_variance_min\n    5.642    0.003    0.001   0.000%     99.999%         Const  mixed_9/tower_1/mixed/conv/batchnorm/moving_mean_min\n    5.620    0.002    0.001   0.000%     99.999%         Const  mixed_9/tower_1/conv_1/batchnorm/beta_max\n    5.223    0.003    0.001   0.000%     99.999%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/beta_max\n    5.005    0.002    0.001   0.000%     99.999%         Const  mixed_10/tower_1/conv/batchnorm/moving_mean_min\n    4.997    0.002    0.001   0.000%    100.000%         Const  mixed_10/tower_1/conv/conv2d_params_max\n    2.191    0.002    0.001   0.000%    100.000%         Const  mixed_4/tower/conv_1/batchnorm/moving_mean_min\n    5.762    0.002    0.001   0.000%    100.000%         Const  mixed_9/tower_2/conv/batchnorm/moving_mean_max\n    5.746    0.003    0.001   0.000%    100.000%         Const  mixed_9/tower_2/conv/conv2d_params_max\n    5.323    0.003    0.001   0.000%    100.000%         Const  softmax/biases_min\n    5.276    0.003    0.001   0.000%    100.000%         Const  mixed_10/tower_2/conv/batchnorm/beta_min\n    4.894    0.002    0.001   0.000%    100.000%         Const  mixed_10/tower/mixed/conv/batchnorm/moving_mean_min\n    4.585    0.002    0.001   0.000%    100.000%         Const  mixed_8/tower_1/conv_3/batchnorm/moving_variance_max\n    3.869    0.002    0.001   0.000%    100.000%         Const  mixed_7/tower/conv_1/batchnorm/moving_mean_max\n    4.593    0.002    0.001   0.000%    100.000%         Const  mixed_8/tower_1/conv_3/batchnorm/beta_min\n    3.364    0.002    0.001   0.000%    100.000%         Const  mixed_6/tower/conv_2/conv2d_params_max\n    2.760    0.002    0.001   0.000%    100.000%         Const  mixed_5/tower/conv_1/conv2d_params_max\n    2.315    0.002    0.001   0.000%    100.000%         Const  mixed_4/tower_1/conv/batchnorm/moving_variance_min\n    5.605    0.002    0.001   0.000%    100.000%         Const  mixed_9/tower_1/conv_1/batchnorm/moving_variance_max\n\nI tensorflow/core/util/stat_summarizer.cc:263] \n", "comments": ["the source info:\nI tensorflow/core/util/stat_summarizer.cc:262] 50 runs, avg 182.9 ms, 1004 nodes defined 902 nodes observed\n============ Top by duration =================\n  [start]  [first]    [avg]      [%]      [cdf%]          [Op]  [Name]\n   94.061   21.435    7.980   4.363%      4.363%        Conv2D  conv_4/Conv2D\n   38.605   26.650    5.479   2.995%      7.358%        Conv2D  conv_2/Conv2D\n  151.106    8.526    4.764   2.604%      9.962%        Conv2D  mixed_3/conv/Conv2D\n   17.560   17.822    4.198   2.295%     12.257%        Conv2D  conv_1/Conv2D\n  136.326    3.314    2.619   1.432%     13.688%        Conv2D  mixed_1/tower/conv_1/Conv2D\n  144.631    4.066    2.569   1.405%     15.093%        Conv2D  mixed_2/tower/conv_1/Conv2D\n  122.645   11.722    2.563   1.401%     16.494%        Conv2D  mixed/tower/conv_1/Conv2D\n  206.586    2.555    2.258   1.235%     17.729%        Conv2D  mixed_10/tower_1/conv/Conv2D\n  206.590    2.607    2.166   1.184%     18.913%        Conv2D  mixed_10/tower/conv/Conv2D\n  202.814    2.530    2.156   1.179%     20.091%        Conv2D  mixed_9/tower_1/conv_1/Conv2D\n  209.295    2.582    2.142   1.171%     21.262%        Conv2D  mixed_10/tower_1/conv_1/Conv2D\n  152.894    7.624    2.004   1.096%     22.358%        Conv2D  mixed_3/tower/conv_1/Conv2D\n  206.588    2.412    2.001   1.094%     23.452%        Conv2D  mixed_10/conv/Conv2D\n  136.342    4.142    1.849   1.011%     24.463%        Conv2D  mixed_1/tower_1/conv_1/Conv2D\n  144.815    2.646    1.839   1.006%     25.468%        Conv2D  mixed_2/tower_1/conv_1/Conv2D\n  185.613    2.002    1.817   0.993%     26.461%        Conv2D  mixed_7/tower/conv/Conv2D\n  185.612    2.204    1.773   0.969%     27.431%        Conv2D  mixed_7/conv/Conv2D\n  185.609    2.443    1.755   0.960%     28.390%        Conv2D  mixed_7/tower_1/conv/Conv2D\n  188.258    2.275    1.742   0.952%     29.342%        Conv2D  mixed_7/tower_1/conv_1/Conv2D\n  177.345    2.158    1.740   0.951%     30.293%        Conv2D  mixed_6/conv/Conv2D\n  122.162    3.315    1.710   0.935%     31.228%        Conv2D  mixed/tower_1/conv_1/Conv2D\n  188.239    2.180    1.692   0.925%     32.153%        Conv2D  mixed_7/tower/conv_1/Conv2D\n  168.775    2.537    1.692   0.925%     33.078%        Conv2D  mixed_5/conv/Conv2D\n  190.679    2.732    1.620   0.886%     33.963%        Conv2D  mixed_7/tower/conv_2/Conv2D\n  168.775    1.907    1.603   0.876%     34.840%        Conv2D  mixed_5/tower/conv/Conv2D\n  190.722    1.595    1.601   0.875%     35.715%        Conv2D  mixed_7/tower_1/conv_2/Conv2D\n  161.589    2.124    1.591   0.870%     36.585%        Conv2D  mixed_4/conv/Conv2D\n  168.771    1.447    1.573   0.860%     37.445%        Conv2D  mixed_5/tower_1/conv/Conv2D\n  177.342    1.877    1.538   0.841%     38.285%        Conv2D  mixed_6/tower_1/conv/Conv2D\n  177.347    1.806    1.531   0.837%     39.122%        Conv2D  mixed_6/tower/conv/Conv2D\n  186.648    1.652    1.520   0.831%     39.953%        Conv2D  mixed_7/tower_2/conv/Conv2D\n  200.782    1.755    1.479   0.809%     40.762%        Conv2D  mixed_9/tower_1/conv/Conv2D\n  129.421    3.118    1.433   0.783%     41.545%        Conv2D  mixed/tower_1/conv_2/Conv2D\n  200.787    1.514    1.427   0.780%     42.325%        Conv2D  mixed_9/tower/conv/Conv2D\n  207.336    1.637    1.424   0.778%     43.104%        Conv2D  mixed_10/tower_2/conv/Conv2D\n  140.832    1.924    1.401   0.766%     43.870%        Conv2D  mixed_1/tower_1/conv_2/Conv2D\n  161.587    1.626    1.393   0.761%     44.631%        Conv2D  mixed_4/tower_1/conv/Conv2D\n  161.592    2.007    1.373   0.750%     45.381%        Conv2D  mixed_4/tower/conv/Conv2D\n  148.444    2.111    1.371   0.749%     46.131%        Conv2D  mixed_2/tower_1/conv_2/Conv2D\n   65.272   15.005    1.357   0.742%     46.873%    BatchNormWithGlobalNormalization    conv_2/batchnorm\n  200.783    1.725    1.345   0.735%     47.608%        Conv2D  mixed_9/conv/Conv2D\n  143.184    2.203    1.335   0.730%     48.338%       AvgPool  mixed_2/tower_2/pool\n  197.185    1.639    1.327   0.726%     49.063%        Conv2D  mixed_8/tower/conv_1/Conv2D\n  181.535    1.741    1.320   0.721%     49.785%        Conv2D  mixed_6/tower/conv_2/Conv2D\n  178.417    1.634    1.303   0.713%     50.497%        Conv2D  mixed_6/tower_2/conv/Conv2D\n  179.568    1.695    1.303   0.713%     51.210%        Conv2D  mixed_6/tower/conv_1/Conv2D\n  173.453    1.711    1.263   0.691%     51.900%        Conv2D  mixed_5/tower/conv_2/Conv2D\n  197.149    1.504    1.246   0.681%     52.581%        Conv2D  mixed_8/tower_1/conv_1/Conv2D\n  179.855    1.557    1.245   0.681%     53.262%        Conv2D  mixed_6/tower_1/conv_1/Conv2D\n  171.149    1.888    1.245   0.680%     53.942%        Conv2D  mixed_5/tower/conv_1/Conv2D\n   89.420    1.278    1.244   0.680%     54.622%    CheckNumerics   pool/CheckNumerics\n   80.287    1.205    1.243   0.680%     55.302%    CheckNumerics   conv_2/CheckNumerics\n  169.879    1.642    1.237   0.676%     55.979%        Conv2D  mixed_5/tower_2/conv/Conv2D\n  162.735    2.098    1.233   0.674%     56.653%        Conv2D  mixed_4/tower_2/conv/Conv2D\n  170.852    1.771    1.226   0.670%     57.323%        Conv2D  mixed_5/tower_1/conv_1/Conv2D\n  181.621    1.490    1.223   0.669%     57.992%        Conv2D  mixed_6/tower_1/conv_2/Conv2D\n  134.795    1.971    1.221   0.668%     58.659%       AvgPool  mixed_1/tower_2/pool\n  172.992    1.166    1.219   0.666%     59.326%        Conv2D  mixed_5/tower_1/conv_2/Conv2D\n   90.702    1.124    1.136   0.621%     59.947%       MaxPool  pool\n  202.514    1.339    1.126   0.616%     60.563%        Conv2D  mixed_9/tower/mixed/conv/Conv2D\n  209.345    1.300    1.120   0.612%     61.175%        Conv2D  mixed_10/tower/mixed/conv/Conv2D\n  202.515    1.514    1.119   0.612%     61.787%        Conv2D  mixed_9/tower/mixed/conv_1/Conv2D\n    0.221    5.524    1.096   0.599%     62.386%        Conv2D  conv/Conv2D\n  209.342    1.296    1.087   0.594%     62.980%        Conv2D  mixed_10/tower/mixed/conv_1/Conv2D\n  120.736    2.019    1.061   0.580%     63.560%       AvgPool  mixed/tower_2/pool\n  163.587    1.584    1.043   0.570%     64.130%        Conv2D  mixed_4/tower_1/conv_1/Conv2D\n  195.722    1.226    1.033   0.564%     64.694%        Conv2D  mixed_8/tower/conv/Conv2D\n  195.717    1.207    0.999   0.546%     65.241%        Conv2D  mixed_8/tower_1/conv/Conv2D\n  201.178    1.200    0.998   0.546%     65.787%        Conv2D  mixed_9/tower_2/conv/Conv2D\n  192.659    1.540    0.964   0.527%     66.313%        Conv2D  mixed_7/tower_1/conv_3/Conv2D\n  164.052    1.313    0.939   0.513%     66.827%        Conv2D  mixed_4/tower/conv_1/Conv2D\n  165.577    1.248    0.923   0.505%     67.331%        Conv2D  mixed_4/tower/conv_2/Conv2D\n  198.907    1.097    0.918   0.502%     67.833%        Conv2D  mixed_8/tower_1/conv_2/Conv2D\n  194.387    1.080    0.915   0.500%     68.334%        Conv2D  mixed_7/tower_1/conv_4/Conv2D\n  145.393    1.863    0.914   0.500%     68.833%        Conv2D  mixed_2/tower_2/conv/Conv2D\n  136.775    1.513    0.912   0.499%     69.332%        Conv2D  mixed_1/tower_2/conv/Conv2D\n  118.357    1.407    0.878   0.480%     69.812%    CheckNumerics   pool_1/CheckNumerics\n  116.741    1.388    0.877   0.479%     70.291%    CheckNumerics   conv_4/CheckNumerics\n  143.187    1.349    0.827   0.452%     70.743%        Conv2D  mixed_2/conv/Conv2D\n  160.912    0.475    0.820   0.448%     71.191%        Conv2D  mixed_3/tower/conv_2/Conv2D\n  143.185    1.188    0.817   0.447%     71.638%        Conv2D  mixed_2/tower_1/conv/Conv2D\n  151.103    1.070    0.798   0.436%     72.075%        Conv2D  mixed_3/tower/conv/Conv2D\n  205.463    0.924    0.789   0.431%     72.506%        Conv2D  mixed_9/tower_1/mixed/conv/Conv2D\n  211.994    0.881    0.785   0.429%     72.935%        Conv2D  mixed_10/tower_1/mixed/conv/Conv2D\n  205.460    0.920    0.784   0.429%     73.364%        Conv2D  mixed_9/tower_1/mixed/conv_1/Conv2D\n  211.991    0.937    0.779   0.426%     73.790%        Conv2D  mixed_10/tower_1/mixed/conv_1/Conv2D\n  143.190    0.927    0.779   0.426%     74.215%        Conv2D  mixed_2/tower/conv/Conv2D\n  161.587    1.139    0.771   0.421%     74.637%       AvgPool  mixed_4/tower_2/pool\n  185.609    1.034    0.769   0.420%     75.057%       AvgPool  mixed_7/tower_2/pool\n  176.069    0.980    0.768   0.420%     75.477%        Conv2D  mixed_5/tower_1/conv_4/Conv2D\n  177.341    1.071    0.767   0.420%     75.896%       AvgPool  mixed_6/tower_2/pool\n  115.507    1.228    0.762   0.417%     76.313%    BatchNormWithGlobalNormalization    conv_4/batchnorm\n  168.769    1.102    0.759   0.415%     76.728%       AvgPool  mixed_5/tower_2/pool\n  184.398    0.934    0.757   0.414%     77.142%        Conv2D  mixed_6/tower_1/conv_4/Conv2D\n  134.796    1.142    0.753   0.412%     77.554%        Conv2D  mixed_1/conv/Conv2D\n  134.792    1.138    0.747   0.408%     77.962%        Conv2D  mixed_1/tower_1/conv/Conv2D\n  165.466    1.095    0.744   0.407%     78.369%        Conv2D  mixed_4/tower_1/conv_2/Conv2D\n  134.797    1.118    0.703   0.384%     78.753%        Conv2D  mixed_1/tower/conv/Conv2D\n  183.400    0.814    0.698   0.382%     79.135%        Conv2D  mixed_6/tower_1/conv_3/Conv2D\n  174.580    1.306    0.694   0.379%     79.515%        Conv2D  mixed_5/tower_1/conv_3/Conv2D\n    7.336    1.785    0.682   0.373%     79.887%    CheckNumerics   conv/CheckNumerics\n   36.782    1.489    0.657   0.359%     80.247%    CheckNumerics   conv_1/CheckNumerics\n  120.825    0.863    0.623   0.340%     80.587%        Conv2D  mixed/conv/Conv2D\n  151.104    0.920    0.622   0.340%     80.927%    CheckNumerics   mixed_3/pool/CheckNumerics\n  167.647    0.780    0.616   0.337%     81.264%        Conv2D  mixed_4/tower_1/conv_4/Conv2D\n  120.860    0.838    0.613   0.335%     81.599%        Conv2D  mixed/tower/conv/Conv2D\n  119.770    0.920    0.612   0.335%     81.934%       MaxPool  pool_1\n  213.147    0.648    0.609   0.333%     82.267%        MatMul  softmax/logits/MatMul\n    5.764    1.560    0.604   0.330%     82.597%    BatchNormWithGlobalNormalization    conv/batchnorm\n   35.399    1.374    0.576   0.315%     82.912%    BatchNormWithGlobalNormalization    conv_1/batchnorm\n  122.782    0.751    0.560   0.306%     83.218%        Conv2D  mixed/tower_2/conv/Conv2D\n  120.703    0.972    0.558   0.305%     83.523%        Conv2D  mixed/tower_1/conv/Conv2D\n  166.864    0.625    0.523   0.286%     83.809%        Conv2D  mixed_4/tower_1/conv_3/Conv2D\n  138.297    0.394    0.451   0.246%     84.056%    BatchNormWithGlobalNormalization    mixed_1/tower_2/conv/batchnorm\n   81.498    7.916    0.434   0.237%     84.293%          Relu  conv_2\n   91.829    0.756    0.429   0.234%     84.528%        Conv2D  conv_3/Conv2D\n   93.259    0.679    0.394   0.215%     84.743%    CheckNumerics   conv_3/CheckNumerics\n  147.265    0.281    0.387   0.212%     84.955%    BatchNormWithGlobalNormalization    mixed_2/tower_2/conv/batchnorm\n  206.844    0.487    0.368   0.201%     85.156%       MaxPool  mixed_10/tower_2/pool\n  160.522    0.180    0.368   0.201%     85.357%    BatchNormWithGlobalNormalization    mixed_3/tower/conv_1/batchnorm\n  200.231    0.424    0.368   0.201%     85.558%        Conv2D  mixed_8/tower_1/conv_3/Conv2D\n    9.132    8.416    0.366   0.200%     85.758%          Relu  conv\n  195.719    0.482    0.365   0.200%     85.958%    CheckNumerics   mixed_8/pool/CheckNumerics\n   92.588    0.660    0.363   0.199%     86.157%    BatchNormWithGlobalNormalization    conv_3/batchnorm\n  152.182    0.437    0.362   0.198%     86.354%    BatchNormWithGlobalNormalization    mixed_3/tower/conv/batchnorm\n  125.482    2.546    0.353   0.193%     86.547%    BatchNormWithGlobalNormalization    mixed/tower_1/conv_1/batchnorm\n  123.539    1.569    0.346   0.189%     86.736%    BatchNormWithGlobalNormalization    mixed/tower_2/conv/batchnorm\n  147.470    0.533    0.308   0.168%     86.904%    BatchNormWithGlobalNormalization    mixed_2/tower_1/conv_1/batchnorm\n  152.034    0.494    0.307   0.168%     87.072%       MaxPool  mixed_3/pool\n  140.488    0.187    0.307   0.168%     87.240%    BatchNormWithGlobalNormalization    mixed_1/tower_1/conv_1/batchnorm\n  148.704    0.484    0.286   0.156%     87.396%    BatchNormWithGlobalNormalization    mixed_2/tower/conv_1/batchnorm\n  200.782    0.390    0.286   0.156%     87.552%       AvgPool  mixed_9/tower_2/pool\n  171.529    0.495    0.280   0.153%     87.706%    BatchNormWithGlobalNormalization    mixed_5/tower_2/conv/batchnorm\n  134.373    0.133    0.276   0.151%     87.857%    BatchNormWithGlobalNormalization    mixed/tower/conv_1/batchnorm\n  187.822    0.172    0.261   0.143%     87.999%    BatchNormWithGlobalNormalization    mixed_7/conv/batchnorm\n  171.319    0.451    0.260   0.142%     88.142%    BatchNormWithGlobalNormalization    mixed_5/conv/batchnorm\n  179.511    0.234    0.259   0.142%     88.283%    BatchNormWithGlobalNormalization    mixed_6/conv/batchnorm\n  164.840    0.487    0.259   0.142%     88.425%    BatchNormWithGlobalNormalization    mixed_4/tower_2/conv/batchnorm\n  188.058    0.108    0.255   0.140%     88.565%    BatchNormWithGlobalNormalization    mixed_7/tower_1/conv/batchnorm\n  187.623    0.489    0.254   0.139%     88.704%    BatchNormWithGlobalNormalization    mixed_7/tower/conv/batchnorm\n  180.056    0.287    0.250   0.136%     88.840%    BatchNormWithGlobalNormalization    mixed_6/tower_2/conv/batchnorm\n  179.159    0.250    0.246   0.135%     88.975%    BatchNormWithGlobalNormalization    mixed_6/tower/conv/batchnorm\n  139.646    0.358    0.246   0.135%     89.109%    BatchNormWithGlobalNormalization    mixed_1/tower/conv_1/batchnorm\n  121.680    0.301    0.245   0.134%     89.243%    BatchNormWithGlobalNormalization    mixed/tower_1/conv/batchnorm\n  163.722    0.419    0.242   0.132%     89.376%    BatchNormWithGlobalNormalization    mixed_4/conv/batchnorm\n  144.379    0.168    0.239   0.131%     89.506%    BatchNormWithGlobalNormalization    mixed_2/tower_1/conv/batchnorm\n  144.542    2.304    0.237   0.130%     89.636%    BatchNormWithGlobalNormalization    mixed_2/conv/batchnorm\n  179.228    0.454    0.234   0.128%     89.764%    BatchNormWithGlobalNormalization    mixed_6/tower_1/conv/batchnorm\n  170.223    0.448    0.232   0.127%     89.891%    BatchNormWithGlobalNormalization    mixed_5/tower_1/conv/batchnorm\n  170.690    0.283    0.227   0.124%     90.015%    BatchNormWithGlobalNormalization    mixed_5/tower/conv/batchnorm\n  188.306    0.360    0.207   0.113%     90.128%    BatchNormWithGlobalNormalization    mixed_7/tower_2/conv/batchnorm\n  163.607    0.304    0.199   0.109%     90.237%    BatchNormWithGlobalNormalization    mixed_4/tower/conv/batchnorm\n  135.927    0.221    0.195   0.107%     90.344%    BatchNormWithGlobalNormalization    mixed_1/tower/conv/batchnorm\n  163.219    0.232    0.193   0.106%     90.449%    BatchNormWithGlobalNormalization    mixed_4/tower_1/conv/batchnorm\n  160.707    0.158    0.193   0.105%     90.555%    CheckNumerics   mixed_3/tower/conv_1/CheckNumerics\n  148.011    0.324    0.188   0.103%     90.658%    CheckNumerics   mixed_2/tower_1/conv_1/CheckNumerics\n  135.935    0.299    0.186   0.102%     90.759%    BatchNormWithGlobalNormalization    mixed_1/tower_1/conv/batchnorm\n  121.709    0.685    0.184   0.101%     90.860%    BatchNormWithGlobalNormalization    mixed/tower/conv/batchnorm\n  121.695    0.329    0.183   0.100%     90.960%    BatchNormWithGlobalNormalization    mixed/conv/batchnorm\n  206.586    0.250    0.181   0.099%     91.059%    CheckNumerics   mixed_10/tower_2/pool/CheckNumerics\n  128.035    0.305    0.181   0.099%     91.158%    CheckNumerics   mixed/tower_1/conv_1/CheckNumerics\n  140.678    0.106    0.179   0.098%     91.256%    CheckNumerics   mixed_1/tower_1/conv_1/CheckNumerics\n  135.942    0.314    0.170   0.093%     91.349%    BatchNormWithGlobalNormalization    mixed_1/conv/batchnorm\n  196.210    0.217    0.169   0.092%     91.442%       MaxPool  mixed_8/pool\n  144.125    0.317    0.161   0.088%     91.530%    BatchNormWithGlobalNormalization    mixed_2/tower/conv/batchnorm\n  159.639    0.184    0.147   0.080%     91.610%    BatchNormWithGlobalNormalization    mixed_3/conv/batchnorm\n  193.417    0.146    0.147   0.080%     91.691%    BatchNormWithGlobalNormalization    mixed_7/tower/conv_2/batchnorm\n  190.537    0.088    0.143   0.078%     91.769%    BatchNormWithGlobalNormalization    mixed_7/tower_1/conv_1/batchnorm\n  138.703    0.234    0.140   0.076%     91.845%    CheckNumerics   mixed_1/tower_2/conv/CheckNumerics\n  152.627    0.204    0.136   0.075%     91.919%    CheckNumerics   mixed_3/tower/conv/CheckNumerics\n  147.553    0.208    0.136   0.075%     91.994%    CheckNumerics   mixed_2/tower_2/conv/CheckNumerics\n  173.044    0.238    0.135   0.074%     92.068%    BatchNormWithGlobalNormalization    mixed_5/tower/conv_1/batchnorm\n  132.544    0.227    0.133   0.072%     92.140%    BatchNormWithGlobalNormalization    mixed/tower_1/conv_2/batchnorm\n  142.760    0.195    0.133   0.072%     92.213%    BatchNormWithGlobalNormalization    mixed_1/tower_1/conv_2/batchnorm\n  190.425    0.100    0.130   0.071%     92.284%    BatchNormWithGlobalNormalization    mixed_7/tower/conv_1/batchnorm\n  166.829    0.140    0.129   0.071%     92.354%    BatchNormWithGlobalNormalization    mixed_4/tower/conv_2/batchnorm\n  175.171    0.297    0.129   0.070%     92.425%    BatchNormWithGlobalNormalization    mixed_5/tower/conv_2/batchnorm\n  174.164    0.251    0.127   0.069%     92.494%    BatchNormWithGlobalNormalization    mixed_5/tower_1/conv_2/batchnorm\n  181.417    0.083    0.126   0.069%     92.563%    BatchNormWithGlobalNormalization    mixed_6/tower_1/conv_1/batchnorm\n  150.561    0.186    0.126   0.069%     92.632%    BatchNormWithGlobalNormalization    mixed_2/tower_1/conv_2/batchnorm\n  140.012    0.233    0.124   0.068%     92.700%    CheckNumerics   mixed_1/tower/conv_1/CheckNumerics\n  149.196    0.214    0.124   0.068%     92.767%    CheckNumerics   mixed_2/tower/conv_1/CheckNumerics\n  118.136    0.217    0.123   0.067%     92.835%          Relu  conv_4\n   38.280    0.312    0.122   0.067%     92.901%          Relu  conv_1\n  192.321    0.196    0.122   0.067%     92.968%    BatchNormWithGlobalNormalization    mixed_7/tower_1/conv_2/batchnorm\n  134.511    0.122    0.121   0.066%     93.034%    CheckNumerics   mixed/tower/conv_1/CheckNumerics\n  183.281    0.099    0.120   0.066%     93.100%    BatchNormWithGlobalNormalization    mixed_6/tower/conv_2/batchnorm\n  172.628    0.209    0.118   0.065%     93.164%    BatchNormWithGlobalNormalization    mixed_5/tower_1/conv_1/batchnorm\n  183.116    0.156    0.117   0.064%     93.228%    BatchNormWithGlobalNormalization    mixed_6/tower_1/conv_2/batchnorm\n  181.272    0.141    0.117   0.064%     93.292%    BatchNormWithGlobalNormalization    mixed_6/tower/conv_1/batchnorm\n  159.830    0.287    0.117   0.064%     93.356%    CheckNumerics   mixed_3/conv/CheckNumerics\n  166.567    0.211    0.115   0.063%     93.419%    BatchNormWithGlobalNormalization    mixed_4/tower_1/conv_2/batchnorm\n  132.783    0.350    0.113   0.062%     93.480%    CheckNumerics   mixed/tower_1/conv_2/CheckNumerics\n  136.237    0.072    0.113   0.062%     93.542%    CheckNumerics   mixed_1/tower_1/conv/CheckNumerics\n  134.681    0.096    0.112   0.061%     93.603%        Concat  mixed/join\n  165.368    0.089    0.112   0.061%     93.664%    BatchNormWithGlobalNormalization    mixed_4/tower/conv_1/batchnorm\n  121.988    0.118    0.111   0.061%     93.725%    CheckNumerics   mixed/tower_1/conv/CheckNumerics\n  146.854    0.212    0.111   0.061%     93.786%    CheckNumerics   mixed_2/conv/CheckNumerics\n  150.753    0.199    0.110   0.060%     93.846%    CheckNumerics   mixed_2/tower_1/conv_2/CheckNumerics\n  142.959    0.105    0.110   0.060%     93.906%    CheckNumerics   mixed_1/tower_1/conv_2/CheckNumerics\n  122.032    0.129    0.110   0.060%     93.966%    CheckNumerics   mixed/conv/CheckNumerics\n  136.261    0.123    0.108   0.059%     94.025%    CheckNumerics   mixed_1/conv/CheckNumerics\n  144.554    0.202    0.107   0.058%     94.083%    CheckNumerics   mixed_2/tower_1/conv/CheckNumerics\n  143.102    0.069    0.106   0.058%     94.141%        Concat  mixed_1/join\n  196.929    0.117    0.106   0.058%     94.199%    BatchNormWithGlobalNormalization    mixed_8/tower_1/conv/batchnorm\n  209.004    0.088    0.105   0.058%     94.257%    BatchNormWithGlobalNormalization    mixed_10/conv/batchnorm\n  165.177    0.192    0.103   0.056%     94.313%    BatchNormWithGlobalNormalization    mixed_4/tower_1/conv_1/batchnorm\n  210.641    0.113    0.102   0.056%     94.369%    BatchNormWithGlobalNormalization    mixed_10/tower/mixed/conv_1/batchnorm\n  202.511    0.072    0.101   0.055%     94.424%    BatchNormWithGlobalNormalization    mixed_9/conv/batchnorm\n  204.032    0.074    0.101   0.055%     94.479%    BatchNormWithGlobalNormalization    mixed_9/tower/mixed/conv_1/batchnorm\n  198.658    0.098    0.100   0.055%     94.534%    BatchNormWithGlobalNormalization    mixed_8/tower_1/conv_1/batchnorm\n  196.956    0.115    0.100   0.055%     94.589%    BatchNormWithGlobalNormalization    mixed_8/tower/conv/batchnorm\n  165.334    0.096    0.097   0.053%     94.642%    CheckNumerics   mixed_4/tower_2/conv/CheckNumerics\n  180.350    0.136    0.097   0.053%     94.695%    CheckNumerics   mixed_6/tower_2/conv/CheckNumerics\n  179.751    0.144    0.096   0.053%     94.747%    CheckNumerics   mixed_6/conv/CheckNumerics\n  171.776    0.098    0.096   0.053%     94.800%    CheckNumerics   mixed_5/conv/CheckNumerics\n  172.032    0.137    0.096   0.053%     94.852%    CheckNumerics   mixed_5/tower_2/conv/CheckNumerics\n  188.000    0.130    0.096   0.052%     94.905%    CheckNumerics   mixed_7/conv/CheckNumerics\n  210.649    0.110    0.096   0.052%     94.957%    BatchNormWithGlobalNormalization    mixed_10/tower/mixed/conv/batchnorm\n  160.871    0.038    0.095   0.052%     95.009%          Relu  mixed_3/tower/conv_1\n  164.148    0.149    0.095   0.052%     95.061%    CheckNumerics   mixed_4/conv/CheckNumerics\n  202.382    0.085    0.095   0.052%     95.113%    BatchNormWithGlobalNormalization    mixed_9/tower_2/conv/batchnorm\n  188.673    0.141    0.092   0.050%     95.163%    CheckNumerics   mixed_7/tower_2/conv/CheckNumerics\n  188.118    0.088    0.091   0.050%     95.213%    CheckNumerics   mixed_7/tower/conv/CheckNumerics\n  208.979    0.145    0.091   0.050%     95.263%    BatchNormWithGlobalNormalization    mixed_10/tower_2/conv/batchnorm\n  136.158    0.125    0.090   0.049%     95.312%    CheckNumerics   mixed_1/tower/conv/CheckNumerics\n  188.172    0.052    0.090   0.049%     95.361%    CheckNumerics   mixed_7/tower_1/conv/CheckNumerics\n   93.945    0.112    0.089   0.049%     95.410%          Relu  conv_3\n  161.391    0.063    0.087   0.047%     95.457%    BatchNormWithGlobalNormalization    mixed_3/tower/conv_2/batchnorm\n  122.402    0.188    0.087   0.047%     95.504%    CheckNumerics   mixed/tower/conv/CheckNumerics\n  151.003    0.087    0.086   0.047%     95.552%        Concat  mixed_2/join\n  128.349    1.067    0.083   0.046%     95.597%          Relu  mixed/tower_1/conv_1\n  203.857    0.217    0.083   0.045%     95.643%    BatchNormWithGlobalNormalization    mixed_9/tower/mixed/conv/batchnorm\n  193.570    0.124    0.082   0.045%     95.687%    CheckNumerics   mixed_7/tower/conv_2/CheckNumerics\n  166.976    0.158    0.082   0.045%     95.732%    CheckNumerics   mixed_4/tower/conv_2/CheckNumerics\n  190.530    0.110    0.081   0.044%     95.776%    CheckNumerics   mixed_7/tower/conv_1/CheckNumerics\n  179.690    0.124    0.081   0.044%     95.821%    CheckNumerics   mixed_6/tower_1/conv/CheckNumerics\n  144.449    0.134    0.081   0.044%     95.865%    CheckNumerics   mixed_2/tower/conv/CheckNumerics\n  170.980    0.126    0.081   0.044%     95.909%    CheckNumerics   mixed_5/tower/conv/CheckNumerics\n  175.476    0.136    0.080   0.044%     95.953%    CheckNumerics   mixed_5/tower/conv_2/CheckNumerics\n    0.000    0.056    0.079   0.043%     95.996%                _SOURCE\n  170.678    0.135    0.078   0.043%     96.039%    CheckNumerics   mixed_5/tower_1/conv/CheckNumerics\n  179.416    0.115    0.078   0.043%     96.082%    CheckNumerics   mixed_6/tower/conv/CheckNumerics\n  209.146    0.063    0.077   0.042%     96.124%    BatchNormWithGlobalNormalization    mixed_10/tower_1/conv/batchnorm\n  175.891    0.085    0.076   0.042%     96.166%    BatchNormWithGlobalNormalization    mixed_5/tower_1/conv_3/batchnorm\n  183.384    0.107    0.076   0.042%     96.207%    CheckNumerics   mixed_6/tower/conv_2/CheckNumerics\n  190.628    0.052    0.076   0.041%     96.249%    CheckNumerics   mixed_7/tower_1/conv_1/CheckNumerics\n  209.200    0.057    0.075   0.041%     96.290%    BatchNormWithGlobalNormalization    mixed_10/tower/conv/batchnorm\n  202.304    0.154    0.075   0.041%     96.331%    BatchNormWithGlobalNormalization    mixed_9/tower/conv/batchnorm\n  184.218    0.084    0.074   0.041%     96.371%    BatchNormWithGlobalNormalization    mixed_6/tower_1/conv_3/batchnorm\n  202.541    0.142    0.074   0.041%     96.412%    BatchNormWithGlobalNormalization    mixed_9/tower_1/conv/batchnorm\n  200.008    0.080    0.074   0.041%     96.453%    BatchNormWithGlobalNormalization    mixed_8/tower_1/conv_2/batchnorm\n  195.470    0.081    0.074   0.040%     96.493%    BatchNormWithGlobalNormalization    mixed_7/tower_1/conv_4/batchnorm\n  168.431    0.088    0.074   0.040%     96.533%    BatchNormWithGlobalNormalization    mixed_4/tower_1/conv_4/batchnorm\n  185.336    0.082    0.074   0.040%     96.574%    BatchNormWithGlobalNormalization    mixed_6/tower_1/conv_4/batchnorm\n  194.202    0.078    0.074   0.040%     96.614%    BatchNormWithGlobalNormalization    mixed_7/tower_1/conv_3/batchnorm\n  177.055    0.086    0.073   0.040%     96.654%    BatchNormWithGlobalNormalization    mixed_5/tower_1/conv_4/batchnorm\n  125.120    0.115    0.071   0.039%     96.693%    CheckNumerics   mixed/tower_2/conv/CheckNumerics\n  198.760    0.111    0.070   0.038%     96.731%    CheckNumerics   mixed_8/tower_1/conv_1/CheckNumerics\n  198.827    0.051    0.069   0.038%     96.769%    BatchNormWithGlobalNormalization    mixed_8/tower/conv_1/batchnorm\n  173.289    0.124    0.069   0.038%     96.806%    CheckNumerics   mixed_5/tower/conv_1/CheckNumerics\n  197.075    0.071    0.068   0.037%     96.843%    CheckNumerics   mixed_8/tower/conv/CheckNumerics\n  167.492    0.075    0.067   0.037%     96.880%    BatchNormWithGlobalNormalization    mixed_4/tower_1/conv_3/batchnorm\n  192.522    0.097    0.067   0.037%     96.916%    CheckNumerics   mixed_7/tower_1/conv_2/CheckNumerics\n  197.052    0.068    0.066   0.036%     96.953%    CheckNumerics   mixed_8/tower_1/conv/CheckNumerics\n  172.844    0.112    0.064   0.035%     96.988%    CheckNumerics   mixed_5/tower_1/conv_1/CheckNumerics\n  181.505    0.076    0.064   0.035%     97.023%    CheckNumerics   mixed_6/tower_1/conv_1/CheckNumerics\n  163.919    0.096    0.064   0.035%     97.058%    CheckNumerics   mixed_4/tower/conv/CheckNumerics\n  163.460    0.090    0.064   0.035%     97.093%    CheckNumerics   mixed_4/tower_1/conv/CheckNumerics\n  206.387    0.071    0.063   0.034%     97.127%    BatchNormWithGlobalNormalization    mixed_9/tower_1/mixed/conv_1/batchnorm\n  181.418    0.088    0.061   0.034%     97.161%    CheckNumerics   mixed_6/tower/conv_1/CheckNumerics\n  183.277    0.094    0.061   0.033%     97.194%    CheckNumerics   mixed_6/tower_1/conv_2/CheckNumerics\n  212.930    0.055    0.060   0.033%     97.227%    BatchNormWithGlobalNormalization    mixed_10/tower_1/mixed/conv_1/batchnorm\n  174.423    0.119    0.059   0.032%     97.259%    CheckNumerics   mixed_5/tower_1/conv_2/CheckNumerics\n  212.877    0.055    0.059   0.032%     97.291%    BatchNormWithGlobalNormalization    mixed_10/tower_1/mixed/conv/batchnorm\n  140.789    0.041    0.058   0.032%     97.323%          Relu  mixed_1/tower_1/conv_1\n  206.390    0.071    0.058   0.032%     97.355%    BatchNormWithGlobalNormalization    mixed_9/tower_1/mixed/conv/batchnorm\n  177.145    0.074    0.056   0.030%     97.385%    CheckNumerics   mixed_5/tower_1/conv_4/CheckNumerics\n  200.094    0.102    0.056   0.030%     97.416%    CheckNumerics   mixed_8/tower_1/conv_2/CheckNumerics\n  168.526    0.116    0.055   0.030%     97.446%    CheckNumerics   mixed_4/tower_1/conv_4/CheckNumerics\n  195.657    0.052    0.055   0.030%     97.476%        Concat  mixed_7/join\n  165.462    0.079    0.055   0.030%     97.506%    CheckNumerics   mixed_4/tower/conv_1/CheckNumerics\n  194.285    0.077    0.055   0.030%     97.536%    CheckNumerics   mixed_7/tower_1/conv_3/CheckNumerics\n  185.422    0.076    0.054   0.030%     97.566%    CheckNumerics   mixed_6/tower_1/conv_4/CheckNumerics\n  195.556    0.066    0.054   0.029%     97.595%    CheckNumerics   mixed_7/tower_1/conv_4/CheckNumerics\n  148.345    0.093    0.052   0.029%     97.624%          Relu  mixed_2/tower_1/conv_1\n  165.373    0.069    0.052   0.028%     97.652%    CheckNumerics   mixed_4/tower_1/conv_1/CheckNumerics\n  177.260    0.069    0.051   0.028%     97.680%        Concat  mixed_5/join\n  185.538    0.060    0.050   0.027%     97.707%        Concat  mixed_6/join\n  168.683    0.075    0.049   0.027%     97.734%        Concat  mixed_4/join\n  161.519    0.057    0.049   0.027%     97.761%        Concat  mixed_3/join\n  205.347    0.056    0.049   0.027%     97.788%    BatchNormWithGlobalNormalization    mixed_9/tower_1/conv_1/batchnorm\n  211.879    0.056    0.049   0.027%     97.814%    BatchNormWithGlobalNormalization    mixed_10/tower_1/conv_1/batchnorm\n  206.516    0.057    0.048   0.026%     97.840%        Concat  mixed_9/join\n  166.785    0.050    0.048   0.026%     97.866%    CheckNumerics   mixed_4/tower_1/conv_2/CheckNumerics\n  175.980    0.061    0.046   0.025%     97.892%    CheckNumerics   mixed_5/tower_1/conv_3/CheckNumerics\n  213.089    0.052    0.046   0.025%     97.917%       AvgPool  pool_3\n  184.306    0.065    0.046   0.025%     97.942%    CheckNumerics   mixed_6/tower_1/conv_3/CheckNumerics\n  213.038    0.045    0.046   0.025%     97.968%        Concat  mixed_10/join\n  133.144    0.150    0.044   0.024%     97.991%          Relu  mixed/tower_1/conv_2\n  209.214    0.056    0.041   0.022%     98.014%    CheckNumerics   mixed_10/tower_1/conv/CheckNumerics\n  204.111    0.031    0.041   0.022%     98.036%    CheckNumerics   mixed_9/tower/mixed/conv_1/CheckNumerics\n  210.764    0.071    0.041   0.022%     98.058%    CheckNumerics   mixed_10/tower/mixed/conv/CheckNumerics\n  204.077    0.039    0.041   0.022%     98.081%    CheckNumerics   mixed_9/tower/mixed/conv/CheckNumerics\n  210.758    0.072    0.040   0.022%     98.103%    CheckNumerics   mixed_10/tower/mixed/conv_1/CheckNumerics\n  122.169    0.053    0.040   0.022%     98.124%          Relu  mixed/conv\n  160.126    0.100    0.039   0.021%     98.146%          Relu  mixed_3/conv\n  167.571    0.050    0.038   0.021%     98.166%    CheckNumerics   mixed_4/tower_1/conv_3/CheckNumerics\n  209.261    0.052    0.038   0.021%     98.187%    CheckNumerics   mixed_10/tower/conv/CheckNumerics\n  136.394    0.045    0.038   0.021%     98.208%          Relu  mixed_1/conv\n  202.693    0.067    0.037   0.020%     98.228%    CheckNumerics   mixed_9/tower_1/conv/CheckNumerics\n  200.658    0.044    0.037   0.020%     98.248%    BatchNormWithGlobalNormalization    mixed_8/tower_1/conv_3/batchnorm\n  161.458    0.039    0.036   0.020%     98.268%    CheckNumerics   mixed_3/tower/conv_2/CheckNumerics\n  134.639    0.039    0.036   0.020%     98.287%          Relu  mixed/tower/conv_1\n  138.948    0.044    0.036   0.020%     98.307%          Relu  mixed_1/tower_2/conv\n  200.735    0.037    0.036   0.020%     98.326%        Concat  mixed_8/join\n  140.255    0.047    0.036   0.020%     98.346%          Relu  mixed_1/tower/conv_1\n  122.113    0.046    0.036   0.020%     98.366%          Relu  mixed/tower_1/conv\n  147.771    0.041    0.035   0.019%     98.385%          Relu  mixed_2/tower_2/conv\n  209.097    0.043    0.034   0.019%     98.403%    CheckNumerics   mixed_10/conv/CheckNumerics\n  147.075    0.046    0.034   0.019%     98.422%          Relu  mixed_2/conv\n  143.068    0.032    0.034   0.018%     98.440%          Relu  mixed_1/tower_1/conv_2\n  149.419    0.044    0.034   0.018%     98.459%          Relu  mixed_2/tower/conv_1\n  152.841    0.047    0.033   0.018%     98.477%          Relu  mixed_3/tower/conv\n  144.765    0.045    0.033   0.018%     98.495%          Relu  mixed_2/tower_1/conv\n  136.313    0.027    0.033   0.018%     98.513%          Relu  mixed_1/tower_1/conv\n  202.461    0.030    0.032   0.017%     98.530%    CheckNumerics   mixed_9/tower/conv/CheckNumerics\n  150.958    0.041    0.032   0.017%     98.547%          Relu  mixed_2/tower_1/conv_2\n  164.305    0.038    0.031   0.017%     98.564%          Relu  mixed_4/conv\n  206.465    0.031    0.031   0.017%     98.581%    CheckNumerics   mixed_9/tower_1/mixed/conv/CheckNumerics\n  202.588    0.032    0.031   0.017%     98.598%    CheckNumerics   mixed_9/conv/CheckNumerics\n  198.883    0.049    0.030   0.017%     98.615%    CheckNumerics   mixed_8/tower/conv_1/CheckNumerics\n  206.462    0.031    0.030   0.017%     98.631%    CheckNumerics   mixed_9/tower_1/mixed/conv_1/CheckNumerics\n  212.988    0.031    0.030   0.016%     98.647%    CheckNumerics   mixed_10/tower_1/mixed/conv_1/CheckNumerics\n  122.601    0.038    0.029   0.016%     98.663%          Relu  mixed/tower/conv\n  212.935    0.040    0.029   0.016%     98.679%    CheckNumerics   mixed_10/tower_1/mixed/conv/CheckNumerics\n  171.880    0.032    0.028   0.015%     98.694%          Relu  mixed_5/conv\n  211.939    0.031    0.027   0.015%     98.709%    CheckNumerics   mixed_10/tower_1/conv_1/CheckNumerics\n  213.810    0.033    0.027   0.015%     98.724%       Softmax  softmax\n  183.499    0.031    0.027   0.015%     98.739%          Relu  mixed_6/tower/conv_2\n  136.290    0.033    0.027   0.015%     98.753%          Relu  mixed_1/tower/conv\n  165.436    0.031    0.027   0.015%     98.768%          Relu  mixed_4/tower_2/conv\n  205.406    0.031    0.027   0.015%     98.783%    CheckNumerics   mixed_9/tower_1/conv_1/CheckNumerics\n  179.904    0.032    0.026   0.014%     98.797%          Relu  mixed_6/conv\n  172.177    0.028    0.026   0.014%     98.811%          Relu  mixed_5/tower_2/conv\n  167.142    0.030    0.026   0.014%     98.826%          Relu  mixed_4/tower/conv_2\n  197.151    0.029    0.026   0.014%     98.840%          Relu  mixed_8/tower/conv\n  188.822    0.030    0.026   0.014%     98.854%          Relu  mixed_7/tower_2/conv\n  188.137    0.034    0.026   0.014%     98.868%          Relu  mixed_7/conv\n  188.228    0.028    0.026   0.014%     98.883%          Relu  mixed_7/tower_1/conv\n  144.593    0.033    0.026   0.014%     98.897%          Relu  mixed_2/tower/conv\n  188.212    0.024    0.025   0.013%     98.910%          Relu  mixed_7/tower/conv\n  193.699    0.024    0.025   0.013%     98.924%          Relu  mixed_7/tower/conv_2\n  180.494    0.030    0.024   0.013%     98.937%          Relu  mixed_6/tower_2/conv\n  175.620    0.029    0.024   0.013%     98.950%          Relu  mixed_5/tower/conv_2\n  190.685    0.033    0.024   0.013%     98.963%          Relu  mixed_7/tower_1/conv_1\n  190.648    0.027    0.024   0.013%     98.976%          Relu  mixed_7/tower/conv_1\n  209.129    0.024    0.023   0.012%     98.988%    CheckNumerics   mixed_10/tower_2/conv/CheckNumerics\n  198.876    0.028    0.022   0.012%     99.000%          Relu  mixed_8/tower_1/conv_1\n  192.625    0.031    0.022   0.012%     99.012%          Relu  mixed_7/tower_1/conv_2\n  202.470    0.018    0.022   0.012%     99.024%    CheckNumerics   mixed_9/tower_2/conv/CheckNumerics\n  197.125    0.022    0.022   0.012%     99.036%          Relu  mixed_8/tower_1/conv\n  172.964    0.024    0.021   0.012%     99.048%          Relu  mixed_5/tower_1/conv_1\n  125.245    0.027    0.021   0.012%     99.059%          Relu  mixed/tower_2/conv\n  173.422    0.027    0.021   0.012%     99.071%          Relu  mixed_5/tower/conv_1\n  195.626    0.029    0.021   0.011%     99.082%          Relu  mixed_7/tower_1/conv_4\n  170.822    0.026    0.021   0.011%     99.094%          Relu  mixed_5/tower_1/conv\n  185.503    0.032    0.021   0.011%     99.105%          Relu  mixed_6/tower_1/conv_4\n  177.224    0.033    0.021   0.011%     99.116%          Relu  mixed_5/tower_1/conv_4\n  165.549    0.023    0.021   0.011%     99.128%          Relu  mixed_4/tower/conv_1\n  171.116    0.028    0.021   0.011%     99.139%          Relu  mixed_5/tower/conv\n  194.366    0.019    0.021   0.011%     99.150%          Relu  mixed_7/tower_1/conv_3\n  200.200    0.028    0.020   0.011%     99.161%          Relu  mixed_8/tower_1/conv_2\n  179.539    0.025    0.020   0.011%     99.173%          Relu  mixed_6/tower/conv\n  179.823    0.028    0.020   0.011%     99.184%          Relu  mixed_6/tower_1/conv\n  181.590    0.027    0.020   0.011%     99.195%          Relu  mixed_6/tower_1/conv_1\n  168.646    0.034    0.020   0.011%     99.206%          Relu  mixed_4/tower_1/conv_4\n  164.024    0.023    0.020   0.011%     99.217%          Relu  mixed_4/tower/conv\n  183.376    0.021    0.019   0.011%     99.227%          Relu  mixed_6/tower_1/conv_2\n  165.447    0.016    0.019   0.010%     99.238%          Relu  mixed_4/tower_1/conv_1\n  181.511    0.021    0.019   0.010%     99.248%          Relu  mixed_6/tower/conv_1\n  174.550    0.026    0.018   0.010%     99.258%          Relu  mixed_5/tower_1/conv_2\n  163.560    0.022    0.018   0.010%     99.268%          Relu  mixed_4/tower_1/conv\n  166.840    0.019    0.018   0.010%     99.278%          Relu  mixed_4/tower_1/conv_2\n  176.047    0.019    0.018   0.010%     99.288%          Relu  mixed_5/tower_1/conv_3\n    0.406    0.772    0.017   0.009%     99.297%         Const  mixed_1/tower_1/conv/batchnorm/moving_mean\n  184.375    0.020    0.017   0.009%     99.307%          Relu  mixed_6/tower_1/conv_3\n  202.781    0.028    0.016   0.009%     99.315%          Relu  mixed_9/tower_1/conv\n  167.625    0.020    0.016   0.009%     99.324%          Relu  mixed_4/tower_1/conv_3\n  161.502    0.015    0.015   0.008%     99.332%          Relu  mixed_3/tower/conv_2\n  204.122    0.016    0.015   0.008%     99.341%          Relu  mixed_9/tower/mixed/conv\n  209.276    0.015    0.015   0.008%     99.349%          Relu  mixed_10/tower_1/conv\n    1.667    0.636    0.015   0.008%     99.357%         Const  mixed_2/tower_2/conv/conv2d_params\n  210.842    0.016    0.015   0.008%     99.365%          Relu  mixed_10/tower/mixed/conv\n  204.148    0.015    0.014   0.008%     99.373%          Relu  mixed_9/tower/mixed/conv_1\n  200.705    0.016    0.014   0.008%     99.381%    CheckNumerics   mixed_8/tower_1/conv_3/CheckNumerics\n  202.625    0.012    0.014   0.008%     99.388%          Relu  mixed_9/conv\n  202.497    0.014    0.013   0.007%     99.396%          Relu  mixed_9/tower/conv\n  209.320    0.018    0.013   0.007%     99.403%          Relu  mixed_10/tower/conv\n  210.838    0.015    0.013   0.007%     99.410%          Relu  mixed_10/tower/mixed/conv_1\n  198.940    0.023    0.013   0.007%     99.417%          Relu  mixed_8/tower/conv_1\n  209.146    0.010    0.013   0.007%     99.424%          Relu  mixed_10/conv\n    6.345    0.011    0.012   0.007%     99.431%         Const  mixed_5/tower_1/conv_2/batchnorm/beta\n  211.975    0.011    0.011   0.006%     99.437%          Relu  mixed_10/tower_1/conv_1\n  205.442    0.013    0.010   0.006%     99.442%          Relu  mixed_9/tower_1/conv_1\n  206.497    0.010    0.010   0.006%     99.448%          Relu  mixed_9/tower_1/mixed/conv_1\n  212.979    0.010    0.010   0.006%     99.454%          Relu  mixed_10/tower_1/mixed/conv\n  206.501    0.013    0.010   0.005%     99.459%          Relu  mixed_9/tower_1/mixed/conv\n  213.024    0.012    0.010   0.005%     99.464%          Relu  mixed_10/tower_1/mixed/conv_1\n  209.161    0.011    0.009   0.005%     99.469%          Relu  mixed_10/tower_2/conv\n    0.175    0.010    0.009   0.005%     99.474%         Const  conv_4/batchnorm/gamma\n  202.494    0.007    0.008   0.004%     99.478%          Relu  mixed_9/tower_2/conv\n  213.856    0.009    0.008   0.004%     99.483%                _SINK\n    2.334    0.288    0.008   0.004%     99.487%         Const  mixed_3/conv/conv2d_params\n  200.726    0.008    0.007   0.004%     99.491%          Relu  mixed_8/tower_1/conv_3\n  213.797    0.012    0.005   0.003%     99.494%       BiasAdd  softmax/logits\n    0.336    0.014    0.005   0.003%     99.496%         Const  mixed/join/concat_dim\n    2.890    0.009    0.005   0.003%     99.499%         Const  mixed_3/conv/batchnorm/gamma\n    0.136    0.004    0.005   0.002%     99.501%         Const  conv_2/batchnorm/gamma\n    6.172    0.008    0.005   0.002%     99.504%         Const  mixed_5/tower/conv/batchnorm/gamma\n    0.298    0.003    0.003   0.002%     99.505%         Const  mixed/tower_1/conv_1/batchnorm/gamma\n    1.207    0.064    0.003   0.002%     99.507%         Const  mixed_1/tower_1/conv_1/batchnorm/moving_mean\n    1.447    0.057    0.003   0.002%     99.509%         Const  mixed_2/conv/batchnorm/beta\n    1.374    0.062    0.003   0.002%     99.510%         Const  mixed_1/tower_2/conv/batchnorm/beta\n    5.799    0.004    0.003   0.002%     99.512%         Const  mixed_4/tower/conv/batchnorm/gamma\n    0.401    0.003    0.003   0.002%     99.514%         Const  mixed_1/tower_1/conv/batchnorm/beta\n    1.522    0.042    0.003   0.001%     99.515%         Const  mixed_2/tower/conv/batchnorm/beta\n  152.028    0.003    0.003   0.001%     99.516%      Identity  mixed_3/pool/control_dependency\n    6.508    0.012    0.003   0.001%     99.518%         Const  mixed_6/conv/conv2d_params\n  213.142    0.003    0.003   0.001%     99.519%       Reshape  pool_3/_reshape\n  180.488    0.004    0.003   0.001%     99.521%      Identity  mixed_6/tower_2/conv/control_dependency\n  206.839    0.002    0.002   0.001%     99.522%      Identity  mixed_10/tower_2/pool/control_dependency\n    6.381    0.035    0.002   0.001%     99.523%         Const  mixed_5/tower_1/conv_3/batchnorm/beta\n    6.157    0.005    0.002   0.001%     99.525%         Const  mixed_5/tower/conv/conv2d_params\n  179.817    0.003    0.002   0.001%     99.526%      Identity  mixed_6/tower_1/conv/control_dependency\n    5.847    0.032    0.002   0.001%     99.527%         Const  mixed_4/tower/conv_2/batchnorm/beta\n    0.088    0.006    0.002   0.001%     99.529%         Const  conv/conv2d_params\n  209.156    0.003    0.002   0.001%     99.530%      Identity  mixed_10/tower_2/conv/control_dependency\n  196.204    0.003    0.002   0.001%     99.531%      Identity  mixed_8/pool/control_dependency\n  140.248    0.004    0.002   0.001%     99.533%      Identity  mixed_1/tower/conv_1/control_dependency\n    6.834    0.005    0.002   0.001%     99.534%         Const  mixed_6/tower_2/conv/conv2d_params\n  204.143    0.002    0.002   0.001%     99.535%      Identity  mixed_9/tower/mixed/conv_1/control_dependency\n  171.109    0.003    0.002   0.001%     99.536%      Identity  mixed_5/tower/conv/control_dependency\n  152.835    0.003    0.002   0.001%     99.538%      Identity  mixed_3/tower/conv/control_dependency\n    0.219    0.004    0.002   0.001%     99.539%         Const  mixed/tower/conv/batchnorm/gamma\n    7.010    0.006    0.002   0.001%     99.540%         Const  mixed_7/tower_1/conv/conv2d_params\n    6.234    0.005    0.002   0.001%     99.542%         Const  mixed_5/tower/conv_2/conv2d_params\n  179.534    0.003    0.002   0.001%     99.543%      Identity  mixed_6/tower/conv/control_dependency\n  172.171    0.004    0.002   0.001%     99.544%      Identity  mixed_5/tower_2/conv/control_dependency\n  140.786    0.001    0.002   0.001%     99.545%      Identity  mixed_1/tower_1/conv_1/control_dependency\n    6.941    0.005    0.002   0.001%     99.547%         Const  mixed_7/tower/conv_1/conv2d_params\n    6.612    0.009    0.002   0.001%     99.548%         Const  mixed_6/tower/conv_2/conv2d_params\n  171.876    0.002    0.002   0.001%     99.549%      Identity  mixed_5/conv/control_dependency\n  164.299    0.004    0.002   0.001%     99.551%      Identity  mixed_4/conv/control_dependency\n    7.420    0.003    0.002   0.001%     99.552%         Const  mixed_9/tower/mixed/conv_1/conv2d_params\n  188.816    0.004    0.002   0.001%     99.553%      Identity  mixed_7/tower_2/conv/control_dependency\n  188.132    0.003    0.002   0.001%     99.554%      Identity  mixed_7/conv/control_dependency\n  160.867    0.002    0.002   0.001%     99.556%      Identity  mixed_3/tower/conv_1/control_dependency\n  147.764    0.004    0.002   0.001%     99.557%      Identity  mixed_2/tower_2/conv/control_dependency\n  138.941    0.004    0.002   0.001%     99.558%      Identity  mixed_1/tower_2/conv/control_dependency\n  136.387    0.004    0.002   0.001%     99.559%      Identity  mixed_1/conv/control_dependency\n  134.635    0.003    0.002   0.001%     99.561%      Identity  mixed/tower/conv_1/control_dependency\n  122.164    0.002    0.002   0.001%     99.562%      Identity  mixed/conv/control_dependency\n    6.578    0.009    0.002   0.001%     99.563%         Const  mixed_6/tower/conv_1/conv2d_params\n  188.225    0.002    0.002   0.001%     99.564%      Identity  mixed_7/tower_1/conv/control_dependency\n  170.816    0.004    0.002   0.001%     99.566%      Identity  mixed_5/tower_1/conv/control_dependency\n  163.553    0.004    0.002   0.001%     99.567%      Identity  mixed_4/tower_1/conv/control_dependency\n  149.413    0.003    0.002   0.001%     99.568%      Identity  mixed_2/tower/conv_1/control_dependency\n  125.239    0.003    0.002   0.001%     99.569%      Identity  mixed/tower_2/conv/control_dependency\n    7.695    0.003    0.002   0.001%     99.571%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/beta\n    6.123    0.006    0.002   0.001%     99.572%         Const  mixed_5/conv/conv2d_params\n    6.048    0.013    0.002   0.001%     99.573%         Const  mixed_4/tower_1/conv_4/conv2d_params\n    5.839    0.005    0.002   0.001%     99.574%         Const  mixed_4/tower/conv_2/conv2d_params\n  210.837    0.003    0.002   0.001%     99.576%      Identity  mixed_10/tower/mixed/conv/control_dependency\n  122.108    0.003    0.002   0.001%     99.577%      Identity  mixed/tower_1/conv/control_dependency\n    7.216    0.010    0.002   0.001%     99.578%         Const  mixed_8/tower/conv/conv2d_params\n    6.302    0.005    0.002   0.001%     99.579%         Const  mixed_5/tower_1/conv_1/conv2d_params\n    5.764    0.004    0.002   0.001%     99.581%         Const  mixed_4/conv/conv2d_params\n    6.977    0.005    0.002   0.001%     99.582%         Const  mixed_7/tower/conv_2/conv2d_params\n    6.442    0.016    0.002   0.001%     99.583%         Const  mixed_5/tower_1/conv_4/batchnorm/beta\n    6.084    0.011    0.002   0.001%     99.584%         Const  mixed_4/tower_2/conv/conv2d_params\n    5.896    0.006    0.002   0.001%     99.585%         Const  mixed_4/tower_1/conv/conv2d_params\n  165.432    0.002    0.002   0.001%     99.587%      Identity  mixed_4/tower_2/conv/control_dependency\n    7.077    0.005    0.002   0.001%     99.588%         Const  mixed_7/tower_1/conv_2/conv2d_params\n    6.800    0.005    0.002   0.001%     99.589%         Const  mixed_6/tower_1/conv_4/conv2d_params\n    6.685    0.005    0.002   0.001%     99.590%         Const  mixed_6/tower_1/conv_1/conv2d_params\n    6.544    0.009    0.002   0.001%     99.591%         Const  mixed_6/tower/conv/conv2d_params\n  210.833    0.003    0.002   0.001%     99.593%      Identity  mixed_10/tower/mixed/conv_1/control_dependency\n  164.018    0.003    0.002   0.001%     99.594%      Identity  mixed_4/tower/conv/control_dependency\n  122.594    0.003    0.002   0.001%     99.595%      Identity  mixed/tower/conv/control_dependency\n    7.149    0.009    0.002   0.001%     99.596%         Const  mixed_7/tower_1/conv_4/conv2d_params\n    1.621    0.006    0.002   0.001%     99.597%         Const  mixed_2/tower_1/conv_1/conv2d_params\n    0.260    0.002    0.002   0.001%     99.599%         Const  mixed/tower/conv_1/conv2d_params\n  190.643    0.002    0.002   0.001%     99.600%      Identity  mixed_7/tower/conv_1/control_dependency\n  188.208    0.002    0.002   0.001%     99.601%      Identity  mixed_7/tower/conv/control_dependency\n    6.900    0.006    0.002   0.001%     99.602%         Const  mixed_7/tower/conv/conv2d_params\n    6.338    0.005    0.002   0.001%     99.603%         Const  mixed_5/tower_1/conv_2/conv2d_params\n    6.268    0.005    0.002   0.001%     99.605%         Const  mixed_5/tower_1/conv/conv2d_params\n    6.201    0.005    0.002   0.001%     99.606%         Const  mixed_5/tower/conv_1/conv2d_params\n    5.790    0.003    0.002   0.001%     99.607%         Const  mixed_4/tower/conv/conv2d_params\n  204.118    0.002    0.002   0.001%     99.608%      Identity  mixed_9/tower/mixed/conv/control_dependency\n  179.898    0.003    0.002   0.001%     99.609%      Identity  mixed_6/conv/control_dependency\n  148.339    0.003    0.002   0.001%     99.611%      Identity  mixed_2/tower_1/conv_1/control_dependency\n    7.672    0.006    0.002   0.001%     99.612%         Const  mixed_10/tower_1/mixed/conv/batchnorm/beta\n    7.400    0.003    0.002   0.001%     99.613%         Const  mixed_9/tower/mixed/conv/conv2d_params\n    6.718    0.005    0.002   0.001%     99.614%         Const  mixed_6/tower_1/conv_2/conv2d_params\n    6.164    0.005    0.002   0.001%     99.615%         Const  mixed_5/tower/conv/batchnorm/beta\n    6.013    0.011    0.002   0.001%     99.616%         Const  mixed_4/tower_1/conv_3/conv2d_params\n  202.763    0.015    0.002   0.001%     99.618%      Identity  mixed_9/tower_1/conv/control_dependency\n  202.622    0.001    0.002   0.001%     99.619%      Identity  mixed_9/conv/control_dependency\n  202.490    0.001    0.002   0.001%     99.620%      Identity  mixed_9/tower_2/conv/control_dependency\n  136.310    0.002    0.002   0.001%     99.621%      Identity  mixed_1/tower_1/conv/control_dependency\n    7.043    0.005    0.002   0.001%     99.622%         Const  mixed_7/tower_1/conv_1/conv2d_params\n    6.867    0.005    0.002   0.001%     99.624%         Const  mixed_7/conv/conv2d_params\n    6.434    0.005    0.002   0.001%     99.625%         Const  mixed_5/tower_1/conv_4/conv2d_params\n    5.817    0.003    0.002   0.001%     99.626%         Const  mixed_4/tower/conv_1/conv2d_params\n    1.283    0.004    0.002   0.001%     99.627%         Const  mixed_1/tower_1/conv_2/conv2d_params\n    0.384    0.002    0.002   0.001%     99.628%         Const  mixed_1/tower/conv_1/batchnorm/beta\n    0.289    0.003    0.002   0.001%     99.629%         Const  mixed/tower_1/conv_1/conv2d_params\n  209.272    0.002    0.002   0.001%     99.631%      Identity  mixed_10/tower_1/conv/control_dependency\n  165.543    0.004    0.002   0.001%     99.632%      Identity  mixed_4/tower/conv_1/control_dependency\n    6.752    0.005    0.002   0.001%     99.633%         Const  mixed_6/tower_1/conv_3/conv2d_params\n    1.516    0.004    0.002   0.001%     99.634%         Const  mixed_2/tower/conv/conv2d_params\n    1.444    0.002    0.002   0.001%     99.635%         Const  mixed_2/conv/conv2d_params\n  144.586    0.003    0.002   0.001%     99.636%      Identity  mixed_2/tower/conv/control_dependency\n    7.361    0.003    0.002   0.001%     99.637%         Const  mixed_9/conv/conv2d_params\n    6.373    0.005    0.002   0.001%     99.639%         Const  mixed_5/tower_1/conv_3/conv2d_params\n    5.742    0.003    0.002   0.001%     99.640%         Const  mixed_3/tower/conv_2/conv2d_params\n    1.599    0.004    0.002   0.001%     99.641%         Const  mixed_2/tower_1/conv/conv2d_params\n    0.308    0.002    0.002   0.001%     99.642%         Const  mixed/tower_1/conv_2/conv2d_params\n  209.141    0.002    0.002   0.001%     99.643%      Identity  mixed_10/conv/control_dependency\n  147.069    0.003    0.002   0.001%     99.644%      Identity  mixed_2/conv/control_dependency\n    7.528    0.003    0.002   0.001%     99.645%         Const  mixed_9/tower_2/conv/conv2d_params\n    6.475    0.005    0.002   0.001%     99.647%         Const  mixed_5/tower_2/conv/conv2d_params\n    5.981    0.006    0.002   0.001%     99.648%         Const  mixed_4/tower_1/conv_2/conv2d_params\n    5.931    0.005    0.002   0.001%     99.649%         Const  mixed_4/tower_1/conv_1/conv2d_params\n    5.698    0.004    0.002   0.001%     99.650%         Const  mixed_3/tower/conv/conv2d_params\n    1.577    0.003    0.002   0.001%     99.651%         Const  mixed_2/tower/conv_1/conv2d_params\n    0.396    0.004    0.002   0.001%     99.652%         Const  mixed_1/tower_1/conv/conv2d_params\n    0.351    0.002    0.002   0.001%     99.653%         Const  mixed_1/conv/conv2d_params\n  209.315    0.003    0.002   0.001%     99.655%      Identity  mixed_10/tower/conv/control_dependency\n  173.416    0.003    0.002   0.001%     99.656%      Identity  mixed_5/tower/conv_1/control_dependency\n  144.759    0.003    0.002   0.001%     99.657%      Identity  mixed_2/tower_1/conv/control_dependency\n    7.260    0.004    0.002   0.001%     99.658%         Const  mixed_8/tower/conv_1/batchnorm/gamma\n    7.116    0.005    0.002   0.001%     99.659%         Const  mixed_7/tower_1/conv_3/conv2d_params\n    6.985    0.009    0.002   0.001%     99.660%         Const  mixed_7/tower/conv_2/batchnorm/beta\n    6.651    0.005    0.002   0.001%     99.661%         Const  mixed_6/tower_1/conv/conv2d_params\n    5.992    0.005    0.002   0.001%     99.662%         Const  mixed_4/tower_1/conv_2/batchnorm/beta\n    1.193    0.005    0.002   0.001%     99.664%         Const  mixed_1/tower_1/conv_1/conv2d_params\n    0.274    0.002    0.002   0.001%     99.665%         Const  mixed/tower_1/conv/conv2d_params\n  183.493    0.004    0.002   0.001%     99.666%      Identity  mixed_6/tower/conv_2/control_dependency\n  128.343    0.003    0.002   0.001%     99.667%      Identity  mixed/tower_1/conv_1/control_dependency\n    6.875    0.005    0.002   0.001%     99.668%         Const  mixed_7/conv/batchnorm/beta\n    1.644    0.006    0.002   0.001%     99.669%         Const  mixed_2/tower_1/conv_2/conv2d_params\n    1.371    0.002    0.002   0.001%     99.670%         Const  mixed_1/tower_2/conv/conv2d_params\n  167.136    0.003    0.002   0.001%     99.671%      Identity  mixed_4/tower/conv_2/control_dependency\n    7.708    0.005    0.002   0.001%     99.672%         Const  mixed_10/tower_2/conv/conv2d_params\n    7.571    0.003    0.002   0.001%     99.674%         Const  mixed_10/tower/conv/batchnorm/beta\n    6.816    0.005    0.002   0.001%     99.675%         Const  mixed_6/tower_1/conv_4/batchnorm/moving_mean\n    6.487    0.005    0.002   0.001%     99.676%         Const  mixed_5/tower_2/conv/batchnorm/beta\n    6.209    0.005    0.002   0.001%     99.677%         Const  mixed_5/tower/conv_1/batchnorm/beta\n    5.722    0.003    0.002   0.001%     99.678%         Const  mixed_3/tower/conv_1/conv2d_params\n  136.285    0.003    0.002   0.001%     99.679%      Identity  mixed_1/tower/conv/control_dependency\n    6.842    0.004    0.002   0.001%     99.680%         Const  mixed_6/tower_2/conv/batchnorm/beta\n    6.242    0.005    0.002   0.001%     99.681%         Const  mixed_5/tower/conv_2/batchnorm/beta\n    6.132    0.004    0.002   0.001%     99.682%         Const  mixed_5/conv/batchnorm/beta\n    5.904    0.010    0.002   0.001%     99.683%         Const  mixed_4/tower_1/conv/batchnorm/beta\n    0.102    0.002    0.002   0.001%     99.685%         Const  conv/batchnorm/gamma\n  175.615    0.003    0.002   0.001%     99.686%      Identity  mixed_5/tower/conv_2/control_dependency\n    7.547    0.004    0.002   0.001%     99.687%         Const  mixed_10/conv/conv2d_params\n    7.508    0.003    0.002   0.001%     99.688%         Const  mixed_9/tower_1/mixed/conv_1/conv2d_params\n    7.468    0.003    0.002   0.001%     99.689%         Const  mixed_9/tower_1/conv_1/conv2d_params\n    7.296    0.004    0.002   0.001%     99.690%         Const  mixed_8/tower_1/conv_1/conv2d_params\n    7.018    0.009    0.002   0.001%     99.691%         Const  mixed_7/tower_1/conv/batchnorm/beta\n    6.808    0.005    0.002   0.001%     99.692%         Const  mixed_6/tower_1/conv_4/batchnorm/beta\n    6.733    0.004    0.002   0.001%     99.693%         Const  mixed_6/tower_1/conv_2/batchnorm/moving_mean\n    6.692    0.005    0.002   0.001%     99.694%         Const  mixed_6/tower_1/conv_1/batchnorm/beta\n    6.276    0.004    0.002   0.001%     99.695%         Const  mixed_5/tower_1/conv/batchnorm/beta\n    6.183    0.009    0.002   0.001%     99.696%         Const  mixed_5/tower/conv/batchnorm/moving_mean\n    0.322    0.004    0.002   0.001%     99.698%         Const  mixed/tower_2/conv/conv2d_params\n  198.934    0.003    0.002   0.001%     99.699%      Identity  mixed_8/tower/conv_1/control_dependency\n    6.760    0.004    0.002   0.001%     99.700%         Const  mixed_6/tower_1/conv_3/batchnorm/beta\n    1.629    0.003    0.002   0.001%     99.701%         Const  mixed_2/tower_1/conv_1/batchnorm/beta\n    1.582    0.006    0.002   0.001%     99.702%         Const  mixed_2/tower/conv_1/batchnorm/beta\n    0.257    0.002    0.002   0.001%     99.703%         Const  mixed/tower/conv/batchnorm/moving_variance\n  197.147    0.002    0.002   0.001%     99.704%      Identity  mixed_8/tower/conv/control_dependency\n    7.608    0.003    0.002   0.001%     99.705%         Const  mixed_10/tower/mixed/conv_1/conv2d_params\n    7.587    0.003    0.002   0.001%     99.706%         Const  mixed_10/tower/mixed/conv/conv2d_params\n    7.488    0.003    0.002   0.001%     99.707%         Const  mixed_9/tower_1/mixed/conv/conv2d_params\n    7.127    0.005    0.002   0.001%     99.708%         Const  mixed_7/tower_1/conv_3/batchnorm/beta\n    6.310    0.011    0.002   0.001%     99.709%         Const  mixed_5/tower_1/conv_1/batchnorm/beta\n    5.924    0.005    0.002   0.001%     99.710%         Const  mixed_4/tower_1/conv/batchnorm/moving_variance\n    0.379    0.003    0.002   0.001%     99.711%         Const  mixed_1/tower/conv_1/conv2d_params\n    0.293    0.002    0.002   0.001%     99.712%         Const  mixed/tower_1/conv_1/batchnorm/beta\n    7.381    0.003    0.002   0.001%     99.713%         Const  mixed_9/tower/conv/conv2d_params\n    7.251    0.003    0.002   0.001%     99.715%         Const  mixed_8/tower/conv_1/conv2d_params\n    7.182    0.010    0.002   0.001%     99.716%         Const  mixed_7/tower_2/conv/conv2d_params\n    7.051    0.009    0.002   0.001%     99.717%         Const  mixed_7/tower_1/conv_1/batchnorm/beta\n    6.915    0.004    0.002   0.001%     99.718%         Const  mixed_7/tower/conv/batchnorm/beta\n    6.556    0.005    0.002   0.001%     99.719%         Const  mixed_6/tower/conv/batchnorm/beta\n    6.139    0.004    0.002   0.001%     99.720%         Const  mixed_5/conv/batchnorm/moving_mean\n    6.063    0.005    0.002   0.001%     99.721%         Const  mixed_4/tower_1/conv_4/batchnorm/beta\n    5.794    0.003    0.002   0.001%     99.722%         Const  mixed_4/tower/conv/batchnorm/beta\n    2.880    0.007    0.002   0.001%     99.723%         Const  mixed_3/conv/batchnorm/beta\n    0.365    0.002    0.002   0.001%     99.724%         Const  mixed_1/tower/conv/conv2d_params\n  161.499    0.002    0.002   0.001%     99.725%      Identity  mixed_3/tower/conv_2/control_dependency\n    7.668    0.003    0.002   0.001%     99.726%         Const  mixed_10/tower_1/mixed/conv/conv2d_params\n    7.649    0.003    0.002   0.001%     99.727%         Const  mixed_10/tower_1/conv_1/conv2d_params\n    7.339    0.003    0.002   0.001%     99.728%         Const  mixed_8/tower_1/conv_3/conv2d_params\n    7.317    0.003    0.002   0.001%     99.729%         Const  mixed_8/tower_1/conv_2/conv2d_params\n    6.249    0.009    0.002   0.001%     99.730%         Const  mixed_5/tower/conv_2/batchnorm/moving_mean\n    6.097    0.005    0.002   0.001%     99.731%         Const  mixed_4/tower_2/conv/batchnorm/beta\n    5.747    0.003    0.002   0.001%     99.732%         Const  mixed_3/tower/conv_2/batchnorm/beta\n    0.271    0.002    0.002   0.001%     99.733%         Const  mixed/tower/conv_1/batchnorm/moving_variance\n  193.696    0.002    0.002   0.001%     99.734%      Identity  mixed_7/tower/conv_2/control_dependency\n  181.585    0.003    0.002   0.001%     99.735%      Identity  mixed_6/tower_1/conv_1/control_dependency\n   81.494    0.002    0.002   0.001%     99.736%      Identity  conv_2/control_dependency\n    6.882    0.008    0.002   0.001%     99.737%         Const  mixed_7/conv/batchnorm/moving_mean\n    6.726    0.004    0.002   0.001%     99.738%         Const  mixed_6/tower_1/conv_2/batchnorm/beta\n    6.034    0.005    0.002   0.001%     99.739%         Const  mixed_4/tower_1/conv_3/batchnorm/moving_mean\n    6.000    0.004    0.002   0.001%     99.741%         Const  mixed_4/tower_1/conv_2/batchnorm/moving_mean\n    5.821    0.006    0.002   0.001%     99.742%         Const  mixed_4/tower/conv_1/batchnorm/beta\n    5.770    0.003    0.002   0.001%     99.743%         Const  mixed_4/conv/batchnorm/beta\n    5.703    0.003    0.002   0.001%     99.744%         Const  mixed_3/tower/conv/batchnorm/beta\n    2.311    0.006    0.002   0.001%     99.745%         Const  mixed_2/tower_2/conv/batchnorm/beta\n    0.281    0.004    0.002   0.001%     99.746%         Const  mixed/tower_1/conv/batchnorm/moving_mean\n    7.688    0.003    0.002   0.001%     99.747%         Const  mixed_10/tower_1/mixed/conv_1/conv2d_params\n    7.567    0.003    0.002   0.001%     99.748%         Const  mixed_10/tower/conv/conv2d_params\n    7.102    0.004    0.002   0.001%     99.749%         Const  mixed_7/tower_1/conv_2/batchnorm/moving_mean\n    7.095    0.004    0.002   0.001%     99.750%         Const  mixed_7/tower_1/conv_2/batchnorm/beta\n    6.963    0.004    0.002   0.001%     99.751%         Const  mixed_7/tower/conv_1/batchnorm/moving_mean\n    6.923    0.008    0.002   0.001%     99.752%         Const  mixed_7/tower/conv/batchnorm/moving_mean\n    6.827    0.004    0.002   0.001%     99.753%         Const  mixed_6/tower_1/conv_4/batchnorm/moving_variance\n    5.726    0.003    0.002   0.001%     99.754%         Const  mixed_3/tower/conv_1/batchnorm/beta\n    1.607    0.003    0.002   0.001%     99.755%         Const  mixed_2/tower_1/conv/batchnorm/beta\n    1.590    0.003    0.002   0.001%     99.756%         Const  mixed_2/tower/conv_1/batchnorm/moving_mean\n    1.200    0.004    0.002   0.001%     99.757%         Const  mixed_1/tower_1/conv_1/batchnorm/beta\n    1.184    0.006    0.002   0.001%     99.758%         Const  mixed_1/tower_1/conv/batchnorm/moving_variance\n    0.305    0.002    0.002   0.001%     99.759%         Const  mixed/tower_1/conv_1/batchnorm/moving_variance\n    0.286    0.002    0.002   0.001%     99.760%         Const  mixed/tower_1/conv/batchnorm/moving_variance\n  172.958    0.003    0.002   0.001%     99.761%      Identity  mixed_5/tower_1/conv_1/control_dependency\n    7.653    0.006    0.002   0.001%     99.762%         Const  mixed_10/tower_1/conv_1/batchnorm/beta\n    7.629    0.003    0.002   0.001%     99.763%         Const  mixed_10/tower_1/conv/conv2d_params\n    7.029    0.005    0.002   0.001%     99.764%         Const  mixed_7/tower_1/conv/batchnorm/moving_mean\n    6.949    0.004    0.002   0.001%     99.765%         Const  mixed_7/tower/conv_1/batchnorm/beta\n    6.893    0.005    0.002   0.001%     99.766%         Const  mixed_7/conv/batchnorm/moving_variance\n    6.707    0.008    0.002   0.001%     99.767%         Const  mixed_6/tower_1/conv_1/batchnorm/moving_variance\n    6.674    0.008    0.002   0.001%     99.768%         Const  mixed_6/tower_1/conv/batchnorm/moving_variance\n    6.666    0.005    0.002   0.001%     99.769%         Const  mixed_6/tower_1/conv/batchnorm/moving_mean\n    6.419    0.005    0.002   0.001%     99.770%         Const  mixed_5/tower_1/conv_3/batchnorm/moving_mean\n    6.027    0.005    0.002   0.001%     99.771%         Const  mixed_4/tower_1/conv_3/batchnorm/beta\n    5.959    0.004    0.002   0.001%     99.772%         Const  mixed_4/tower_1/conv_1/batchnorm/beta\n    5.752    0.005    0.002   0.001%     99.773%         Const  mixed_3/tower/conv_2/batchnorm/moving_mean\n    5.708    0.008    0.002   0.001%     99.774%         Const  mixed_3/tower/conv/batchnorm/moving_mean\n    1.567    0.003    0.002   0.001%     99.775%         Const  mixed_2/tower/conv/batchnorm/moving_mean\n    1.437    0.002    0.002   0.001%     99.776%         Const  mixed_1/tower_2/conv/batchnorm/moving_mean\n    1.289    0.004    0.002   0.001%     99.777%         Const  mixed_1/tower_1/conv_2/batchnorm/beta\n    7.452    0.003    0.002   0.001%     99.778%         Const  mixed_9/tower_1/conv/batchnorm/gamma\n    7.281    0.003    0.002   0.001%     99.779%         Const  mixed_8/tower_1/conv/batchnorm/beta\n    6.624    0.005    0.002   0.001%     99.780%         Const  mixed_6/tower/conv_2/batchnorm/beta\n    6.590    0.005    0.002   0.001%     99.781%         Const  mixed_6/tower/conv_1/batchnorm/beta\n    6.501    0.005    0.002   0.001%     99.782%         Const  mixed_5/tower_2/conv/batchnorm/moving_variance\n    6.461    0.004    0.002   0.001%     99.783%         Const  mixed_5/tower_1/conv_4/batchnorm/moving_mean\n    6.324    0.004    0.002   0.001%     99.784%         Const  mixed_5/tower_1/conv_1/batchnorm/moving_mean\n    6.216    0.008    0.002   0.001%     99.785%         Const  mixed_5/tower/conv_1/batchnorm/moving_mean\n    6.194    0.005    0.002   0.001%     99.786%         Const  mixed_5/tower/conv/batchnorm/moving_variance\n    6.007    0.004    0.002   0.001%     99.787%         Const  mixed_4/tower_1/conv_2/batchnorm/moving_variance\n    5.966    0.005    0.002   0.001%     99.788%         Const  mixed_4/tower_1/conv_1/batchnorm/moving_mean\n    5.730    0.006    0.002   0.001%     99.789%         Const  mixed_3/tower/conv_1/batchnorm/moving_mean\n    1.639    0.003    0.002   0.001%     99.790%         Const  mixed_2/tower_1/conv_1/batchnorm/moving_variance\n    0.369    0.001    0.002   0.001%     99.791%         Const  mixed_1/tower/conv/batchnorm/beta\n    0.263    0.002    0.002   0.001%     99.792%         Const  mixed/tower/conv_1/batchnorm/beta\n    0.212    0.002    0.002   0.001%     99.793%         Const  mixed/tower/conv/conv2d_params\n    0.205    0.002    0.002   0.001%     99.794%         Const  mixed/conv/batchnorm/moving_mean\n  202.494    0.002    0.002   0.001%     99.795%      Identity  mixed_9/tower/conv/control_dependency\n  181.508    0.002    0.002   0.001%     99.796%      Identity  mixed_6/tower/conv_1/control_dependency\n   38.274    0.004    0.002   0.001%     99.797%      Identity  conv_1/control_dependency\n    7.715    0.002    0.002   0.001%     99.798%         Const  mixed_10/tower_2/conv/batchnorm/beta\n    7.633    0.006    0.002   0.001%     99.799%         Const  mixed_10/tower_1/conv/batchnorm/beta\n    7.385    0.006    0.002   0.001%     99.800%         Const  mixed_9/tower/conv/batchnorm/beta\n    7.194    0.005    0.002   0.001%     99.801%         Const  mixed_7/tower_2/conv/batchnorm/beta\n    7.161    0.005    0.002   0.001%     99.802%         Const  mixed_7/tower_1/conv_4/batchnorm/beta\n    6.849    0.008    0.002   0.001%     99.803%         Const  mixed_6/tower_2/conv/batchnorm/moving_mean\n    6.740    0.009    0.002   0.001%     99.804%         Const  mixed_6/tower_1/conv_2/batchnorm/moving_variance\n    6.537    0.005    0.002   0.001%     99.805%         Const  mixed_6/conv/batchnorm/moving_variance\n    6.494    0.005    0.002   0.001%     99.806%         Const  mixed_5/tower_2/conv/batchnorm/moving_mean\n    6.426    0.005    0.002   0.001%     99.807%         Const  mixed_5/tower_1/conv_3/batchnorm/moving_variance\n    6.359    0.005    0.002   0.001%     99.808%         Const  mixed_5/tower_1/conv_2/batchnorm/moving_mean\n    6.288    0.004    0.002   0.001%     99.809%         Const  mixed_5/tower_1/conv/batchnorm/moving_mean\n    6.105    0.004    0.002   0.001%     99.810%         Const  mixed_4/tower_2/conv/batchnorm/moving_mean\n    6.077    0.005    0.002   0.001%     99.811%         Const  mixed_4/tower_1/conv_4/batchnorm/moving_variance\n    5.882    0.005    0.002   0.001%     99.812%         Const  mixed_4/tower/conv_2/batchnorm/moving_mean\n    2.902    0.004    0.002   0.001%     99.813%         Const  mixed_3/conv/batchnorm/moving_mean\n    1.364    0.002    0.002   0.001%     99.814%         Const  mixed_1/tower_1/conv_2/batchnorm/moving_mean\n    0.357    0.003    0.002   0.001%     99.815%         Const  mixed_1/conv/batchnorm/moving_mean\n    0.327    0.002    0.002   0.001%     99.816%         Const  mixed/tower_2/conv/batchnorm/beta\n  190.682    0.001    0.002   0.001%     99.817%      Identity  mixed_7/tower_1/conv_1/control_dependency\n    7.734    0.003    0.002   0.001%     99.818%         Const  softmax/weights\n    7.727    0.004    0.002   0.001%     99.818%         Const  pool_3/_reshape/shape\n    7.256    0.003    0.002   0.001%     99.819%         Const  mixed_8/tower/conv_1/batchnorm/beta\n    7.003    0.005    0.002   0.001%     99.820%         Const  mixed_7/tower/conv_2/batchnorm/moving_variance\n    6.934    0.004    0.002   0.001%     99.821%         Const  mixed_7/tower/conv/batchnorm/moving_variance\n    6.631    0.005    0.002   0.001%     99.822%         Const  mixed_6/tower/conv_2/batchnorm/moving_mean\n    6.564    0.004    0.002   0.001%     99.823%         Const  mixed_6/tower/conv/batchnorm/moving_mean\n    6.468    0.004    0.002   0.001%     99.824%         Const  mixed_5/tower_1/conv_4/batchnorm/moving_variance\n    6.070    0.005    0.002   0.001%     99.825%         Const  mixed_4/tower_1/conv_4/batchnorm/moving_mean\n    1.662    0.003    0.002   0.001%     99.826%         Const  mixed_2/tower_1/conv_2/batchnorm/moving_variance\n    1.652    0.003    0.002   0.001%     99.827%         Const  mixed_2/tower_1/conv_2/batchnorm/beta\n    1.612    0.003    0.002   0.001%     99.828%         Const  mixed_2/tower_1/conv/batchnorm/moving_mean\n    1.594    0.003    0.002   0.001%     99.829%         Const  mixed_2/tower/conv_1/batchnorm/moving_variance\n    1.276    0.004    0.002   0.001%     99.830%         Const  mixed_1/tower_1/conv_1/batchnorm/moving_variance\n    0.375    0.003    0.002   0.001%     99.831%         Const  mixed_1/tower/conv/batchnorm/moving_variance\n    0.278    0.002    0.002   0.001%     99.832%         Const  mixed/tower_1/conv/batchnorm/beta\n  165.443    0.002    0.002   0.001%     99.833%      Identity  mixed_4/tower_1/conv_1/control_dependency\n    7.739    0.004    0.002   0.001%     99.834%         Const  softmax/biases\n    7.641    0.002    0.002   0.001%     99.835%         Const  mixed_10/tower_1/conv/batchnorm/moving_mean\n    7.592    0.002    0.002   0.001%     99.836%         Const  mixed_10/tower/mixed/conv/batchnorm/beta\n    7.496    0.003    0.002   0.001%     99.837%         Const  mixed_9/tower_1/mixed/conv/batchnorm/moving_mean\n    7.440    0.003    0.002   0.001%     99.838%         Const  mixed_9/tower_1/conv/conv2d_params\n    7.277    0.003    0.002   0.001%     99.839%         Const  mixed_8/tower_1/conv/conv2d_params\n    7.230    0.005    0.002   0.001%     99.840%         Const  mixed_8/tower/conv/batchnorm/beta\n    6.700    0.004    0.002   0.001%     99.841%         Const  mixed_6/tower_1/conv_1/batchnorm/moving_mean\n    6.639    0.004    0.002   0.001%     99.842%         Const  mixed_6/tower/conv_2/batchnorm/moving_variance\n    6.112    0.004    0.002   0.001%     99.843%         Const  mixed_4/tower_2/conv/batchnorm/moving_variance\n    5.917    0.004    0.002   0.001%     99.844%         Const  mixed_4/tower_1/conv/batchnorm/moving_mean\n    5.717    0.003    0.002   0.001%     99.845%         Const  mixed_3/tower/conv/batchnorm/moving_variance\n    2.327    0.004    0.002   0.001%     99.846%         Const  mixed_2/tower_2/conv/batchnorm/moving_variance\n    2.320    0.004    0.002   0.001%     99.847%         Const  mixed_2/tower_2/conv/batchnorm/moving_mean\n    1.634    0.003    0.002   0.001%     99.847%         Const  mixed_2/tower_1/conv_1/batchnorm/moving_mean\n    1.572    0.003    0.002   0.001%     99.848%         Const  mixed_2/tower/conv/batchnorm/moving_variance\n    0.302    0.002    0.002   0.001%     99.849%         Const  mixed/tower_1/conv_1/batchnorm/moving_mean\n    0.266    0.004    0.002   0.001%     99.850%         Const  mixed/tower/conv_1/batchnorm/moving_mean\n    7.612    0.003    0.002   0.001%     99.851%         Const  mixed_10/tower/mixed/conv_1/batchnorm/beta\n    7.596    0.006    0.002   0.001%     99.852%         Const  mixed_10/tower/mixed/conv/batchnorm/moving_mean\n    7.477    0.003    0.002   0.001%     99.853%         Const  mixed_9/tower_1/conv_1/batchnorm/moving_mean\n    7.135    0.004    0.002   0.001%     99.854%         Const  mixed_7/tower_1/conv_3/batchnorm/moving_mean\n    7.036    0.005    0.002   0.001%     99.855%         Const  mixed_7/tower_1/conv/batchnorm/moving_variance\n    6.571    0.004    0.002   0.001%     99.856%         Const  mixed_6/tower/conv/batchnorm/moving_variance\n    6.146    0.004    0.002   0.001%     99.857%         Const  mixed_5/conv/batchnorm/moving_variance\n    5.808    0.003    0.002   0.001%     99.858%         Const  mixed_4/tower/conv/batchnorm/moving_mean\n    5.774    0.003    0.002   0.001%     99.859%         Const  mixed_4/conv/batchnorm/moving_mean\n    5.686    0.008    0.002   0.001%     99.860%         Const  mixed_3/conv/batchnorm/moving_variance\n    0.225    0.002    0.002   0.001%     99.861%         Const  mixed/tower/conv/batchnorm/moving_mean\n    0.171    0.003    0.002   0.001%     99.862%         Const  conv_4/batchnorm/beta\n    0.168    0.002    0.002   0.001%     99.863%         Const  conv_4/conv2d_params\n  198.873    0.002    0.002   0.001%     99.864%      Identity  mixed_8/tower_1/conv_1/control_dependency\n  197.122    0.002    0.002   0.001%     99.865%      Identity  mixed_8/tower_1/conv/control_dependency\n  183.372    0.003    0.002   0.001%     99.866%      Identity  mixed_6/tower_1/conv_2/control_dependency\n  118.131    0.003    0.002   0.001%     99.867%      Identity  conv_4/control_dependency\n    7.660    0.003    0.002   0.001%     99.867%         Const  mixed_10/tower_1/conv_1/batchnorm/moving_mean\n    7.513    0.002    0.002   0.001%     99.868%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/beta\n    7.500    0.006    0.002   0.001%     99.869%         Const  mixed_9/tower_1/mixed/conv/batchnorm/moving_variance\n    7.425    0.006    0.002   0.001%     99.870%         Const  mixed_9/tower/mixed/conv_1/batchnorm/beta\n    7.321    0.003    0.002   0.001%     99.871%         Const  mixed_8/tower_1/conv_2/batchnorm/beta\n    6.598    0.004    0.002   0.001%     99.872%         Const  mixed_6/tower/conv_1/batchnorm/moving_mean\n    6.366    0.005    0.002   0.001%     99.873%         Const  mixed_5/tower_1/conv_2/batchnorm/moving_variance\n    6.261    0.004    0.002   0.001%     99.874%         Const  mixed_5/tower/conv_2/batchnorm/moving_variance\n    6.041    0.005    0.002   0.001%     99.875%         Const  mixed_4/tower_1/conv_3/batchnorm/moving_variance\n    5.829    0.002    0.002   0.001%     99.876%         Const  mixed_4/tower/conv_1/batchnorm/moving_mean\n    5.813    0.002    0.002   0.001%     99.877%         Const  mixed_4/tower/conv/batchnorm/moving_variance\n    5.779    0.009    0.002   0.001%     99.878%         Const  mixed_4/conv/batchnorm/moving_variance\n    1.440    0.003    0.002   0.001%     99.879%         Const  mixed_1/tower_2/conv/batchnorm/moving_variance\n    0.387    0.004    0.002   0.001%     99.880%         Const  mixed_1/tower/conv_1/batchnorm/moving_mean\n    0.313    0.002    0.002   0.001%     99.881%         Const  mixed/tower_1/conv_2/batchnorm/beta\n    0.216    0.002    0.002   0.001%     99.882%         Const  mixed/tower/conv/batchnorm/beta\n    7.603    0.003    0.002   0.001%     99.882%         Const  mixed_10/tower/mixed/conv/batchnorm/moving_variance\n    6.996    0.005    0.002   0.001%     99.883%         Const  mixed_7/tower/conv_2/batchnorm/moving_mean\n    6.970    0.004    0.002   0.001%     99.884%         Const  mixed_7/tower/conv_1/batchnorm/moving_variance\n    6.331    0.004    0.002   0.001%     99.885%         Const  mixed_5/tower_1/conv_1/batchnorm/moving_variance\n    5.974    0.004    0.002   0.001%     99.886%         Const  mixed_4/tower_1/conv_1/batchnorm/moving_variance\n    5.759    0.003    0.002   0.001%     99.887%         Const  mixed_3/tower/conv_2/batchnorm/moving_variance\n    1.657    0.003    0.002   0.001%     99.888%         Const  mixed_2/tower_1/conv_2/batchnorm/moving_mean\n    1.511    0.003    0.002   0.001%     99.889%         Const  mixed_2/conv/batchnorm/moving_variance\n    0.371    0.002    0.002   0.001%     99.890%         Const  mixed_1/tower/conv/batchnorm/moving_mean\n  174.545    0.003    0.002   0.001%     99.891%      Identity  mixed_5/tower_1/conv_2/control_dependency\n  166.837    0.002    0.002   0.001%     99.892%      Identity  mixed_4/tower_1/conv_2/control_dependency\n  143.065    0.002    0.002   0.001%     99.893%      Identity  mixed_1/tower_1/conv_2/control_dependency\n    7.472    0.003    0.002   0.001%     99.894%         Const  mixed_9/tower_1/conv_1/batchnorm/beta\n    7.456    0.003    0.002   0.001%     99.895%         Const  mixed_9/tower_1/conv/batchnorm/moving_mean\n    7.432    0.003    0.002   0.001%     99.895%         Const  mixed_9/tower/mixed/conv_1/batchnorm/moving_mean\n    7.392    0.003    0.002   0.001%     99.896%         Const  mixed_9/tower/conv/batchnorm/moving_mean\n    7.372    0.003    0.002   0.001%     99.897%         Const  mixed_9/conv/batchnorm/moving_mean\n    7.301    0.003    0.002   0.001%     99.898%         Const  mixed_8/tower_1/conv_1/batchnorm/beta\n    7.244    0.003    0.002   0.001%     99.899%         Const  mixed_8/tower/conv/batchnorm/moving_variance\n    7.063    0.004    0.002   0.001%     99.900%         Const  mixed_7/tower_1/conv_1/batchnorm/moving_mean\n    6.860    0.004    0.002   0.001%     99.901%         Const  mixed_6/tower_2/conv/batchnorm/moving_variance\n    6.659    0.005    0.002   0.001%     99.902%         Const  mixed_6/tower_1/conv/batchnorm/beta\n    6.605    0.004    0.002   0.001%     99.903%         Const  mixed_6/tower/conv_1/batchnorm/moving_variance\n    6.530    0.005    0.002   0.001%     99.904%         Const  mixed_6/conv/batchnorm/moving_mean\n    6.523    0.004    0.002   0.001%     99.905%         Const  mixed_6/conv/batchnorm/beta\n    6.227    0.004    0.002   0.001%     99.906%         Const  mixed_5/tower/conv_1/batchnorm/moving_variance\n    1.506    0.003    0.002   0.001%     99.906%         Const  mixed_2/conv/batchnorm/moving_mean\n    0.361    0.003    0.002   0.001%     99.907%         Const  mixed_1/conv/batchnorm/moving_variance\n    0.201    0.002    0.002   0.001%     99.908%         Const  mixed/conv/batchnorm/beta\n    0.194    0.006    0.002   0.001%     99.909%         Const  mixed/conv/conv2d_params\n   90.699    0.002    0.002   0.001%     99.910%      Identity  pool/control_dependency\n    7.680    0.002    0.002   0.001%     99.911%         Const  mixed_10/tower_1/mixed/conv/batchnorm/moving_mean\n    7.552    0.003    0.002   0.001%     99.912%         Const  mixed_10/conv/batchnorm/beta\n    7.492    0.003    0.002   0.001%     99.913%         Const  mixed_9/tower_1/mixed/conv/batchnorm/beta\n    7.412    0.003    0.002   0.001%     99.914%         Const  mixed_9/tower/mixed/conv/batchnorm/moving_mean\n    7.405    0.005    0.002   0.001%     99.915%         Const  mixed_9/tower/mixed/conv/batchnorm/beta\n    7.366    0.005    0.002   0.001%     99.916%         Const  mixed_9/conv/batchnorm/beta\n    7.344    0.002    0.002   0.001%     99.916%         Const  mixed_8/tower_1/conv_3/batchnorm/beta\n    7.109    0.004    0.002   0.001%     99.917%         Const  mixed_7/tower_1/conv_2/batchnorm/moving_variance\n    6.792    0.005    0.002   0.001%     99.918%         Const  mixed_6/tower_1/conv_3/batchnorm/moving_variance\n    5.889    0.005    0.002   0.001%     99.919%         Const  mixed_4/tower/conv_2/batchnorm/moving_variance\n    1.368    0.002    0.002   0.001%     99.920%         Const  mixed_1/tower_1/conv_2/batchnorm/moving_variance\n  206.494    0.001    0.002   0.001%     99.921%      Identity  mixed_9/tower_1/mixed/conv_1/control_dependency\n  150.954    0.002    0.002   0.001%     99.922%      Identity  mixed_2/tower_1/conv_2/control_dependency\n  119.766    0.003    0.002   0.001%     99.923%      Identity  pool_1/control_dependency\n    7.719    0.002    0.002   0.001%     99.924%         Const  mixed_10/tower_2/conv/batchnorm/moving_mean\n    7.556    0.005    0.002   0.001%     99.925%         Const  mixed_10/conv/batchnorm/moving_mean\n    7.543    0.003    0.002   0.001%     99.925%         Const  mixed_9/tower_2/conv/batchnorm/moving_variance\n    7.376    0.003    0.002   0.001%     99.926%         Const  mixed_9/conv/batchnorm/moving_variance\n    7.168    0.005    0.002   0.001%     99.927%         Const  mixed_7/tower_1/conv_4/batchnorm/moving_mean\n    5.833    0.004    0.002   0.001%     99.928%         Const  mixed_4/tower/conv_1/batchnorm/moving_variance\n    0.316    0.002    0.002   0.001%     99.929%         Const  mixed/tower_1/conv_2/batchnorm/moving_mean\n    0.113    0.003    0.002   0.001%     99.930%         Const  conv_1/conv2d_params\n  206.498    0.001    0.002   0.001%     99.931%      Identity  mixed_9/tower_1/mixed/conv/control_dependency\n    7.532    0.003    0.002   0.001%     99.932%         Const  mixed_9/tower_2/conv/batchnorm/beta\n    7.448    0.002    0.002   0.001%     99.933%         Const  mixed_9/tower_1/conv/batchnorm/beta\n    7.335    0.002    0.002   0.001%     99.934%         Const  mixed_8/tower_1/conv_2/batchnorm/moving_variance\n    7.070    0.004    0.002   0.001%     99.934%         Const  mixed_7/tower_1/conv_1/batchnorm/moving_variance\n    6.295    0.004    0.002   0.001%     99.935%         Const  mixed_5/tower_1/conv/batchnorm/moving_variance\n    0.333    0.002    0.002   0.001%     99.936%         Const  mixed/tower_2/conv/batchnorm/moving_variance\n    0.208    0.002    0.002   0.001%     99.937%         Const  mixed/conv/batchnorm/moving_variance\n  192.621    0.002    0.002   0.001%     99.938%      Identity  mixed_7/tower_1/conv_2/control_dependency\n  168.643    0.002    0.002   0.001%     99.939%      Identity  mixed_4/tower_1/conv_4/control_dependency\n  160.120    0.003    0.002   0.001%     99.940%      Identity  mixed_3/conv/control_dependency\n   93.940    0.003    0.002   0.001%     99.941%      Identity  conv_3/control_dependency\n    7.723    0.002    0.002   0.001%     99.941%         Const  mixed_10/tower_2/conv/batchnorm/moving_variance\n    7.621    0.002    0.002   0.001%     99.942%         Const  mixed_10/tower/mixed/conv_1/batchnorm/moving_mean\n    7.481    0.005    0.002   0.001%     99.943%         Const  mixed_9/tower_1/conv_1/batchnorm/moving_variance\n    7.353    0.003    0.002   0.001%     99.944%         Const  mixed_8/tower_1/conv_3/batchnorm/moving_mean\n    7.325    0.006    0.002   0.001%     99.945%         Const  mixed_8/tower_1/conv_2/batchnorm/moving_mean\n    7.285    0.006    0.002   0.001%     99.946%         Const  mixed_8/tower_1/conv/batchnorm/moving_mean\n    7.272    0.003    0.002   0.001%     99.947%         Const  mixed_8/tower/conv_1/batchnorm/moving_variance\n    7.209    0.004    0.002   0.001%     99.948%         Const  mixed_7/tower_2/conv/batchnorm/moving_variance\n    5.738    0.002    0.002   0.001%     99.948%         Const  mixed_3/tower/conv_1/batchnorm/moving_variance\n    0.392    0.002    0.002   0.001%     99.949%         Const  mixed_1/tower/conv_1/batchnorm/moving_variance\n    0.355    0.001    0.002   0.001%     99.950%         Const  mixed_1/conv/batchnorm/beta\n    0.186    0.002    0.002   0.001%     99.951%         Const  conv_4/batchnorm/moving_mean\n    0.149    0.002    0.002   0.001%     99.952%         Const  conv_3/conv2d_params\n  212.976    0.002    0.002   0.001%     99.953%      Identity  mixed_10/tower_1/mixed/conv/control_dependency\n  133.137    0.003    0.002   0.001%     99.954%      Identity  mixed/tower_1/conv_2/control_dependency\n    7.645    0.002    0.002   0.001%     99.955%         Const  mixed_10/tower_1/conv/batchnorm/moving_variance\n    7.582    0.003    0.002   0.001%     99.955%         Const  mixed_10/tower/conv/batchnorm/moving_variance\n    7.266    0.002    0.002   0.001%     99.956%         Const  mixed_8/tower/conv_1/batchnorm/moving_mean\n    7.202    0.004    0.002   0.001%     99.957%         Const  mixed_7/tower_2/conv/batchnorm/moving_mean\n    6.780    0.005    0.002   0.001%     99.958%         Const  mixed_6/tower_1/conv_3/batchnorm/moving_mean\n    1.616    0.003    0.002   0.001%     99.959%         Const  mixed_2/tower_1/conv/batchnorm/moving_variance\n    0.330    0.002    0.002   0.001%     99.960%         Const  mixed/tower_2/conv/batchnorm/moving_mean\n    0.190    0.002    0.002   0.001%     99.961%         Const  conv_4/batchnorm/moving_variance\n  205.438    0.002    0.002   0.001%     99.961%      Identity  mixed_9/tower_1/conv_1/control_dependency\n    7.576    0.005    0.002   0.001%     99.962%         Const  mixed_10/tower/conv/batchnorm/moving_mean\n    7.563    0.002    0.002   0.001%     99.963%         Const  mixed_10/conv/batchnorm/moving_variance\n    7.517    0.002    0.002   0.001%     99.964%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/moving_mean\n    7.396    0.003    0.002   0.001%     99.965%         Const  mixed_9/tower/conv/batchnorm/moving_variance\n    7.175    0.005    0.002   0.001%     99.966%         Const  mixed_7/tower_1/conv_4/batchnorm/moving_variance\n    7.142    0.004    0.002   0.001%     99.967%         Const  mixed_7/tower_1/conv_3/batchnorm/moving_variance\n    0.121    0.002    0.002   0.001%     99.967%         Const  conv_1/batchnorm/moving_mean\n  213.020    0.002    0.002   0.001%     99.968%      Identity  mixed_10/tower_1/mixed/conv_1/control_dependency\n  194.363    0.002    0.002   0.001%     99.969%      Identity  mixed_7/tower_1/conv_3/control_dependency\n    7.704    0.002    0.002   0.001%     99.970%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/moving_variance\n    7.699    0.003    0.002   0.001%     99.971%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/moving_mean\n    7.537    0.002    0.002   0.001%     99.972%         Const  mixed_9/tower_2/conv/batchnorm/moving_mean\n    7.416    0.003    0.002   0.001%     99.972%         Const  mixed_9/tower/mixed/conv/batchnorm/moving_variance\n    7.237    0.005    0.002   0.001%     99.973%         Const  mixed_8/tower/conv/batchnorm/moving_mean\n    7.683    0.003    0.002   0.001%     99.974%         Const  mixed_10/tower_1/mixed/conv/batchnorm/moving_variance\n    7.625    0.003    0.002   0.001%     99.975%         Const  mixed_10/tower/mixed/conv_1/batchnorm/moving_variance\n    7.460    0.003    0.002   0.001%     99.976%         Const  mixed_9/tower_1/conv/batchnorm/moving_variance\n    7.305    0.006    0.002   0.001%     99.977%         Const  mixed_8/tower_1/conv_1/batchnorm/moving_mean\n    7.292    0.003    0.002   0.001%     99.977%         Const  mixed_8/tower_1/conv/batchnorm/moving_variance\n    0.319    0.002    0.002   0.001%     99.978%         Const  mixed/tower_1/conv_2/batchnorm/moving_variance\n    0.128    0.003    0.002   0.001%     99.979%         Const  conv_2/conv2d_params\n  211.972    0.001    0.002   0.001%     99.980%      Identity  mixed_10/tower_1/conv_1/control_dependency\n  176.043    0.002    0.002   0.001%     99.981%      Identity  mixed_5/tower_1/conv_3/control_dependency\n  167.622    0.002    0.002   0.001%     99.982%      Identity  mixed_4/tower_1/conv_3/control_dependency\n    7.523    0.003    0.002   0.001%     99.982%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/moving_variance\n    0.160    0.003    0.002   0.001%     99.983%         Const  conv_3/batchnorm/moving_mean\n    0.153    0.002    0.002   0.001%     99.984%         Const  conv_3/batchnorm/beta\n  185.500    0.001    0.001   0.001%     99.985%      Identity  mixed_6/tower_1/conv_4/control_dependency\n    9.125    0.004    0.001   0.001%     99.986%      Identity  conv/control_dependency\n    7.664    0.003    0.001   0.001%     99.986%         Const  mixed_10/tower_1/conv_1/batchnorm/moving_variance\n    7.436    0.002    0.001   0.001%     99.987%         Const  mixed_9/tower/mixed/conv_1/batchnorm/moving_variance\n    7.357    0.002    0.001   0.001%     99.988%         Const  mixed_8/tower_1/conv_3/batchnorm/moving_variance\n  200.723    0.001    0.001   0.001%     99.989%      Identity  mixed_8/tower_1/conv_3/control_dependency\n    7.312    0.003    0.001   0.001%     99.990%         Const  mixed_8/tower_1/conv_1/batchnorm/moving_variance\n    0.142    0.002    0.001   0.001%     99.990%         Const  conv_2/batchnorm/moving_mean\n    0.117    0.002    0.001   0.001%     99.991%         Const  conv_1/batchnorm/beta\n    0.110    0.002    0.001   0.001%     99.992%         Const  conv/batchnorm/moving_variance\n  200.197    0.002    0.001   0.001%     99.993%      Identity  mixed_8/tower_1/conv_2/control_dependency\n  177.221    0.001    0.001   0.001%     99.994%      Identity  mixed_5/tower_1/conv_4/control_dependency\n    0.132    0.002    0.001   0.001%     99.994%         Const  conv_2/batchnorm/beta\n    0.146    0.002    0.001   0.001%     99.995%         Const  conv_2/batchnorm/moving_variance\n    0.097    0.003    0.001   0.001%     99.996%         Const  conv/batchnorm/beta\n  195.623    0.002    0.001   0.001%     99.997%      Identity  mixed_7/tower_1/conv_4/control_dependency\n    0.157    0.002    0.001   0.001%     99.997%         Const  conv_3/batchnorm/gamma\n    0.106    0.002    0.001   0.001%     99.998%         Const  conv/batchnorm/moving_mean\n    0.164    0.002    0.001   0.001%     99.999%         Const  conv_3/batchnorm/moving_variance\n  184.372    0.002    0.001   0.001%     99.999%      Identity  mixed_6/tower_1/conv_3/control_dependency\n    0.125    0.002    0.001   0.001%    100.000%         Const  conv_1/batchnorm/moving_variance\n\nI tensorflow/core/util/stat_summarizer.cc:263] \n", "This appears to be a duplicate of #2807 Please continue the discussion there.\n\nIt would be more helpful if you\n- describe the system you are running on\n- describe how the quantization was done\n- maybe give the times you see without quantization, for comparison, i.e. which Ops slow down, and by how much.\n"]}, {"number": 4152, "title": "Not able to get NV profile for tensorflow(inception)", "body": "### Environment info\n\nOperating System: Ubuntu 14.04\nMachine: x86\nInstalled version of CUDA and cuDNN: \nCUDA 7.5, cuDNN 5.1.3\nInstalled from source:\n1. The commit hash (`git rev-parse HEAD`)\n9454b9015567ae0641250004fd2fafccede54a93\n2. The output of `bazel version`\n(tensorflow)asis@ws:~$ bazel version\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n### Getting NV Profile for the following sample code:\n\n```\n(tensorflow)asis@ws:~$ cat test.py\nimport tensorflow as tf\nc = []\nfor d in ['/gpu:0', '/gpu:1']:\n  with tf.device(d):\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\n    c.append(tf.matmul(a, b))\nwith tf.device('/cpu:0'):\n  sum = tf.add_n(c)\nsess = tf.Session()\nprint sess.run(sum)\n\n(tensorflow)asis@ws:~$ nvprof -o nvtest_x86 python test.py\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5.1.3 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\n==41815== NVPROF is profiling process 41815, command: python test.py\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.562\npciBusID 0000:84:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x313af00\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties:\nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.562\npciBusID 0000:85:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:85:00.0)\n[[  44.   56.]\n [  98.  128.]]\n==41815== Generated result file: /home/asis/samples/asis/nvtest_x86\n(tensorflow)asis@ws:~$\n```\n\n**Generated result file: /home/asis/samples/asis/nvtest_x86**\n### Not getting VN profile for the following TensorFlow(Inception)\n\n```\n(tensorflow)asis@ws:~$ nvprof -o nvprof_bs64_2gpu_3 bazel-bin/inception/imagenet_train --max_steps=3 --num_gpus=2 --batch_size=64 --train_dir=/home/asis/googlenet/test/train_bs64_2gpu_20160901_041429 --data_dir=/home/asis/inception_output_data\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5.1.3 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:84:00.0\nTotal memory: 12.00GiB\nFree memory: 11.88GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7eb0000\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties:\nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:85:00.0\nTotal memory: 12.00GiB\nFree memory: 11.88GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:85:00.0)\nWARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7536 get requests, put_count=4071 evicted_count=1000 eviction_rate=0.24564 and unsatisfied allocation rate=0.605759\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\n2016-09-01 07:00:14.565170: step 0, loss = 13.07 (0.6 examples/sec; 104.984 sec/batch)\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=2012 evicted_count=2000 eviction_rate=0.994036 and unsatisfied allocation rate=0\n======== Warning: No CUDA application was profiled, exiting\n```\n\n**======== Warning: No CUDA application was profiled, exiting**\n### Problem\n\nFor the Sample code we are getting nv profile, but for the tensorflow(inception) code we are not getting nv profile and getting the following warning.\n**Warning: No CUDA application was profiled, exiting**\n\nHow can we get nv profile on x86 for tensorflow(inception)\n", "comments": ["This happens because the inception script being run is actually a wrapper that launches the real script in a subprocess.\n\nA workaround is to use the real script directly:\n\n`nvprof -o nvprof_bs64_2gpu_3 python bazel-bin/inception/imagenet_train.runfiles/__main__/inception/inception_train.py`\n", "Hi @benbarsdell \nIf I try as you specified, I am getting the following error:\n\n```\nTraceback (most recent call last):\n  File \"bazel-bin/inception/imagenet_train.runfiles/__main__/inception/inception_train.py\", line 30, in <module>\n    from inception import image_processing\nImportError: No module named inception\n======== Warning: No CUDA application was profiled, exiting\n======== Error: Application returned non-zero code 1\n```\n", "Sorry, I forgot about another step to the WAR that needs to be done first:\n\n```\n$ touch inception/__init__.py\n$ touch inception/slim/__init__.py\n```\n", "This also did not help.\n", "Hi @benbarsdell \n\nFinally it worked. I ran the following command under \"bazel-bin/inception/imagenet_train.runfiles/**main**/\" directory where as it should on the current directory. \n\n```\n$ touch inception/__init__.py\n$ touch inception/slim/__init__.py\n```\n\nNow I am able to get NV profile.\n\nThanks :+1: \n", "Closing the issue.\n", "Hi @asispatra \r\nI'm trying to learn how to profile tensorflow codes with nvprof. I tried this simple code that you shared:\r\n\r\nimport tensorflow as tf\r\nc = []\r\nfor d in ['/gpu:0', '/gpu:1']:\r\n  with tf.device(d):\r\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])\r\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\r\n    c.append(tf.matmul(a, b))\r\nwith tf.device('/cpu:0'):\r\n  sum = tf.add_n(c)\r\nsess = tf.Session()\r\nprint sess.run(sum)\r\n\r\n\r\nbut I'm getting this error:\r\n\r\n======== Warning: No CUDA application was profiled, exiting\r\n======== Error: Application returned non-zero code 127\r\n\r\n\r\nI can run this program successfully without using nvprof. ", "> Hi @benbarsdell\r\n> Finally it worked. I ran the following command under \"bazel-bin/inception/imagenet_train.runfiles/main/\" directory where as it should on the current directory.\r\n> $ touch inception/__init__.py\r\n> $ touch inception/slim/__init__.py\r\n> \r\n> Now I am able to get NV profile.\r\n> Thanks \ud83d\udc4d\r\n\r\nI deployed tensorflow with Anaconda. The directories are different with that listed above. Similarly, when I try to measure the bandwidth of some specific neural networks, such as alexnet, the .py of which locates in two directories, one is \"xxx\\Anaconda3\\pkgs\\tensorflow-base-1.13.1-eigen_py36hf8af7b3_0\\Lib\\site-packages\\tensorflow\\contrib\\slim\\python\\slim\\nets\" and the other is \"C:\\Users\\hygon\\Anaconda3\\pkgs\\tensorflow-gpu-1.10.0-py36_0\\Lib\\site-packages\\tensorflow\\contrib\\slim\\python\\slim\\nets\". The command  \"nvprof --log-file xxx.log python alexnet.py\", however, brings about the same warning \"No CUDA application was profiled, exiting\", while the error that follows returns non-zero code of -1 rather than 127. I have no idea of how to profile the net. Could you please give me some advice? "]}, {"number": 4151, "title": "Memory leak when continuously run assign op", "body": "### Environment info\n\nOperating System:\nUbuntu 16.04\nInstalled version of CUDA and cuDNN: \n8.0 RC, 5.1 (on GTX 1080)\nIf installed from source, provide \nHEAD: a8512e24deeeebe57eb1be14726634e7e6c23545\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n### Problem:\n\nIn my application, I need to change value of some variable, and run the minimize in a loop, thus I have to run assign op continuously, which may add new op to graph every time, thus the program become very slow and memory explode.\n\n```\nsess = tf.Session()\na = tf.Variable(np.ones((5, 10000, 10000, 3)))\nsess.run(tf.initialize_all_variables())\n\nt0 = time.time()\nfor i in range(10000):\n    sess.run(tf.assign(a,np.ones((5, 10000, 10000, 3)) ))\n    t1 = time.time()\n    print(t1-t0)\n    t0 = t1\n```\n\nSo is there a method to change the value of Variables without adding an op to graph? Or a way to remove it after(I have a big graph defined before the loop, I don't want to reset all of them and define again with reset_default_graph)?\nAnd since the value assigned to the variable is different all the time, I cannot define the op before the for loop.\n", "comments": ["Could you please try something like:\n\n```\nsess = tf.Session()\na = tf.Variable(np.ones((5, 100, 100, 3)))\nsess.run(tf.initialize_all_variables())\nupdate_a = tf.assign(a,np.ones((5, 100, 100, 3)) )  #Define update operation here \nt0 = time.time()\nfor i in range(10):\n    sess.run(update_a)\n    t1 = time.time()\n    print(t1-t0)\n    t0 = t1\n```\n\nPlease also refer to this [SO](http://stackoverflow.com/questions/37966924/why-does-tf-assign-slow-the-execution-time) thread.\n", "the data assign to the variable is different in every iteration, so I cannot define the op before the loop.\n", "For example in neural art, I want to init the generate image with content image, in each iteration, I read new content image, and assign it to the variable.\n", "This memory leak is caused by adding new nodes (a `tf.assign()` node and an implicitly created `tf.constant()` node) on each iteration of the training loop. This [documentation](http://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow/13426/use-graph-finalize-to-catch-nodes-being-added-to-the-graph#t=201609011401186013951) has a guide to tracking down leaks like this.\n\nThe solution for your particular problem is to define a single assign op that takes its input from a `tf.placeholder()`, and feed different values into that placeholder on each iteration:\n\n``` python\nsess = tf.Session()\na = tf.Variable(np.ones((5, 10000, 10000, 3)))\nupdate_placeholder = tf.placeholder(a.dtype, shape=a.get_shape())\nupdate_op = a.assign(update_placeholder)\n\nsess.run(tf.initialize_all_variables())\n\nt0 = time.time()\nfor i in range(10000):\n    # Obviously, you'd change the value being assigned in each step in a real program.\n    sess.run(update_op, {update_placeholder: np.ones((5, 10000, 10000, 3))})\n    t1 = time.time()\n    print(t1-t0)\n    t0 = t1\n```\n", "@mrry Hello, the document link is not available. Could you update it?"]}, {"number": 4150, "title": "[wip] 4d image ops", "body": "Continuation of #3566 - I decided to change to master branch, since I needed some stuff that wasn't in r0.9.\n\nI've extended hue, saturation, and the flip ops now. Comments or suggestions are welcome.\n\nI'll continue working on the other ops here and there, when I have the time, and after that I'll write some tests.\n\nThe latest commit might look a bit strange, because of a merge with the latest upstream master.\n", "comments": ["Can one of the admins verify this patch?\n", "@ppries I think we should hold off on review until you add the tests, does that seem reasonable to you?\n", "Yes, of course. I'll slowly chip away at it as I have time.\n", "@ppries Let us know when you're ready for a review.\n", "Closing due to inactivity. Let us know when it's ready.\n"]}, {"number": 4149, "title": "Wide and Deep Learning tutorial example fails on Python 3.4", "body": "When using the \"deep\" functionality in the Wide and Deep Learning tutorial on Python 3.4, I get the following error:\n\n```\nValueError: Duplicate feature column key found for column: education_embedding. This usually means that the column is almost identical to another column, and one must be discarded.\n```\n\nLooks like a bug in tensorflow/contrib/layers/python/layers/feature_column.py in the _EmbeddingColumn class. The key(self) property is plagued by this bug: https://bugs.python.org/issue24931\n\nSo instead of coming out with a nice unique key, we get the following key for all _EmbeddingColumn instances: '_EmbeddingColumn()'\n\nThis causes the feature_column_ops.py's check_feature_columns() function to determine that the second _EmbeddingColumn instance is a duplicate since they keys of all of them are the same.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nI found this StackOverflow thread http://stackoverflow.com/questions/39249704/tensorflow-valueerror-duplicate-feature-column-key-found-for-column/ where someone had the same problem, with no answers.  So I did the debugging and answered the question.\n### Environment info\n\nOperating System: Fedora Core 23\n\nInstalled version of CUDA and cuDNN: 8 and 5.1\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n[jpangburn@localhost examples]$ ls /usr/local/cuda/lib64/libcud*\n/usr/local/cuda/lib64/libcudadevrt.a\n/usr/local/cuda/lib64/libcudart.so\n/usr/local/cuda/lib64/libcudart.so.8.0\n/usr/local/cuda/lib64/libcudart.so.8.0.27\n/usr/local/cuda/lib64/libcudart_static.a\n/usr/local/cuda/lib64/libcudnn.so\n/usr/local/cuda/lib64/libcudnn.so.5\n/usr/local/cuda/lib64/libcudnn.so.5.1.5\n/usr/local/cuda/lib64/libcudnn_static.a\n\n```\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n\n```\n[jpangburn@localhost examples]$ pwd\n/home/jpangburn/open_source/tensorflow/tensorflow/examples\n[jpangburn@localhost examples]$ git rev-parse HEAD\n17d0e46e6cc31af0bcf6e80ff4c5670d233b4940\n\n```\n1. The output of `bazel version`\n\n```\n[jpangburn@localhost examples]$ bazel version\n.\nBuild label: 0.3.1\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\nBuild timestamp: 1469783392\nBuild timestamp as int: 1469783392\n\n```\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nBe in the \"tensorflow/tensorflow/examples/learn\" directory.  Patch the wide_n_deep_tutorial.py to work in Python 3 by replacing \"urllib\" instances with \"urllib.request\".  Then run it \"python3 wide_n_deep_tutorial_py3.py --train_data=train_data --test_data=test_data --model_type=wide_n_deep\", obviously replacing the train_data and test_data files with your own copies, or omitting those switches altogether to download the files fresh.\n### What other attempted solutions have you tried?\n\nI worked around the problem by creating a subclass of _EmbeddedColumn and using that, but not sure my key(self) property implementation is working because the accuracy result with wide_n_deep is worse than with just \"wide\".  You can see this in my answer to the StackOverflow question at http://stackoverflow.com/questions/39249704/tensorflow-valueerror-duplicate-feature-column-key-found-for-column/39268045#39268045\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\n```\nTraceback (most recent call last):\n  File \"wide_n_deep_tutorial_py3.py\", line 220, in <module>\n    tf.app.run()\n  File \"/usr/lib/python3.4/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"wide_n_deep_tutorial_py3.py\", line 216, in main\n    train_and_eval()\n  File \"wide_n_deep_tutorial_py3.py\", line 204, in train_and_eval\n    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\n  File \"/usr/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 219, in fit\n    max_steps=max_steps)\n  File \"/usr/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 479, in _train_model\n    train_op, loss_op = self._get_train_ops(features, targets)\n  File \"/usr/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 166, in _get_train_ops\n    logits = self._logits(features, is_training=True)\n  File \"/usr/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 244, in _logits\n    dnn_feature_columns = self._get_dnn_feature_columns()\n  File \"/usr/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 208, in _get_dnn_feature_columns\n    feature_column_ops.check_feature_columns(self._dnn_feature_columns)\n  File \"/usr/lib/python3.4/site-packages/tensorflow/contrib/layers/python/layers/feature_column_ops.py\", line 318, in check_feature_columns\n    f.name))\nValueError: Duplicate feature column key found for column: education_embedding. This usually means that the column is almost identical to another column, and one must be discarded.\n\n```\n", "comments": ["The bug has been resolved in the master branch, I was using the 0.10rc0 branch.  All that's required now for wide_n_deep_tutorial.py to work on Python 3 is the urllib -> urllib.request change mentioned above.  With that change on the master branch, you get the same accuracy on Python 3 as Python 2 for \"wide_n_deep\" model type.\n\nI'd close this, except not sure if there's a way to make the urllib problem work on both Python 2 and 3.  Maybe you want to leave this open for that bit of it?  Anyway, thanks!\n", "Hmm, I'm not familiar with that particular tutorial, but if the `urllib` import is the only issue, would it work to replace:\n\n``` python\nimport urllib\n```\n\n...with:\n\n``` python\nfrom six.moves import urllib\n```\n\n...at the [top of `wide_n_deep_tutorial.py`](https://github.com/tensorflow/tensorflow/blob/461caa86137aee3d2e40843ea308c216f10c4655/tensorflow/examples/learn/wide_n_deep_tutorial.py#L21)? We do a similar thing in more battle-hardened tutorials like [`convolutional.py`](https://github.com/tensorflow/tensorflow/blob/461caa86137aee3d2e40843ea308c216f10c4655/tensorflow/models/image/mnist/convolutional.py#L32)\n\n@jpangburn If you could test that this works for you on Python 3, would you mind sending a PR?\n", "OK @mrry I created a PR https://github.com/tensorflow/tensorflow/pull/4183 for this.  I see that this issue is also reported as #3956 \n", "@mrry \nI'm also getting worse accuracy (usually) for wide_n_deep compare to just wide.  The values do fluctuate since I'm using 980 GTX GPU.  But the values seems to fluctuate only for the wide_n_deep accuracies (I've gotten as high as .840 and as low as .796), the wide is fixed (always get 0.835452 with GPU).\nThis is using the wide_n_deep_tutorial.py as of 9/9/2016\nrunning with \nAnaconda 3.5 \nTensorflow 0.10.0rc0\nUbuntu 16.04\n", "@gsiisg This sounds like a separate issue, unrelated to Python 3 compatibility. Please open a new issue describing your problem.\n"]}, {"number": 4148, "title": "Build ERROR: undeclared inclusion(s) in rule '//tensorflow/core/kernels:batchtospace_op_gpu':", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System: Linux RHL 7.2\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): cuda 7.5 + cudnn 5\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n./configure (select to support GPU)\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package  --verbose_failures\n### What other attempted solutions have you tried?\n\nRebuild from scratch, still repro.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\nERROR: /data/tools/tensorflow/core/kernels/BUILD:1509:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:batchtospace_op_gpu':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/batchtospace_op_gpu.cu.cc':\n  '/usr/local/cuda-7.5/include/cuda_runtime.h'\n  '/usr/local/cuda-7.5/include/host_config.h'\n  '/usr/local/cuda-7.5/include/builtin_types.h'\n  '/usr/local/cuda-7.5/include/device_types.h'\n  '/usr/local/cuda-7.5/include/host_defines.h'\n  '/usr/local/cuda-7.5/include/driver_types.h'\n  '/usr/local/cuda-7.5/include/surface_types.h'\n  '/usr/local/cuda-7.5/include/texture_types.h'\n  '/usr/local/cuda-7.5/include/vector_types.h'\n  '/usr/local/cuda-7.5/include/channel_descriptor.h'\n  '/usr/local/cuda-7.5/include/cuda_runtime_api.h'\n  '/usr/local/cuda-7.5/include/cuda_device_runtime_api.h'\n  '/usr/local/cuda-7.5/include/driver_functions.h'\n  '/usr/local/cuda-7.5/include/vector_functions.h'\n  '/usr/local/cuda-7.5/include/vector_functions.hpp'\n  '/usr/local/cuda-7.5/include/common_functions.h'\n  '/usr/local/cuda-7.5/include/math_functions.h'\n  '/usr/local/cuda-7.5/include/math_functions.hpp'\n  '/usr/local/cuda-7.5/include/math_functions_dbl_ptx3.h'\n  '/usr/local/cuda-7.5/include/math_functions_dbl_ptx3.hpp'\n  '/usr/local/cuda-7.5/include/cuda_surface_types.h'\n  '/usr/local/cuda-7.5/include/cuda_texture_types.h'\n...\n", "comments": ["@skydoorkai - Are you explicitly specifying the CUDA version when running `./configure`? If not, then most likely, you are hitting #3985.\n", "I had this issue and fixed it by explicitly specifying the CUDA version (8.0) when running ./configure.\n", "I'm closing this as a duplicate of #3985.\n"]}, {"number": 4147, "title": "`math_grad._ProdGrad` fails to handle cases where reduction_indices is scalar", "body": "```\nx = tf.zeros([10])\ntf.gradients(tf.reduce_prod(x, 0), [x])\n```\n\nGives\n\n```\nmore traceback\n\n/home/***/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_grad.pyc in _ProdGrad(op, grad)\n    128   reduced = math_ops.cast(op.inputs[1], dtypes.int32)\n    129   idx = math_ops.range(0, array_ops.rank(op.inputs[0]))\n--> 130   other, _ = array_ops.listdiff(idx, reduced)\n    131   perm = array_ops.concat(0, [reduced, other])\n    132   reduced_num = math_ops.reduce_prod(array_ops.gather(input_shape, reduced))\n\nmore traceback\n```\n\nIn line 128, `op.inputs[1]` could be a scalar, which will cause a shape mismatch when passed to `array_ops.listdiff` in line 130.\n\nTF version: master branch a week ago\n", "comments": ["I think this is a duplicate of #3815, which is being fixed by #3858.\n"]}, {"number": 4146, "title": "Provide generic android JNI interface", "body": "In the Android example, the developer need to write some customized C++ code for the JNI call, this extra effort is not needed especially in the prototyping step.\n\nCould we provide a generic JNI interface (just like generic inference serving, with input/output as a map of tensors, using serialized protobuf bytes or customized serialization), then the developer will not need to write and BUILD C++ code anymore.\n\nOf course, the developers are free to optimize the code by writing the customized JNI calls. This feature will definitely boost the development process.\n", "comments": ["We have plans to provide a basic Java interface as you suggest for inference, which should handle most situations without the need for custom C++ modification.\n", "Sounds great @andrewharp\n", "Android-specific Java interface for inference added in 54bd703932480790643c641796386c225e4e1cf3. It may eventually be superseded by a full Java API implementation, but for now it should allow running a variety of graphs without any native code recompilation.\n"]}, {"number": 4145, "title": "remove XCode 7.2 instructions", "body": "As of CUDA 7.5.27, CUDA is working with Xcode 7.3. We no longer need the 7.2 instructions.\n", "comments": ["Can one of the admins verify this patch?\n", "@jmhodges, thanks for your PR! By analyzing the annotation information on this pull request, we identified @yifeif, @keveman and @vrv to be potential reviewers\n", "Or maybe I'm wrong? The cuda-samples deviceQuery command passes, but I'm running into segfaults when importing tensorflow from inside `_pywrap_tensorflow.so``perftools::gputools::internal::DsoLoader::GetDsoHandle(tensorflow::StringPiece, void**, perftools::gputools::internal::DsoLoader::LoadKind)`\n\n(That's using the pre-built python2 GPU wheel)\n", "Building from source and think I'm running into the \"El Capitan doesn't let DYLD_LIBRARY_PATH be passed to certain executables\" error because `//tensorflow/cc:tutorials_example_trainer` is crashing with `Library not loaded: @rpath/libcudart.7.5.dylib`. I've managed to get it found by lldb and such, so it's really a matter of finding the right place to add the extra linker flags, I think. Early indication is that `--linkopt` alone isn't the right place.\n\nOutput below. I'm sure I'll wake up to someone telling me I'm missing something clear to them. Thank you much for reading this.\n\n```\n$ bazel build --linkopt=\"-L/usr/local/cuda/lib\" -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures\n  INFO: Found 1 target...\nERROR: /Users/jmhodges/src/github.com/tensorflow/tensorflow/tensorflow/cc/BUILD:179:1: Executing genrule //tensorflow/cc:image_ops_genrule failed: bash failed: error executing command \n  (cd /private/var/tmp/_bazel_jmhodges/f1566b21c1a8243b6c363a1a9e5e03a4/execroot/tensorflow && \\\n  exec env - \\\n    PATH='/Users/jmhodges/google-cloud-sdk/bin:/usr/local/cuda/bin:/Users/jmhodges/bin:/Users/jmhodges/projects/go/bin:/Library/Application Support/VMware Fusion:/Users/jmhodges/sys/depot_tools:/Users/jmhodges/bin:/Users/jmhodges/sys/bin:/Users/jmhodges/sys/sbin:/Users/jmhodges/projects/android_install/tools:/opt/local/bin:/opt/local/sbin:/Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/MacGPG2/bin' \\\n    TMPDIR=/var/folders/t7/t3l9x81d7r710b8m1p0wxqp00000gn/T/ \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/cc/ops/image_ops_gen_cc bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/image_ops.h bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/image_ops.cc 0'): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 5.\ndyld: Library not loaded: @rpath/libcudart.7.5.dylib\n  Referenced from: /private/var/tmp/_bazel_jmhodges/f1566b21c1a8243b6c363a1a9e5e03a4/execroot/tensorflow/bazel-out/host/bin/tensorflow/cc/ops/image_ops_gen_cc\n  Reason: image not found\n/bin/bash: line 1:  5404 Trace/BPT trap: 5       bazel-out/host/bin/tensorflow/cc/ops/image_ops_gen_cc bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/image_ops.h bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/image_ops.cc 0\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n```\n", "Should we still leave the comment in, being specific about the version of cuda 7.5? \n", "Yeah, it was a CUDA 7.5 fix that should have corrected things.\n\nHowever, I'm still getting the error above (and sometimes [this one](https://github.com/tensorflow/tensorflow/issues/4105)) so I can't confirm right now.\n", "Yeah, ugh, it looks like the DYLD_LIBRARY_PATH isn't being seen by crosstool_wrapper_driver_is_not_gcc.tpl\n\nI put\n\n```\npa = os.getenv(\"DYLD_LIBRARY_PATH\")\nprint('DYLDCUDA')\nprint(pa)\n```\n\nreturn before the `return subprocess.call([CPU_COMPILER] + cpu_compiler_flags)` and its spitting out `None`s when it is for sure set in my shell.\n\nThis happens on OS X 10.11 El Capitan when your code gets run through binaries that live in /usr/bin (and, I believe, /bin) when you have System Integrity Protection on (which it is by default).\n\nI'm trying to come up with what binary that's the problem. Current hypothesis is bash\n", "Oh, this is maybe bazel stripping the environment variables? http://www.bazel.io/docs/designs/2016/06/02/sandboxing.html#handling-of-environment-variables\n\nWhich means that the problem is maybe that none of the `--linkopts` or `linkopt` on targets get to passed to `crosstool_wrapper_driver_is_not_gcc`?\n", "I see it's been a week since there was activity on this PR. @jmhodges are you blocked waiting for a response on our end? \n", "I've not been able to get the tensorflow build to carry my DYLD_LIBRARY_FLAGS down inside the templated cross_tool_not_gcc_yadda command so that I can build against CUDA to double check this.\n\nIf someone else can confirm that Xcode 7.3.1 with the latest CUDA works or can clue me in on how to fix the DYLD_LIBRARY_FLAGS problem, that'd be cool\n", "@benoitsteiner Any idea here regarding Xcode and CUDA?\n", "It's quite possible bazel strips environment variables. Instead of removing the 7.2 instructions, can we make them more specific to the problematic cuda versions?\n", "Closing due to inactivity. Feel free to reply to @martinwicke's comment and reopen if this is current.\n"]}, {"number": 4144, "title": "use 'typedef' instead of 'using' to compile on gcc4.8.2", "body": "Currently, gcc 4.8.2 complains with the following error message:\n\n```\nIn file included from tensorflow/core/distributed_runtime/rpc/grpc_master_service.cc:38:0:\n./tensorflow/core/distributed_runtime/rpc/grpc_call.h:251:35: error: 'Tag' is not a class, namespace, or enumeration\n   Tag request_received_tag_{this, Tag::kRequestReceived};\n                                   ^\n./tensorflow/core/distributed_runtime/rpc/grpc_call.h:252:32: error: 'Tag' is not a class, namespace, or enumeration\n   Tag response_sent_tag_{this, Tag::kResponseSent};\n                                ^\n./tensorflow/core/distributed_runtime/rpc/grpc_call.h:253:28: error: 'Tag' is not a class, namespace, or enumeration\n   Tag cancelled_tag_{this, Tag::kCancelled};                            ^\n```\n\nThis PR proposes to use old fashioned typedef, and the build has passed with gcc4.8.2\n", "comments": ["@raingo, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @mrry and @suharshs to be potential reviewers\n", "Can one of the admins verify this patch?\n", "@mrry Yes, it works. Only one line is changed now.\n", "Great, thanks!\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "That failure looks like a flake, but let's roll the dice again to see if we can get a clean build before submitting....\n"]}, {"number": 4143, "title": "improve error message when a category is empty", "body": "When a category has no images, the error message doesn't display which label causes it:\n\n```\n2016-09-01 06:08:13.722176: Step 0: Train accuracy = 64.0%\n2016-09-01 06:08:13.722418: Step 0: Cross entropy = 1.541732\nCRITICAL:tensorflow:Category has no images - validation.\n```\n\nThis change includes the label in the error message so it reads\n\n```\nCRITICAL:tensorflow:Category flowers has no images - validation\n```\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 4142, "title": "set attr num_sampled <= range_max", "body": "For Log Uniform Candidate sampler the num_sampled cannot be more than the range_max as mentioned in #4112 \n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n", "I think a better fix is to add a validation check on \nthird_party/tensorflow/core/kernels/candidate_sampler_ops.cc:113\n\nOP_REQUIRES(context, num_samples_ <= range_max, ...)\n", "@zffchen78 I am unable to find `https://github.com/tensorflow/tensorflow/blob/master/third_party/tensorflow/core/kernels/candidate_sampler_ops.cc:113`. Can you provide a link to the file where you think the validation will be better off ?\n", "@shkr look here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/candidate_sampler_ops.cc\n", "Thanks @danmane I will commit the change\n", "@danmane let me know if any other changes are required\n", "ping @danmane \n", "@zffchen78 I have made the requisite change\n", "Jenkins, test this please.\n", "@zffchen78 does this look good now?\n", "No. I requested a test.\n", "@zffchen78 Did I miss anything or are you waiting for a jenkins test ?\n", "@shkr I believe @zffchen78 means, he requests that you add a Python unit test (probably in python/kernel_tests) to verify the new behavior.\n", "@danmane thanks I will add a unit test\n", "Any updates?\n", "Closing to help clear out our backlog, feel free to send a new PR once you've addressed these changes, or ping this thread and we'll reopen.\n"]}, {"number": 4141, "title": "Fix bug where TensorBoard will crash if it can't determine host ip address", "body": "", "comments": ["LGTM\n\nIn a future change I'd like to have it display the hostname (if it's in DNS) rather than the IP address.\n"]}, {"number": 4140, "title": "update reader._file_to_word_ids", "body": "_file_to_word_ids throws a key error if there is a word in the validation or test data that doesn't exist in the training data.\n\nThe unknown words won't exist in word_to_id dictionary and so will throw a key error while converting validation and test files into ids.\n\nChanged to way the file id sequence is built by checking if the word exists in the word_to_id dictionary to prevent the key error.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@afshinrahimi Please sign the CLA so we can move forward with the review process.\n", "I have signed.\n\nOn Sep 7, 2016 06:51, \"Daniel W Mane\" notifications@github.com wrote:\n\n> @afshinrahimi https://github.com/afshinrahimi Please sign the CLA so we\n> can move forward with the review process.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4140#issuecomment-245086443,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AFNIK6watXLjEscFic-AeW-7p-W8k5Thks5qndJFgaJpZM4JyMep\n> .\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins Test this please.\n", "Looks like the build failure was a flake due to connection timeout. I'm going to merge it. Thanks!\n"]}, {"number": 4139, "title": "Add the mention bot to tensorflow's github", "body": "Integrate Mention-bot with the tensorflow github pull request system\n\nhttps://github.com/facebook/mention-bot \n\n> The mention bot will automatically mention potential reviewers on pull requests. It helps getting faster turnaround on pull requests by involving the right people early on.\n", "comments": ["By analyzing the text of this issue I've identified @aselle as someone who will want to know about this. \n\nI'm all for it. Adding it now.\n", "Let's see how this works. We have a PR triage, but I bet it'll be relaxing as hell to have a reviewer recommendation. We can also play with the config. I wish it had an option to not @mention people, but just say their names instead (as a recommendation for a human). \n\nIf this catches on, we may make a PR.\n", "Pretty cool.\n"]}, {"number": 4138, "title": "seq2seq model is not efficient when using sampled softmax", "body": "Hi, \nmy commit number is\n70de76e696c21da617fd2e6435cf7fedab220db8\n\nI want to try sampled softmax to speed up my lm training. I'm changing slightly on the ptb training example code. I'm working on a CPU machine.\nFirst, on ptb(vocab 10k), I'm using the \"small\" config, I see some speed up gain:\nnormal softmax 896wps\n\nsample_softmax h256L1ba20 sample512 1975wps\n\nThen, I move to a larger set, and using 100k vocab, I'm still setting the sample number to 512, I think the speed should be similar to the ptb case. But I got the speed to be about 300 wps, even if I set the sample number as small as 4. I don't understand why it should be so slower than the ptb case. Do you know how can I make it faster?\n\nHere's some of the related code:\n\n```\n      output = tf.reshape(tf.concat(1, outputs), [-1, size])\n      softmax_w = tf.get_variable(\"softmax_w\", [size, vocab_size])\n      softmax_w_t = tf.transpose(softmax_w)\n      softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n\n    if use_sample_softmax == True:\n      loss = tf.nn.sampled_softmax_loss(softmax_w_t, softmax_b, output, tf.reshape(self._targets, [-1, 1]), num_samples, vocab_size) #todo\n      self._cost = cost = tf.reduce_sum(loss) / batch_size\n    else:\n      logits = tf.matmul(output, softmax_w) + softmax_b\n      loss = tf.nn.seq2seq.sequence_loss_by_example(\n          [logits],\n          [tf.reshape(self._targets, [-1])],\n          [tf.ones([batch_size * num_steps])])\n      self._cost = cost = tf.reduce_sum(loss) / batch_size\n\n\n```\n\nThanks!\nGoose\n", "comments": ["I suspect the issue is that the weight matrix is transposed, and this is causing the automatically-generated gradients code to take a slow path. In particular, `tf.nn.sampled_softmax_loss()` will generate sparse gradients for the weights, but the [gradient function for `tf.transpose()`](https://github.com/tensorflow/tensorflow/blob/9725395b8d68df30cfc81d0076db2365e49753fb/tensorflow/python/ops/array_grad.py#L341) doesn't have an optimized implementation for handling sparse `tf.IndexedSlices` objects.\n\nCan you try running an experiment where you define the `softmax_w` variable as having shape `[vocab_size, size]` so that you don't have to transpose it? I would expect the performance of the sampled softmax to be much better in that case.\n", "Hi Mrry,\n\nThanks very much for your kind reply!\nas you suggested, I changed the part of code to:\n\n```\n    output = tf.reshape(tf.concat(1, outputs), [-1, size])\n    softmax_w_t = tf.get_variable(\"softmax_w\", [vocab_size, size])\n    softmax_w = tf.transpose(softmax_w_t)\n    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n\n    if use_sample_softmax == True:\n      loss = tf.nn.sampled_softmax_loss(softmax_w_t, softmax_b, output, tf.reshape(self._targets, [-1, 1]), num_samples, vocab_size) #todo\n      self._cost = cost = tf.reduce_sum(loss) / batch_size\n    else:\n      logits = tf.matmul(output, softmax_w) + softmax_b\n      loss = tf.nn.seq2seq.sequence_loss_by_example(\n          [logits],\n          [tf.reshape(self._targets, [-1])],\n          [tf.ones([batch_size * num_steps])])\n      self._cost = cost = tf.reduce_sum(loss) / batch_size\n```\n\nAnd yes, the speed now becomes\nsample \n1024 1500wps \n128 1731wps\n\nIt seems more normal now.\n\nAlso, maybe the developers want to change the code in source/tensorflow/tensorflow/models/rnn/translate/seq2seq_model.py, because there a transpose of matrix is also feed into a smapled softmax\n\nThanks!\nGoose\n", "Thanks for the suggestion - assuming there are no knock-on effects to the full softmax case, we should fix the sample code to avoid the performance pitfall on sampled softmax. I'll assign this to the original author, @lukaszkaiser, in case there's a reason to avoid doing this, but this is probably an area where we'd appreciate contributions.\n", "Fix in #4270.\n", "@cloudygoose I am doing the same thing for a proper sampled_softmax performance.\r\nit works fine for training.\r\nHowever, a testing time, with num_steps=1 and bat_size=1, it make a huge loop with a tf.transpose for each iter (ie word) which makes it very slow.\r\nHow can we avoid to execute this transpose so many times at test time ?\r\n"]}, {"number": 4137, "title": "Merge TensorBoard 28 into r0.10", "body": "This includes many bugfixes, and some new features, like better smoothing and log scale option for the event plots.\n", "comments": []}, {"number": 4136, "title": "Branch 131878651", "body": "", "comments": []}, {"number": 4135, "title": "Seg fault when computing gradient of 3D convolution filter with (1,1,1) kernel", "body": "Minimal example to reproduce the bug:\n\n``` python\nimport numpy as np\nimport tensorflow as tf\n\nsess = tf.Session()\nwith sess.as_default():\n    # Input: [batch, height, width, depth, input_channels]\n    x_shape = [3, 85, 65, 83, 8]\n    # Filter: [kernel_height, kernel_width, depth, input_channels, output_channels]\n    f_shape = [1, 1, 1, x_shape[-1], 32]\n    # Output: [batch, height, width, depth, output_channels]\n    y_shape = [3, 85, 65, 83, f_shape[-1]]\n\n    np.random.seed(1)  # Make it reproducible.\n    x_val = np.random.random_sample(x_shape).astype(np.float32)\n    f_val = np.random.random_sample(f_shape).astype(np.float32)\n    output_val = np.random.random_sample(y_shape).astype(np.float32)\n    x = tf.constant(x_val, name=\"x\", dtype=tf.float32)\n    f = tf.constant(f_val, name=\"f\", dtype=tf.float32)\n\n    output = tf.nn.conv3d(x, f, strides=(1,1,1,1,1), padding=\"SAME\")\n\n    r = tf.gradients(output, f)\n    print(r[0].eval())\n```\n\nthis leads to a seg fault, tested on a Mac with CPU, both with tensorflow 0.9.0 (binary release) and when compiled from source (last commit https://github.com/tensorflow/tensorflow/commit/ad4f02a69162abe5d242b7d94f62138849aec9ab ).\n\nFor certain values of `(x_shape[\u20131], f_shape[-1])`, for instance (2,4) or (8,32), the seg fault occurs, but for other values (2,2) or (16,32), there is no seg fault.\n\nInserting some `std::cout <<` print statements in the code shows that the segmentation fault occurs during this function call in Conv3DBackpropFilterOp::Compute https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_grad_ops_3d.cc#L315\n", "comments": ["Thanks for the report.  Internal bug opened.\n", "Also running into this on 0.10.0rc0 (and nightly) on ubuntu 14.04 and python 3.5\n", "I'm not seeing this with the latest nightly?\nIf I don't hear back I'll assume this was resolved.\n", "I was able to reproduce this on Mac OS X with the nightly. So this appears to be a Mac only bug.\n", "It does not cause crash on my Linux/Ubuntu 14.04, for both CPU and GPU.\r\n\r\nBtw, try not to use Mac OS because Apple developers tend to create their own standards which is different from the state-of-the-art convention, causing inconvenience and trouble. Last time, when I was doing online translation service. I found that only for Mac OS, if your URL ends with a multi-byte UTF-8 character, the system API will automatically append a NULL character at the end. This causes my web program to function incorrectly for Apple devices.", "@kevin-keraudren could you try with the latest version and build with `--config=asan`?", "Closing due to inactivity. Feel free to re-open if you would like us to look again."]}, {"number": 4133, "title": "Compile error: invalid use of incomplete type 'class perftools::gputools::StreamExecutor' in stream_executor/kernel.h", "body": "#### Issue\n\nBuilding tensorflow from source using gcc 7.0.0 results in the below compile error.\n\nThe error is clearly enough because the structure of class StreamExecutor is not known from kernel.h.\n\n```\n$ bazel build --verbose_failures -c opt //tensorflow/tools/pip_package:build_pip_package\n...\nERROR: /opt/tensorflow-r0.10/tensorflow/stream_executor/BUILD:5:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed: gcc failed: error executing command \n  (cd /home/foreese/.cache/bazel/_bazel_foreese/2c35d3fa162a38720aa5307b2053cde8/execroot/tensorflow-r0.10 && \\\n  exec env - \\\n    LD_LIBRARY_PATH=/opt/gcc-dev/lib64:/opt/gcc-dev/lib:/opt/support/lib:/opt/support/lib32:/opt/gdb-7.7.1/lib \\\n    PATH=/opt/gcc-dev/bin:/opt/gcc-dev/libexec/gcc/x86_64-pc-linux-gnu/7.0.0:/home/foreese/bin:/home/foreese/.local/bin:/opt/python-3.5/bin:/opt/support/bin:/usr/local/bin:/usr/bin:/bin: \\\n  /opt/gcc-dev/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/opt/gcc-dev/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-canonical-system-headers -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/local-py3-opt/bin/tensorflow/stream_executor/_objs/stream_executor/tensorflow/stream_executor/machine_manager.pic.d '-frandom-seed=bazel-out/local-py3-opt/bin/tensorflow/stream_executor/_objs/stream_executor/tensorflow/stream_executor/machine_manager.pic.o' -fPIC -DHAVE_CONFIG_H -iquote . -iquote bazel-out/local-py3-opt/genfiles -iquote external/protobuf -iquote bazel-out/local-py3-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/local-py3-opt/genfiles/external/bazel_tools -iquote external/farmhash_archive -iquote bazel-out/local-py3-opt/genfiles/external/farmhash_archive -iquote external/gif_archive -iquote bazel-out/local-py3-opt/genfiles/external/gif_archive -iquote external/highwayhash -iquote bazel-out/local-py3-opt/genfiles/external/highwayhash -iquote external/jpeg_archive -iquote bazel-out/local-py3-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local-py3-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local-py3-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local-py3-opt/genfiles/external/eigen_archive -iquote external/zlib_archive -iquote bazel-out/local-py3-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/local-py3-opt/genfiles/external/local_config_cuda -isystem external/protobuf/src -isystem bazel-out/local-py3-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem bazel-out/local-py3-opt/genfiles/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem external/gif_archive/giflib-5.1.4/lib -isystem bazel-out/local-py3-opt/genfiles/external/gif_archive/giflib-5.1.4/lib -isystem external/highwayhash -isystem bazel-out/local-py3-opt/genfiles/external/highwayhash -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local-py3-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local-py3-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local-py3-opt/genfiles/external/re2 -isystem external/eigen_archive -isystem bazel-out/local-py3-opt/genfiles/external/eigen_archive -isystem external/zlib_archive/zlib-1.2.8 -isystem bazel-out/local-py3-opt/genfiles/external/zlib_archive/zlib-1.2.8 -isystem external/local_config_cuda/cuda -isystem bazel-out/local-py3-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/include -isystem bazel-out/local-py3-opt/genfiles/external/local_config_cuda/cuda/include -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/stream_executor/machine_manager.cc -o bazel-out/local-py3-opt/bin/tensorflow/stream_executor/_objs/stream_executor/tensorflow/stream_executor/machine_manager.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nIn file included from ./tensorflow/stream_executor/stream.h:33:0,\n                 from ./tensorflow/stream_executor/machine_manager.h:51,\n                 from tensorflow/stream_executor/machine_manager.cc:16:\n./tensorflow/stream_executor/kernel.h: In member function 'void perftools::gputools::TypedKernel<Params>::PackOneParam(std::vector<perftools::gputools::KernelArg>*, const T&, typename std::enable_if<perftools::gputools::IsDeviceMemoryValueLike<T>::value>::type*) const':\n./tensorflow/stream_executor/kernel.h:358:32: error: invalid use of incomplete type 'class perftools::gputools::StreamExecutor'\n     args->emplace_back(parent()->DeviceMemoryToKernelArg(arg));\n                                ^~\nIn file included from ./tensorflow/stream_executor/stream.h:29:0,\n                 from ./tensorflow/stream_executor/machine_manager.h:51,\n                 from tensorflow/stream_executor/machine_manager.cc:16:\n./tensorflow/stream_executor/device_memory.h:35:7: note: forward declaration of 'class perftools::gputools::StreamExecutor'\n class StreamExecutor;\n       ^~~~~~~~~~~~~~\nIn file included from ./tensorflow/stream_executor/stream.h:33:0,\n                 from ./tensorflow/stream_executor/machine_manager.h:51,\n                 from tensorflow/stream_executor/machine_manager.cc:16:\n./tensorflow/stream_executor/kernel.h: In member function 'void perftools::gputools::TypedKernel<Params>::PackOneParam(std::vector<perftools::gputools::KernelArg>*, T, typename std::enable_if<perftools::gputools::IsDeviceMemoryPointer<T>::value>::type*) const':\n./tensorflow/stream_executor/kernel.h:368:32: error: invalid use of incomplete type 'class perftools::gputools::StreamExecutor'\n     args->emplace_back(parent()->DeviceMemoryToKernelArg(*ptr));\n                                ^~\nIn file included from ./tensorflow/stream_executor/stream.h:29:0,\n                 from ./tensorflow/stream_executor/machine_manager.h:51,\n                 from tensorflow/stream_executor/machine_manager.cc:16:\n./tensorflow/stream_executor/device_memory.h:35:7: note: forward declaration of 'class perftools::gputools::StreamExecutor'\n class StreamExecutor;\n       ^~~~~~~~~~~~~~\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n```\n#### System details\n- OS: CentOS 6.8 x86_64-redhat-linux using\n- Compiler:  gcc 7.0.0 (I do this because my system gcc is 4.4.7 which is too old to build bazel properly with -std=c++11)\n- CUDA/cuDNN: None\n- tensorflow: branch r0.10 6ce5b5c8298273e3861a75fb6ccde63b9dd157c5\n- bazel:\n\n```\nBuild label: 0.3.1-2016-08-31 (@1b2071f)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Wed Aug 31 16:52:51 2016 (1472662371)\nBuild timestamp: 1472662371\nBuild timestamp as int: 1472662371\n```\n", "comments": ["Adding @henline, the proud owner of stream-executor. \n", "gcc-7 is still in development. Please use 4.8 or 6.x. Feel free to open a new github issue if the problem still persists in recent versions."]}, {"number": 4132, "title": "Unexpected performance changes as a function of batch size", "body": "I am observing unexpected performance from tensorflow as I change the batch size that I feed to the session.\n\n![image](https://cloud.githubusercontent.com/assets/966348/18139518/cfa80cc0-6fa9-11e6-9b7f-20bbfb30ddc9.png)\n\nI have created a [small jupyter](https://gist.github.com/tillahoffmann/63cbe9fce331df75a6b57420c48b7c36) notebook to demonstrate the issue. Errors bars correspond to the standard deviation of the mean over multiple runs.\n\nIn some of our more complex models, the jump in runtime occurs at small batch sizes (around 200 images of 40 by 80 pixels).\n", "comments": ["I tried your simple example and got a plot that doesn't show the trend reversal to the same degree, only a very slight one.\n\n![screenshot from 2016-08-31 15 19 22](https://cloud.githubusercontent.com/assets/15676913/18148411/58d8f0f2-6f8e-11e6-98ed-a996ae3f4e2d.png)\n\nI ran on a Google server which likely explains the difference from your results.\n\nIt's not surprising that there would be some kind of U-shaped curve in this experiment. Intuitively, it seems that amortized per-element computation costs should decrease as the batch size increases, but that trend can't continue forever.  Eventually you'll exceed some limit that poses increased costs, likely related to memory or thread management.   So, in practice the ideal batch size will be < infinity, and you'll need to tune for it. \n", "I agree that the trend should not continue forever but as sharp an increase in computation time as observed in my experiments seems to hint at a problem. I observe problems even when the amount of memory per batch is only a few megabytes--smaller than a typical image taken on a cell phone.\n\nI am using a 48 core server with 128GB of memory running centOS.\n", "I'm now quite puzzled.  The chart I posted above was from running in a python notebook, not jupyter, but something similar, with unknown tensorflow version and configuration state.  I was able to reproduce your results by switching to a different tensorflow version that I thought was built from a recent code tree sync.   The execution times I was seeing were 10x too slow for large tensors (batches, in your program), and the problem seemed to be related to use of placeholder, e.g. if I substituted use of a Var or constant for the fed placeholder, the slowdown disappeared.  In the process of looking deeper I did a recompile which caused the anomaly to disappear entirely.  \n\nI then retested your program using the most recent tensorflow-0.10.0rc0, and again I don't see the problem.   What version are you using?  \n", "Great to hear back from you. It's interesting that you could reproduce the problem with some versions of tensorflow. I think we pip installed ours and are using 0.10.0rc0. I will run some experiments tomorrow and will get back to you.\n\nRegarding the variables: Could you not reproduce the problem at all when you were using variables or was it reproducible if you replaced the variable values in the `feed_dict`?\n", "I have now rerun the benchmark with the convolutional network in a range of settings and could reproduce the problem in all of them. In particular, I used a python 3.5.2 environment installed with conda and tried the following tensorflow installations\n- `pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp35-cp35m-linux_x86_64.whl`\n- `pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp35-cp35m-linux_x86_64.whl`\n- `conda install tensorflow` using conda-forge\n- installed from master compiled without GPU support\n\nInterestingly, I cannot reproduce the problem on my MacBook which has fewer resources than the big server.\n", "I'm going to look at this a bit more, but first, in case you're willing to do some more experiments, let me give my thoughts.  I'm suspicious of the interaction between python and the backend graph execution environment.  I'm much more familiar with that backend environment and how it actually executes large tensor Ops, and there's nothing I know of there that seems like a plausible cause for the problem.  Due to the hybrid python/compiled nature of the binary, my usual profiling tools are not much good, so it's difficult to identify where the time is going, but I'm doubtful it's actually the tensor Op execution.  I wonder whether we might sometimes be seeing a slow data transfer from python to feed the placeholder.  Grasping at straws, maybe data alignment might be an issue.  If you see reproducible differences between python versions (and/or SWIG?) that would be interesting.\n", "We were able to resolve the problem by updating our linux kernel.\n\nOld: 3.10.0-327.13.1.el7.x86_64\u2028\nNew: 3.10.0-327.28.3.el7.x86_64\n", "I've spent some more time on this, and I have not been able to reproduce your original results since last week.  I've seen highly variable behavior in the execution times (on the order of ~2x, not 10x) and it looks like that's mainly due to threadpool contention (at least in my environment).  \n\nRecap: Your model basically tests doing one wide pairwise Op versus a sequential series of narrow pairwise Ops, where the inner loop performs the same number of atomic ops in aggregate.  In wide configuration, there's one iteration on the entire input, in narrow configuration there's many iterations on shorter segments.  In narrow configuration there's only a few threads active in the inner loop.  In wide configuration there may be as many threads active as cores available.\n\nWhat I'm seeing is that sometimes there's little thread contention on my test machine and the wide configuration runs slightly faster than narrow, as expected, with low latency.  However, sometimes there's contention from other processes and the wide configuration is much more vulnerable to having one or more closures delayed, so the execution variance is much higher and the mean time also drifts up above that over the narrow configuration.\n\nI'm going to close this issue unless evidence surfaces that there's a fixable problem.\n"]}, {"number": 4131, "title": "reduce_max and maximum give different results for negative infinity", "body": "Using `tf.maximum` with negative inf inputs as follows:\n\n```\ntf.maximum(-math.inf, -math.inf).eval()\n```\n\ngives the expected result `-inf`\n\nHowever, `tf.reduce_max`, on the same inputs:\n\n```\ntf.reduce_max([-math.inf, -math.inf]).eval()\n```\n\ngives: `-3.40282e+38` which is the min float32.\n\nFor positive infinity inputs, both functions result in `inf`.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nI posted this as an SO question first:\nhttp://stackoverflow.com/questions/39211546/bug-in-tensorflow-reduce-max-for-negative-infinity\n### Environment info\n\nOperating System: Ubuntu 16.04\n\nInstalled version of CUDA and cuDNN: \n\n```\n-rw-r--r-- 1 root root   560184 May 25 17:48 /usr/local/cuda-8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 May 25 17:52 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 May 25 17:52 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rw-r--r-- 1 root root   394472 May 25 17:48 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 May 25 17:48 /usr/local/cuda-8.0/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       13 Jul  4 16:22 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       17 Jul  4 16:22 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.0.5\n-rwxr-xr-x 1 root root 78065952 Jul  4 16:22 /usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5\n-rw-r--r-- 1 root root 68709594 Jul  4 16:22 /usr/local/cuda-8.0/lib64/libcudnn_static.a\n\n```\n\nInstalled from source.\n\nCommit hash: 3cb39956e622b322e43547cf2b6e337020643f21\n\nBazel:\n\n```\nBuild label: 0.3.0\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jun 10 11:38:23 2016 (1465558703)\nBuild timestamp: 1465558703\nBuild timestamp as int: 1465558703\n\n```\n", "comments": ["@pronobis Thanks for the bug report. I have fixed the bug internally at Google and in the open source Eigen repository (https://bitbucket.org/eigen/eigen/commits/741b932c41cebff49c4419b7e0ef9910ef1b2907). We will have to wait until a few unrelated issues are resolved in Eigen before pushing the fix to open source TensorFlow. Stay tuned! :-)\n", "@pronobis Update: The issues in Eigen have been resolved and the fix for this issue will be pushed out on Monday.\n", "Great! Thanks!\n"]}, {"number": 4130, "title": "Training a neural net in C++", "body": "1. There are examples in the repo for loading a pre-trained model in C++. Can someone add an example to train a simple 1 hidden layer fully connected neural net (or even logistic regression using SGD) in C++? \n2. I realize there is no publicly available support for auto differentiation in C++. Are there any plans to do so?\n", "comments": ["@sonalgupta [This](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/tutorials/example_trainer.cc) is an example that constructs a graph is C++ and runs many iterations of it to solve a numerical optimization problem. That example is missing automatic differentiation which is the missing component for training a neural net model, but @andydavis1 is making rapid progress towards that. Stay tuned.\n", "Great! Is there an estimate on when would the C++ implementation of auto-differentiation get released? Is it planned to be released before/with TF 1.0?\n", "@jhseu and @asimshankar for additional comments.", "Broadly speaking, there are two things holding training in languages other than python up. One is support for gradients (#6268) and the other is the fact that the \"control logic\" for training (optimizers, estimators etc.) are in python.\r\n\r\n#6268 talks about covering gradients. Once that is is resolved (and that might be a while), we'll have enough functionality in these languages to build up the control logic.\r\n\r\nWith that said, I'm tempted to close this out as a duplicate of #6268."]}]