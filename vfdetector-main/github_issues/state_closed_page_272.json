[{"number": 46226, "title": "[TFLM] Added support for optimized quantize kernel for CEVA-BX1 and CEVA-SP500 cores", "body": "see issue https://github.com/tensorflow/tensorflow/issues/45607\r\n\r\nStarted with quantize but more to follow\r\nAlso some fixes for CEVA makefiles in this PR.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@yair-ehrenwald  Can you please resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Closing this, opened a new PR https://github.com/tensorflow/tensorflow/pull/47781\r\n"]}, {"number": 46225, "title": "Add int8 and int16x8 support for WHERE operator", "body": "Added int8 and int16x8 support for WHERE operator and associated tests", "comments": ["Merge conflict resolved!"]}, {"number": 46224, "title": "Add int8 and int16x8 support for BROADCAST_TO operator", "body": "* Added support for quantized broadcast_to operator\r\n* Added tests for quantizing broadcast_to model", "comments": ["@MeghnaNatraj One of the internal checks (feedback/copybara) is failing. Any idea what could be causing that?"]}, {"number": 46223, "title": "Running multiple hexagon delegates sequentially. ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Snapdragon 855 running Android 9\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nRight now, I have two instances of TfLite interpreter created using the same tflite model. Some of the network is delegated to DSP using hexagon delegate. Both interpreters are initialized one after other. Then, Invoke() is called many times, on either one or the other interpreter. This works fine. \r\n\r\n**Describe the expected behavior**\r\n\r\nWhat happens if I create two instances of tfliteInterpreter from different models, both using hexagon delegate? Will this work as expected? \r\n\r\n", "comments": ["Yes this should work without issues.", "Can you briefly explain how sharing the same DSP between tflite interpreters work?", "Each delegate instance creates separate hexagon graph with separate rpc calls to the dsp.", "Closing this issue since it's addressed. Feel free to reopen if necessary. Thank you", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46223\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46223\">No</a>\n"]}, {"number": 46222, "title": "[ROCm] Updating XLA custom_call_test to enable it for the ROCm platform", "body": "--------------------------\r\n\r\n/cc @chsigg @cheshire @nvining-work ", "comments": ["think the errors in the `Linux GPU` CI job are because I accidentally made the the inclusion of cuda headers unconditional. I just pushed out a new commit to fix that, hopefully that will fix the CI errors.\r\n\r\n@cheshire , please re-approve", "@gbaned I believe this PR is actually merged, but somehow that is not getting reflected in the status....how do we fix that?", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR."]}, {"number": 46221, "title": "[ROCm] Raising the memory allocation cap for GPU unit tests from 1GB to 2GB", "body": "This PR/commit updates the `parallel_gpu_execute.sh` script to raise the GPU memory allocation cap from 1GB to 2GB when running unit-tests.\r\n\r\nRecently a couple of unit tests started failing on the ROCm platform because they were running out of memory\r\n\r\n```\r\n//tensorflow/python/kernel_tests:extract_image_patches_grad_test_gpu\r\n//tensorflow/python/ops/numpy_ops:np_interop_test_gpu\r\n```\r\n\r\nGPU unit tests (atleast on the ROCm platform) are run with a cap that is set and implemented as shown here :\r\n\r\n* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/gpu_build/parallel_gpu_execute.sh#L26-L32\r\n* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/stream_executor_pimpl.cc#L130-L137\r\n* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/stream_executor_pimpl.cc#L151\r\n* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/stream_executor_pimpl.cc#L487-L503\r\n\r\nIt does not seem that the `parallel_gpu_execute.sh` is being used on the CUDA platform (anymore...think it was in the past). There does not seem to be any reference to it in the `Invocation Details` tab of the `Linux GPU` CI job.\r\n\r\nfor e.g - https://source.cloud.google.com/results/invocations/09d63e6a-f7a9-4fc6-9708-2fdd40b8b193/details\r\n\r\nIt also does not seem that GPU unit tests on the CUDA platform are being subjected to the 1GB memory cap. This can be verified by looking the at `Target Log` for the `//tensorflow/python/ops/numpy_ops:np_interop_test_gpu` test in the `Linux GPU` CI job (actually any GPU unit test)\r\n\r\nfor e.g. - https://source.cloud.google.com/results/invocations/09d63e6a-f7a9-4fc6-9708-2fdd40b8b193/targets/%2F%2Ftensorflow%2Fpython%2Fops%2Fnumpy_ops:np_interop_test_gpu/log\r\n\r\nOn the ROCm platform, we see the following log messages which are generated as a consequence of the memory cap (when TF tried to grab the entire available GPU memory on startup)\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/stream_executor_pimpl.cc#L488-L494\r\n\r\n```\r\n W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 16133306368 on device 0 within provided limit. [used=0, limit=1073741824]\r\n W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 14519974912 on device 0 within provided limit. [used=0, limit=1073741824]\r\n W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 13067976704 on device 0 within provided limit. [used=0, limit=1073741824]\r\n W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 11761178624 on device 0 within provided limit. [used=0, limit=1073741824]\r\n...\r\n...\r\n...\r\n```\r\n\r\nThese messsage are not present in unit tests logs for `Linux GPU` CI job, which seems to suggest that the env var `TF_PER_DEVICE_MEMORY_LIMIT_MB` is not set when the unit tests are run. Either that or the GPU on which the tests are being run has 1GB total memory which is unlikely.\r\n\r\n--------------------------------------------------\r\n\r\n/cc @chsigg @cheshire @nvining-work \r\n\r\n", "comments": []}, {"number": 46220, "title": "Tensorflow Lite Micro micro_speech example suppress warning on MacOS build", "body": "When building for MacOS an error occurs when using the built in Apple clang (mentioned in https://github.com/tensorflow/tensorflow/issues/46218). The built in Apple clang version:\r\n\r\n```\r\nConfigured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include/c++/4.2.1\r\nApple clang version 12.0.0 (clang-1200.0.32.27)\r\nTarget: x86_64-apple-darwin19.6.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n```\r\n\r\nThis fix selectively suppresses the warning to work on MacOS. Alternatively, the below works on the same Clang version\r\n\r\n```\r\nAudioStreamBasicDescription recordFormat = { };\r\n```", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46220) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Thanks for the pull request. Let's go with proper zero initialization (the fix that you have mentioned in the PR description) instead of suppressing the warning.\r\n", "@advaitjain added proper zero initialization "]}, {"number": 46219, "title": "[ROCm] Fix for breakage in XLA Conv Op functionality", "body": "The following commit breaks Conv Op functionality (in the XLA backend) for ROCm platform.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/8684c6b2e95601542c6c5c006bde5dd50f589a50\r\n\r\nThe cause seems to be that the `scratch_size` field in the new `GpuConvDescriptor` is not getting correctly populated in the new MLIR path. It is being used correctly in the convolution runner code.\r\n\r\ndeclaration:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/8684c6b2e95601542c6c5c006bde5dd50f589a50#diff-6453912dbc4ee715a56da9d7b218b52795dea2aa631a482101fc6d58c573d9ccR122-R135\r\n\r\nuse (get access) in conv runner:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/8684c6b2e95601542c6c5c006bde5dd50f589a50#diff-a01181d08b28a9c7432f22439622f16725126184283a73822c70b2151098a8adR277\r\n\r\nset access in non-MLIR(?) based path:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/8684c6b2e95601542c6c5c006bde5dd50f589a50#diff-a01181d08b28a9c7432f22439622f16725126184283a73822c70b2151098a8adR450\r\n\r\nThis commit merely adds the missing \"set\" in the MLIR based path\r\n\r\n------------------------------------------\r\n\r\nthanks to @ekuznetsov139 for identifying the fix\r\n\r\n/cc @chsigg @cheshire @nvining-work \r\n\r\n", "comments": ["@timshen91 could you take a look?", "Adding @jurahul who authored the original convolution thunk patch."]}, {"number": 46218, "title": "micro_speech: Run on macOS make error", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.7\r\n- TensorFlow installed from (source or binary): binary\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): macOS\r\n\r\n**Describe the problem**\r\nError when running\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile micro_speech\r\n```\r\n\r\nError message:\r\n```\r\ntensorflow/lite/micro/examples/micro_speech/osx/audio_provider.cc:64:48: error: missing field 'mFormatID' initializer [-Werror,-Wmissing-field-initializers]\r\n  AudioStreamBasicDescription recordFormat = {0};\r\n                                               ^\r\n1 error generated.\r\ngmake: *** [tensorflow/lite/micro/tools/make/Makefile:623: tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/examples/micro_speech/osx/audio_provider.o] Error 1\r\n```\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n\r\n", "comments": ["Further information:\r\n\r\nrunning:\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile test_micro_speech_test\r\n```\r\n\r\nresults in\r\n\r\n```\r\ntensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/micro_speech_test: PASS\r\n```"]}, {"number": 46217, "title": "DepthwiseConv2D is slower than Conv2D", "body": "I saw the similar issue in \r\nhttps://github.com/tensorflow/tensorflow/issues/42172\r\nAs in this post, when the batch size is larger, DepthwiseConv2D can be faster than Conv2D.\r\nHowever, if I have to set the batch size to 1, is there any method to speed up DepthwiseConv2D?\r\n", "comments": ["> I saw the similar issue in\r\n> #42172\r\n> As in this post, when the batch size is larger, DepthwiseConv2D can be faster than Conv2D.\r\n> However, if I have to set the batch size to 1, is there any method to speed up DepthwiseConv2D?\r\n\r\nThe version I used is tensorflow 2.3.1", "@cookingbear What platform are u running with that ?\r\nAre u running using gpu/cuda, or pure CPU. ARM  or x86 platform ?", "I tested it on x86 using cpu only", "@cookingbear Is this still an issue for you? Can you please test with recent `TF2.6` and `tf-nightly` and let us know whether the issue persists. [Here](https://colab.research.google.com/gist/jvishnuvardhan/1844f21420de68d53ea92d8b0ebb4a17/untitled233.ipynb) is a gist for reference. Thanks!", "> @cookingbear Is this still an issue for you? Can you please test with recent `TF2.6` and `tf-nightly` and let us know whether the issue persists. [Here](https://colab.research.google.com/gist/jvishnuvardhan/1844f21420de68d53ea92d8b0ebb4a17/untitled233.ipynb) is a gist for reference. Thanks!\n\nthanks for your attention. I would test again with the newest version.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46217\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46217\">No</a>\n"]}, {"number": 46216, "title": "Extract reference for operator LEAKY_RELU to standalone header", "body": "Move the reference implementation to its own header so that micro\r\ncan use it without the unrelated depedencies of reference_ops.h.\r\n\r\nPR step 2 for issue #46161", "comments": ["@ddavis-2015  Can you please resolve conflicts? Thanks!"]}, {"number": 46215, "title": "Extract a function for parsing operator LEAKY_RELU", "body": "Extract the parsing out of a switch statement case to create a\r\nstandalone function which can be called by the micro op resolver.\r\n\r\nPR step 1 for issue #46161", "comments": []}, {"number": 46214, "title": "micro: port operator LEAKY_RELU kernel from lite with test", "body": "Complete implementation of TFLM operator LEAKY_RELU and associated TFLM test code.\r\n\r\nPR step 5 of the work to port operator LEAKY_RELU as tracked in Issue #46161", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@ddavis-2015  This PR is in draft, any update on this? Please. Thanks!", "@ddavis-2015  Can you please check @advaitjain's comments and resolve conflicts?. Thanks!"]}, {"number": 46213, "title": "micro: prepare to port operator LEAKY_RELU kernel from lite with test", "body": "Implement skeleton (non-working) code for operator and test.\r\nHeader files changed.\r\nNamespaces changed.\r\nSome original code deleted.\r\nSome original code modified.\r\n\r\nPR step 4 of the work to port operator LEAKY_RELU as tracked in Issue #46161", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46212, "title": "micro: copy operator LEAKY_RELU kernel from lite", "body": "This is a copy with minimal modification of the kernel and test for\r\noperator LEAKY_RELU from tensorflow/lite/kernels.\r\nAdaptations to micro and addition to the micro build to follow.\r\n\r\nPR step 3 for issue #46161", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", ":frowning_face: Sorry, but only Googlers may change the label `cla: yes`."]}, {"number": 46211, "title": "Many errors in the Example of tf.feature_column.categorical_column_with_vocabulary_file", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file\r\n\r\n## Description of issue (what needs changing):\r\nIn the sentence, \r\n\r\n> Use either (but not both) of num_oov_buckets and default_value to specify how to include out-of-vocabulary values.\r\n\r\nsince **either** is used, **or** should be used instead of **and**.\r\n\r\nWhen running the **`Example Code`**, it is resulting in the errors mentioned below:\r\n\r\n1. \r\n\r\n> NameError: name 'categorical_column_with_vocabulary_file' is not defined\r\n\r\n\r\n2. \r\n\r\n> NameError: name 'linear_model' is not defined\r\n\r\n3. \r\n\r\n> ValueError: All feature_columns must be FeatureColumn instances. Given: Ellipsis\r\n\r\n4. \r\n\r\n> NameError: name 'input_layer' is not defined\r\n\r\nThe code in the documentation should be modified to fix all the above errors. \r\n\r\nPlease find the [Github Gist](https://colab.research.google.com/gist/rmothukuru/74b353ebc7046dee6220cb158f25e5bc/categorical_column_with_vocabulary_file_error.ipynb) demonstrating the errors.", "comments": ["I would like to contribute. Could somebody give me some pointers to begin. It will be of immense help. Thank you.\r\n\r\nBelow is my understanding of the issue:\r\n\r\n0. The sentence needs to be updated. This is quiet straight forward.\r\n1. This could be due to the fact that the code has a missing import for `categorical_column_with_vocabulary_file`\r\n2, 3 and 4: I think in this particular example, the writer, tried to provide a pseudo code.\r\n\r\nSo if I understand correctly, the issue, would like to have a running code in place of pseudo code.\r\n\r\nAny pointers will be of immense help."]}, {"number": 46210, "title": "Control dependency doesn't work in distribute MirroredStrategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 20.04 LTS` with `nvcr.io/nvidia/tensorflow:19.12-tf1-py3`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): none\r\n- TensorFlow version (use command below): `unknown 1.15.0`\r\n- Python version: `3.6.9`\r\n- Bazel version (if compiling from source): none\r\n- GCC/Compiler version (if compiling from source): none\r\n- CUDA/cuDNN version: CUDA 10.2.89, cuDNN 7.6.5\r\n- GPU model and memory: 2x GeForce GTX 1080 Ti with 11178 MiB fb memory\r\n```\r\n        GPU0    GPU1    CPU Affinity\r\nGPU0     X      PHB     0-11\r\nGPU1    PHB      X      0-11\r\n```\r\n\r\n**Describe the current behavior**\r\nIt seems that `tf.control_dependencies` is useless in the `load_fn`, which is called by a `MirroredStrategy`.\r\nThe wall times of `load` and `ret` differ greatly with each other.\r\n\r\n**Describe the expected behavior**\r\nThe wall times of `load` and `ret` is comparable.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\n\r\nSHAPE = [2048, 2048]\r\nSTEP = 100\r\nWARMUP = 100\r\n\r\n_LOAD_OPS = list()\r\n\r\n\r\ndef load_fn():\r\n    v = tf.get_variable('v', shape=SHAPE, initializer=tf.ones_initializer)\r\n    m = tf.matmul(v, v)\r\n    _LOAD_OPS.append(m.op)\r\n    with tf.control_dependencies([m.op]):\r\n        return tf.constant(1.0)\r\n\r\n\r\ndef bench(name, session, ops):\r\n      stime = time.time()\r\n      for _ in range(STEP):\r\n          session.run(ops)\r\n      etime = time.time() - stime\r\n      print('{:6s} takes {:.3e} sec.'.format(name, etime))\r\n    \r\n\r\ndef main():\r\n    strategy = tf.distribute.MirroredStrategy()\r\n    with strategy.scope():\r\n        r = strategy.experimental_run_v2(load_fn)\r\n        r = strategy.reduce(tf.distribute.ReduceOp.SUM, r)\r\n\r\n    with tf.train.MonitoredSession() as mon_sess:\r\n        for _ in range(WARMUP):\r\n            mon_sess.run([_LOAD_OPS, r, []])\r\n\r\n        bench('load', mon_sess, _LOAD_OPS)\r\n        bench('ret',  mon_sess, r)\r\n        bench('null', mon_sess, [])\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n**Other info / logs**\r\nThe non-trivial output is\r\n```\r\nload   takes 8.350e-01 sec.\r\nret    takes 3.153e-02 sec.\r\nnull   takes 4.847e-03 sec.\r\n```", "comments": ["@whhu,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same issue. Thanks!", "> @whhu,\r\n> TensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same issue. Thanks!\r\n\r\nThe problem remains. The output,\r\n```bash\r\nload   takes 7.845e-01 sec.\r\nret    takes 2.101e-02 sec.\r\nnull   takes 3.858e-03 sec.\r\n```\r\nis obtained in the latest `tensorflow/tensorflow:2.4.0-gpu` with `Python 3.6.9` and `v2.4.0-rc4-71-g582c8d236cb 2.4.0`.\r\nThe `import` part of the code is updated to\r\n```\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n```\r\nand the `strategy.experimental_run_v2` is updated to `strategy.experimental_run`.\r\n", "Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. The output is as follows\r\n\r\n- TF v2.3\r\n```\r\nload   takes 3.725e+00 sec.\r\nret    takes 2.209e-02 sec.\r\nnull   takes 3.345e-03 sec.\r\n```\r\n- TF v2.4\r\n```\r\nload   takes 1.684e-01 sec.\r\nret    takes 4.121e-02 sec.\r\nnull   takes 4.050e-03 sec.\r\n```\r\n- TF-nightly i.e. v2.5.0-dev20210111\r\n```\r\nload   takes 1.672e-01 sec.\r\nret    takes 4.292e-02 sec.\r\nnull   takes 4.081e-03 sec.\r\n```\r\n\r\nThanks!", "Hi @whhu, in general you should not use distribution strategies with legacy TF1 graph mode, as this is not supported. You'll need to rewrite your code for TF2 such that you do not need `tf.disable_v2_behavior()` for the code to run."]}, {"number": 46209, "title": "Tensorflow-gpu in RTX3070", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WIndows 10 64bit\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n- TensorFlow version: tensorflow-gpu 2.4.0\r\n- Python version: Python 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: CUDA11, cuDNN 8.0.5\r\n- GPU model and memory: RTX3070 8GB OC\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen I run the training using GPU, it raise me an error saying that my cuDNN is not initialize properly. I've no idea on how to solve it. I did tried with CUDA 11 with cuDNN 8.0.4 and 8.0.3 but keep raising the same problem.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`tf.config.list_physical_devices('GPU')` --> \"Physical Device GPU 0\"\r\n`tf.test.is_built_with_cuda()` --> True\r\n\r\n\r\n**Log**\r\n![Capture](https://user-images.githubusercontent.com/52826239/103755378-b1747980-5048-11eb-8bc0-f5dfc10d69c4.PNG)\r\n\r\n", "comments": ["@hansheng0512 \r\nPlease paste the error message (using makrdown formatting around it) instead of screenshotting. Screenshots are not searchable so they don't help in looking for the issue and also don't help other people having the same error from finding about the issue.", "> @hansheng0512\r\n> Please paste the error message (using makrdown formatting around it) instead of screenshotting. Screenshots are not searchable so they don't help in looking for the issue and also don't help other people having the same error from finding about the issue.\r\n\r\nNoted.\r\n\r\nThe log shown as below:\r\n```\r\n2021-01-06 18:21:30.484673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:09:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.755GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-01-06 18:21:30.485159: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-06 18:21:30.485494: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-06 18:21:30.486402: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-06 18:21:30.486726: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-06 18:21:30.487637: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-06 18:21:30.487971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-06 18:21:30.488935: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-06 18:21:30.489355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-06 18:21:30.489743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-06 18:21:36.156145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-06 18:21:36.156275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2021-01-06 18:21:36.157045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2021-01-06 18:21:36.157466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6589 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:09:00.0, compute capability: 8.6)\r\n2021-01-06 18:21:36.158785: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-06 18:21:37.935914: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-01-06 18:21:38.345639: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-06 18:21:44.113075: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-06 18:21:44.113322: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-06 18:21:44.114653: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-06 18:21:44.116732: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-06 18:21:44.117351: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-06 18:21:44.118153: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-06 18:21:44.683728: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-06 18:21:48.700894: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2021-01-06 18:21:48.701422: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2021-01-06 18:21:48.701512: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops_fused_impl.h:697 : Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n```\r\n\r\n", "@hansheng0512 \r\nPlease refer to this [issue](https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-727666363) and let us know.\r\n\r\nSimilar issues: #45779, #43761", "> @hansheng0512\r\n> Please refer to this [issue](https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-727666363) and let us know.\r\n> \r\n> Similar issues: #45779, #43761\r\n\r\nI tried with:\r\n1. `pip uninstall tensorflow-gpu` as my pc just installed tensorflow-gpu\r\n2. `pip cache purge`\r\n3. Deleted tensorflow folders in .../python38/Lib/Site-packages\r\n4. `pip install tensorflow-gpu --no-cache-dir`\r\n\r\nBut it raise me same error.", "Add this to the start of your code\r\nphysical_devices = tf.config.list_physical_devices('GPU') \r\ntf.config.experimental.set_memory_growth(physical_devices[0], True). ", "> Add this to the start of your code\r\n> physical_devices = tf.config.list_physical_devices('GPU')\r\n> tf.config.experimental.set_memory_growth(physical_devices[0], True).\r\n\r\nIt works! Thanks man!\r\nDo you mind to explain me what's going on?\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46209\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46209\">No</a>\n"]}, {"number": 46208, "title": "micro: port operator ADD_N kernel from lite with test", "body": "Complete implementation of TFLM operator ADD_N and associated TFLM test code.\r\n\r\nPR step 5 of the work to port operator ADD_N as tracked in Issue  #46162", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@ddavis-2015  This PR is in draft, any update on this? Please. Thanks!", "@ddavis-2015 This PR is in draft, any update on this? Please. Thanks!"]}, {"number": 46207, "title": "micro: prepare to port operator ADD_N kernel from lite with test", "body": "Implement skeleton (non-working) code for operator and test.\r\nHeader files changed.\r\nNamespaces changed.\r\nSome original code deleted.\r\nSome original code modified.\r\n\r\nPR step 4 of the work to port operator ADD_N as tracked in Issue #46162", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@ddavis-2015  This PR is in draft, any update on this? Please. Thanks!"]}, {"number": 46206, "title": "micro: copy operator ADD_N kernel from lite", "body": "This is a copy with minimal modification of the kernel and test for\r\noperator ADD_N from tensorflow/lite/kernels.\r\nAdaptations to micro and addition to the micro build to follow.\r\n\r\nPR step 3 for issue #46162", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@ddavis-2015  Can you please resolve conflicts? Thanks!"]}, {"number": 46204, "title": "Update custom_gradient.py", "body": "fix typo in doc string", "comments": []}, {"number": 46203, "title": "Many errors in the Example of tf.feature_column.categorical_column_with_identity", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_identity#linear_model\r\n\r\n## Description of issue (what needs changing):\r\nWhen running the **`Example Code`**, it is resulting in the errors mentioned below:\r\n\r\n1. \r\n\r\n> NameError: name 'categorical_column_with_identity' is not defined\r\n\r\n\r\n2. \r\n\r\n> NameError: name 'linear_model' is not defined\r\n\r\n3. \r\n\r\n> ValueError: All feature_columns must be FeatureColumn instances. Given: Ellipsis\r\n\r\n4. \r\n\r\n> NameError: name 'input_layer' is not defined\r\n\r\nThe code in the documentation should be modified to fix all the above errors. \r\n\r\nPlease find the [Github Gist](https://colab.research.google.com/gist/rmothukuru/a05819cc660a7b677e53b189affbe4d0/categorical_column_with_identity_error.ipynb) demonstrating the errors.", "comments": []}, {"number": 46202, "title": "Extract a function for parsing operator ADD_N", "body": "Extract the parsing out of a switch statement case to create a\r\nstandalone function which can be called by the micro op resolver.\r\n\r\nPR step 1 for issue #46162", "comments": []}, {"number": 46201, "title": "Many errors in the Example of tf.feature_column.categorical_column_with_hash_bucket", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket\r\n\r\n## Description of issue (what needs changing):\r\nWhen running the **`Example Code`**, it is resulting in the errors mentioned below:\r\n\r\n1. \r\n\r\n> File \"<ipython-input-1-e0419158b389>\", line 3\r\n>     keywords = categorical_column_with_hash_bucket(\"keywords\", 10K)\r\n>                                                                  ^\r\n> SyntaxError: invalid syntax\r\n\r\n2. \r\n\r\n> NameError: name 'categorical_column_with_hash_bucket' is not defined\r\n\r\n3. \r\n\r\n> NameError: name 'linear_model' is not defined\r\n\r\n4. \r\n\r\n> NameError: name 'input_layer' is not defined\r\n\r\n5. \r\n\r\n> ValueError: All feature_columns must be FeatureColumn instances. Given: Ellipsis\r\n\r\nThe code in the documentation should be modified to fix all the above errors. \r\n\r\nPlease find the [Github Gist](https://colab.research.google.com/gist/rmothukuru/eba74b40ed40964c730e4e667ed0c8f1/categorical_column_with_hash_bucket_error.ipynb) demonstrating the errors.", "comments": ["Hy, @rakeshmothukuru1  I found this as a duplicate issue of https://github.com/tensorflow/tensorflow/issues/46203 . Would you please close one of the issue.\r\nThanks.", "@abhinavsp0730,\r\nNo, the code present in this issue and #46203 are different corresponding to different APIs. "]}, {"number": 46200, "title": "Extract reference for operator ADD_N to standalone header", "body": "Move the reference implementation to its own header so that micro\r\ncan use it without the unrelated depedencies of reference_ops.h.\r\n\r\nPR step 2 for issue #46162", "comments": []}, {"number": 46199, "title": "Failed invoke tflite model in swift code(Provided data count 18181 must match the required count 4536.)", "body": "**System information**\r\n- OS Platform): macOS 11.1\r\n- TensorFlow version: 2.4.0\r\n\r\nInput shape of my tflite model - (18, 63) or 1134 float numbers.\r\n\r\nI get the data itself in objective-c code, and then I send it to swift code and an error already occurs there. In detail I do the following\r\n\r\n1. Receive the data in NSMutableArray. The length of the array is 1134 NSNumber\r\n2. Converting NSMutableArray to NSData\r\n```\r\nNSData *d = [NSKeyedArchiver archivedDataWithRootObject:_data];\r\nNSLog(@\"output: %@\", d);\r\n// output: {length = 18181, bytes = 0x62706c69 73743030 d4000100 02000300 ... 00000000 000034fd }\r\n```\r\n\r\n3. I send data to the swift code.\r\n```[_model predict:d];```\r\nSwift code:\r\n```\r\n@objc public func predict(_ data: NSData) {\r\n        guard\r\n          let modelPath = Bundle.main.path(forResource: \"model\", ofType: \"tflite\")\r\n        else {\r\n            return\r\n        }\r\n\r\n        do {\r\n          let interpreter = try Interpreter(modelPath: modelPath)\r\n          try interpreter.allocateTensors()\r\n          let inputData: Data = data as Data\r\n\r\n          try interpreter.copy(inputData, toInputAt: 0) // <-- an error occurs in this line\r\n\r\n          try interpreter.invoke()\r\n          let outputTensor = try interpreter.output(at: 0)\r\n        } catch {\r\n          print(error)\r\n        }\r\n}\r\n```\r\n4. I get an error in the above line of code. Error:\r\n```Provided data count 18181 must match the required count 4536.```", "comments": ["My model(here I did with the input size (1134,)):\r\n```\r\nmodel = Sequential()\r\n\r\nmodel.add(keras.layers.InputLayer(input_shape=(1134,), name=\"input_1\"))\r\nmodel.add(Dense(1024, activation='relu'))\r\nmodel.add(Dense(512, activation='relu'))\r\nmodel.add(keras.layers.Dropout(0.35))\r\nmodel.add(Dense(3, activation='softmax'))\r\n\r\nmodel.compile(loss='sparse_categorical_crossentropy',\r\n              optimizer='adam', metrics=['accuracy'])\r\n```\r\nI save the model like this:\r\n```\r\nimport tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\nthe error is the same", "Not an ObjC expert myself, but I think you are converting the `NSMutableArray` of `NSNumber`s in an incorrect way.\r\n\r\n1. `NSNumber` is an object, which doesn't necessarily have the same in-memory representation as a float number. You should get the float representation by retrieving the `floatValue` property on an `NSNumber` object.\r\n2. `archivedDataWithRootObject` is an encoding method according to the [documentation](https://developer.apple.com/documentation/foundation/nskeyedarchiver/1413189-archiveddatawithrootobject), which complicates the issue even more.\r\n\r\nBased on these, I think you should manually convert each element to float in a loop, something like the following (haven't tested this code myself).\r\n\r\n```objective-c\r\nint count = [_data count];\r\nint num_bytes = sizeof(float) * count;\r\nfloat *arr = (float*)malloc(num_bytes);\r\nfor (int i = 0; i < count; ++i) {\r\n  arr[i] = [_data[i] floatValue];\r\n}\r\nNSData *d = [NSData dataWithBytesNoCopy:arr length:num_bytes freeWhenDone:YES];\r\n// pass this NSData object to Swift side...\r\n```\r\n\r\nAgain, I'm not an ObjC expert, so there might be a more efficient way to convert the data.", "Thank you, so the error was really in the wrong conversion. I also wanted to ask you do know how to print prediction?\r\n I'm trying to ```print(outputTensor)``` But I get:\r\n```\r\nTensor(name: \"Identity\", dataType: TensorFlowLite.Tensor.DataType.float32, shape: TensorFlowLite.Tensor.Shape(rank: 2, dimensions: [1, 3]), data: 12 bytes, quantizationParameters: nil)\r\n```\r\n\r\n", "Figured out the problem. Thank you again", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46199\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46199\">No</a>\n", "> Figured out the problem. Thank you again\r\nHello, I also encountered this problem. I want to change the width and height of the output image to 500. How can I solve it?\r\n"]}, {"number": 46198, "title": "Move CMSIS downloads to a separate script.", "body": "This is a first step towards adding back some of the CMSIS patching needed to fix the Arduino build (http://b/175435756).\r\n\r\nMoving to a stand-alone download script is part of the cleanup described in http://b/143904317\r\n\r\nAlso, this change introduces a common location for helper functions and fixes #46020\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "> Looks good. Just curious about the broken Arduino build (didn't know it was broken)? I am not able to access the link (http://b/175435756).\r\n\r\nTurns out that that was something specific to Pete's setup: https://github.com/arduino/Arduino/issues/11141\r\n\r\nI'll still plan on merging this change since we do want to decentralize the download logic.", "> On second thought, could you also update tensorflow/lite/micro/tools/make/ext_libs/ethosu.inc?\r\n\r\ndone."]}, {"number": 46197, "title": "Error in example because of Incomplete API names for tf.keras.experimental.SequenceFeatures", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/experimental/SequenceFeatures#example\r\n\r\n## Description of issue (what needs changing): \r\nThe APIs, **`sequence_numeric_column, sequence_categorical_column_with_identity, embedding_column`**, etc.. are incomplete. Consequently, it is resulting in the error, \r\n\r\n> NameError: name 'sequence_numeric_column' is not defined\r\n\r\nIt should be **`tf.feature_column.sequence_numeric_column, tf.feature_column.sequence_categorical_column_with_identity, tf.feature_column.embedding_column`**, instead.\r\n\r\nPlease find the [Github Gist](https://colab.research.google.com/gist/rmothukuru/90002755d1b45f6f94709a9623259d9f/sequencefeatures_error.ipynb).\r\n\r\nThere is an error in the above [Gist](https://colab.research.google.com/gist/rmothukuru/90002755d1b45f6f94709a9623259d9f/sequencefeatures_error.ipynb) because of **`Ellipsis`** and it is being tracked in #46128.", "comments": ["What do you mean by incomplete API names ? Do you mean package name ? You simply dont tag without knowing what is the real error.", "@summa-code,\r\nBy incomplete API names, I mean, using **`sequence_numeric_column`** instead of **`tf.feature_column.sequence_numeric_column`**.\r\n\r\nI didn't understand your point,\r\n\r\n> You simply dont tag without knowing what is the real error.\r\n\r\nI have provided a detailed [Colab Gist](https://colab.research.google.com/gist/rmothukuru/90002755d1b45f6f94709a9623259d9f/sequencefeatures_error.ipynb) demonstrating the errors.", "Oh my god, do you even know how python works ? Those are package names. They dont have to necessarily add if you import that. There is nothing wrong in API names. Don't waste our time unnecessarily while we are trying to solve actual bug.", "@summa-code,\r\n\r\n> Oh my god, do you even know how python works ?\r\n\r\nI know how python works but my point is that the Example code in the Documentation should be Executable without many changes from our side. Such a code will be helpful, especially to the Newbies.\r\n\r\nFor example, [this code](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column#example) uses the complete API name, **`tf.feature_column.numeric_column`**.\r\n\r\nSo, Please don't overreact. \r\n\r\n> Don't waste our time unnecessarily while we are trying to solve actual bug.\r\n\r\nFixing issues in the documentation is as important as fixing issues in the Code because clearer the documentation is, easier it will be for the Developer Community.\r\n\r\nIf you feel it's a waste of time, who is forcing you to respond?\r\n "]}, {"number": 46196, "title": "Add `.bazelrc` to `.gitignore`", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, we use `configure.py` for modifying `.bazelrc`, I think we should add `.bazelrc` to `.gitignore` and use `configure.py` for completely generating `bazelrc` from scratch. Afterward, we could add our own config to `.bazelrc` without any conflict with the upstream ( I usually add `build:asan ----copt=-fsanitize=address --linkopt=-fsanitize=address` ). Furthermore, we could generate a platform-specific `.bazelrc` using `python` ( which is good, I think ).\r\n\r\n**Will this change the current api? How?** Nothing\r\n\r\n**Who will benefit with this feature?** TensorFlow developers.\r\n\r\n/cc: @mihaimaruseac \r\n\r\n", "comments": ["I just read `.bazelrc` again and see that we could add custom config by `.bazelrc.user`. Close", "We are also moving away from running configure.py, especially in automated setups. This entails having `.bazelrc` and similar being synced with the repo."]}]