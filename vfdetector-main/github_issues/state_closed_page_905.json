[{"number": 26318, "title": "[FR] A better way instead of  `while True: sess.run(..)`", "body": "I'd like to make tensorflow as the inference backend, and there are quantities of inference inputs to handle.\r\n\r\nCurrently, tensorflow manages multiple inference inputs as different execution triggers of `sess.run`, e.g:\r\n\r\n```python\r\nwhile True:\r\n    sess.run(infer_op)\r\n```\r\n\r\nHowever, the throughput is very bad because every inference task will run through python runtime and also tensorflow scheduler, so I require something like this **directly**:\r\n\r\n```python\r\n    sess.run(tf.loop_forever(infer_op))\r\n```\r\n\r\nYou can assume the infer_op read inputs from an one_shot_iterator, and I never want this loop to stop just like it is a long-run server.\r\n\r\nAnyway to do this at the moment, or the possibility to support this op like `tf.loop_forever` ?\r\n\r\nYou can also share a simple hack in tensorflow source code to make the `sess.run` (in C++ session) to repeat same computing again without returning data to python session. Thanks!", "comments": ["Using `tf.while_loop(0 < 1, ..)` might be a little ugly to understand and also not the most efficient to execute (**both unnecessary kernels and unnecessary stream synchronizations will be triggered just for condition checking which should be always true**).\r\n\r\nSo strongly suggest creating an OP named `tf.loop_forever()` not to wait and check the condition very time, and this is very necessary for GPU-based operations because we don't want to launch unnecessary kernels and data synchronization, which highly increases the latency of body execution.", "I think tensorflow doesn't have a break statement. So implementing an operation for which the only way of stopping it is by killing the process seems to go against a lot of principles. \r\n\r\nI don't think the overhead of checking a simple condition like `tf.while_loop(True, ..)` is going to be noticeable.", "@Gonzalo933 An ugly problem for `tf.while_loop(True, body, ..)` is there are output shape restrictions among `cond`, `body` and `loop_vars`, and I have to output something from `body` for checking even if it is not needed, thus it doesn't support many existing ops while a ideal `tf.loop_forever` should be expected to support.\r\n\r\nJust a example, `tf.group` it can be the final wrapper of train_op to get away with I/O sychronization from GPU to CPU after the graph terminates its one-shot execution, but `tf.while_loop` doesn't work for this case:\r\n\r\n```python\r\ntrain_op = tf.matmul([[4]], [[5]])\r\ngraph = tf.group(train_op)\r\n\r\nsess.run(graph)  # OK\r\n\r\nloop = tf.while_loop(lambda i: True, lambda i: graph, [])\r\nsess.run(loop)  # Fail for output type\r\n```\r\n\r\nSo how to make `tf.while_loop` work in a way like `tf.loop_forever(any_op)`?", "> \r\n> \r\n> I think tensorflow doesn't have a break statement. So implementing an operation for which the only way of stopping it is by killing the process seems to go against a lot of principles.\r\n> \r\n> I don't think the overhead of checking a simple condition like `tf.while_loop(True, ..)` is going to be noticeable.\r\n\r\nEverything is noticeable in TF. It shouldn't be, but it is. There are unintended consequences for everything.", "There are 2 problems using `tf.while_loop` than ideal `tf.loop_forever`:\r\n1) **Performance**: `while_loop` has to wait for a body answer before it can decide whether to schedule another body execution, so this is apparently in a synchronous style which is much inefficient to feed tasks to GPU, so the latency between adjacent body execution is high;\r\n2) **Not for all TF_Ops**: it can not fully support all operations so it should not be a good `loop_forever` alternative solution;\r\n\r\nI think either enhancing `while_loop` or extending `loop_forever` is needed, otherwise, TF engine would be only good in terms of offline batch processing, not efficient for lots of tasks with **low-latency** requirement.", "break statement is not needed because the `sess.run` could be break or interrupted by any long-run operators that can return non-OK status, so the task could be terminated by a surrounding `try: .. expect: ..`.", "Another advantage for support `loop_forever` is that it could fully replace the Tensorflow C++ programming which is very uncommon to most users (for example, complex tool chain is needed, fat source code is needed, building takes too much time, ..).\r\n\r\nTensorflow can development a series of stateful operators not pass through python runtime, which could enable most long run services efficiently using python-wrapped Tensorflow engine.\r\n\r\nIf `tf.loop_forever` breaks some graph principles, this option should be set at `sess.run(.., run_forever=True)` which could not be enabled by default, but should be enabled for long-run services.\r\n\r\nSo any agreement or supporting plans?", "Closing this issue as per [mrry's comment in the PR 26561](https://github.com/tensorflow/tensorflow/pull/26561#issuecomment-471751819). Thanks!"]}, {"number": 26317, "title": "Fixes issue #26276 - Update CONTRIBUTING.md", "body": "Fixes issue #26276", "comments": ["@dynamicwebpaige Can you please review this \ud83d\ude42 ", "Nagging Reviewer @dynamicwebpaige, @yifeif: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 26316, "title": "TF crashes when tensor size is increased", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): not sure \r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: release 9.0, V9.0.176/ 9.0\r\n- GPU model and memory: GeForce RTX 2080 Ti 11GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI am running a custom code using TF. Part of the code includes a 2d convolution using tf.nn.conv2d. The code is running great, but when I increase the size of one of the tensors by a factor of 2 then everything crashes and I get this error:\r\n2019\udae1\udea7\udae1\udea8 12:29:56.729188: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2019\udae1\udea7\udae1\udea8 12:29:56.988944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65\r\npciBusID: 0000:65:00.0\r\ntotalMemory: 11.00GiB freeMemory: 8.99GiB\r\n2019\udae1\udea7\udae1\udea8 12:29:56.995629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019\udae1\udea7\udae1\udea8 12:29:57.467089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019\udae1\udea7\udae1\udea8 12:29:57.467875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0 \r\n2019\udae1\udea7\udae1\udea8 12:29:57.468098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N \r\n2019\udae1\udea7\udae1\udea8 12:29:57.468449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8665 MB memory) \u2011> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5)\r\n2019\udae1\udea7\udae1\udea8 12:30:06.043926: E tensorflow/stream_executor/cuda/cuda_driver.cc:981] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019\udae1\udea7\udae1\udea8 12:30:06.044411: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 000001F8E589DCD0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019\udae1\udea7\udae1\udea8 12:30:06.044888: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 000001F8E589DCD0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019\udae1\udea7\udae1\udea8 12:30:06.045401: F tensorflow/stream_executor/cuda/cuda_dnn.cc:231] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.\r\n\r\nThe problem might not be connected to the convolution operation but it fails after I increase the size of this tensor. \r\nCan you help me solving this issue?\r\nThank you very much,\r\nGilad\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@lermag May be there is an issue with selection of some parameters. Could you provide a code to reproduce the bug? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Hello, do you solve it? I met the same error using RTX2070, thank you", "Same problem and could you post the solution to fix it? Thank you!!", "@KimMeen @wumeng2 Can you create a new issue with a standalone code and also fill issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A?"]}, {"number": 26314, "title": "output 'beam_search_ops_gpu.cu.pic.o' was not created", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no\r\n- TensorFlow installed from (source or binary): git clone https://github.com/tensorflow/tensorflow -b r1.8\r\n- TensorFlow version: r1.8\r\n- Python version:3.6.8\r\n- Installed using virtualenv? pip? conda?: conda \r\n- Bazel version (if compiling from source): 0.14.0\r\n- GCC/Compiler version (if compiling from source):Xcode 8.2\r\n- CUDA/cuDNN version: 9.2/ 7.2\r\n- GPU model and memory: 1060 6G\r\n\r\n\r\n\r\n**Describe the problem**\r\n[1,081 / 1,095] 4 actions running\r\n    Compiling tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.cc [for host]; 7s local\r\n    Compiling tensorflow/contrib/tensor_forest/kernels/v4/decision-tree-resourERROR: /Users/aiamjay/PycharmProjects/tensorflow/tensorflow/contrib/seq2seq/BUILD:64:1: output 'tensorflow/contrib/seq2seq/_objs/python/ops/_beam_search_ops_gpu/tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.pic.o' was not created\r\n[1,081 / 1,095] 4 actions running\r\n    Compiling tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.cc [for host]; 7s local\r\n    Compiling tensorflow/contrib/tensor_forest/kernels/v4/decision-tree-resourERROR: /Users/aiamjay/PycharmProjects/tensorflow/tensorflow/contrib/seq2seq/BUILD:64:1: not all outputs were created or valid\r\n[1,081 / 1,095] 4 actions running\r\n    Compiling tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.cc [for host]; 7s local\r\n    Compiling tensorflow/contrib/tensor_forest/kernels/v4/decision-tree-resourTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 287.490s, Critical Path: 21.75s\r\nINFO: 1046 processes, local.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I still cannot compile successfully, so please offer me some help.", "Can you please switch to latest version of TF and build again?\r\nTF 1.13 comes with pre-built cuda 10.0 binaries. Thus you have to upgrade your cuda version.", "ok, I will try. Thank you! \n\n\n> \u5728 2019\u5e743\u67089\u65e5\uff0c\u4e0a\u53487:39\uff0cymodak <notifications@github.com> \u5199\u9053\uff1a\n> \n> Can you please switch to latest version of TF and build again?\n> TF 1.13 comes with pre-built cuda 10.0 binaries. Thus you have to upgrade your cuda version.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "Closing this issue for now. Please reopen if still facing problems with the latest version of TF. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26314\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26314\">No</a>\n"]}, {"number": 26313, "title": "All mirrors are down: [GET returned 404 Not Found]", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOS 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): git clone https://github.com/tensorflow/tensorflow -b r1.8\r\n- TensorFlow version: r1.8\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: conda python=3.6.8\r\n- Bazel version (if compiling from source): 0.14.0\r\n- GCC/Compiler version (if compiling from source):Xcode 8.2    \r\n- CUDA/cuDNN version:cud 9.2 / cudnn 7.2.1\r\n- GPU model and memory:   1066 6G\r\n\r\n\r\n\r\n**Describe the problem**\r\n/Users/aiamjay/PycharmProjects/tensorflow/tensorflow/tools/pip_package/BUILD:166:1: error loading package 'tensorflow': \r\nEncountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': \r\njava.io.IOException: Error downloading [https://mirror.bazel.build/github.com/dtrebbien/protobuf/archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz, \r\nhttps://github.com/dtrebbien/protobuf/archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz] \r\nto /private/var/tmp/_bazel_aiamjay/e76cd148deca576702165034f4a7f23b/external/protobuf_archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz: \r\nAll mirrors are down: [GET returned 404 Not Found] and referenced by '//tensorflow/tools/pip_package:build_pip_package'\r\n\r\n\r\n\r\nERROR: /Users/aiamjay/PycharmProjects/tensorflow/tensorflow/tools/pip_package/BUILD:166:1: error loading package 'tensorflow': \r\nEncountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': \r\njava.io.IOException: Error downloading [https://mirror.bazel.build/github.com/dtrebbien/protobuf/archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz, \r\nhttps://github.com/dtrebbien/protobuf/archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz] \r\nto /private/var/tmp/_bazel_aiamjay/e76cd148deca576702165034f4a7f23b/external/protobuf_archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz: \r\nAll mirrors are down: [GET returned 404 Not Found] and referenced by '//tensorflow/tools/pip_package:build_pip_package'\r\n\r\n\r\n\r\n\r\nAnalysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: error loading package 'tensorflow': \r\nEncountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': \r\njava.io.IOException: \r\nError downloading [https://mirror.bazel.build/github.com/dtrebbien/protobuf/archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz, \r\nhttps://github.com/dtrebbien/protobuf/archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz] \r\nto /private/var/tmp/_bazel_aiamjay/e76cd148deca576702165034f4a7f23b/external/protobuf_archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz: \r\nAll mirrors are down: [GET returned 404 Not Found]\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel clean\r\nbazel build --config=opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --action_env PATH --action_env DYLD_LIBRARY_PATH //tensorflow/tools/pip_package:build_pip_package**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Actually I want to install tensorflow-gpu with version higher than 1.2 for my GPU, but I cannot do that by running conda or pip, the only way is to build one for myself, so please help me.", "Sorry, it seems that I applied a unnecessary patch to the repository, after I checkout all the changes, I started to compile."]}, {"number": 26312, "title": "clean up backticks in docs for tf.norm", "body": "clean up backticks in docs for tf.norm", "comments": []}, {"number": 26311, "title": "tf.function creates a new concrete function multiple times for the same inputs", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOSX 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below <= needs to be updated to TF2):\r\ntf.version.VERSION: '2.0.0-dev20190303'\r\ntf.version.GIT_VERSION: 'v1.12.0-9460-gbfa3fcead3'\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nWhen calling a TF function multiple times with the same arguments, the function keeps getting traced and more and more new concrete functions get generated. This will hinder performance and waste RAM.\r\n\r\n**Describe the expected behavior**\r\nI expect a function to be trace just once for a given set of inputs.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef f(x):\r\n    print(\"  Tracing with shape\", x.shape)\r\n    return x ** 3\r\n\r\nfor round in range(5):\r\n    print(\"Round\", round)\r\n    f(tf.constant(3.))\r\n    f(tf.constant(4.))\r\n    f(tf.constant([[1., 2.]]))\r\n    f(tf.constant([[]]))\r\n    f(tf.constant([[3., 4.], [5., 6.]]))\r\n    f(tf.constant([[7., 8.], [9., 10.], [11., 12.]]))        \r\n    print()\r\n```\r\n\r\nWhen you run this, you see that `f()` gets traced at every round, but it should only be traced during round 0.\r\n\r\nAlso, not sure if this is related or if I should file another issue, but I noticed this:\r\n\r\n```python\r\ncf1 = f.get_concrete_function(tf.TensorSpec(shape=[], dtype=tf.float32))\r\ncf2 = f.get_concrete_function(tf.constant(0.))\r\nassert cf1 is cf2  # AssertionError\r\n```\r\n\r\n**Other info / logs**\r\nHere is the output:\r\n\r\n```\r\nRound 0\r\n  Tracing with shape ()\r\n  Tracing with shape (1, 2)\r\n  Tracing with shape (1, None)\r\n  Tracing with shape (None, 2)\r\n\r\nRound 1\r\n  Tracing with shape (1, None)\r\n  Tracing with shape (None, 2)\r\n\r\nRound 2\r\n  Tracing with shape (1, None)\r\n  Tracing with shape (None, 2)\r\n\r\nRound 3\r\n  Tracing with shape (1, None)\r\n  Tracing with shape (None, 2)\r\n\r\nRound 4\r\n  Tracing with shape (1, None)\r\n  Tracing with shape (None, 2)\r\n```", "comments": ["Thank you for the report! There should be a fix landing any minute.\r\n\r\nThe `get_concrete_function` issue is different; feel free to file another issue if it's causing an issue (I think that's much less serious). Basically we treat TensorSpecs and opaque Python objects for caching purposes at the moment. We can treat them like Tensors, although we'd probably want to re-trace if the TensorSpec's `name` changed.", "Thanks @allenlavoie . The TensorSpec thing is not a big deal, you're right, no need to file an issue if it doesn't break anything."]}, {"number": 26310, "title": "Trained model inference on GPU of nvidia TX2 get poor result  even error result", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:nvidia TX2\r\n- TensorFlow installed from (source or binary):binary from https://nvidia.box.com/v/JP33-TF1-11-0-py35-wTRT\r\n- TensorFlow version (use command below):1.110\r\n- Python version:3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:CUDA9.0 cudnn 7.15\r\n- GPU model and memory:8G\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI trained the model on the server and deployed the same version of tensorflow on TX2, but when I run the trained model with the GPU on TX2, I get a lot worse than on the server, but running the model on the CPU of TX2 does not cause this problem.\r\n**Describe the expected behavior**\r\n1. The result of running on the GPU of the server should be the same as the result of the GPU running on TX2. There should not be such a big gap.\r\n2. The GPU running result on TX2 should be the same as the CPU running result on TX2.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@xuefengxiaoyang Can you please update to a more recent version of jetpack and TF version. Also could you please clarify what do you mean by a lot worse?", "@xuefengxiaoyang  hello, did you get the answer of this issue? I am facing the same problem, trained model, the TX2 GPU inference result diff with PC inference result.", "I got  the answer,  just change the Compute Capability of ARCH to match your edge device. for TX2, its 6.2", "Closing since it looks like this is resolved as per https://github.com/tensorflow/tensorflow/issues/26310#issuecomment-541312223", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26310\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26310\">No</a>\n"]}, {"number": 26309, "title": "tf.Data Pipeline using interleave starts fast then becomes extremely slow ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.13\r\n- **Python version**: 3.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9\r\n- **GPU model and memory**: GTX 980 4GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nI am building a pipeline to feed a DNNRegressor model and the issue is that the input pipeline becomes extremely slow after the first few 100's steps.\r\n\r\nI have thousands of TFRecord files of varying size (from 10MB to 3GB in size). Each input has 1038 columns. \r\nI am trying to create batches of size 512 with 1 \"row\" from each TFRecord file (each file has correlated data so I need 1 line from each of the 512 files being interleaved to have a \"shuffled\" batch). To do this,\r\nI am using the Data.interleave method which seems perfect for my case.\r\n\r\nProblem is the input is super fast at the start but gets painfully slow after the first 100's of steps.\r\nCPU usage goes quickly from 100% at the start to near 0% as the pipeline becomes slower. However the disk read speed remains always at  > 60 MBps. What is the issue ? Are the smaller files straggling the pipeline in someway ?\r\n\r\nI have tried using parralel interleave but that was even slower\r\n\r\n### Source code / logs\r\n\r\n```\r\ndef parse_tf(example):\r\n    with tf.device('/cpu:0'):\r\n        parsed_features = tf.parse_example(example, feature)\r\n        feats = parsed_features\r\n        labels = parsed_features.pop('labels')\r\n        return feats, labels\r\n\r\n\r\ndef tf_input_fn(batch_size):\r\n    with tf.device('/cpu:0'):\r\n        files = os.listdir('D:/data/data')\r\n        files = ['D:/data/data/' + file for file in files]\r\n\r\n        dataset = (tf.data.Dataset.from_tensor_slices(files)\r\n                   .interleave(lambda x:\r\n                               tf.data.TFRecordDataset(x),\r\n                               cycle_length=batch_size, block_length=1, num_parallel_calls=8)).batch(batch_size).map(\r\n            parse_tf, num_parallel_calls=1)\r\n\r\n        dataset = dataset.prefetch(100)\r\n        return dataset\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    batch_size_ = 512\r\n\r\n    data = tf_input_fn(batch_size_)\r\n    data_next = data.make_one_shot_iterator().get_next()\r\n    ctr = 0\r\n    with tf.Session() as data_sess:\r\n        data_sess.run(tf.global_variables_initializer())\r\n        tld_start = dt.datetime.now()\r\n        try:\r\n            while True:\r\n                tld_out = data_sess.run(data_next)\r\n                ctr += 1\r\n                if ctr % 100 == 0:\r\n                    tld_end = dt.datetime.now()\r\n                    print(\"Time for 100 steps: \" + str(tld_end - tld_start))\r\n                    tld_start = dt.datetime.now()\r\n                    ctr = 0\r\n        except tf.errors.OutOfRangeError:\r\n            print(\"Done\")\r\n```\r\n\r\n\r\nLogs:\r\n\r\n```\r\nTime for 100 steps: 0:00:04.547647\r\nTime for 100 steps: 0:00:03.132636\r\nTime for 100 steps: 0:00:02.839416\r\nTime for 100 steps: 0:00:02.639220\r\nTime for 100 steps: 0:00:02.492228\r\nTime for 100 steps: 0:00:01.991023\r\nTime for 100 steps: 0:00:12.380473\r\nTime for 100 steps: 0:00:29.162056\r\nTime for 100 steps: 0:00:23.747592\r\nTime for 100 steps: 0:00:28.041797\r\nTime for 100 steps: 0:00:28.309055\r\nTime for 100 steps: 0:00:26.240207\r\nTime for 100 steps: 0:00:27.894402\r\nTime for 100 steps: 0:00:25.832465\r\nTime for 100 steps: 0:00:27.795538\r\nTime for 100 steps: 0:00:28.219248\r\nTime for 100 steps: 0:00:26.962029\r\nTime for 100 steps: 0:00:24.568246\r\nTime for 100 steps: 0:00:29.052262\r\nTime for 100 steps: 0:00:27.003928\r\nTime for 100 steps: 0:00:27.839937\r\nTime for 100 steps: 0:00:28.380154\r\nTime for 100 steps: 0:00:27.109850\r\nTime for 100 steps: 0:00:28.234840\r\n```", "comments": ["facing the same issue on distributed set up in r1.12 @mehdi-beng. CPU utilization and Global Steps/sec drop after some initial steps in my case.\r\n\r\nAttaching screenshot [here](https://s3-ap-southeast-1.amazonaws.com/sahilbprojects/Screenshot+2019-03-04+at+3.40.44+PM.png)", "@sahilbadyal I got it fixed. For me it was the HDD read speed not keeping up and becoming bottleneck. \r\nThe TFRecord format is pretty big on volume, try using : tf.io.TFRecordCompressionType.GZIP) when creating your TFRecords.\r\n\r\nThe file will be 1/5th of the size once compressed making it easier for the HDD. The CPU will take the additional load to decompress each line. But CPU is harder to bottleneck than the HDD read speed. Good luck !", "Closing this issue since its resolved. Feel free to reopen if have further problems. Thanks!"]}, {"number": 26308, "title": "Update username in TODO", "body": "As [recommended by GitHub](https://github.community/t5/Support-Protips/Using-one-account-for-all-your-projects/ba-p/5509), I merged my personal and professional GitHub accounts, and I changed my username. I don't anticipate my username ever changing again.", "comments": []}, {"number": 26307, "title": "Inconsistent encoding leads to AttributeError", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 7.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): compiled from source\r\n- TensorFlow version (use command below): r13.1\r\n- Python version: 3.6.3\r\n- Bazel version (if compiling from source): 0.22\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: P5000/16G\r\n\r\nDeepMind open-sourced the implementation of IMPALA: https://github.com/deepmind/scalable_agent\r\n\r\nFor parallelism, they wrap a mechanism which is class-based in doing so, the file is: https://github.com/deepmind/scalable_agent/blob/master/py_process.py\r\n\r\nThere is a code snippet at the beginning of the file:\r\n\r\n```\r\n  class Zeros(object):\r\n    def __init__(self, dim0):\r\n      self._dim0 = dim0\r\n    def compute(self, dim1):\r\n      return np.zeros([self._dim0, dim1], dtype=np.int32)\r\n    @staticmethod\r\n    def _tensor_specs(method_name, kwargs, constructor_kwargs):\r\n      dim0 = constructor_kwargs['dim0']\r\n      dim1 = kwargs['dim1']\r\n      if method_name == 'compute':\r\n        return tf.contrib.framework.TensorSpec([dim0, dim1], tf.int32)\r\n  with tf.Graph().as_default():\r\n    p = py_process.PyProcess(Zeros, 1)\r\n    result = p.proxy.compute(2)\r\n    with tf.train.SingularMonitoredSession(\r\n        hooks=[py_process.PyProcessHook()]) as session:\r\n      print(session.run(result))  # Prints [[0, 0]].\r\n\r\n```\r\nhowever, when I tried to run it, I got the following error:\r\n\r\n```\r\n2019-03-01 17:37:16.260732: W tensorflow/core/framework/op_kernel.cc:1389] Unknown: AttributeError: 'Zeros' object has no attribute 'b'compute''\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/yuming/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 207, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/data/yuming/eeg-dpg/py_process.py\", line 89, in py_call\r\n    raise result\r\n\r\nAttributeError: 'Zeros' object has no attribute 'b'compute''\r\n\r\n```\r\n\r\nI suspect maybe there is a mismatch between encoding, but not for sure, since I have no problem in running the code if I use Python 2.7.\r\n\r\nCould anyone please take some effort on looking into it?\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "I already posted on stackoverflow but got no answer there. I would think it is a bug, I would guess potentially there is a different encoding representation of python interpreter which is distinguished from source code encoding.\r\n\r\nKindly suggest you try the problem because it is quite easy to re-produce but not that easy to fix.", "@mingyr could you post it in DeepMind Repo [here](https://github.com/deepmind/scalable_agent/issues). Thanks!"]}, {"number": 26306, "title": "tf.python no longer available in 1.13.1?", "body": "Sorry if this is not exactly a documentation bug, but it is, for us, a big change in 1.13.1 that is not addressed in the release notes.\r\n\r\nI realize that tf.python was never part of the public API, but it used to be available, and after upgrading from 1.12, it isn't any longer.  The reason we need it is that it is heavily used internally by tensorflow code, and occasionally we need to \"vendor\" some of this code.  It would be really inconvenient to do this by maintaining our own fork of tensorflow; instead we just copied a python file from the tensorflow source into our repo and made the changes we needed.  Is there some way to keep doing this in 1.13.1 with minimal changes (i.e., not finding all the uses of tf.python and changing them to their public api equivalents)?  (Also, btw, why do the python parts of the Tensorflow source use tf.python so much in the first place?  Couldn't they just use the public api?)", "comments": ["Hi @greaber, \r\n\r\nInteresting, I don't see a `tf.python` in 1.12 either.\r\n\r\nAnyway,  the module is still available.\r\n\r\nHave you tried patching it in with something like this as the first lines in your program:\r\n\r\n```\r\nfrom tensorflow import python as tf_python\r\ntensorflow.python = tf_python\r\n```\r\n\r\nI bet that will get you unstuck."]}, {"number": 26305, "title": "ValueError: as_list() is not defined on an unknown TensorShape.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 pro\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):1.12.0\r\n- Python version:3.6.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:9.0\r\n- GPU model and memory:Nvidida1070/ 8GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nIt gives an error when I am calling the unknown shape tensor into the dynamic_rnn function\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n`def LSTM_model(embed, lstm_sizes, keep_prob_, batch_size):\r\n     lstms = [tf.contrib.rnn.BasicLSTMCell(size) for size in lstm_sizes]\r\n     drops = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_) for lstm in lstms]\r\n     cell = tf.contrib.rnn.MultiRNNCell(drops)\r\n     initial_state = cell.zero_state(batch_size, tf.float64)\r\n     lstm_outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\r\n     return lstm_outputs, final_state`\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n`\r\n  File \"<ipython-input-2-fc074b4d9b11>\", line 23, in <module>\r\n    output_data, final_state = LSTM_model(e_tr1, lstm_size, keep_prob_, batch_size)\r\n\r\n  File \"C:\\Users\\rpsworker\\Documents\\GitHub\\KGE-FE-SR_1\\NAM_Modified\\test_LSTM.py\", line 54, in LSTM_model\r\n    lstm_outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\new_ten\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 664, in dynamic_rnn\r\n    dtype=dtype)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\new_ten\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 733, in _dynamic_rnn_loop\r\n    const_time_steps, const_batch_size = inputs_got_shape[0].as_list()[:2]\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\new_ten\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\", line 904, in as_list\r\n    raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\n\r\nValueError: as_list() is not defined on an unknown TensorShape.`\r\n", "comments": ["My issue has been addressed in the issue [4588](https://github.com/tensorflow/tensorflow/issues/4588), however the solution is not very clear, hence reopening a new issue here", "The code snippet provided looks incomplete. In order to expedite the trouble-shooting process, please provide a minimal code snippet to reproduce the issue reported here. Thanks!\r\n", "Here is the well formatted code with the error\r\n```\r\ndef LSTM_model(embed, lstm_sizes, keep_prob_, batch_size):\r\n    lstms = [tf.contrib.rnn.BasicLSTMCell(size) for size in lstm_sizes]\r\n    drops = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_) for lstm in lstms]\r\n    cell = tf.contrib.rnn.MultiRNNCell(drops)\r\n    initial_state = cell.zero_state(batch_size, tf.float64)\r\n    lstm_outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\r\n    return lstm_outputs, final_state\r\n```\r\n```\r\nFile \"\", line 23, in \r\noutput_data, final_state = LSTM_model(e_tr1, lstm_size, keep_prob_, batch_size)\r\n\r\nFile \"C:\\Users\\rpsworker\\Documents\\GitHub\\KGE-FE-SR_1\\NAM_Modified\\test_LSTM.py\", line 54, in LSTM_model\r\nlstm_outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\r\n\r\nFile \"C:\\ProgramData\\Anaconda3\\envs\\new_ten\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 664, in dynamic_rnn\r\ndtype=dtype)\r\n\r\nFile \"C:\\ProgramData\\Anaconda3\\envs\\new_ten\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 733, in _dynamic_rnn_loop\r\nconst_time_steps, const_batch_size = inputs_got_shape[0].as_list()[:2]\r\n\r\nFile \"C:\\ProgramData\\Anaconda3\\envs\\new_ten\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\", line 904, in as_list\r\nraise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\n\r\nValueError: as_list() is not defined on an unknown TensorShape.`\r\n```\r\nMinor issue when passing embedding vector from the embedding layer to the TensorFlow function tf.nn.dynamic_rnn() function.Not able to pass it directly as a tensor, as the tensor shape is unknown, the function is not able to recognize the actual shape and it gives the error.\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "Thank you for the help. I will go ahead and close the issue \r\n", "@rajathpatel23 did you manage to solve this issue?\r\nI'm getting the same error from model.fit when passing two tensors(input and label) using dataset_from_generator, but no issues when passing the two tensors manually", "My program has the same problem, [this is my Colaboratory](https://colab.research.google.com/drive/1kVSJzt4FzqdtIdqx0UOYXC0C6Zqb1nVL)"]}, {"number": 26304, "title": "Trouble Importing TensorFlow", "body": "\r\n**System information**\r\n- Windows 10 Home\r\n\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6.0\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI installed python 3.6.0 to make sure that tensorflow could run, because before i had python 3.7.1. Then I installed tensorflow via \"pip install tensorflow\". The installation was successful, but when trying to import tensorflow I got the error message shown below. I am unsure of what to do next, please help. PS: I am running this on a Lenovo ideapad 100, so there is not a dedicated graphics card or gpu.\r\n\r\n\r\n\r\n**Any other info / logs**\r\nFile \"c:\\Users\\patri_yllzc1c\\Desktop\\TensorFlow Projects\\NumberGuessCNN.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\patri_yllzc1c\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\patri_yllzc1c\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\patri_yllzc1c\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\patri_yllzc1c\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n", "comments": ["@pbcubes This error signals a mismatch between protobuf and TensorFlow versions.\r\n\r\nTake the following steps to fix this error:\r\n\r\nUninstall TensorFlow.\r\nUninstall protobuf (if protobuf is installed).\r\nReinstall TensorFlow, which will also install the correct protobuf dependency.\r\n\r\nhttps://stackoverflow.com/questions/42006320/tensorflow-pip-installation-issue-cannot-import-name-descriptor\r\n\r\nSome common installation errors can be found at : https://www.tensorflow.org/install/errors", "@pbcubes Please follow @Ayush517 suggestion and the link also has couple of other solutions, if you like them. Please let me know how it progresses. Please close the issue if it was resolved. Thanks!", "@pbcubes Any progress?", "I fixed it after reinstalling it.\n\nOn Thu, Mar 7, 2019 at 8:11 AM Ayush Agrawal <notifications@github.com>\nwrote:\n\n> @pbcubes <https://github.com/pbcubes> Any progress?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26304#issuecomment-470518205>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AbqiFOUmd7c7hAY3jsa-JD_TpSgZbAVYks5vURAHgaJpZM4bbXD3>\n> .\n>\n"]}, {"number": 26303, "title": "[TF 2.0] Support for TFGAN.", "body": "[TFGAN](https://www.tensorflow.org/api_docs/python/tf/contrib/gan) is a popular lightweight library for training and evaluating GANs. In addition to providing the infrastructure for easily training and evaluating GANS, this library contains modules for a TFGAN-backed Estimator, evaluation metrics, features (such as virtual batch normalization), and losses.\r\n\r\nThe purpose of this feature request would be to migrate TFGAN to its own repo, and to ensure it is compatible with TensorFlow 2.0.", "comments": ["@dynamicwebpaige I want to work on that!! How do I start??", "@Dexter2389 see https://github.com/zurutech/gans-from-theory-to-production", "I'm currently migrating all notebooks (https://github.com/zurutech/gans-from-theory-to-production) to their Tensorflow 2.0 version.\r\nRight now, without the new location of `tf.contrib.gan` the migration is stopped.\r\n\r\nThus @Dexter2389 I guess you can use notebooks 2.1, 3.0, and 3.1 as a reference.\r\nIn particular, in the 2.1 I already migrated the `discriminator_fn` to what I think the new implementation tf2 compatible (Keras based) should be. The other notebooks are just how to use tfgan.\r\n\r\nWhen `tf.contrib.gan` is moved to its own repo and the models are migrated as in notebook 2.1 everything should work as in 1.x version.", "IF someone is willing to contribute and maintain, I propose that we move this to tf.addons for now.", "Ok!!", "> Ok!!\r\n\r\nHey @Dexter2389 what's the progress on this? I'ld like to collaborate.", "TF-GAN is now compatible with TensorFlow 2.0, and can be found [here](https://github.com/tensorflow/gan). Let us know what you think! \ud83d\ude04 ", "@dynamicwebpaige so what would be the equivalent of tf.contrib.gan.eval.get_graph_def_from_disk('path to .pb file)"]}, {"number": 26302, "title": "[TF 2.0] StridedSlice issue with empty slice.", "body": "Empty arrays cause TypeErrors with `StridedSlice`. A one-liner to reproduce would be:\r\n\r\n```python\r\n tf.constant([1, 2, 3])[tf.constant([], dtype=tf.int32)]\r\n```\r\n\r\nWhich returns:\r\n\r\n```\r\nTypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got <tf.Tensor: id=1, shape=(0,), dtype=int32, numpy=array([], dtype=int32)>\r\n```\r\n\r\nThe numpy equivalent:\r\n\r\n```python\r\nnp.array([1, 2, 3])[np.array([], dtype=np.int32)]\r\n```\r\n\r\nworks as expected.", "comments": ["I met the same problem, and I found that if we are using tensor, we could just convert it into numpy by \"tensor.numpy()\" in tf2.", "Could reproduce the issue with TF version 2.2.0-dev20200319. Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/3c0794133681c3d337bb72c13300d00e/untitled735.ipynb). Thanks!", "I get it. Thanks for your notification.\n\n\n| |\nyhy_workspace\n|\n|\n\u90ae\u7bb1\uff1ayhy_workspace@163.com\n|\n\nSignature is customized by Netease Mail Master\n\nOn 03/19/2020 19:44, ravikyram wrote:\n\nCould reproduce the issue with TF version 2.2.0-dev20200319. Please, find the gist here.. Thanks!\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or unsubscribe.", "I tried in colab with TF versions 2.2,2.3-rc1, nightly versions(`2.4.0-dev20200712`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/b005cb70fa2f4fa9fb440f643a0b8098/untitled129.ipynb).Thanks!", "@dynamicwebpaige , you can convert Tensor to Numpy and then perform slicing like below. \r\nimport tensorflow as tf\r\n```\r\ntf.constant([1, 2, 3]).numpy()[tf.constant([], dtype=tf.int32)]\r\n\r\narray([], dtype=int32)\r\n```\r\n\r\nAlso you can perform Tensor slicing like numpy using [tf.slice ](https://www.tensorflow.org/guide/tensor_slicing#extract_tensor_slices)\r\n```\r\nt1 = tf.constant([1, 2, 3])\r\n\r\nprint(tf.slice(t1,\r\n               begin=[0],size=[0]))\r\n\r\ntf.Tensor([], shape=(0,), dtype=int32)\r\n```\r\nPlease find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/e2f55aa1b91c9831680c7a3681044114/26302.ipynb).\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26302\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26302\">No</a>\n"]}, {"number": 26301, "title": "Extract legality and profitability checks into separate functions in fusion_merger", "body": "", "comments": ["@sana-damani could you please resolve conflicts ?", "@thomasjoerg can you please review new changes "]}, {"number": 26300, "title": "Random Uniform Not Supported for TFlite Conversion", "body": "**System **information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): tf-nightly 1.14.1\r\n\r\n**Describe the Problem**\r\nTrying to convert frozen tensorflow model to .tflite so I can use it on my android phone, but conversion fails due to RandomUniform OP being unsupported.\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). \r\nOtherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). \r\nHere is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DIV, FLOOR, MAX_POOL_2D, MUL, SHAPE, SPLIT, SUB. \r\nHere is a list of operators for which you will need custom implementations: RandomUniform.\r\n\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nToco command used\r\n\r\ntflite_convert \r\n--output_file=segmentleaves.tflite \r\n--graph_def_file=frozenGraph/frozen_model.pb \r\n--input_shapes=1,224,224,3 \r\n--input_arrays=image_input \r\n--output_arrays=layer7_out\r\n\r\n", "comments": ["yes\uff0ci have been seeking for a solution  for the  unsupported RandomUniform  ops these days\uff0cwhen  it can be solved ", "The Random operators should be available using our [experimental pipeline for using select TensorFlow ops](https://www.tensorflow.org/lite/guide/ops_select). Feel free to re-open if you encounter issues using that pipeline. Cheers.", "> \r\n> \r\n> The Random operators should be available using our [experimental pipeline for using select TensorFlow ops](https://www.tensorflow.org/lite/guide/ops_select). Feel free to re-open if you encounter issues using that pipeline. Cheers.\r\n\r\nHi, thanks for this.  Using this I am now able to create the .tflite file.\r\nI have followed the rest of the guide but am unable to run my app after adding the tensorflow-lite-with-select-tf-ops.aar module to the app.\r\n\r\nWhen running the app and trying to use tflite I now get the error:\r\n`No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)`\r\n\r\nIs this caused by me building it incorrectly? or is this a known issue?", "You might need to clean your build. Are you using the select-tf-ops.aar in place of the regular tflite aar?", "Yes, I have added the select-tf-ops.aar file and added it to my dependencies in place of the regular tensorflow lite and then cleaned and rebuilt my project.  Are there any additional steps not listed in the page you linked?", "Hmm, that might be caused by failure to actually load the library. Are you running on an x86 emulator? I believe we only ship arm64 libs for the select-tf-ops library. Can you look at the extended log for anything that mentions something like \"TensorFlowLite: failed to load native library\"?", "I'm running it on a pixel 2 xl, but you are correct about the error message.\r\n\r\n`W/System.err: TensorFlowLite: failed to load native library: dalvik.system.PathClassLoader[DexPathList[[zip file \"/data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/base.apk\", zip file \"/data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_dependencies_apk.apk\", zip file \"/data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_resources_apk.apk\", zip file \"/data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_0_apk.apk\", zip file \"/data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_1_apk.apk\", zip file \"/data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_2_apk.apk\", zip file \"/data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_3_apk.apk\", zip file \"/data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_4_apk.apk\", zip file \"/data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_5_apk.apk\", zip file \"/data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_6_apk.apk\", zip file \"/data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_7_apk.apk\", zip file \"/data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_8_apk.apk\", zip file \"/data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_9_apk.apk\"],nativeLibraryDirectories=[/data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/lib/arm64, /data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/base.apk!/lib/arm64-v8a, /data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_dependencies_apk.apk!/lib/arm64-v8a, /data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_resources_apk.apk!/lib/arm64-v8a, /data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_0_apk.apk!/lib/arm64-v8a, /data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_1_apk.apk!/lib/arm64-v8a, /data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_2_apk.apk!/lib/arm64-v8a, /data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_3_apk.apk!/lib/arm64-v8a, /data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_4_apk.apk!/lib/arm64-v8a, /data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_5_apk.apk!/lib/arm64-v8a, /data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_6_apk.apk!/lib/arm64-v8a, /data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_7_apk.apk!/lib/arm64-v8a, /data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_8_apk.apk!/lib/arm64-v8a, /data/app/nehartley.za.shrubnet-STg6ZQZ0cqj1gnGF4hCSXA==/split_lib_slice_9_apk.apk!/lib/arm64-v8a, /system/lib64]]] couldn't find \"libtensorflowlite_jni.so\"\r\n`\r\n\r\nCan you suggest how to fix this?", "Can you look at the contents of your APK to make sure the library is present? Or can you attach the gradle rule for including the .aar?", "Inside the APK there is libtensorflowlite_flex_jni.so inside /lib/amreabi-v7a, so I assume that is what you are referring to as I can find no other files referencing tensorflow.\r\nIn build.gradle I added the line\r\n`implementation project(':tensorflow-lite-with-select-tf-ops')`\r\nto import the new module.  Is this what you mean?", "Is your project configured to use arm32? How exactly did you build the .aar?"]}, {"number": 26299, "title": "TF_SessionRun_wrapper: expected all values in input dict to be ndarray", "body": "I am using Tensorflow 1.13 in windows 10 \r\n\r\nI am converting image data into tf.records. I am using build_image_data.py in which i am getting below error. \r\n\r\n**TF_SessionRun_wrapper: expected all values in input dict to be ndarray SKIPPED: Unexpected error while decoding** \r\n\r\nCode section \r\n    def decode_jpeg(self, image_data):\r\n        print(type(image_data))\r\n        **image = self._sess.run([self._decode_jpeg],\r\n                               feed_dict={self._decode_jpeg_data: image_data})**\r\n        assert len(image.shape) == 3\r\n        assert image.shape[2] == 3\r\n        return image\r\n\r\n\r\n\r\nPlease find below code snippet \r\n-----------------------------------------------------------------START-------------------------------------------\r\n\r\n\"\"\"Converts image data to TFRecords file format with Example protos.\r\nThe image data set is expected to reside in JPEG files located in the\r\nfollowing directory structure.\r\n  data_dir/label_0/image0.jpeg\r\n  data_dir/label_0/image1.jpg\r\n  ...\r\n  data_dir/label_1/weird-image.jpeg\r\n  data_dir/label_1/my-image.jpeg\r\n  ...\r\nwhere the sub-directory is the unique label associated with these images.\r\nThis TensorFlow script converts the training and evaluation data into\r\na sharded data set consisting of TFRecord files\r\n  train_directory/train-00000-of-01024\r\n  train_directory/train-00001-of-01024\r\n  ...\r\n  train_directory/train-00127-of-01024\r\nand\r\n  test_directory/test-00000-of-00128\r\n  test_directory/test-00001-of-00128\r\n  ...\r\n  test_directory/test-00127-of-00128\r\nwhere we have selected 1024 and 128 shards for each data set. Each record\r\nwithin the TFRecord file is a serialized Example proto. The Example proto\r\ncontains the following fields:\r\n  image/encoded: string containing JPEG encoded image in RGB colorspace\r\n  image/height: integer, image height in pixels\r\n  image/width: integer, image width in pixels\r\n  image/colorspace: string, specifying the colorspace, always 'RGB'\r\n  image/channels: integer, specifying the number of channels, always 3\r\n  image/format: string, specifying the format, always'JPEG'\r\n  image/filename: string containing the basename of the image file\r\n            e.g. 'n01440764_10026.JPEG' or 'ILSVRC2012_val_00000293.JPEG'\r\n  image/class/label: integer specifying the index in a classification layer.\r\n    The label ranges from [0, num_labels] where 0 is unused and left as\r\n    the background class.\r\n  image/class/text: string specifying the human-readable version of the label\r\n    e.g. 'dog'\r\nIf you data set involves bounding boxes, please look at build_imagenet_data.py.\r\n\"\"\"\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nfrom datetime import datetime\r\nimport os\r\nimport random\r\nimport sys\r\nimport threading\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom utils import constants\r\n\r\ntf.app.flags.DEFINE_string('train_directory', constants.training_images_dir,\r\n                           'Training data directory')\r\n\r\ntf.app.flags.DEFINE_string('test_directory', constants.test_images_dir,\r\n                           'Test data directory')\r\n\r\ntf.app.flags.DEFINE_string('output_directory', constants.data_dir,\r\n                           'Output data directory')\r\n\r\ntf.app.flags.DEFINE_integer('train_shards', 3,\r\n                            'Number of shards in training TFRecord files.')\r\ntf.app.flags.DEFINE_integer('test_shards', 3,\r\n                            'Number of shards in test TFRecord files.')\r\n\r\ntf.app.flags.DEFINE_integer('num_threads', 3,\r\n                            'Number of threads to preprocess the images.')\r\n\r\n# The labels file contains a list of valid labels are held in this file.\r\n# Assumes that the file contains entries as such:\r\n#   dog\r\n#   cat\r\n#   flower\r\n# where each line corresponds to a label. We map each label contained in\r\n# the file to an integer corresponding to the line number starting from 0.\r\ntf.app.flags.DEFINE_string('labels_file', constants.labels_file, 'Labels file')\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n\r\ndef _int64_feature(value):\r\n    \"\"\"Wrapper for inserting int64 features into Example proto.\"\"\"\r\n    if not isinstance(value, list):\r\n        value = [value]\r\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\r\n\r\n\r\ndef _bytes_feature(value):\r\n    \"\"\"Wrapper for inserting bytes features into Example proto.\"\"\"\r\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n\r\n\r\ndef _convert_to_example(filename, image_buffer, label, text, height, width):\r\n    \"\"\"Build an Example proto for an example.\r\n\r\n    Args:\r\n      filename: string, path to an image file, e.g., '/path/to/example.JPG'\r\n      image_buffer: string, JPEG encoding of RGB image\r\n      label: integer, identifier for the ground truth for the network\r\n      text: string, unique human-readable, e.g. 'dog'\r\n      height: integer, image height in pixels\r\n      width: integer, image width in pixels\r\n    Returns:\r\n      Example proto\r\n    \"\"\"\r\n\r\n    colorspace = 'RGB'\r\n    channels = 3\r\n    image_format = 'JPEG'\r\n\r\n    example = tf.train.Example(features=tf.train.Features(feature={\r\n        'height': _int64_feature(height),\r\n        'width': _int64_feature(width),\r\n        'label': _int64_feature(label),\r\n        'image_raw': _bytes_feature(tf.compat.as_bytes(image_buffer))}))\r\n    return example\r\n\r\n\r\nclass ImageCoder(object):\r\n    \"\"\"Helper class that provides TensorFlow image coding utilities.\"\"\"\r\n\r\n    def __init__(self):\r\n        # Create a single Session to run all image coding calls.\r\n        self._sess = tf.Session()\r\n\r\n        # Initializes function that converts PNG to JPEG data.\r\n        self._png_data = tf.placeholder(dtype=tf.string)\r\n        image = tf.image.decode_png(self._png_data, channels=3)\r\n        self._png_to_jpeg = tf.image.encode_jpeg(image, format='rgb', quality=100)\r\n\r\n        # Initializes function that decodes RGB JPEG data.\r\n        self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\r\n        self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\r\n\r\n    def png_to_jpeg(self, image_data):\r\n        print(type(image_data))\r\n        return self._sess.run([self._png_to_jpeg],\r\n                              feed_dict={self._png_data: image_data})\r\n\r\n    def decode_jpeg(self, image_data):\r\n        print(type(image_data))\r\n        image = self._sess.run([self._decode_jpeg],\r\n                               feed_dict={self._decode_jpeg_data: image_data})\r\n        assert len(image.shape) == 3\r\n        assert image.shape[2] == 3\r\n        return image\r\n\r\n\r\ndef _is_png(filename):\r\n    \"\"\"Determine if a file contains a PNG format image.\r\n\r\n    Args:\r\n      filename: string, path of the image file.\r\n\r\n    Returns:\r\n      boolean indicating if the image is a PNG.\r\n    \"\"\"\r\n    return filename.endswith('.png')\r\n\r\n\r\ndef _process_image(filename, coder):\r\n    \"\"\"Process a single image file.\r\n\r\n    Args:\r\n      filename: string, path to an image file e.g., '/path/to/example.JPG'.\r\n      coder: instance of ImageCoder to provide TensorFlow image coding utils.\r\n    Returns:\r\n      image_buffer: string, JPEG encoding of RGB image.\r\n      height: integer, image height in pixels.\r\n      width: integer, image width in pixels.\r\n    \"\"\"\r\n    # Read the image file.\r\n    with tf.gfile.FastGFile(filename, 'rb') as f:\r\n        image_data = f.read()\r\n\r\n    # Convert any PNG to JPEG's for consistency.\r\n    if _is_png(filename):\r\n        image_data = coder.png_to_jpeg(image_data)\r\n\r\n    # Decode the RGB JPEG\r\n    image = coder.decode_jpeg(image_data)\r\n    # Check that image converted to RGB\r\n    assert len(image.shape) == 3\r\n    height = image.shape[0]\r\n    width = image.shape[1]\r\n    assert image.shape[2] == 3\r\n\r\n    return image_data, height, width\r\n\r\n\r\ndef _process_image_files_batch(coder, thread_index, ranges, name, filenames,\r\n                               texts, labels, num_shards):\r\n    \"\"\"Processes and saves list of images as TFRecord in 1 thread.\r\n\r\n    Args:\r\n      coder: instance of ImageCoder to provide TensorFlow image coding utils.\r\n      thread_index: integer, unique batch to run index is within [0, len(ranges)).\r\n      ranges: list of pairs of integers specifying ranges of each batches to\r\n        analyze in parallel.\r\n      name: string, unique identifier specifying the data set\r\n      filenames: list of strings; each string is a path to an image file\r\n      texts: list of strings; each string is human readable, e.g. 'dog'\r\n      labels: list of integer; each integer identifies the ground truth\r\n      num_shards: integer number of shards for this data set.\r\n    \"\"\"\r\n    # Each thread produces N shards where N = int(num_shards / num_threads).\r\n    # For instance, if num_shards = 128, and the num_threads = 2, then the first\r\n    # thread would produce shards [0, 64).\r\n    num_threads = len(ranges)\r\n    assert not num_shards % num_threads\r\n    num_shards_per_batch = int(num_shards / num_threads)\r\n\r\n    shard_ranges = np.linspace(ranges[thread_index][0],\r\n                               ranges[thread_index][1],\r\n                               num_shards_per_batch + 1).astype(int)\r\n    num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\r\n\r\n    counter = 0\r\n    for s in range(num_shards_per_batch):\r\n        # Generate a sharded version of the file name, e.g. 'train-00002-of-00010'\r\n        shard = thread_index * num_shards_per_batch + s\r\n        output_filename = '%s-%.5d-of-%.5d' % (name, shard, num_shards)\r\n        output_file = os.path.join(FLAGS.output_directory, output_filename)\r\n        writer = tf.python_io.TFRecordWriter(output_file)\r\n\r\n        shard_counter = 0\r\n        files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\r\n        for i in files_in_shard:\r\n            filename = filenames[i]\r\n            label = labels[i]\r\n            text = texts[i]\r\n\r\n            try:\r\n                image_buffer, height, width = _process_image(filename, coder)\r\n            except Exception as e:\r\n                print(e)\r\n                print('SKIPPED: Unexpected error while decoding %s.' % filename)\r\n                continue\r\n\r\n            example = _convert_to_example(filename, image_buffer, label,\r\n                                          text, height, width)\r\n            writer.write(example.SerializeToString())\r\n            shard_counter += 1\r\n            counter += 1\r\n\r\n            if not counter % 1000:\r\n                print('%s [thread %d]: Processed %d of %d images in thread batch.' %\r\n                      (datetime.now(), thread_index, counter, num_files_in_thread))\r\n                sys.stdout.flush()\r\n\r\n        writer.close()\r\n        print('%s [thread %d]: Wrote %d images to %s' %\r\n              (datetime.now(), thread_index, shard_counter, output_file))\r\n        sys.stdout.flush()\r\n        shard_counter = 0\r\n    print('%s [thread %d]: Wrote %d images to %d shards.' %\r\n          (datetime.now(), thread_index, counter, num_files_in_thread))\r\n    sys.stdout.flush()\r\n\r\n\r\ndef _process_image_files(name, filenames, texts, labels, num_shards):\r\n    \"\"\"Process and save list of images as TFRecord of Example protos.\r\n\r\n    Args:\r\n      name: string, unique identifier specifying the data set\r\n      filenames: list of strings; each string is a path to an image file\r\n      texts: list of strings; each string is human readable, e.g. 'dog'\r\n      labels: list of integer; each integer identifies the ground truth\r\n      num_shards: integer number of shards for this data set.\r\n    \"\"\"\r\n    assert len(filenames) == len(texts)\r\n    assert len(filenames) == len(labels)\r\n\r\n    # Break all images into batches with a [ranges[i][0], ranges[i][1]].\r\n    spacing = np.linspace(0, len(filenames), FLAGS.num_threads + 1).astype(np.int)\r\n    ranges = []\r\n    for i in range(len(spacing) - 1):\r\n        ranges.append([spacing[i], spacing[i + 1]])\r\n\r\n    # Launch a thread for each batch.\r\n    print('Launching %d threads for spacings: %s' % (FLAGS.num_threads, ranges))\r\n    sys.stdout.flush()\r\n\r\n    # Create a mechanism for monitoring when all threads are finished.\r\n    coord = tf.train.Coordinator()\r\n\r\n    # Create a generic TensorFlow-based utility for converting all image codings.\r\n    coder = ImageCoder()\r\n\r\n    threads = []\r\n    for thread_index in range(len(ranges)):\r\n        args = (coder, thread_index, ranges, name, filenames,\r\n                texts, labels, num_shards)\r\n        t = threading.Thread(target=_process_image_files_batch, args=args)\r\n        t.start()\r\n        threads.append(t)\r\n\r\n    # Wait for all the threads to terminate.\r\n    coord.join(threads)\r\n    print('%s: Finished writing all %d images in data set.' %\r\n          (datetime.now(), len(filenames)))\r\n    sys.stdout.flush()\r\n\r\n\r\ndef _find_image_files(data_dir, labels_file):\r\n    \"\"\"Build a list of all images files and labels in the data set.\r\n\r\n    Args:\r\n      data_dir: string, path to the root directory of images.\r\n\r\n        Assumes that the image data set resides in JPEG files located in\r\n        the following directory structure.\r\n\r\n          data_dir/dog/another-image.JPEG\r\n          data_dir/dog/my-image.jpg\r\n\r\n        where 'dog' is the label associated with these images.\r\n\r\n      labels_file: string, path to the labels file.\r\n\r\n        The list of valid labels are held in this file. Assumes that the file\r\n        contains entries as such:\r\n          dog\r\n          cat\r\n          flower\r\n        where each line corresponds to a label. We map each label contained in\r\n        the file to an integer starting with the integer 0 corresponding to the\r\n        label contained in the first line.\r\n\r\n    Returns:\r\n      filenames: list of strings; each string is a path to an image file.\r\n      texts: list of strings; each string is the class, e.g. 'dog'\r\n      labels: list of integer; each integer identifies the ground truth.\r\n    \"\"\"\r\n    print('Determining list of input files and labels from %s.' % data_dir)\r\n    unique_labels = [l.strip() for l in tf.gfile.FastGFile(\r\n        labels_file, 'r').readlines()]\r\n\r\n    labels = []\r\n    filenames = []\r\n    texts = []\r\n\r\n    # Leave label index 0 empty as a background class.\r\n    label_index = 1\r\n\r\n    # Construct the list of JPEG files and labels.\r\n    for text in unique_labels:\r\n        jpeg_file_path = '%s/%s/*' % (data_dir, text)\r\n        matching_files = tf.gfile.Glob(jpeg_file_path)\r\n\r\n        labels.extend([label_index] * len(matching_files))\r\n        texts.extend([text] * len(matching_files))\r\n        filenames.extend(matching_files)\r\n\r\n        if not label_index % 100:\r\n            print('Finished finding files in %d of %d classes.' % (\r\n                label_index, len(labels)))\r\n        label_index += 1\r\n\r\n    # Shuffle the ordering of all image files in order to guarantee\r\n    # random ordering of the images with respect to label in the\r\n    # saved TFRecord files. Make the randomization repeatable.\r\n    shuffled_index = list(range(len(filenames)))\r\n    random.seed(12345)\r\n    random.shuffle(shuffled_index)\r\n\r\n    filenames = [filenames[i] for i in shuffled_index]\r\n    texts = [texts[i] for i in shuffled_index]\r\n    labels = [labels[i] for i in shuffled_index]\r\n\r\n    print('Found %d JPEG files across %d labels inside %s.' %\r\n          (len(filenames), len(unique_labels), data_dir))\r\n    return filenames, texts, labels\r\n\r\n\r\ndef _process_dataset(name, directory, num_shards, labels_file):\r\n    \"\"\"Process a complete data set and save it as a TFRecord.\r\n    Args:\r\n      name: string, unique identifier specifying the data set.\r\n      directory: string, root path to the data set.\r\n      num_shards: integer number of shards for this data set.\r\n      labels_file: string, path to the labels file.\r\n    \"\"\"\r\n    filenames, texts, labels = _find_image_files(directory, labels_file)\r\n    _process_image_files(name, filenames, texts, labels, num_shards)\r\n\r\n\r\ndef main(unused_argv):\r\n    assert not FLAGS.train_shards % FLAGS.num_threads, (\r\n        'Please make the FLAGS.num_threads commensurate with FLAGS.train_shards')\r\n    assert not FLAGS.test_shards % FLAGS.num_threads, (\r\n        'Please make the FLAGS.num_threads commensurate with '\r\n        'FLAGS.test_shards')\r\n    print('Saving results to %s' % FLAGS.output_directory)\r\n\r\n    # Run it!\r\n    _process_dataset('test', FLAGS.test_directory,\r\n                     FLAGS.test_shards, FLAGS.labels_file)\r\n    _process_dataset('train', FLAGS.train_directory,\r\n                     FLAGS.train_shards, FLAGS.labels_file)\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.app.run()\r\n-----------------------------------------------------------------END---------------------------------------------\r\n \r\n\r\n", "comments": ["I tried to convert data into ndarray still getting the same issue. Using below code\r\n\r\n print(type(image_data))\r\n image_data_ndarry = np.fromstring(image_data, dtype=np.uint8)\r\n print(type(image_data_ndarry))\r\n image = self._sess.run([self._decode_jpeg], feed_dict={self._decode_jpeg_data: image_data_ndarry})\r\n\r\nWhen I print the type I am getting **<class 'numpy.ndarray'>** Still tf is not accepting the data in fee dictionary and raising same exception. \r\n\r\n**<class 'numpy.ndarray'> TF_SessionRun_wrapper: expected all values in input dict to be ndarray**\r\n\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 26298, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "I get the following error message:\r\n\r\n> 'ImportError: DLL load failed: The specified module could not be found.'.\r\n\r\nwhen I run the following code:\r\n\r\n> python -c 'import tensorflow as tf; print(tf.__version__)'\r\n\r\nPlease check this screenshoot to see the list of my environment variables and the error message itself.\r\nhttps://www.screencast.com/t/PsEXiaM0S\r\n\r\nI'm running Windows 10 PRO and I'm using Geforce RTX 2070 and TensorFlow-GPU 1.13.1\r\n\r\nI have installed the following:\r\n\r\n1. Python 3.6.7\r\n2. Microsoft Visual C++ 2015 Redistributable (x64) - 14.0.24215 (could not be installed - legacy now)\r\n2. Microsoft Visual C++ 2017 Redistributable (x64) - 14.16.27027\r\n3. Microsoft Visual C++ Build Tools - 15.9.28307.423\r\n4. msys2-x86_64-20180531.exe\r\n5. Cuda_10.1.105_win10_network\r\n6. Cudnn 7.4.1.5 or 7.5 (I tried both)\r\n7. Nvidia driver 419.17-desktop-win10-64bit-international-whql.exe\r\n", "comments": ["It solved it like this:\r\n\r\n1. Uninstalled CUDA 10.1\r\n2. Restarted\r\n3. Installed CUDA 10.0\r\n\r\nThats it! :)\r\n\r\n/Cheers peps!", "Under Windows 10 / Py 3.6.x / with RTX2080, the combination of CUDA 10.0 (not 10.1) - as pointed out by Garfield2013 - with cuDNN 7.5.0 works. Don't forget to add the location of the cuDNN *.dll file(s) (this is /bin/ folder in the CUDA dir) to your PATH under Windows.\r\nTensorflow can be installed using `pip install tensorflow-gpu` (Version 1.13.1 as of April 12, 2019)"]}, {"number": 26297, "title": "Tensorflow lite gpu delegate inference using opengl and SSBO in android", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes, modified inference code from tflite gpu delegate android sample with additional code from https://www.tensorflow.org/lite/performance/gpu_advanced#android_2.\r\n- OS Platform and Distribution : Android 8.0.0\r\n- Mobile device: OnePlus 3\r\n- TensorFlow version: 12.0\r\n\r\n**Describe the current behavior**\r\nThe tensorflow lite gpu delegate documentation has provided a sample code for running the tflite inference efficiently on android, avoiding CPU_GPU memory copying with the help of opengl and SSBO in a egl context. However, this method does not seem to give any performance gains; rather it degraded the inference performance in terms of speed.The documentation mentions a method - 'interpreter.runInference(null, outputArray)' for running the inference in this case.Is this method same as the basic run method i.e interpreter.run(inputTensor, outputTensor). (There seems to be no method in the current api called 'interpreter.runInference').Is the method suggested currently supported in the experimental gpu delegate api (i.e accessing input image from opengl ssbo directly for running the inference)?How can we ensure that the model takes the input from this SSBO in GPU memory?\r\n\r\n** Expected behaviour**\r\nThe tflite inference using opengl ssbo should be faster than the basic gpu delegate inference, where data is copied every-time from cpu to gpu.\r\n\r\n**Other info / logs**\r\nWe measured the time for the 'tflite.run' method in android studio.The input was in the recommended ByteBuffer format.\r\n\r\nError: Cannot resolve method runInference(null, ?)", "comments": ["@anilsathyan7 \r\n\r\nThanks for trying out the GPU delegate.\r\n\r\nCan you provide a little bit more context in terms of timing, i.e. how many milliseconds/seconds was it before and after?\r\n\r\nWhat kind of network are you using?  Specifically, are all ops supported?\r\n\r\nHave you written a custom shader code to copy camera texture into SSBO, or are you just dumping CPU memory to SSBO by yourself?  If it's the former, you're doing things right and it should get faster.  If it's the latter, it's only going to get slower.\r\n\r\n", "Model: Similar to the Official [TF-Lite Segmentation Model](https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/deeplabv3_257_mv_gpu.tflite.) (model inference graph attached as image).The last three additional nodes are not supported by gpu delegate, it seems.The input image size is 129*129.\r\n\r\nPhone: OnePlus 3, GPU: Adreno 530\r\n\r\nTimings:-\r\nCPU Inference: 60-70 ms\r\nGPU Inference: 40-50 ms\r\nGPU Inference (SSBO): 80-90 ms\r\n\r\ni.e Time for executing 'interpreter.run()' method.\r\n\r\nHere is the method that we  used to copy camera texture into SSBO:-\r\n\r\n```\r\n//Initialise SSBO\r\npublic int[] initializeShaderBuffer(){\r\n    android.opengl.EGLContext eglContext = eglGetCurrentContext();\r\n    int[] id = new int[1];\r\n    GLES31.glGenBuffers(id.length, id, 0);\r\n    GLES31.glBindBuffer(GLES31.GL_SHADER_STORAGE_BUFFER, id[0]);\r\n    GLES31.glBufferData(GLES31.GL_SHADER_STORAGE_BUFFER, mWidth * mHeight, null, GLES31.GL_STREAM_COPY);\r\n    GLES31.glBindBuffer(GLES31.GL_SHADER_STORAGE_BUFFER, 0);// unbind\r\n    return id;\r\n}\r\nint inputSsboId = initializeShaderBuffer()[0];\r\n\r\n //After that every time a frame is available OR in onDraFrame(), call \r\nfillSsboWithCameraImageTexture(inputSsboId,data);\r\n\r\n//(Note: Data is Nothing but Camera Frame ByteBuffer)\r\n\r\n// Fill Ssbo With CameraImageTexture \r\n\r\nprivate int fillSsboWithCameraImageTexture(int inputSsboId,ByteBuffer cameraFramme) {\r\n\r\n    GLES31.glBufferData(GLES31.GL_SHADER_STORAGE_BUFFER, mWidth * mHeight, cameraFramme, GLES31.GL_STREAM_COPY);\r\n    return inputSsboId;\r\n\r\n}\r\n```\r\n![129_80k_dm05](https://user-images.githubusercontent.com/1130185/53906703-97933580-4071-11e9-8241-260e3a262f73.png)\r\n\r\nCan the same 'Interpreter.run()' method handle normal input from CPU and SSBO? Or is there any other options/functions for running the inference in this case?\r\n\r\n\r\n\r\n\r\n", "@anilsathyan7 \r\n\r\nApologies for the delayed response.  For some reason, I just got this in my inbox >_<\r\n\r\nQuick question re: your code:\r\n\r\nDoesn't it have to be \r\n\r\n`GLES31.glBufferData(GLES31.GL_SHADER_STORAGE_BUFFER, 3 * mWidth * mHeight, null, GLES31.GL_STREAM_COPY);`\r\n\r\n?\r\n\r\nAlso, do you have the luxury to make the input SSBO of shape 1x129x129x4 ?  Then you could eliminate one hidden memcpy inside.\r\n\r\nFrom the graph you shared (btw, nice visualization; appreciate that), it indeed looks like everything would be handled until the last ResizeBilinear.  The shape of it is also not too bad (129x129x2), in terms of, it has too many channels etc.  So I wouldn't expect any slow down.\r\n\r\nDid you properly call `BindGlBufferToTensor` before `ModifyGraphWithDelegate`?  Can you share the shader code that converts your texture to SSBO?  I was doing something like:\r\n\r\n>        #version 310 es\r\n>        layout(local_size_x = 16, local_size_y = 16) in;\r\n>        layout(binding = 0) uniform sampler2D input_texture;\r\n>        layout(std430) buffer;\r\n>        layout(binding = 1) buffer Output { float elements[]; } output_data;\r\n>        void main() {\r\n>          ivec2 gid = ivec2(gl_GlobalInvocationID.xy);\r\n>          if (gid.x >= 224 || gid.y >= 224) return;\r\n>          vec3 pixel = texelFetch(input_texture, gid, 0).xyz;\r\n>          int linear_index = 3 * (gid.y * 224 + gid.x);\r\n>          output_data.elements[linear_index + 0] = pixel.x;\r\n>          output_data.elements[linear_index + 1] = pixel.y;\r\n>          output_data.elements[linear_index + 2] = pixel.z;\r\n>        }\r\n\r\nfor MobileNet.  Might not be directly applicable, but you roughly get the idea...", "Not officially announced yet, but FYI: GPU code is now visible at:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/gpu\r\n\r\nif you need the code for better insight what is happening.", "Hi @impjdi ,\r\n      Can you just share the sample classification app using ssbo or atleast the opengl related code?\r\nWe used the following shader code based on your inputs.But we encountered some [errors](https://stackoverflow.com/questions/55438256/unable-to-compile-opengl-fragment-shader-in-android-error-07-gl-globalinvo) related to shader version, which we could not resolve being opengl beginners.\r\n\r\n```\r\n#version 310 es\r\nlayout(local_size_x = 16, local_size_y = 16) in;\r\nlayout(binding = 0) uniform sampler2D u_Texture0;\r\nlayout(std430) buffer;\r\nlayout(binding = 1) buffer Output { float elements[]; } output_data;\r\nvoid main() {\r\n    ivec2 gid = ivec2(gl_GlobalInvocationID.xy);\r\n    if (gid.x >= 257 || gid.y >= 257) return;\r\n    vec3 pixel = texelFetch(u_Texture0, gid, 0).xyz;\r\n    int linear_index = 3 * (gid.y * 257 + gid.x);\r\n    output_data.elements[linear_index + 0] = pixel.x;\r\n    output_data.elements[linear_index + 1] = pixel.y;\r\n    output_data.elements[linear_index + 2] = pixel.z;\r\n}\r\n```\r\n\r\n```\r\nmTextureUniformHandle0 = GLES31.glGetUniformLocation(mProgramHandle,\r\n\t\t\t\t\"u_Texture0\");\r\n// Set the active texture0 unit to texture unit 0.\r\n\t\tGLES31.glActiveTexture(GLES31.GL_TEXTURE0);\r\n\r\n\t\t// Bind the texture to this unit.\r\n\t\tGLES31.glBindTexture(GLES31.GL_TEXTURE_2D, mTextureDataHandle0);\r\n\r\n\t\t// Tell the texture uniform sampler to use this texture in the shader by\r\n\t\t// binding to texture unit 0.\r\n\t\tGLES31.glUniform1i(mTextureUniformHandle0, 0);\r\n```\r\n\r\n```\r\n\tpublic int[] initializeShaderBuffer(){\r\n\t\tandroid.opengl.EGLContext eglContext = eglGetCurrentContext();\r\n\t\tint[] id = new int[1];\r\n\t\tGLES31.glGenBuffers(id.length, id, 0);\r\n\t\tGLES31.glBindBuffer(GLES31.GL_SHADER_STORAGE_BUFFER, id[0]);\r\n\t\tGLES31.glBufferData(GLES31.GL_SHADER_STORAGE_BUFFER, 257*257*3*4, null, GLES31.GL_STREAM_COPY);\r\n\t\tGLES31.glBindBufferBase(GLES31.GL_SHADER_STORAGE_BUFFER,1,id[0]);\r\n\t\tGLES31.glBindBuffer(GLES31.GL_SHADER_STORAGE_BUFFER, 0);// unbind\r\n\t\treturn id;\r\n\t}\r\n```\r\n", "@anilsathyan7\r\n\r\nI am out of office on vacation this week with limited network access and there's a good chance I'll forget about this.  Could you please nudge me again next week?", "Sure porygon  ...\ud83d\ude09", "Hi @impjdi ,\r\n                     Can you help us with the ssbo tflite inferecne issue?? We could not run the tflite inference using ssbo in android.Can you just share the sample classification app using ssbo or atleast the opengl related code?How much speed up can we expect in this scenario?", "Hi @impjdi ,\r\nI'll second a request for a demo illustrating SSBO inference.\r\n\r\nMaybe I should open a separate issue... We're attempting to use a GLSurfaceView in our app, alongside the tflite GPUDelegate. Our renderer works fine until `interpreter.modifyGraphWithDelegate(delegate);` is called, which results in a black screen. No glErrors are produced. Its difficult to understand how commenting/uncommenting the above line changes the behaviour, even after looking at the newly released GPU delegates source.\r\n\r\nA working example might clear things up...\r\n\r\nThank you!", "@anilsathyan7 \r\n\r\nHeh, I missed the porygon part earlier :)\r\n\r\nThe below is in C++, but should be similar in Java too.\r\n\r\n        glActiveTexture(GL_TEXTURE0 + 0);\r\n        glBindTexture(GL_TEXTURE_2D, /*your gl texture that has the image*/);\r\n        glBindBufferRange(GL_SHADER_STORAGE_BUFFER, 1, /*your ssbo*/, 0, /*size in bytes*/);\r\n        glUseProgram(/*the program above*/);\r\n        glDispatchCompute(width / 16, height / 16, 1);  // these are work group sizes\r\n        glBindBuffer(GL_SHADER_STORAGE_BUFFER, 0);  // unbind\r\n        glBindTexture(GL_TEXTURE_2D, 0);  // unbind", "@ktgordon \r\n\r\nHm, the only official example code is the TFLite demo app that is in the TF repository.  As an Android app consists of a lot more than just a single Java file, that'd be difficult unless I start up a whole new git repo with the files.  Unfortunately, on top of that, I'm not a real mobile app developer; I do most of my stuff in Android C++ without cameras.  I'll see whether I can cook up a C++ binary that can do all this in a single C++ file =/  That discussion aside...\r\n\r\n`modifyGraphWithDelegate` hanging sounds like you have an issue somewhere else.  Make sure that your `TfLiteGpuDelegateBindBufferToTensor` is called before `modifyGraphWithDelegate`, and that your SSBO is already created.  The flow of the program with `modifyGraphWithDelegate` is as follows:\r\n\r\n`Interpreter.modifyGraphWithDelegate` (Java)\r\n`Interpreter::ModifyGraphWithDelegate` (C++)\r\n`tflite::gpu::gl::(anonymous)::DelegatePrepare` (C++)\r\n`tflite::gpu::gl::(anonymous)::Delegate::Prepare` (C++)\r\n\r\nYou can probably trace back what is causing the hanging.", "@anilsathyan7 \r\n\r\nDid things work out?  Can this issue be closed?", "The code is working fine; but we are not able to get correct output using the ssbo as input.The output seems to be black (i.e output is all zeroes).We are not able to ensure that data is correctly copied into ssbo or whether it is correctly accessed by tensorflow; even though it is running without errors.It seems there is no way to debug and see shader codes (GLSL) in android.", "Attached with this is the logfile containing the errors when we tried to use SSBO with the tflite model.\r\nThe code works properly in Mobiles with Adreno-GPU without any errors but no output is visualized. But in phones with Mali-GPU, there are some issues even before the model comes into picture.\r\n\r\nThe errors vary between Mali Devices, whereas the output is not getting visualized in Adreno Devices. \r\nThe devices used in the below testing are:\r\n\r\n**Mali** (Error logs are attached with the issue: mali-gpu-ssbo-errorlog.txt)\r\n_Samsung A8+_\r\n_Honor Play_\r\n_Moto C plus_\r\n\r\n**Adreno** (Error Logs are attached: adreno-gpu-ssbo-errorlog.txt)\r\n_Poco F1_\r\n\r\n[mali-gpu-ssbo-errorlog.txt](https://github.com/tensorflow/tensorflow/files/3103167/mali-gpu-sbbo-errorlog.txt)\r\n\r\n[adreno-gpu-ssbo-errorlog.txt](https://github.com/tensorflow/tensorflow/files/3103182/adreno-gpu-ssbo-errorlog.txt)\r\n\r\n@impjdi Could you have a look at it.. And it would be better if you could share with us the working app code for reference.", "@impjdi Any updates on SSBO??", "> Hi @impjdi ,\r\n> I'll second a request for a demo illustrating SSBO inference.\r\n> \r\n> Maybe I should open a separate issue... We're attempting to use a GLSurfaceView in our app, alongside the tflite GPUDelegate. Our renderer works fine until `interpreter.modifyGraphWithDelegate(delegate);` is called, which results in a black screen. No glErrors are produced. Its difficult to understand how commenting/uncommenting the above line changes the behaviour, even after looking at the newly released GPU delegates source.\r\n> \r\n> A working example might clear things up...\r\n> \r\n> Thank you!\r\n\r\n@ktgordon Have you found a resolution/workaround for this issue?  I am experiencing exactly the same problem.  After calling modifyGraphWithDelegate(), all glDraw calls results in black.  Does not even need to associate SSBO buffer to TFLite Tensors.  This is strange.  Taking a deeper look as well.\r\n\r\n", "We did find a workaround. I'm assuming you're using the Java API and bringing in gpu delegates via \r\n`implementation 'org.tensorflow:tensorflow-lite:0.0.1-gpu-experimental'`\r\n\r\nWhat I think is happening is that modifyGraphWithDelegate() modifies the current context so that our display surface is no longer current... not a problem if we had access to our original state variables. However, since we originally tried using GLSurfaceView we didn't have access to any of these variables. In effect modifyGraphWithDelegate made changes to the gl state we couldn't recover from.\r\n\r\nSwitching from GLSurfaceView to TextureView gave us more control at the cost of more complexity. We created a dummy context, initialized our interpreter and called modifyGraphWithDelegate(), then created a new shared context with the dummy context. This way we could make our display surface current and render to it.\r\n\r\nManaging the egl context was handled by reusing code from [Grafika](https://github.com/google/grafika).\r\n\r\nThis got us passed the black screen problem anyways...\r\n\r\n", "I am doing exactly what you said here as I based on TFLite demo (which uses TextureView).  Mainly the following:\r\n1. Create gl context, set gl viewport, etc.  Stores eglDisplay, eglSurface, eglContext.\r\n2. Make call to `modifyGraphWithDelegate()`.\r\n3. Set the eglContext, eglSurface, eglDisplay as current using `eglMakeCurrent`\r\n\r\nThe draws using `glDrawArrays`, results in black.  Interestingly, if step 1 & step 2 is swapped in sequence, everything works.\r\n\r\nThe Grafika code was also referenced as well.  \r\n\r\nWill try to setup a dummy context next...\r\n", "Hi @ktgordon , @gnsmrky , \r\n                      Are you suggesting that ssbo method would not work with normal GLSurfaceView? What about something like GLTextureView( [link1](https://github.com/ykulbashian/LiquidSurface/blob/master/liquidview/src/main/java/com/mycardboarddreams/liquidsurface/GLTextureView.java), [link2](https://github.com/google/grafika/blob/master/app/src/main/java/com/android/grafika/TextureViewGLActivity.java))?\r\n\r\nFinally, are you able to achieve any speedup compared to normal GPU inference? If so, can you share a basic working demo app? Just to clear things up ...", "@ktgordon Just got it working!  Indeed, the dummy shared context is the key to make it work.  I guess the GLES context setting/switching can be a lot more complicated than one can imagine...\r\n\r\n@anilsathyan7 I based on the [TFLite demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo), which is the main sample project that [TFLite GPU delegate page](https://www.tensorflow.org/lite/performance/gpu) provides.  This sample project uses `TextureView`.  Don't know if SSBO works with other surface types.  I would imagine it should as `eglCreateWindowSurface()` takes `SurfaceView`, `SurfaceTexture`, `SurfaceHolder` or a `Surface`,  according to [Android eglSurface doc](https://source.android.com/devices/graphics/arch-egl-opengl#egl_surface).  GLTextureView from your link extends SurfaceTexture, should work as well.\r\n\r\nThe performance gain is **significant**.  I was trying a 448x448 image.  (Trying a larger image to amplify the copy time).  The time it takes w/o SSBO/Image2D copy shader is around 900ms on a Snapdragon 808.  Using copy shader the time comes down to < 20ms!\r\n\r\n", "@gnsmrky  Could you share your repo, so that it could be a better thing for everyone to start exploring ssbo with that. ", "> @gnsmrky Could you share your repo, so that it could be a better thing for everyone to start exploring ssbo with that.\r\n\r\n@SanthoshRajendiran Trying to find the time to do that. The code is very messy now and unreadable. Will get it cleaned up as soon as I get spare cycles. ", "@gnsmrky @impjdi Any updates on the repo?? Can you provide some code fragments on where the changes need to be incorporated in the mobile app?? ", "@gnsmrky  thank you so much for your efforts. let us know when you are adding sample code here. ", "@SanthoshRajendiran @soham24 Plan to publish the repo over the weekend.  Still doing some tweaks.  :)", "@SanthoshRajendiran \r\n\r\nI bubbled up this request in last couple of meetings.  The example will be added to the TFLite demo app, but I have some deadline coming up, so it will be a couple months until I can get to it :(", "@SanthoshRajendiran @soham24 @impjdi \r\nI just put up the repo at [tensorflow-lite-ssbo Android classifier demo](https://github.com/gnsmrky/tensorflow-lite-ssbo/tree/master/tensorflow/lite/java/demo).  Should just open the project in Android Studio to build and run.  Once in the app on the phone, select `GPU` and `mobilenet v1 float` to see the `copy time` for the time it takes to copy a frame to SSBO.\r\n\r\nThe code is still very rough.  But should serve the purpose to get started playing around SSBO in TFLite GPU delegate.\r\n\r\nOn my LG G4 (Android M, Snapdragon 808), the time it takes to copy a 224x224 pixel buffer is reduced significantly.  From 180ms ~ 200ms (Java ByteBuffer `putFloat()` copy), down to 1ms (shader + SSBO).  As LG G4 is a relatively an old phone (> 5 years now), the time it saves on a more recent phone may not be as significant.  But really, if G4 can do a frame copy in < 1ms, surely any other Android phone can do better.  :)\r\n\r\nBasically what it does is the following:\r\n1. Initialize GLES Context A (`eglContext`).\r\n2. Create a surface texture for camera.\r\n3. Create SSBO\r\n4. Create compute shader needed to copy surface texture to SSBO.\r\n5. Initialize GLES Context B (`gpuContext`) with Context A being shared.\r\n6. Call modifyGraphWithDelegate()\r\n7. Do proper context switching using eglMakeCurrent()\r\n     - Switch to Context A when camera -> surface texture -> SSBO.\r\n     - Switch to Context B when calling TFLite `Interpreter.run()`.\r\n\r\nNote: I didn't create a separate thread to simplify the process.  Usually Context A & B should be in 2 separate threads, so eglMakeCurrent() is called only once in a thread.\r\n\r\nHaven't got the time to put up a readme.  Just take a look into the [commit](https://github.com/gnsmrky/tensorflow-lite-ssbo/commit/fe228c6a1f266c6276f91b6201747735210beaeb).  Should be fairly straightforward to figure out what's in there.  Hope this helps to clarify a few things about TFLite + GPU delegate + SSBO.\r\n\r\nLet me know if it works out for you guys...\r\n", "@gnsmrky Congrats and thanks for the amazing work on SSBO. We tried out the application in some of our mobile phones. The working methodology on various phones is discussed below:\r\n\r\n1) Oneplus 3 - Model running time is around 40ms, the same as without SSBO. Copy time is around 0 or 1 in all cases\r\n2) Poco F1 - Model running time is around 25ms, But we are not able to get the actual output from the app.\r\n3) Samsung A8+, Honor Play - The apps are crashing with linkage error, saying maximum number of work group invocations. We modified the sizes for work group to 8, and we obtained a model running time of 5ms, but we were not able to get proper output from the model.", "@gnsmrky thank you so much. I will let you know about working after implementation.", "@gnsmrky Great work. thank you so much!  Did you try deeplab segmentation model?", "> @gnsmrky Congrats and thanks for the amazing work on SSBO. We tried out the application in some of our mobile phones. The working methodology on various phones is discussed below:\r\n> \r\n> 1. Oneplus 3 - Model running time is around 40ms, the same as without SSBO. Copy time is around 0 or 1 in all cases\r\n\r\nSo it is only working and have proper output on OnePlus 3 among these phones?  Let me see if I can get hold of a Snapdragon 845 phone. \r\n", "> @gnsmrky Great work. thank you so much! Did you try deeplab segmentation model?\r\n\r\n@junhwanjang I haven't tried deeplab yet.  But I did try the output SSBO with other models, which works correctly as well.  Does deeplab work with GPU Delegate fully yet, do you know?\r\n", "@SanthoshRajendiran I just updated the repo.  Seems like compute shader needs a real on-display surface on some devices.  I added a 1dp x 1dp view to the asset to associate it with gles surface.  Can you give the updated repo another try on your phones again?\r\n\r\nHere is the latest [commit](https://github.com/gnsmrky/tensorflow-lite-ssbo/commit/912c37dd456374407373f7665985c19cdccd45a5).\r\n\r\nBTW, the Cam -> SSBO copy does not take transformation from `updateTexImage()` into account.  You may need to position your phone counter clock-wise (i.e. bottom of the phone points to the right) to have correct inference result.\r\n", "@gnsmrky Thanks for the update. With POCO F1 (Adreno 630, Snapdragon 845), the output is coming now around a speed of 20-30ms and copy time is around 0-1ms. \r\nStill the problem persists in Mali GPU Devices (tested on Honor Play)\r\nAttached below is the error log with Honor Play:\r\n\r\n[mali-ssbo-android-errorlog.txt](https://github.com/tensorflow/tensorflow/files/3224208/mali-ssbo-android-errorlog.txt)\r\n", "@SanthoshRajendiran Have you tried setting work group to 8, or even 4, for Mali devices?  Here are the 2 lines you should change `16` to `8` or `4`. \r\n[local_size in compute shader @ L1092](https://github.com/gnsmrky/tensorflow-lite-ssbo/blob/912c37dd456374407373f7665985c19cdccd45a5/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/Camera2BasicFragment.java#L1092)\r\n[glDispatchCompute @ L1162](https://github.com/gnsmrky/tensorflow-lite-ssbo/blob/912c37dd456374407373f7665985c19cdccd45a5/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/Camera2BasicFragment.java#L1162)", "@gnsmrky We tried setting work groups as both 4 and 8 in Samsung A8+, the model is not running properly even when we tried it in landscape mode. When we change the work groups, the app crashes within some time due to GL out of memory error.\r\n\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: CameraBackground\r\n    Process: android.example.com.tflitecamerademo, PID: 23378\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Next operations are not supported by GPU delegate:\r\n    SQUEEZE: Operation is not supported.\r\n    First 29 operations will run on the GPU, and the remaining 2 on the CPU.TfLiteGpuDelegate Invoke: [GL_OUT_OF_MEMORY]: There is not enough memory left to execute the command.Node number 31 (TfLiteGpuDelegate) failed to invoke.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:149)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:249)\r\n        at com.example.android.tflitecamerademo.ImageClassifierFloatMobileNet.runInference(ImageClassifierFloatMobileNet.java:101)\r\n        at com.example.android.tflitecamerademo.ImageClassifier.classifyFrameSSBO(ImageClassifier.java:167)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrameSSBO(Camera2BasicFragment.java:967)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.access$1200(Camera2BasicFragment.java:91)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment$8.run(Camera2BasicFragment.java:785)\r\n        at android.os.Handler.handleCallback(Handler.java:873)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:214)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\nI/Process: Sending signal. PID: 23378 SIG: 9", "@gnsmrky By default in models like deeplab (models not fully capable of running in GPU), there is a fallback happening in GPU Delegate from GPU to CPU. Does this behavior change in SSBO and how do we get the data if it is falling back to CPU? ", "@gnsmrky  thanks. this worked like charm on low-end devices. \r\nOne question, It's true that we have to rotate the phone counter-clockwise. Can I add the rotating logic in the shader.\r\nref link:  https://stackoverflow.com/questions/28074977/rotating-a-texture-on-a-fragment-shader-in-glsl-es", "@gnsmrky Could you give insight on what changes have to be done in your application code if I want to get an image output from the tflite model, with respect to SSBO.", "> @gnsmrky We tried setting work groups as both 4 and 8 in Samsung A8+, the model is not running properly even when we tried it in landscape mode. When we change the work groups, the app crashes within some time due to GL out of memory error.\r\n>\r\n\r\n@SanthoshRajendiran I just updated the repo with few tweaks.  Should lower the memory requirement a big.\r\n1. Use FP16 precision.\r\n2. Use `8` as workgroup size.\r\n3. Add a check for SSBO buffer size upon creation.\r\n\r\nThe `SQUEEZE` error you are seeing may due to the failure when creating SSBO buffer.  Are you running the repo as-is?\r\n\r\nLet me know if the updated repo works out for you.\r\n\r\n", "> @gnsmrky By default in models like deeplab (models not fully capable of running in GPU), there is a fallback happening in GPU Delegate from GPU to CPU. Does this behavior change in SSBO and how do we get the data if it is falling back to CPU?\r\n\r\n@SanthoshRajendiran The SSBO in the repo is only for input buffer.  Nothing is changed for output buffer.  So the code for getting the output data should be the same way as you do with CPU (i.e. ByteBuffer).\r\n\r\nI haven't got my hands on deeplab yet.  Do you know which op it is causing the CPU fallback?\r\n\r\n\r\n", "> @gnsmrky thanks. this worked like charm on low-end devices.\r\n> One question, It's true that we have to rotate the phone counter-clockwise. Can I add the rotating logic in the shader.\r\n> ref link: https://stackoverflow.com/questions/28074977/rotating-a-texture-on-a-fragment-shader-in-glsl-es\r\n\r\n@soham24 The transformation happens when you use the regular glViewPort, glDraw, etc. with corresponding vertexfragment shader.  The SSBO code in the repo is a simple memory float copy and does not involve any vertex/fragment shader.  If we do the transformation on per-float basis, it will most likely slow things down.  \r\n\r\nThe best way to do it is to \"draw\" the camera texture to another texture, with the desired transformation, and then do texture -> SSBO copy.  That would take some efforts.  Will need to find more time to do that.\r\n\r\n\r\n", "> @gnsmrky Could you give insight on what changes have to be done in your application code if I want to get an image output from the tflite model, with respect to SSBO.\r\n\r\n@SanthoshRajendiran What do you want to do with the image output?  Creating an SSBO and bind it to TFLite GPU delegate is as easy as creating one and call `bindGlBufferToTensor()` to the output tensor `getOutputTensor()`, as says in [GPU Delegate document](https://www.tensorflow.org/lite/performance/gpu_advanced#android).", "> > @gnsmrky thanks. this worked like charm on low-end devices.\r\n> > One question, It's true that we have to rotate the phone counter-clockwise. Can I add the rotating logic in the shader.\r\n> > ref link: https://stackoverflow.com/questions/28074977/rotating-a-texture-on-a-fragment-shader-in-glsl-es\r\n> \r\n> @soham24 The transformation happens when you use the regular glViewPort, glDraw, etc. with corresponding vertexfragment shader. The SSBO code in the repo is a simple memory float copy and does not involve any vertex/fragment shader. If we do the transformation on per-float basis, it will most likely slow things down.\r\n> \r\n> The best way to do it is to \"draw\" the camera texture to another texture, with the desired transformation, and then do texture -> SSBO copy. That would take some efforts. Will need to find more time to do that.\r\n\r\nThanks @gnsmrky . It will be great if you update the sample with desired transformation.   \r\n ", "@gnsmrky We figured out the issue with Squeeze operation not getting supported. It is because, by default the Squeeze operation is not working in GPU on Mali Devices (verified with benchmark tool). Hope, we will open a separate issue for that, or since @impjdi   is linked with the thread, he will handle that.. Other than that, the repo works as it is... In our case, we are handling a full GPU supported model and getting an image output to be rendered on to the surface, and so, we are going on with the SSBO output too..", "@SanthoshRajendiran  I have a doubt. Are you resizing input size texture before passing it to tflite?\r\nop will be resized. How you will render it directly via texture? ", "@soham24 Input to the model we are resizing in order to make sure the model is running.. The output of the model we will resize it to the desired size that we will need to render. ", "> @soham24 Input to the model we are resizing in order to make sure the model is running.. The output of the model we will resize it to the desired size that we will need to render.\r\n\r\n@SanthoshRajendiran It sounds odd to me as well.  What I was trying to say is that if the size is not correct for SSBO, GPU delegate will then say SQUEEZE has problem, even though it is not the case.\r\n\r\n", "> Thanks @gnsmrky . It will be great if you update the sample with desired transformation.\r\n\r\nWork in progress, albeit very slowly...", "In the current version of the app, it is developed with EGL Surface. We tried using GL Surface View, but it is not working. Is there any work around that can be done to facilitate the ssbo output to be rendered directly on a GL Surface View? ", "@gnsmrky We tried figuring out Output SSBO, but we are unable to do it correctly.. Could you tell us the exact places we need to make the changes in order to make it working.\r\n\r\nBasically, we made these modifications.\r\n1) Initialized tflite instance by setting setAllowBufferHandleOutput(true) as per the tflite gpu documentation.\r\n2) Binded buffer output to model SSBO using gpuDelegate.bindGlBufferToTensor(outputTensor, outputSsboId);\r\n\r\n3) Rendered the output on the mobile screen.\r\n\r\nCould you check if SSBO output is working in your case.. Or some changes that were done previously like rotating the screen or something is needed now too to visualize the output on screen..\r\n\r\nHerewith, I have attached the tflite we used for testing output SSBO, wherein we are not doing anything, but just resizing an image from 197 to 257 using a ResizeBilinear operation. \r\n\r\n[just_resize_ssbo.tflite.zip](https://github.com/tensorflow/tensorflow/files/3266327/just_resize_ssbo.tflite.zip)\r\n", "> Could you check if SSBO output is working in your case.. Or some changes that were done previously like rotating the screen or something is needed now too to visualize the output on screen..\r\n> \r\n\r\n@SanthoshRajendiran I did not do anything for output SSBO in the repo I posted here.  But I output SSBO does work.  So it may be something in your shader code that moves data from SSBO to texture buffer for drawing on screen.\r\n\r\nWhat I would suggest is to try out an op that does not change any shapes.  `sqrt` op as one example, which is an unary op that does not change tensor shape.  Fill in values that's predictable, say `100`, the result should be `10`.  That was how I worked on both input/output SSBO at the beginning.  \r\n\r\nMost problems I ran into was not on TFLite GPU delegate part of the code, but on OpenGL ES in Android.  Just needed to dissect the code piece by piece to get it to work correctly from SSBO to screen.\r\n\r\nHope this helps...\r\n\r\nBTW, try not to use Bilinear Resize with non-integral resizing first.  Try something like `2` as scaling factor.   So `157` will be resized to `314`.  It may help...\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Hello, @gnsmrky:\r\n I test your code in two different devices and it seems to use GPU only randomly. Most of the time while in GPU mode it does nothing. I tried less working groups (8 or 4) but doesn't make any difference...\r\n\r\n Do you have any idea about why this is happening?\r\n\r\n Thanks in advance.", "> I test your code in two different devices and it seems to use GPU only randomly. Most of the time while in GPU mode it does nothing. I tried less working groups (8 or 4) but doesn't make any difference...\r\n\r\n@jsolves Can you elaborate more?  Did you mean Camera --> SSBO does not work, or GPU delegate?  How did you observe whether it works or not?\r\n", " I hope i could tell you more. But every mode in the app works correctly until it goes GPU. Most of the time, classifies everything as 0% or near 0% and device gpu utilization doesn't goes up. Only in a few ocassions gpu classification goes well (and gpu utilization goes up, accordingly).\r\n\r\n I tried other \"gpu apps\" and they worked as intended. I don't know how to determine if the problem is Camera-->SSBO or GPU delegate or something related with shaders. Do you know anything I can try to see if the problem is one of those things?\r\n\r\n Thanks for your answer.", "> I tried other \"gpu apps\" and they worked as intended. I don't know how to determine if the problem is Camera-->SSBO or GPU delegate or something related with shaders. Do you know anything I can try to see if the problem is one of those things?\r\n\r\n@jsolves What you can definitely try is the original [Tensorflow Lite Android repo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo), which already has GPU supported.  My SSBO repo only adds the camera --> SSBO based on this repo.  You can see if GPU in the TFLite Android repo has faster inference time.\r\n\r\n", "Ah, sorry, I'm a little tired with this problem. Yes, GPU Delegate works correctly in original repo and in my own custom apps. It gives faster inference time than CPU inference.\r\n\r\nSo the problem is in the camera->SSBO part, then?", "> So the problem is in the camera->SSBO part, then?\r\n\r\n@jsolves The main purpose for SSBO is to reduce the pixel copy time from Camera to input SSBO for TFLite.  GPU inference time should not be affected at all.  \r\n\r\nDo you see \"copy time\" when you run the app when running in GPU mode?  \r\n\r\nAlso, how did you check GPU utilization?  Are you getting expected inference output when GPU utilization is low?\r\n", " Yes, I know. In GPU (with SSBO) the copy time is very low (0 - 2) but there is no right classification (it  goes random values or all 0s) most of the time.\r\n\r\n In device configuration, there is an option like \"show gpu utilization\" and in the few times that the app works fine (with GPU), that gpu indicator goes up.\r\n\r\n It's like if the image camera doesn't always go to SSBO or some initialization trouble. But my Android-Fu isn't strong enough to get it... :(", "@jsolves As I tested gpu inference with basic 3-channel input model, I couldn't get right results. \r\nHowever, when I changed model with 3-channel into fake 4-channels input (using new Input and strided_slice ops), finally get right results :)\r\n>> https://www.tensorflow.org/lite/performance/gpu_advanced#tips_and_tricks ", "Intriguing. How do you make that \"fake 4-channels\", setting the \"fake\" alpha to 1 in every pixel?\r\n\r\nThanks in advance.", "`input_shape = (224, 224, 4)`\r\n`inputs = Input(input_shape, dtype=np.float32)`\r\n`x = Lambda(lambda x: x[:, :, :, :3])(inputs)`\r\n`model_pre = Model(inputs, x)`\r\n`model_pre.summary()`\r\n`sess_fake = K.get_session()`\r\n`graph_def_fake = sess_fake.graph_def`\r\n`nodes_fake = [n for n in graph_def_fake.node]`\r\n\r\nI converted the model as follows.\r\n1. Create fake inputs (including strided slice operation)\r\n2. Change previous 3-channel input into fake inputs as above in graph (should aware of previous input names if possible)\r\n3. Convert TFLite model\r\n", "I think https://mediapipe.dev does this. ", "@soham24 I went through mediapipe, but could not understand how this works. The tflite provided by mediapipe team, has ops that are not supported by TFLite-GPU, nor those were even tensorflow operations as it is. Can anyone provide suggestions on how to train the segmentation model based on the mediapipe architecture.", "@SanthoshRajendiran Even I am trying to figure out pipeline by looking at mediapipe code. \r\nIT will be great if guys at tf help us ", "@SanthoshRajendiran @soham24 \r\n\r\nYes, MediaPipe probably uses all features of the GPU delegate and is a good place to start (I used to work on MediaPipe a couple years ago :D).  I agree that the GPU path is not super easy to read, but is still a decent place to start.  If you look at the `TfLiteInferenceCalculator`, first of all, you will see tons of `RunInGlContext` thing, that ensures you stay in the same GL context.  Then, all it really does is, copy input SSBO, run inference, and copy output SSBO.  I think there is still room for improvement, which is going to happen very soon(tm).  Well, that's on my plate for next 3 months :P \r\n\r\nFor the segmentation model, you want to check in the MediaPipe github page and ping those guys.", "Can we have update on this?\r\n", "Ummm, can you elaborate what kind of update you expect?  Do you want us to walk through another open source software?", "I\u2019ve tried to associate my custom tflite model to SSBO in android as @gnsmrky did,  but I couldn\u2019t make it work so far.\r\n(By the way, the latest tflite seems not to support bindGlBufferToTensor but the official tflite gpu delegate document still introduces bindGlBufferToTensor in using SSBO.)\r\nAnyway I\u2019ve built tensorflow from https://github.com/gnsmrky/tensorflow-lite-ssbo and managed to run the image classification demo with SSBO. Even if it shows different results compared to CPU version and official GPU version without SSBO, it\u2019s at least working--it has prediction values and copy time has reduced.\r\nBut as I changed the provided mobilenet model to my custom model (I\u2019ve tried even a very simple model with add operation only), it looks like working but the output gets all zeroes, or it sometimes produces error that Tensor is not bound to a buffer handle depending on the used model. \r\nSince I\u2019ve tried models with the same 224x224x3 input as the original demo and changed nothing except for the model path, I\u2019d like to know if there is any other modification I should take care of when I change or make a model.\r\nBelow are some examples of the simple models I\u2019ve tried. (Visualized by Netron)\r\n![image](https://user-images.githubusercontent.com/60721476/73905199-dc5ac880-48e1-11ea-9ed6-dbca510893c4.png)\r\n![image](https://user-images.githubusercontent.com/60721476/73905209-e250a980-48e1-11ea-9263-f1cd0a64fbd1.png)\r\nIt would be great if TensorFlow offers an official SSBO demo with the latest tflite.", "tensorflow-lite-ssbo/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java:212: error: cannot find symbol\r\n        gpuDelegate.bindGlBufferToTensor(inputTensor, inputSsboId);\r\n                   ^\r\n  symbol:   method bindGlBufferToTensor(Tensor,int)\r\n  location: variable gpuDelegate of type GpuDelegate\r\n@jmhodges @gnsmrky ", "I don't work in Java lands, and thus I don't know which delegate Java APIs are using, but bindGlBufferToTensor got renamed in the deprecated GL delegate, and removed in the new GPU delegate.  Check out `//tf/lite/delegates/gpu/gl_delegate` & `//tf/lite/delegates/gpu/gpu_delegate`.", "@impjdi @jmhodges @gnsmrky \r\nMy model input pixel value is float with range 0.0 - 1.0(1.0/255) can use ssbo?\r\n", "How to dump a ssbo buf to cpu for evaluating?", "You're looking for `glMapBufferRange`", "![image](https://user-images.githubusercontent.com/17869361/78094522-4df95380-7407-11ea-95f0-df2752e60310.png)\r\nWhy transformedData print out zero value after glDispatchCompute invoked \r\nWhere is wrong?\r\nI need to inpect the ssbo value after copyCamtextToSsbo if has an better method?\r\n@impjdi @svenstaro @bmabey ", "Not super familiar with Java `ByteBuffer` and `FloatBuffer`, but aren't you missing a `glFinish` before you start reading from the memory location?", "@impjdi @svenstaro @ktgordon @SanthoshRajendiran @gnsmrky \r\nYou can think FloatBuffer as a float * pointer(buffer) in c++\r\nI have tried with glFinish but the same result \r\nbut if I add follow code:\r\nGLES31.glBufferData(GL_SHADER_STORAGE_BUFFER, ssboSize, ssboData, GL_STREAM_COPY);\r\nI can get ssboData content with glMapBufferRange why?\r\n![image](https://user-images.githubusercontent.com/17869361/78121376-e5799900-743d-11ea-99a6-68836da0bb0a.png)\r\n![image](https://user-images.githubusercontent.com/17869361/78121408-ef030100-743d-11ea-85c2-1efd7a902fb6.png)\r\n![image](https://user-images.githubusercontent.com/17869361/78122191-faa2f780-743e-11ea-93b9-ea66fd25c488.png)\r\nI want to evaluate out_data.elements content is correct? after dispatchcomute is done.\r\nI have googled for two days but not found solution for it", "> GLES31.glBufferData(GL_SHADER_STORAGE_BUFFER, ssboSize, ssboData, GL_STREAM_COPY);\r\n> I can get ssboData content with glMapBufferRange why?\r\n\r\nNot talking about the \"why\" part, but isn't your problem solved if you can access `ssboData`?\r\n\r\nI also remember that I couldn't find enough examples on the web to make reasonable progress.  What you're asking right now seems slightly out of scope for TFLite GPU support, as you're asking pure OpenGLES compute shader questions.  I suggest asking Khronos forums and/or follow the code paths inside TFLIte GPU and MediaPipe; these two frameworks use SSBO and textures a lot.  I'm sure you will find your use case there.", "@impjdi @svenstaro @ktgordon @SanthoshRajendiran @gnsmrky\r\n\r\nI`m a newbie for opengl es.\r\n\r\nI have viewed MediaPipe and Tflite GPU to try solved it but failed\r\n\r\nI`m curious how to debug compute shader in android to you\r\n\r\nit has few stuff on the network about ssbo\r\n\r\nThe code is provided by https://github.com/gnsmrky/tensorflow-lite-ssbo\r\n\r\nSorry for my english if you do not understand ", "Stop highlighting me.", "@svenstaro Very sorry for disturbing you", "@impjdi \r\nI finally located why glmapbufferrange return all zero. \r\nGL_OES_EGL_image_external_essl3 not working with some android device\r\nhttps://community.arm.com/developer/tools-software/graphics/f/discussions/9432/is-extension-gl_oes_egl_image_external_essl3-not-working-properly-in-compute-shader-on-mali-g71-gpu\r\n\r\n", "Ah, thanks for the update and sharing!", "I followed the official documentation for android for the GPU delegate and got stuck at the bindBuffer step, too.\r\n\r\n> I don't work in Java lands, and thus I don't know which delegate Java APIs are using, but bindGlBufferToTensor got renamed in the deprecated GL delegate, and removed in the new GPU delegate. Check out `//tf/lite/delegates/gpu/gl_delegate` & `//tf/lite/delegates/gpu/gpu_delegate`.\r\n\r\nI checked out the current master and there is no gpu_delegate(.cc?), only a gpu_delegate_jni(.cc). Did you mean that?\r\n\r\nAnyways, I found that TfLiteGpuDelegateBindBufferToTensor seems to be an exported symbol of the library and we can get the native handle of the delegate so we might be able to call that method directly from java.", "Sorry, the last file should have been `//tf/lite/delegates/gpu/delegate.cc`.  We were internally trying to use `bindBuffer` (without the delegate API, but with GPU-internal functions directly) and saw that the new API is a bit broken, so that it's not usable with the new API.  Someone is working on fixing those.  For now, if you want to use `bindBuffer`, I guess you are stuck with the old API, i.e. `gl_delegate`.", "@impjdi Thanks for the update. Does that mean the SSBO route is currently only available with the C bindings or not at all?", "I haven't checked Java, but if Java has migrated to the new API (delegate.cc), your assessment is correct.\r\n\r\nFor C++, it's only available in v1 (gl_delegate.cc), but not in v2 (delegate.cc).\r\n\r\n", "@impjdi is the SSBO bindBuffer issue in v2 delegate resolved? ", "The current plan is not to support bindBuffer in delegate v2.", "@impjdi we have our image frame in GPU memory. Should we move it to CPU just to start inference, which will move it to GPU again? The time spent doing this would waste the benefits of gpu inference in many cases.", "@impjdi Could you share anything information about why bindBuffer will not be supported in delegate v2? I believe that it improves gpu end to end inference time by eliminating memcpy actions. Does tflite team run into some unresolvable issues or the decision is made only by product requirements?", "There are many advanced usages of the mobile GPU inference, and for each of those, GPU delegate needs helper functions like `bindBuffer` because it doesn't fit in the delegate framework.  After adding a bunch of support for extended usages either through the helper functions or options, we decided it's no more maintainable with the combinatoric growth and gives an inconsistent look even within the GPU delegates (OpenCL, OpenGL, Metal, etc.).  Note that, we have to wrap it up with a Java API.  With the majority of the users wanting the GPU delegate as just a quick blackbox accelerator, we made the final decision that the delegate API will stay simple and clean.  For advanced usages that supports a streamlined GPU execution pipeline, we will still have an example code through, e.g. MediaPipe's `TfLiteInferenceCalculator`. Note that it's not there yet, as it still uses the v1 delegate and thus has access to `bindBuffer`.", "@impjdi This information is helpful. Another question is when will MediaPipe delegate v2 integration be released? Thank you.", "Someone's working on it :)", "Has anyone managed to bind the buffer with the v2 delegate?\r\n\r\nIt seems to me that mediapipe is already using it, see [mediapipe/tflite_gpu_runner.h](https://github.com/google/mediapipe/blob/master/mediapipe/util/tflite/tflite_gpu_runner.h) . This runner is used [in the calculator](https://github.com/google/mediapipe/blob/master/mediapipe/calculators/tflite/tflite_inference_calculator.cc#L241) mentioned by impjdi under the `use_advanced_gpu_api_` flag. It replaces the interpreter/delegate flow and uses low level components instead.\r\n\r\nThis is very unfriendly for those who want to have the SSBO utility without maintaining their own interpreter, but going deeper, the bind logic is in [mediapipe/tflite_gpu_runner.cc](https://github.com/google/mediapipe/blob/master/mediapipe/util/tflite/tflite_gpu_runner.cc#L109-L114) and simply calls `InferenceRunner::SetInputObject`.\r\n\r\nThe v2 delegate owns an `InferenceRunner` itself so maybe a small patch to the v2 delegate could add the required `SetInputObject` (or output) call. But I haven't tested, setting this up would be hard for me at the moment.\r\n\r\n@impjdi , any word of guidance would be helpful here. Is this correct? Can we simply patch the v2 delegate with a InferenceRunner::SetInputObject call, and invoke it instead of the v1 bindBuffer? I don't think I'm on the right track, but I do think it would be very useful to the community if we could achieve a patch file and share it here.\r\n", "@natario1 I think @impjdi explained that `bindBuffer` APIs don't fit in the v2 delegation design. The key difference between v1 & v2 delegate is v2 supports both OpenCL and OpenGL backend while v1 only supports OpenGL. This will affect how Tflite handles data ownership exchange. Moreover, many devices on the market don't fully support OpenCL-OpenGL interoperability. I've also tried the `use_advanced_gpu_api_` flag in MediaPipe, the app crashes when I turn it on. So I think it's not a trivial patch for v2 delegate to support `bindBuffer` features. If you need this feature, I think the most simple solution is stick to mediapipe with opengl backend. ", "Thanks for your comment @brucechou1983 . A simpler solution for me is to stick to v1 delegate, but to be honest it doesn't seem like mediapipe runner is doing anything complex/fancy, other than calling `InferenceRunner::SetInputObject` and `InferenceRunner::SetInputObjectDef` when preparing. I understand that it might not be ready yet though, as it is under a flag.\r\n\r\nThe v2 delegate also does the same object/objectdef calls, but the difference is that it uses `ObjectType::CPU_MEMORY` instead of `ObjectType::OPENGL_SSBO` like mediapipe does.\r\n\r\nI don't know what's the support of OpenCL in Android, but OpenGL works just fine, so we could have a flag in v2 delegate options that tells the delegate to **not** try OpenCL and go with OpenGL. It's something that the TF team could add to ease the v1-v2 transition I think, since people who were using v1 likely have a SSBO set up.", "@natario1 If a flag for only using OpenGL is what you need, it's already [there](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/java/src/main/native/gpu_delegate_jni.cc#L40) though it's still experimental. You can set the flag to `TFLITE_GPU_EXPERIMENTAL_FLAGS_GL_ONLY`.\r\n\r\nHowever, when you need realtime (>>30fps) semantic segmentation and/or face mesh running on a $200 dollar phone, choosing a right GPU backend in tflite runtime for efficient execution is really not a trivial problem. I do see the value of using OpenCL for some MALI gpu devices. The `invoke()` execution is 2x-3x faster than OpenGL ES. Although I have to copy the data to/from the tensors, the overall performance is still better. I think tflite team is trying to design the v2 delegate as a blackbox accelerator for general purpose/arbitrary IoT device/easy to use, while creating interfaces for other frameworks like MediaPipe to optimize for specific usage like streamlined GPU execution on mobile/desktop.", "@natario1 I see you did your homework there, good job \ud83d\udc4d \r\n\r\nYou might have noticed, but TFLite is adding bunch of delegates for various accelerators or APIs.  Each of them having custom helper functions didn't help usage, but makes it more confusing for 99% of the users who want to use TFLite GPU delegate just as a magic box doing GPU-accelerated inference.  So the final decision we made was to keep the TFLite GPU delegate as simple as possible, but leave the room open for advanced users who want to do real performant things.\r\n\r\nThe team that delivers TFLite GPU and MediaPipe are sister teams sharing one manager.  Having said that, TFLite GPU won't break MediaPIpe, and that's a guarantee.  And in that sense, going deeper and using advanced internal APIs like `InferenceRunner::SetInputObject` the way MediaPipe uses it is safe.  Of course, because it's not the public API, but an advanced internal one, there might be API changes that may break you every once in a while, but you will always have the MediaPipe's reference implementation.", "I understand the situation @impjdi . Would you consider something like `V2Delegate::GetInferenceRunner()`? <strike>So that we can call `InferenceRunner::SetInputObject` or whatever else from _outside_ the delegate. This makes all the difference, because we'd still have to do our homework for integration and maintenance, but at least we don't have to fork Tensorflow or use a bazel patch, which is honestly a big burden, although MediaPipe helps.\r\n\r\nYou say that `SetInput/OutputObject` and `SetInput/OutputObjectDef` APIs are \"advanced\" and they are to some extent, but at the same time, it makes all the sense that to bind a tensor to \"something\", one has to specify its data layout, size, object type and so on. They're actually very elegant and easy to understand with respect to `BindGlBufferToTensor`, which from my point of view, was just doing some obscure magic under the hood which I couldn't really grasp.\r\n\r\nThese APIs would also be hidden behind the GetInferenceRunner() API, which you could document as a \"use at your own risk\" function, and keep the black-box surface clean. I think this approach would really \"leave the room open\" as you say.</strike> (maybe it would be more work for you than just adding a getter for the inference runner, but you get the point - being able to control the delegate objects from outside)\r\n\r\nApart from this, I'll try to use these low-level APIs this weekend and see if I manage to get v2 working. Thanks for helping!\r\n\r\n**Edit:** After spending the weekend on it I realized this suggestion was not possible, but I hope you can consider something like [what I ended up doing](https://github.com/natario1/tensorflow/commit/7401fbb4fa0c94004865c089d8c89bdd566ad747) which is clean and keeps the delegate header untouched.", "@impjdi any suggestions on how to fix this error? It seems to be an issue with the BHWC > BHWC4 conversion, but I have no clue at how to address it. It happens in ToTensorConverter.\r\n\r\n```\r\nE/tflite:\r\n    TfLiteGpuDelegate Invoke: Missing output in converter\r\n    Node number 1 (TfLiteGpuDelegateV2) failed to invoke.\r\n```\r\n\r\nI create the object def and tensor object as follows:\r\n\r\n```c++\r\n// object def\r\ntflite::gpu::ObjectDef object_def;\r\nobject_def.data_type = tflite::gpu::DataType::FLOAT32;\r\nobject_def.data_layout = tflite::gpu::DataLayout::BHWC;\r\nobject_def.object_type = tflite::gpu::ObjectType::OPENGL_SSBO;\r\nobject_def.user_provided = true;\r\n\r\n// tensor object\r\ntflite::gpu::OpenGlBuffer tensor_object;\r\ntensor_object.id = ssbo;\r\n```\r\n\r\nThen pass both to the delegate before `ModifyGraphWithDelegate`. They are correctly passed to the inference runner and the runner builder, however I get that converter error.\r\n\r\nTF version is 2.2.0 and the model I am using is extremely simple, takes a 400x400x1 image and calculates the average intensity, returning a single float. I am trying to use a SSBO object for the input only.\r\n\r\nAlso I'm running the OpenGL backend, OpenCL not available on my phone.", "After many hours, I think I hit a bug that is still present in 2.2.0, but was fixed in master by these commits: https://github.com/tensorflow/tensorflow/commit/4000a5c75cdbe49d77bcac93a7f21070a31c4cce https://github.com/tensorflow/tensorflow/commit/dffe6a0e810f4c3d9968ddb56fd58c8f405eb846\r\n\r\nFor those who are interested, in short, the fact that I'm using BHWC with 1 color channel (instead of 4), requires the gl engine to do a conversion and this conversion (before https://github.com/tensorflow/tensorflow/commit/4000a5c75cdbe49d77bcac93a7f21070a31c4cce and https://github.com/tensorflow/tensorflow/commit/dffe6a0e810f4c3d9968ddb56fd58c8f405eb846) is completely broken, because `user_provided` is hardcoded to true (https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/lite/delegates/gpu/gl/api2.cc#L595) but when `user_provided` is true, the engine will not bother to create the output GL buffer (https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/lite/delegates/gpu/gl/api2.cc#L199-L202), so the C->C4 conversion can't happen.\r\n\r\nBy cherry-picking https://github.com/tensorflow/tensorflow/commit/4000a5c75cdbe49d77bcac93a7f21070a31c4cce and https://github.com/tensorflow/tensorflow/commit/dffe6a0e810f4c3d9968ddb56fd58c8f405eb846 into v2.2.0 and exposing the necessary APIs, I'm able to do SSBO I/O with the v2 delegate. These commits are pretty old so I hope they can make it into next release.\r\n\r\nThese are the changes I had to make to expose the necessary APIs: https://github.com/natario1/tensorflow/commit/7401fbb4fa0c94004865c089d8c89bdd566ad747 . I don't know C++ so there might be errors, but the point is to create an interface that the V2 delegate extends. This interface can be retrieved from the delegate using a separate C++ header (delegate_core.h) so the high-level delegate is still a black box.", "@anilsathyan7  Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26297\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26297\">No</a>\n"]}, {"number": 26296, "title": "Excpeption in using Tensorflow 1.12 from java on windows 10", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\njar\r\n- TensorFlow version (use command below):\r\n1.12.0\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nI am trying to use tensorflow from java program on windows 10. I have followed all the instruction from here https://www.tensorflow.org/install/lang_java carefully. I am getting following exception:\r\n\r\nMar 03, 2019 5:45:53 PM org.openqa.selenium.remote.ProtocolHandshake createSession\r\nINFO: Detected dialect: OSS\r\nException in thread \"Thread-4\" java.lang.UnsatisfiedLinkError: C:\\Users\\olelo\\AppData\\Local\\Temp\\tensorflow_native_libraries-1551615368173-0\\tensorflow_jni.dll: A dynamic link library (DLL) initialization routine failed\r\n\tat java.lang.ClassLoader$NativeLibrary.load(Native Method)\r\n\tat java.lang.ClassLoader.loadLibrary0(Unknown Source)\r\n\tat java.lang.ClassLoader.loadLibrary(Unknown Source)\r\n\tat java.lang.Runtime.load0(Unknown Source)\r\n\tat java.lang.System.load(Unknown Source)\r\n\tat org.tensorflow.NativeLibrary.load(NativeLibrary.java:101)\r\n\tat org.tensorflow.TensorFlow.init(TensorFlow.java:66)\r\n\tat org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)\r\n\tat org.tensorflow.Graph.<clinit>(Graph.java:361)\r\n\r\n\t\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I think this might be an error caused because of MS Visual C++ 2015 Redistributable package. Try obtaining msvcp140.dll and placing it into a folder in PATH. Maybe that helps.\r\nThere is a similar closed issue : #14456 ", "I tried that. C++ 2015 Redistributable package is installed and msvcp140.dll  is on the path. It does NOT resolve the issue. I have gone through every issue filed about tensorflow and windows. Nothing seems to resolve it.", "You are installing it for CPU Support Only or GPU Support?", "just cpu. Following instructions from here https://www.tensorflow.org/install/lang_java", "@jvishnuvardhan Can someone from TensorFlow team look into this problem? I'm a bit occupied by my own work.", "Same here with 1.13.1 tried to narrow it down with procmon.exe because I also have a working developer machine, and a non working deploy device. Unluckily I could not narrow it down. By the Looks of it all dependant dlls were found. ", "Hello Folks... I could solve this puzzle (at least for my issue).\r\nAfter digging deeper and deeper I found that my processer (on the Target System) does not support AVX, but the library was built with AVX support. Therefore the DLL did not work. The same happend when I built the DLL for myself. \r\nI fixed it Building the DLL myself. During pyhton configure.py I had to set the option /arch:atom otherwise it would assume the AVX capabilities of my buildmachine CPU!\r\n\r\nYou might also checkout here: https://github.com/fo40225/tensorflow-windows-wheel\r\n", "@cogmeta Did you follow @DJSpyman suggestion? Please let us know how it progresses. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 26295, "title": "AttributeError: module 'tensorflow' has no attribute 'uint32'", "body": "import tensorflow as tf\r\ntf.unit32\r\n\r\nit works fine with the tf.unit16 and tf.unit8 but it gives error while accessing tf.unit32\r\nI tried reinstalling tensorflow but it wont work.\r\nI have installed tensorflow using anaconda . then i run the code and i have this problem:\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-16-0a81b7b43fe1> in <module>\r\n----> 1 tf.uint32\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'uint32'\r\n----------------------------------------------------------------------------\r\n\r\ni have also tried ' pip install --upgrade --force-reinstall tensorflow ' as per the answer in\r\nhttps://stackoverflow.com/questions/51654346/attributeerror-module-tensorflow-has-no-attribute-float32\r\nbut it still wont work ", "comments": ["Please provide following information. Thanks!\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Have you tried tf.cast(x, dtype=tf.int32)?"]}, {"number": 26294, "title": "[TF 2.0] Conversion script fails to parse IPython functions.", "body": "The [`tf_upgrade_v2`](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/upgrade.md) tool fails to parse commands with common IPython functions (for example, `!pip install tf-nightly` and `%matplotlib inline`).\r\n\r\n```\r\nERROR: Failed to parse.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/tools/compatibility/ast_edits.py\", line 510, in update_string_pasta\r\n    t = pasta.parse(text)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/__init__.py\", line 23, in parse\r\n    t = ast_utils.parse(src)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/ast_utils.py\", line 56, in parse\r\n    tree = ast.parse(sanitize_source(src))\r\n  File \"/usr/lib/python3.6/ast.py\", line 35, in parse\r\n    return compile(source, filename, mode, PyCF_ONLY_AST)\r\n  File \"<unknown>\", line 37\r\n    !pip install tf-nightly\r\n    ^\r\nSyntaxError: invalid syntax\r\n```", "comments": ["Hi, I would like to work on this issue. Any starting pointers?", "would this work https://github.com/tensorflow/tensorflow/pull/26332/files", "Is anyone currently working on this Issue?", "Hi @dynamicwebpaige @kyscg @NikolaMandic @shreyashub  , I am doing a research about \"how the labels such as `good first issue` help newcomers to contribute\" , but I found many PR were rejected because of duplication or newcomers feel puzzled whether this issue are under processing, especially for popular projects. How do you think of this problem? I wonder whether mechanisms to avoid this problem are needed, for example, in addition to `open` and `closed`, add a new status of issue: `ongoing`.", "you like read what people wrote then think\r\nwhat was last written that is the status\r\nhere for example one puts a pull request then people look at that one\r\n", "@dynamicwebpaige this one should be been fixed for quite a while. Would you mind to close the issue or point us out if you still have any open issues with current nightly.\r\n\r\nThanks!", "Closing, and thanks to @lc0 for the contribution!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26294\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26294\">No</a>\n"]}, {"number": 26293, "title": "In a @tf.function, after `for i in tf.range(10)`, i is still undefined", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOSX 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\ntf.version.VERSION: 2.0.0-dev20190302\r\ntf.version.GIT_VERSION: v1.12.0-9439-g837e5feed2\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nWhen using a `for` loop in a @tf.function (with autograph), such as `for i in tf.range(10)`, `i` is undefined after the loop ends, unless it is defined in the global scope, in which case it takes this value.\r\n\r\n**Describe the expected behavior**\r\nI expect the same behavior as in regular Python: `i` should be the last value that was run in the `for` loop.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef loop():\r\n    for i in tf.range(10):\r\n        pass\r\n    return i\r\n\r\nloop() # NameError: name 'i' is not defined (see full stacktrace below)\r\n```\r\n\r\nIf I define `i` outside of the function, then autograph uses that:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ni = 1234\r\n\r\n@tf.function\r\ndef loop():\r\n    for i in tf.range(10):\r\n        pass\r\n    return i\r\n\r\nprint(loop().numpy()) # prints 1234, which is *incorrect*\r\n```\r\n\r\nThe correct behavior should be the same as without decorating with @tf.function:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ni = 1234\r\n\r\ndef loop():\r\n    for i in tf.range(10):\r\n        pass\r\n    return i\r\n\r\nprint(loop().numpy()) # prints 9, which is correct.\r\n```\r\n\r\n**Other info / logs**\r\nHere is the stacktrace:\r\n\r\n```\r\nNameError                                 Traceback (most recent call last)\r\n<ipython-input-1-2ae01b93772c> in <module>\r\n      7     return i\r\n      8\r\n----> 9 loop() # NameError: name 'i' is not defined (see full stacktrace below)\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    424     # This is the first call of __call__, so we have to initialize.\r\n    425     initializer_map = {}\r\n--> 426     self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n    427     if self._created_variables:\r\n    428       try:\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    368     self._concrete_stateful_fn = (\r\n    369         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 370             *args, **kwds))\r\n    371\r\n    372     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1311     if self._input_signature:\r\n   1312       args, kwargs = None, None\r\n-> 1313     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1314     return graph_function\r\n   1315\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   1578           or call_context_key not in self._function_cache.missed):\r\n   1579         self._function_cache.missed.add(call_context_key)\r\n-> 1580         graph_function = self._create_graph_function(args, kwargs)\r\n   1581         self._function_cache.primary[cache_key] = graph_function\r\n   1582         return graph_function, args, kwargs\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   1510             arg_names=arg_names,\r\n   1511             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 1512             capture_by_value=self._capture_by_value),\r\n   1513         self._function_attributes)\r\n   1514\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    692                                           converted_func)\r\n    693\r\n--> 694       func_outputs = python_func(*func_args, **func_kwargs)\r\n    695\r\n    696       # invariant: `func_outputs` contains only Tensors, IndexedSlices,\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    315         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    316         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    318     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    319\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    684                   optional_features=autograph_options,\r\n    685                   force_conversion=True,\r\n--> 686               ), args, kwargs)\r\n    687\r\n    688         # Wrapping around a decorator allows checks like tf_inspect.getargspec\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)\r\n    390     return _call_unconverted(f, args, kwargs)\r\n    391\r\n--> 392   result = converted_f(*effective_args, **kwargs)\r\n    393\r\n    394   # The converted function's closure is simply inserted into the function's\r\n\r\n/var/folders/wy/h39t6kb11pnbb0pzhksd_fqh0000gn/T/tmpk_ywisd3.py in tf__loop()\r\n     10   ag__.for_stmt(ag__.converted_call('range', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (10,), {}), None, loop_body, ())\r\n     11   do_return = True\r\n---> 12   retval_ = i\r\n     13   return retval_\r\n     14\r\n\r\nNameError: name 'i' is not defined\r\n```\r\n\r\n", "comments": ["@mdanatg would you mind triaging?", "Thanks for the report! We're working on fixing this issue.", "Quick update: we'll definitely fix this for Python control flow, so that it behaves like normal code. For TensorFlow loops, that task will be more challenging and initially we will probably raise an error instead.\r\n\r\nConsider for example:\r\n\r\n```\r\nfor i in range(0):\r\n  pass\r\nreturn i  # Runtime error: i is undefined\r\n```\r\n\r\nBecause TensorFlow doesn't support the notion of undefined variables, replicating that behavior using `tf.while_loop` would be a bit trickier, although possible. Strictly speaking, we would need to generate the following code:\r\n\r\n```\r\niterated = tf.range(0)\r\n\r\ndef cond(idx, i, i_is_defined):\r\n  return tf.less(idx, iterated.shape[0])\r\n\r\ndef body(idx, i, i_is_defined):\r\n  i = iterated[idx]\r\n  i_is_defined = True\r\n  return idx + 1, i, i_is_defined\r\n\r\ndummy_for_undefined_i = tf.zeros(iterated.shape[1:])  # This value should never be used\r\n_, i_is_defined = tf.while_loop(cond, body, [0, dummy_for_undefined_i, False])\r\ntf.assert(i_is_defined, 'i is undefined')\r\n```", "@brilee FYI", "This needs attention. I'm having quite some problems with using tf.function and autograph whose basic features supposedly have been implemented some time ago.", "Working on it now!", "fixed in https://github.com/tensorflow/tensorflow/commit/d686d910e66392d27a0d67891248fa989b32e782", "FYI - The construct \r\n```\r\n@tf.function\r\ndef f():\r\n  for i in tf.range(10):\r\n    pass\r\n  return i\r\n```\r\nwill still raise an exception saying \"i must be defined for a tf.while_loop\". That's a much lower priority bug but it's on our radar."]}, {"number": 26292, "title": "can not find tensor flow/contrib/lite/java/demo this demo", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\nI want to download tensor flow code to install TFLite  in my android phone. But in tensorflow directory, I don not find tensorflow/contirbute/lite this directory. \r\ncode : https://github.com/tensorflow/tensorflow, branch is master\r\nAnd I hope you can provide install_docs to help me install and run model in Android \r\nThanks very much", "comments": ["@chenjiaoAngel Hi! I have mentioned some links below. Can you please take a look at these.\r\n\r\nTFLite code Repository : https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite\r\nTFLite Website Guide : https://www.tensorflow.org/lite/overview\r\nTFLite Github Guide : https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/android.md\r\n\r\nIs this what you were asking for?\r\nYou can close the issue if it's resolved.", "ok, TFlite can support  ssd-mobilenet  model? And shufflenet, shuffle net-v1 model support in TFLite?", "Ssd-mobilenet and Shufflenet-v2 are supported I think but I am not sure of Shufflenet-v1 in TFLite. \r\n@jvishnuvardhan Can you please verify this. Thanks :)", "```lite``` folder has moved out of contrib starting TF 1.13. You should find it on ```tensorflow/tensorflow/lite/```(https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite) as mentioned by @Ayush517  in previous comments", "Thanks, I have find that and build success", "1. ", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 26291, "title": "fix typo", "body": "", "comments": ["@Corea could you please fix errors", "@rthadur can you check this again?", "I found this is not a typo. Sorry for my mistake."]}, {"number": 26290, "title": "Failed to pip install tensorflow-gpu ", "body": "\r\n```\r\n# sudo pip install -U tensorflow-gpu --user\r\nCollecting tensorflow-gpu\r\n  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\r\n    99% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 345.2MB 53.0MB/s eta 0:00:01Exception:\r\nTraceback (most recent call last):\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_internal/cli/base_command.py\", line 179, in main\r\n    status = self.run(options, args)\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 315, in run\r\n    resolver.resolve(requirement_set)\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_internal/resolve.py\", line 131, in resolve\r\n    self._resolve_one(requirement_set, req)\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_internal/resolve.py\", line 294, in _resolve_one\r\n    abstract_dist = self._get_abstract_dist_for(req_to_install)\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_internal/resolve.py\", line 242, in _get_abstract_dist_for\r\n    self.require_hashes\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_internal/operations/prepare.py\", line 334, in prepare_linked_requirement\r\n    progress_bar=self.progress_bar\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_internal/download.py\", line 878, in unpack_url\r\n    progress_bar=progress_bar\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_internal/download.py\", line 702, in unpack_http_url\r\n    progress_bar)\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_internal/download.py\", line 946, in _download_http_url\r\n    _download_url(resp, link, content_file, hashes, progress_bar)\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_internal/download.py\", line 639, in _download_url\r\n    hashes.check_against_chunks(downloaded_chunks)\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_internal/utils/hashes.py\", line 62, in check_against_chunks\r\n    for chunk in chunks:\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_internal/download.py\", line 607, in written_chunks\r\n    for chunk in chunks:\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_internal/utils/ui.py\", line 159, in iter\r\n    for x in it:\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_internal/download.py\", line 596, in resp_read\r\n    decode_content=False):\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_vendor/urllib3/response.py\", line 494, in stream\r\n    data = self.read(amt=amt, decode_content=decode_content)\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_vendor/urllib3/response.py\", line 442, in read\r\n    data = self._fp.read(amt)\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 65, in read\r\n    self._close()\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 52, in _close\r\n    self.__callback(self.__buf.getvalue())\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_vendor/cachecontrol/controller.py\", line 300, in cache_response\r\n    cache_url, self.serializer.dumps(request, response, body=body)\r\n  File \"/root/.local/lib/python3.6/site-packages/pip/_vendor/cachecontrol/serialize.py\", line 72, in dumps\r\n    return b\",\".join([b\"cc=4\", msgpack.dumps(data, use_bin_type=True)])\r\nMemoryError\r\n```", "comments": ["I would like to solve this error\r\n", "@jiapei100 You are having a Memory Issue.The space is not getting allocated.\r\nYou will be able to run this command in google [colab](https://www.google.com/search?q=google+colab&oq=googlecol&aqs=chrome.1.69i57j0l5.3345j0j7&sourceid=chrome&ie=UTF-8) by typing the following command\r\n\"! sudo pip install -U tensorflow-gpu --user\"\r\nThank you for asking questions and please do ask if you have anymore doubts \r\nWould be please to answer them", "\r\n@kunakl07 \r\nHey, did you ever try to build tensorflow from source ?\r\n[https://github.com/tensorflow/tensorflow/issues/26289](https://github.com/tensorflow/tensorflow/issues/26289)", "You are having a memory error. If you are using a VM, make sure you allocate more RAM . Try to build tensorflow-gpu from source. Still if its not done. I prefer you try it on Google-Colab. ", "@jiapei100 I provided a solution in the other issue that you opened and referenced [here](https://github.com/tensorflow/tensorflow/issues/26289). Please let me know how it progresses. You could close this issue and we can resolve there in the other post. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 26289, "title": "Compatibility with Cuda 10.1? ", "body": "Just wanna compile tensorflow 1.12 against Cuda 10.1 ?\r\n\r\n```\r\n\u279c  tensorflow git:(master) bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nDEBUG: Rule 'build_bazel_rules_swift' modified arguments {\"commit\": \"001736d056d7eae20f1f4da41bc9e6f036857296\", \"shallow_since\": \"1547844730 -0800\"} and dropped [\"tag\"]\r\nDEBUG: ~/.cache/bazel/_bazel_user/15086820fc7a6f1383d8c38c62220208/external/build_bazel_rules_apple/apple/repositories.bzl:35:5: \r\nWARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.\r\n\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': in ....../tensorflow/tensorflow.bzl: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"....../third_party/gpus/cuda_configure.bzl\", line 1501\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"....../third_party/gpus/cuda_configure.bzl\", line 1266, in _create_local_cuda_repository\r\n                _find_libs(repository_ctx, cuda_config)\r\n        File \"....../third_party/gpus/cuda_configure.bzl\", line 859, in _find_libs\r\n                _find_cuda_lib(\"cublas\", repository_ctx, cpu_value, c..., ...)\r\n        File \"....../third_party/gpus/cuda_configure.bzl\", line 773, in _find_cuda_lib\r\n                find_lib(repository_ctx, [(\"%s/%s%s\" % (bas...], ...)))\r\n        File \"....../third_party/gpus/cuda_configure.bzl\", line 750, in find_lib\r\n                auto_configure_fail((\"No library found under: \" + \",...)))\r\n        File \"....../third_party/gpus/cuda_configure.bzl\", line 341, in auto_configure_fail\r\n                fail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: No library found under: /usr/local/cuda-10.1/lib64/libcublas.so.10.1, /usr/local/cuda-10.1/lib64/stubs/libcublas.so.10.1, /usr/local/cuda-10.1/lib/powerpc64le-linux-gnu/libcublas.so.10.1, /usr/local/cuda-10.1/lib/x86_64-linux-gnu/libcublas.so.10.1, /usr/local/cuda-10.1/lib/x64/libcublas.so.10.1, /usr/local/cuda-10.1/lib/libcublas.so.10.1, /usr/local/cuda-10.1/libcublas.so.10.1\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': in ....../tensorflow/tensorflow.bzl: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"....../third_party/gpus/cuda_configure.bzl\", line 1501\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"....../third_party/gpus/cuda_configure.bzl\", line 1266, in _create_local_cuda_repository\r\n                _find_libs(repository_ctx, cuda_config)\r\n        File \"....../third_party/gpus/cuda_configure.bzl\", line 859, in _find_libs\r\n                _find_cuda_lib(\"cublas\", repository_ctx, cpu_value, c..., ...)\r\n        File \"....../third_party/gpus/cuda_configure.bzl\", line 773, in _find_cuda_lib\r\n                find_lib(repository_ctx, [(\"%s/%s%s\" % (bas...], ...)))\r\n        File \"....../third_party/gpus/cuda_configure.bzl\", line 750, in find_lib\r\n                auto_configure_fail((\"No library found under: \" + \",...)))\r\n        File \"....../third_party/gpus/cuda_configure.bzl\", line 341, in auto_configure_fail\r\n                fail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: No library found under: /usr/local/cuda-10.1/lib64/libcublas.so.10.1, /usr/local/cuda-10.1/lib64/stubs/libcublas.so.10.1, /usr/local/cuda-10.1/lib/powerpc64le-linux-gnu/libcublas.so.10.1, /usr/local/cuda-10.1/lib/x86_64-linux-gnu/libcublas.so.10.1, /usr/local/cuda-10.1/lib/x64/libcublas.so.10.1, /usr/local/cuda-10.1/lib/libcublas.so.10.1, /usr/local/cuda-10.1/libcublas.so.10.1\r\nINFO: Elapsed time: 10.828s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n    Fetching @local_config_cuda; fetching\r\n```\r\n\r\n\r\nAny suggestions?", "comments": ["@jiapei100 For Faster resolutions, Please provide the details requested at issue template [here](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). \r\n\r\nPlease check tested build configurations [here](https://www.tensorflow.org/install/source_windows#gpu). Tensorflow 1.12 was built with CUDA9.0. So when you install some of the paths in different modules are looking for CUDA9.0. If you want to use CUDA10 or CUDA 10.1, use latest version of TF and check whether CUDA and cuDNN are referencing correct paths. First, uninstall python and tensorflow and reinstall following the instructions [here](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12) but use new version of CUDA and cuDNN. Please let me know how it progresses. Thanks!", "Since based on [cublas issues](https://devtalk.nvidia.com/default/topic/1047981/cublas-for-10-1-is-missing/) and [cublas new features](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-new-features). the cublas is in the seperate directory and therefore you may could use soft link to temporarily fix it.", "I meet the same problem about cublas. Path `/usr/local/cuda/lib64`, does not include cublas library.", "So, how to modify the code and complie the path for CUDA 10.1.", "@wjcskqygj2015 \r\n\r\nSymlinking doesn't work, try build tensorflow master\r\n```\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': in /home/bernard/opt/cuda_test/tensorflow/tensorflow/tensorflow.bzl: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/bernard/opt/cuda_test/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1502\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/bernard/opt/cuda_test/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, cuda_config)\r\n\tFile \"/home/bernard/opt/cuda_test/tensorflow/third_party/gpus/cuda_configure.bzl\", line 859, in _find_libs\r\n\t\t_find_cuda_lib(\"cublas\", repository_ctx, cpu_value, c..., ...)\r\n\tFile \"/home/bernard/opt/cuda_test/tensorflow/third_party/gpus/cuda_configure.bzl\", line 773, in _find_cuda_lib\r\n\t\tfind_lib(repository_ctx, [(\"%s/%s%s\" % (bas...], ...)))\r\n\tFile \"/home/bernard/opt/cuda_test/tensorflow/third_party/gpus/cuda_configure.bzl\", line 747, in find_lib\r\n\t\tauto_configure_fail((\"None of the libraries match th...)))\r\n\tFile \"/home/bernard/opt/cuda_test/tensorflow/third_party/gpus/cuda_configure.bzl\", line 341, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: None of the libraries match their SONAME: /home/bernard/opt/cuda_test/cuda/lib64/libcublas.so.10.1\r\n```\r\nBesides, all libcu***.so.10.1 are missing, not just libcublas, it is a pain to have to create symlink for all these even if symlinking works.", "I have the same problem using tensorflow 1.13.1 and CUDA 10.1.  Use of symlinks 'worked' for me in so far as there were no path errors - however attempting to run the tensorflow sanity check results in a core dump, presumably as there are genuine incompatabilities between those versions of tensorflow and CUDA\r\n\r\n```\r\n(venv) \u279c  tensorflow python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\npython: Relink `/lib64/libmount.so.1' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\r\npython: Relink `/lib64/libsystemd.so.0' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\r\n[1]    24472 segmentation fault (core dumped)  python -c \r\n```\r\nEDIT: the following link commands fixed the path errors\r\n```\r\nsudo ln -s /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcublas.so.10.1.0.105 /usr/lib64/libcublas.so.10.0\r\nsudo ln -s /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcusolver.so.10.1.105 /usr/lib64/libcusolver.so.10.0 \r\nsudo ln -s /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart.so.10.1.105 /usr/lib64/libcudart.so.10.0 \r\n```", "@beew \r\nIt seems mainly because the SONAME is incompatible for some library in cuda\r\nI just modify the third_party/gpus/cuda_configure.bzl at line 871, 878, 885, 892, 907\r\nreplace cuda_config.cuda_version, with \"10\",\r\nthen using symlinks it works, I hope it will help you", "If you are heading down this particular rabbit hole, you will find #26155 relevant.", "Interestingly, got it working on windows, with an unsupported compiler - #28086", "This is due to conflicting behaviour at configure/build time and run time. Have a look at the patch in #28093 for a temporary workaround.", "@chsigg @annarev has done a lot on this. I think we are compatible now.", "Dear\r\ni`m trying to install tensorflow using an Nvidia 1070, CUDA 10.1, cudnn7.5.1.10 on Ubuntu 18.04.\r\nthe nvidia drivers are NVIDIA-SMI 418.40.04    Driver Version: 418.40.04    CUDA Version: 10.1  (this comes from nvidia-smi).\r\n\r\nI install tensorflow-gpu using pip3.\r\n\r\nthe error that i get when i run:\r\n`python3 -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"` \r\nis the following\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\ncan you please help me?\r\n\r\nCheers\r\nLuigi", "If you want to use TensorFlow with CUDA 10.1, you currently need to build it from source. The binaries we ship are built for CUDA 10.0.", "> If you want to use TensorFlow with CUDA 10.1, you currently need to build it from source. The binaries we ship are built for CUDA 10.0.\r\n\r\nClosing this issue since chsigg's explanation addresses the issue. Feel free to reopen if have any further questions. Thanks!", "chsigg's suggestion is not helpful. Most of us are building from source.", "Please re-open this issue.", "```\r\ntensorflow/lite/delegates/nnapi/nnapi_delegate.cc: In static member function 'static void tflite::StatefulNnApiDelegate::DoFreeBufferHandle(TfLiteContext*, TfLiteDelegate*, TfLiteBufferHandle*)':\r\ntensorflow/lite/delegates/nnapi/nnapi_delegate.cc:2136:31: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (*handle >= 0 && *handle < delegate_data->tensor_memory_map.size()) {\r\n                       ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nERROR: /home/leimao/.cache/bazel/_bazel_leimao/b5222491e5f5e6954d481a492cdf3412/external/nccl_archive/BUILD.bazel:54:1: undeclared inclusion(s) in rule '@nccl_archive//:device_lib':\r\nthis rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/collectives/device/functions.cu.cc':\r\n  'external/nccl_archive/src/collectives/device/common.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 101.561s, Critical Path: 19.52s\r\nINFO: 513 processes: 513 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "The error you are seeing is an unrelated bug that will be fixed shortly. Or\nyou can use a commit from 2 days ago for now.\n\nOn Wed, Jun 12, 2019, 03:16 Lei Mao <notifications@github.com> wrote:\n\n> `\n> tensorflow/lite/delegates/nnapi/nnapi_delegate.cc: In static member\n> function 'static void\n> tflite::StatefulNnApiDelegate::DoFreeBufferHandle(TfLiteContext*,\n> TfLiteDelegate*, TfLiteBufferHandle*)':\n> tensorflow/lite/delegates/nnapi/nnapi_delegate.cc:2136:31: warning:\n> comparison between signed and unsigned integer expressions [-Wsign-compare]\n> if (*handle >= 0 && *handle < delegate_data->tensor_memory_map.size()) {\n> ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n> ERROR:\n> /home/leimao/.cache/bazel/_bazel_leimao/b5222491e5f5e6954d481a492cdf3412/external/nccl_archive/BUILD.bazel:54:1:\n> undeclared inclusion(s) in rule '@nccl_archive//:device_lib':\n> this rule is missing dependency declarations for the following files\n> included by 'external/nccl_archive/src/collectives/device/functions.cu.cc\n> ':\n> 'external/nccl_archive/src/collectives/device/common.h'\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\n> Use --verbose_failures to see the command lines of failed build steps.\n> INFO: Elapsed time: 101.561s, Critical Path: 19.52s\n> INFO: 513 processes: 513 local.\n> FAILED: Build did NOT complete successfully\n>\n> `\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26289?email_source=notifications&email_token=ABZM5DXBBSGF4XNOXVQGXADP2BE5HA5CNFSM4G3LCTW2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXO6EKI#issuecomment-501080617>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABZM5DSNAHFSDXGKHERD5ADP2BE5HANCNFSM4G3LCTWQ>\n> .\n>\n", "Hello everyone,\r\nif you want to keep CUDA10.1 just install the new release of tensorflow 2.0, it will work for sure.\r\nuse this for python2:  pip install tf-nightly-gpu-2.0-preview\r\nuse this for python3: pip3 install tf-nightly-2.0-preview\r\n### ### ### ", "Hi @elachkarcharbel I am pretty sure we are building those two packages with cuda 10.", "hello @gunan, well i have CUDA 10.1 and these packages worked for me without any need to downgrade to CUDA10.0", "> Hello everyone,\r\n> if you want to keep CUDA10.1 just install the new release of tensorflow 2.0, it will work for sure.\r\n> use this for python2: pip install tf-nightly-gpu-2.0-preview\r\n> use this for python3: pip3 install tf-nightly-2.0-preview\r\n> \r\n> ### ###\r\n\r\nI tried, not working for me Ununtu 19.04, cuda10.1", "@selvamsandeep the ubuntu version should not be the problem I think ( I'm using the 18.04 LTS).\r\nRecheck if Cuda is installed correctly on your machine, I recommend using the Cuda-runtime version for installation, this is the only installation method that worked for me with Cuda 10.1", "New TensorFlow 1.14 pip wheel installer supports CUDA 10.1.", "I use ubuntu 16.04 LTS and I have Cuda 10.1 installed. When I run tensorflow (version 1.12) in python2.6 (anaconda dist) i have no issues, the GPU registers, and the model runs fast. However, when I run tensorflow (version 1.14) in python3.6 I get the following error:\r\nCould not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0\r\nand the same error for the following files:\r\nlibcudart.so.10.0\r\nlibcublas.so.10.0\r\nlibcufft.so.10.0\r\nlibcusolver.so.10.0\r\nlibcusparse.so.10.0\r\nthe model runs but very slowly.\r\nI don't understand why I would have trouble finding these files using python3.6 when I don't have this issue for python2.6", "@leimao I can't seem to make it work, I have cuda 10.1 installed and I ran `pip install tensorflow-gpu`, which installs the latest `1.14`, but still can't get it to work", "> I use ubuntu 16.04 LTS and I have Cuda 10.1 installed. When I run tensorflow (version 1.12) in python2.6 (anaconda dist) i have no issues, the GPU registers, and the model runs fast. However, when I run tensorflow (version 1.14) in python3.6 I get the following error:\r\n> Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0\r\n> and the same error for the following files:\r\n> libcudart.so.10.0\r\n> libcublas.so.10.0\r\n> libcufft.so.10.0\r\n> libcusolver.so.10.0\r\n> libcusparse.so.10.0\r\n> the model runs but very slowly.\r\n> I don't understand why I would have trouble finding these files using python3.6 when I don't have this issue for python2.6\r\n\r\nI have the same issue. tf 1.14 keeps finding cuda 10.0 instead of 10.1", "The same story here. Ubuntu 18.04, Cuda 10.1.\r\nThe current workaround is to copy \r\nlibcudart.so.10.0\r\nlibcublas.so.10.0\r\nlibcufft.so.10.0\r\nlibcusolver.so.10.0\r\nlibcusparse.so.10.0\r\nfrom Cuda 10.0", "> New TensorFlow 1.14 pip wheel installer supports CUDA 10.1.\r\n\r\n\r\n\r\n> > Hello everyone,\r\n> > if you want to keep CUDA10.1 just install the new release of tensorflow 2.0, it will work for sure.\r\n> > use this for python2: pip install tf-nightly-gpu-2.0-preview\r\n> > use this for python3: pip3 install tf-nightly-2.0-preview\r\n> \r\n> I tried, not working for me Ununtu 19.04, cuda10.1\r\n\r\nnot working too, Ubuntu 16.04 cuda10.1 tensorflow-gpu-2.0-beta\r\n\r\n", "It is searching for cudart64_100.dll and says install cuda10.0 so that it can find it. Why not tensorflow release a new build for cuda10.1 ? ", "Unassigning myself, as there is nothing more I can do here.\r\nTF can build with cuda 10.1 now, and our builds will stay with 10.1 until they move forward.", "Using tensorflow-gpu==1.14 with cuda10.0 (and cuDNN v7.6.0) on windows 10 worked for me\r\nthis is also specified in https://www.tensorflow.org/install/source#tested_build_configurations", "Same problem here.\r\n\r\n* Ubuntu 18.04.2 LTS\r\n* Python 3.7.3 (default, Jun 24 2019, 18:29:44) \r\n\r\n```\r\n~$: cat /usr/local/cuda/version.txt \r\nCUDA Version 10.1.168\r\n\r\n>>> import tensorflow\r\n>>> print(tensorflow.__version__)\r\n   1.14.0\r\n\r\n~$ nvidia-smi \r\nMon Jul  8 12:57:09 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 208...  On   | 00000000:81:00.0  On |                  N/A |\r\n| 24%   35C    P8    16W / 250W |    748MiB / 10955MiB |      4%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      2128      G   /usr/lib/xorg/Xorg                            40MiB |\r\n|    0      2202      G   /usr/bin/gnome-shell                          58MiB |\r\n|    0      3233      G   /usr/lib/xorg/Xorg                           398MiB |\r\n|    0      3364      G   /usr/bin/gnome-shell                         247MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n```", "There seems to be a lot of confusion on this ticket.\r\n\r\nThis ticket was about *compiling from source* against CUDA 10.1. nVidia gratuitously moved various things around in 10.1, so a fair number of changes were needed just to get TensorFlow to compile.\r\n\r\nBinary TensorFlow releases *never* (to my knowledge) support any version of CUDA other than the one they are built against. TF1.14 was built against CUDA 10.0, so that is the version you need if you want to use the binary tensorflow_gpu pip package.\r\n\r\nThe changes needed to compile TF against CUDA 10.1 went into r1.14 (and mainline). So if you really want TensorFlow + CUDA 10.1, you can compile it for yourself.\r\n\r\nThis ticket should remain closed unless there is some problem compiling TF against CUDA 10.1.\r\n", "Is there prebuilded wheel for tensorflow 1.14 + CUDA 10.1?", "No, there is no prebuilt wheel for CUDA 10.1 in any released version.\r\n\r\nTF 1.14 was released against CUDA 10.0.", "> > Hello everyone,\r\n> > if you want to keep CUDA10.1 just install the new release of tensorflow 2.0, it will work for sure.\r\n> > use this for python2: pip install tf-nightly-gpu-2.0-preview\r\n> > use this for python3: pip3 install tf-nightly-2.0-preview\r\n> \r\n> I tried, not working for me Ununtu 19.04, cuda10.1\r\n\r\nHi  working for me after i did some sym link, \r\nin  /usr/local/cuda-10.1/lib64\r\n\r\nlibcublas.so -> /usr/local/cuda-10.1/lib64/libcublas.so.10.1\r\nlibcublas.so.10.0 -> /usr/local/cuda-10.1/lib64/libcublas.so.10.1\r\nlibcusolver.so.10.0 -> /usr/lib/x86_64-linux-gnu/libcusolver.so.10\r\nlibcurand.so.10.0 -> /usr/lib/x86_64-linux-gnu/libcurand.so.10\r\nlibcufft.so.10.0 -> /usr/lib/x86_64-linux-gnu/libcufft.so.10\r\nlibcudart.so.10.0 -> /usr/lib/x86_64-linux-gnu/libcudart.so.10.1\r\nlibcusparse.so.10.0 -> /usr/lib/x86_64-linux-gnu/libcusparse.so.10", "> > > Hello everyone,\r\n> > > if you want to keep CUDA10.1 just install the new release of tensorflow 2.0, it will work for sure.\r\n> > > use this for python2: pip install tf-nightly-gpu-2.0-preview\r\n> > > use this for python3: pip3 install tf-nightly-2.0-preview\r\n> > \r\n> > \r\n> > I tried, not working for me Ununtu 19.04, cuda10.1\r\n> \r\n> Hi working for me after i did some sym link,\r\n> in /usr/local/cuda-10.1/lib64\r\n> \r\n> libcublas.so -> /usr/local/cuda-10.1/lib64/libcublas.so.10.1\r\n> libcublas.so.10.0 -> /usr/local/cuda-10.1/lib64/libcublas.so.10.1\r\n> libcusolver.so.10.0 -> /usr/lib/x86_64-linux-gnu/libcusolver.so.10\r\n> libcurand.so.10.0 -> /usr/lib/x86_64-linux-gnu/libcurand.so.10\r\n> libcufft.so.10.0 -> /usr/lib/x86_64-linux-gnu/libcufft.so.10\r\n> libcudart.so.10.0 -> /usr/lib/x86_64-linux-gnu/libcudart.so.10.1\r\n> libcusparse.so.10.0 -> /usr/lib/x86_64-linux-gnu/libcusparse.so.10\r\n\r\nHi, is it working with tensorflow 2.0 with cuda 10.1 and do I have to install cuda 10.0 seperately? Can you share the symlinking Linux command here so that I can try and confirm it's indeed working.", "Just to tell what I have found, in order to use GPU with TF2.0, one has to install Cuda 10.0, because 10.1 is not yet supported by TF2.0-beta1. Once you think everything is installed, you can check if TF can access the GPU or not using the command stated [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/test/is_gpu_available).\r\n`value = tf.test.is_gpu_available(cuda_only=False,\r\n   min_cuda_compute_capability=None)`\r\n`print (value)`", "> New TensorFlow 1.14 pip wheel installer supports CUDA 10.1.\r\n\r\nsorry. I've installed 1.14 using pip install, but when import it still says cudart64_100.dll is missing. Cuda 10.0 is need .", "@selvamshan It didn't work for me:\r\n```\r\n  sudo ln -s /usr/local/cuda/lib64/libcublas.so /usr/local/cuda/lib64/libcublas.so.10.0\r\n  sudo ln -s /usr/local/cuda/targets/x86_64-linux/lib/libcusolver.so.10 /usr/local/cuda/lib64/libcusolver.so.10.0\r\n  sudo ln -s /usr/local/cuda/targets/x86_64-linux/lib/libcurand.so.10 /usr/local/cuda/lib64/libcurand.so.10.0\r\n  sudo ln -s /usr/local/cuda/targets/x86_64-linux/lib/libcufft.so.10 /usr/local/cuda/lib64/libcufft.so.10.0\r\n  sudo ln -s /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.10 /usr/local/cuda/lib64/libcudart.so.10.0\r\n  sudo ln -s /usr/local/cuda/targets/x86_64-linux/lib/libcusparse.so.10 /usr/local/cuda/lib64/libcusparse.so.10.0\r\n```", "> > New TensorFlow 1.14 pip wheel installer supports CUDA 10.1.\r\n> \r\n> sorry. I've installed 1.14 using pip install, but when import it still says cudart64_100.dll is missing. Cuda 10.0 is need .\r\n\r\nI finally installed third-party-built tensorflow with cuda101. It works . Here is the link https://github.com/fo40225/tensorflow-windows-wheel. By the way, I am using Windows 10 .", "@7oranger which one? how?\r\nThanks.", "> @7oranger which one? how?\r\n> Thanks.\r\n\r\nhttps://github.com/fo40225/tensorflow-windows-wheel . If your computer support avx 2, \r\n you can download package from https://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.14.0/py37/GPU/cuda101cudnn76avx2 and unzip them. Use pip  to install the wheel . Notice that it's for windows.", "@7oranger Yeah, well, i'm working on linux.", "`conda install -c anaconda tensorflow-gpu` seems install `tensorflow 1.14.0` with` CUDA 10.1` for `Python 3.6.5 :: Anaconda, Inc.`\r\n\r\n```\r\nconda list | grep tensor\r\ntensorboard               1.14.0           py36hf484d3e_0    anaconda\r\ntensorflow                1.14.0          gpu_py36h3fb9ad6_0    anaconda\r\ntensorflow-base           1.14.0          gpu_py36he45bfe2_0    anaconda\r\ntensorflow-estimator      1.14.0                     py_0    anaconda\r\ntensorflow-gpu            1.14.0               h0d30ee6_0    anaconda\r\n```", "@mrgloom worked for me. thank you", "I am using anaconda and python 3.7.2 with Cuda 10.1 and my error says it cannot find the path of *_100.dll but I have *_101.dll and TensorFlow v 1.13 is being installed not 1.14, I support TensorFlow 1.14 supports Cuda 10.1? help me cant install TensorFlow, should I create 3.6 env and then install TensorFlow?\r\n", "CUDA 10.1 is not supported yet.", "For other users having same problem like me, make a conda env with python 3\r\nconda create -n tensorflow-gpu python=3 \r\nThen \r\nconda install -c anaconda tensorflow-gpu \r\nThis would install tensorflow 1.13.1 in python 3.6.9 env and would work fine on cuda 10.1 \r\n\r\nMy specs, \r\nWindows 10\r\nCuda 10.1, cuDNN\r\nGtx 1060 , i7-7770k, 16 GB ram", "> @selvamshan It didn't work for me:\r\n> \r\n> ```\r\n>   sudo ln -s /usr/local/cuda/lib64/libcublas.so /usr/local/cuda/lib64/libcublas.so.10.0\r\n>   sudo ln -s /usr/local/cuda/targets/x86_64-linux/lib/libcusolver.so.10 /usr/local/cuda/lib64/libcusolver.so.10.0\r\n>   sudo ln -s /usr/local/cuda/targets/x86_64-linux/lib/libcurand.so.10 /usr/local/cuda/lib64/libcurand.so.10.0\r\n>   sudo ln -s /usr/local/cuda/targets/x86_64-linux/lib/libcufft.so.10 /usr/local/cuda/lib64/libcufft.so.10.0\r\n>   sudo ln -s /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.10 /usr/local/cuda/lib64/libcudart.so.10.0\r\n>   sudo ln -s /usr/local/cuda/targets/x86_64-linux/lib/libcusparse.so.10 /usr/local/cuda/lib64/libcusparse.so.10.0\r\n> ```\r\n\r\nThis works for me", "I follow https://github.com/tensorflow/tensorflow/issues/26150#issuecomment-469058265 resolved `Cuda Configuration Error: Cannot find cuda library libcublas.so.10.1` similar issue in `nvidia/cuda:10.1-cudnn7-devel-centos7` docker image, anaconda `Python 3.6` to compile GPU `tensorflow==1.12`:\r\n\r\n```\r\n[root@skyaxe-computing-1 tensorflow]# find / -name \"libcublas.so.10*\"\r\n/usr/lib64/libcublas.so.10\r\n/usr/lib64/libcublas.so.10.2.0.168\r\n[root@skyaxe-computing-1 tensorflow]# readlink -e -v /usr/lib64/libcublas.so.10\r\n/usr/lib64/libcublas.so.10.2.0.168\r\n[root@skyaxe-computing-1 tensorflow]# cp /usr/lib64/libcublas.so.10.2.0.168  /usr/local/cuda/lib64/\r\n[root@skyaxe-computing-1 tensorflow]# ln -sf /usr/local/cuda/lib64/libcublas.so.10.2.0.168 /usr/local/cuda/lib64/libcublas.so.10.1\r\n\r\n[root@skyaxe-computing-1 tensorflow]# ln -s /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcusolver.so.10 /usr/local/cuda-10.1/lib64/libcusolver.so.10.1\r\n[root@skyaxe-computing-1 tensorflow]# ln -s /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcurand.so.10 /usr/local/cuda-10.1/lib64/libcurand.so.10.1\r\n[root@skyaxe-computing-1 tensorflow]# ln -s /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcufft.so.10 /usr/local/cuda-10.1/lib64/libcufft.so.10.1\r\n```\r\n\r\nUnfortunately, raise another issue for me:\r\n```\r\n... ...\r\n\r\nERROR: /opt/conda/conda-bld/tensorflow-gpu_1566813226659/work/tensorflow/core/kernels/BUILD:2632:1: error while parsing .d file: /root/.cache/bazel/_bazel_root/a4c2e48480cce311327576d3bdfd29\r\n7d/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/core/kernels/_objs/determinant_op_gpu/tensorflow/core/kernels/determinant_op_gpu.cu.pic.d (No such file or directory)\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:269:0,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/framework/tensor_types.h:19,\r\n                 from ./tensorflow/core/kernels/determinant_op.h:19,\r\n                 from tensorflow/core/kernels/determinant_op_gpu.cu.cc:20:\r\n/usr/local/cuda-10.1/bin/../targets/x86_64-linux/include/host_defines.h:54:2: warning: #warning \"host_defines.h is an internal header file and must not be used directly.  This file will be r\r\nemoved in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\" [-Wcpp]\r\n #warning \"host_defines.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h inst\r\nead.\"\r\n  ^\r\nIn file included from tensorflow/core/kernels/determinant_op_gpu.cu.cc:25:0:\r\n./tensorflow/core/kernels/cuda_solvers.h:29:36: fatal error: cuda/include/cublas_v2.h: No such file or directory\r\n #include \"cuda/include/cublas_v2.h\"\r\n                                    ^\r\ncompilation terminated.\r\n\r\n```\r\n\r\nEDIT:\r\n* `rm -rf /root/.cache/bazel/` and then refresh install again, it gets the same errors", "Hi. I got mine to work. I did alto of things, not sure exactly what did the trick, but I'm fairly sure it was doing the following:\r\nI'm running Arch Linux. \r\nCuda 10.1.\r\nPython 3.7.4\r\nI installed tf 2: sudo pip3.7 install tf-nightly-gpu-2.0-preview\r\nI found my cuda directory: sudo find / -name \"libcublas.so*\"\r\nMine is: /opt/cuda/targets/x86_64-linux/lib/\r\nI did all the symbolic links as above:\r\nsudo ln -s /opt/cuda/targets/x86_64-linux/lib/libcublas.so /opt/cuda/targets/x86_64-linux/lib/libcublas.so.10.0\r\nsudo ln -s /opt/cuda/targets/x86_64-linux/lib/libcusolver.so.10 /opt/cuda/targets/x86_64-linux/lib/libcusolver.so.10.0\r\n sudo ln -s /opt/cuda/targets/x86_64-linux/lib/libcurand.so.10 /opt/cuda/targets/x86_64-linux/lib/libcurand.so.10.0\r\n sudo ln -s /opt/cuda/targets/x86_64-linux/lib/libcufft.so.10 /opt/cuda/targets/x86_64-linux/lib/libcufft.so.10.0\r\nsudo ln -s /opt/cuda/targets/x86_64-linux/lib/libcudart.so.10 /opt/cuda/targets/x86_64-linux/lib/libcudart.so.10.0\r\nsudo ln -s /opt/cuda/targets/x86_64-linux/lib/libcusparse.so.10 /opt/cuda/targets/x86_64-linux/lib/libcusparse.so.10.0\r\n\r\nAnd the missing link! I added:\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/cuda/targets/x86_64-linux/lib/\r\n\r\n2019-08-27 12:12:24.212933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 1716 MB memory) -> physical GPU (device: 0, name: GeForce GT 710, pci bus id: 0000:01:00.0, compute capability: 3.5)\r\nGPU Available:  True\r\n\r\n", "Tensorflow still does not support CUDA 10.1?? Is there any workaround to getting it working with 10.1? I can't downgrade without completely blowing away Windows 10 and reloading which is a royal pain to say the least.", "I installed the Tensorflow-gpu-1.14 with the following modifications:\r\n\r\nsudo cp /usr/lib/x86_64-linux-gnu/libcublas.so.10 /usr/local/cuda-10.1/lib64/\r\nsudo ln -s /usr/local/cuda-10.1/lib64/libcublas.so.10 /usr/local/cuda-10.1/lib64/libcublas.so.10.0\r\n\r\nsudo ln -s /usr/local/cuda/targets/x86_64-linux/lib/libcusolver.so.10 /usr/local/cuda/lib64/libcusolver.so.10.0\r\n\r\nsudo ln -s /usr/local/cuda/targets/x86_64-linux/lib/libcurand.so.10 /usr/local/cuda/lib64/libcurand.so.10.0\r\n\r\nsudo ln -s /usr/local/cuda/targets/x86_64-linux/lib/libcufft.so.10 /usr/local/cuda/lib64/libcufft.so.10.0\r\n\r\nsudo ln -s /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so /usr/local/cuda/lib64/libcudart.so.10.0\r\n\r\nsudo ln -s /usr/local/cuda/targets/x86_64-linux/lib/libcusparse.so.10 /usr/local/cuda/lib64/libcusparse.so.10.0\r\n\r\nI use CUDA 10.1 and cudnn7_7.6.3.30 in Ubuntu18.04", "Do no bother any step above. the tensorflow master branch can built tensorflow 1.14 without pain with cdau 10.1\r\n\r\nthe only question is on another branch such as r2.0\r\n\r\nthey were totally messed.\r\n\r\nI can not build tensorflow 2.0 from source now.", "Can I use TensorFlow-1.14 with Cuda 10.1 now without building it?", "The best is to downgrade to Cuda 10 and install whatever version of TF you want. It worked for me.\r\n\r\n![image](https://user-images.githubusercontent.com/4516927/66541259-a1ca6480-eb69-11e9-8e33-16c0110893e7.png)\r\n", "This issue affects TF 1.15 as well. I get error when using with CUDA 10.1:\r\n\r\n` library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found`\r\n\r\nI feel CUDA 10.1 compatibility is important for \"last\" old version of TF as 10.1 has some critical bug fixes.\r\n\r\nIs there a hope that TF can be easy to use for different CUDA versions like PyTorch? I like the PyTorch's approch and it could be easily implemented. At basic level just allow select your OS + CPU/GPU + CUDA version and get appropriate compiled binaries. I don't think there are way too many combinations here that usual automated release process cannot handle.", "@sytelus to be honest, I don't think it's in the roadmap of tensorflow to provide wheels ready to use for all the versions of CUDA. Best I did was to downgrade from 10.1 to 10.", "@philipperemy not asking for all CUDA versions. Is it possible to make TF 1.15 compatible with CUDA 10.1? This because numerous important fixes.", "@sytelus from my understanding yes it's compatible but you have to compile it from the sources.", "It would be extremely useful to have CUDA 10.1 compatibility.  Centos 8, for example, only has prebuilt 10.1 installers from nvidia.  The end result is that if you want to use centos, at some level you experience some inconvenience.  Can't use the latest centos, which means none of the latest libraries, or you have to rebuild TF yourself, or you are stuck without gpu when using TF.\r\n\r\nIs there a projection for when this will be compatible?", "Hi all\r\nSince my own experiences on trying and failing installations with many different combinations of PythonX.x.x, PipX, Debian 9.x/10.x and CUDA 10.0.x/10.1.x, I think this is NVIDIA's part on solution. Since the actual errors with latest AND legacy releases of TF (as described from many users above) can only be solved by hands-on doing symlinking, the actual quality level NVIDIA shows on its heroical CUDA env is a very high crappy/shitty one. So, last but not least, me (and maybe some of you also) are damned to stay on Kernel version maximum 4.19 (best 4.9, does also not run with kernel 5.2.x), CUDA10.0.x incl. patch, cuDNN7.4.2 (i think is the latest for CUDA10.0.x) and system based pip version. Also, focusing on those versions, I get cuda up and running using the GPU on TF; nevertheless, also this version/installation shows missing some libraries after setting up CUDA, all necessary device files, shown be CUDA10.0.x version, are missing in CUDA10.1.x, which are NOT included. Also the NVIDIA_Samples will not be compiled completely since missing depending libs and programs.\r\nSo, in my eyes NVIDIA is doing a very bad and unuseful job here. They are going to scare away their users from CUDA this way.\r\nThat is only my opinion on this.\r\nRegards,\r\nRoger", "I agree that it sucks for interfaces to change in a point release, and it's\ntoo bad nvidia hasn't released a working 10.0 on rhel/centos 8.  At the\nsame time, it's been 8 months since 10.1 came out.  Some timeline for a TF\nupdate would be nice.\n\nOn Sun, Oct 27, 2019, 12:17 PM Roger Weihrauch <notifications@github.com>\nwrote:\n\n> Hi all\n> Since my own experiences on trying and failing installations with many\n> different combinations of PythonX.x.x, PipX, Debian 9.x/10.x and CUDA\n> 10.0.x/10.1.x, I think this is NVIDIA's part on solution. Since the actual\n> errors with latest AND legacy releases of TF (as described from many users\n> above) can only be solved by hands-on doing symlinking, the actual quality\n> level NVIDIA shows on its heroical CUDA env is a very high crappy/shitty\n> one. So, last but not least, me (and maybe some of you also) are damned to\n> stay on Kernel version maximum 4.19 (best 4.9, does also not run with\n> kernel 5.2.x), CUDA10.0.x incl. patch, cuDNN7.4.2 (i think is the latest\n> for CUDA10.0.x) and system based pip version. Also, focusing on those\n> versions, I get cuda up and running using the GPU on TF; nevertheless, also\n> this version/installation shows missing some libraries after setting up\n> CUDA, all necessary device files, shown be CUDA10.0.x version, are missing\n> in CUDA10.1.x, which are NOT included. Also the NVIDIA_Samples will not be\n> compiled completely since missing depending libs and programs.\n> So, in my eyes NVIDIA is doing a very bad and unuseful job here. They are\n> going to scare away their users from CUDA this way.\n> That is only my opinion on this.\n> Regards,\n> Roger\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26289?email_source=notifications&email_token=AA2AYWMSERUHFXTQU7EFMBLQQXLKLA5CNFSM4G3LCTW2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECLFSJI#issuecomment-546724133>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AA2AYWOJ5PTHCE7MFODA3NTQQXLKLANCNFSM4G3LCTWQ>\n> .\n>\n", "I would also like to point out (in addition to everything that is already said) that PyTorch officially only displays CUDA 10.1 support. For someone like me who often end up having TF and PyTorch in same environment, this makes it very difficult to have them install side by side.  Fortunately PyTorch builds have been superbly flexible with CUDA and I eventually figured out that merely including `cudatoolkit=10.0` in `conda install` makes it work with CUDA 10.0 as well! I hope something like this  might be possible for TF.", "In addition to what @sytelus said, the exact statement is:\r\n`conda install pytorch torchvision cudatoolkit=10.0 python==X -c pytorch`\r\nwhere X is your current python version if you don't want to upgrade it.", "Unfortunately, the issue I'm currently running into is that cuda 10.0\ndoesn't even exist for centos 8.  Perhaps centos / redhat was not the most\nconvenient linux distro to choose, but switching to a different distro\nwould be even more inconvenient at this point.\n\nOn Wed, Nov 6, 2019 at 8:57 PM Jeffrey Wardman <notifications@github.com>\nwrote:\n\n> In addition to what @sytelus <https://github.com/sytelus> said, the exact\n> statement is:\n> conda install pytorch torchvision cudatoolkit=10.0 python==X -c pytorch\n> where X is your current python version if you don't want to upgrade it.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26289?email_source=notifications&email_token=AA2AYWKPRR3MWPUEVBDVP4LQSON4RA5CNFSM4G3LCTW2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDJ3EOA#issuecomment-550744632>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AA2AYWKWEN336LADF5HCCD3QSON4RANCNFSM4G3LCTWQ>\n> .\n>\n", "We are working on updating TF builds to CUDA 10.1.", "Release a timeline. So that we can adjust accordingly. Simply staying mum for 6 months is unacceptable. We also have jobs.", "Looking at https://github.com/tensorflow/tensorflow/pull/34327 seems like TF team already solved the compatibility issue. Hopefully the 2.1 release happens soon", "As an intermediary workaround, I had no issues symlinking the affected libraries to *.so.10.0.", "Very unprofessional attitude towards managing a software. ", "Nightly release builds and r2.1 release are built against CUDA 10.1. I'm going to close this issue.\r\n\r\n@dexception, I agree I could have handled this better. Sorry.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26289\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26289\">No</a>\n", "`2.1`? Try `2.0.1`. `2.1` will be forever, right?", "TF 2.0 release will probably stay on CUDA 10.0. You can build TF with CUDA 10.1 from any branch, or use nightly releases before TF 2.1 is released.", "> You can build TF with CUDA 10.1 from any branch\r\n\r\nAre you sure about that? But I **build from source** on branch: r1.13 with the **cuda 10.1**, and obviously it does not work. \r\nErrors:\r\n- [x] library cannot find: libcublas.so.10.1 etc. Solved through syslink.\r\n- [x] cannot find cublas_v2.h. Solved through syslink.\r\n- [ ] `tensorflow/tensorflow/compiler/xla/service/gpu/BUILD:661:1: C++ compilatio\r\nn of rule '//tensorflow/compiler/xla/service/gpu:gpu_compiler' failed`  I have no clue for this...\r\n\r\nCould you help me, to **verify tf-1.\\* work with cuda10.1**?\r\n\r\n-------\r\nTo be precise, I compiled with XLA=on (tensorRT=off). If turn it off, everything just fine. Thus i thought the above XLA related problem is not big and solvable. Please check it.", "TF r1.13 does not build with CUDA 10.1. Please use r1.15 or r2.x.", "9 months and Tensorflow is still catching up. Oh boy ! Is this library too complicated to work with ?", "Version 2.1 uploaded yesterday.\r\nIt supports cuda 10.1, enjoy)", "@dishkakrauch What's about CUDA 10,2?", "You can always compile from source to get compatibility with other CUDA versions.\r\n\r\nWe only provide a subset of the infinite amount of combinations that you can build with (python version, operating system version, cuda version, compiler version, libraries/dependencies versions, etc.) For all other cases all that we can do is provide instructions on how to build from source and let community do the builds.", "Just downgraded to `Cuda 10,1`, thanks.", "I have made a script for install tf 1.12 with CUDA 10.1 in Ubuntu.\r\ncheck my repo https://github.com/uranusx86/Tensorflow1.12-CUDA10.1-Build", "**For folks who want to use tensorflow 1.xx with CUDA 10.1, I find conda has the correct compilation of tensorflow against CUDA 10.1**. Just install tensorflow 1.xx with conda within your conda environment. E.g., I install with the following command\r\n`conda install tensorflow=1.14`", " I \"fixed\" by changing this line to always return false\r\n```python\r\ndef _should_check_soname(version, static):\r\n      return False\r\n```\r\nhttps://github.com/tensorflow/tensorflow/blob/5bc72656b7b382c6f5c3f6b9f078fda5b05774aa/third_party/gpus/cuda_configure.bzl#L478-L479\r\n"]}, {"number": 26288, "title": "Remove all TF Logging ", "body": "\r\nI wanted to exclude all of the tensorflow logs in output. I searched and found that one or both of these setting should work:\r\n\r\n    os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\r\n    tf.logging.set_verbosity(tf.logging.ERROR)\r\n\r\nThough, I am still getting network loading logs, e.g.:\r\n\r\n    2019-02-28 23:51:13,520:INFO::Restoring parameters from ./pre_model/classic/brain4/network--6009999\r\n\r\nI was wondering why the `INFO` logs are still there. I also searched a lot about it and read the previous issues, but neither had a certain solution for this problem. \r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 8.7\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: K80 12GB\r\n- **Exact command to reproduce**: \r\n```\r\ncheckpoint = tf.train.get_checkpoint_state(os.path.join(directory, saved_model_address))\r\ntf.train.Saver.restore(session, checkpoint.model_checkpoint_path)\r\n\r\n```\r\nin which `saved_model_address` is the address of the saved model, and session is an instance of  tf.Session() which the model is created in. \r\n", "comments": ["duplicate #1258", "That log statement appears to be a python log statement in saver.py, so the set_verbosity call should work.\r\n\r\n>>> import tensorflow as tf\r\n>>> tf.logging.info('hi')\r\nINFO:tensorflow:hi\r\n>>> tf.logging.set_verbosity(tf.logging.ERROR)\r\n>>> tf.logging.info('hi')\r\n>>> \r\n\r\nMaybe add some well-placed print statements to \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/a3786f5d589a3b10f6152558dfb6f19070a6f30d/tensorflow/python/platform/tf_logging.py#L156\r\n\r\nto figure out what the verbosity level is when that line is printed out, and see why it's not being set properly.", "Also I can't seem to reproduce this in the latest version of TF on my Ubuntu-based machine:\r\n\r\n>>> tf.logging.set_verbosity(tf.logging.INFO)\r\n>>> saver.restore(s, cs.model_checkpoint_path)\r\nINFO:tensorflow:Restoring parameters from /path....\r\n\r\n>>> tf.logging.set_verbosity(tf.logging.ERROR)\r\n>>> saver.restore(s, cs.model_checkpoint_path)\r\n(No INFO log is printed).\r\n\r\nPerhaps this is fixed in a newer version of TF?", "Closing since this behavior is not observed in the latest version of TF. Feel free to reopen if have any further conflicts. Thanks!", "Sorry for late response. \r\nI updated to tensorflow 1.12.2, and still get the info logs:\r\n\r\n` 2019-04-22 23:43:44,608:INFO::Restoring parameters from ./pre_model/uniform/0-9/brain4/network--6009999`\r\n\r\n`Successfully loaded: ./pre_model/uniform/0-9/brain4/network--6009999\r\n`"]}]