[{"number": 7783, "title": "Segfault in runtime executor due to variable overflow", "body": "I'm running Tensorflow 1.0, and I'm encountering a segfault in tensorflow, with the following output:\r\n\r\n     F tensorflow/core/common_runtime/executor.cc:484] Check failed: e->src_output() < 32768 (38774 vs. 32768)\r\n     Aborted (core dumped)\r\n\r\n\r\nMy program loads a fairly large sparse matrix (1879549 samples, 556926 features, 0.000038 of the entries in the matrix are nonzero) into memory. I then create a `tf.SparseTensor` out of it:\r\n\r\n    # x_indices and x.data are the data for the sparse matrix in the correct format\r\n    x_ind = tf.Variable(initial_value=x_indices.astype(np.int64), trainable=False)\r\n    x_val = tf.Variable(initial_value=x.data, dtype=tf.float32, trainable=False)\r\n    return tf.SparseTensor(x_ind, x_val, dense_shape=x_sparse.shape)\r\n\r\nI then split this SparseTensor into minibatch-sized splits. I have several queue-runners feed that feed a Queue by taking a minibatch, transforming it to dense and putting it into the queue.\r\n\r\nIn code, the process looks like this (where `self._input_x_sp` is the SparseTensor):\r\n\r\n        self.x_shape = self._input_x_sp.get_shape().as_list()\r\n        self.y_shape = self._input_y.get_shape().as_list()\r\n        n_batches = self.x_shape[0] // self.batch_size\r\n        self._x_split = tf.sparse_split(sp_input=self._input_x_sp, num_split=n_batches, axis=0)\r\n        self._y_split = tf.split(self._input_y, num_or_size_splits=n_batches, axis=0, name=\"y_batch\")\r\n\r\n        #   ...  creating a Queue\r\n\r\n        # build a list of all possible enqueue OPs.\r\n        # We need to do this here while we're still single-threaded, as the\r\n        # Graph creation is not thread-safe\r\n        # Later in the different threads we can just run the OPs\r\n        self._op_list = []\r\n        for i in range(n_batches):\r\n            x_batch = tf.sparse_tensor_to_dense(self._x_split[i], name=\"x_batch\")\r\n            y_batch = self._y_split[i]\r\n            self._op_list.append(self._queue.enqueue_many([x_batch, y_batch]))\r\n\r\nThe queue-runners randomly pick an operation from `self._op_list` and execute it, in a loop. \r\n\r\n\r\n I am confident that my code is not to blame, as the program runs just fine on smaller input sizes (a sparse matrix of 206208 samples and 133515 features of which 0.000135 nonzero entries), but encounters the segfault on the larger matrix.\r\n\r\nLooking at the TF code where the error is generated ([here](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/core/common_runtime/executor.cc#L484)) it seems like the cause is a variable in tensorflow that is an unsigned short when it probably should be something larger than that.", "comments": ["Thanks for making a detailed analysis of the problem.", "You're welcome. I updated my post to remove some duplicated code, and corrected the link to the runtime-executor. If you need any further details let me know.", "Should be fixed in the next push."]}, {"number": 7782, "title": "control_dependencies and assign new shape not working (using validate_shape=False)", "body": "### Environment info\r\nOperating System: OSX on CPU\r\nTensorflow 1.0.0\r\n\r\n### Problem\r\nHello, i've been trying to use `tf.assign` with a `tf.control_dependencies` scheme when changing the shape on the fly.\r\n```python\r\nimport tensorflow as tf\r\n\r\n# I define a \"shape-able\" Variable\r\nx = tf.Variable(\r\n    [], \r\n    dtype=tf.int32,\r\n    validate_shape=False,\r\n    trainable=False\r\n)\r\n# I build a new shape and assign it to x\r\nconcat = tf.concat([x, [0]], 0)\r\nassign_op = tf.assign(x, concat, validate_shape=False)\r\n\r\nwith tf.control_dependencies([assign_op]):\r\n    # I print x after the assignment\r\n    # Note that the Print call is on \"x\" and NOT \"assign_op\"\r\n    print_op_dep = tf.Print(x, data=[x], message=\"print_op_dep:\")\r\n    # The assign_op is called, but it seems that print statement happens\r\n    # before the assignment\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    for i in range(3):\r\n        sess.run(print_op_dep)\r\n```\r\n\r\n**Outputs**:\r\n```bash\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[]\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0]\r\n```\r\n\r\n**I would expect**:\r\n```\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0 0]\r\n```\r\n\r\nIs this a bug ?", "comments": ["Interesting.   I modified your program as follows:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.Variable(\r\n    [], \r\n    dtype=tf.int32,\r\n    validate_shape=False,\r\n    trainable=False\r\n)\r\nx_alias = tf.Print(x, data=[x], message=\"x_alias\")\r\nconcat = tf.concat([x_alias, [0]], 0)\r\nconcat_alias = tf.Print(concat, data=[concat], message=\"concat_alias\")\r\nassign_op = tf.assign(x, concat_alias, validate_shape=False)\r\n\r\nwith tf.control_dependencies([assign_op]):\r\n    y = assign_op\r\n    # y = x\r\n    print_op_dep = tf.Print(y, data=[y], message=\"print_op_dep:\")\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    for i in range(3):\r\n        print sess.run(print_op_dep)\r\n```\r\n\r\nIf \"y = x\" is used instead of \"y = assign_op\", I get this output:\r\n```\r\nI tensorflow/core/kernels/logging_ops.cc:79] x_alias[]\r\nI tensorflow/core/kernels/logging_ops.cc:79] concat_alias[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[]\r\n[]\r\nI tensorflow/core/kernels/logging_ops.cc:79] x_alias[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] concat_alias[0 0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0]\r\n[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] x_alias[0 0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] concat_alias[0 0 0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0]\r\n[0 0]\r\n```\r\nSetting \"y = assign_op\", I get what you expected.  \r\n```\r\nI tensorflow/core/kernels/logging_ops.cc:79] x_alias[]\r\nI tensorflow/core/kernels/logging_ops.cc:79] concat_alias[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0]\r\n[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] x_alias[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] concat_alias[0 0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0]\r\n[0 0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] x_alias[0 0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] concat_alias[0 0 0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0 0]\r\n[0 0 0]\r\n\r\n```\r\nIt appears as though the control_dependencies construct correctly forces assign_op to execute, but the new value isn't really accessible to an evaluation of x until later.  This surpasses my understanding.  Summon @mrry.\r\n", "This is a subtle corner of the `tf.Variable` semantics, which has tripped up a few people. The main thing to note is that when you first *read* a `tf.Variable`\u2014in this case, when it is used as part of the argument to `tf.concat()`\u2014the value read is \"cached\".\r\n\r\nWhat does it mean for a value to be \"cached\"? In the code, it's fed to a `tf.identity()`, which implicitly dereferences the ref-typed variable tensor, but then (perhaps surprisingly) returns a value-typed tensor that *aliases* the buffer used for the variable. This behavior was chosen for distributed (or multi-device) execution, where the aliasing isn't usually noticeable because the reader is typically on a remote device, and the buffer will be copied between devices anyway.\r\n\r\nHowever, when you assign a tensor of a different shape to a `tf.Variable`, the snapshot and current variable value can no longer be aliases, because they're buffers of a different size. \r\n(Aside: If you'd done `tf.assign_add(x, x + 1)` (or something else that preserved the shape of `x`) you would see things happen in the order you expected, because everything happens on the same device, and the \"snapshot\" remains an alias of the underlying buffer.) The `tf.Print()` op gets the old snapshot value, and prints that.\r\n\r\nHow can you avoid this? One way is to force an explicit `x.read_value()`, which forces a new snapshot to be taken, respecting the control dependencies. Changing your program as follows will give the expected output:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# I define a \"shape-able\" Variable                                                                                                                             \r\nx = tf.Variable([], dtype=tf.int32, validate_shape=False, trainable=False)\r\n# I build a new shape and assign it to x                                                                                                                       \r\nconcat = tf.concat([x, [0]], 0)\r\nassign_op = tf.assign(x, concat, validate_shape=False)\r\n\r\nwith tf.control_dependencies([assign_op]):\r\n  # I print x after the assignment                                                                                                                         \r\n  # Note that the Print call is on \"x\" and NOT \"assign_op\"                                                                                                 \r\n  new_x = x.read_value()\r\n  print_op_dep = tf.Print(new_x, data=[new_x], message=\"print_op_dep:\")\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  for i in range(3):\r\n    sess.run(print_op_dep)\r\n```\r\n\r\nThe output is:\r\n\r\n```\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0 0]\r\n```\r\n", "Of course, these semantics are not very intuitive, and there's no way you'd have guessed that from the documentation. @alextp is working on a new version of variables that will have more sensible semantics. I'll let him comment on how things will look in the brave new world.", "\ud83d\udc4d \r\nThanks a lot for this fast answer. I understand now and i've been testing this successfully.\r\n\r\nAlso, one last question on my side: \r\nIs using `tf.assign` in this way (especially the fact that i change the shape) leads to poor optimization on TensorFlow and so, poor performance ?", "Glad to hear it!\r\n\r\nAs for the performance impact, it's hard to say. At the level of individual assign calls, TensorFlow doesn't do much to optimize your code, so you aren't necessarily missing any optimizations. Concatenating and copying like you do in that code snippet will have quadratic time complexity, but I'm not sure if you're going to be doing that in such a tight loop that it matters :). (If you find yourself concatenating dynamic lists of tensors a lot, you might be interested in `tf.TensorArray` instead... it was introduced in part to avoid doing quadratic concatenation when accumulating loop state in a `tf.while_loop()`.)\r\n\r\nIt is possible that having varying-shape variables will lead to e.g. more unknowns in shape inference, which could inhibit some nice optimizations that are possible when the shape of a tensor is static, but I assume you have a reason for wanting to change the shape of a variable, so some amount of dynamism is probably necessary.", "All right, thanks for those insights!"]}, {"number": 7781, "title": "Distribution of sum of several discrete variables", "body": "I've looked for this feature, and I don't think it exists, but it would be really useful for modelling the latent variable models which result in the observation being a **sum** of several discrete variables. This would be especially useful in signal processing, where different signals add up. Additionally, the idea I am presenting below involves clipping, which is also a feature of the signal recording equipment.\r\n\r\nLet's say I have a discrete set of signal values that I am tracking, like so:\r\n```\r\nx = tf.constant(\r\n    [-2.0, -1.0, 0.0, 1.0, 2.0],\r\n    name='x')\r\n```\r\nIt is important for the below that the values in x are sequential and equally spaced. In fact, I am only interested in integer values centered around zero.\r\n\r\nLet's say that some latent variable model produces the probabilities of 3 signals taking on these specific values like so (I am writing as a constant, but it will be in fact a variable):\r\n```\r\np = tf.constant(\r\n    [[0.1, 0.1, 0.2, 0.4,  0.2],\r\n     [0.5, 0.3, 0.1, 0.05, 0.05],\r\n     [0.1, 0.2, 0.4, 0.2,  0.1]],\r\n    name='p')\r\n```\r\nAssuming these three variables are independent, I would now like a function res = f(x, p) that would compute res[i] = p(sum(signal1 + signal2 + signal3) = x[i]) for 0 < i < len(x) -1. \r\n\r\nMoreover, for at the edges if should produce the output like so (clipping): res[0] = p(sum(signal1 + signal2 + signal3) <= x[0]) and res[-1] = p(sum(signal1 + signal2 + signal3) >= x[-1])\r\n\r\nThe output of this operation would be of the same shape as x. The result of the operation would be equivalent to a series of convolutions with clipping: conv(p[-1,:], conv(... conv(p[0, :], p[1, :]))). Here we assume the normal convolution function (not cross-correlation).\r\n\r\nIt might be useful to return the results in the log-space (and be able to accept the inputs as logits too).\r\n", "comments": ["Seems like something you can do with existing ops from your description, and elsewise likely much too specialized to make it into the main repo."]}, {"number": 7780, "title": "fix some documentation style faults for dynamic_rnn, dynamic_rnn_decod\u2026", "body": "fix some documentation style faults for dynamic_rnn, dynamic_rnn_decoder and tensordot\r\nSee: \r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder\r\nhttps://www.tensorflow.org/api_docs/python/tf/tensordot\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 7779, "title": "tensorflow custom op : user_ops", "body": "I want to add my custom op.\r\nso I read https://www.tensorflow.org/extend/adding_an_op.\r\nAnd then I tried to wirte down follow code with your tutorial.\r\nfile name is zero_out.cc and path is ~/tensorflow/tensorflow/core/user_ops.\r\n```\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"ZeroOut\")\r\n    .Input(\"to_zero: int32\")\r\n    .Output(\"zeroed: int32\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n      c->set_output(0, c->input(0));\r\n      return Status::OK();\r\n    });\r\n\r\nclass ZeroOutOp : public OpKernel {\r\n public:\r\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n  void Compute(OpKernelContext* context) override {\r\n    // Grab the input tensor\r\n    const Tensor& input_tensor = context->input(0);\r\n    auto input = input_tensor.flat<int32>();\r\n\r\n    // Create an output tensor\r\n    Tensor* output_tensor = NULL;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\r\n                                                     &output_tensor));\r\n    auto output = output_tensor->flat<int32>();\r\n\r\n    // Set all but the first element of the output tensor to 0.\r\n    const int N = input.size();\r\n    for (int i = 1; i < N; i++) {\r\n      output(i) = 0;\r\n    }\r\n\r\n    // Preserve the first input value if possible.\r\n    if (N > 0) output(0) = input(0);\r\n  }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp);\r\n```\r\n\r\nAnd I made BUILD file in same path.\r\n```\r\nload(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")\r\n\r\ntf_custom_op_library(\r\n    name = \"zero_out.so\",\r\n    srcs = [\"zero_out.cc\"],\r\n)\r\n```\r\n\r\n\r\nAnd then, follow your tutorial, I build with bazel.\r\nI wrote this command on ~/tensorflow.\r\n`bazel build --config opt //tensorflow/core/user_ops:zero_out.so`\r\n\r\ncomplete massage is here.\r\n`Target //tensorflow/core/user_ops:zero_out.so up-to-date:\r\n  bazel-bin/tensorflow/core/user_ops/zero_out.so`\r\n\r\nand I put the exactly same tutorial code but it doesn't work.\r\n```\r\nimport tensorflow as tf\r\nzero_out_module = tf.load_op_library('zero_out.so')\r\nwith tf.Session(''):\r\n  zero_out_module.zero_out([[1, 2], [3, 4]]).eval()\r\n```\r\n\r\n\r\nerror massage is this.\r\n`Traceback (most recent call last):\r\n  File \"/Users/buginus/tf_installwithpip3/Study_TensorFlow/01 - LinearRegression/hi.py\", line 2, in <module>\r\n    zero_out_module = tf.load_op_library('zero_out.so')\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 64, in load_op_library\r\n    None, None, error_msg, error_code)\r\ntensorflow.python.framework.errors_impl.NotFoundError: dlopen(zero_out.so, 6): image not found`", "comments": ["Hm, I went over the tutorial recently and it worked for me...can you try with TF 1.0 and also with `gcc` instead of bazel to build?", "ah okay. I'll try with gcc. I'm already in tensorflow1.0. ", "I tried gcc. compile works too but not for using tf.load_op_library"]}, {"number": 7778, "title": "\"The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations\" in \"Hello, TensorFlow!\" program", "body": "Opening this with reference to https://github.com/tensorflow/tensorflow/issues/7500. \r\n\r\nInstalled TensorFlow 1.0 with reference to https://www.tensorflow.org/install/install_windows on Windows 10 and hit the same issue discussed in https://github.com/tensorflow/tensorflow/issues/7500.  With applying the solution suggested in that thread, the original issue disappeared but got the new warnings: \r\n\r\nC:\\Users\\geldqb>python\r\nPython 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\n2017-02-22 22:28:20.696929: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-22 22:28:20.698285: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-22 22:28:20.700143: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-22 22:28:20.700853: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-22 22:28:20.701498: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-22 22:28:20.702190: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-22 22:28:20.702837: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-22 22:28:20.703460: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n>>> print(sess.run(hello))\r\nb'Hello, TensorFlow!'\r\n", "comments": ["Those are simply warnings. They are just informing you if you build TensorFlow from source it can be faster on your machine. Those instructions are not enabled by default on the builds available I think to be compatible with more CPUs as possible. \r\nIf you have any other doubts regarding this please feel free to ask, otherwise this can be closed.\r\n\r\nedit: To deactivate these warnings as @yaroslavvb suggested in another comment, do the following:\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\r\nimport tensorflow as tf\r\n```\r\nor if you're on a Unix system simply do `export TF_CPP_MIN_LOG_LEVEL=2`.\r\n\r\n`TF_CPP_MIN_LOG_LEVEL` is a TensorFlow environment variable responsible for the logs, to silence INFO logs set it to **1**, to filter out WARNING **2** and to additionally silence ERROR logs (not recommended) set it to **3**", "Thanks Camezim !\r\nAny hint of how to kill those warnings?  ", "@bingq the best way I found to kill unwanted TF output:\r\n\r\nrun your script as `tf.sh myscript.py` where tf.sh contains\r\n\r\n```\r\n#!/bin/sh\r\n# Run python script, filtering out TensorFlow logging\r\n# https://github.com/tensorflow/tensorflow/issues/566#issuecomment-259170351\r\npython $* 3>&1 1>&2 2>&3 3>&- | grep -v \":\\ I\\ \" | grep -v \"WARNING:tensorflow\" | grep -v ^pciBusID | grep -v ^major: | grep -v ^name: |grep -v ^Total\\ memory:|grep -v ^Free\\ memory:\r\n```\r\n\r\nYou can add extra `|grep -v` parts to get rid of more things", "@Carmezim Any estimate of how much faster when compiling from source using advanced CPU instructions? Any reason to do this at all when running most of the graph on a GPU?", "@tomrunia  I haven't tested myself yet (actually am building it now with SSE) although heard 4-8x. If  @yaroslavvb wants to chime in as [he himself got 3x speed improvement](https://github.com/tensorflow/tensorflow/issues/7257#issuecomment-277456370). ", "Yup, 3x for large matrix multiply on Xeon-V3, I expect it's probably due to FMA/AVX rather than SSE", "@tomrunia Well pointed by @yaroslavvb, not SSE specifically in his case although those CPU instructions are expected to provide performance improvement", "It would be very nice to silence these warnings from the Python side, it's not so easy to use grep from Windows.", "Try export TF_CPP_MIN_LOG_LEVEL=2\r\n", "> Try export TF_CPP_MIN_LOG_LEVEL=2\r\n\r\nThanks @yaroslavvb, I haven't tried it yet but an environment variable is definitely more useful.", "It works, thank you", "Is it possible that these errors are coming from the fact that with MSVC, x64, SSE2 is implicit (all x64 chips have SSE2) but the `__SSE2__` et al defines are not explicitly set?\r\nPerhaps the guards should be on EIGEN_VECTORIZE_SSE2 etc instead.", "Could you say what should I exactly do according to your idea? What does \"guards on\" mean?", "The SSE warnings use code like this:\r\n```cpp\r\n#ifndef __SSE__\r\n    WarnIfFeatureUnused(CPUFeature::SSE, \"SSE\");\r\n#endif  // __SSE__\r\n#ifndef __SSE2__\r\n    WarnIfFeatureUnused(CPUFeature::SSE2, \"SSE2\");\r\n#endif  // __SSE2__\r\n```\r\nBut the Eigen imeplementation (eigen/Eigen/Core) uses more complicated logic to work out whether to use SSS1/2:\r\n```cpp\r\n#ifndef EIGEN_DONT_VECTORIZE\r\n  #if defined (EIGEN_SSE2_ON_NON_MSVC_BUT_NOT_OLD_GCC) || defined(EIGEN_SSE2_ON_MSVC_2008_OR_LATER)\r\n\r\n    // Defines symbols for compile-time detection of which instructions are\r\n    // used.\r\n    // EIGEN_VECTORIZE_YY is defined if and only if the instruction set YY is used\r\n    #define EIGEN_VECTORIZE\r\n    #define EIGEN_VECTORIZE_SSE\r\n    #define EIGEN_VECTORIZE_SSE2\r\n\r\n    // Detect sse3/ssse3/sse4:\r\n    // gcc and icc defines __SSE3__, ...\r\n    // there is no way to know about this on msvc. You can define EIGEN_VECTORIZE_SSE* if you\r\n    // want to force the use of those instructions with msvc.\r\n    #ifdef __SSE3__\r\n      #define EIGEN_VECTORIZE_SSE3\r\n    #endif\r\n    #ifdef __SSSE3__\r\n      #define EIGEN_VECTORIZE_SSSE3\r\n    #endif\r\n    #ifdef __SSE4_1__\r\n      #define EIGEN_VECTORIZE_SSE4_1\r\n    #endif\r\n    #ifdef __SSE4_2__\r\n      #define EIGEN_VECTORIZE_SSE4_2\r\n    #endif\r\n    #ifdef __AVX__\r\n      #define EIGEN_VECTORIZE_AVX\r\n      #define EIGEN_VECTORIZE_SSE3\r\n      #define EIGEN_VECTORIZE_SSSE3\r\n      #define EIGEN_VECTORIZE_SSE4_1\r\n      #define EIGEN_VECTORIZE_SSE4_2\r\n    #endif\r\n```\r\nDue mainly to the fact Visual Studio assumes SSE1/SSE2 when compiling for x64.\r\nAlso note that is depends on EIGEN_DONT_VECTORIZE - perhaps some user customization.\r\n\r\nSo one solution would be to #include eigen/Eigen/Core and use the \"EIGEN_VECTORIZE_SSE\" symbols in the conditional code-guard (\"`#ifndef EIGEN_VECTORIZE_SSE`\"),\r\nI'm not 100% sure about the build system and whether Eigen is the only source of SSE operations, so I'm not 100% sure that this is the right answer.\r\n\r\nI'm also not sure what is the right thing to do if building a binary for distribution.  Do you include AVX and risk it not running, or do you not include it and risk the warning (and low performance)? Ideally you would build with full vectorization and let the software choose at runtime.  I guess another possibility would be to build 2 dlls, and dynamically load the right one at runtime.", "You can try [this post](http://stackoverflow.com/a/42755665/4503060) and tell me if it really becomes faster.", "@Juanlu001 Check this [comment](https://github.com/tensorflow/tensorflow/issues/1258#issuecomment-261365022) for how this variable works and @yaroslavvb's code below for a handy way to change it.", "Thanks @Carmezim \r\nSetting the env. variable `TF_CPP_MIN_LOG_LEVEL=3` via the os package inside the code worked :+1: ", "@hughsando, these are in fact the debates we had internally. Ideally in the future we'd be able to ship all compiled versions and choose at runtime, but logistically that's actually quite time consuming and tricky to implement. We are looking into it, but this was the best solution we had for now. We wanted people to know that if they have that warning, things are working, but it could be faster if you build it yourself. I.e. if you benchmarking our system, it's not a valid benchmark w/o compiling it with the best optimizations.", "@hughsando PS people have been uploading wheels built for their favorite configuration to https://github.com/yaroslavvb/tensorflow-community-wheels ", "I wonder if a solution to this problem might to to treat the cpu\narchitectures as different processors, like \"cpu\", \"gpu\", \"cpu-avx\".  Then\nthe kernels can register multiple versions, and the runtime can choose\nwhere to run depending on the client machine.\nThis could be done for the main \"hot\" inference functions (eg, Conv2D), and\nthe deployment problem would largely go away.\nCustom builds for training is not so difficult to manage - the community\nwheels look like a good idea.\n\n\nOn Sun, Apr 2, 2017 at 10:43 PM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> @hughsando <https://github.com/hughsando> PS people have been uploading\n> wheels built for their favorite configuration to\n> https://github.com/yaroslavvb/tensorflow-community-wheels\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7778#issuecomment-290990644>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABlp1q-aGPy-lna8-2DLAo7xGrl04_3Wks5rr7P9gaJpZM4MIq5A>\n> .\n>\n", "@hughsando, in spirit that is the idea we would like to pursue. However, it is more difficult than that in that we use Eigen for a lot of the implementations of kernels. Eigen would have to be compiled multiple ways without causing any symbol conflicts int the final binary, and also we would probably have to break up modules into more dsos so as not to have too large of a binary resident.", "So after reading these, I went ahead and reinstalled, this time from Source following instructions on [https://www.tensorflow.org/install/install_sources](https://www.tensorflow.org/install/install_sources). I still see the \"The TensorFlow library wasn't compiled to to use XXXX instructions...\" warnings. So did I miss something or is installing from source and building is not what you meant by \"building it yourself\"?", "What did you put for the compiler optimization options ./configure asked you)? and what are the remaining warnings it shows you? Are you running the binary on the same machine you compiled it on? ", "Thank you aselle, my problem is solved now but here is what happened:\r\n- I used all default options with ./configure except: 1) Y for CUDA support, and 2) compute capability\r\n- The warnings were about the SSE3, SSE4.1, SSE4.2, AVX, AVX2, and FMA capabilities of my machine (+ negative NUMA node read)\r\n- Yes, the same machine...\r\n\r\nHowever, after reading your message, I changed my bazel build command from \r\n`bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package ` to \r\n`bazel build -c opt --copt=-march=native --config=cuda //tensorflow/tools/pip_package:build_pip_package` \r\nwith that extra `--copt=-march=native` in there and that did the trick. The warnings on instructions disappeared (albeit the NUMA warning still remaining). So although my problem seems to be solved, I wonder if it is possible that the `-march=native` is not really the default option for the \"optimization flags to use\" question in the ./configure options?", "@gunan, is the default different as @hekimgil suggests. Is this expected behavior?", "To fix the error messages, the bazel build command should be:\r\n`bazel build --config opt --config cuda tensorflow/tools/pip_package:build_pip_package`\r\n\r\n`-c opt` != `--config opt`\r\n-c means \"--compilation_mode\" ", "Thanks @gunan and @aselle and sorry for the confusion.\r\n\r\nI saw the `-c opt` option in another website and wrongfully assumed it was equivalent to `--config=opt`... To double-check, I ran the bazel build again with `--config=opt` and without any extras (that is, no `--copt=-march=native`) and it works just fine...\r\n\r\nApologies for taking your time and thanks again for your responses...", "Sorry for bothering, but I think the second question by @tomrunia was not answered? While it is clear that the CPU runtimes benefit a lot by compiling TensorFlow from source with optimizations enabled, I am also wondering if it has an impact on the runtimes when using the GPU-version?\r\nI guess it should not make much of a difference for the GPU-version of TensorFlow?", "@Daniel451, not as much,  but @zheng-xq can comment further.", "Depending on the model, it could make quite a bit of difference when the model has a lot to process with its input pipeline. ", "Ok, thanks for the clarification!", "Well is there an official (or semi-official) build available that uses the \"advanced\" CPU instruction set?\r\nMaybe add an official release for PIP, something like \"tensorflow-sse\" and \"tensorflow-gpu-sse\"?\r\n\r\nAfter the official stable release of 1.0 I was kind of looking forward to not having to build TensorFlow myself anymore.\r\nBut if that implies taking a serious performance hit, I guess it's back to self-compiling again. :unamused:", "I am unfamiliar with SSE, AVX, and FMA. I'm confused why I would be getting the same CPU warnings when I'm computing on my GPU. Furthermore, when I upgraded to r1.0 my compute time was significantly increased. What is going on? I can confirm that Tensorflow is using the GPU by the following initialization messages:\r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 1070\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.797\r\npciBusID 0000:03:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 6.98GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:03:00.0)\r\n```\r\n \r\nFurthermore, I can see Tensorflow is using GPU memory:\r\n\r\n```\r\nvolcart@volcart-Precision-Tower-7910:/usr/bin$ nvidia-smi\r\nTue Apr 11 12:25:36 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1070    Off  | 0000:03:00.0      On |                  N/A |\r\n| 17%   59C    P2    38W / 185W |   7871MiB /  8105MiB |      1%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1120    G   /usr/lib/xorg/Xorg                             412MiB |\r\n|    0      2188    G   compiz                                         244MiB |\r\n|    0      2674    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    48MiB |\r\n|    0      3529    G   ...eCTForProblematicRoots/disabled/ExpectCTR    72MiB |\r\n|    0     15786    C   python3                                          8MiB |\r\n|    0     20754    C   python3                                       7082MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nWhy is my training so much slower now?\r\n\r\n> ", "Hi all,\r\n\r\nI'm using Macbook Pro 2015 (8gb) to do some simple feature extraction with only CPU support. I first easily installed tf by pip but got those warnings. Thanks all above and I successfully recompiled tf from source follow the official instruction. Here're some result I want to share:\r\n- Compiling (installing from source) time: about 1.5 hour\r\n  (There're a lot warnings during compilation but it works at last, hopefully. Just follow the [instruction](https://www.tensorflow.org/install/install_sources))\r\n- CPU computing speed up: about 2x\r\n  (I'm using ResNet-50 to do feature extraction, the pip version takes about 85s a batch v.s. source version takes about 41s)\r\n\r\nHope this will help. But still, GTX will be a much better choice...", "I have successfully built the GPU version from source, but I'm not sure if the GPU is being used. Ill attach output below, but basically I don't see the CUDA imports. Does this mean something went wrong? \r\n\r\n`Using TensorFlow backend.\r\nFound 2125 images belonging to 2 classes.\r\nFound 832 images belonging to 2 classes.\r\ndemo.py:64: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=<keras.pre..., steps_per_epoch=128, epochs=5, validation_steps=832)`\r\n  nb_val_samples=nb_validation_samples)\r\nEpoch 1/5\r\n2017-04-12 20:41:32.502286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:865] OS X does not support NUMA - returning NUMA node zero\r\n2017-04-12 20:41:32.502390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \r\nname: GeForce GT 750M\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.9255\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.74GiB\r\n2017-04-12 20:41:32.502402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \r\n2017-04-12 20:41:32.502405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \r\n2017-04-12 20:41:32.502412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)`", "Here is the build command I used to get rid of those warnings by actually optimizing TensorFlow to my CPU:\r\n\r\n```bash\r\nsudo bazel build --config opt --copt=-msse4.1 --copt=-msse4.1 --copt=-mavx --copt=-mavx2 --copt=-mfma //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n\r\n", "@jshin49: Why are you using `--copt=-msse4.1` twice?  Or is the second one a typo, and you meant to write `--copt=-msse4.2`?", "@logological: You are right. It is a typo and I meant msse4.2.\r\n\r\nThanks for pointing it out!", "Just as a matter of interest: How comes, these warnings were not present in Tensorflow 1.0.1 but only in newer version Tensorflow 1.1.0?", "> Just as a matter of interest: How comes, these warnings were not present in Tensorflow 1.0.1 but only in newer version Tensorflow 1.1.0?\r\n\r\nNot quite sure what you mean, I am using the binaries from PIP for TF 1.0.1, and the warnings have been there already.", "Aha... then maybe my cached wheel of TF 1.0.1 was somehow different. It displayed some warnings regarding OpKernels but no warnings regarding SSE instructions.", "@jshin49 and others who have tried it\r\nHow much performance improvement are you seeing by recompiling with compiler optimizations?\r\n", "No performance improvements here, when activating AVX, but probably because I was building and using the GPU-version. I thought it would speed up some parts at least, but I didn't find it to make a big impact. Probably when using the CPU-version it does make an impact.", "[Single Instruction Multiple Data](https://www.kernel.org/pub/linux/kernel/people/geoff/cell/ps3-linux-docs/CellProgrammingTutorial/BasicsOfSIMDProgramming.html) makes sense when you perform vector computations on the CPU.  Thanks for the build-command examples.", "@apacha  you should uninstall tensorflow and then reinstall again to kill the OpKernels warnings ", "If I do `export TF_CPP_MIN_LOG_LEVEL=2` then console don't show results, it just empty. \r\nand if I do `export TF_CPP_MIN_LOG_LEVEL=0` then only it popsup with all the warning and ***results***\r\n```\r\n2017-06-24 07:02:26.650752: W tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\r\n2017-06-24 07:02:26.851332: I tensorflow/examples/label_image/main.cc:251] withoutshadow (0): 0.801019\r\n2017-06-24 07:02:26.851356: I tensorflow/examples/label_image/main.cc:251] withshadow (1): 0.198981\r\n```\r\nany thoughts why this happening?\r\n", "I am also getting this  Warning while using tensorflow api ' s  in java,I have no idea why i am getting this.Any idea to resolve this warning!", " if you want to disable them, you may use the code below\r\n\r\n    import os\r\n    os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\r\n    import tensorflow as tf\r\n\r\nthis should silence the warnings. 'TF_CPP_MIN_LOG_LEVEL' represents the Tensorflow environment variable responsible for logging. Also if you are on Ubuntu you may use this code below\r\n\r\n    export TF_CPP_MIN_LOG_LEVEL=2 \r\n\r\nI hope this helps.", "@Carmezim  If I compute with GPU, will I got this warning?\r\nAnd how to know if tensorflow run with GPU or CPU?\r\nThanks.", "Yes. You will still get the warning,  even if you install and run the GPU-version. Because this warning come from CPU, when your operation run on CPU and  can be speed up by SSE. I think you can ignore these warnings if you are not so concern with speed. If you do want go faster, you need install tensorflow from source to optimize tensorflow for some specific instruction set.\n\n \n\nThe easiest way of check if tensorflow run on CPU or GPU is open a resource manager for GPU or CPU (ps, nvidia-smi) to check loading when you run your training. \n\nYou can also log the device placement ( config=tf.ConfigProto(log_device_placement=True) ), or use tensorboard to check it.\n\n \n\nFrom: bounces+848413-6da0-scotthuang1989=163.com@sgmail.github.com [mailto:bounces+848413-6da0-scotthuang1989=163.com@sgmail.github.com] On Behalf Of g10guang\nSent: Thursday, August 31, 2017 12:39 PM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] \"The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations\" in \"Hello, TensorFlow!\" program (#7778)\n\n \n\n@Carmezim <https://github.com/carmezim>  If I compute with GPU, will I got this warning?\nAnd how to know if tensorflow run with GPU or CPU?\nThanks.\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/7778#issuecomment-326187505> , or mute the thread <https://github.com/notifications/unsubscribe-auth/AFFDdqPYNDB7QseUwJ6eNtkU1oR_0t9Tks5sdjjagaJpZM4MIq5A> .  <https://github.com/notifications/beacon/AFFDdkhWIJERtKgV7ARwVUhuVRDlxEGbks5sdjjagaJpZM4MIq5A.gif> \n\n", "@g10guang Yeah, TF can use both CPU and GPU but even if you're using GPU only it will inform you of the SIMD instructions available when you run the code.\r\n\r\nTo know in which device TF is running you can set `log_device_placement` to `True` when creating the session as in:\r\n`sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))`\r\n\r\nYou can see more details on this under **Logging Device placement** in the [documentation](https://www.tensorflow.org/tutorials/using_gpu).\r\n", "Follow instructions [here](https://github.com/lakshayg/tensorflow-build) (Just one instruction !!!)\r\nIt is amazing. Time taken for for a training step is halved. #8037 ", "Thanks @Carmezim  !\r\n", "I am getting the same warning while using tensorflow_gpu 1.1.0 on win10, the version of Python is 3.6.3 installed in Anaconda.\r\nSince the tf for **G**PU version is used, then what does that matters **C**PU?\r\nI want to know what do these warnings mean, and what should I do or just leave them?\r\n\r\nHere is my warnings:\r\n\r\n```\r\nC:\\DevTools\\Anaconda3\\envs\\py36_tfg>python\r\nPython 3.6.3 | packaged by conda-forge | (default, Dec  9 2017, 16:22:46) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session()\r\n2017-12-15 09:59:27.506604: W c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-12-15 09:59:27.507839: W c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-12-15 09:59:27.509196: W c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-12-15 09:59:27.509641: W c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-12-15 09:59:27.510098: W c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-12-15 09:59:27.510475: W c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-12-15 09:59:27.512253: W c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-12-15 09:59:27.512821: W c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-12-15 09:59:27.824267: I c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:887] Found device 0 with properties:\r\nname: GeForce GTX 1060\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6705\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.00GiB\r\nFree memory: 2.43GiB\r\n2017-12-15 09:59:27.824508: I c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:908] DMA: 0\r\n2017-12-15 09:59:27.826709: I c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:918] 0:   Y\r\n2017-12-15 09:59:27.827566: I c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)\r\n```\r\n\r\nAnother question:\r\nHow will I know if the tf using Gpu or Cpu while runing a program such as object detection in a video, are there any tools suggested for monitoring my devices?\r\n\r\nThanks a lot.\r\n: )", "Instead of removing the warning is there any way to use those SSE instructions to speed up the training", "@gknight7 \r\nPlease check my comment above."]}, {"number": 7777, "title": "Make ExamplePartialRun hidden", "body": "This is follow up of https://github.com/tensorflow/tensorflow/pull/7667\r\n\r\nSince using op package in tf package causes cyclic dependencies,\r\nusing wrapper ops in `util_test.go`  is required. But the interface of\r\nthese ops are different to public one.\r\nIn order to avoid confusing of API usage, it is better to hide ExamplePartialRun\r\nand add note of reference to `op` document.", "comments": ["Can one of the admins verify this patch?", "I'd prefer to leave it as an example perhaps with a comment explaining that you should use the ops from the ops package instead.", "Closing for now, but feel free to make a new pull request if you'd like to add a comment for clarification."]}, {"number": 7776, "title": "Update Performance guide compiler recommendation for macOS users", "body": "Hi!  \r\n\r\nAs per the [Performance guide](https://www.tensorflow.org/performance/performance_guide) I tried to build TF from source using gcc version 4.8.3+ on a MacBook.  I tried gcc 4.9 and 6.3 (Homebrew versions).  I got the following errors (same errors for 6.3):\r\n\r\n- gcc-4.9: error: unrecognized command line option '-fcolor-diagnostics'\r\n- gcc-4.9: error: unrecognized command line option '-Wthread-safety'\r\n- gcc-4.9: error: unrecognized command line option '-Wself-assign'\r\n\r\nAccording to [one](https://github.com/tensorflow/tensorflow/issues/1192) of the last years issues with the same errors, this is due to the fact that \r\n\r\n> We generally use the default of clang for OS X compilation, which explains your error. When I run gcc --version I get Apple LLVM version 7.0.2 (clang-700.1.81). You should try switching to clang instead of gcc, since that's the supported approach.\r\n\r\nIf this is still the case and all macOS users should stick to clang, could you please update the performance guide accordingly?\r\n\r\n\r\nBest,\r\nAndrei\r\n\r\n", "comments": ["This is on me.  Thank you for the feedback.  I will update the document.  ", "https://github.com/tensorflow/tensorflow/pull/10781\r\n\r\nApologies that I did not understand the issue as much until I just started building on OS X as of last night.  Added the PR where I added the small change.  I hope to re do the build in the new few weeks but this is a great little add to have incase it takes a long time.  ", "PR with updates has been merged it may take a bit for the website to be updated but the tweak is in.  Again as I said on another thread, I hope to redo the perf guide with everything that has been learned in the past 3-4 months by me personally and what I learned from all of the input from the community.  There is so much info to add.  Thank you for the feedback.  "]}, {"number": 7775, "title": "TF-SLIM : Tensor name \"InceptionV3/*/*/*/*/ not found in checkpoint files", "body": "Similar problems are in stackoverlow, however they are using flowers example with flowers_train. Using\r\nhttps://github.com/tensorflow/models/tree/master/slim Fine-tuning a model from an existing checkpoint. The  moving_variance and moving_mean have both had the NotFoundError.\r\n\r\n\r\nThe following train_iamge_classifier\r\n\r\ntrain_image_classifier_nx.py \\\r\n--checkpoint_path=data/model/inception-v3/model.ckpt-157585 \\\r\n--checkpoint_exclude_scopes=InceptionV3/Logits,InceptionV3/AuxLogits/Logits \\\r\n--trainable_scopes=InceptionV3/Logits,InceptionV3/AuxLogits/Logits \\\r\n--model_name=inception_v3 \\\r\n--train_dir=data/test/train \\\r\n--dataset_dir=data/test/traindata\r\n\r\n\r\nGPU initializes and immediately produces the traceback as follows:\r\n\r\nNotFoundError (see above for traceback): Tensor name\r\n\"InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance\" not\r\nfound in checkpoint files\r\ndata/model/inception-v3/model.ckpt-157585\r\n         [[Node: save/RestoreV2_304 = RestoreV2[dtypes=[DT_FLOAT],\r\n         _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0,\r\n         save/RestoreV2_304/tensor_names,\r\n         save/RestoreV2_304/shape_and_slices)]]\r\n                  [[Node: save/RestoreV2_70/_653 =\r\n                  _Recv[client_terminated=false,\r\n                  recv_device=\"/job:localhost/replica:0/task:0/gpu:0\",\r\n                  send_device=\"/job:localhost/replica:0/task:0/cpu:0\",\r\n                  send_device_incarnation=1,\r\n                  tensor_name=\"edge_1621_save/RestoreV2_70\",\r\n                  tensor_type=DT_FLOAT,\r\n                  _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\n\r\n", "comments": ["Does this help? https://github.com/tensorflow/tensorflow/issues/4249", "arg_scope is mentioned by others, however i was following the documentation\nusing the command line train example which has no arg scope option.\n\nOn Wed, Feb 22, 2017 at 3:57 PM, Paul Tucker <notifications@github.com>\nwrote:\n\n> Does this help? #4249\n> <https://github.com/tensorflow/tensorflow/issues/4249>\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7775#issuecomment-281800841>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABXVTUP6LkI2daBztL8XAT933Dz4iqpIks5rfKEtgaJpZM4MImli>\n> .\n>\n", "The problem may be due to the fact that the TF-SLIM effort focused on\nPython 2.7 and I'm using 3.5 on windows. I just tried the same TF-SLIM with\nTF 1.0 and many more issues, it seems few can keep up with all progress on\nTF and now with TF1.0 there have been more changes making TF1.0\ncompatibility with TF-SLIM more challenging. It seems we need a list\nstating with contrib/etc. is compatible what version of TF.\n\n\nOn Wed, Feb 22, 2017 at 3:57 PM, Paul Tucker <notifications@github.com>\nwrote:\n\n> Does this help? #4249\n> <https://github.com/tensorflow/tensorflow/issues/4249>\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7775#issuecomment-281800841>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABXVTUP6LkI2daBztL8XAT933Dz4iqpIks5rfKEtgaJpZM4MImli>\n> .\n>\n", "The checkpoints get harder to deal with as TF moves forward.  One example is we would like to add fused batch_norm to slim to speed up training but that will likely further cause issues with the checkpoint.  \r\n\r\nI am marking this item as closed as I do not think this will get resolved until we redo this and create new checkpoints.  I know it does not help you but I am sorry that you had trouble."]}, {"number": 7774, "title": "after type 'run' in tensorflow debugger, the terminal reappears and stucks", "body": "**My program can run and show some traing results, but for a classification question, the accuracies of training set and validation set both keep at around 0.5 for 40 epoches.\r\nSo I want to use tensorflow debugger to watch what the variables are.\r\nWhen I type 'run' and push enter, it jump out of the debugger and the terminal reshows, then the terminal screen stuck at below.** \r\n\r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\r\npciBusID 0000:01:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.28GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\r\n\r\n\r\n**the nvidia-smi results is below:**\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080    Off  | 0000:01:00.0      On |                  N/A |\r\n|  0%   43C    P2    53W / 200W |   1857MiB /  8105MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1011    G   /usr/lib/xorg/Xorg                             300MiB |\r\n|    0      1802    G   compiz                                         133MiB |\r\n|    0      3362    G   .../Enabled/MarkNonSecureAs/show-non-secure-    99MiB |\r\n|    0     14422    C   python                                        1321MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n-------------------------------------------------------------------------------------------------------------------------------------------\r\n**any suggestion for solve the problem?**", "comments": ["@jetW Does your graph contain any tf.while loops? If so, it may get stuck because of a bug in tfdbg in version 1.0.0 that causes the debugger to freeze on certain while loops. This bug has been fixed in master/HEAD. Maybe you can try our [nightly builds](https://github.com/tensorflow/tensorflow/#installation), or wait for the next release for the fix.", "The graph does not have tf.while. I may wait for the next release. Thank you!", "Has the tf.while loop freeze in tfdbg been fixed in any new builds?", "@caisq Has this problem been resolved?\r\n\r\nI'm having a similar problem when using tfdbg. My network is returning nan after 1 optimization step, so I'm trying to resolve what's going on. My problem is same as above: \"when I type 'run' and push enter, it jump out of the debugger and the terminal reshows, then the terminal screen stuck at below.\" There are no errors or warnings of any kind. Also, if I use the invoke_stepper, it freezes during some while steps indefinitely (or at least several hours until I cut it off). In particular, it freezes on steps that look like `bilstm1/bilstm1/bidirectional_rnn_1/fw/fw/while/layer_norm_basic_lstm_cell/state_1/batchnorm/sub/Enter`\r\n\r\nI definitely have while loops from tf.nn.bidirectional_dynamic_rnn and tf.map_fn, but I set parallel_iterations=1 for all of those calls. I read as well that there are some bugs with while loops on gpu's, so I have tried running all ops `with tf.device(\"/cpu:0\"):` as well with the same results.\r\n\r\nAny ideas I can try to get tfdbg to work in this situation?\r\n\r\nI am using the nightly build of tensorflow_gpu-1.1.0rc2-cp35-cp35m-win_amd64.whl (Build #149 (Apr 19, 2017 2:25:00 AM) which is the last stable build)", "There have been a few bug fixes for tfdbg related to while loops in the past several weeks. @fredtony , can you please let me know what version of TensorFlow you are using?", "@caisq I a using 1.1.0-rc2. I have the nightly build (Build #149 (Apr 19, 2017 2:25:00 AM) which is the last stable build). Revision ed6bb758dabc612234e9afd38bdaeb1b84e09955", "@fredtony Thanks for the info. I was under the impression that this PR (https://github.com/tensorflow/tensorflow/commit/99559a3455d7c3b3663968a55bf56801743201d0) should have resolve a previous problem related to tf.while_loop. This commit is in both the master and r1.1 branches. But there might be other corner cases not covered here. \r\n\r\nCan you provide some sample code (and some fake data) to reproduce the issue?"]}, {"number": 7773, "title": "ADD: section for troubleshooting in readme.md", "body": "I added a section with information on the problems I ran into getting the HVX sample application to run on my OpenQ 820 develoment board.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!\n\nOn Wed, Feb 22, 2017 at 11:08 AM, googlebot <notifications@github.com>\nwrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> verify. Thanks.\n> ------------------------------\n>\n>    - If you've already signed a CLA, it's possible we don't have your\n>    GitHub username or you're using a different email address. Check your\n>    existing CLA data <https://cla.developers.google.com/clas> and verify\n>    that your email is set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - If you signed the CLA as a corporation, please let us know the\n>    company's name.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/7773#issuecomment-281625003>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFRAOVSl5Zn1CfIvAP4NVRS7-E5rkFzWks5rfAkvgaJpZM4MIcFp>\n> .\n>\n", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 7772, "title": "where is the gradient computation of a variable in cpu", "body": "There are some variables of a neural network. Some are in GPU, and some are in CPU. For the variables in CPU, where are their gradients computed? CPU or GPU?\r\nFor example, where is the gradient of `c` computed?\r\n```python\r\nwith tf.device('/gpu:0'):\r\n  a = tf.get_variable(\"a\", [100, 3])\r\n  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\r\n  with tf.device('/cpu:0'):\r\n    c = tf.matmul(a, b)\r\n  d = tf.matmul(b,c)\r\n```\r\nThanks very much.", "comments": ["An interesting problem...", "You can see all the variables and operations placement by adding config=tf.ConfigProto(log_device_placement=True) to session creation.\r\nThe gradient computation operations will probably be placed on the device where the optimzer was placed (you don't have it in your example).\r\nBTW your example doesn't make sense, b and c are incompatible for matmul, and also the device placements are nested.\r\n", "From the documentation at https://www.tensorflow.org/tutorials/using_gpu\r\n\r\nIf a TensorFlow operation has both CPU and GPU implementations, the GPU devices will be given priority when the operation is assigned to a device. For example, matmul has both CPU and GPU kernels. On a system with devices cpu:0 and gpu:0, gpu:0 will be selected to run matmul.\r\n\r\n\r\n", "I have used  config=tf.ConfigProto(log_device_placement=True) to see the device placement log. The gradients are computed in GPU preferentially."]}, {"number": 7771, "title": "Update deepdream.ipynb", "body": "Gives an error saying it can take only one argument so removed 'utf-8', which solves the problem", "comments": ["Can one of the admins verify this patch?", "????", "For version 2.7 it doesn't require UTF-8", "Jenkins, test this please"]}, {"number": 7770, "title": "\"GraphDef cannot be larger than 2GB.\" error when saving model after assigning variables", "body": "I want to use a pretrained model to warmly start another model with a little difference. Simply, I create a new model, and assign the variables with same name with pretrained model weights. But, when saving the model, error occurred.\r\n```\r\nTraceback (most recent call last):\r\n  File \"tf_test.py\", line 23, in <module>\r\n    save_path = saver.save(sess, \"./model.ckpt\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1308, in save\r\n    self.export_meta_graph(meta_graph_filename)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1331, in export_meta_graph\r\n    graph_def=ops.get_default_graph().as_graph_def(add_shapes=True),\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2268, in as_graph_def\r\n    result, _ = self._as_graph_def(from_version, add_shapes)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2231, in _as_graph_def\r\n    raise ValueError(\"GraphDef cannot be larger than 2GB.\")\r\nValueError: GraphDef cannot be larger than 2GB.\r\n```\r\n\r\nThe example code is as follow:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Create some variables.\r\nv1 = tf.get_variable(\"L_enc\", [400000, 1024])\r\nv2 = tf.get_variable(\"L_dec\", [400000, 1024])\r\n\r\n# Add an op to initialize the variables.\r\ninit_op = tf.initialize_all_variables()\r\n\r\n# Add ops to save and restore all the variables.\r\nsaver = tf.train.Saver(tf.all_variables())\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(init_op)\r\n  for v in tf.trainable_variables():\r\n    embedding = np.random.uniform(-1, 1, (400000, 1024))\r\n    sess.run(v.assign(embedding))\r\n  # Save the variables to disk.\r\n  save_path = saver.save(sess, \"./model.ckpt\")\r\n  print(\"Model saved in file: %s\" % save_path)\r\n```\r\n", "comments": ["You are modifying your graph when you do `v.assign(embedding)`, and the graph exceeds 2GB limit. Asking on Stackoverflow may help finding strategies to make your graphs smaller", "@iamhankai Hello, I am doing the similar operations as you, and meet the same issue. Have you solved it?", "Nope", "With keras, you can use `keras.backend.clear_session()` in loop to clear graph.\r\nWith TF, try: https://stackoverflow.com/questions/36349049/overcome-graphdef-cannot-be-larger-than-2gb-in-tensorflow#new-answer", "**tf.reset_default_graph()**    this alone will solve the problem"]}, {"number": 7769, "title": "How to implement a custom Estimator with multiple model_fns?", "body": "How to implement a estimator with multiple model_fn so that we can train GAN-like models like following:\r\n```\r\ngan = my_estimator(model_fn=multi_model_fn, params=params)\r\nfor i in range(nb_train):\r\n  gan.fit(x, y, model_id='gen', ...)\r\n  gan.fit(x, y, model_id='cri', `...)\r\n```", "comments": ["This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag.", "I came to the same problem.  \r\n\r\nJust quickly scanned through the [code](https://github.com/tensorflow/tensorflow/blob/107cc777af7880c140d089e44ad898a6ba929286/tensorflow/python/estimator/estimator.py#L629), and it seems that there are no ways other than **hacking the internal implementation**.\r\n\r\nMy 2 cents on this issue is that maybe the `Estimator` could be more flexible.\r\n\r\nFor example, one possible solution on top of my head is that the `Estimator.train` function may take another optional argument, i.e., the `mode`, so that in the `Estimator.__init__`, we could create multiple `train_op` based on user-defined modes. \r\n\r\n```python\r\ndef model_fn(features, labels, mode, params):\r\n  if mode == 'train.discriminator':\r\n    train_op = ...\r\n    loss = ...\r\n  elif mode == 'train.generator':\r\n    train_op = ...\r\n    loss = \r\n  return tf.estimator.EstimatorSpec(...)\r\n\r\ngan = tf.estimator.Estimator(model_fn=model_fn, ...)\r\nfor _ in range(10):\r\n  gan.train(input_fn=input_fn, mode='train.discriminator', steps=2)\r\n  gan.train(input_fn=input_fn, mode='train.generator', steps=2)\r\n```", "There is some work going at [`tf.contrib.gan`][1] however I doubt there will be any compatibility with the `tf.estimator` interface as this would probably require api changes.\r\n\r\n[1]: https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/gan", "In the next 24 hours, I will opensource a `GANEstimator`, which is a tf.Estimator backed by TFGAN. I'll update the thread when I do.", "I submitted the code. It'll get added to github at the next sync, no later than EOD monday.", "I think it is not just GAN, many other models may need a more flexible interface, e.g., adversarial transform network, where training affects a different part of the model.", "The `GANEstimator` is synced to opensource TF: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/gan/python/estimator/python. You should be able to use this example to solve the general problem of training different parts of the model.", "When does tfgan appear in the docs? I only see `tf.contrib.gan` modules from 9/21.\n\n> Am 03.10.2017 um 19:23 schrieb Joel Shor <notifications@github.com>:\n> \n> Closed #7769 <https://github.com/tensorflow/tensorflow/issues/7769>.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/7769#event-1276430134>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABsq8qqrJndHLultS-6E9l7-yl3oCQvTks5som11gaJpZM4MIYGY>.\n> \n\n", "> The `GANEstimator` is synced to opensource TF: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/gan/python/estimator/python. You should be able to use this example to solve the general problem of training different parts of the model.\r\n\r\nThank you for your code. But I saw the code \"scalar_loss = gan_loss.generator_loss + gan_loss.discriminator_loss\", so the hyperparameter k is always equal to 1. If I want to change this hyperparameter, how can I do?  Below is the k where it is from.\r\n![image](https://user-images.githubusercontent.com/3667242/55311992-fd1eea00-5496-11e9-9ec7-f15ad847b668.png)\r\n", "I encountered the same problem. I want to run two different models with different graphs, before I could use tf.Session() to set two graphs as default graph separately when using one of them, but now I don't know how to use them in Estimator...", "> I came to the same problem.\r\n> \r\n> Just quickly scanned through the [code](https://github.com/tensorflow/tensorflow/blob/107cc777af7880c140d089e44ad898a6ba929286/tensorflow/python/estimator/estimator.py#L629), and it seems that there are no ways other than **hacking the internal implementation**.\r\n> \r\n> My 2 cents on this issue is that maybe the `Estimator` could be more flexible.\r\n> \r\n> For example, one possible solution on top of my head is that the `Estimator.train` function may take another optional argument, i.e., the `mode`, so that in the `Estimator.__init__`, we could create multiple `train_op` based on user-defined modes.\r\n> \r\n> ```python\r\n> def model_fn(features, labels, mode, params):\r\n>   if mode == 'train.discriminator':\r\n>     train_op = ...\r\n>     loss = ...\r\n>   elif mode == 'train.generator':\r\n>     train_op = ...\r\n>     loss = \r\n>   return tf.estimator.EstimatorSpec(...)\r\n> \r\n> gan = tf.estimator.Estimator(model_fn=model_fn, ...)\r\n> for _ in range(10):\r\n>   gan.train(input_fn=input_fn, mode='train.discriminator', steps=2)\r\n>   gan.train(input_fn=input_fn, mode='train.generator', steps=2)\r\n> ```\r\n\r\nHi, I have the same problem, so I wonder if the method you provide works well. Thank you!", "> > I came to the same problem.\r\n> > Just quickly scanned through the [code](https://github.com/tensorflow/tensorflow/blob/107cc777af7880c140d089e44ad898a6ba929286/tensorflow/python/estimator/estimator.py#L629), and it seems that there are no ways other than **hacking the internal implementation**.\r\n> > My 2 cents on this issue is that maybe the `Estimator` could be more flexible.\r\n> > For example, one possible solution on top of my head is that the `Estimator.train` function may take another optional argument, i.e., the `mode`, so that in the `Estimator.__init__`, we could create multiple `train_op` based on user-defined modes.\r\n> > ```python\r\n> > def model_fn(features, labels, mode, params):\r\n> >   if mode == 'train.discriminator':\r\n> >     train_op = ...\r\n> >     loss = ...\r\n> >   elif mode == 'train.generator':\r\n> >     train_op = ...\r\n> >     loss = \r\n> >   return tf.estimator.EstimatorSpec(...)\r\n> > \r\n> > gan = tf.estimator.Estimator(model_fn=model_fn, ...)\r\n> > for _ in range(10):\r\n> >   gan.train(input_fn=input_fn, mode='train.discriminator', steps=2)\r\n> >   gan.train(input_fn=input_fn, mode='train.generator', steps=2)\r\n> > ```\r\n> \r\n> Hi, I have the same problem, so I wonder if the method you provide works well. Thank you!\r\n\r\nThe estimator is limited. Maybe you can try keras. There is an implementation on GAN, I hope it will help you. https://www.diqiuzhuanzhuan.com/2019/05/05/tensorflow%E5%AE%9E%E7%8E%B0%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"]}, {"number": 7768, "title": "Update iris_monitors.py", "body": "Interface has changed.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it! : )", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "Looks like this already got fixed in master. Thanks for your change!"]}, {"number": 7767, "title": "ParameterServer restart crashes distributed training process", "body": "I've tried running distributed TensorFlow with Supervisor and new MonitoredTrainingSession and I observe following behavior if Parameter Server is restarted.  Code is available at http://pastebin.com/HBUicRp2.  I'm using TensorFlow freshly built from r1.0.\r\n\r\nAt first, this error happens which is OK:\r\n```\r\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.UnavailableError'>, {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"ex\r\nternal/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":235,\"grpc_status\":14}\r\nW tensorflow/core/framework/op_kernel.cc:993] Unavailable: {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":\r\n235,\"grpc_status\":14}\r\n         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_\r\ndevice_incarnation=8036561443230364107, tensor_name=\"edge_30_Assign_1\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\nW tensorflow/core/framework/op_kernel.cc:993] Unavailable: {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":\r\n235,\"grpc_status\":14}\r\n         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_\r\ndevice_incarnation=8036561443230364107, tensor_name=\"edge_30_Assign_1\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\nW tensorflow/core/framework/op_kernel.cc:993] Unavailable: {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":\r\n235,\"grpc_status\":14}\r\n         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_\r\ndevice_incarnation=8036561443230364107, tensor_name=\"edge_30_Assign_1\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\n```\r\n\r\nHowever, after Parameter Server is restarted, I see this error in logs and training worker crashes:\r\n```\r\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000729. Possibly, this worker just restarted.\r\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000727. Possibly, this worker just restarted.\r\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000731. Possibly, this worker just restarted.\r\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000723. Possibly, this worker just restarted.\r\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 000000000000072b. Possibly, this worker just restarted.\r\nTraceback (most recent call last):\r\n  File \"tf_dist_mnist.py\", line 140, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"tf_dist_mnist.py\", line 107, in main\r\n    mon_sess.run(train_op, feed_dict={image: image_, label: label_})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 478, in __exit__\r\n    self._close_internal(exception_type)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 511, in _close_internal\r\n    self._sess.close()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 739, in close\r\n    self._sess.close()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 827, in close\r\n    self._coord.join()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 386, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\r\n    sess.run(enqueue_op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnavailableError: {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":235,\"grpc_status\":14}\r\n```\r\n\r\nI expect it to recover from latest checkpoint instead.", "comments": ["@jhseu we discussed this on TF Dev Summit and you recommended raising issue.", "Thanks! Adding to my todo list (but may not get to it for a little bit).", "Sent out a fix internally.", "Will you backport to Supervisor's managed_session as well?  It's still listed on TensorFlow documentation website as one of the recommended ways to do simplified session management - https://www.tensorflow.org/programmers_guide/supervisor", "My understanding is that managed_session never did recovery for failed workers. Also, Supervisor is eventually going away. (I never use Supervisor, so I'm probably wrong.)", "AFAIK it is has similar level of recoverability as MonitoredTrainingSession (before your fix).  Here's my observations for Supervisor's managed_session:\r\n\r\n* Parameter Server failure.  Training throws UnavailableError on all workers.  All workers crash.  After restart of failed PS and all workers, training continues from the last checkpoint.\r\n* Chief worker failure.  Training freezes.  After chief worker is restarted, training continues from the last checkpoint.\r\n* Non-Chief worker failure.  Training freezes.  After failed worker is restarted, training continues from the last iteration.\r\n* Backup worker failure.  Training continues, with potential slowdown.  After failed backup worker is restarted, training continues with original pace.\r\n\r\nSince it's still widely advertised a lot of people may jump to use it.", "@alsrgv @jhseu Hi, sorry to comment on this closed issue.\r\n\r\nBut I'm wondering will training freezes if non-chief workers failure in the latest release (v1.5.0)?\r\n"]}, {"number": 7766, "title": "Buzel build error.", "body": " Hello,\r\n  I am using TensorFlow Android Camera Demo. I follow step given in README file.\r\n\r\n  1) Import andorid project\r\n  2) Install bazel using chocolaty\r\n  3) Edit in workspace file.\r\n \r\n While I run bazel build command :\r\n bazel build -c opt //tensorflow/examples/android:tensorflow_demo\r\n\r\n I getting error that **Couldn't find java at '$(ls -d C:/Program\\ Files/Java/jdk* | sort | tail -n 1)/bin/java.exe**'.\r\n\r\nMy JDK is installed at C:\\Program Files (x86)\\Java\\jdk1.8.0_66\\bin . I also set environment variable    JAVA_HOME=\"$(ls -d C:/Program\\ Files/Java/jdk* | sort | tail -n 1)\"\r\nBAZEL_SH=c:/tools/msys64/usr/bin/bash.exe\r\n  ", "comments": ["Unfortunately Bazel doesn't support Android on Windows yet -- see [roadmap](https://bazel.build/roadmap.html).\r\n\r\nWe're working on a full gradle/cmake/makefile solution that will work on Windows, but until then you can use the [prebuilt libs](https://ci.tensorflow.org/view/Nightly/job/nightly-android/) and just drop libtensorflow_demo.so and libtensorflow_inference.so into your app.", "@andrewharp, As per your suggestion I use prebuilt libs (libtensorflow_demo.so and libtensorflow_inference.so) in jniLibs and comment line related to bazel build in build.gradle.while I run project the and get error \r\n\r\n`No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)`\r\n\r\n`dlopen(\"/data/app/org.tensorflow.demo-1/lib/arm/libtensorflow_inference.so\", RTLD_LAZY) failed: dlopen failed: \"/data/app/org.tensorflow.demo-1/lib/arm/libtensorflow_inference.so\" is too small to be an ELF executable: only found 0 bytes`\r\n\r\n`  java.lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present and loaded.\r\n                                                                         at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:57)\r\n                                                                         at org.tensorflow.demo.StylizeActivity.onPreviewSizeChosen(StylizeActivity.java:368)\r\n                                                                         at org.tensorflow.demo.CameraActivity$1.onPreviewSizeChosen(CameraActivity.java:158)\r\n                                                                         at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:394)\r\n                                                                         at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:411)\r\n                                                                         at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:63)\r\n                                                                         at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:94)`\r\n\r\n\r\nHave you any idea that why these error occurs. can you provide exact step for non bazel build in android studio in windows os.\r\n", "@kinDSa If you do `unzip -v bazel-bin/tensorflow/examples/android/tensorflow_demo.apk`, what does it print out?", "@andrewharp \r\n `unzip:  cannot find or open bazel-bin/tensorflow/examples/android/tensorflow_demo.apk, bazel-bin/tensorflow/examples/android/tensorflow_demo.apk.zip or bazel-bin/tensorflow/examples/android/tensorflow_demo.apk.ZIP.\r\n`\r\n", "Sorry, that's the bazel output location. If you unzip the apk that gradle produces instead what does it say?", " while I unzip apk  produce by gradle I get output which attach in these file: [output.txt](https://github.com/tensorflow/tensorflow/files/816410/output.txt)\r\n", "@kinDSa Somehow your armeabi-v7a file is 0 bytes:\r\n`      0  Stored        0   0% 1980-00-00 00:00 00000000  lib/armeabi-v7a/libtensorflow_inference.so`\r\n\r\nIf you look in your libs/ directory, is the file there also 0 bytes? If so, try downloading again and make sure it gets overwritten properly.", "@andrewharp Thank you, Now I successfully run TensorFlow Android Camera Demo after replacing libtensorflow_inference.so ", "Great!"]}, {"number": 7765, "title": "ImportError: No module named '_pywrap_tensorflow'", "body": "Hi, i installed tensorflow in windows 7 SP1. without GPU. (My GPU is AMD so i just tried CPU version.)\r\n\r\nI follow the guidelines,\r\ninstall python 3.5 and set the path automatically.\r\n\r\nthen\r\npip3 install --upgrade tensorflow\r\n\r\nimport tensorflow as tf\r\n\r\nand I obtain the following error:\r\n>>> import tensorflow as tf\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:126] Couldn't open CUDA library cublas64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\cuda\\cuda_blas.cc:2294] Unable to load cuBLAS DSO.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:126] Couldn't open CUDA library cudnn64_5.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\cuda\\cuda_dnn.cc:3517] Unable to load cuDNN DSO\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:126] Couldn't open CUDA library cufft64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\cuda\\cuda_fft.cc:344] Unable to load cuFFT DSO.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:126] Couldn't open CUDA library nvcuda.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\cuda\\cuda_diagnostics.cc:165] hostname: x\u2265\u2593\u2580\u25a0\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\LAB2\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\LAB2\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__ini\r\nt__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: Invalid access to memory location.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\LAB2\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\LAB2\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\LAB2\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Users\\LAB2\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__ini\r\nt__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\LAB2\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\LAB2\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\LAB2\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\LAB2\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__ini\r\nt__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: Invalid access to memory location.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\LAB2\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\LAB2\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\LAB2\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Users\\LAB2\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__ini\r\nt__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_st\r\narted/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n\r\n\r\nNote:\r\nI have seen [https://github.com/tensorflow/tensorflow/issues/7705] and [https://github.com/tensorflow/tensorflow/issues/7529]\r\ni also have installed Visual C++ Redistributable 2015 x64.\r\n\r\n\r\nThank you for any help\r\n\r\n\r\n\r\n\r\n", "comments": ["Apparently it is using GPU, as TensorFlow tried to find CUDA and cuDNN:\r\n```\r\nimport tensorflow as tf\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:126] Couldn't open CUDA library cublas64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\cuda\\cuda_blas.cc:2294] Unable to load cuBLAS DSO.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:126] Couldn't open CUDA library cudnn64_5.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\cuda\\cuda_dnn.cc:3517] Unable to load cuDNN DSO\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:126] Couldn't open CUDA library cufft64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\cuda\\cuda_fft.cc:344] Unable to load cuFFT DSO.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:126] Couldn't open CUDA library nvcuda.dll\r\n```\r\nSo at first to get this out of the way I'd suggest uninstalling TensorFlow then installing it again. In case you still face issues please follow up.\r\n", "It's working now. Thank you."]}, {"number": 7764, "title": "native Cuda kernel for tf.dynamic_stitch op", "body": "Correspond to issue #7251 \r\nHere is the comparison between \r\nmy implementation:\r\n![dynamic_stitch_gpu](https://cloud.githubusercontent.com/assets/6672514/23201105/ed219d82-f912-11e6-9ed1-c86c80d63921.png)\r\nand current implementation:\r\n![dynamic_stitch_cpu](https://cloud.githubusercontent.com/assets/6672514/23201120/faf28278-f912-11e6-9b4f-f8793bd9faa6.png)\r\ngenerated by Nvidia visual profiler with [this code](https://gist.github.com/MycChiu/c353fd3c98c47dceae866ed30f40a5a8). *Since the kernel uses `CudaDeviceArrayStruct`, which depends on code from `//tensorflow/core:lib`, I can't generate custom_op library and use the custom_op for performance comparison. I ended up manually switch the tensorflow directory python uses between runs*\r\nCurrently, I still have two problems:\r\n1. For some reason, tensorflow won't use the GPU kernel automatically, I had to force it with `tf.device`. What's even worse, tensorflow seems to refuse loading the GPU kernel in `test_session()` even if I tried to force it with `tf.device`, so this code is currently not tested with `dynamic_stitch_test.py`.\r\n~~2. As @yaroslavvb suggested, Cuda kernels need to be real-valued, so I ended up calling `TF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_DYNAMIC_STITCH_GPU);` to register the kernel. However, I feel like this will break some people's current code if their input data is from the unsupported data type.~~ *fixed in the latest commit.*\r\n\r\nFinally, this implementation is really under-optimized for cases where the size of each input data slice is small, because the indices are still processed by the CPU (plus CUDA reads data in warps of 32 threads). However, I still don't think it will be worse than processing everything on CPU then copy them back to GPU though.", "comments": ["Can one of the admins verify this patch?", "Not sure who the best reviewers are - added @girving and @yuanbyu, feel free to hot potato away if you're not the right reviewer. But we've let this sit for 14 days without any attention so we really should take a look promptly.", "I can review this.  Can you squash into a single CL before I start, since it looks like you undid some mistakes?", "@girving Yes, I redirected inputs with datatypes not supported by current CUDA kernel implementation to CPU in the latest commit, but I think the latest commit reflects all the changes I made though? Or perhaps you and I are talking about different things?", "Squashing won't change the final diff, but it makes it easier to read all the changes together as I review them.  So please squash as I requested.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@girving Welp... Sorry, I think I F'ed up the rebasing part...I think it will be easier if I close this pull request and start a new one. I will tag you when I am done.", "@MycChiu I believe you did `git pull` into your PR branch. Never do a `git pull`, only `git rebase` :)", "@yaroslavvb Haha, yes, that's indeed what I did. I will definitely keep this in mind next time, thank you!"]}, {"number": 7763, "title": "TensorFlow for Poets: Section 5: label_image.py on site is different than curl version", "body": "Link: https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/index.html?index=..%2F..%2Findex#4\r\n\r\nOn the site, the `label_image.py` script uses relative paths for files while the version the tutorial asks users to curl uses absolute paths for files.\r\n\r\nIf the user is knowledgeable enough to prefer copying the script on the website vs. curl'ing it from https://goo.gl/tx3dqg, then the user might run into an issue where they first need to run `cd /` before being able to run the script as-is.\r\n\r\n![screen shot 2017-02-21 at 10 47 37 pm](https://cloud.githubusercontent.com/assets/1815591/23200175/c6512762-f887-11e6-9d7f-f031bd839ecd.png)\r\n![screen shot 2017-02-21 at 10 47 27 pm](https://cloud.githubusercontent.com/assets/1815591/23200176/c6682af2-f887-11e6-82a2-75a0641a127d.png)\r\n", "comments": ["@wolffg Is this unfixed?", "Fixed."]}, {"number": 7762, "title": "TensorFlow for Poets: Section 5: Missing a verb", "body": "In https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/index.html?index=..%2F..%2Findex#4, \"You might warnings\" is missing a verb. Maybe \"You might get warnings\" or \"You might see warnings\"?", "comments": ["![screen shot 2017-02-21 at 10 40 41 pm](https://cloud.githubusercontent.com/assets/1815591/23200020/cdf8761a-f886-11e6-8cd4-96ef83bbe81a.png)\r\n", "Also \"they not harmful\" might need a verb too...", "@wolffg If this is poetry, does it really need to be grammatically correct?", "No, the user is the poet; the codelab is not, so we should probably fix the grammar.", "This is fixed.\r\n\r\nNext codelab could be: TensorFlow poetry -  RNNs"]}, {"number": 7761, "title": "Use Ubuntu 14.04 for the 1.0 branch GPU build.", "body": "nvidia/cuda:8.0-cudnn5-devel currently points to Ubuntu 16.04 which causes it to fail on Jenkins. This change pins to the same Ubuntu version we used for the 1.0 release.\r\n\r\nTested with docker build.", "comments": ["Looks like that image installs CUDA in a non-standard location. Abandoning for now.", "Jenkins, test this please", "@caisq ok, looks like this fix worked. PTAL."]}, {"number": 7760, "title": "The GPU docker image doesn't work in CentOS 7", "body": "### Environment info\r\nOperating System: CentOS 7.0\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n```\r\n# ls /usr/local/cuda/lib64/libcuda*\r\n/usr/local/cuda/lib64/libcudadevrt.a    /usr/local/cuda/lib64/libcudart.so.8.0\r\n/usr/local/cuda/lib64/libcudart.so      /usr/local/cuda/lib64/libcudart.so.8.0.27\r\n/usr/local/cuda/lib64/libcudart.so.7.5  /usr/local/cuda/lib64/libcudart_static.a\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nRefer to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/README.md , we change the path of cuda files for CentOS and run with the following commands.\r\n\r\n```\r\nexport CUDA_SO=$(\\ls /usr/local/cuda/lib64/libcuda* | xargs -I{} echo '-v {}:{}')\r\nexport DEVICES=$(\\ls /dev/nvidia* | xargs -I{} echo '--device {}:{}')\r\ndocker run -it -p 8888:8888 $CUDA_SO $DEVICES gcr.io/tensorflow/tensorflow:latest-gpu bash\r\n```\r\n\r\nNow it can find the GPU devices but fail to init cuda library.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n```\r\nroot@f8e47e56d59d:/notebooks# python test_tensorflow.py\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:119] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: f8e47e56d59d\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  361.77  Sun Jul 17 21:18:18 PDT 2016\r\nGCC version:  gcc version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC)\r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 361.77.0\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1092] LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1093] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: f8e47e56d59d\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: f8e47e56d59d\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  361.77  Sun Jul 17 21:18:18 PDT 2016\r\nGCC version:  gcc version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC)\r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 361.77.0\r\nWARNING:tensorflow:From a.py:20 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\r\nInstructions for updating:\r\nUse `tf.global_variables_initializer` instead.\r\nEpoch: 1, w: -0.844658732414, b: 9.68447113037\r\nEpoch: 2, w: 0.321342378855, b: 10.4512624741\r\nEpoch: 3, w: 1.12122178078, b: 10.3018264771\r\nEpoch: 4, w: 1.55480694771, b: 10.155872345\r\nEpoch: 5, w: 1.77876627445, b: 10.0725793839\r\nEpoch: 6, w: 1.89310014248, b: 10.0290489197\r\nEpoch: 7, w: 1.95129656792, b: 10.0067615509\r\nEpoch: 8, w: 1.98089802265, b: 9.99540710449\r\nEpoch: 9, w: 1.99595046043, b: 9.98963069916\r\nEpoch: 10, w: 2.00360536575, b: 9.98669433594\r\n```", "comments": ["@tobegit3hub we recently run a custom built tensorflow docker images successfully on Amazon Linux, which I believe is Redhat-based. Not sure if those are applicable to your case but we did:\r\n\r\n- Based on `tensorflow/tensorflow:1.0.0-devel-gpu`\r\n- Nvidia driver installed both on the host and container (same version). CuDNN is only inside the container though\r\n- Use privileged mode (not sure if you need this when using `nvidia-docker`)\r\n- Mount `/lib/modules` on the host to `/lib/modules` (this step is not needed on ubuntu, not sure why)", "Thanks @chenliu0831 . \r\n\r\nCan you tell more details about the mount path for your container? Like `/usr/local/cuda/lib64/libcuda*` or `/usr/lib/x86_64-linux-gnu/libcuda.*`? We're using `docker` instead of `nvidia-docker` and need to mount all the dependent `.so` files.", "I think mounting the `/usr/lib64/nvidia/` in CentOS into `/usr/local/nvidia/lib64/` in Ubuntu container will work. And we should use `--device` for `/dev/nvidia*` instead of `-v`.\r\n\r\nNow this command works in `CentOS 7.0`.\r\n\r\n```\r\ndocker run -it -v /usr/lib64/nvidia/:/usr/local/nvidia/lib64/ --device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidia1:/dev/nvidia1 --device /dev/nvidia2:/dev/nvidia2 --device /dev/nvidia3:/dev/nvidia3 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm --device /dev/nvidia-uvm-tools:/dev/nvidia-uvm-tools tensorflow/tensorflow:latest-gpu bash\r\n```"]}, {"number": 7759, "title": "Add MAVX2_FMA_DBG build option and rename MAVXDBG to MAVX_DBG.", "body": "Cherry-picking into the r1.0 branch for Jenkins.", "comments": ["(Abandoning for now)"]}, {"number": 7758, "title": "Make the comment explaining arg_scope clearer", "body": "I found the use of \"...\" in a comment providing an example of the functionality of arg_scope to be very confusing, so I expanded it out to be less ambiguous.", "comments": ["Sorry that I didn't reply earlier. I think this change will lead to more problems, you will need to keep the inlined default values synced with the place where they are defined - and this is a thing people for sure will forget.", "Hi @maciekcc, I'm not sure I understand what you mean. You're saying that with future changes to this explanatory comment, people will modify one part and forget to modify the other affected parts? That problem should be handled by future code reviews.\r\n\r\nOn top of that, the current explanation had little value to me, as the \"...\" omitted the very part I was trying to understand. If the explanation had included these changes when I first looked at it, it would have been much easier to understand, and I expect that to be true for others as well.", "The change doesn't make it much worse: the `padding=` argument in the docstring already depends on the value in the code, and would have to be changed along with any changes in the function.\r\n\r\nYou could also omit the ... entirely, since it is irrelevant -- default args are used anyway, so not additional args need to be set.", "@martinwicke Right, I'm assuming the docstring is meant to explain how the functionality works, in which case expanding out the full functionality makes it clearer. As someone who didn't fully understand what `arg_scope` does (and possibly still doesn't), I found the existing docstring to be very unhelpful for figuring it out.", "@martinwicke @maciekcc Any thoughts on this?", "(On sync rotation, clearing out old pull requests). Seems strictly better than current docs, so merging."]}, {"number": 7757, "title": "Broken link to doc in extras.", "body": "In the [doc on `nec_loss`](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss), the target doc of the \"Candidate Sampling Algorithms Reference \", of which the url is https://www.tensorflow.org/api_docs/python/extras/candidate_sampling.pdf, is missing and gives just a 404 error.\r\n\r\n", "comments": ["I find the doc in https://www.tensorflow.org/extras/candidate_sampling.pdf . So I believe it's due to an improperly specified URL path. ", "I put in a redirect for this and corrected the API docstrings internally, but it has to propagate out to the site.  Still, you should get redirected now (you may have to reload your browser if the 404 is cached)"]}, {"number": 7756, "title": "Add MAVX2_FMA_DBG to the CI build and rename MAVXDBG to MAVX_DBG.", "body": "I'm the only one using MAVXDBG, so the rename is safe.", "comments": ["Jenkins, test this please"]}, {"number": 7755, "title": "sess.run(tf.global_variables_initializer()) is slow after installing 1.0.0.", "body": "### The issue\r\n\r\n`sess.run(tf.global_variables_initializer())` intermittently takes a long time since installing 1.0.0. I believe this also happened on my Python 2 0.12 install, but only after installing 1.0.0 for Python 3. This particular run was in Python 2, and the initializer took 32 seconds - see bottom of this post (and the attached cprofile output) for more detail.\r\n\r\nStrangely, it seems like the more I run the same file, the faster it runs.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\nls -l /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root   560184 Oct 27 16:26 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Oct 27 16:26 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Oct 27 16:26 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\r\n-rwxr-xr-x 1 root root   394472 Oct 27 16:26 /usr/local/cuda/lib64/libcudart.so.8.0.27\r\n-rw-r--r-- 1 root root   737516 Oct 27 16:26 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 60696704 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 60696704 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 60696704 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn.so.5.1.3\r\n-rw-r--r-- 1 root root 59715990 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn_static.a\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: `sudo pip install tensorflow-gpu` on 2/21/17\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 1.0.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nimport time\r\nfrom collections import deque\r\n\r\nimport numpy as np\r\nprint('Importing tensorflow')\r\nimport tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nxavier = tf.contrib.layers.xavier_initializer\r\nbatch_norm = tf.contrib.layers.batch_norm\r\n\r\ntf.set_random_seed(1)\r\n\r\nflags = tf.app.flags\r\nFLAGS = flags.FLAGS\r\nflags.DEFINE_string('data_dir', 'data/', 'Directory for storing data')\r\nprint('Reading data.')\r\nmnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\r\ninitialization_variance = 0.1\r\n\r\nprint('Creating a session.')\r\nsess = tf.Session()\r\n\r\ndef fully_connected_layer(inputs, output_size, scope, nonlinearity=True):\r\n  with tf.variable_scope(scope):\r\n    input_size = inputs.get_shape()[1].value\r\n\r\n    weights = tf.get_variable('w', \r\n                              [input_size, output_size],\r\n                              initializer=tf.constant_initializer(0.0))\r\n\r\n    bias = tf.get_variable('b', \r\n                           [output_size],\r\n                           initializer=tf.constant_initializer(0.0))\r\n\r\n    multiplied = tf.matmul(inputs, weights)\r\n\r\n    return tf.nn.elu(multiplied + bias) if nonlinearity else multiplied + bias\r\n\r\nprint('Setting up the model.')\r\ninput_images = tf.placeholder(tf.float32, [None, 784])\r\nlabels = tf.placeholder(tf.float32, [None, 10]) # labels\r\nlearning_rate_placeholder = tf.placeholder(tf.float32, name='learning-rate')\r\n\r\nlayer_a = fully_connected_layer((input_images), 50, 'a')\r\n\r\nfor i in range(5):\r\n  layer_a = batch_norm(fully_connected_layer(layer_a, 50, str(i)))\r\n\r\nlayer_b = fully_connected_layer(layer_a, 10, 'b', nonlinearity=False)\r\n\r\npredictions = tf.nn.softmax(layer_b) \r\n \r\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(labels * tf.log(predictions), reduction_indices=[1]))\r\ntrain_step = tf.train.AdamOptimizer(learning_rate_placeholder).minimize(cross_entropy)\r\n\r\nprint('Initializing variables.')\r\nsess.run(tf.global_variables_initializer())\r\n```\r\n### What other attempted solutions have you tried?\r\n\r\nNone.\r\n\r\n### Logs or other output that would be helpful\r\n\r\nSome output from `python2 -m cProfile -o out.profile 1-mnist-simple.py`. This is a call to `sess.run(tf.global_variables_initializer())`.\r\n![image](https://cloud.githubusercontent.com/assets/5461398/23191984/132ca252-f85e-11e6-8c55-9b48adc3d40b.png)\r\n\r\n\r\n[cprofile-output.profile.zip](https://github.com/tensorflow/tensorflow/files/791923/cprofile-output.profile.zip)\r\n\r\n\r\n", "comments": ["Is it the first call, or can it be the second call? If it's the first call, it could have something to do with CUDA generating PTX, which happens the first time op is executed and is slow (and mega-slow if you are on nfs, set CUDA_CACHE_PATH=/non_nfs_dir). Whether or not PTX has to be generated depends on whether the binary was built with compute capability of your card\r\n\r\nDoes the problem disappear if you revert to older version? Does it happen with CPU-only TensorFlow?", "> Is it the first call, or can it be the second call? \r\n\r\nThere's only one call to sess.run - for initializing the variables. If you're talking about running the script as a whole, so far it has been kinda unpredictable.\r\n\r\n> If it's the first call, it could have something to do with CUDA generating PTX, which happens the first time op is executed and is slow (and mega-slow if you are on nfs, \r\n\r\nI'm running this all on my own machine.\r\n\r\n> set CUDA_CACHE_PATH=/non_nfs_dir). Whether or not PTX has to be generated depends on whether the binary was built with compute capability of your card\r\n\r\nIt's a GTX 1060, so that's compute capability 6.1, I guess.\r\n\r\n> Does the problem disappear if you revert to older version? Does it happen with CPU-only TensorFlow?\r\n\r\nI switched to 0.12, CPU version: `tensorflow-0.12.0-cp27-cp27mu-manylinux1_x86_64.whl`. Waaaaay better! After that, I tried 0.12, GPU version: `tensorflow_gpu-0.12.0-cp27-cp27mu-manylinux1_x86_64.whl`. This returned to the same problem.\r\n\r\n", "Also around the same time I started noticing the problem, I installed opencv for Python 3. I guess that uses CUDA, so maybe that is related?", "Running opencv with gpu tensorflow causes crashes on my machine, there's some kind of interference, I have to disable opencv GPU using `export OPENCV_OPENCL_RUNTIME=`", "@chrisranderson what I meant was trying to compare against a different GPU TF version. GPU TF is known to be slow on first kernel launch (up to a minute in some bad cases) when your architecture compute capability is not included which is the case for 6.1 (see discussion here https://github.com/tensorflow/tensorflow/issues/6914 )", "I tried `export OPENCV_OPENCL_RUNTIME=` before running tensorflow-gpu again, but it didn't seem to have an effect. I don't know what you mean by your second comment. \r\n\r\nSlowness is expected, but is 30 second for a network like the one I posted normal? It's pretty small. It seemed like initialization used to be quite fast. Is there something else I need to try with opencv?\r\n\r\nThanks for your help.", "The \"up to a minute\" slowness I refer to is what happens when CUDA starts PTX compilation and is independent of your computation size. If that's your bottleneck, then the solution is compile with your architecture since the official binary only comes with support for ancient cards (I usually compile with 6.1 support, some wheels are here -- https://github.com/yaroslavvb/tensorflow-community-wheels). I'm not sure how to tell if it's indeed the PTX compilation that's causing the slowness, maybe @zheng-xq knows  some tricks.", "If it is indeed PTX compilation, you can look at the timeline, like this:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/timeline_test.py#L44\r\n\r\nThe first GPU kernel takes a really long time, without a good reason. ", "Okay, here are the results of the profile. I looked at it in chrome://tracing/, but wasn't able to make sense of it.\r\n[out.profile.zip](https://github.com/tensorflow/tensorflow/files/798039/out.profile.zip)\r\n", "Also you could try with a TF version compiled with support for 6.1 architecture (just enter 6.1 during configure step)", "Anything else I should be aware of besides the stuff in here?\nhttps://www.tensorflow.org/install/install_sources\n\nOn Thu, Feb 23, 2017, 7:07 PM Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> also you could try recompiling with support 6.1 architecture (just enter\n> 6.1 during configure step)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7755#issuecomment-282182584>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFNVls58QDYGuQPVwGT4R1Q0wRBk14_8ks5rfjtkgaJpZM4MIEM0>\n> .\n>\n", "Starting the process of building from source right now.", "FWIW, I had the same slow startup problem with the 'latest-gpu' docker image and a GFX1080. \r\nMoving over to 'latest-devel-gpu', which is compiled with 6.1 architecture support, fixed it.", "I had this same issue with my own docker container, even while using the 'latest-devel-gpu' container as @afri mentioned.\r\n\r\nHowever, @chrisranderson, your comment about opencv made me realize what the problem was. You're right, compiling opencv with cuda support caused the slow startup for me.\r\n\r\nBy compiling opencv with `-D WITH_CUDA=OFF`, the problem is fixed, and my models run almost instantly.\r\n\r\nEDIT: Also, if you want a quick fix, you can also just try importing cv2 before importing tensorflow. That worked for me too.", "I built from source, and I've also tried 0.12 again. Still very slow. @neil454 thanks for the reply, I'd like to give that a shot.", "@neil454 I think that compiling without CUDA did the trick, thank you!", "@chrisranderson Is this fixed, or is there a code fix warranted?", "@girving I'd call it \"band-aid fixed\". Fully fixed would mean I could do whatever I wanted with opencv and not have it affect TensorFlow, I think. I have no idea why this happens or what the situation is, so maybe it's actually opencv's fault, or nvidia's fault or something.", "Alas!  I'm going to close, but with sadness.  Let us know if a clearer picture ever emerges."]}, {"number": 7754, "title": "tf.split(num_or_size_splits=x,...) fails for x=Dimension(128)", "body": "Seen while upgrading our code to TF 1.0, somehow this used to work in 12.1, but in TF 1.0 the following fails with `IndexError: list index out of range` inside of `array_ops.py`\r\n\r\n`tf.split(num_or_size_splits=X.get_shape()[1], ...)\r\n`\r\n\r\nLooking at array_ops.py, the relevant logic\r\n\r\n```\r\n  if isinstance(num_or_size_splits, six.integer_types):\r\n    return gen_array_ops._split(\r\n        split_dim=axis, num_split=num_or_size_splits, value=value, name=name)\r\n  else:\r\n    size_splits = ops.convert_to_tensor(num_or_size_splits)\r\n\r\n```\r\n\r\nSo `Dimension(128)` is treated as \"Tensor\" and code fails with `IndexError: list index out of range` inside `array_ops.py`. I think it would make more sense if the Tensor path checked if `num_or_size_splits` was `Tensor` or convertible to non-scalar Tensor, and then have a catch-all `else` for all other cases. Or perhaps documentation could be updated to say that this path is taken if `num_split` is not a scalar, which is what's happening now, despite the documentation implying that this is only for `Tensor` arguments.\r\n\r\nI would fix this myself, but I already have an outstanding PR, and the current system makes it too painful to switch branches -- https://github.com/tensorflow/tensorflow/issues/6911\r\n", "comments": ["Thanks for the detailed bug report.  Internal bug opened.\r\n", "Use python type conversion to solve this issue\r\n\r\ntf.split(num_or_size_splits=int(X.get_shape()[1]), ...)\r\n", "Close for now since I assume this has already been fixed. If not, please reopen."]}]