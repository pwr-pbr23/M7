[{"number": 40909, "title": "Cannot save Model with Custom Layer : tensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: \r\nGit version : v2.2.0-rc4-8-g2b96f3662b \r\nTensorflow version: 2.2.0\r\n- Python version: 3.7\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nFor an NLP project, in order to directly preprocess data in a model , I have implemented a custom tokenization layer with a StaticHashTable (see below). Afterwards, I prepend this layer to a base model. Everything seems working fine, I even can make computations with the model.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n\r\n```python\r\nimport pickle\r\nimport tensorflow as tf\r\n\r\ntokenizer = pickle.load(tokenizer_path) ##pickled tf.keras tokenizer\r\n\r\nclass OwnPadder(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n        super(OwnPadder, self).__init__(name = \"padder\", trainable = False, **kwargs)\r\n        self.tokenizer_keys = tf.constant(list(tokenizer.word_index.keys()), dtype = tf.string)\r\n        self.tokenizer_values = tf.constant(list(tokenizer.word_index.values()), dtype = tf.int32)\r\n        self.initializer = tf.lookup.KeyValueTensorInitializer(keys = self.tokenizer_keys,\r\n                 values= self.tokenizer_values, key_dtype = tf.string, value_dtype = tf.int32)\r\n        self.table = tf.lookup.StaticHashTable(self.initializer, default_value=tf.constant(-1), name = \"static_hash\")\r\n\r\n    def call(self, inputs, **kwargs):\r\n        pipe = tf.strings.split(inputs)\r\n        pipe = pipe.to_tensor(shape = pipe.bounding_shape(), name = \"uniform_tensor\")[:,0,:]\r\n        tokenized = self.table.lookup(pipe, name = \"tokenize\")\r\n        masked = tf.ragged.boolean_mask(tokenized, tokenized>0)\r\n        res = masked.to_tensor(shape = [None, 100], name =\"oad\")\r\n        return res\r\n\r\n    def get_config(self):\r\n        config = super(OwnPadder, self).get_config()\r\n        return config\r\n\r\n[...]\r\n\r\nbase_model = tf.keras.models.load_model(base_model_path)\r\nbase_model.trainable = False\r\ninput_text = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name = \"input\")\r\n x = OwnPadder()(input_text)\r\n for layers in base_model.layers:\r\n     x = layers(x)\r\n\r\nholistic_model = tf.keras.models.Model(inputs = input_text, outputs = x)\r\nholistic_model.compile()\r\nfinal_model_path\r\nholistic_model.save(final_model_path, save_format = \"tf\")\r\n\r\n```\r\nSo, no bug here, everything seems fine until I try to serve with TensorflowServing, where the serving made a segmentation fault...\r\nI tried to make a diagnostic with `saved_model_cli` shell command with the newly saved model, which gave me expected inputs/outputs signatures, but ended with a weird TF Error.\r\n\r\n```python\r\nWARNING:tensorflow:From $conda_envlib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n2020-06-29 06:24:26.528835: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-06-29 06:24:26.528907: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-06-29 06:24:26.528947: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (nlp-compute-2.sesamm.com): /proc/driver/nvidia/version does not exist\r\n2020-06-29 06:24:26.529160: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-29 06:24:26.566924: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2397160000 Hz\r\n2020-06-29 06:24:26.574709: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f85f0000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-29 06:24:26.574758: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nException ignored in: <function CapturableResourceDeleter.__del__ at 0x7f86ba4de8c8>\r\nTraceback (most recent call last):\r\n  File \"$conda_env/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 191, in __del__\r\n    self._destroy_resource()\r\n  File \"$conda_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"$conda_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 627, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"$conda_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 506, in _initialize\r\n    *args, **kwds))\r\n  File \"$conda_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"$conda_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"$conda_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2667, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"$conda_env/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"$conda_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 441, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"$conda_env/lib/python3.7/site-packages/tensorflow/python/saved_model/function_deserialization.py\", line 241, in restored_function_body\r\n    return _call_concrete_function(function, inputs)\r\n  File \"$conda_envlib/python3.7/site-packages/tensorflow/python/saved_model/function_deserialization.py\", line 72, in _call_concrete_function\r\n    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access\r\n  File \"$conda_env/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 101, in _call_flat\r\n    cancellation_manager)\r\n  File \"$conda_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1760, in _call_flat\r\n    flat_outputs = forward_function.call(ctx, args_with_tangents)\r\n  File \"$conda_envlib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 627, in call\r\n    executor_type=executor_type)\r\n  File \"$conda_envlib/python3.7/site-packages/tensorflow/python/ops/functional_ops.py\", line 1165, in partitioned_call\r\n    f.add_to_graph(graph)\r\n  File \"$conda_envlib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 543, in add_to_graph\r\n    g._add_function(self)\r\n  File \"$conda_env/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3187, in _add_function\r\n    gradient)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\r\n```\r\nPlease notice that the base model (without preprocessing layer) doesn't give this issue, and can be perfectly served via TensorflowServing. So I assume this is a matter of serialization of the new layer. Moreover, I tried to fit the new model from scratch (by fitting with raw data, compiling and saving preprocessed), and it also gave me an error. \r\n\r\nDo you have any hint how to fix this ? What am I doing wrong here ? Many thanks in advance ! ", "comments": ["@mNemlaghi,\r\nIssues related to TensorFlow Serving are handled in the Serving repo. \r\n\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/serving/issues/new) and fill in the template so that we can track the issue there. Thanks!"]}, {"number": 40908, "title": "Using zeros_like in tensorflow with keras add_loss leads to error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 4.4.0-109-generic\r\n- TensorFlow installed from (source or binary): binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\ngit_version == v2.0.0-rc2-26-g64c3d38\r\nversion == 2.0.0\r\n\r\n\r\n### ISSUE\r\n\r\nIf I run the following code, I get an the error \"InternalError: Invalid tape state.\". However, if I switch tf.keras.backend.zeros_like(x) to ones_like(x) the issue disappears. It appears that the issue is arising from the zeros_like()\r\n`imgin = tf.keras.Input((112,112,3))\r\nx = tf.keras.layers.GlobalAvgPool2D()(imgin)\r\nx = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\r\nx_ = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\r\nmodel = tf.keras.Model(imgin, x_)\r\nmodel.add_loss(tf.keras.losses.binary_crossentropy(tf.keras.backend.zeros_like(x), x))\r\nmodel.compile(\"Adam\", \"binary_crossentropy\")\r\nmodel.train_on_batch(tf.ones((1,112,112,3)), [1])`", "comments": ["Switching to tf 2.2.0 fixes the issue.", "Moving this to closed status as this is resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40908\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40908\">No</a>\n"]}, {"number": 40907, "title": "KeyError: 'Failed to format this callback filepath: \"checkpoint_5000/checkpoint_{epoch:02d}_{batch:04d}\". Reason: \\'batch\\''", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Installed using Pip\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n```\r\n nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Fri_Feb__8_19:08:26_Pacific_Standard_Time_2019\r\nCuda compilation tools, release 10.1, V10.1.105\r\n```\r\n- GPU model and memory: NVIDIA MX110 2GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"` v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n\r\n\r\n**Describe the current behavior**\r\nI am following a tutorial where we can save model weights in Tensorflow. We are saving weights every 5000 training points. Code of instructor and my code is same. But his version is 2.0, and my version is 2.2.0. There is error so I guess it is a bug in version.\r\n**Describe the expected behavior**\r\nIt should save the model weights every 5k training points.\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\r\n\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint\r\n\r\n\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\r\nx_train = x_train / 255.0\r\nx_test = x_test / 255.0\r\n\r\ndef get_new_model():\r\n    model = Sequential([\r\n        Conv2D(filters=16, input_shape=(32, 32, 3), kernel_size=(3, 3), \r\n               activation='relu', name='conv_1'),\r\n        tf.keras.layers.BatchNormalization(),\r\n        Conv2D(filters=8, kernel_size=(3, 3), activation='relu', name='conv_2'),\r\n        MaxPooling2D(pool_size=(4, 4), name='pool_1'),\r\n        tf.keras.layers.BatchNormalization(),\r\n        Conv2D(filters=8, kernel_size=(3, 3), activation='relu', name='conv_3'),\r\n        MaxPooling2D(pool_size=(4, 4), name='pool_2'),\r\n        Flatten(name='flatten'),\r\n        Dense(units=32, activation='relu', name='dense_1'),\r\n        tf.keras.layers.Dropout(0.5),\r\n        Dense(units=10, activation='softmax', name='dense_2')\r\n    ])\r\n    model.compile(optimizer='adam',\r\n                  loss='sparse_categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\n    return model\r\n\r\n\r\ncheckpoint_5000_path = 'checkpoint_5000/checkpoint_{epoch:02d}_{batch:04d}'\r\n\r\nmodel = get_new_model()\r\ncheckpoint_5000 = ModelCheckpoint(filepath=checkpoint_5000_path, verbose=True, save_weights_only=True,\r\n                                  save_freq=5000)\r\nmodel.fit(x_train, y_train, batch_size=10, validation_data=(x_test,y_test), epochs=3, verbose= True, callbacks=[checkpoint_5000])\r\n\r\n```\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nFull traceback is \r\n\r\n```\r\n\r\n---------------------------------------------------------------------------\r\n\r\nKeyError                                  Traceback (most recent call last)\r\n\r\nC:\\Anaconda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in _get_file_path(self, epoch, logs)\r\n   1243         # placeholders can cause formatting to fail.\r\n-> 1244         return self.filepath.format(epoch=epoch + 1, **logs)\r\n   1245       except KeyError as e:\r\n\r\nKeyError: 'batch'\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nKeyError                                  Traceback (most recent call last)\r\n\r\n<ipython-input-11-cc68dad1ac2c> in <module>\r\n      7 checkpoint_5000 = ModelCheckpoint(filepath=checkpoint_5000_path, verbose=True, save_weights_only=True,\r\n      8                                   save_freq=5000)\r\n----> 9 model.fit(x_train, y_train, batch_size=10, validation_data=(x_test,y_test), epochs=3, verbose= True, callbacks=[checkpoint_5000])\r\n     10 \r\n     11 \r\n\r\nC:\\Anaconda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\nC:\\Anaconda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    853                 context.async_wait()\r\n    854               logs = tmp_logs  # No error, now safe to assign to logs.\r\n--> 855               callbacks.on_train_batch_end(step, logs)\r\n    856         epoch_logs = copy.copy(logs)\r\n    857 \r\n\r\nC:\\Anaconda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in on_train_batch_end(self, batch, logs)\r\n    388     if self._should_call_train_batch_hooks:\r\n    389       logs = self._process_logs(logs)\r\n--> 390       self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)\r\n    391 \r\n    392   def on_test_batch_begin(self, batch, logs=None):\r\n\r\nC:\\Anaconda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in _call_batch_hook(self, mode, hook, batch, logs)\r\n    296     for callback in self.callbacks:\r\n    297       batch_hook = getattr(callback, hook_name)\r\n--> 298       batch_hook(batch, logs)\r\n    299     self._delta_ts[hook_name].append(time.time() - t_before_callbacks)\r\n    300 \r\n\r\nC:\\Anaconda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in on_train_batch_end(self, batch, logs)\r\n    613     \"\"\"\r\n    614     # For backwards compatibility.\r\n--> 615     self.on_batch_end(batch, logs=logs)\r\n    616 \r\n    617   @doc_controls.for_subclass_implementers\r\n\r\nC:\\Anaconda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in on_batch_end(self, batch, logs)\r\n   1160       self._batches_seen_since_last_saving += 1\r\n   1161       if self._batches_seen_since_last_saving >= self.save_freq:\r\n-> 1162         self._save_model(epoch=self._current_epoch, logs=logs)\r\n   1163         self._batches_seen_since_last_saving = 0\r\n   1164 \r\n\r\nC:\\Anaconda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in _save_model(self, epoch, logs)\r\n   1194                   int) or self.epochs_since_last_save >= self.period:\r\n   1195       self.epochs_since_last_save = 0\r\n-> 1196       filepath = self._get_file_path(epoch, logs)\r\n   1197 \r\n   1198       try:\r\n\r\nC:\\Anaconda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in _get_file_path(self, epoch, logs)\r\n   1245       except KeyError as e:\r\n   1246         raise KeyError('Failed to format this callback filepath: \"{}\". '\r\n-> 1247                        'Reason: {}'.format(self.filepath, e))\r\n   1248     else:\r\n   1249       # If this is multi-worker training, and this worker should not\r\n\r\nKeyError: 'Failed to format this callback filepath: \"checkpoint_5000/checkpoint_{epoch:02d}_{batch:04d}\". Reason: \\'batch\\''\r\n```", "comments": ["@ahmadmustafaanis \r\n\r\nI have tried in colab with TF 2.2 ,nightly versions and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/434752b511fefd711e13c9a24eb795cf/untitled60.ipynb).Thanks!", "facing the same issue\r\n", "> facing the same issue\r\n\r\nUse Tensorflow 2.0.0 to overcome this problem.", "I could replicate the issue in colab with TF nightly`(2.4.0-dev20200916`) version.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/76d9c5431651c45aa8bc4333e788786b/untitled371.ipynb).Thanks!", "I checked the source code and found that you can try this:\r\n\r\n\r\nclass MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\r\n\r\n    def __init__(self, **kwargs):\r\n        super(MyModelCheckpoint, self).__init__(**kwargs)\r\n\r\n    def on_train_batch_end(self, batch, logs=None):\r\n        if (batch + 1) % self.save_freq == 0:\r\n            logs['batch'] = batch + 1\r\n            self._save_model(epoch=self._current_epoch, logs=logs)\r\n\r\n\r\n\r\nThen replace your checkpoint with MyModelCheckpoint.", "> I checked the source code and found that you can try this:\r\n> \r\n> class MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\r\n> \r\n> ```\r\n> def __init__(self, **kwargs):\r\n>     super(MyModelCheckpoint, self).__init__(**kwargs)\r\n> \r\n> def on_train_batch_end(self, batch, logs=None):\r\n>     if (batch + 1) % self.save_freq == 0:\r\n>         logs['batch'] = batch + 1\r\n>         self._save_model(epoch=self._current_epoch, logs=logs)\r\n> ```\r\n> \r\n> Then replace your checkpoint with MyModelCheckpoint.\r\n\r\nThis class should be written in the source code callbacks.py of the Tensorflow library or in the train script?", "Was able to reproduce the issue using TF 2.5 version. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/95899b0e88e809d45898352eb6210df3/untitled85.ipynb).Thanks!", "> class MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\r\n> \r\n> ```\r\n> def __init__(self, **kwargs):\r\n>     super(MyModelCheckpoint, self).__init__(**kwargs)\r\n> \r\n> def on_train_batch_end(self, batch, logs=None):\r\n>     if (batch + 1) % self.save_freq == 0:\r\n>         logs['batch'] = batch + 1\r\n>         self._save_model(epoch=self._current_epoch, logs=logs)\r\n> ```\r\n\r\n@ahmadmustafaanis Custom callback as mentioned by @Brucewuzhang is the best possible solution. [Here](https://colab.research.google.com/gist/jvishnuvardhan/d0149355c723d885b161a1090680bee7/untitled371.ipynb) is a gist with @Brucewuzhang solution. \r\n\r\nOn the other hand, if you want to update the codes, feel free to raise a PR and make sure that backward compatibility exists. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40907\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40907\">No</a>\n", "I have the same problem? the solution mentioned by @Brucewuzhang did not work for me.", "@afi1289 Please open a new issue with a simple standalone code to reproduce the issue. Thanks!", "Here is the solution:\r\njust replace \r\nsave_freq=5000\r\nTo\r\nsave_freq='epoch'\r\n\r\nwelcome :+1: "]}, {"number": 40906, "title": "Segmentation fault in keras.backend.temporal_padding", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nSegfault occurs when passing the tuple of large values for `padding`.\r\n\r\n**Describe the expected behavior**\r\nNo segfault.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = [[[3, 2, 3],\r\n        [1, 3, 4]],\r\n\r\n       [[3, 1, 2],\r\n        [3, 2, 4]],\r\n\r\n       [[4, 4, 2],\r\n        [1, 1, 1]]]\r\npadding = (1130323445, 1510667856)\r\n\r\ntf.keras.backend.temporal_padding(x, padding)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nSegmentation fault (core dumped)\r\n```", "comments": ["Was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/234812d20aac48606c491b0f6bd48d1b/40906.ipynb). Thanks!", "Was able to reproduce the issue  using TF 2.5 and colab crashes as expected. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/9dacdfb9d212caca73dd654aa2ded7c3/untitled85.ipynb).Thanks!", "@mjkim720 It is working as expected when the `padding` is within a range. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/a37429bca1abf79e9d93dde2954d971c/untitled85.ipynb) with smaller padding. \r\n\r\nHowever, when passing large padding as in your example, padding = (1130323445, 1510667856), segfault occurs as it couldn't allocate such a high memory. \r\n\r\nPlease note that `temporal_padding` pads zeros at the start and end of dim 1. \r\n\r\nFor example, if the input tensor is of shape (3, 2, 3), passing a padding of (2,2) results in a tensor of shape (3, 6, 3). In your case, the resulting tensor is of shape (3,2640991303,3) which requires huge memory. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40906\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40906\">No</a>\n"]}, {"number": 40904, "title": "ERROR: tf.function, tf.data.Dataset, tensorflow-gpu incompatible  ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- TensorFlow version (use command below): 2.3.0 (affects all TF > 2.0.0 )\r\n- Python version: 3.7\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI encountered an error when a `tf.data.Dataset` object is created or modified inside a `tf.function` graph *while* using tensorflow-gpu. Based on other issues, it seems this is caused by TF improperly placing the _VariantWrapper on a GPU. A _VariantWrapper is caused when a dataset object is created or modified inside a tf.function graph. \r\n\r\nHere is a small [Colab gist](https://colab.research.google.com/drive/1L_k_jomuAwKoM7RdiR-PLj7AfNZQVzh1?usp=sharing) highlighting code snippet of this issue and the related issues: [issue1](https://github.com/tensorflow/tensorflow/issues/34112), [issue2](https://github.com/tensorflow/tensorflow/issues/34519). This error has persisted since tf>2.0.0.\r\n\r\nTo put it plainly, **does this mean we should avoid wrapping any dataset operations in a tf.function**? \r\n\r\neg.\r\n```python\r\n# Error\r\n@tf.function\r\ndef f():\r\n    dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3], [4, 5, 6]))\r\n    for e in tf.range(3):\r\n        for x, y in dataset:\r\n            tf.print(x, y, e)\r\nf()\r\n```\r\n\r\n```python\r\n#Succeeds outside graph\r\ndataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3], [4, 5, 6]))\r\n\r\n@tf.function\r\ndef f():\r\n    for e in tf.range(3):\r\n        for x, y in dataset: # `dataset` is transformed into `_VariantWrapper` rather than staying as a `tf.data.Dataset` object\r\n            tf.print(x, y, e)\r\nf()\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nColab: https://colab.research.google.com/drive/1L_k_jomuAwKoM7RdiR-PLj7AfNZQVzh1?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe complete error message is: \r\n```python \r\nNo unary variant device copy function found for direction: 1 and Variant type_index: \r\ntensorflow::data::(anonymous namespace)::DatasetVariantWrapper\r\n```\r\n", "comments": ["I have tried in colab with TF version 2.3.0rc0 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/34e39e888cee56a745c883924117498c/untitled58.ipynb).Thanks!", "I have tried in colab with TF version 2.2 it encounters problem @tf.function both inside and outside the graph.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/5d2ad5300d0a5c898a6bc9e9c5462598/untitled67.ipynb).However in nightly version (`2.4.0-dev20200701`) i was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/c879985eb80eff2e8d9d60ec5325b011/untitled68.ipynb).Thanks!", "Thank you for looking into it! Do you know if there is a current workaround when creating a tf.data object inside a `tf.function`?", "Hey @luischinchillagarcia, @ravikyram , I just noticed this issue. Sorry it has not been replied by TF team for such a long time.\r\n\r\nThe following workaround works for me on codelab. Could you try and see if it works?\r\n\r\n```\r\n@tf.function\r\ndef f():\r\n    dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3], [4, 5, 6]))\r\n\r\n    with tf.device('/CPU:0'):\r\n        iterator = iter(dataset)\r\n\r\n    for e in tf.range(3):\r\n        for x, y in iterator:\r\n            tf.print(x, y, e)\r\nf()\r\n```\r\n\r\n", "@luischinchillagarcia @lindong28 \r\n\r\nI have tried in colab with TF nightly version with @lindong28  suggestion and was not able to reproduce the issue even \r\n`dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3], [4, 5, 6]))` is wrapped inside @tf.function. Please,find the gist [here](https://colab.research.google.com/gist/ravikyram/1223e75116a1decc181fb0a3401585dc/untitled188.ipynb).\r\n\r\nNote:\r\nNo issue with  \r\n```\r\n for x, y in iterator:\r\n            tf.print(x, y, e)\r\n```\r\nStill i am seeing the issue with\r\n\r\n```\r\nfor x, y in dataset:\r\n            tf.print(x, y, e)\r\n```\r\nThanks!", "Thanks for the reply @ravikyram. It is great to know that the workaround can unblock your work. In the meantime, I acknowledge that we should still try to fix the issue in TF so that users would not have to make this code change.\r\n\r\nBTW, the gist link provided in your comment can not be opened due to the permission restriction.", "@lindong28 \r\n\r\nPlease check if the below link works\r\nhttps://colab.research.google.com/gist/ravikyram/4a919f0d13d28ffd0f93f1fc8a469448/untitled188.ipynb\r\n\r\nAlso, thank you for the information. I was able to run the code without any issues using your workaround. Thanks!\r\n\r\n", "@luischinchillagarcia,\r\n\r\nPlease take a look at @lindong28 comment above and let us know if the issue is fixed. \r\nCheck the [gist](https://colab.research.google.com/gist/ravikyram/4a919f0d13d28ffd0f93f1fc8a469448/untitled188.ipynb) for your reference.Thanks!\r\n\r\n", "@ravikyram @lindong28 Thanks you both for aiding with this issue! It definitely helps. \r\n\r\nTo be clear, will we be able to use tf.data within tf.function without the need to explicitly state a tf.data.Dataset as an iterator  as follows? Is this merely a temporary fix that still requires a fix down the line as @lindong28 pointed out?\r\n\r\n```python\r\nwith tf.device('/CPU:0'):\r\n        iterator = iter(dataset)\r\n```", "Hey @luischinchillagarcia, thank you for your reply. This is just a temporary fix which hopefully can unblock your ongoing work. It is still a bug if the original program does not work. This issue needs to be fixed so that TF users would not have to make this code change.", "Hey everyone, the issue has been fixed in tf-nightly==2.4.0.dev20200902 and it should be fixed in the upcoming TF 2.4 release.\r\n\r\nThe fix can be validated using this colab example: https://colab.research.google.com/drive/1C4AXS8cyEsA_JmU7QVVk7UXM_uygVf05?authuser=1#scrollTo=jJBsJsRSR6AD", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40904\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40904\">No</a>\n"]}, {"number": 40903, "title": "[-Wsign-compare] warning fixes batch 6", "body": "@mihaimaruseac ", "comments": ["@tg-at-google Can you please check @mihaimaruseac's comments and keep us posted. Thanks!", "@gbaned updated.", "build failed due to undeclared, `copy_i_size` in `bcast.h`, updated `bcast.h` to resolve. "]}, {"number": 40902, "title": "Small update to tf.vectorized_map() and code syntax in docs in control_flow_ops.py", "body": "While reading about `tf.vectorized_map()`, I noticed the code syntax looked a bit off on the [docs site](https://www.tensorflow.org/api_docs/python/tf/vectorized_map). The research paper title should also be mentioned.\r\n\r\n- Added a few backticks in to some TF functions in `control_flow_ops.py` for consistent Markdown rendering on www.tensorflow.org \r\n- Also added a research paper title to the the plain arXiv link in the description (Auto-Vectorizing TensorFlow Graphs: Jacobians, Auto-Batching and Beyond).\r\n\r\nScreenshot from the [site](https://www.tensorflow.org/api_docs/python/tf/vectorized_map):\r\n<img width=\"500\" alt=\"image\" src=\"https://user-images.githubusercontent.com/19637339/85961391-051f4f80-b9a2-11ea-84c6-3cdfc49e0ce3.png\">", "comments": ["@jaingaurav Can you please review this PR ? Thanks!", "@8bitmp3 Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned Hi, I'm not sure how to fix it. I amended a bit Markdown in this PR - see https://github.com/tensorflow/tensorflow/pull/40902 that is not passing Ubuntu test(s).", "It's complaining about line 342 being too long. I think you just need to move the last work to the next line.", "Done. Thanks @jaingaurav \ud83d\udc4d"]}, {"number": 40901, "title": "Type annotations to improve the tracing process of tf.function", "body": "Users can express that a function argument should be treated as a Tensor using `tf.Tensor` as the type annotation.\r\n\r\nSpecify `experimental_follow_type_hints=True` in `tf.function` to limit tracing and give a boost to the usability and performance of `tf.function`.\r\n\r\n### Usage\r\nWith `experimental_follow_type_hints=True` \r\n```python\r\n@tf.function(experimental_follow_type_hints=True)\r\ndef f(x: tf.Tensor):\r\n return x\r\n\r\nf(1)\r\nf(2) # Function is not retraced\r\n```\r\nWith `experimental_follow_type_hints=False` which is the default.\r\n```python\r\n@tf.function\r\ndef f(x):\r\n return x\r\n\r\nf(1)\r\nf(2) # Function is retraced\r\n```", "comments": ["@rahul-kamat Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned could you have a look at the copybara error? Thanks!", "@gbaned can you help finalize merging this PR, or assign to someone who can?"]}, {"number": 40900, "title": "Where is imgae_recognition_model.cc in image_recognition_experimental?", "body": "Sorry to bother. I would like to run the image recognition model by Keil but I could not find \"image recognition model data\".\r\n\r\nCould you tell me where the document is?\r\n\r\nOr could you tell me where I can find the tutorial about how to train this model?", "comments": ["@uf19414,\r\nCould you please provide more information about 'image recognition model by Keil' or provide any link about its description. Thanks! ", "> @uf19414,\r\n> Could you please provide more information about 'image recognition model by Keil' or provide any link about its description. Thanks!\r\n![image](https://user-images.githubusercontent.com/61425756/86044895-ad84f080-ba42-11ea-9cf3-5263d3c414f5.png)\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/image_recognition_experimental\r\n\r\nThere is only \"image_recognition_model.h\" in this link.\r\n\r\nI think there should be \"image_recognition_model.cc\" which is the data of image recogntion model in this link too.\r\n\r\nLike \u201cmodel.cc\" and \"model.h\" in this link \r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech/micro_features", "The model is downloaded from [here](https://storage.googleapis.com/download.tensorflow.org/models/tflite/cifar_image_recognition_model_2020_05_27.zip) when running the make-command, due to it being too large to store in the git repository.\r\n\r\nIf you take a look in the [Makefile.inc ](https://github.com/tensorflow/tensorflow/blob/fe968502a9835afec951a669d64224e411746605/tensorflow/lite/micro/examples/image_recognition_experimental/Makefile.inc#L1) file in the example it downloads the IMAGE_RECOGNITION_MODEL_URL as specified [here](https://github.com/tensorflow/tensorflow/blob/fe968502a9835afec951a669d64224e411746605/tensorflow/lite/micro/tools/make/third_party_downloads.inc#L70). It then moves the downloaded .cc-file into the generated directory.\r\n\r\n", "@uf19414 Could you please have a look at the[ file link](https://github.com/tensorflow/tensorflow/blob/fe968502a9835afec951a669d64224e411746605/tensorflow/lite/micro/examples/image_recognition_experimental/Makefile.inc#L1) and let us know if it helps ? Please refer to the [comment](https://github.com/tensorflow/tensorflow/issues/40900#issuecomment-671199667) above as well. Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40899, "title": "tf nightly not supporting EXP, stack and depth to space layers on gpu", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android Studio\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) Redmi note 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): nightly\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\ntf nightly not supporting EXP, stack and depth to space layers on gpu\r\n\r\n**Describe the expected behavior**\r\n\r\nthese layers should be supported as per documentation.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@rlrahulkanojia \r\nPlease provide complete stand alone code to reproduce the issue reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40899\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40899\">No</a>\n"]}, {"number": 40898, "title": "Failed to run the tflite model on Interpreter due to Internal Error", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0-dev20200617\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI am trying to run a neural machine translator on android. The model runs perfectly on my jupyter notebook but when I used the generated tflite model to get predictions on android, it throws the error ```java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/concatenation.cc:73 t->dims->data[d] != t0->dims->data[d] (8 != 1) Node number 84 (CONCATENATION) failed to prepare.``` \r\n\r\nThis is strange because I have provided the exact same input dimensions as I did in my jupyter notebook. \r\n\r\nAs the error pointed, there was something wrong with dimensions at node 84. So I went ahead and visualised the tflite file using Netron. I have zoomed the concatenation node, you can find the pic of the node along with input and output dimensions [here](https://i.stack.imgur.com/3unpX.png). You can find the whole generated graph [here](https://drive.google.com/file/d/1DXxuke12NwwFREaSJyaoXrSNJ_nmjZ4O/view).\r\n\r\nAs it turns out, the concatenation node at location 84 isn't actually concatenating, you can see this from the input and output dimensions. It just spits out a 1X1X1 matrix after processing 1X1X1 and 1X1X256 matrix. But is this responsible for the crash? Is the node computing correctly?\r\n\r\nAlso, could anyone please explain me what does the error mean by t->dims->data[d] != t0->dims->data[d] what is d?\r\n**Describe the expected behavior**\r\n\r\nThe tflite model should be able to make predictions\r\n\r\n**Standalone code to reproduce the issue**\r\nPlease find the tflite file [here](https://drive.google.com/file/d/1zCHHbg20FPfRqqGEho8XtppMcmzxMKG8/view?usp=sharing). You can reproduce the error using the following snippet on android once you have the Interpreter ```tfLite``` initialised:\r\n\r\n```\r\n        HashMap<Integer, Object> outputVal = new HashMap<>();\r\n        for(int i=0; i<2; i++) outputVal.put(i, new float[1][5]);\r\n        float[][] inp_test = new float[1][8];\r\n        float[][] enc_hidden = new float[1][1024];\r\n        float[][] dec_input = new float[1][1];\r\n        float[][] dec_test = new float[1][8];\r\n\r\n        tfLite.runForMultipleInputsOutputs(new Object[] {inp_test,enc_hidden, dec_input, dec_test},outputVal);\r\n```\r\nMy gradle dependencies:\r\n```\r\ndependencies {\r\n    implementation fileTree(dir: 'libs', include: ['*.jar'])\r\n\r\n    implementation 'androidx.appcompat:appcompat:1.1.0'\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n    // This dependency adds the necessary TF op support.\r\n    implementation 'androidx.constraintlayout:constraintlayout:1.1.3'\r\n    testImplementation 'junit:junit:4.12'\r\n    androidTestImplementation 'androidx.test.ext:junit:1.1.1'\r\n    androidTestImplementation 'androidx.test.espresso:espresso-core:3.2.0'\r\n}\r\n```\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nHere is the code for the model:\r\nIt is highly inspired by this guide: https://www.tensorflow.org/tutorials/text/nmt_with_attention \r\n\r\n```\r\n\r\nTx = 8\r\ndef Partial_model():\r\n    outputs = []\r\n    X = tf.keras.layers.Input(shape=(Tx,))\r\n    partial = tf.keras.layers.Input(shape=(Tx,))\r\n    enc_hidden = tf.keras.layers.Input(shape=(units,))\r\n    dec_input = tf.keras.layers.Input(shape=(1,))\r\n    \r\n    d_i = dec_input\r\n    e_h = enc_hidden\r\n    X_i = X\r\n    \r\n    enc_output, e_h = encoder(X, enc_hidden)\r\n    \r\n    \r\n    dec_hidden = enc_hidden\r\n    print(dec_input.shape, 'inp', dec_hidden.shape, 'dec_hidd')\r\n    for t in range(1, Tx):\r\n        print(t, 'tt')\r\n      # passing enc_output to the decoder\r\n        predictions, dec_hidden, _ = decoder(d_i, dec_hidden, enc_output)\r\n#         outputs.append(predictions)\r\n        print(predictions.shape, 'pred')\r\n        d_i = tf.reshape(partial[:, t], (-1, 1))\r\n        print(dec_input.shape, 'dec_input')\r\n    \r\n    predictions, dec_hidden, _ = decoder(d_i, dec_hidden, enc_output)\r\n    d_i = tf.squeeze(d_i)\r\n    \r\n    outputs.append(tf.math.top_k(predictions, 5))\r\n    \r\n    return tf.keras.Model(inputs = [X, enc_hidden, dec_input, partial], outputs = [outputs[0][0], outputs[0][1]])\r\n\r\n\r\n\r\n\r\nclass Encoder():\r\n  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\r\n    self.batch_sz = batch_sz\r\n    self.enc_units = enc_units\r\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n    self.gru = tf.keras.layers.GRU(self.enc_units,\r\n                                   return_sequences=True,\r\n                                   return_state=True,\r\n                                   recurrent_initializer='glorot_uniform')\r\n\r\n  def __call__(self, x, hidden):\r\n    x = self.embedding(x)\r\n    output, state = self.gru(x, initial_state = hidden)\r\n    print(output.shape, hidden.shape, \"out\", \"hid\")\r\n    return output, state\r\n\r\n\r\n  def initialize_hidden_state(self):\r\n    return tf.zeros((self.batch_sz, self.enc_units))\r\n\r\n\r\n\r\nclass BahdanauAttention():\r\n  def __init__(self, units):\r\n    self.W1 = tf.keras.layers.Dense(units)\r\n    self.W2 = tf.keras.layers.Dense(units)\r\n    self.V = tf.keras.layers.Dense(1)\r\n\r\n  def __call__(self, query, values):\r\n    # query hidden state shape == (batch_size, hidden size)\r\n    # query_with_time_axis shape == (batch_size, 1, hidden size)\r\n    # values shape == (batch_size, max_len, hidden size)\r\n    # we are doing this to broadcast addition along the time axis to calculate the score\r\n    print(query.shape, 'shape')\r\n    query_with_time_axis = tf.expand_dims(query, 1)\r\n    # score shape == (batch_size, max_length, 1)\r\n    # we get 1 at the last axis because we are applying score to self.V\r\n    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\r\n    print(\"2\")\r\n    score = self.V(tf.nn.tanh(\r\n        self.W1(query_with_time_axis) + self.W2(values)))\r\n    print(\"3\")\r\n\r\n    # attention_weights shape == (batch_size, max_length, 1)\r\n    attention_weights = tf.nn.softmax(score, axis=1)\r\n\r\n    # context_vector shape after sum == (batch_size, hidden_size)\r\n    context_vector = attention_weights * values\r\n    context_vector = tf.reduce_sum(context_vector, axis=1)\r\n    \r\n    return context_vector, attention_weights\r\n\r\n\r\nclass Decoder():\r\n  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\r\n    self.dec_units = dec_units\r\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n    self.gru = tf.keras.layers.GRU(self.dec_units,\r\n                                   return_sequences=True,\r\n                                   return_state=True,\r\n                                   recurrent_initializer='glorot_uniform')\r\n    self.fc = tf.keras.layers.Dense(vocab_size)\r\n\r\n    # used for attention\r\n    self.attention = BahdanauAttention(self.dec_units)\r\n\r\n  def __call__(self, x, hidden, enc_output):\r\n    # enc_output shape == (batch_size, max_length, hidden_size)\r\n    context_vector, attention_weights = self.attention(hidden, enc_output)\r\n    \r\n    print(context_vector.shape, 'c_v', attention_weights.shape, \"attention_w\")\r\n\r\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\r\n    x = self.embedding(x)\r\n\r\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\r\n    print(x.shape, 'xshape', context_vector.shape, 'context')\r\n    expanded_dims = tf.expand_dims(context_vector, 1)\r\n    x = tf.concat([expanded_dims, x], axis=-1)\r\n\r\n    # passing the concatenated vector to the GRU\r\n    output, state = self.gru(x)\r\n\r\n    # output shape == (batch_size * 1, hidden_size)\r\n    output = tf.reshape(output, (-1, output.shape[2]))\r\n\r\n    # output shape == (batch_size, vocab)\r\n    x = self.fc(output)\r\n\r\n    return x, state, attention_weights\r\n\r\n\r\n\r\n\r\n```\r\nThe snippet I used to generate the tflite file:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(partial_model)\r\ntflite_model = converter.convert()\r\n\r\nwith tf.io.gfile.GFile('goog_nmt_v2.tflite', 'wb') as f:\r\nf.write(tflite_model)\r\n```\r\n\r\nAnd this is the error log: \r\n```\r\n2020-06-29 01:16:38.290 8824-8824/com.example.inmt_offline E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.inmt_offline, PID: 8824\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/concatenation.cc:73 t->dims->data[d] != t0->dims->data[d] (8 != 1)\r\n    Node number 84 (CONCATENATION) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:158)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:343)\r\n        at com.example.inmt_offline.MainActivity.runModel(MainActivity.java:207)\r\n        at com.example.inmt_offline.MainActivity.access$400(MainActivity.java:30)\r\n        at com.example.inmt_offline.MainActivity$1.onClick(MainActivity.java:83)\r\n        at android.view.View.performClick(View.java:7870)\r\n        at android.widget.TextView.performClick(TextView.java:14970)\r\n        at android.view.View.performClickInternal(View.java:7839)\r\n        at android.view.View.access$3600(View.java:886)\r\n        at android.view.View$PerformClick.run(View.java:29363)\r\n        at android.os.Handler.handleCallback(Handler.java:883)\r\n        at android.os.Handler.dispatchMessage(Handler.java:100)\r\n        at android.os.Looper.loop(Looper.java:237)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7814)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1068)\r\n```", "comments": ["For Concat, every dims of inputs and output except axis should be the same.\r\nThe dim at axis of output is equal to the sum of all input dims at axis.\r\nCould you printout the dims of the input in:\r\nx = tf.concat([expanded_dims, x], axis=-1)\r\n", "I tried to run the goog_nmt_v2.tflite, it seems to be OK. ", "@anuragshukla06 \r\n\r\nIs this still an issue.Please, close this thread if your issue was resolved.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40898\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40898\">No</a>\n"]}, {"number": 40897, "title": "[Wsign-compare] warning fixes batch 5", "body": "associated_warning_ids: [\r\n114, 118, 119, 121,\r\n124, 127, 133, 142, \r\n145, 146, 148, 159, \r\n160, 161, 164, 173, \r\n174, 176\r\n]", "comments": ["@mihaimaruseac ", "```\r\ntensorflow/core/profiler/convert/xplane_to_memory_profile.cc: In function 'void tensorflow::profiler::{anonymous}::ProcessActiveAllocations(tensorflow::int64, tensorflow::profiler::PerAllocatorMemoryProfile*)':\r\ntensorflow/core/profiler/convert/xplane_to_memory_profile.cc:416:16: error: '_active_allocs_size' was not declared in this scope\r\n     while (i < _active_allocs_size &&\r\n                ^~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/profiler/convert/xplane_to_memory_profile.cc:416:16: note: suggested alternative: 'active_allocs_size'\r\n     while (i < _active_allocs_size &&\r\n                ^~~~~~~~~~~~~~~~~~~\r\n                active_allocs_size\r\n```\r\n\r\nPlease make sure everything builds", "@tg-at-google Can you please check @mihaimaruseac's comments and keep us posted. Thanks!", "@gbaned updated.", "Closing as it has been handled by new PRs."]}, {"number": 40896, "title": "TimeDistributed Layer Does Not Support Multiple Outputs", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from source or binary: No\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.1/7.6.4\r\n- GPU model and memory: Tesla V100 - 16GB\r\n\r\n**Describe the current behavior**\r\nThe [TimeDistributed layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed) does not support layers with multiple outputs. This issue is also related to #35824, where the missing support for multiple inputs is mentioned, which leads to the overall missing support for nested structures of inputs and outputs.\r\n\r\n**Describe the expected behavior**\r\nThe TimeDistributed wrapper should support the wrapping of layers with multiple inputs and multiple outputs.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nclass CustomLayer(tf.keras.layers.Layer):\r\n    def __init__(self, name=None, **kwargs):\r\n        super(CustomLayer, self).__init__(name=name, **kwargs)\r\n        self.conv_1 = tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 1))\r\n        self.conv_2 = tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 1))\r\n\r\n    def call(self, inputs):\r\n        output_1 = self.conv_1(inputs)\r\n        output_2 = self.conv_2(inputs)\r\n\r\n        return output_1, output_2\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        output_shape_1 = self.conv_1.compute_output_shape(input_shape)\r\n        output_shape_2 = self.conv_2.compute_output_shape(input_shape)\r\n\r\n        return output_shape_1, output_shape_2\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    inputs = tf.keras.Input(shape=(None, None, None, 1))\r\n\r\n    custom_layer = CustomLayer()\r\n    output_1, output_2 = tf.keras.layers.TimeDistributed(custom_layer)(inputs)\r\n\r\n```\r\n**Other info / logs**\r\n```\r\nFile \"/reproduce/template.py\", line 29, in <module>\r\n    output_1, output_2 = tf.keras.layers.TimeDistributed(custom_layer)(inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 922, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/wrappers.py\", line 246, in call\r\n    output_shape = self.compute_output_shape(input_shape).as_list()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/wrappers.py\", line 192, in compute_output_shape\r\n    child_output_shape = tensor_shape.TensorShape(child_output_shape)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 771, in __init__\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 771, in <listcomp>\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 716, in as_dimension\r\n    return Dimension(value)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 200, in __init__\r\n    None)\r\n  File \"<string>\", line 3, in raise_from\r\nTypeError: Dimension value must be integer or None or have an __index__ method, got TensorShape([None, None, None, 1])\r\n```", "comments": ["Was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/207c828cbc9741176497c7883729f6a2/35824.ipynb). Thanks!", "@ffent This is  a duplicate of this issue #35824, Can we go ahead and close this issue and track it as a single issue? Thanks!", "@gowthamkpr As mentioned in my issue description, both issues refer to the overall missing support for nested structures within the wrapped layer inputs and outputs. Since both issues are addressed in my recent [pull request](https://github.com/tensorflow/tensorflow/pull/40993), feel free to close one of the issues.", "Thanks @ffent I am closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40896\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40896\">No</a>\n"]}, {"number": 40895, "title": "nested gradients for convolution layer fail under tf.function", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): pip tf_nightly\r\n- TensorFlow version (use command below): v1.12.1-35353-gcbb94efa58 2.5.0-dev20200628\r\n- Python version:3.6.9\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce GTX 1050 Ti with Max-Q 4 Gb\r\n\r\n**Describe the current behavior**\r\nThe code below works in eager mode, but fails with the following error if using tf.function\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"bugreport.py\", line 55, in <module>\r\n    value = func(x)\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 823, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 697, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2870, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 3227, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 3089, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 600, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"bugreport.py\", line 48, in func\r\n    grads = tape.gradient(loss, variables)\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 1073, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\", line 77, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 162, in _gradient_function\r\n    return grad_fn(mock_op, *out_grads)\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py\", line 50, in _Conv2DBackpropInputGrad\r\n    strides=op.get_attr(\"strides\"),\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 121, in get_attr\r\n    raise KeyError(attr)\r\nKeyError: 'strides'\r\n```\r\n\r\n**Describe the expected behavior**\r\nI would expect this code to work the same under eager mode or wrapped with tf.function.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nshape = (2, 3, 3, 5, 7)\r\ninit = tf.random.normal(shape=shape, dtype=tf.float32)\r\nweights = tf.Variable(initial_value=init, trainable=True, shape=shape, dtype=tf.float32)\r\n\r\ndef conv(x):\r\n    kernel = tf.reshape(tf.eye(3 * 3, dtype=x.dtype), shape=(3, 3, 1, 3 * 3))\r\n\r\n    x = tf.reshape(x, shape=(2 * 5, 1, 4, 4))\r\n    x = tf.nn.conv2d(x, kernel, strides=(1,1), padding='SAME', data_format='NCHW')\r\n    x = tf.reshape(x, shape=(2, 5 * 3 * 3, 4 * 4))\r\n\r\n    W = tf.reshape(weights, shape=(2, 3 * 3, 5, 7))\r\n    W = tf.transpose(W, perm=[0, 2, 1, 3])\r\n    W = tf.reshape(W, shape=(2, 5 * 3 * 3, 7))\r\n    \r\n    y = tf.linalg.matmul(W, x, transpose_a=True)\r\n    y = tf.reshape(y, shape=(2, 7, 4, 4))\r\n    y = tf.square(y)\r\n\r\n    return y\r\n\r\ndef func_flat(x_flat):\r\n    x_unflat = tf.reshape(x_flat, shape=(2, 5, 4, 4))\r\n\r\n    with tf.GradientTape() as tape:\r\n        tape.watch(x_unflat)\r\n        y = conv(x_unflat)\r\n        u = tf.reduce_sum(y, axis=[-3, -2, -1])\r\n        u = tf.reshape(u, shape=(2, 1))\r\n\r\n    jac = tape.batch_jacobian(u, x_unflat)\r\n    return jac\r\n\r\nvariables = [weights]\r\n\r\n@tf.function(autograph=False)\r\ndef func(x):\r\n    with tf.GradientTape() as tape:\r\n        tape.watch(weights)\r\n\r\n        y_flat = func_flat(x)\r\n        loss = tf.reduce_sum(tf.square(y_flat))\r\n\r\n    grads = tape.gradient(loss, variables)\r\n\r\n    return tf.reduce_sum(grads)\r\n\r\n\r\nx = tf.random.normal(shape=(2, 5 * 4 * 4), dtype=tf.float32)\r\n\r\nvalue = func(x)\r\nprint(value)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@Saduf2019  yes, I forgot to mention it in the report, there is some amount of non-determinism. sometimes it reports KeyError on strides, sometimes on the explicit_paddings, but the cause of the error seems to be the same I reported.", "Also, let me clarify that this requires a notebook with GPU. If we remove the `@tf.function(autograph=False)` line, it works as expected on gpu, but would complain about an unrelated thing on cpu: no Conv2D kernel in the NCHW order.", "I ran the code on different versions of tf and the error is different as reported, please find the gist here [for nightly](https://colab.research.google.com/gist/Saduf2019/3ba4a2f8f9f8f220fcee89398adfc6cf/untitled246.ipynb), [1.12](https://colab.research.google.com/gist/Saduf2019/135221a048088eedc9ab3d638e2f934e/untitled246.ipynb), ", "@Saduf2019 As I mentioned before, the error on the nightly gist above is almost the same. If you repeat the execution several times it changed back and forth between KeyError on strides, explicit_paddings or dilations.\r\n\r\nThe stack trace is identical. See an other execution on this [gist](https://colab.research.google.com/drive/1v9ge0nrS8hpe5EChn251TNUeOXRcEKvw#scrollTo=SRWRfyFm8Zy9)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40895\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40895\">No</a>\n", "Hi,\r\n\r\nAny update on this?\r\n@aroig did you find a solution?", "https://github.com/tensorflow/tensorflow/issues/45063 might be a duplicate, @Saduf2019 @gowthamkpr you could consider that reproduction too."]}, {"number": 40894, "title": "directory structure has changed", "body": "This is a Codelabs doc bug\r\n\r\n## URL(s) with the issue:\r\nhttps://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android/#3\r\n\r\n## Description of issue (what needs changing):\r\nChange directory structure from\r\nexamples/lite/codelabs/flower_classification/start\r\nto\r\nexamples/lite/codelabs/flower_classification/android/start\r\n\r\n{Admittedly, a minor issue, but am I doing this right?}", "comments": ["@NotUserX \r\n\r\nIt was moved out of tensorflow GitHub repository to a new repository on GitHub. Link to the new repository is [here](https://github.com/googlecodelabs/tools/issues). Please file an issue there so that it will be resolved faster. You could also raise a PR to update the docs in that repository. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40893, "title": "d", "body": "d", "comments": ["@Tpeyre,\r\nChanging the code from `from .model import mymodel` to `from model import mymodel` in the `import.py` file seems to work. \r\n\r\nI was able to run the code without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/c138d23bd8e50f43e2a261503251ff78/40893.ipynb). \r\n\r\n\r\n\r\n\r\n![Screenshot 2020-06-29 at 10 31 45 PM](https://user-images.githubusercontent.com/57165142/86034504-5ab63380-ba58-11ea-8fe7-f7e728d611ce.png)\r\n\r\n\r\nThanks!"]}, {"number": 40892, "title": "model.set_inputs does not work for keras model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS Linux release 7.7.1908 (Core) \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: Python 3.6.10 :: Anaconda, Inc.\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nmodel.set_inputs does not work for keras model. \r\nAfter setting model._set_inputs, the model.inputs and model.outputs are still all None. \r\nThe bug happens for tensorflow>=2.2. It works fine for tensorflow<2.2.\r\n**Describe the expected behavior**\r\nmodel.inputs will tell the shape [1,64] and model.outputs tell the shape [1,10]\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass MYMODEL(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MYMODEL, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(10)\r\n    def call(self, inputs):\r\n        output = self.dense1(inputs)\r\n        return output\r\n\r\nmodel_keras = MYMODEL()\r\n\r\ninput_spec = tf.TensorSpec([1, 64], tf.int32)\r\nmodel_keras._set_inputs(input_spec, training=False)\r\n\r\n# keras_input = tf.keras.Input([64], batch_size=1, dtype=tf.int32)\r\n# keras_output = model_keras(keras_input, training=False)\r\n# model_keras = tf.keras.Model(keras_input, keras_output)\r\n\r\nprint(model_keras.inputs)\r\nprint(model_keras.outputs)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nAlthough I can set inputs by adding an additional input layer before keras model as in the comment of source code as above.\r\nI still want to know whether it is a bug.", "comments": ["I am able to replicate the issue faced, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/2d061d8cb96bf1c8e6c204dd4bfb1745/untitled244.ipynb)", "The code snippet works with TF 2.1 \r\nSee attached [GitHub gist](https://colab.research.google.com/gist/ymodak/3b05475603160bb0130be1e9ee1d81dd/untitled244.ipynb)", "Thanks! It wors on TF<2.2. I am curious on whether it is a bug in TF>=2.2 or the set_inputs does not be supported in TF>=2.2.\r\nIf a bug, will It by fixed in the future version.", "The `_set_inputs` method is a private API, and is not intended to be called in user code. The solution you suggest (using an input layer) would work, or moving to the [Functional API](https://keras.io/guides/functional_api/)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40892\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40892\">No</a>\n", "Thanks!"]}, {"number": 40891, "title": "trying model with different sizes (The first argument to `Layer.call` must always be passed)", "body": "I face problem when I try to build model and try different batch sizes \r\n code \r\n```\r\ndef model():\r\n    model = Sequential()\r\n    model.add(Dense(64, activation='relu'))\r\n    model.add(Dense(32, activation='relu'))\r\n    model.add(Dense(1, activation='linear'))\r\n\r\n    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse', 'mae'])\r\n    return model \r\n\r\nbatch_sizes = [32, 64, 128, 256, 512]\r\n\r\nfig = plt.figure(figsize=(12, 12))\r\nfor i in range(1, 6):\r\n\r\n    fig.add_subplot(2, 3, i)\r\n    model = model()\r\n    history= model.fit(x, y, batch_size=batch_sizes[i-1], \r\n                        validation_data=(x_test, y_test), verbose=0,epochs=100)\r\n   plt.plot(history.history['loss'], label='train')\r\n    plt.plot(history.history['val_loss'], label='validation')\r\n    plt.legend(['train', 'test'], loc='upper right')\r\n    plt.ylabel('loss')\r\n    plt.xlabel('epoch')\r\n    plt.title('model loss in batch size = '+str(batch_sizes[i-1]), pad=-80)\r\n```\r\n\r\n\r\nit gives this error \r\n```\r\n----> 7     model = model()\r\n      8     history= model.fit(x, y, batch_size=batch_sizes[i-1], \r\n      9                         validation_data=(x_test, y_test), verbose=0,epochs=100)\r\n\r\n~\\anaconda3\\envs\\teachopencadd\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, *args, **kwargs)\r\n    798     else:\r\n    799       raise ValueError(\r\n--> 800           'The first argument to `Layer.call` must always be passed.')\r\n    801 \r\n    802     call_context = base_layer_utils.call_context()\r\n\r\nValueError: The first argument to `Layer.call` must always be passed.\r\n        \r\n```", "comments": ["@Ahmed-fub \r\n\r\nRequest you to fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nwhat platform you are using (operating system, architecture). Also include your TensorFlow version.\r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram  \r\nI filled the as other issue \r\nI solved the problem was in the same variable as model= model() gives error when change the na,e of variable as model1=model() ,it not gives me the error \r\n"]}, {"number": 40890, "title": "Tensorflow Installation via windows command prompt", "body": "Hi,\r\nI have python 3.8 in my computer (windows-10 64 bit) and i'm getting error while installing tensor via command prompt.\r\nBelow are the lines i executed:\r\nC:\\Program Files (x86)\\Python38-32\\Scripts>pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-2.2.0-cp38-cp38m-window.whl\r\n\r\nERROR: tensorflow-2.2.0-cp38-cp38m-window.whl is not a supported wheel on this platform. \r\n\r\nNeed help to install.\r\n\r\nNOTE: I'm able to install tensorflow-1.12.0 . But during import i'm getting error\r\nImportError: No module named '_pywrap_tensorflow_internal'", "comments": ["@Praba00,\r\nCould you please check if you are using 64-bit version of Python? \r\n\r\nTensorFlow is tested and supported on 64-bit systems, for more information please check the system requirements [here](https://www.tensorflow.org/install/pip#system-requirements). Thanks!", "Hi @Praba00 , have you tried downgrading Python to 3.7 and installing it again? I am running TF 2.2 on Windows 10 with Python 3.7", "Hi @amahendrakar\r\n\r\nMy python version is of 32 bit. I will install 64 bit and try installing tensorflow again. Thanks for your response", "> Hi,\r\n> I have python 3.8 in my computer (windows-10 64 bit) and i'm getting error while installing tensor via command prompt.\r\n> Below are the lines i executed:\r\n> C:\\Program Files (x86)\\Python38-32\\Scripts>pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-2.2.0-cp38-cp38m-window.whl\r\n> \r\n> ERROR: tensorflow-2.2.0-cp38-cp38m-window.whl is not a supported wheel on this platform.\r\n> \r\n> Need help to install.\r\n> \r\n> NOTE: I'm able to install tensorflow-1.12.0 . But during import i'm getting error\r\n> ImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nThe Problem might be with python version, check again if you installed 64-bit version of python.", "> My python version is of 32 bit. I will install 64 bit and try installing tensorflow again. Thanks for your response\r\n\r\n@Praba00,\r\nAny updates regarding this? Please feel free to close the issue if resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40890\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40890\">No</a>\n"]}, {"number": 40889, "title": "Yes", "body": "Okay", "comments": []}, {"number": 40888, "title": "How to enable XNNPACK on android using official repository?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: armv7 and v8\r\n- TensorFlow version: 2.2.0\r\n- Installed using virtualenv?\r\n    implementation 'org.tensorflow:tensorflow-lite:2.2.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:2.2.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly'\r\n\r\n**Describe the problem**\r\n\r\nI am using tflite on android using below repos. Is the XNNPACK enabled by default on android or is there anything else required to enable it?\r\n```\r\n    implementation 'org.tensorflow:tensorflow-lite:2.2.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:2.2.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly'\r\n```", "comments": ["You can try enabling XNNPACK using Java Api for android.\r\nSee https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack#enable-xnnpack-via-java-api-on-android-recommended-on-android", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40888\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40888\">No</a>\n"]}, {"number": 40887, "title": "Create Tensorflow", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40887) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40887) for more info**.\n\n<!-- ok -->", "How do i reviwe Ubuntu CPU\r\n"]}, {"number": 40886, "title": "How can I use tensorflow with Graphcore IPU", "body": "I want to test performance with graphcore IPU, but I don't know how to do with tensorflow. Someone can help me to do this?", "comments": ["@hypercxx \r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!\r\n", "ye I think so", "Automatically closing the issue as this questions need to be asked in stack overflow since it is not a bug or feature request of Tensorflow.Thanks!"]}, {"number": 40885, "title": "tf.GradientTape.batch_jacobian fails on compiled tf.function due to assert op", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): pip tf-nightly\r\n- TensorFlow version (use command below): v1.12.1-35353-gcbb94efa58 2.5.0-dev20200628\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce GTX 1050 Ti with Max-Q Design 4 Gb\r\n\r\n\r\n**Describe the current behavior**\r\n\r\ntf.GradientTape.batch_jacobian fails to run on compiled tf.function due to an unsupported graph assert.\r\n\r\n**Describe the expected behavior**\r\nI would expect batch_jacobian to work the same in compiled or uncompiled tf.function\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function(experimental_compile=True)\r\ndef func(x):\r\n    with tf.GradientTape() as tape:\r\n        tape.watch(x)\r\n        y = tf.square(x)\r\n\r\n    jac = tape.batch_jacobian(y, x)\r\n\r\n    return jac\r\n\r\nx = tf.zeros(shape=(2, 2), dtype=tf.float32)\r\njac = func(x)\r\n```\r\nfails with this error\r\n```\r\nTraceback (most recent call last):\r\n  File \"bugreport.py\", line 14, in <module>\r\n    jac = func(x)\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 775, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 846, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1847, in _filtered_call\r\n    cancellation_manager=cancellation_manager)\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1923, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 550, in call\r\n    ctx=ctx)\r\n  File \"/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Function invoked by the following node is not compilable: {{node __inference_func_203}} = __inference_func_203[_XlaMustCompile=true, config_proto=\"\\n\\007\\n\\003CPU\\020\\001\\n\\007\\n\\003GPU\\020\\0012\\005*\\0010J\\0008\\001\\202\\001\\000\", executor_type=\"\"](dummy_input).\r\nUncompilable nodes:\r\nassert_equal_1/Assert/Const: unsupported op: Const op with type DT_STRING is not supported by XLA.\r\n\tStacktrace:\r\n\t\tNode: __inference_func_203, function: \r\n\t\tNode: assert_equal_1/Assert/Const, function: __inference_func_203\r\n\r\nassert_equal_1/Assert/Const_1: unsupported op: Const op with type DT_STRING is not supported by XLA.\r\n\tStacktrace:\r\n\t\tNode: __inference_func_203, function: \r\n\t\tNode: assert_equal_1/Assert/Const_1, function: __inference_func_203\r\n\r\nassert_equal_1/Assert/Const_2: unsupported op: Const op with type DT_STRING is not supported by XLA.\r\n\tStacktrace:\r\n\t\tNode: __inference_func_203, function: \r\n\t\tNode: assert_equal_1/Assert/Const_2, function: __inference_func_203\r\n\r\nassert_equal_1/Assert/Assert/data_0: unsupported op: Const op with type DT_STRING is not supported by XLA.\r\n\tStacktrace:\r\n\t\tNode: __inference_func_203, function: \r\n\t\tNode: assert_equal_1/Assert/Assert/data_0, function: __inference_func_203\r\n\r\nassert_equal_1/Assert/Assert/data_1: unsupported op: Const op with type DT_STRING is not supported by XLA.\r\n\tStacktrace:\r\n\t\tNode: __inference_func_203, function: \r\n\t\tNode: assert_equal_1/Assert/Assert/data_1, function: __inference_func_203\r\n\r\nassert_equal_1/Assert/Assert/data_3: unsupported op: Const op with type DT_STRING is not supported by XLA.\r\n\tStacktrace:\r\n\t\tNode: __inference_func_203, function: \r\n\t\tNode: assert_equal_1/Assert/Assert/data_3, function: __inference_func_203\r\n [Op:__inference_func_203]\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe problem vanishes if I remove the assert_equal on line 1276 in `tensorflow/python/eager/backprop.py`\r\n\r\n```python\r\n    ...\r\n    with ops.control_dependencies(\r\n        [check_ops.assert_equal(batch_size, source_shape[0])]):\r\n      target = array_ops.reshape(target, [batch_size, target_row_size])\r\n    ...\r\n\r\n```\r\nI'd be happy to submit a PR, however I am not sure how to replace the assert with a compile-friendly version.", "comments": ["Was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/956165732b72253d3af339c97f58fad1/40885.ipynb). Thanks!", "@aroig I tried to reproduce the issue  but it doesnt throw any error and resulting output as expected. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/2689b32ad29233412b75613db80a2371/untitled85.ipynb).\r\n\r\nClosing this issue since the issue is resolved in latest TF version. Please feel free to re-open the issue if needed. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40885\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40885\">No</a>\n"]}, {"number": 40884, "title": "Pinned scipy dependency", "body": "**System information**\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.7\r\n- Installed using: pip\r\n\r\n**Describe the problem**\r\nPinned `scipy` dependency. Accepted requirements should be greater than or equal to the version tested by devs, but here are just equal (currently pinned to 1.4.1).\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\npip install scipy\r\npip install tensorflow\r\n```\r\n\r\n**Any other info / logs**\r\n```\r\nERROR: tensorflow 2.2.0 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.5.0 which is incompatible.\r\n```\r\n", "comments": ["Can you make a PR to completely remove the dependency please?\r\n\r\nSee also #35709 and #40789 ", "@mihaimaruseac - I did not realize it can be an even greater bug, i.e. an unused dependency... I can only report the mismatch of requirements with the latest version of this dependency, and an easy workaround - replacing the first equality sign in the requirements file with a \"greater than\" sign:) Are we absolutely sure the dependency is unused?", "A few months ago it was unused. I'm quite confident it is unused now too.", "> \r\n> \r\n> A few months ago it was unused. I'm quite confident it is unused now too.\r\n\r\nI think some unit tests would fail if the user had no `scipy` package pre-installed. But you may argue they are to be run only at build time and thus not needed on the typical user machine.... unless she does the build + tests herself.\r\n\r\nSome tests where `scipy` is required:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/random/random_binomial_test.py#L53\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/composite_tensor_support_test.py#L386\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/distributions/student_t_test.py#L46\r\n\r\n", "Yes, `scipy` is used only at test time. It should not show as install dependency in `setup.py` though.", "@mirekphd \r\nPlease update as per above comment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40884\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40884\">No</a>\n", "hum, the scipy==1.4.2 dependancy is apparently not removed in Tensorflow_cpu-2.3.0rc2. is it normal ?", "The `r2.3` branch used for the 2.3 release has https://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/tools/pip_package/setup.py#L72-L73\r\n\r\nThe relaxation only occurred on master.", "it may become a problem when  pip new strict resolver comes in full force this fall 2020. Is there any hope to remove it for final 2.3.0 ?", "Unfortunately we have started the final build steps, so we can only attempt removal on a subsequent patch release.", "Can we re-open this issue so it doesn't get lost?", "Did it in a better way. Now, if there is a patch release for any of the affected branches the scipy pinning will also be removed.", "> Did it in a better way. Now, if there is a patch release for any of the affected branches the scipy pinning will also be removed.\r\n\r\nThanks very much for fixing this (really fast too! :rocket:), as this is keeping `pyhf` from using TensorFlow Probability `v0.11.0`. :+1: ", "I'm seeing similar issue on aarch64 platform. \r\n`tensorflow 2.3.0 requires scipy==1.4.1, but you'll have scipy 1.5.3 which is incompatible`\r\n\r\nGetting error:\r\nTraceback (most recent call last):\r\n  File \"face_recognition_pymodule_sklearn.py\", line 26, in <module>\r\n    from sklearn.preprocessing import LabelEncoder\r\n  File \"/usr/lib/python3.7/site-packages/sklearn/__init__.py\", line 134, in <module>\r\n    from .base import clone\r\n  File \"/usr/lib/python3.7/site-packages/sklearn/base.py\", line 11, in <module>\r\n    from scipy import sparse\r\n  File \"/usr/lib/python3.7/site-packages/scipy/__init__.py\", line 151, in <module>\r\n    from scipy._lib._ccallback import LowLevelCallable\r\n  File \"/usr/lib/python3.7/site-packages/scipy/_lib/_ccallback.py\", line 1, in <module>\r\n    from . import _ccallback_c\r\nImportError: cannot import name '_ccallback_c' from 'scipy._lib' (/usr/lib/python3.7/site-packages/scipy/_lib/__init__.py)\r\n"]}, {"number": 40882, "title": "Building C API under Windows 10 fails", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.2\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): Build Tools VC 19\r\n- CUDA/cuDNN version: 10.1, cudann 7.6\r\n- GPU model and memory: RTX 2080 Ti 11 Gb\r\n\r\n\r\n**Describe the problem**\r\nBuilding C API fails with this command:\r\n`bazel build --config=opt --config=cuda --incompatible_strict_action_env=false --define=no_tensorflow_py_deps=true //tensorflow:libtensorflow.so`\r\n\r\nError:\r\n```\r\nLINK : warning LNK4217: symbol 'TF_DeleteDimensionHandle' defined in 'libops.lo(ops.o)' is imported by 'libbitcast_op_lib.lo(bitcast.o)' in function '\"void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,unsigned __int64,unsigned __int64,struct TF_Status *)\" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@_K2PEAUTF_Status@@@Z)'\r\nERROR: D:/dev/tensorflow/tensorflow/BUILD:677:1: declared output 'tensorflow/libtensorflow.so.2' is a dangling symbolic link\r\nERROR: D:/dev/tensorflow/tensorflow/BUILD:677:1: not all outputs were created or valid\r\nTarget //tensorflow:libtensorflow.so failed to build\r\nINFO: Elapsed time: 8143.925s, Critical Path: 1171.29s\r\nINFO: 6722 processes: 6722 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["@InfiniteLife,\r\nIs this still an issue?\r\n\r\nCould you please update TensorFlow to the latest stable version v2.4.1 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40882\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40882\">No</a>\n"]}, {"number": 40881, "title": "Make ProgBarLogger, ModelCheckpoint and TensorBoard callbacks not use batch hooks when possible", "body": "This PR changes the `ProgBarLogger`, `ModelCheckpoint` and `TensorBoard` callbacks to not use batch hooks when possible.\r\n\r\n`ProgBarLogger` will now only require batch hooks if `verbose == 1` or the cardinality of the dataset cannot be determined in v1 execution. `ModelCheckpoint` will require batch hooks only if `save_freq != \"epoch\"` and `TensorBoard` only when profiling is enabled.\r\n\r\nThese changes may enable future optimizations where the entire epoch iteration in the training loop could be wrapped in a `tf.function` if no batch hooks need to be called:\r\nhttps://github.com/tensorflow/tensorflow/blob/9b13f1b47fc97cb8b167d7fe5f1dc2277c85698d/tensorflow/python/keras/engine/training.py#L1089-L1103\r\n\r\nThe 2.3.0-rc0 release notes mention that `experimental_steps_per_execution` can result in up to 3x preformance improvements on TPUs. Unfortunately in my opinions the usablility of `experimental_steps_per_execution` is not really optimal since exposes too many implementation details to the user. I think it would be a lot nicer if performance optimizations like this could be done automatically for common use cases where only epoch level logging is required so users only need to care about `experimental_steps_per_execution` when requiring more customizability.\r\nThis PR is intended as a first step to explore if we can wrap the entire epoch iterations into a `tf.function` if no batch hooks need to be called, I am happy to explore this in a future PR.\r\n\r\nThis fixed `b/150629188`", "comments": ["@omalleyt12 Do you mind taking a look at this?", "@omalleyt12 Thanks for merging.\r\n\r\nIt looks like wrapping an entire epoch in a `tf.function` is now possible for a few common cases where no batch level callbacks are needed.\r\n\r\nI was playing around with something like this:\r\n```diff\r\ndiff --git a/tensorflow/python/keras/engine/training.py b/tensorflow/python/keras/engine/training.py\r\nindex bbab6fc7f9..d1c53b6a9b 100644\r\n--- a/tensorflow/python/keras/engine/training.py\r\n+++ b/tensorflow/python/keras/engine/training.py\r\n@@ -1085,25 +1085,41 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\r\n       # happen after `callbacks.on_train_begin`.\r\n       data_handler._initial_epoch = (  # pylint: disable=protected-access\r\n           self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\r\n+\r\n+      # maybe also try this on the second epoch when more cases will have data_handler.inferred_steps set\r\n+      run_compiled = (not self.run_eagerly and\r\n+          not callbacks._should_call_train_batch_hooks and\r\n+          data_handler.inferred_steps is not None and\r\n+          data_handler.inferred_steps > 1)\r\n+\r\n+      if run_compiled:\r\n+        @def_function.function(experimental_relax_shapes=True)\r\n+        def epoch_train_function(iterator, steps):\r\n+          logs = train_function(iterator)\r\n+          for _ in math_ops.range(steps - 1):\r\n+            logs = train_function(iterator)\r\n+          return logs\r\n+\r\n       logs = None\r\n       for epoch, iterator in data_handler.enumerate_epochs():\r\n         self.reset_metrics()\r\n         callbacks.on_epoch_begin(epoch)\r\n-        with data_handler.catch_stop_iteration():\r\n-          for step in data_handler.steps():\r\n-            with trace.Trace(\r\n-                'train',\r\n-                epoch_num=epoch,\r\n-                step_num=step,\r\n-                batch_size=batch_size,\r\n-                _r=1):\r\n-              callbacks.on_train_batch_begin(step)\r\n-              tmp_logs = train_function(iterator)\r\n-              if data_handler.should_sync:\r\n-                context.async_wait()\r\n-              logs = tmp_logs  # No error, now safe to assign to logs.\r\n-              end_step = step + data_handler.step_increment\r\n-              callbacks.on_train_batch_end(end_step, logs)\r\n+        if run_compiled:\r\n+          logs = epoch_train_function(iterator, data_handler.inferred_steps)\r\n+        else:\r\n+          with data_handler.catch_stop_iteration():\r\n+            for step in data_handler.steps():\r\n+              with trace.Trace(\r\n+                  'train',\r\n+                  epoch_num=epoch,\r\n+                  step_num=step,\r\n+                  batch_size=batch_size,\r\n+                  _r=1):\r\n+                callbacks.on_train_batch_begin(step)\r\n+                tmp_logs = train_function(iterator)\r\n+                if data_handler.should_sync:\r\n+                  context.async_wait()\r\n+                logs = tmp_logs  # No error, now safe to assign to logs.\r\n+                end_step = step + data_handler.step_increment\r\n+                callbacks.on_train_batch_end(end_step, logs)\r\n\r\n         if logs is None:\r\n           raise ValueError('Expect x to be a non-empty array or dataset.')\r\n```\r\nBut I won't have time to flesh this our into a PR (this code sample will probably fail when used together with `experimental_steps_per_execution` and hasn't been properly benchmarked), but I thought I mention it here in case anyone wants to pick it up.", "@lgeiger Thanks! I really like this idea. It would be cool if we could default to the most performant option like this\r\n\r\nProbably the only concern right now is that the way we compile multiple batches into one `tf.function` right now actually ends up with a 2x larger TF Graph than doing each batch separately. This has the potential to cause some issues, I think especially for TPUs. The reason is we call one batch before we enter the for-loop, so that inside the for-loop we know what the structure of the outputs is. There's some `tf.function` and `tf.data` work going on right now that should make this unnecessary in the future though in which case we should do this", "@omalleyt12 Thanks for the fast answer, that makes a lot of sense.\r\n\r\n> The reason is we call one batch before we enter the for-loop, so that inside the for-loop we know what the structure of the outputs is.\r\n\r\nIf the structure of the output is the only limitation here, I guess one could run the first epoch loop eagerly and only compile the entire loop starting from the second epoch. This would have the additional benefit that `data_handler.inferred_steps` will be defined in more cases. Although I guess having proper support for it in `tf.function` would be nicer.\r\n\r\nI am thinking of something similar to this:\r\n```diff\r\n--- a/tensorflow/python/keras/engine/training.py\r\n+++ b/tensorflow/python/keras/engine/training.py\r\n@@ -1083,27 +1083,40 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\r\n       # Handle fault-tolerance for multi-worker.\r\n       # TODO(omalleyt): Fix the ordering issues that mean this has to\r\n       # happen after `callbacks.on_train_begin`.\r\n-      data_handler._initial_epoch = (  # pylint: disable=protected-access\r\n-          self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\r\n-      logs = None\r\n+      initial_epoch = self._maybe_load_initial_epoch_from_ckpt(initial_epoch)\r\n+      data_handler._initial_epoch = initial_epoch  # pylint: disable=protected-access\r\n+      logs, epoch_train_function = None, None\r\n       for epoch, iterator in data_handler.enumerate_epochs():\r\n         self.reset_metrics()\r\n         callbacks.on_epoch_begin(epoch)\r\n-        with data_handler.catch_stop_iteration():\r\n-          for step in data_handler.steps():\r\n-            with trace.Trace(\r\n-                'train',\r\n-                epoch_num=epoch,\r\n-                step_num=step,\r\n-                batch_size=batch_size,\r\n-                _r=1):\r\n-              callbacks.on_train_batch_begin(step)\r\n-              tmp_logs = train_function(iterator)\r\n-              if data_handler.should_sync:\r\n-                context.async_wait()\r\n-              logs = tmp_logs  # No error, now safe to assign to logs.\r\n-              end_step = step + data_handler.step_increment\r\n-              callbacks.on_train_batch_end(end_step, logs)\r\n+        if epoch_train_function or (not self.run_eagerly and\r\n+                                    not callbacks._should_call_train_batch_hooks\r\n+                                    and data_handler.inferred_steps is not None\r\n+                                    and epoch - initial_epoch > 0):\r\n+          if not epoch_train_function:\r\n+            @def_function.function(experimental_relax_shapes=True)\r\n+            def epoch_train_function(iterator, steps, logs):\r\n+              for _ in math_ops.range(steps):\r\n+                logs = train_function(iterator)\r\n+              return logs\r\n+          logs = epoch_train_function(iterator, data_handler.inferred_steps, logs)\r\n+        else:\r\n+          with data_handler.catch_stop_iteration():\r\n+            for step in data_handler.steps():\r\n+              with trace.Trace(\r\n+                  'train',\r\n+                  epoch_num=epoch,\r\n+                  step_num=step,\r\n+                  batch_size=batch_size,\r\n+                  _r=1):\r\n+                callbacks.on_train_batch_begin(step)\r\n+                tmp_logs = train_function(iterator)\r\n+                if data_handler.should_sync:\r\n+                  context.async_wait()\r\n+                logs = tmp_logs  # No error, now safe to assign to logs.\r\n+                end_step = step + data_handler.step_increment\r\n+                callbacks.on_train_batch_end(end_step, logs)\r\n\r\n         if logs is None:\r\n           raise ValueError('Expect x to be a non-empty array or dataset.')\r\n```", "@omalleyt12 I submitted a followup PR in #41742 to take advantage of these changes in `model.fit()`"]}, {"number": 40880, "title": "Eager execution is turned off with a custom train_step() function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nAccording to the TF documentation, from TF 2.0 onwards Eager Execution is enabled by default: https://www.tensorflow.org/guide/eager\r\n\r\nIn TF 2.2, train_step function was added so we can create custom training loops using Subclassing API (thanks for that!). An expected behaviour would be that eager execution is enabled by default in this train_step function (according to docs and a common sense). \r\n\r\nWhen train_step function is implemented Eager execution is disabled even if it was previously enabled. It is not possible to switch it back to eager execution (e.g. by forcing tf.compat.v1.enable_eager_execution in the train step function). I would also suspect that it might be happening for test_step and predict_step but I haven't checked that yet.\r\n\r\n**Describe the expected behavior**\r\ntrain_step, predict_step and test_step run in eager mode by default.\r\n\r\n**Standalone code to reproduce the issue**\r\n_Link to colab notebook:_\r\nhttps://colab.research.google.com/drive/1LZmKSlzYgOpX_pjnpB7P5EiRxm9_2CmN?usp=sharing\r\n\r\n_Copy-paste version below_\r\n``` Python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nprint(tf.__version__) # Has to be 2.2\r\n\r\n# Example modified from: https://keras.io/api/models/model/\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\r\n        self.dense2 = tf.keras.layers.Dense(1, activation=tf.nn.softmax)\r\n\r\n    def compile(self, optimizer, loss, metric):\r\n        \"\"\"\r\n        Overriden .compile() method to initialize\r\n        optimizer, loss, and metric for train_step function\r\n        \"\"\"\r\n        super().compile()\r\n        self.opt = optimizer\r\n        self.loss = loss\r\n        self.metric = metric  \r\n    \r\n    def train_step(self, data):\r\n        print(f\"Eager execution mode: {tf.executing_eagerly()}\")\r\n        X, y = data\r\n        \r\n        # Track gradients\r\n        with tf.GradientTape() as tape:\r\n             y_pred = self.call(X)\r\n             loss = self.loss(y, y_pred)\r\n        \r\n        # Compute and modify the weights\r\n        grads = tape.gradient(loss, self.trainable_weights)\r\n        self.opt.apply_gradients(zip(grads, self.trainable_weights))        \r\n\r\n        # Compute metric\r\n        metric = self.metric(y, y_pred)\r\n        return {\"loss\": loss, \"metric\": metric}\r\n    \r\n    def call(self, inputs):\r\n        x = self.dense1(inputs)\r\n        return self.dense2(x)\r\n\r\nif __name__ == \"__main__\":\r\n    # Should be True\r\n    print(f\"Eager execution mode: {tf.executing_eagerly()}\")\r\n\r\n    X = np.random.rand(10, 4)\r\n    y = np.random.randint(0, 2, (10, 1))\r\n    model = MyModel()\r\n    model.compile(tf.keras.optimizers.Adam(), \r\n                tf.keras.losses.BinaryCrossentropy(),\r\n                tf.keras.metrics.Accuracy())\r\n    # Should print True\r\n    model.fit(X, y)\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n![image](https://user-images.githubusercontent.com/33067446/85945306-403b6780-b93d-11ea-8b7f-21486eb9fe1d.png)\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/c63aadb726f15417b0399992ab80e985/40880.ipynb). Thanks!", "@KacperKubara By default, model.compile() runs in graph mode. This is to get better performance. If you want to run the training eagerly, then pass `run_eagerly=True` to the compile. As we are inheriting the functionality, compile is running in graph mode by default. \r\n\r\nI changed one line in your code from \r\n`super().compile()` to `super().compile(run_eagerly=True)`\r\n\r\nWith the above change, model prints \"Eager execution mode: True\". Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/0165e193fed78a5acb5cccb77b18387a/40880.ipynb)\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "Hey @jvishnuvardhan , I can confirm that it's now working as expected. Thanks for clarification! Issue can be closed."]}, {"number": 40879, "title": "Why models with different parameter size all allocate all the GPU memory", "body": "I use tensorflow-gpu 1.15 trains models , and for resnet models with different parameter size, they all allocate all the memory for a V100 GPU. However, when I train a larger transformer model, it only allocate a part of the memory.  When I set:\r\nfraction = round(0.5, 1)\r\n    # config = tf.ConfigProto()\r\n    # sess_config.gpu_options.per_process_gpu_memory_fraction = fraction\r\n    config = tf.ConfigProto()\r\n    gpu_option = tf.GPUOptions(allow_growth=True,per_process_gpu_memory_fraction=fraction)\r\n    config.allow_soft_placement=True\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\nthe gpu_options do not work effectively.  May the code cause this problem?\r\n![image](https://user-images.githubusercontent.com/34976119/85942146-1c3a4f00-b95a-11ea-86b5-a8a2d808db51.png)\r\n", "comments": ["@qinshuaihua \r\nPlease share complete code for us to replicate the issue faced or share a colab gist with error to analyse.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40878, "title": "Bug when serializing optimizer with `tf.keras.utils.serialize_keras_object` but works fine with `tf.keras.optimizers.serialize`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n**Describe the current behavior**\r\nI have provided two code snippets for serializing and deserializing Keras optimizer. One of them works and other doesn't.\r\n\r\n**Describe the expected behavior**\r\nBoth the code snippets should work.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n<a href=\"https://colab.research.google.com/github/SpikingNeuron/tfpy_warrior/blob/master/try_optimizer_serialization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\r\n\r\nFor your quick reference, I provide here the two code snippets\r\n\r\n### Snippet 1 - does not work\r\n\r\n```python\r\nimport tensorflow.keras as tk\r\nopt = tk.optimizers.Adam()\r\nopt_ser = tk.utils.serialize_keras_object(opt)\r\nopt_deser = tk.utils.deserialize_keras_object(opt_ser)\r\nprint(opt_deser)\r\n```\r\n\r\n### Snippet 2 - works\r\n\r\n```python\r\nimport tensorflow.keras as tk\r\nopt = tk.optimizers.Adam()\r\nopt_ser = tk.optimizers.serialize(opt)\r\nopt_deser = tk.optimizers.deserialize(opt_ser)\r\nprint(opt_deser)\r\n```\r\n", "comments": ["I have tried in colab with TF version 2.2, nightly and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/008110269c775bd28e1ce2515ddbdaee/untitled.ipynb).Thanks!\r\n", "@SpikingNeuron  Was able to replicate the issue using TF 2.5 but error message has been given clearly how it should be passed.\r\nCould you please check the [gist](https://colab.research.google.com/gist/saikumarchalla/f2ddc24fbd3df428845fa3cfa1649206/untitled85.ipynb) and confirm if we can go ahead and close the issue. Thanks!", "The error says `ValueError: Unknown object: Adam.`\r\nBut the Adam object is inbuilt.\r\nIf you think that should not be an issue and users can understand that they still need to use the `custom_objects` argument for builtins then please close the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40878\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40878\">No</a>\n"]}]