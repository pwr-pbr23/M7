[{"number": 757, "title": "Tensor indexing issue", "body": "How to make confusion matrix with tensorflow?\nis it possible or is there any work-around for the \nfollowing code\n\n```\nnb_classes = 4\nnb_samples = 10\na_ = np.random.randint(0, high=nb_classes, size=(nb_samples,))\nb_ = np.random.randint(0, high=nb_classes, size=(nb_samples,))\n\nconf_mat = tf.zeros([nb_classes, nb_classes], tf.int32)\n\nfor i in xrange(nb_samples):\n    #conf_mat[a[i]][b[i]] += 1\n    # above one gives error so I tried to use gather\n    # from https://github.com/tensorflow/tensorflow/issues/418\n    # but problem with it is that we can't set the values\n    # with gather\n    tf.gather(tf.gather(conf_mat, a[i]), b[i]) += 1\n\ninit = tf.initialize_all_variables()\nwith tf.Session as sess:\n    sess.run(init)\n    sess.run(conf_mat, feed_dict={a=a_, b=b_})\n```\n\nThe error I am getting is \n\n```\ntf.gather(tf.gather(conf_mat, a[i]), b[i]) += 1\nSyntaxError: can't assign to function call\n```\n", "comments": ["I answered a related question on StackOverflow (which could generalize to your setting):\n\nhttp://stackoverflow.com/questions/34685947/adjust-single-value-within-tensor-tensorflow/34686952#34686952\n\nCan you take a look and see if it helps?\n", "In this case, is there a reason to declare the confusion matrix as a\nTF tensor?\n\nOn Wednesday, January 13, 2016, Derek Murray notifications@github.com\nwrote:\n\n> I answered a related question on StackOverflow (which could generalize to\n> your setting):\n> \n> http://stackoverflow.com/questions/34685947/adjust-single-value-within-tensor-tensorflow/34686952#34686952\n> \n> Can you take a look and see if it helps?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/757#issuecomment-171477720\n> .\n", "(Probably a StackOverflow question)\n"]}, {"number": 756, "title": "Also checking if all_model_checkpoint_paths is an empty list", "body": "Fixes #755\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins, test this please.\n", "Looks like the mac build is flacky...\n", "Yeah, mac builds have been broken for a while -- but otherwise looks good, will merge in a sec.\n"]}, {"number": 755, "title": "Saver is broken", "body": "Upon calling `self.saver.save(self.sess, os.path.join(checkpoint_dir, model_name), global_step=step)`, I get the following stack trace. \n\n```\nTraceback (most recent call last):\n  File \"main.py\", line 83, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"main.py\", line 50, in main\n    dcgan.train(FLAGS)\n  File \"/home/panmari/DCGAN-tensorflow/model.py\", line 140, in train\n    self.save(config.checkpoint_dir, counter)\n  File \"/home/panmari/DCGAN-tensorflow/model.py\", line 245, in save\n    global_step=step)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 886, in save\n    self.last_checkpoints, latest_filename)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 485, in update_checkpoint_state\n    elif all_model_checkpoint_paths[-1] != model_checkpoint_path:\nIndexError: list index out of range\n```\n\nThe problem lies in this line:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L483\n\n`all_model_checkpoint_paths` passed is not none, but an empty list, leading to the wrong branch of this if statement.\n", "comments": ["Dupe of #751.\n", "@vrv is it? It could have the same cause, but the exception is raised in a completely different place.\n", "oh, sorry, I didn't look carefully enough.  do you think you know how to fix it?\n", "Well, my issue went away after changing https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L483 from\n\n```\nif all_model_checkpoint_paths is None:\n```\n\nto \n\n```\nif all_model_checkpoint_paths is None or len(all_model_checkpoint_paths) == 0:\n```\n\nBut it's more of a workaround than a proper fix. Still, should I make a PR?\n", "I would say that's a proper fix :).  The logic is just incorrect when you pass in an empty list, which seems like a valid thing to pass.  Feel free to send us a PR, or else let us know you don't want to, and we'll fix it.\n"]}, {"number": 754, "title": "Allow sessions to be started in cpu-only mode.", "body": "I would find it convenient to be able to start a session in cpu only mode (e. g. because you have another session running already that hogs all the VRAM). My workaround for now is to use\n\n```\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.01)\nwith tf.Session(config=tf.ConfigProto(log_device_placement=True, gpu_options=gpu_options)) as sess:\n```\n\nAnd then wrap `with tf.device('/cpu:0'):` around the model construction.\n", "comments": ["Setting CUDA_VISIBLE_DEVICES to the empty string will accomplish the desired result.\n", "Doesn't that need to be set before running python? Or is it enough to export it before launching a TF session?\n", "As long as you set it before creating any session it will work.\n", "(I have a potentially better but bigger change that should help with this, but no guarantee that I'll get it in in the next week).\n", "@vrv have you implemented your solution to this yet? Would be nice to have some sort of simple command to not call upon the gpu if you already have another session going on. \n", "In https://github.com/tensorflow/tensorflow/commit/8421f177ced940374e06a9daf9b4ee638e207f10 I made it so that if you never use the GPU device, you never allocate any memory for it.\n\nLonger term I want to make the memory allocator grow as needed, but my test change showed a 5% reduction in memory capacity due to the additional fragmentation created.\n\nWe're working on a bigger change to the memory allocation strategy in general, but hopefully the commit above will help.\n", "> We're working on a bigger change to the memory allocation strategy in general, but hopefully the commit above will help.\n\nThanks! Allowing RNN's to take less memory would be such a game changer. I know you have heard that from several individuals already, so don't want to be overbearing. \n", "Closing since this is probably addressed for the time being.\n", "CUDA_VISIBLE_DEVICES='' does not work anymore.", "```\r\n(tf)$ CUDA_VISIBLE_DEVICES='' python\r\n<snip>\r\n>>> import tensorflow as tf\r\n>>> s = tf.Session()\r\n<snip>\r\n2017-05-02 09:48:02.014954: E tensorflow/stream_executor/cuda/cuda_driver.cc:405] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\n2017-05-02 09:48:02.015045: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 370.28.0\r\n2017-05-02 09:48:02.015259: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  370.28  Thu Sep  1 19:45:04 PDT 2016\r\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04.3) \r\n\"\"\"\r\n2017-05-02 09:48:02.015285: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 370.28.0\r\n2017-05-02 09:48:02.015293: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 370.28.0\r\n```\r\n\r\nIn another process:\r\n```\r\n$ nvidia-smi\r\n\r\n<snip>\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0     96368    G   /usr/lib/xorg/Xorg                             451MiB |\r\n|    0     96949    G   cinnamon                                       114MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n@Steviey we need more evidence, reproduction steps, etc.", "I also seem to encounter this:\r\n\r\n```\r\n\u00b1 CUDA_VISIBLE_DEVICES=\"\" python grasp_dataset.py --grasp_download 102\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nUsing TensorFlow backend.\r\nALSA lib confmisc.c:768:(parse_card) cannot find card '0'\r\nALSA lib conf.c:4185:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\r\nALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\r\nALSA lib conf.c:4185:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\r\nALSA lib confmisc.c:1251:(snd_func_refer) error evaluating name\r\nALSA lib conf.c:4185:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\r\nALSA lib conf.c:4664:(snd_config_expand) Evaluate error: No such file or directory\r\nALSA lib pcm.c:2209:(snd_pcm_open_noupdate) Unknown PCM default\r\nALSA lib confmisc.c:768:(parse_card) cannot find card '0'\r\nALSA lib conf.c:4185:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\r\nALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\r\nALSA lib conf.c:4185:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\r\nALSA lib confmisc.c:1251:(snd_func_refer) error evaluating name\r\nALSA lib conf.c:4185:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\r\nALSA lib conf.c:4664:(snd_config_expand) Evaluate error: No such file or directory\r\nALSA lib pcm.c:2209:(snd_pcm_open_noupdate) Unknown PCM default\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nNVIDIA: no NVIDIA devices found\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_UNKNOWN\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:145] kernel driver does not appear to be running on this host (login-node03): /proc/driver/nvidia/version does not exist\r\n```", "I also get the same errors. Any progress or fix on this issue? ", "Improved example command:\r\n```\r\nexport CUDA_VISIBLE_DEVICES=\"\" && python my_script.py\r\n```"]}, {"number": 753, "title": "Delete the local copy of eigen that is shipped with tensorflow", "body": "TensorFlow now pulls the eigen code directly from the eigen upstream repository. There is no need to keep a local copy anymore.\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 752, "title": "Using a model (post training)", "body": "I've trained a simple model of my own in python, categorizing images into two categories (Imagenet apples, vs imagenet oranges). I'm looking to now evaluate arbitrary images using this model to get a prediction (apples v oranges) and I'm having trouble finding out the right way to do that for some reason.\n\nAt the moment I'm just using a random training image for simplicity.\n\n``` Python\n# Use the existing graph and grab a sample image from the valid dataset.\ntf_image = tf_valid_dataset[1001,:,:,:] \nresult = tf.Session(graph=graph).run(tf_image)\n```\n\nI was trying to look at some of the API docs and the C++ imagenet example as inspiration, but I think I'm off track. `result` ends up being tf_image it seems.\n\nI know that I want the result of a single forward pass, but having a hard time seeing the right way to get that.\n", "comments": ["`run` takes as first argument the ops to run. You want to set a feed_dict. \n\nThis is a question better suited for StackOverflow, where you'll get more in depth answers. We try to keep issues for bug reports.\n", "Fair enough, I'll add some of my findings here though for others looking at least:\n\nOne question I had quickly after training is how to save my work to a single \"baked\" model with the graph, weights, and biases in a single file.  I was assuming that I could do that with the existing graph easily, but it looks like you currently need to create an alternate graph to do that. See http://stackoverflow.com/questions/34343259/is-there-an-example-on-how-to-generate-protobuf-files-holding-trained-tensorflow/34343517#34343517\n"]}, {"number": 751, "title": "Saver seems broken: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for model.ckpt-0", "body": "Updated to the last commit of 1/11. Saver seems broken. Here is the stack. \n\nFile \"cnn_eval.py\", line 132, in evaluate\n    eval_once(saver, summary_writer, logits, top_k_op,labels, names, prob, summary_op)\n  File \"cnn_eval.py\", line 43, in eval_once\n    saver.restore(sess, ckpt.model_checkpoint_path)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 904, in restore\n    sess.run([self._restore_op_name], {self._filename_tensor_name: save_path})\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 388, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 464, in _do_run\n    e.code)\ntensorflow.python.framework.errors.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for model.ckpt-0\n         [[Node: save/restore_slice_2 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice_2/tensor_name, save/restore_slice_2/shape_and_slice)]]\nCaused by op u'save/restore_slice_2', defined at:\n  File \"cnn_eval.py\", line 147, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"cnn_eval.py\", line 143, in main\n    evaluate()\n  File \"cnn_eval.py\", line 121, in evaluate\n    saver = tf.train.Saver(variables_to_restore)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 723, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 432, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 191, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 106, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 192, in _restore_slice\n    preferred_shard, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 271, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 659, in apply_op\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1882, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1081, in __init__\n    self._traceback = _extract_stack()\n", "comments": ["Could you please try the following in order?\n1. Roll tensorflow/python/training/saver.py back to 1c579361cd1e088dd5e05a394b1561a73e3667ba and see if it works?\n2. Check the paths in {your-saver-dir}/checkpoint to make sure they are pointing to the right files. Also please attach a copy in this report.\n3. Attach your cnn_eval.py.\n\nThanks,\nSherry\n", "Sherry,\nI haven't tried everything you said, but I do find the cause is the model file path in checkpint file is incorrect. \nHere I attach my training script to and evaluation script , as you can see from cnn_train.py, even the FLAGS.train_dir is already defined, it's excluded for some reason\nFor a older version I use (till Jan 5), the checkpoint file look like:\n    model_checkpoint_path: \"./train/model.ckpt-XXX\"\n    all_model_checkpoint_paths: \"./train/model.ckpt-XXX\"\nWith Yesterday's version (till Jan 11), the checkpoint file look like:\n    model_checkpoint_path: \"model.ckpt-XXX\"\n    all_model_checkpoint_paths: \"model.ckpt-XXX\"\n[cnn_eval_train.zip](https://github.com/tensorflow/tensorflow/files/88055/cnn_eval_train.zip)\n", "The problem is caused by the fix for #571.\n\nI will contact MarkDaoust to see if he can provide a fix today. In the meantime, you can do one of the following:\n- Rollback to 1c579361cd1e088dd5e05a394b1561a73e3667ba (before the fix for #571)\n\nor\n- Add this line to your code that restores from checkpoint:\n\nif ckpt and ckpt.model_checkpoint_path:\n  model_checkpoint_path = ckpt.model_checkpoint_path\n  if not os.path.isabs(model_checkpoint_path):\n    model_checkpoint_path = os.path.relpath(model_checkpoint_path, save_dir)\n  # Replace all ckpt.model_checkpoint_path with model_checkpoint_path to make sure\n  # we are using the correct path.\n  saver.restore(sess, model_checkpoint_path)\n  global_step = model_checkpoint_path\n\nPlease let me know if it works for you.\n\nThanks,\nSherry\n", "Sorry about that.\n\nIt sounds like you've working on this already.\n\nYou've probably already figured out that maybe one starting point could be moving the [`path.join` in `saver.latest_checkpoint`](https://github.com/tensorflow/tensorflow/blob/eef2aaa3f7931cc3c0e7bbc6757386cfbcf5e3ef/tensorflow/python/training/saver.py#L923-L925) to the [end of `saver.get_checkpoint_state`](https://github.com/tensorflow/tensorflow/blob/eef2aaa3f7931cc3c0e7bbc6757386cfbcf5e3ef/tensorflow/python/training/saver.py#L550), and applying it instead to all the paths in `ckp` (unless I've missed something else). \nI'd also add asserts to the tests checking that all the file paths returned from `save.last_checkpoints` and  `tf.train.get_checkpoint_state` exist (which would have caught this bug).\n\nJust let me know if you want any of that in a pull request, or if there's anything else I can do.\n", "The issue is because of compatibility with versions of tensorflow on which the model is built.\r\nTo downgrade tensorflow to v.0.11.0 :  \r\n\r\n- Ubuntu/Linux 64-bit, GPU enabled, Python 2.7\r\n- Requires CUDA toolkit 7.5 and CuDNN v5.1\r\n\r\n\r\n```\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl\r\nsudo pip install --upgrade $TF_BINARY_URL\r\n```\r\n* cuDNN v5.1 (Jan 20, 2017), for CUDA 7.5\r\n* Download cudnn-7.5-linux-x64-v5.1.tgz from https://developer.nvidia.com/cudnn\r\n\r\n```\r\ntar xvzf cudnn-7.5-linux-x64-v5.1.tgz\r\nsudo cp cuda/include/cudnn.h /usr/local/cuda/include\r\nsudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\r\nsudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\r\n\r\n```\r\n"]}, {"number": 750, "title": "Adding new Ops, how to install it?", "body": "Okay I have added a new op as suggested in tutorial, however I can't find instruction to compile it. Will I have to use bazel to add it to the current TensorFlow framework (which already installed in my machine)? Do I have to re-build the whole framework or just the part that I have modified?\n", "comments": ["Reading the instructions more carefully I found the answers so I will close this.\n", "@keveman is also working on a better way to add new ops, so stay tuned.\n"]}, {"number": 749, "title": "Adding Python 3 support to Tensorboard", "body": "This PR implements Python 3 support into Tensorboard in a portable way, by importing packages from six when possible and converting strings into the correct encoding whenever necessary.\n", "comments": ["Can one of the admins verify this patch?\n", "1) have you verified that this works on python2, or just python3?\n2) If it works for both, can you squash the commits?\n", "I tested it on Python 2.7 (on Linux only) and it works. I could not generate a wheel to test it on OS X due to an unrelated compilation error.\n", "Commits are squashed now, thanks!\n", "Assigning to @danmane for final checking. Thanks!\n", "Sorry, by mistake I submitted the PR from my master instead of a branch and ended up adding other stuff on top of it. I'll rebase and squash now.\n", "@girving: want to take a quick look?  Seems fine to merge but just want your extra eye.\n", "Looks good, except for a couple of places where `compat.as_bytes` could be used.  I added two comments to that effect.\n", "@girving I amended the commit and used `compat.as_bytes` instead of `.encode('utf-8')`. Thanks for reviewing this PR!\n", "Looks good, assuming that's the right way to import `compat`.  @vrv: Did the tests automatically rerun when the PR was changed?\n", "Nope, you have to invoke it.  Like so:\n\n@tensorflow-jenkins: test this please.\n", "Merged\n", "there isnt a 6.1 or 6.0.1 release version of tensorflow with this feature.\n", "No, but backporting it is straightforward. You basically have to make the\nsame edits in the same functions. I did that in my local `tensorboard.py`\nand `tensorboard_handler.py`, and I could share them if there's any\ninterest.\n", "Yeah , i also finally ended up using the 2to3 with some encodes at places and it was working.\n"]}, {"number": 748, "title": "Fix typos in tensorboard and tools", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thank you, @vrv .\n"]}, {"number": 747, "title": "imagnet example classify_image.py results in `AttrValue missing value with expected type 'bool'`", "body": "Hi,\n\nafter having gotten the mnist example to run, I tried the imagenet/classify_image.py, but the script results an `AttrValue missing value with expected type 'bool'` error.\n\nI use a ec2 g2.2xlarge with GRID K520 and ubuntu 14.04. For the installation I used [erikbern's install-tensorflow.sh](https://gist.github.com/erikbern/78ba519b97b440e10640), but used the cuda 7.0 deb installer and installed Tensorflow master (22b6b23d9778fe7ef53e1597248558f21d1dda48) instead of 0.5.0.\n\nThis is the complete error message\n\n``` bash\npython classify_image.py\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties:\nname: GRID K520\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.797\npciBusID 0000:00:03.0\nTotal memory: 4.00GiB\nFree memory: 3.95GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:707] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:43] Allocating 3.66GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] GPU 0 memory begins at 0x7022a0000 extends to 0x7ec588000\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 4.00GiB\nE tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue missing value with expected type 'bool'\n         for attr 'align_corners'\n        ; NodeDef: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, align_corners=<Unknown AttrValue type>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ExpandDims/_3, ResizeBilinear/size/_5); Op<name=ResizeBilinear; signature=images:T, size:int32 -> resized_images:float; attr=T:type,allowed=[DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE]; attr=align_corners:bool,default=false>\nE tensorflow/core/common_runtime/executor.cc:265] Executor failed to create kernel. Invalid argument: AttrValue missing value with expected type 'bool'\n         for attr 'align_corners'\n        ; NodeDef: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, align_corners=<Unknown AttrValue type>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ExpandDims/_3, ResizeBilinear/size/_5); Op<name=ResizeBilinear; signature=images:T, size:int32 -> resized_images:float; attr=T:type,allowed=[DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE]; attr=align_corners:bool,default=false>\n         [[Node: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, align_corners=<Unknown AttrValue type>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ExpandDims/_3, ResizeBilinear/size/_5)]]\nTraceback (most recent call last):\n  File \"classify_image.py\", line 214, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"classify_image.py\", line 210, in main\n    run_inference_on_image(image)\n  File \"classify_image.py\", line 173, in run_inference_on_image\n    {'DecodeJpeg/contents:0': image_data})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 388, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 464, in _do_run\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: AttrValue missing value with expected type 'bool'\n         for attr 'align_corners'\n        ; NodeDef: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, align_corners=<Unknown AttrValue type>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ExpandDims/_3, ResizeBilinear/size/_5); Op<name=ResizeBilinear; signature=images:T, size:int32 -> resized_images:float; attr=T:type,allowed=[DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE]; attr=align_corners:bool,default=false>\n         [[Node: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, align_corners=<Unknown AttrValue type>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ExpandDims/_3, ResizeBilinear/size/_5)]]\nCaused by op u'ResizeBilinear', defined at:\n  File \"classify_image.py\", line 214, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"classify_image.py\", line 210, in main\n    run_inference_on_image(image)\n  File \"classify_image.py\", line 160, in run_inference_on_image\n    create_graph()\n  File \"classify_image.py\", line 143, in create_graph\n    _ = tf.import_graph_def(graph_def, name='')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 243, in import_graph_def\n    compute_shapes=False)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1882, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1081, in __init__\n    self._traceback = _extract_stack()\n```\n", "comments": ["I think I fixed this in 5853ad99dd3f8b443fdb97d83de35fe057db9e7f\n"]}, {"number": 746, "title": "Fix broken reference to TensorBase::Serialize", "body": "I spent a few hours trying to track down this reference. Hopefully future readers won't have to!\n\nIn case anyone's curious, `TensorBase` is part of [Eigen](https://bitbucket.org/eigen/eigen/src/ce5a455b34c0a0ac3545a1497cb4a16c38ed90e8/unsupported/Eigen/CXX11/src/Tensor/TensorBase.h?at=3.3-beta1&fileviewer=file-view-default). I believe the contents of the `tensor_content` field are just packed arrays of raw floats, doubles, or whatever the value type is.\n", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Can one of the admins verify this patch?\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 745, "title": "fix misspells.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Merged\n"]}, {"number": 744, "title": "`tf.zeros_like()` and `tf.ones_like()` do not preserve partial shape information", "body": "For example:\n\n``` python\n>>> p = tf.placeholder(tf.float32, [None, 4])\n>>> z = tf.zeros_like(p)\n>>> z.get_shape()\nTensorShape([Dimension(None), Dimension(None)])\n```\n\nThis happens because `ConstantValue(shape(p))` can't be evaluated completely. We should set the shape of the returned tensor.\n", "comments": []}, {"number": 743, "title": "Where is conv2d implemented?", "body": "I have been looking for the code of tf.nn.conv2d(...), so I looked in the directory tensorflow/python/ops/nn.py and the only things I found is \n\"depthwise_conv2d\"\nAND\n\"separable_conv2d\"\nCan someone point me to the location of conv2d?\n", "comments": ["Look for something like this\nREGISTER_KERNEL_BUILDER(Name(\"Conv2D\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\"),\n                        Conv2DOp<CPUDevice, float>);\n\nhence, search for \"Conv2DOp\" to see the implementation\n", "Found it, thank you very much.\n"]}, {"number": 742, "title": "Added some examples to the doc strings.", "body": "This is related to the issue #618.\n", "comments": ["Can one of the admins verify this patch?\n", "Done.\n", "Can you squash the commits, then I'll merge.\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "It looks that I messed it up. I guess I better close this one :-(\n", "no need to close the pull request. Just fetch the code again and do something like \n\n``` bash\ngit reset origin/master\ngit add  tensorflow/python/ops/math_ops.py\ngit commit -m \"Adds some examples to the doc strings. See issue #618\"\n```\n"]}, {"number": 741, "title": "Added some missing examples to the doc strings.", "body": "This is related to the issue #618.\n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "I will submit it again after signing CLA\n", "You can just comment on this thread once you've signed (or the people/email addresses authoring the commits have signed), no need to close and submit another one.\n"]}, {"number": 740, "title": "PC cold boots during MNIST example on GPU (GF980Ti); PSU problem; reduce load?", "body": "After getting Tensorflow to work at all, now I've got another issue: I can run through the 20.000 training steps (within about 1 second; holy sh... that thing's fast) of the Deep MNIST example (code below). But after the last step my PC does a cold reboot. It just turns off hard without shutdown and reboots after a few seconds.\n\nIt appears to be caused by an undersized PSU. Gaming on the GPU however works fine so far... \n\nSo, is there any way that I can reduce the load on the GPU during usage of Tensorflow? I'd be fine if the MNIST example took 2 seconds instead of 1 if only my PC wouldn't reboot. \n\nAt the moment unfortunately I cannot afford a new PSU, spent my whole available budget on the GF980Ti. \n\nAny ideas? \n", "comments": ["Oops, forgot the code. Here you go, the code I use to test my Tensorflow setup: http://pastebin.com/Wad9eRBE\n\nIt's not my own code. I'm not sure where I got it but at first glance it looked pretty much like the Deep MNIST example from the tutorials. Strange thing is: it converges to an accuracy of about 0.9, whereas the actual Deep MNIST example from the tutorials converges to about 0.99. \n\nAnyway... I was, again, able to kind of solve it myself by throttling the GPU maximum power using the command \n\n```\n~$ nvidia-smi -pl 220\n```\n\nwhich, if I got it right, limits the power consumption of the GPU to 220 watts. \n", "@HWiese1980 \nI have been encountering reboot problems like yours. However, reboot happens sometimes as early as importing tensorflow (loading *.so), and sometimes after running a few iterations of training. In this case do you this it's a PSU problem like yours? Since I have tested out loading Theano and the same problem occurs on loading Theano. \n", "@stmharry Yes, I'd still say it's a PSU problem. The evidence is pretty much the same. Definitely sounds like an undersized PSU. I bought a Corsair HX750i several weeks ago and since then never again experienced these sudden reboots. So I highly recommend upgrading your PSU to something around at least 700 watts. \n"]}, {"number": 739, "title": "Does TensorFlow support Fast R-CNN currenly?", "body": "I've been working on transplanting Fast R-CNN to a TensorFlow version. And I came across 2 problems as below:\n1. In [roi_pooling_layer.cpp](https://github.com/rbgirshick/caffe-fast-rcnn/blob/fast-rcnn/src/caffe/layers/roi_pooling_layer.cpp), the C++ implementation of RoI Pooling layer, a dynamic variable `num_rois` is used to control a loop to process each roi of each picture. Its value is stored in the original dataset and will only be valid during the graph is running. However, since the graph in TensorFlow has to be settled before it being run, I don't know what to put in the loop control as the iteration limit.\n2. After the roi_pooling_layer, there follows a InnerProduct layer (in Caffe), the shape of whose input depends on the output shape of the roi_pooling_layer, which ultimately depends on `num_rois`. So the shape of the corresponding `weight_variable` and `bias_variable` for the following operation `matmul`, which is going to act as the InnerProduct layer, cannot be settled before the graph is running.\n\nIn addition, in the RoI Pooling layer, there are some other loop control variables based on the former calculation, like `hstart`, `hend`, `wstart` and `wend`. They are even harder to get valid values before the graph is built, because the `num_rois` is part of the input after all.\n\nSo may I ask if TensorFlow supports Fast R-CNN (currently)? Or are there any possible specific tips to solve these problems?\n\nThank you!\n", "comments": ["This is probably a question meant for StackOverflow, Github issues are for bugs / feature requests.  You'll get more help from the community there.\n", " @ExonRen I was thinking to that too, did you get any further with fast R-CNN on tensorflow?\n", "I too am interested in fast rcnn implementation on tensorflow.\nIs there any progress so far?\n", "@Irtza @parhartanvir Sorry to reply so late. I'm a Chinese and it was the Spring Festival shortly after I finished my work on this. And now I've turned to my graduation project. But I still think I should provide some introduction to my progress here.\n\nIn fact I didn't get to the expected destination. **I only implemented the test part (i.e. the forward calculation part) of Fast-RCNN.** The whole graph has to be split into three phases: before the `roi_pooling`, after it and itself. \n- _Part one_ is just constructed in TensorFlow operations and executed by `Session.run()`.\n- And _part two_, i.e. `roi_pooling` itself is written in native Python. In other words, I implement a Python function playing the same role as the [roi_pooling_layer.cpp](https://github.com/rbgirshick/caffe-fast-rcnn/blob/fast-rcnn/src/caffe/layers/roi_pooling_layer.cpp), which leads to a definitely slow speed. \n- Additionally, _part three_, which is also a native Python loop (for each ROI) with TensorFlow `run()` inside, makes some contributions to the poor performance too. \n- Approximately it would take about 70s to process one image, where the first part less than 1s and third part about 6s.\n\nThen I spent some more time to change the Python `roi_pooling` function back in C, and use Cython to linked them together. Due to the efficiency of C, the time cost per image takes a great step to 7s or 8s. Actually, the main 60s of old _part two_ has become to 1/300, which is 0.2s, and faster than any other part now.\n\nI'm not sure if my reply is too long, but I hope it can be helpful to you. :D\n", "@ExonRen hi, thanks for info. do u have any plan to share the code sometimes soon :) ? thanks\n", "@ExonRen Any way to share your code or work together?\n", "@rollingstone @gongenhao I'm still busy working on my graduation paper, and I think I'll upload my code after finishing this. Maybe at the end of May or the beginning of June.\n", "@ExonRen Sounds great. \n", "@ExonRen @Irtza  I saw https://github.com/yuxng/tensorflow, he also implemented roi pooling layer in tensorflow. But I do not have time to test this. Maybe you will get more idea for Faster-RCNN in TF.\n", "@ck196 Ah yes he did a good job. He implemented the RoIPooling directly in C++ as a new operation, while I just worked out some Python codes to do a similar task, due to the narrow schedule. So I believe his implementation should be much more efficient and helpful.\n", "If you don't mind not using TF, then there are implementations (fast/faster) with chainer:\n\nhttps://github.com/apple2373/chainer-simple-fast-rnn\nhttps://github.com/mitmul/chainer-faster-rcnn\n", "I've also implemented the ROI pooling op for Tensorflow. I have both the forward and gradient op, with both a CPU and GPU (CUDA) kernel. There's also example Ipython notebooks demoing how they're used. \n\nThe full Fast-RCNN network is still a work-in-progress, but I've run a simplified version of it successfully on my own computer. I'm still working on packaging it up so other people can try it out too.\n\nCheck it all out here: [https://github.com/zplizzi/tensorflow-fast-rcnn](https://github.com/zplizzi/tensorflow-fast-rcnn).\n", "@ExonRen as to your original second point about undetermined tensor sizes: Tensorflow does support this. For example, setting the shape of the input ROI tensor to [None, 4] will allow an input of any number of ROIs (each a vector of length 4). Then, if you want to do reshapes later in the network, setting a specific dimension to -1 (you can do this for only one dimension, of course) will have that dimension auto-sized according to the number of elements in the tensor.\n\nFor an example, here's my (not quite completed) Fast-RCNN network, which uses both of these features. [https://github.com/zplizzi/cs80/blob/master/fast_rcnn_tensorflow-vgg-med-train.ipynb](https://github.com/zplizzi/cs80/blob/71cce3af841c95f639145e317bfa1a8296195110/fast_rcnn_tensorflow-vgg-med-train.ipynb)\n", "Can we hope to see these new features added to default tensorflow any time soon?\n", "Hopefully! Personally, I'd prefer to finish up my Fast-RCNN model and do some more testing to be extra sure everything works properly before moving towards a merge into master Tensorflow. Or, if someone else tested out some of the code and reported positive results, I'd be good with that also. \n", "@zplizzi Can I only use user_ops to compile with TF binary version?\n", "@ck196 I think that is supposed to be possible (check out the guide [here](https://www.tensorflow.org/versions/r0.10/how_tos/adding_an_op/index.html#with-tensorflow-binary-installation)), but I've not tried it myself. \n", "@zplizzi Thank you. \n", "@zplizzi I can run roi pooling with you implementation now. In your examples, I do not see the bounding box regression step. Do you have plan to write some instructions about it.\n", "@ck196 That's great! I've not yet implemented the bounding box regression part of the network - that's one of the things on my to-do list for this project. It should be pretty straightforward, though, if you want to try it yourself - just a couple fully connected layers on top of the ROI pooling layer if I remember correctly.\n", "@zplizzi thanks for sharing your code here. In your preliminary demo for fast-RCNN you used Selective Search for ROI extraction. I've been searching for an implementation of EdgeBoxes in python but couldn't find anything available without the requirement of a MATLAB license. I'm assuming that you also couldn't find an implementation. Or did you have another reason for taking Selective Search?\n", "@isabel-schwende I used Selective Search since, if I remember correctly, it's what was used in the original RCNN and Fast-RCNN papers. Not knowing much of anything about region proposal algorithms, I figured that was probably a good endorsement and haven't tried anything else. I'm not sure that the Python implementation is very good or fast, but I'd personally probably lean towards working on getting Faster-RCNN working vs trying to improve an external region proposal system.\n", "@zplizzi I have a question about rois processing. input_rois  are original (from xml) rois/ scale_factor(16), aren't they ?\nDo we need to resize rois? Because in network we resize image such as \"original image -> input image (224x224 - vgg) ...).\n", "If you guys don't mind, let's try to move discussion specifically about my implementation of Fast RCNN / ROI pooling off of this issue, to avoid bothering the nice TensorFlow folks. Feel free to open an issue on my repository, even if it's just to ask a question about something that doesn't quite make sense. \n\n@ck196, I'm going to copy your question to a new issue on my repo and I'll answer it there.\n"]}, {"number": 738, "title": "Speeds up the bidirectional RNN by providing the sequence length to e\u2026", "body": "\u2026ach RNN. Also fixes shape inference when using the sequence length.\n\nChange-Id: Ib2843c620bc4bd348aafb2d676b61fa35f1223af\n", "comments": ["Can one of the admins verify this patch?\n", "Thank you for the contribution!  What are your sequence length\ndistributions like and how much speedup to you get from this change?\n\nOn Sun, Jan 10, 2016 at 6:18 PM, Martin Wicke notifications@github.com\nwrote:\n\n> Assigned #738 https://github.com/tensorflow/tensorflow/pull/738 to\n> @ebrevdo https://github.com/ebrevdo.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/738#event-510665510.\n", "These are Penn Treebank sentences. With batches of 32 sentences, I'm seeing about 30% improvement in throughput during training.\n", "Great to hear!  I was afraid of pushing this exact change because I wasn't\nconfident the conditional flow control would improve performance\neverywhere.  Is this on CPU or GPU?\n\nOn Sun, Jan 10, 2016 at 8:30 PM, Kenton Lee notifications@github.com\nwrote:\n\n> These are Penn Treebank sentences. With batches of 32 sentences, I'm\n> seeing about 30% improvement in throughput during training.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/738#issuecomment-170428206\n> .\n", "LGTM.\n\nOn Sun, Jan 10, 2016 at 8:34 PM, Eugene Brevdo ebrevdo@gmail.com wrote:\n\n> Great to hear!  I was afraid of pushing this exact change because I wasn't\n> confident the conditional flow control would improve performance\n> everywhere.  Is this on CPU or GPU?\n> \n> On Sun, Jan 10, 2016 at 8:30 PM, Kenton Lee notifications@github.com\n> wrote:\n> \n> > These are Penn Treebank sentences. With batches of 32 sentences, I'm\n> > seeing about 30% improvement in throughput during training.\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/tensorflow/tensorflow/pull/738#issuecomment-170428206\n> > .\n", "@tensorflow-jenkins, test this please.\n", "I was reporting speedups on CPUs. On GPUs, the speedup is less significant (about 5%).\n", "Looks like a CPU test failed:\n\n```\nINFO: From Testing //tensorflow/python:rnn_test:\n==================== Test output for //tensorflow/python:rnn_test:\nF................\n======================================================================\nFAIL: testBidirectionalRNN (__main__.BidirectionalRNNTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/rnn_test.runfiles/tensorflow/python/kernel_tests/rnn_test.py\", line 579, in testBidirectionalRNN\n    self._testBidirectionalRNN(use_gpu=False)\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/rnn_test.runfiles/tensorflow/python/kernel_tests/rnn_test.py\", line 539, in _testBidirectionalRNN\n    self.assertEqual(out.get_shape().as_list(), [batch_size, 2 * num_units])\nAssertionError: Lists differ: [None, 6] != [2, 6]\n\nFirst differing element 0:\nNone\n2\n\n- [None, 6]\n+ [2, 6]\n\n----------------------------------------------------------------------\nRan 17 tests in 23.687s\n\nFAILED (failures=1)\n```\n\n@kentonl: can you take a look and fix?\n", "Actually, could you also add a version of the bidirectional RNN test when\nsequence_length is not passed in (thus all sequences are considered full\nlength)?\n\nOn Sun, Jan 10, 2016 at 9:05 PM, Vijay Vasudevan notifications@github.com\nwrote:\n\n> Looks like a CPU test failed:\n> \n> INFO: From Testing //tensorflow/python:rnn_test:\n> ==================== Test output for //tensorflow/python:rnn_test:\n> F................ FAIL: testBidirectionalRNN (_main_.BidirectionalRNNTest)\n> \n> Traceback (most recent call last):\n> File\n> \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/rnn_test.runfiles/tensorflow/python/kernel_tests/rnn_test.py\",\n> line 579, in testBidirectionalRNN\n> self._testBidirectionalRNN(use_gpu=False)\n> File\n> \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/rnn_test.runfiles/tensorflow/python/kernel_tests/rnn_test.py\",\n> line 539, in _testBidirectionalRNN\n> self.assertEqual(out.get_shape().as_list(), [batch_size, 2 \\* num_units])\n> AssertionError: Lists differ: [None, 6] != [2, 6]\n> \n> First differing element 0:\n> None\n> 2\n> - [None, 6]\n> - [2, 6]\n> \n> ---\n> \n> Ran 17 tests in 23.687s\n> \n> FAILED (failures=1)\n> \n> @kentonl https://github.com/kentonl: can you take a look and fix?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/738#issuecomment-170433484\n> .\n", "(btw you can infer the batch size by calling inputs.shape, checking if not\nNone, and getting the first dimension from the input)\n\nOn Sun, Jan 10, 2016 at 9:11 PM, Eugene Brevdo ebrevdo@gmail.com wrote:\n\n> Actually, could you also add a version of the bidirectional RNN test when\n> sequence_length is not passed in (thus all sequences are considered full\n> length)?\n> \n> On Sun, Jan 10, 2016 at 9:05 PM, Vijay Vasudevan <notifications@github.com\n> \n> > wrote:\n> > \n> > Looks like a CPU test failed:\n> > \n> > INFO: From Testing //tensorflow/python:rnn_test:\n> > ==================== Test output for //tensorflow/python:rnn_test:\n> > F................ FAIL: testBidirectionalRNN (_main_\n> > .BidirectionalRNNTest)\n> > \n> > Traceback (most recent call last):\n> > File\n> > \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/rnn_test.runfiles/tensorflow/python/kernel_tests/rnn_test.py\",\n> > line 579, in testBidirectionalRNN\n> > self._testBidirectionalRNN(use_gpu=False)\n> > File\n> > \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/rnn_test.runfiles/tensorflow/python/kernel_tests/rnn_test.py\",\n> > line 539, in _testBidirectionalRNN\n> > self.assertEqual(out.get_shape().as_list(), [batch_size, 2 \\* num_units])\n> > AssertionError: Lists differ: [None, 6] != [2, 6]\n> > \n> > First differing element 0:\n> > None\n> > 2\n> > - [None, 6]\n> > - [2, 6]\n> > \n> > ---\n> > \n> > Ran 17 tests in 23.687s\n> > \n> > FAILED (failures=1)\n> > \n> > @kentonl https://github.com/kentonl: can you take a look and fix?\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/tensorflow/tensorflow/pull/738#issuecomment-170433484\n> > .\n", "Thanks for the pointers. The bug fix and additional test are in the latest revision.\n", "Thanks, can you squash the commits?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "These lines can fail because get_shape() will sometimes return None (e.g.\nif using a Placeholder with no shape!)\n\noutput.set_shape([input_.get_shape()[0], self.output_size])\n\nOn Mon, Jan 11, 2016 at 4:48 PM, googlebot notifications@github.com wrote:\n\n> CLAs look good, thanks!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/738#issuecomment-170745134\n> .\n", "Sorry - not none, but a TensorShape with no dimensions.  the **getitem**\nwill fail.\n\nOn Mon, Jan 11, 2016 at 4:50 PM, Eugene Brevdo ebrevdo@gmail.com wrote:\n\n> These lines can fail because get_shape() will sometimes return None (e.g.\n> if using a Placeholder with no shape!)\n> \n> output.set_shape([input_.get_shape()[0], self.output_size])\n> \n> On Mon, Jan 11, 2016 at 4:48 PM, googlebot notifications@github.com\n> wrote:\n> \n> > CLAs look good, thanks!\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/tensorflow/tensorflow/pull/738#issuecomment-170745134\n> > .\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "It looks like TensorShape's `__getitem__` is ok to call with no dimensions (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/tensor_shape.py#L507). I added a test that uses a shapeless placeholder to be sure.\n\nNote that the extra logic here: https://github.com/kentonl/tensorflow/commit/04036d7e443da373cc151305ba81ffacafe85d13#diff-9d717423e6d3f4359151c45dfaa554b6R234 is required for shapeless placeholders to work correctly for bidirectional RNNs.\n", "A cleaner way would be to move the shape check to the top (before anything\nhappens) and do:\n\nfor input in inputs:\n  input.set_shape(input.shape.with_rank(2))\n\nBut it does indeed seem that this is not necessary.\n\nOn Mon, Jan 11, 2016 at 7:01 PM, Kenton Lee notifications@github.com\nwrote:\n\n> It looks like TensorShape's **getitem** is ok to call with no dimensions\n> (see\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/tensor_shape.py#L507).\n> I added a test that uses a shapeless placeholder to be sure.\n> \n> Note that the extra logic here: kentonl@04036d7\n> #diff-9d717423e6d3f4359151c45dfaa554b6R234\n> https://github.com/kentonl/tensorflow/commit/04036d7e443da373cc151305ba81ffacafe85d13#diff-9d717423e6d3f4359151c45dfaa554b6R234\n> is required for shapeless placeholders to work correctly for bidirectional\n> RNNs.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/738#issuecomment-170768968\n> .\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "I switched to `with_rank` for the shape checking. Looking cleaner now, thanks!\n\nThis is necessary because of this shape check in the `reverse_sequence` op: https://github.com/tensorflow/tensorflow/blob/d1b8333effdcb031e6e34a2835a2f1c877fdd79b/tensorflow/python/ops/array_ops.py#L1014. We can't promote the input to rank 2 there, because `tf.reverse_sequence` does not make assumptions about the exact rank of the input.\n", "It looks like setting the shape in the RNN is actually not the right fix. When the gradients are computed, the shape inference still fails due to the conservative logic in the `merge` op here:  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_ops.py#L1533. Would it be acceptable to relax it so that merging tensors with shapes, for example, `[None, 10]` and `[None, 10]` would produce a `[None, 10]` tensor?\n", "I'd be OK with this.\n\nOn Tue, Jan 12, 2016 at 10:18 PM, Kenton Lee notifications@github.com\nwrote:\n\n> It looks like setting the shape in the RNN is actually not the right fix.\n> When the gradients are computed, the shape inference still fails due to the\n> conservative logic in the merge op here:\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_ops.py#L1533.\n> Would it be acceptable to relax it so that merging tensors with shapes, for\n> example, [None, 10] and [None, 10] would produce a [None, 10] tensor?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/738#issuecomment-171181586\n> .\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "The latest version fixes the above issues with stronger shape inference for the merge op.\n", "@tensorflow-jenkins: test this please.\n", "The merge op shape inference is now more conservative and fixes problems mentioned before.\n", "@tensorflow-jenkins, test this please\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 737, "title": "Following cuda install directions can lead to wrong version", "body": "If you follow the link from the cuda install directions, and are trying to install under Ubuntu you will end up on this page:\n\nhttps://developer.nvidia.com/cuda-toolkit-70\n\nUnfortunately if you select the \"Network Install\" option and install the repository deb, you will end up installing cuda 7.5 not 7.0.\n\nI suggest mentioning this before users click through to nvidia's site in the docs:\n\nhttps://www.tensorflow.org/versions/master/get_started/os_setup.html#optional-install-cuda-gpus-on-linux\n", "comments": ["@martinwicke: Is this obsolete now that we support 7.5? \n", "Yes.\n\nOn Mon, Mar 7, 2016 at 5:12 PM Geoffrey Irving notifications@github.com\nwrote:\n\n> @martinwicke https://github.com/martinwicke: Is this obsolete now that\n> we support 7.5?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/737#issuecomment-193540473\n> .\n"]}, {"number": 736, "title": "Build tools and versions listed in Jenkins build log.", "body": "Since TensorFlow is built regularly and it success it reported on the GitHub page, it would be nice to see in the logs e.g. (http://ci.tensorflow.org/job/tensorflow-master/47/configure) the OS configuration, tools used and their version. This way if people want to know what tools and version of the tools currently work they would be available and automatically updated.\n\nExample:\nOS: Ubuntu 14.04 LTS\nMemory: 2G\nSwap space: 2G\nProcessor: i7\nFree hard disk space: 2G\nJavac: Oracle Javac 1.8.0_66\nPython: 2.7\ng++: 4.9\nBazel: 0.1.1\nSWIG: 2.0.11\nTensorFlow:  f9514a917265f0e98c8fb44abc51158685f72ca6\n", "comments": ["We'll look into modifying the build scripts to that information lands in the logs.\n", "Thanks. I may have more details as I find other tools/options that effect the success of the builds.\n", "Please add the Bazel --local_resouces configuration to the list as it is significant:\n --local_resouces: http://bazel.io/docs/bazel-user-manual.html#flag--local_resources\nResults of changing values: http://stackoverflow.com/a/34770965/1243762\n", "@caisq can you add some output to the scripts? I think they're all run through configured, so maybe that's the right place to put it.\n", "Please close the issue. See: https://github.com/tensorflow/tensorflow/pull/979.\n\nJenkins builds now shows descriptions that contain JSON objects with info about the build, including build container type, command, platform/OS, build tools and source version. For example: \nhttp://ci.tensorflow.org/job/tensorflow-master/171/\n", "I have removed the machine configuration from build descriptions. It was making too much noise. The description stays in the output log.\n\n@EricGT what is your use case to ask for this? Note you are able to replicate any of the linux builds trivially. The only thing you need is docker. See [ci_build/README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/README.md) These scripts can be used for great dev setup. I will update the documentation how to do it on windows and mac as well.\n\nAlso the most of the things you suggest (and we currently print) does not (should not) matter for the build. Swap, cpu, disk space, kernel, java version, and bunch of other things does not matter. On the other hand I'm not sure how long does it take people to figure out they need to use something like \"--jobs 1 --local_resources 512,1,1\" to build on 4GB ram. We should write a blog about those things.\n"]}, {"number": 735, "title": "Fix typos in tensorboard/components", "body": "", "comments": ["Can one of the admins verify this patch?\n", "LGTM. Thanks @dongjoon-hyun!\n", "Thank you, @dsmilkov ! :)\n", "@vrv You can go ahead and merge.\n", "Merged into master.\n", "Thank you, @vrv .\n"]}, {"number": 734, "title": "failed to enqueue convolution on stream: CUDNN_STATUS_BAD_PARAM", "body": "Hi guys,\n\nI'm just trying to run the Deep MNIST example on my GeForce GTX 980Ti (it worked on the CPU) but am getting the above mentioned error message during the training phase. Can anyone point me to the right direction what might be wrong?\n\nHere's what `ipython notebook` outputs on my Ubuntu 14.04 system with CUDA 7.0, cuDNN 6.5 and NVIDIA drivers version 352.39 (linux-x86_64): http://pastebin.com/f4y3c9CH\n\nThanks\n", "comments": ["Seems this was related to an incorrectly configured LD_LIBRARY_PATH variable. Now it works. Well, kind of... \n", "Hello, I have the same problem, can you please point out how you solved this issue?\n", "I rebooted my PC. Seems there were some env variables screwed up that have been reset after rebooting. \n\nI think, the important variables are LD_LIBRARY_PATH which should contain the path to your libcudaxyz.so files and PATH which should contain the path to the nvcc binary. See also there: http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux/#environment-setup\n\nI hope this helps...\n", "I have the same problem with version 0.7.0, \nand I can't solve it even after I followed your suggestion.\nTensorflow works fine except for CNN modules.\n\n==== Commands & Error Mesages ====\n~$ python /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/cifar10/cifar10_train.py\n\n...\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 11.27GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x1306d40000 extends to 0x15d8484a67\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:828] failed to enqueue convolution on stream: CUDNN_STATUS_BAD_PARAM\nAborted (core dumped)\n", "I have a similar error message, in my case I am able to train let say model A, and the only thing I change in model B are sizes of convolution filters and pooling windows. I am wondering if it can be caused because the matrix operations are too computionally expensive? The error message:\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Quadro K2000\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.954\npciBusID 0000:01:00.0\nTotal memory: 1.94GiB\nFree memory: 1.83GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K2000, pci bus id: 0000:01:00.0)\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1110] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_TIMEOUT\nE tensorflow/stream_executor/cuda/cuda_timer.cc:54] Internal: error destroying CUDA event in context 0x2f838f0: CUDA_ERROR_LAUNCH_TIMEOUT\nE tensorflow/stream_executor/cuda/cuda_timer.cc:59] Internal: error destroying CUDA event in context 0x2f838f0: CUDA_ERROR_LAUNCH_TIMEOUT\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:1251] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED\nAborted (core dumped)\n", "@joaanna have you solve your problem? I have the same error with you", "I have the same error if my input image size is too large for a convnet model. So it might be related to the out of memory issue. ", "@joaanna, @HWiese1980: Try reducing your batch size, image size,... anything that reduces needed memory. This will avoid running out of memory (including this error message). ", "@lauphedo  so what you mean is cudnn_status_mapping_error is related to memory too? I have been having this error for a couple of days even when I followed instructions from different webpages and still get the error. I am using a Nvidia Quadro K2000 2GB of memory. ", "@fastlater: Yes, in my experience it is related to the running-out-of-memory issue. It seems that weiliu620 made the same experience. \r\nWhat kind of images do you use? What's your batch size?", "@lauphedo  Well, in my case, I guess it is because problems with cuda installation. I was testing a code with the same image used in their demo. Then, I tried one of my old codes and the same error came out so it should be something else and not a memory issue. I hope I could solve this problem soon and report the solution."]}, {"number": 733, "title": "Build error under Python 3: error: void function 'InitIfNeeded' should not return a value", "body": "Fresh clone with up to date bazel and swig from homebrew.\n\n```\ntensorflow/python/lib/core/py_func.cc:47:5: error: void function 'InitIfNeeded' should not return a value [-Wreturn-type]\n    import_array();\n    ^~~~~~~~~~~~~~\nthird_party/py/numpy/numpy_include/numpy/__multiarray_api.h:1532:144: note: expanded from macro 'import_array'\n#define import_array() {if (_import_array() < 0) {PyErr_Print(); PyErr_SetString(PyExc_ImportError, \"numpy.core.multiarray failed to import\"); return NUMPY_IMPORT_ARRAY_RETVAL; } }\n```\n\nThe following seems to fix the build issue:\n\n``` diff\ndiff --git a/tensorflow/python/lib/core/py_func.cc b/tensorflow/python/lib/core/py_func.cc\nindex 5da997e..8f9221b 100644\n--- a/tensorflow/python/lib/core/py_func.cc\n+++ b/tensorflow/python/lib/core/py_func.cc\n@@ -39,7 +39,7 @@ PyObject* GetPyTrampoline() {\n }\n\n // Module initialization (mainly import numpy) if needed.\n-void InitIfNeeded() {\n+int InitIfNeeded() {\n   mutex_lock l(mu);\n   if (!initialized) {\n     PyGILState_STATE py_threadstate;\n```\n\nbut:\n- I am not sure this is the correct fix: should we check for `NUMPY_IMPORT_ARRAY_RETVAL` instead?\n- Why does this break now? Was it just a warning previously?\n", "comments": ["Note: I am using Python 3.5. It might be a problem related to the build support of Python 3 as `NUMPY_IMPORT_ARRAY_RETVAL` is `NULL` only under Python 3.\n", "Yeah, for this to work with python 2 and 3, you have to make another macro\nNUMPY_IMPORT_ARRAY_RETURN_TYPE, which is set to int or void depending on\nthe value of NUMPY_IMPORT_ARRAY_RETVAL. I'm wondering whether that exists\nalready someplace, it seems like an obvious thing to have.\n\nOn Sat, Jan 9, 2016 at 3:57 AM Olivier Grisel notifications@github.com\nwrote:\n\n> Note: I am using Python 3.5. It might be a problem related to the build\n> support of Python 3 as NUMPY_IMPORT_ARRAY_RETVAL is NULL only under\n> Python 3.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/733#issuecomment-170229301\n> .\n", "I was having the same trouble on Linux so I applied the fix proposed by @martinwicke, but the build breaks when compiling rule `//tensorflow/python:_pywrap_tensorflow.so`. Even with --verbose_failures it does not show where GCC found an issue:\n\n```\nERROR: /tensorflow/tensorflow/python/BUILD:860:1: C++ compilation of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: gcc failed: error executing command \n  (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-py3-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-py3-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-5651786d5e59 -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive/eigen-eigen-5651786d5e59 -isystem third_party/py/numpy/numpy_include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/py/numpy/numpy_include -isystem util/python/python_include -isystem bazel-out/local_linux-py3-opt/genfiles/util/python/python_include -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/gpus/cuda/include -Wno-self-assign -Wno-write-strings -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.d -fPIC -c bazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc -o bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: gcc failed: error executing command \n  (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-py3-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-py3-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-5651786d5e59 -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive/eigen-eigen-5651786d5e59 -isystem third_party/py/numpy/numpy_include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/py/numpy/numpy_include -isystem util/python/python_include -isystem bazel-out/local_linux-py3-opt/genfiles/util/python/python_include -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/gpus/cuda/include -Wno-self-assign -Wno-write-strings -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.d -fPIC -c bazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc -o bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n```\n\nIs this related to the same issue or should I open another issue?\n", "Separate issue I think. If you would submit the import_array fix as a PR\nthough that would be very much appreciated.\n\nOn Wed, Jan 13, 2016 at 7:48 AM Jo\u00e3o Felipe Santos notifications@github.com\nwrote:\n\n> I was having the same trouble on Linux so I applied the fix proposed by\n> @martinwicke https://github.com/martinwicke, but the build breaks when\n> compiling rule //tensorflow/python:_pywrap_tensorflow.so. Even with\n> --verbose_failures it does not show where GCC found an issue:\n> \n> ERROR: /tensorflow/tensorflow/python/BUILD:860:1: C++ compilation of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: gcc failed: error executing command\n>   (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/tensorflow && \\\n>   exec env - \\\n>     INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n>     PATH=/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n>   /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-py3-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-py3-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-5651786d5e59 -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive/eigen-eigen-5651786d5e59 -isystem third_par\n>  ty/py/numpy/numpy_include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/py/numpy/numpy_include -isystem util/python/python_include -isystem bazel-out/local_linux-py3-opt/genfiles/util/python/python_include -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/gpus/cuda/include -Wno-self-assign -Wno-write-strings -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.d -fPIC -c bazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc -o bazel-out/local_linux-py3-opt/bin/tensorf\n>  low/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: gcc failed: error executing command\n>   (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/tensorflow && \\\n>   exec env - \\\n>     INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n>     PATH=/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n>   /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-py3-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-py3-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-5651786d5e59 -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive/eigen-eigen-5651786d5e59 -isystem third_par\n>  ty/py/numpy/numpy_include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/py/numpy/numpy_include -isystem util/python/python_include -isystem bazel-out/local_linux-py3-opt/genfiles/util/python/python_include -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/gpus/cuda/include -Wno-self-assign -Wno-write-strings -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.d -fPIC -c bazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc -o bazel-out/local_linux-py3-opt/bin/tensorf\n>  low/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\n> \n> Is this related to the same issue or should I open another issue?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/733#issuecomment-171337706\n> .\n", "Done (#764), thanks! \n", "I'll close this one then. Make another issue for the other failure.\n", "I just wanted to say thank you.  This PR in conjunction with your corrections (https://goo.gl/ZtvBzY) helped me edit my friend's code (https://github.com/natelust/least_asymmetry/).  I thought it would be fun to show how well open collaborations like these work!\n", "Glad to hear that it helped! :)\n"]}, {"number": 732, "title": "Fix unmatched html tags", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@dsmilkov: let me know if this LGTY and I can merge\n", "LGTM\n", "Merged\n", "Thank you, @vrv .\n"]}, {"number": 731, "title": "Replace javascript function name `isNodeAuxilliary` with `isNodeAuxiliary`", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thank you, @dsmilkov .\n"]}, {"number": 730, "title": "Fix typos in strings and docs in CUDA module", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@dongjoon-hyun, Thank you for your contribution. Unfortunately, TensorFlow doesn't really own stream-executor. And this has to be fixed there first. I will pass on your changes to the stream-executor team to incorporate them upstream. Then TensorFlow will benefit from it automatically. \n", "Oh, I see. Thank you for informing me that, @zheng-xq .\n\nBy the way, could you give me some URLs for contributing stream-executor code? Github or googlesource? I hope to contribute there directly next time if it is possible.\n", "Unfortunately, the stream-executor team is still working on their own open-sourcing. That's why TensorFlow currently carries a copy. When they are ready for their own open-sourcing, we will drop our copy, and refer to the public one. But for now, you are doing the right thing. Although we cannot accept the pull request, we will pass them to the stream-executor to make the change upstream. \n", "I see. Now I understand the situation. Okay, no problem.\nThank you for taking the passing jobs. :)\n", "Closing for now, I assume this is on the roadmap, @zheng-xq and @leary-google ?\n"]}, {"number": 729, "title": "Parameter Server in python", "body": "Hello, I'm trying to implement an asynchronous parameter server, DistBelief style using TensorFlow. I found that minimize() is split into two functions, compute_gradients and apply_gradients, so my plan is to insert a network boundary between them. I have a question about how to evaluate all the gradients simultaneously and pull them out all at once. I understand that eval only evaluates the subgraph necessary, but it also only returns one tensor, not the chain of tensors required to compute that tensor.\n\nHow can I do this more efficiently? I took the Deep MNIST example as a starting point:\n\n```\nimport tensorflow as tf\nimport download_mnist\n\ndef weight_variable(shape, name):\n   initial = tf.truncated_normal(shape, stddev=0.1)\n   return tf.Variable(initial, name=name)\n\ndef bias_variable(shape, name):\n   initial = tf.constant(0.1, shape=shape)\n   return tf.Variable(initial, name=name)\n\ndef conv2d(x, W):\n   return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n   return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                         strides=[1, 2, 2, 1], padding='SAME')\n\nmnist = download_mnist.read_data_sets('MNIST_data', one_hot=True)\nsession = tf.InteractiveSession()\nx = tf.placeholder(\"float\", shape=[None, 784], name='x')\nx_image = tf.reshape(x, [-1,28,28,1], name='reshape')\ny_ = tf.placeholder(\"float\", shape=[None, 10], name='y_')\nW_conv1 = weight_variable([5, 5, 1, 32], 'W_conv1')\nb_conv1 = bias_variable([32], 'b_conv1')\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\nW_conv2 = weight_variable([5, 5, 32, 64], 'W_conv2')\nb_conv2 = bias_variable([64], 'b_conv2')\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\nW_fc1 = weight_variable([7 * 7 * 64, 1024], 'W_fc1')\nb_fc1 = bias_variable([1024], 'b_fc1')\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\nkeep_prob = tf.placeholder(\"float\", name='keep_prob')\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\nW_fc2 = weight_variable([1024, 10], 'W_fc2')\nb_fc2 = bias_variable([10], 'b_fc2')\ny_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\nvariables = [W_conv1, b_conv1, W_conv2, b_conv2, W_fc1, b_fc1, W_fc2, b_fc2]\nloss = -tf.reduce_sum(y_ * tf.log(y_conv))\noptimizer = tf.train.AdamOptimizer(1e-4)\ncorrect_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\ncompute_gradients = optimizer.compute_gradients(loss)\nsession.run(tf.initialize_all_variables())\n\nbatch = mnist.train.next_batch(50)\nfeed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5}\n\n\ngradients = []\nfor grad_var in compute_gradients:\n    grad = grad_var[0].eval(feed_dict=feed_dict)\n    var = grad_var[1]\n    gradients.append((grad, var))\n```\n\nI think this last for loop is actually recalculating the last gradient several times, whereas the first gradient is computed only once? How can I grab all the gradients without recomputing them?\n", "comments": ["This question is more appropriate to stackoverflow, github issues are for bugs / feature requests.  You'll likely get better community help there!\n"]}, {"number": 728, "title": "Adding basic CMake support", "body": "This PR adds very basic CMake support, to unblock people who can't use Bazel (eg. Windows users).\n\nThe CMake files are all located in the `cmake` subdirectory.  Usage:\n\n```\ncd cmake\nmkdir build & cd build\nmkdir release & cd release\ncmake -DCMAKE_BUILD_TYPE=Release ../..\n```\n\nThis builds the tutorial C++ training example (along with all dependencies).\n\nLimitations: no GPU, no python, no tests, no install targets, tested only on MacOSX, and needs a bit of code cleanup.  It's a first version to get the ball rolling.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I added my email to my profile, hope this helps the CLA check.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "I would love to have this in contrib/ can you move the files there? Then we can merge.\n", "Is this being actively worked on? If not, I'd be glad to give this a go. It would be very useful in getting tensorflow building on clusters with old versions of glibc and the like.\n", "Can one of the admins verify this patch?\n", "@rbharath Hi, sorry for the delay, I've been caught up in another project over the past few weeks, I was hoping to get back to this sooner. Your help is most welcome, thanks a lot! Please feel free to ping me if something is not clear in the code, and I'll try to come back to this within the next couple of weeks.\n", "@ageron: Glad to be useful! I'll give this a shot. I'm no cmake expert, but hopefully I can make enough progress to get somewhere useful.\n", "@ageron I checked out this PR and tried to get tensorflow to build on my machine using cmake (Ubuntu 12.04 with glic 2.15, gcc 4.9.1, cmake 3.5). I made it a ways, but the build crashes when trying to compile the re2 library:\n\n```\nlibbenchmark.a(benchmark.cc.o): In function `_Z8RunBenchPN7testing9BenchmarkEii.part.2':\nbenchmark.cc:(.text+0x4f): undefined reference to `clock_gettime'\nbenchmark.cc:(.text+0x149): undefined reference to `clock_gettime'\nbenchmark.cc:(.text+0x195): undefined reference to `clock_gettime'\nbenchmark.cc:(.text+0x377): undefined reference to `clock_gettime'\nlibbenchmark.a(benchmark.cc.o): In function `StopBenchmarkTiming()':\nbenchmark.cc:(.text+0x47a): undefined reference to `clock_gettime'\nlibbenchmark.a(benchmark.cc.o):benchmark.cc:(.text+0x4da): more undefined references to     `clock_gettime' follow\n```\n\nThe cmake code uses the command ExternalProject to internally download re2 and build it. The failure above is due the linking flag '-lrt' being missing (the rt library was merged into glibc from 2.17 onwards, but I have an old version of glibc locally). The strange thing here though is that I can clone re2 from github manually and get it to build outside of cmake, so there's some arcana about the cmake build that's confusing me. Do you have any thoughts?\n", "@rbharath That's really weird, I'm looking into it.  Also, I'm moving cmake to tensorflow/contrib/cmake, as requested, and catching up to all the changes that have happened in the last few weeks.  I'll try to finish by tomorrow.\n", "@rbharath I updated the code to move cmake/ to tensorflow/contrib/cmake/, and I updated to the latest upstream commit and fixed a couple issues.  The code builds and runs fine on my MacOSX machine (`tf_tutorials_example_trainer`), but I just tried to build it on an Ubuntu VM, and it failed with similar issues as you (linkage errors).  Adding the following code just after \"find_package(Threads)\" in tensorflow/contrib/cmake/CMakeLists.txt fixed the build problem:\n\n```\nIF (UNIX)\n    LINK_LIBRARIES(${CMAKE_THREAD_LIBS_INIT} ${CMAKE_DL_LIBS} -lm -lrt)\nENDIF (UNIX)\n```\n\nNow it builds all the way to the end, but I get a segfault when I run tf_tutorials_example_trainer on Ubuntu.  I'm investigating why, here's a quick dbg session:\n[debug_tf.txt](https://github.com/tensorflow/tensorflow/files/146645/debug_tf.txt)\n\nBefore building TensorFlow, I installed a few packages:\n\n```\nsudo apt-get install build-essential\nsudo apt-get install zlib1g-dev\n```\n\nI also built and installed Protobuf 3 from the source in google/protobuf (I followed the instructions in google/protobuf/cmake/README.md, without gmock)\nAs I installed it in a non-standard directory, I added the following options to the cmake commands when building TensorFlow:\n\n```\n-DPROTOBUF_PROTOC_EXECUTABLE=[...]/install/bin/protoc\n-DPROTOBUF_INCLUDE_DIR=[...]/install/include\n-DPROTOBUF_PROTOC_LIBRARY=[...]/install/lib/libprotoc.a\n-DPROTOBUF_LIBRARY=[...]/install/lib/libprotobuf.a\n```\n", "@martinwicke Hi Martin, I moved the `cmake/` directory to `tensorflow/contrib/cmake`, and caught up to the latest upstream commit.\n", "Thanks @ageron! I'm merging this for the greater good, even if it's still broken. \n", "Merged. Is the segfault the same problem you had in your first version?  \n", "@ageron, @martinwicke  I managed to get around the linking issues in re2 (The fix suggested above didn't quite work for me, but something close did. However, I'm now running into a make error when compiling `tf_core_kernels`\n\n```\n/home/rbharath/tensorflow/tensorflow/core/kernels/matrix_solve_ls_op.cc: In member function \u2018void tensorflow::MatrixSolveLsOp<Scalar, SupportsBatchOperationT>::ComputeMatrix(tensorflow::OpKernelContext*, const typename tensorflow::BinaryLinearAlgebraOp<Scalar, SupportsBatchOperationT>::ConstMatrixMap&, const typename tensorflow::BinaryLinearAlgebraOp<Scalar, SupportsBatchOperationT>::ConstMatrixMap&,   typename tensorflow::BinaryLinearAlgebraOp<Scalar, SupportsBatchOperationT>::MatrixMap*)\u2019:\n/home/rbharath/tensorflow/tensorflow/core/kernels/matrix_solve_ls_op.cc:111:41: error: \u2018Matrix\u2019 is not a class, namespace, or enumeration\n           (Scalar(l2_regularizer) * Matrix::Ones(cols, 1)).asDiagonal();\n                                     ^\n/home/rbharath/tensorflow/tensorflow/core/kernels/matrix_solve_ls_op.cc:131:41: error: \u2018Matrix\u2019 is not a class, namespace, or enumeration\n           (Scalar(l2_regularizer) * Matrix::Ones(rows, 1)).asDiagonal();\n                                     ^\nmake[2]: ***   [CMakeFiles/tf_core_kernels.dir/home/rbharath/tensorflow/tensorflow/core/kernels/matrix_solve_ls_op.cc.o] Error 1\nmake[1]: *** [CMakeFiles/tf_core_kernels.dir/all] Error 2\nmake: *** [all] Error 2\n```\n\nDo you have any thoughts on possible fixes?\n", "I think your version of gcc doesn't know how to parse\n\n\"using typename BinaryLinearAlgebraOp<Scalar, SupportsBatchOperationT>::Matrix;\" higher in the file -- my suggestion is to either upgrade your gcc and/or maybe downgrade our code to use less sophisticated C++ features :)\n", "@vrv Thanks for the pointer!\n\n@martinwicke I've made the changes required to get CMake to compile on ubuntu in PR #1309. Could you take a quick look?\n\n@ageron: Unfortunately, I'm also seeing a segfault in my compiled version. I'll do some digging on my end as well to see if I can figure something out.\n", "I'm closing this PR (it's merged). Feel free to continue using it as a message board, or move to #1309.\n"]}]