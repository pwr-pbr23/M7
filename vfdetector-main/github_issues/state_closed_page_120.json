[{"number": 51370, "title": "Fix build", "body": null, "comments": []}, {"number": 51369, "title": "Update version numbers for TensorFlow 2.4.3", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 4 -> 4\nPatch: 2 -> 3\n\nNo lingering old version strings \"2.4.2\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.4.2\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 51368, "title": "Update version numbers for TensorFlow 2.3.4", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 3 -> 3\nPatch: 3 -> 4\n\nNo lingering old version strings \"2.3.3\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.3.3\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 51367, "title": "Update version numbers for TensorFlow 2.5.1", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 5 -> 5\nPatch: 0 -> 1\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.5.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\nBinary file \ntensorflow/python/compiler/tensorrt/model_tests/sample_model/saved_model.pb \nmatches\ntensorflow/python/keras/__init__.py:33:2.5.0\nBinary file \ntensorflow/lite/python/testdata/control_flow_v1_saved_model/saved_model.pb \nmatches\ntensorflow/lite/micro/tools/ci_build/tflm_bazel/tensorflow.bzl:11:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:64:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:68:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:69:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:105:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:125:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:128:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:194:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:197:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:202:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:206:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:216:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:227:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:249:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:267:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:312:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:328:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:333:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:337:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:340:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:344:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:345:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:346:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:347:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:348:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:349:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:350:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:351:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:352:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:353:2.5.0\ntensorflow/lite/g3doc/guide/op_select_allowlist.md:782:2.5.0\ntensorflow/tools/pip_package/setup.py:104:2.5.0\ntensorflow/tools/pip_package/setup.py:107:2.5.0\ntensorflow/tools/pip_package/setup.py:119:2.5.0\ntensorflow/tools/pip_package/setup.py:121:2.5.0\ntensorflow/tools/pip_package/setup.py:123:2.5.0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.5.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\nBinary file \ntensorflow/python/compiler/tensorrt/model_tests/sample_model/saved_model.pb \nmatches\ntensorflow/python/keras/__init__.py:33:2.5.0\nBinary file \ntensorflow/lite/python/testdata/control_flow_v1_saved_model/saved_model.pb \nmatches\ntensorflow/lite/micro/tools/ci_build/tflm_bazel/tensorflow.bzl:11:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:64:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:68:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:69:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:105:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:125:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:128:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:194:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:197:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:202:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:206:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:216:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:227:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:249:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:267:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:312:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:328:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:333:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:337:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:340:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:344:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:345:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:346:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:347:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:348:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:349:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:350:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:351:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:352:2.5.0\ntensorflow/lite/tools/versioning/runtime_version.cc:353:2.5.0\ntensorflow/lite/g3doc/guide/op_select_allowlist.md:782:2.5.0\ntensorflow/tools/pip_package/setup.py:104:2.5.0\ntensorflow/tools/pip_package/setup.py:107:2.5.0\ntensorflow/tools/pip_package/setup.py:119:2.5.0\ntensorflow/tools/pip_package/setup.py:121:2.5.0\ntensorflow/tools/pip_package/setup.py:123:2.5.0\n```", "comments": []}, {"number": 51366, "title": "Update release notes for TensorFlow 2.5.1", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.5.1\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 51365, "title": "Update release notes for TensorFlow 2.4.3", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.4.3\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 51364, "title": "Update release notes for TensorFlow 2.3.4", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.3.4\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 51363, "title": "Update release notes for the new patch release.", "body": null, "comments": []}, {"number": 51362, "title": "Update release notes for the new patch release", "body": null, "comments": []}, {"number": 51361, "title": "Update release notes for the new patch release", "body": null, "comments": []}, {"number": 51360, "title": "Update release notes with the security updates.", "body": null, "comments": []}, {"number": 51359, "title": "Fix overflow/crash in tf.range when limits is large", "body": "This PR tries to address the issue raised in #46913 where\r\ntf.range (and implicitly tf.keras.layers.RepeatVector)\r\nwill overflow/crash when limits is large.\r\n\r\nThe reason of the overflow is that while calculating\r\nthe size within the kernel, the conditional statements\r\ncomes with `int64 = cond ? int64 : double` will implicitly\r\nconvert to double first and then cast back to int64, causing\r\nthe overflow and crash.\r\n\r\nThis PR fixes the issue by casting to int64 in both selections\r\nwithin the conditional statements first.\r\n\r\nThis PR fixes #46913.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @mihaimaruseac . The PR has been updated."]}, {"number": 51358, "title": "How to interpret the model analysis report of TensorFlow2", "body": "2021-08-07 22:21:14.783664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-08-07 22:21:14.783849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-08-07 22:21:14.783983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7077 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2021-08-07 22:21:14.790480: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: Graph size after: 216 nodes (198), 329 edges (310), time = 3.612ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0.07ms.\r\nHow should we understand these two times\uff08time = 3.612ms and time = 0.07ms\uff09?", "comments": ["The tensorflow version is 2.4.0 with gpu", "@Misoknisky ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.Thanks!", "> @ tilakrayal ,\r\n> \r\n\r\n###  Hellow, i use code  like this \r\n`import tensorflow as tf\r\nfrom tensorflow.python.keras.engine.base_layer import Layer\r\nfrom tensorflow.keras.layers import Conv2D\r\nfrom tensorflow.python.profiler.model_analyzer import profile\r\nfrom tensorflow.python.profiler.option_builder import ProfileOptionBuilder\r\nprint('TensorFlow:', tf.__version__)\r\n\r\n\r\nmodel = tf.keras.applications.ResNet50()\r\nforward_pass = tf.function(\r\n    model.call,\r\n    input_signature=[tf.TensorSpec(shape=(1,) + (1, 1, 3))])\r\n\r\ngraph_info = profile(forward_pass.get_concrete_function().graph,\r\n                        options=ProfileOptionBuilder.float_operation())\r\n\r\nprint('Flops: {:,}'.format(flops))     # 7\r\n`\r\nThe ### outpus:\r\n> 22:21:14.790480:** I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: \r\n  graph_to_optimize  \r\n\r\n> function_optimizer: Graph size after: 216 nodes (198), 329 edges (310), time = 3.612ms.  \r\n\r\n> function_optimizer: function_optimizer did nothing. time = 0.07ms.  \r\n\r\n\r\nBefore getting the FLOPS results, there aforementioned outputs are confusing, I don't know how to interpret the both time = 3.612ms and time = 0.07ms. \r\n", "@Misoknisky ,\r\nOn running the given code snippet, I am facing an error stating **NameError: name 'flops' is not defined**. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/6aea252d919ae748643b57f3491d995f/untitled46.ipynb).Please provide the complete code to reproduce the issue.Thanks!\r\n\r\n", "``@tilakrayal \r\nI'm sorry for providing the imcomplete code. Here is the correct like this:\r\n`import tensorflow as tf\r\nfrom tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\r\n\r\ndef get_flops(model):\r\n    concrete = tf.function(lambda inputs: model(inputs))\r\n    concrete_func = concrete.get_concrete_function(\r\n        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\r\n    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\r\n    with tf.Graph().as_default() as graph:\r\n        tf.graph_util.import_graph_def(graph_def, name='')\r\n        run_meta = tf.compat.v1.RunMetadata()\r\n        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\r\n        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\r\n        return flops.total_float_ops\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.Input(shape=(10, 1)))\r\nmodel.add(tf.keras.layers.Conv1D(2, 3, activation='relu'))\r\nmodel.summary()\r\nprint(\"The FLOPs is:{}\".format(get_flops(model)) ,flush=True )`", "@Misoknisky ,\r\n\r\nPlease take a look at this links for more information on how to interpret the model.[Link1](https://towardsdatascience.com/interpretability-in-machine-learning-70c30694a05f),[Link2](https://towardsdatascience.com/explainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608),It helps.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51358\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51358\">No</a>\n"]}, {"number": 51357, "title": "tf.get_logger().setLevel(\"ERROR\")  dose not work", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nwindows10 \r\ntensorflow 2.5.0\r\n\r\ni run the following code:\r\n`import tensorflow as tf\r\n\r\ntf.get_logger().setLevel(\"ERROR\") \r\n\r\ndef f():\r\n    a = tf.constant([[10,10],[11.,1.]])\r\n    x = tf.constant([[1.,0.],[0.,1.]])\r\n    b = tf.Variable(12.)\r\n    y = tf.matmul(a, x) + b\r\n    print(\"PRINT: \", y)\r\n    tf.print(\"TF-PRINT: \", y)\r\n    return y\r\nf()`\r\n\r\nit is supposed to show noly the logs of level error, but the logs of info level still show up as following:\r\n`2021-08-07 21:38:07.489566: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll\r\n2021-08-07 21:38:09.872614: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll\r\n2021-08-07 21:38:10.474791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: NVIDIA GeForce MX150 computeCapability: 6.1\r\ncoreClock: 1.0375GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2021-08-07 21:38:10.475740: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll\r\n2021-08-07 21:38:10.494343: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll\r\n2021-08-07 21:38:10.494809: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-08-07 21:38:10.503734: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll\r\n2021-08-07 21:38:10.505973: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll\r\n2021-08-07 21:38:10.510533: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusolver64_11.dll\r\n2021-08-07 21:38:10.517252: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll\r\n2021-08-07 21:38:10.520018: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll\r\n2021-08-07 21:38:10.521771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n2021-08-07 21:38:10.523332: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-08-07 21:38:10.524939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: NVIDIA GeForce MX150 computeCapability: 6.1\r\ncoreClock: 1.0375GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2021-08-07 21:38:10.525938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n2021-08-07 21:38:11.193205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-08-07 21:38:11.193561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \r\n2021-08-07 21:38:11.193780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N\r\n2021-08-07 21:38:11.194437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1332 MB memory) -> physical GPU (device: 0, name: \r\nNVIDIA GeForce MX150, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2021-08-07 21:38:11.273136: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll\r\n2021-08-07 21:38:11.820818: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll\r\nPRINT:  tf.Tensor(\r\n[[22. 22.]\r\n [23. 13.]], shape=(2, 2), dtype=float32)\r\nTF-PRINT:  [[22 22]\r\n [23 13]]`\r\n\r\n**Describe the expected behavior**\r\n\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@yuxiazff  Sorry for the late response ! In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51357\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51357\">No</a>\n", "> _Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template_\r\n> \r\n> **System information**\r\n> \r\n> * Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n> * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n> * TensorFlow installed from (source or binary):\r\n> * TensorFlow version (use command below):\r\n> * Python version:\r\n> * Bazel version (if compiling from source):\r\n> * GCC/Compiler version (if compiling from source):\r\n> * CUDA/cuDNN version:\r\n> * GPU model and memory:\r\n> \r\n> You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh) You can also obtain the TensorFlow version with:\r\n> \r\n> 1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n> 2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n> \r\n> **Describe the current behavior** windows10 tensorflow 2.5.0\r\n> \r\n> i run the following code: `import tensorflow as tf\r\n> \r\n> tf.get_logger().setLevel(\"ERROR\")\r\n> \r\n> def f(): a = tf.constant([[10,10],[11.,1.]]) x = tf.constant([[1.,0.],[0.,1.]]) b = tf.Variable(12.) y = tf.matmul(a, x) + b print(\"PRINT: \", y) tf.print(\"TF-PRINT: \", y) return y f()`\r\n> \r\n> it is supposed to show noly the logs of level error, but the logs of info level still show up as following: `2021-08-07 21:38:07.489566: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll 2021-08-07 21:38:09.872614: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll 2021-08-07 21:38:10.474791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: pciBusID: 0000:01:00.0 name: NVIDIA GeForce MX150 computeCapability: 6.1 coreClock: 1.0375GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s 2021-08-07 21:38:10.475740: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll 2021-08-07 21:38:10.494343: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll 2021-08-07 21:38:10.494809: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll 2021-08-07 21:38:10.503734: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll 2021-08-07 21:38:10.505973: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll 2021-08-07 21:38:10.510533: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusolver64_11.dll 2021-08-07 21:38:10.517252: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll 2021-08-07 21:38:10.520018: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll 2021-08-07 21:38:10.521771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0 2021-08-07 21:38:10.523332: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-08-07 21:38:10.524939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: pciBusID: 0000:01:00.0 name: NVIDIA GeForce MX150 computeCapability: 6.1 coreClock: 1.0375GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s 2021-08-07 21:38:10.525938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0 2021-08-07 21:38:11.193205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix: 2021-08-07 21:38:11.193561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264] 0 2021-08-07 21:38:11.193780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0: N 2021-08-07 21:38:11.194437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1332 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce MX150, pci bus id: 0000:01:00.0, compute capability: 6.1) 2021-08-07 21:38:11.273136: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll 2021-08-07 21:38:11.820818: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll PRINT: tf.Tensor( [[22. 22.] [23. 13.]], shape=(2, 2), dtype=float32) TF-PRINT: [[22 22] [23 13]]`\r\n> \r\n> **Describe the expected behavior**\r\n> \r\n> **[Contributing](https://www.tensorflow.org/community/contribute)**\r\n> \r\n> * Do you want to contribute a PR? (yes/no):\r\n> * Briefly describe your candidate solution(if contributing):\r\n> \r\n> **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n> \r\n> **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nHi , i understand your problem, you need to import logging module and change your line of code as shown below.\r\nimport logging\r\nthen   change tf.get_logger().setLevel(\"ERROR\") to tf.get_logger().setLevel(logging.ERROR)"]}, {"number": 51356, "title": "ERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: none)", "body": "System information: \r\n\r\nWindows 10\r\nPython 3.6.13\r\nPip-Version: 21.2.3.\r\nWould like to install tensorflow 1.15\r\n\r\nHey, \r\ni would like to install tensorflow 1.15. \r\nI am setting up a virtual environment for this and typing the following (The variable OUTPUT_FOLDER is defined above).\r\n\r\npip3 install virtualenv \r\npython3 -m virtualenv $OUTPUT_FOLDER/venv\r\nsource $OUTPUT_FOLDER/venv/bin/activate\r\npip3 install --pre \"tensorflow==1.15.*\"\r\n\r\nThe folders are created, but installing tensorflow gives the error message in the title: \r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.15.* (from versions: none)\r\nERROR: No matching distribution found for tensorflow==1.15.*\r\n \r\nCan somebody help me?\r\n\r\nThanks!\r\nHinnerk8", "comments": ["@Hinnerk8 We see that you are using older version of tensorflow (1.15) which is officially considered as end of life that is not actively supported, We recommend that you upgrade to latest stable version of TF( 2.5 )  and let us know if the issue still persists in newer versions .Thank you!", "Thank you for your answer!\r\nI would like to run the temporal fusion transformer of the google-research team, which requires tensorflow 1.15. \r\nhttps://github.com/google-research/google-research/tree/master/tft\r\nI am not sure if this is going to work with tensorflow 2.5. \r\nIs there any possibility to install tensorflow 1.15? On the webpage of tensorflow is still the install descripition: \r\nhttps://www.tensorflow.org/install/pip\r\nBut I am actually getting the error message above. Is there a way to fix it?", "@Hinnerk8 Sorry for the late response! Could you please refer to the similar issue in [link1](https://stackoverflow.com/questions/42317075/tensorflow-r1-0-could-not-a-find-a-version-that-satisfies-the-requirement-tens), [link2](https://github.com/tensorflow/tensorflow/issues/39130) and let us know if it helps ?Thanks!", "Hey, \r\nthank you again for your response. \r\nI have already read those threads before posting. But nothing of the hints in there was true for me. I have Python 3.6. which should be fine for tensorflow 1.15. And my pip command is on the right version, like i wrote above... Since i installed my python via cygwin again, it is the 64bit version for sure.\r\n I dont want to use anaconda, because in this case i would need to make bigger changes on the code above... \r\nDo you have any new ideas?", "@Hinnerk8 Could you please try with following commands . tested with 21.2.3 ,python 3.7.10 ,windows 10 in command prompt.\r\n```\r\npip3 install virtualenv\r\npython3 -m virtualenv OUTPUT_FOLDER/venv\r\nOUTPUT_FOLDER\\venv\\Scripts\\activate\r\npip3 install --pre \"tensorflow==1.15.*\"\r\n```\r\nPlease let us know if it helps ?\r\nWe would recommend you to kindly upgrade the TF version to latest stable version 2.6.0.  Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51356\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51356\">No</a>\n"]}, {"number": 51355, "title": "Move some methods into runtime_shape.h", "body": "Facilitates TFLM code size reductions.\r\n\r\nBUG=b/149862813", "comments": []}, {"number": 51353, "title": "Enable resource update aliasing in auto-clustering", "body": "\r\nWe see cases where enabling resource update aliasing greatly reduces memory footprints.\r\n\r\n", "comments": ["@cheshire could you help to take a look when you have a moment? Thanks!", "Just check if there is anything that needs to be taken care of for this PR? It is fine if it simply waits for merging. Thanks!", "For some reason it's failing TPU tests internally. Let me take a look.", "Is it possible at all to think about tests for this? We have tests measuring GPU memory consumption in `def_function_xla_jit_test`. WDYT about a test which runs in autoclustering configuration, runs a trivially clusterable resource var update, and then asserts that we have not consumed additional RAM?", "> BTW, is the internal TPU failure related to this PR?\r\n\r\nUnfortunately yes. We get different numerics. No idea why.\r\n\r\n>  Is the get_total_memory_usage('GPU') accurate enough and not interfered by BFCAllocator?\r\n\r\nCf. def_function_xla_jit_test. It is accurate, BFCAllocator is disabled for tests IIRC.", "Thanks @gbaned for launching the CI test again!\r\n\r\n@cheshire, could you help me to take a quick look at whether the previous numerical difference persists in this new CI results? If yes, I can try separating the `arg_num` change into another PR with necessary CHECKs.\r\n", "Seems this one did not create an internal version yet, probably due to flaky failures. Let's restart.\r\n\r\n> If yes, I can try separating the arg_num change into another PR with necessary CHECKs.\r\n\r\nIs it possible to separate it into another PR? I thought it was necessary prerequisite for this one? But we should definitely add more checks. Also let's add tests.", "@trentlo Can you please check @cheshire's comments and keep us posted ? Thanks!", "> @trentlo Can you please check @cheshire's comments and keep us posted ? Thanks!\r\n\r\nI'll see if I can separate the arg_num change into another PR and give it some tests. Creating test for it may not be easy. ", "So, I have separated the arg_num change into another PR and it's merged.\r\n\r\nNow, I'm trying to add a python test for this PR, but I don't know what is the best way to trigger XLA AUTO_JIT in the python test?\r\n\r\nI added a test as:\r\n```\r\n tf_xla_py_test(\r\n    name = \"def_function_auto_jit_test\",\r\n    srcs = [\"def_function_auto_jit_test.py\"],\r\n    use_xla_device = True,\r\n    deps = [\r\n        \":def_function\",\r\n        \"//tensorflow/compiler/tests:xla_test\",\r\n        \"//tensorflow/python:constant_op\",\r\n    ],\r\n)\r\n```\r\n\r\nHowever, the AUTO_JIT is not triggered:\r\n\r\n`2021-09-21 22:41:33.498184: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1678] Not compiling cluster with device /job:localhost/replica:0/task:0/device:GPU:0`\r\n\r\n@cheshire, could you give me some guidance about how to trigger XLA AUTO_JIT for  `tf_xla_py_test` (or even `cuda_py_test`)?\r\n\r\n\r\n", "I believe def_function_xla_jit_test has *_xla_gpu autoclustering variant, e.g. tested here: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/eager/def_function_xla_jit_test.py;l=44-68?q=file:def_function_xla_jit_test", "> I believe def_function_xla_jit_test has *_xla_gpu autoclustering variant, e.g. tested here: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/eager/def_function_xla_jit_test.py;l=44-68?q=file:def_function_xla_jit_test\r\n\r\nRight, I did check that one for reference.\r\n\r\n`test_util.is_xla_enabled()` is always `false` in that test though. I did not figure out how to enable it.\r\n ", "It's false if you call (e.g.) def_function_xla_jit_test_cpu, can you run\ndef_function_xla_jit_test_xla_gpu? Speaking of, nearly every TF test should\nhave an _xla_gpu variant which runs autoclustering.\n\nOn Tue, Sep 21, 2021 at 5:34 PM Trent Lo ***@***.***> wrote:\n\n> I believe def_function_xla_jit_test has *_xla_gpu autoclustering variant,\n> e.g. tested here:\n> https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/eager/def_function_xla_jit_test.py;l=44-68?q=file:def_function_xla_jit_test\n>\n> Right, I did check that one for reference.\n>\n> test_util.is_xla_enabled() is always false in that test though. I did not\n> figure out how to enable it.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/51353#issuecomment-924488092>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGHYEKI4YJV4YBJ4LBS3UDEP7XANCNFSM5BWPKQ4Q>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "Like this?\r\n\r\n```\r\nroot@sc-sdgx-420:/opt/tensorflow/tensorflow-source# CUDA_VISIBLE_DEVICES=3 bazel test --jobs=20 --test_output=all --cache_test_results=no -c opt --config=cuda --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --output_filter MATCH_NOTHING --javabase=@bazel_tools//tools/jdk:remote_jdk11 //tensorflow/python/eager:def_function_xla_jit_test_xla_gpu\r\n...\r\nERROR: no such target '//tensorflow/python/eager:def_function_xla_jit_test_xla_gpu': target 'def_function_xla_jit_test_xla_gpu' not declared in package 'tensorflow/python/eager' (did you mean 'def_function_xla_jit_test_gpu'?) defined by /opt/tensorflow/tensorflow-source/tensorflow/python/eager/BUILD\r\nINFO: Elapsed time: 1.968s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (2 packages loaded)\r\nFAILED: Build did NOT complete successfully (2 packages loaded)\r\n```\r\n", "> Like this?\r\n> \r\n> ```\r\n> root@sc-sdgx-420:/opt/tensorflow/tensorflow-source# CUDA_VISIBLE_DEVICES=3 bazel test --jobs=20 --test_output=all --cache_test_results=no -c opt --config=cuda --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --output_filter MATCH_NOTHING --javabase=@bazel_tools//tools/jdk:remote_jdk11 //tensorflow/python/eager:def_function_xla_jit_test_xla_gpu\r\n> ...\r\n> ERROR: no such target '//tensorflow/python/eager:def_function_xla_jit_test_xla_gpu': target 'def_function_xla_jit_test_xla_gpu' not declared in package 'tensorflow/python/eager' (did you mean 'def_function_xla_jit_test_gpu'?) defined by /opt/tensorflow/tensorflow-source/tensorflow/python/eager/BUILD\r\n> INFO: Elapsed time: 1.968s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully (2 packages loaded)\r\n> FAILED: Build did NOT complete successfully (2 packages loaded)\r\n> ```\r\n\r\nFor some reasons, I can only succeed with `def_function_xla_jit_test_gpu`, i.e., with the `_gpu` postfix. Do I miss some configurations?", "\r\nI add a test case here:\r\nhttps://github.com/tensorflow/tensorflow/pull/51353/files#diff-4eb0ebadc5b54abae1338833f7cf762f309c1386e897c3674361a76cb20e8266R28\r\n\r\nIt appears that I have to add `context.context().optimizer_jit = True` to enable the Autoclustering.\r\n", "\r\nAdd one more test to test the signature including compile-time constant, similar to the one tested in `tf.function(jit_compile=True)`.\r\n\r\nPlease take a look again. Thanks!", "\r\n\"Windows Bazel GPU\" failed with the following message. Will try fixing it.\r\n\r\n```\r\n==================== Test output for //py_test_dir/tensorflow/python/eager:def_function_auto_jit_test_gpu:\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\tools\\ci_build\\gpu_build\\parallel_gpu_execute: line 35: is_absolute: command not found\r\nRunning test T:/tmp/bigvaudl/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/py_test_dir/tensorflow/python/eager/def_function_auto_jit_test_gpu.exe --test_device=XLA_GPU --types=DT_HALF,DT_FLOAT,DT_DOUBLE,DT_UINT8,DT_QUINT8,DT_INT8,DT_QINT8,DT_INT32,DT_QINT32,DT_INT64,DT_BOOL,DT_COMPLEX64,DT_COMPLEX128,DT_BFLOAT16 on GPU 1\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_m80h9ijg\\runfiles\\org_tensorflow\\py_test_dir\\tensorflow\\python\\eager\\def_function_auto_jit_test.py\", line 20, in <module>\r\n    from tensorflow.compiler.tests import xla_test\r\nModuleNotFoundError: No module named 'tensorflow.compiler.tests'\r\n```", "If it's just Windows, and if there's a way to disable the test on Windows, it would be fine.", "> If it's just Windows, and if there's a way to disable the test on Windows, it would be fine.\r\n\r\nThanks for the tip. I copied and pasted the `tags` from `def_function_xla_jit_test`.", "> > If it's just Windows, and if there's a way to disable the test on Windows, it would be fine.\r\n> \r\n> Thanks for the tip. I copied and pasted the `tags` from `def_function_xla_jit_test`.\r\n\r\n@cheshire, gentle ping~", "Here are the internal errors, @trentlo can you please verify ?\r\n\r\nTraceback (most recent call last):\r\n  File \"/tensorflow/compiler/tests/adadelta_test.py\", line 114, in testBasic\r\n    rtol=1e-5)\r\n  File \"/tensorflow/python/framework/test_util.py\", line 1446, in decorated\r\n    return f(*args, **kwds)\r\n  File \"/tensorflow/python/framework/test_util.py\", line 3068, in assertAllCloseAccordingToType\r\n    self.assertAllClose(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/tensorflow/python/framework/test_util.py\", line 1446, in decorated\r\n    return f(*args, **kwds)\r\n  File \"/tensorflow/python/framework/test_util.py\", line 3019, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/tensorflow/python/framework/test_util.py\", line 2981, in _assertAllCloseRecursive\r\n    (path_str, path_str, msg)))\r\n  File \"/tensorflow/python/framework/test_util.py\", line 2913, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\n  File \"/py/numpy/testing/_private/utils.py\", line 1528, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/py/numpy/testing/_private/utils.py\", line 840, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.001, atol=0.001\r\nMismatched value: a is different from b. \r\nnot close where = (array([0, 1]),)\r\nnot close lhs = [0.005707 0.005707]\r\nnot close rhs = [0.003883 0.003883]\r\nnot close dif = [0.001823 0.001823]\r\nnot close tol = [0.001004 0.001004]\r\ndtype = float16, shape = (2,)\r\nMismatched elements: 2 / 2 (100%)\r\nMax absolute difference: 0.001823\r\nMax relative difference: 0.4695\r\n x: array([0.005707, 0.005707], dtype=float16)\r\n y: array([0.003883, 0.003883], dtype=float16)", "> 0.001823\r\n\r\nThanks for the info.\r\n\r\nThis looks like a reasonable loss due to finite numerical precision to me. abs_tol=0.001 is too restricted for fp16. Let me see if I can repro the behavior with the OSS build.\r\n", "@trentlo Any update on this PR? Please. Thanks!", "> @trentlo Any update on this PR? Please. Thanks!\r\n\r\nThanks for checking.\r\n\r\nI tested (>10 times) with `python adadelta_test.py --test_device XLA_GPU --types=DT_HALF` but I don't repro the issue somehow. May I know whether the test failed on GPU or TPU in your CI?\r\n", "@cheshire Can you please assist on above comments from @trentlo. Thanks!", "> but I don't repro the issue somehow. May I know whether the test failed on GPU or TPU in your CI?\r\n\r\nMaybe just bump up the tolerance and retry?", "> > but I don't repro the issue somehow. May I know whether the test failed on GPU or TPU in your CI?\r\n> \r\n> Maybe just bump up the tolerance and retry?\r\n\r\nI can't exclude the possibility that there is indeed a bug somewhere after I inspected the logs in your CI and the relevant test codes. So, it is risky to simply bump up the tolerance.\r\n\r\nI tend to believe that this particular test (adadelta_test.py) may be written in a way that causes strange interactions between XLA and TF with the PR. It is also possible that the test itself is not well written but I can't be sure. A data point is that the codes in this PR are already in our internal TF fork for quite a while, and it seems fine.\r\n\r\nLet me close this PR for now and reopen later if needed. Thanks for all the replies and comments.\r\n"]}, {"number": 51351, "title": "TypeError: Cannot convert a symbolic Keras input/output to a numpy array by using a custom layer", "body": "**System information**\r\n* OS Platform and Distribution: Linux Ubuntu 20.10\r\n* TensorFlow installation: pip package\r\n* Tensorflow version: 2.6.0rc1\r\n* Python version: 3.8\r\n\r\nI created a Keras custom layer which implements a custom causal Conv1D. To do it, I simply used the classes Conv and Conv1D (they are in tensorflow/python/keras/layers/convolutional.py) and modified Conv. I'll show you how I use this layer with a very silly example dataset, which is characterized by only two training samples (given an ordered sequence of numbers as a training sample, the model should predict its next sequence: for example, if I've [0, 1, 2] the output will be [3, 4, 5]). However I get the following error:\r\n\r\n   ```\r\n Traceback (most recent call last):\r\nFile \"model.py\", line 406, in <module>\r\nmodel = build_model(x_train, y_train)\r\nFile \"model.py\", line 373, in build_model\r\nr=Myconv1D(filters=1,kernel_size=3,padding='causal',dilation_rate=1,use_bias=False,name=\"MyConv\")(inp)\r\nFile \"/home/es/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1057, in __call__\r\noutputs = call_fn(inputs, *args, **kwargs)\r\nFile \"model.py\", line 181, in call\r\nz[0,4+i,0].assign(inputs[0,i,0])  \r\nFile \"/home/es/venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 1235, in assign\r\nreturn var._strided_slice_assign(\r\nFile \"/home/es/venv/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1392, in   \r\n_strided_slice_assign\r\nvalue=ops.convert_to_tensor(value, dtype=self.dtype),\r\nFile \"/home/es/venv/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\", line 163, in wrapped\r\nreturn func(*args, **kwargs)\r\nFile \"/home/es/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1566, in convert_to_tensor\r\nret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\nFile \"/home/es/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 346, in    \r\n _constant_tensor_conversion_function\r\nreturn constant(v, dtype=dtype, name=name)\r\nFile \"/home/es/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 271, in constant\r\nreturn _constant_impl(value, dtype, shape, name, verify_shape=False,\r\nFile \"/home/es/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 283, in    \r\n_constant_impl\r\nreturn _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\nFile \"/home/es/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 308, in   \r\n_constant_eager_impl\r\nt = convert_to_eager_tensor(value, ctx, dtype)\r\nFile \"/home/es/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 106, in    \r\nconvert_to_eager_tensor\r\nreturn ops.EagerTensor(value, ctx.device_name, dtype)\r\nFile \"/home/es/venv/lib/python3.8/site-packages/keras/engine/keras_tensor.py\", line 244, in __array__\r\nraise TypeError(\r\nTypeError: Cannot convert a symbolic Keras input/output to a numpy array. This error may indicate that you're trying to pass a     \r\nsymbolic value to a NumPy call, which is not supported. Or, you may be trying to pass Keras symbolic inputs/outputs to a TF    \r\nAPI that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the     \r\nFunctional Model.\r\n\r\n```\r\n\r\nI attached the example code used to train the model:\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/6946190/model.zip)\r\n", "comments": ["@sunw70  Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51351\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51351\">No</a>\n"]}, {"number": 51349, "title": "type dependent output", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 5.8.14-arch1-1\r\n- TensorFlow installed from (source or binary): via pip\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.6\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nOutput is type dependent\r\n**Describe the expected behavior**\r\nIt should not be type dependent\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? ?\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nprint(tf.range(21.70725420531782, 42.707254205317824,dtype=tf.float64))\r\nprint(tf.range(21.70725420531782, 42.707254205317824,dtype=tf.float32))\r\n```", "comments": ["@Saduf2019 I am able to reproduce the error in [**`TF v2.5`** ](https://colab.research.google.com/gist/kumariko/98e2b572f9ab95e93632c0fe7f709010/51349.ipynb ),[**`TF nightly`**](https://colab.research.google.com/gist/kumariko/b383fc69c00b004740a79b7025dc6f9f/51349.ipynb) and [**`TF v2.4`**](https://colab.research.google.com/gist/kumariko/f972bbd2364a95cf2a279b32d51bce19/untitled4.ipynb) Please find the gists for your reference here. Thanks!", "@superplay1 \r\nThis is expected behavior, there is minor difference in the output. ", "Yes, so my problem is the not the difference in the values but the size of the output vector. In the documentation it says that \r\n`tf.range(start, limit, delta=1, dtype=None, name='range')`\r\nthe output does not include limit, but for `dtype=float64` it does.\r\n", "@superplay1 \r\nCan you please share the document link you are referring to.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51349\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51349\">No</a>\n"]}, {"number": 51347, "title": "save_model and export categorical_column_with_vocabulary_file", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.4/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file?hl=zh_cn\r\n\r\n## Description of issue (what needs changing):\r\nwhen i used tf.feature_column.categorical_column_with_vocabulary_file(\"column\",\"/tmp/vocab.txt\") to build a keras model, training is ok but when use tf.save_model.save(my_keras_model, \"/target/\"), my vocabulary file did no export to \"/target/assets/\" path.  So if I load this model again and do predict, model still read the origin path(\"/tmp/vocab.txt\"), I want to know how to make sure the model does not depend on original files and can be moved to another path.\r\n\r\nI tried this doc but not works(https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/saved_model/Asset?hl=zh_cn)\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\ntf.__version__ = 2.4.0\r\n\r\n### Parameters defined\r\ncode1:\r\n```python\r\nsex = tf.keras.Input(shape=(1,), name=\"sex\", dtype=tf.string)\r\nx = tf.feature_column.categorical_with_vocabulary_file(\"sex\",\"./sex.txt\", num_oov_bucuckets=5)\r\nx = tf.keras.layers.DenseFeatures([tf.feature_column.embedding_column(x, 2)])\r\nout = x({\"sex\",sex})\r\nmodel = tf.keras.Model(inputs=sex, outputs=out)\r\ntf.save_model.save(model, \"./no_assets_model\")\r\n```\r\n\r\ncode2:\r\n```python\r\nsex = tf.keras.Input(shape=(1,), name=\"sex\", dtype=tf.string)\r\nx = tf.feature_column.categorical_with_vocabulary_file(\"sex\",\"./sex.txt\")\r\nx = tf.keras.layers.DenseFeatures([tf.feature_column.embedding_column(x, 2)])\r\nout = x({\"sex\",sex})\r\nmodel = tf.keras.Model(inputs=sex, outputs=out)\r\ntf.save_model.save(model, \"./has_assets_model\")\r\n```\r\nseems param 'num_oov_bucuckets' must not be 0, or asset file will not be exported\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\nNo error, No warning\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nyes, in https://www.tensorflow.org/tutorials/structured_data/feature_columns\r\n\r\n### Request visuals, if applicable\r\n\r\nNo\r\n\r\n### Submit a pull request?\r\n\r\nNo\r\n", "comments": ["@Linging ,\r\n\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51345, "title": "fix error message", "body": null, "comments": []}, {"number": 51343, "title": "Bump curl dependency to 7.77.0", "body": "Handles the following CVEs:\r\n\r\n* [CVE-2021-22901](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22901)\r\n* [CVE-2021-22898](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22898)\r\n* [CVE-2021-22876](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22876)\r\n* [CVE-2021-22897](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22897)\r\n\r\nPiperOrigin-RevId: 384576784\r\nChange-Id: Iaf4f499736039ea957efb0af596d1a46f3062797", "comments": []}, {"number": 51342, "title": "Bump curl dependency to 7.77.0", "body": "Handles the following CVEs:\r\n\r\n* [CVE-2021-22901](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22901)\r\n* [CVE-2021-22898](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22898)\r\n* [CVE-2021-22876](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22876)\r\n* [CVE-2021-22897](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22897)\r\n\r\nPiperOrigin-RevId: 384576784\r\nChange-Id: Iaf4f499736039ea957efb0af596d1a46f3062797", "comments": []}, {"number": 51341, "title": "Bump curl dependency to 7.77.0", "body": "Handles the following CVEs:\r\n\r\n* [CVE-2021-22901](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22901)\r\n* [CVE-2021-22898](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22898)\r\n* [CVE-2021-22876](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22876)\r\n* [CVE-2021-22897](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22897)\r\n\r\nPiperOrigin-RevId: 384576784\r\nChange-Id: Iaf4f499736039ea957efb0af596d1a46f3062797", "comments": []}, {"number": 51340, "title": "TF1.13.1 how to shiled tensorflow device mapping information?", "body": "i don't want to print the device mapping information in my console when i am training my model, i tried some methods, which only can shiled the tensorflow warning information. How should i do to shiled device mapping and the first warning?\r\n![image](https://user-images.githubusercontent.com/52489106/128446262-5779c7bc-450b-4eab-9dd4-efd770401d1b.png)\r\n![image](https://user-images.githubusercontent.com/52489106/128446278-17fe472b-93e4-4b8f-974c-783255eb3bad.png)\r\n\r\n\r\n", "comments": ["@MrCrazyCrab We see that you are using old version of tensorflow  1.13 which is officially considered as end of life hence not actively supported , We recommend that you upgrade to latest stable version of TF (2.5) and let us know if the issue still persists in newer versions Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51339, "title": "fix segfault on string tensors with mismatched dimensions", "body": "This is a cherrypick of #50508", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51339) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent"]}, {"number": 51338, "title": "Prevent heap OOB read in TFLite's `gather.cc`.", "body": "Passing negative indices is illegal but there was a missing check so that resulted in OOB accesses.\r\n\r\nPiperOrigin-RevId: 387231300\r\nChange-Id: I3111b54b2f232638d795be17efc46abe4ede6bf8", "comments": []}, {"number": 51337, "title": "Prevent heap OOB read in TFLite's `gather.cc`.", "body": "Passing negative indices is illegal but there was a missing check so that resulted in OOB accesses.\r\n\r\nPiperOrigin-RevId: 387231300\r\nChange-Id: I3111b54b2f232638d795be17efc46abe4ede6bf8", "comments": []}, {"number": 51336, "title": "Prevent heap OOB read in TFLite's `gather.cc`.", "body": "Passing negative indices is illegal but there was a missing check so that resulted in OOB accesses.\r\n\r\nPiperOrigin-RevId: 387231300\r\nChange-Id: I3111b54b2f232638d795be17efc46abe4ede6bf8", "comments": []}, {"number": 51335, "title": "Error while using fit_generator on TimeseriesGenerator", "body": "For the record, last May when using this code, it was fine when compiling, but this month, an error appears when running the program which has not changed at all.\r\n\r\nThe code I use\r\n```python\r\nmodel_forecast = tf.keras.models.Sequential([\r\n  tf.keras.layers.LSTM(32, activation='relu', return_sequences=True, input_shape=(look_back, 1)),\r\n  tf.keras.layers.GlobalMaxPooling1D(),\r\n  tf.keras.layers.Dropout(0.25),\r\n  tf.keras.layers.Dense(1)\r\n])\r\noptimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)\r\nmodel_forecast.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\r\nmodel_forecast.fit_generator(train_gen, epochs=10, verbose=1)\r\n```\r\n\r\nThe error I get\r\n```python\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-12-66f244cb1c54> in <module>()\r\n      1 optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)\r\n      2 model_forecast.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\r\n----> 3 model_forecast.fit_generator(train_gen, epochs=10, verbose=1)\r\n\r\n4 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1955         use_multiprocessing=use_multiprocessing,\r\n   1956         shuffle=shuffle,\r\n-> 1957         initial_epoch=initial_epoch)\r\n   1958 \r\n   1959   def evaluate_generator(self,\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1145           use_multiprocessing=use_multiprocessing,\r\n   1146           model=self,\r\n-> 1147           steps_per_execution=self._steps_per_execution)\r\n   1148 \r\n   1149       # Container that configures and calls `tf.keras.Callback`s.\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py in get_data_handler(*args, **kwargs)\r\n   1362   if getattr(kwargs[\"model\"], \"_cluster_coordinator\", None):\r\n   1363     return _ClusterCoordinatorDataHandler(*args, **kwargs)\r\n-> 1364   return DataHandler(*args, **kwargs)\r\n   1365 \r\n   1366 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\r\n   1150       self._steps_per_execution_value = steps_per_execution.numpy().item()\r\n   1151 \r\n-> 1152     adapter_cls = select_data_adapter(x, y)\r\n   1153     self._verify_data_adapter_compatibility(adapter_cls)\r\n   1154     self._adapter = adapter_cls(\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py in select_data_adapter(x, y)\r\n    992         \"Failed to find data adapter that can handle \"\r\n    993         \"input: {}, {}\".format(\r\n--> 994             _type_name(x), _type_name(y)))\r\n    995   elif len(adapter_cls) > 1:\r\n    996     raise RuntimeError(\r\n\r\nValueError: Failed to find data adapter that can handle input: <class 'keras.preprocessing.sequence.TimeseriesGenerator'>, <class 'NoneType'>\r\n```\r\n\r\nNeed help :)", "comments": ["@myarist \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "ah thank you @sushreebarsa for responding to my issue \ud83d\ude05\ud83d\ude4f\ud83c\udffb I will move it to keras repo"]}]