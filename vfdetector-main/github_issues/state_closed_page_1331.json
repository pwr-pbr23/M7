[{"number": 13169, "title": "nightly-devel-gpu Docker broken with ImportError: libcuda.so.1: cannot open shared object file", "body": "To reproduce:\r\nsudo docker run -it --name t2 tensorflow/tensorflow:nightly-devel-gpu\r\npython -c \"import tensorflow\"\r\n\r\nIt's looking for libcuda.so.1, but\u00a0I can't find that file in the image\r\nFurthermore, LD_LIBRARY_PATH is pointing to /usr/local/nvidia/lib64 but there's no such folder. There's /usr/local/cuda/lib64, but no libcuda.so.1 there either (should it be loading libcudart.so.8.0 instead?)\r\n\r\ncc @craigcitro ", "comments": ["/cc @caisq ", "Can this be closed ?", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing as the docker issue is resolved\r\n\r\n\r\n"]}, {"number": 13168, "title": "Branch 169299199", "body": "I needed a file from master asap, so I thought I'd push.", "comments": ["@av8ramit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @mrry and @drpngx to be potential reviewers.", "Jenkins, test this please.", "Looks like a flaky test failure of //tensorflow/core/distributed_runtime/rpc:grpc_session_test_gpu on mac:\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-mac/6424/consoleFull\r\n\r\nTesting again.\r\n\r\n@tensorflow-jenkins test this please", "It failed again. So this does look like a reproducible failure on mac now...\r\n\r\n@mrry for insight. I'll also investigate which CL might be the culprit later.", "@tensorflow-jenkins test this please"]}, {"number": 13167, "title": "*IGNORE*", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Empty problem!", "this is what happens when you type into your github browser window thinking your console has focus, and \"c\" triggers a new issue. SORRY!"]}, {"number": 13166, "title": "Fixing protobuf bazel workspace issue.", "body": "Removing the github mirror which has the wrong sha hash.", "comments": ["Jenkins, test this please."]}, {"number": 13165, "title": "Doc nicety: use * to indicate when arguments must be keywords", "body": "https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits shows the signature\r\n\r\n    softmax_cross_entropy_with_logits(\r\n        _sentinel=None,\r\n        labels=None,\r\n        logits=None,\r\n        dim=-1,\r\n        name=None\r\n    )\r\n\r\nIn Python 3, the syntax for `_sentinel=None` is just `*`, and the signature would be\r\n\r\n    softmax_cross_entropy_with_logits(\r\n        *,\r\n        labels,\r\n        logits,\r\n        dim=-1,\r\n        name=None\r\n    )\r\n\r\nwhere I've additionally removed the false defaults from labels and logits.  This can't be done to the code which has to be 2.7 compatible, but it could be done to the documentation.", "comments": ["I'm fine if this gets closed, but probably won't work on it personally."]}, {"number": 13164, "title": "BUG: No GPU kernel for tf.scatter_nd and tf.gather_nd with int32 or int64 tensors", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary (pip)\r\n- **TensorFlow version (use command below)**:\r\nv1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: \r\nPython 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\nn/a\r\n- **CUDA/cuDNN version**:\r\nCUDA-8.0 / cuDNN-5.1\r\n- **GPU model and memory**:\r\nNVidia GeForce GTX TITAN with 5.93GiB\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nval_num = 5\r\nval_dim = 2\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    indices = tf.reshape(tf.range(val_num, dtype=tf.int64), [-1, 1])\r\n    updates = tf.constant(np.tile(np.expand_dims(np.arange(val_num, dtype=np.int64), 1), [1, val_dim]))\r\n\r\n    res = tf.scatter_nd(indices, updates, [val_num, val_dim])\r\n    #res = tf.gather_nd(updates, indices)\r\n\r\n    sess = tf.Session()\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    print(sess.run(res))\r\n```\r\n\r\n### Describe the problem\r\n**tf.scatter_nd** and **tf.gather_nd** do not support **int32** or **int64** tensors on GPU.\r\n\r\n### Source code / logs\r\ntf.scatter_nd:\r\n```\r\n2017-09-19 22:33:55.447883: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-19 22:33:55.447965: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-19 22:33:55.649706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-09-19 22:33:55.650199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\r\nname: GeForce GTX TITAN\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.8755\r\npciBusID 0000:04:00.0\r\nTotal memory: 5.93GiB\r\nFree memory: 5.63GiB\r\n2017-09-19 22:33:55.650286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\r\n2017-09-19 22:33:55.650299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\r\n2017-09-19 22:33:55.650316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:04:00.0)\r\nTraceback (most recent call last):\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1297, in _run_fn\r\n    self._extend_graph()\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1358, in _extend_graph\r\n    self._session, graph_def.SerializeToString(), status)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'ScatterNd': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: ScatterNd = ScatterNd[T=DT_INT64, Tindices=DT_INT64, _device=\"/device:GPU:0\"](Reshape, Const, ScatterNd/shape)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 15, in <module>\r\n    sess.run(tf.global_variables_initializer())\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'ScatterNd': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: ScatterNd = ScatterNd[T=DT_INT64, Tindices=DT_INT64, _device=\"/device:GPU:0\"](Reshape, Const, ScatterNd/shape)]]\r\n\r\nCaused by op 'ScatterNd', defined at:\r\n  File \"bug.py\", line 11, in <module>\r\n    res = tf.scatter_nd(indices, updates, [val_num, val_dim])\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2961, in scatter_nd\r\n    shape=shape, name=name)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'ScatterNd': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: ScatterNd = ScatterNd[T=DT_INT64, Tindices=DT_INT64, _device=\"/device:GPU:0\"](Reshape, Const, ScatterNd/shape)]]\r\n\r\n```\r\n\r\ntf.gather_nd:\r\n```\r\n2017-09-19 22:34:26.298344: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-19 22:34:26.298436: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-19 22:34:26.504483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-09-19 22:34:26.504948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\r\nname: GeForce GTX TITAN\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.8755\r\npciBusID 0000:04:00.0\r\nTotal memory: 5.93GiB\r\nFree memory: 5.61GiB\r\n2017-09-19 22:34:26.505032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\r\n2017-09-19 22:34:26.505044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\r\n2017-09-19 22:34:26.505061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:04:00.0)\r\nTraceback (most recent call last):\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1297, in _run_fn\r\n    self._extend_graph()\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1358, in _extend_graph\r\n    self._session, graph_def.SerializeToString(), status)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'GatherNd': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: GatherNd = GatherNd[Tindices=DT_INT64, Tparams=DT_INT64, _device=\"/device:GPU:0\"](Const, Reshape)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 15, in <module>\r\n    sess.run(tf.global_variables_initializer())\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'GatherNd': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: GatherNd = GatherNd[Tindices=DT_INT64, Tparams=DT_INT64, _device=\"/device:GPU:0\"](Const, Reshape)]]\r\n\r\nCaused by op 'GatherNd', defined at:\r\n  File \"bug.py\", line 12, in <module>\r\n    res = tf.gather_nd(updates, indices)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1338, in gather_nd\r\n    name=name)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'GatherNd': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: GatherNd = GatherNd[Tindices=DT_INT64, Tparams=DT_INT64, _device=\"/device:GPU:0\"](Const, Reshape)]]\r\n\r\n```", "comments": ["I checked this bug with nightly build from September 19th (1.4.0.dev20170919-cp35-cp35m-linux_x86_64.whl). This is still not implemented. ", "@dantkz it's only implemented with [floating point types](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/gather_nd_op.cc#L208) (half, float, and double). We need to add `int32` and `int64`.\r\n\r\nIt should be relatively easy, just call that macro for those types. Would you like to submit a PR?", "#12943 seems a good example for reference.\r\n\r\nBy the way, since all types are supported on CPU, while only float is registered for GPU:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/3537e663230ba2d694f6daebc2288f180c6db4ea/tensorflow/core/kernels/gather_nd_op.cc#L170\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/3537e663230ba2d694f6daebc2288f180c6db4ea/tensorflow/core/kernels/gather_nd_op.cc#L208\r\n\r\nnot sure whether it is on purpose (say, for performance) ?", "I say we try to register it and see what happens.", "@drpngx \r\nI've registered those types and updated tests: https://github.com/tensorflow/tensorflow/pull/13382\r\n\r\ntf.gather and tf.gather_nd now support int32 and int64 ref tensors when running on GPU. \r\ntf.scatter_nd now supports int32 ref tensors when running on GPU. int64 is not supported as some CudaAtomic operations are not supported.\r\n", "Thanks!", "@drpngx thank you! Still waiting for a code review, though. ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Hi,\r\n@rmlarsen  \r\nit seems the last fix is missing int32 for gather op : \r\nhttps://github.com/tensorflow/tensorflow/commit/83116bafebb500fa963809599b7f2583367c92d6#diff-0a1b05e2252bef657373e11f17bfa5d3 (or at least it isn't present in pip tensorfluw_gpu 1.9 branch)", "I'm afraid the it's on purpose for efficient.", "@facaiy : Are you sure ? It's quite strange, if so it's error prone, currently I have used two different ugly ways to hack_it to get it to work on GPU.\r\nI can either \r\n`tf.squeeze( tf.gather_nd( tf.expand_dims(tab,1),tf.expand_dims(ind,1) )`\r\nor\r\n`tf.cast( tf.gather( tf.cast(tab,tf.int64 ), ind ), tf.int32 )`\r\nIn either way they are both way more efficient, than a copy to cpu, the gather on cpu followed by a copy back to gpu, which happens if you let tensorflow the liberty of choosing where to place ops.", "Is there a plan to support for tflite an OpenCL kernel for Gather(V2)?"]}, {"number": 13163, "title": "Resolved: No GPU kernel for tf.tile with int32 or int64 tensors", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary (pip)\r\n- **TensorFlow version (use command below)**:\r\nv1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: \r\nPython 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\nn/a\r\n- **CUDA/cuDNN version**:\r\nCUDA-8.0 / cuDNN-5.1\r\n- **GPU model and memory**:\r\nNVidia GeForce GTX TITAN with 5.93GiB\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nkey_num = 5\r\nkey_dim = 2\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    keys = tf.tile(tf.expand_dims(tf.range(key_num, dtype=tf.int32), 1), [1, key_dim])\r\n\r\n    sess = tf.Session()\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    print(sess.run(keys))\r\n```\r\n\r\n### Describe the problem\r\ntf.tile doesn't support **tf.int32** or **tf.int64** tensors on GPU.\r\n\r\n### Source code / logs\r\n```\r\n2017-09-19 22:23:42.175225: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-19 22:23:42.175316: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-19 22:23:42.380410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-09-19 22:23:42.380904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\r\nname: GeForce GTX TITAN\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.8755\r\npciBusID 0000:04:00.0\r\nTotal memory: 5.93GiB\r\nFree memory: 5.59GiB\r\n2017-09-19 22:23:42.380985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\r\n2017-09-19 22:23:42.380997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\r\n2017-09-19 22:23:42.381014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:04:00.0)\r\nTraceback (most recent call last):\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1297, in _run_fn\r\n    self._extend_graph()\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1358, in _extend_graph\r\n    self._session, graph_def.SerializeToString(), status)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Tile': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: Tile = Tile[T=DT_INT32, Tmultiples=DT_INT32, _device=\"/device:GPU:0\"](ExpandDims, Tile/multiples)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 11, in <module>\r\n    sess.run(tf.global_variables_initializer())\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Tile': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: Tile = Tile[T=DT_INT32, Tmultiples=DT_INT32, _device=\"/device:GPU:0\"](ExpandDims, Tile/multiples)]]\r\n\r\nCaused by op 'Tile', defined at:\r\n  File \"bug.py\", line 8, in <module>\r\n    keys = tf.tile(tf.expand_dims(tf.range(key_num, dtype=tf.int32), 1), [1, key_dim])\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3847, in tile\r\n    name=name)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Tile': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: Tile = Tile[T=DT_INT32, Tmultiples=DT_INT32, _device=\"/device:GPU:0\"](ExpandDims, Tile/multiples)]]\r\n```", "comments": ["I tried the most recent HEAD and it seems to be fine?\r\n```python\r\nubuntu@ubuntu~$ python\r\nPython 2.7.12 (default, Nov 19 2016, 06:48:10) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> import numpy as np\r\n>>> \r\n>>> key_num = 5\r\n>>> key_dim = 2\r\n>>> \r\n>>> with tf.device(\"/gpu:0\"):\r\n...     keys = tf.tile(tf.expand_dims(tf.range(key_num, dtype=tf.int32), 1), [1, key_dim])\r\n...     sess = tf.Session()\r\n...     sess.run(tf.global_variables_initializer())\r\n...     print(sess.run(keys))\r\n... \r\n2017-09-19 23:17:01.817318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-09-19 23:17:01.817531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 11.17GiB freeMemory: 478.38MiB\r\n2017-09-19 23:17:01.817557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n[[0 0]\r\n [1 1]\r\n [2 2]\r\n [3 3]\r\n [4 4]]\r\n>>> \r\n```", "Yes, I tried with earlier nightly build and it wasn't working. Updated the nightly build, and it works now. Thanks!\r\n\r\nThis issue is resolved. "]}, {"number": 13162, "title": "Fail to build android example", "body": "Hi. I have the same problem - https://github.com/tensorflow/tensorflow/issues/11474\r\n\r\n**System information**\r\nbazel version:\r\nBuild label: 0.5.4-homebrew\r\n\r\njava version: \"1.8.0_144\"\r\n\r\nandroid/sdk/platforms/android-26\r\nandroid/sdk/system-images/android-26\r\nandroid-ndk-r14b\r\n\r\n**workspace config:**\r\nandroid_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 26,\r\n    build_tools_version = \"26.0.1\",\r\n    path = \"/Users/akrasnoperov/Library/Android/sdk\",\r\n)\r\n\r\nandroid_ndk_repository(\r\n    name=\"androidndk\",\r\n    path=\"/Users/akrasnoperov/Library/Android/android-ndk-r14b\",\r\n    api_level=14)\r\n\r\n**Describe the problem**\r\nError when I run \"bazel build -c opt //tensorflow/examples/android:tensorflow_demo\"\r\n\r\n**Source code / logs**\r\nbazel build -c opt //tensorflow/examples/android:tensorflow_demo\r\nERROR: /private/var/tmp/_bazel_akrasnoperov/8a0891b65f3fddb9d9a6d9e66927aea5/external/androidsdk/BUILD.bazel:64:1: Traceback (most recent call last):\r\n\tFile \"/private/var/tmp/_bazel_akrasnoperov/8a0891b65f3fddb9d9a6d9e66927aea5/external/androidsdk/BUILD.bazel\", line 64\r\n\t\tcreate_system_images_filegroups(system_image_dirs = [\"system-ima...\"])\r\n\tFile \"/private/var/tmp/_bazel_akrasnoperov/8a0891b65f3fddb9d9a6d9e66927aea5/external/bazel_tools/tools/android/android_sdk_repository_template.bzl\", line 298, in create_system_images_filegroups\r\n\t\tint(apidir.split(\"-\")[1])\r\ninvalid literal for int() with base 10: \"MNC\".\r\nERROR: /private/var/tmp/_bazel_akrasnoperov/8a0891b65f3fddb9d9a6d9e66927aea5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:build-tools/26.0.1/lib/dx.jar' contains an error and its package is in error and referenced by '@androidsdk//:dx_jar'.\r\nERROR: /private/var/tmp/_bazel_akrasnoperov/8a0891b65f3fddb9d9a6d9e66927aea5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:dx_jar' contains an error and its package is in error and referenced by '@androidsdk//:dx_jar_import'.\r\nERROR: /Users/akrasnoperov/Developer/Repo/tensorflow/WORKSPACE:20:1: Target '@androidsdk//:dx_jar_import' contains an error and its package is in error and referenced by '//external:android/dx_jar_import'.\r\nERROR: Analysis of target '//tensorflow/examples/android:tensorflow_demo' failed; build aborted.\r\nINFO: Elapsed time: 0,463s", "comments": ["@AleksandrKrasnoperov I don't have anything more to add on my comment in #11474, which was:\r\n\r\nIt seems the directory parsing is somehow getting confused and splitting -MNC out of one of the sdk/.../android-MNC/ dirs rather than the appropriate API level. Do you have the indicated APIs from your WORKSPACE file installed? You can check via sdk/tools/android.\r\n\r\nOtherwise this might be an issue better asked over at  https://github.com/bazelbuild/bazel as this seems more like a Bazel configuration issue than a TF issue.", "I used new release - https://github.com/bazelbuild/bazel/releases/tag/0.6.1\r\nNow it works"]}, {"number": 13161, "title": "tf.contrib.data.Dataset outputs have only partial shape", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.2\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0 / 6\r\n- **GPU model and memory**: Titan X 12GB\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\n\r\nDataset outputs should have the first shape dimension specified when dataset.batch is specified. Instead, \"?\" is given.\r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndataset = tf.contrib.data.Dataset.range(100).map(lambda x: [x,x])\r\ndataset = dataset.batch(32)\r\niterator = dataset.make_initializable_iterator()\r\ninputs = iterator.get_next()\r\nprint(inputs.shape)\r\n```\r\n\r\nOutput: ```(?, 2)```\r\n\r\nExpected: ```(32, 2)```", "comments": ["This is intended behavior: if you evaluate `inputs` 4 times, you will notice that the shapes of the returned tensors are `(32, 2)`, `(32, 2)`, `(32, 2)`, and `(4, 2)`.", "@mrry is there a way to prevent the dataset from producing the final batch which has incomplete batch size (in your example, the (4,2))?  ", "Yes! In TF 1.4 (just RC'd yesterday) you can replace:\r\n\r\n```python\r\ndataset = dataset.batch(batch_size)\r\n```\r\n\r\n...with the following:\r\n\r\n```python\r\ndataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\r\n```", "I've been struggling with this problem.\r\nI've put\r\n`dataset = dataset.repeat()`\r\nbefore\r\n`dataset = dataset.batch(batch_size)`\r\nand expected to see fully defined TensorShape.", "@tomwesolowski Did you try the workaround I suggested in [my previous comment](https://github.com/tensorflow/tensorflow/issues/13161#issuecomment-336316164)?", "@mrry Yes and it worked. But my question is if it's a intended behaviour?", "I'd say it's \"expected\" more than \"intended\". `Dataset.batch()` doesn't currently include any special-case code that observes when its input is infinite. `tf.contrib.data.batch_and_drop_remainder()` gives you a workaround for the case when your code requires static shapes, or you can use a manual `Tensor.set_shape()` on the tensors returned from `Iterator.get_next()`.", "@mrry I have struggle with this problem\r\nfirst, when i deal with \r\n```\r\n    reshape = tf.reshape(pool2, [BATCH_SIZE, -1])\r\n    dim = reshape.get_shape()[1].value\r\n    weights = _variable_with_weight_decay('weights', shape=[dim, 384],\r\n                                          stddev=0.04, wd=0.004)\r\n    biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\r\n    print(weights.get_shape(), reshape.get_shape(), biases.get_shape())\r\n    local3 = tf.nn.relu(\r\n        tf.matmul(reshape, pruning.apply_mask(weights, scope)) + biases,\r\n        name=scope.name)\r\n```\r\nI encounter error \"ValueError: Shape of a new variable (local3/weights) must be fully defined, but instead was (?, 384).\"\r\n\r\nso I add \r\n```\r\n    next_example.set_shape([batch_size, height, width, 3])\r\n    next_label.set_shape([batch_size])\r\n```\r\nto \r\n\r\n```\r\n    dataset = tf.data.Dataset.from_tensor_slices((data_dir, labels))\r\n    dataset = dataset.map(_parse_function_distorted)\r\n    dataset = dataset.shuffle(buffer_size=10000)\r\n    dataset = dataset.batch(batch_size)\r\n    dataset = dataset.repeat(num_epochs)\r\n    iterator = dataset.make_one_shot_iterator()\r\n\r\n    next_example, next_label = iterator.get_next()\r\n    #next_label.set_shape([batch_size, 1])\r\n    print(next_label.get_shape())\r\n    print(next_example.get_shape())\r\n    height = IMAGE_SIZE\r\n    width = IMAGE_SIZE\r\n    next_example.set_shape([batch_size, height, width, 3])\r\n    next_label.set_shape([batch_size])\r\n```\r\n\r\nand I got error \r\n\"InvalidArgumentError (see above for traceback): Matrix size-incompatible: In[0]: [128,32], In[1]: [4096,384]\r\n\t [[Node: local3/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](local3/Reshape, local3/weights/masked_weight)]]\"\r\n\r\nand the value of dim should be changed, and in reality it doesn't.\r\n\r\nand how to fix it", "This is definitely confusing and was a headache to solve since I was getting an `Incompatible Shape` error consistently near step 1000 of training. Its worst since there is few documentation. \r\n\r\nI would suggest adding a kwarg like `drop_remainder = False` to the `batch` method that implements the solution suggested by @mrry.", "Thank you to @mrry for the workaround! \r\n\r\nI would like to point out that this also affects the more dynamic model definitions (not just the last batches of training), for example when modifying the example `models/research/resnet/resnet_model.py` to use a `Dataset` instead of the queues, building the model will choke on the fully connected layer at [resnet_model.py#L290](https://github.com/tensorflow/models/blob/95385809e79369337e5431f3e778700a77dc93a9/research/resnet/resnet_model.py#L290).\r\n\r\n", "Hi, \"InvalidArgumentError: ConcatOp : Dimensions of inputs should match\" , I meet the same problem, Did you have some solutions? thanks!!!", "Thanks @mrry , I used the `drop_remainder=true` flag and it fixed this issue. "]}, {"number": 13160, "title": "[feature request]recomputable operation annotation", "body": "'Training Deep Nets with Sublinear Memory Cost' and 'Memory-Efficient Implementation of DenseNets' indicate that use drop intermediate feature map and recompute it if needed can save memory(while add computation burden),\r\n\r\nso we need some mechanism to annotate some op's inputs  `recomputable` , and drop this input memory after op finish(set input's reference count to 0), when need this op again, recompute it.\r\n\r\n```python\r\na = tf.get_varible(shape=[None,10])\r\nb = tf.get_varible(shape=[None,10])\r\nc = tf.get_varible(shape=[None,10])\r\nd = a*b\r\nd_recomputable = tf.recomputable(d)\r\ne = d_recomputable+c\r\n```\r\nrelate issue \r\n- https://github.com/tensorflow/tensorflow/issues/1934\r\n- https://github.com/tensorflow/tensorflow/issues/12948\r\n\r\nin short, normal reference add reference count, recomputable reference do not add reference count\r\n", "comments": ["Marking the node as recomputable is not enough to save memory. You also have to specify when the recomputation happens.\r\n\r\nTensorFlow scheduling logic (in [executor.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.cc)) schedules nodes to evaluate soon as they are ready, so TensorFlow will happily recompute the node right away and hold it in memory which won't save you any memory.\r\n\r\nFun example -- something like `functools.reduce(tf.matmul, [tf.random_uniform((k,k))]*n)` needs O(n) memory in TensorFlow because by the time the first `matmul` is done, it will eval all the `random_uniform` nodes.\r\n\r\nYou can implement this recomputation on client level by inserting control dependencies to force a memory efficient order (like [here](https://github.com/yaroslavvb/stuff/tree/master/linearize)), and duplicating nodes you want to be recomputed (using tf.contrib.graph_editor like [here](https://github.com/yaroslavvb/stuff/blob/master/simple_rewiring.ipynb) or rewriting the graph in C++ API like [here](https://github.com/tensorflow/tensorflow/blob/cd4c17e63125da5e0047c3bdd05b483e86cc6759/tensorflow/core/grappler/optimizers/memory_optimizer.cc#L44)).\r\n\r\nI do agree that current methods are quite awkward", "@zheng-xq, do you want to comment about this issue, as I believe you are investigating automated ways of balancing memory/recomputation tradeoffs in your team.", "The current best practice to manually trade-off recomputation and memory is through TF functions. Through that, you can override the gradient with a custom function.\r\n\r\nI'll leave for zffchen78@ to comment on more details.", "@biolee btw, here's an [example](https://github.com/yaroslavvb/stuff/blob/master/saving%20memory%20by%20using%20functions.ipynb) of saving memory using functions. A function seems to be treated by tf.gradients as a single computation block and values inside are automatically recomputable.\r\n\r\n@zheng-xq \r\nI'm a bit confused on whether we can rely on this behavior in our models using default `tf.Defun` or whether the framework may start looking reusing computation inside functions at any version.\r\n\r\nSome places use `noinline=True` attribute but the tests say its broken\r\n\r\n(https://github.com/tensorflow/tensorflow/blob/01daba61e3a5099c6ad6439fa47e30c71560f06b/tensorflow/compiler/tests/function_test.py#L106)\r\n", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I think this issue is still valid.\r\n\r\n@zffchen78 you were mentioned in https://github.com/tensorflow/tensorflow/issues/13160#issuecomment-331797968 but I think it didn't notify you.", "Since some research groups use tf.Defun for recomputable operations, adding @josh11b to answer the question from @yaroslavvb that whether they can reply on this behavior, or what framework level support is on the roadmap. ", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The forementioned mechanism should be working. The test here illustrates w/ vs. w/o marking the forward function as noinline=True. \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/25d275280dfb163674f81c7681c2c1d34545a155/tensorflow/python/framework/function_test.py#L1435\r\n\r\nTo measure the runtime's peak memory usage, one can use ops demonstrated by this test:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/memory_stats/python/kernel_tests/memory_stats_ops_test.py\r\n\r\nTo see the effect more pronounced, one can increase the vector size (16 to something much larger):\r\nhttps://github.com/tensorflow/tensorflow/blob/25d275280dfb163674f81c7681c2c1d34545a155/tensorflow/python/framework/function_test.py#L1459\r\n\r\n@yaroslavvb I can not predict future. But several production scale models I'm working on depends on noinline=True to be respected and if it were broken somehow, some production teams would complain badly.", "@biolee btw, we just released TensorFlow package that implements recomputation in the style of \"Sublinear Memory Cost\" paper:\r\n\r\nhttps://github.com/openai/gradient-checkpointing\r\nhttps://medium.com/@yaroslavvb/fitting-larger-networks-into-memory-583e3c758ff9", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nice, closing since there is a solution.\r\n\r\nCC @zffchen78 "]}, {"number": 13159, "title": "Refactoring: Pull out repeated shape-related code into shared function ", "body": "While looking for some example code to initialize an op's output shape from a shape provided in an attribute, I found a six-line snippet that is repeated in seven different places. It looks like some copying and pasting has happened in the past. This pull request pulls that shared code into a single function in `common_shape_fns.cc`.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 13158, "title": "Eigen BiasAdd and BiasAddGrad Fix for NCHW Format.", "body": "", "comments": ["Can one of the admins verify this patch?", "@benoitsteiner please review", "Jenkins, test this please.", "@sb2nov: Any update on  pull request. Looks like some permission issues. For example:\r\n\r\nrm: cannot remove '/workspace/tensorflow/contrib/makefile/gen/obj/tensorflow/core/framework/op_def.pb_text.o': Permission denied\r\n........................\r\n\r\nLet me know if anything from my side need to be done.\r\n", "Ah that is just a flake so kicking off the tests again\r\n\r\nJenkins, test this please.", "@mdfaijul @sb2nov Looks like the same issue again on \"Linux CPU tests makefile\" (permission denied). Could it be an issue with the environment or docker image?", "I think that is known issue with the testing environment. @gunan helped with it a while back.\r\n\r\nGoing to try to re-run once more. \r\n\r\nJenkins, test this please.", "@sb2nov @benoitsteiner The unit tests have passed - do let me or @mdfaijul know if there's any review changes blocking the merge."]}, {"number": 13157, "title": "Error for slim dataset using fixed length reader", "body": "I want to use tensorflow slim data provider. All examples I can find only are only reading tfrecord files. However, I want to read binary files directly by my own reader. I use fixed length reader to extract Cifar10 binary data. \r\n\r\nHowever, \"dataBytes = tf.decode_raw(data, tf.unit8)\" always produce error that \"AttributeError: module 'tensorflow' has no attribute 'unit8'\" when using in slim DatasetDataProvider. \r\n\r\nBut no such error occurs when I use the reader directly without using DatasetDataProvider. Did I not use it correctly or does DatasetDataProvider only support tfrecord readers? Thank you.\r\n\r\nMy codes are as below:  \t\r\n\r\n\tCIFAR_LABEL_BYTE = 1\r\n\tCIFAR_HEIGHT = 32\r\n\tCIFAR_WIDTH = 32\r\n\tCIFAR_DEPTH = 3\r\n\tCIFAR_RECORD_BYTE = CIFAR_HEIGHT * CIFAR_WIDTH * CIFAR_DEPTH + CIFAR_LABEL_BYTE\r\n\r\n\tCIFAR_CLASS_NUM = 10\r\n\tCIFAR_TRAINING_NUM = 50000\r\n\tCIFAR_TEST_NUM = 10000\r\n\r\n\t_ITEMS_TO_DESCRIPTIONS = {\r\n\t\t'image': 'A [32 x 32 x 3] color image.',\r\n\t\t'label': 'A single integer between 0 and 9',\r\n\t}\r\n\r\n    class CifarBinaryDecoder(DataDecoder):\r\n\t  def decode(self, data, items):\r\n\t\toutputs = []\r\n                print(data.shape)\r\n\t\tdataBytes = tf.decode_raw(data, tf.uint8)\r\n\t\tfor item in items:\r\n\t\t  if item == 'label':\r\n\t\t\tcurrLabel = tf.cast(tf.strided_slice(dataBytes, [0], CIFAR_LABEL_BYTE), tf.int32)\r\n\t\t\toutputs.append(currLabel)\r\n\t\t  if item == 'image':\r\n\t\t\timageData = tf.reshpae(tf.strided_slice(dataBytes, [CIFAR_LABEL_BYTE],[CIFAR_RECORD_BYTE]), [CIFAR_DEPTH, CIFAR_HEIGHT, CIFAR_WIDTH])\r\n\t\t\timageData = tf.transpose(imageData, [1, 2, 0])\r\n\t\t\toutputs.append(imageData)\r\n\r\n\t\treturn outputs\r\n\r\n\t def list_items(self):\r\n\t\treturn ['label', 'image']\r\n\r\n\tdef get_cifar10_training_filenames(binary_data_dir):\r\n\t  file_pattern = ['test_batch.bin']\r\n\t  return file_pattern\r\n\r\n\tdef get_cifar_training_dataset(binary_data_dir):\r\n\t  file_pattern = get_cifar10_training_filenames(binary_data_dir)\r\n\t  decoder = CifarBinaryDecoder()\r\n\t  return slim.dataset.Dataset(data_sources = file_pattern,\r\n\t\t\t\t\t\t\t\t  reader = tf.FixedLengthRecordReader,\r\n\t\t\t\t\t\t\t\t  decoder = decoder,\r\n\t\t\t\t\t\t\t\t  num_samples = CIFAR_TRAINING_NUM,\r\n\t\t\t\t\t\t\t\t  items_to_descriptions = _ITEMS_TO_DESCRIPTIONS,\r\n\t\t\t\t\t\t\t\t  num_classes = CIFAR_CLASS_NUM)\r\n\r\n\tdef main(_):  \r\n\t  dataset = get_cifar_training_dataset(FLAGS.cifar_data_dir)\r\n\t  provider = slim.dataset_data_provider.DatasetDataProvider(\r\n\t\t  dataset,\r\n\t\t  num_readers = 1,\r\n\t\t  common_queue_capacity = 1000,\r\n\t\t  common_queue_min = 500,\r\n\t\t  reader_kwargs={'record_bytes' : CIFAR_RECORD_BYTE})\r\n\r\n\t with tf.Session() as sess:\r\n\t\tinit_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\r\n\t\tsess.run(init_op)\r\n\t\ttf.train.start_queue_runners()\r\n\r\n\t\tfor i in range(2):\r\n\t\t  [img, lab] = provider.get(['image', 'label'])\r\n                  image, label = sess.run([img, lab])", "comments": ["Looks like a typo: `tf.unit8` should be `tf.uint8`.", "@mrry Sorry for my mistake. I have corrected the typo, but it seems that the reader still does not work. I have added a line at the beginning of function decode as print(data.shape). Please see my modified codes in the original post.\r\n\r\nIt shows \"unknown\" in the output. The error message is \"tensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 1 but is rank 0 for 'StridedSlice' (op: 'StridedSlice') with input shapes: ?, [1], [], [1].\".\r\n\r\nTherefore, I guess the reader still doesn't work. I have double confirmed the file path & name, and I also tried to move the data file to the same folder as the code file. But it still doesn't work. Do I still have some more errors in my codes? Thank you.", "Without the stack trace, it's difficult to say for certain. However, I suspect the problem is in one or both of the two calls to `tf.strided_slice()` in `CifarBinaryDecoder.decode()`.", "I finally find out. The original codes \"currLabel = tf.cast(tf.strided_slice(dataBytes, [0], CIFAR_LABEL_BYTE), tf.int32)\", should be currLabel = tf.cast(tf.strided_slice(dataBytes, [0], [CIFAR_LABEL_BYTE]), tf.int32).\r\n\r\nIt is another typo again! Sorry for the confusion, and thank you for your help."]}, {"number": 13156, "title": "tar: Unrecognized archive format tar: Error exit delayed from previous errors.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["(1) Please review guidelines before raising an issue and provided template has not inputs/information.\r\n(2) Please go to [stackoverflow.com](stackoverflow.com) for technical difficulties and errors troubleshooting.\r\n(3) This [link](https://superuser.com/questions/568520/tgz-file-tar-unrecognized-archive-format) will solve your issue.", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 13155, "title": "error ", "body": "I get error when I make placeholder..", "comments": ["How about some more information?\n\nOn 19 Sep 2017 4:17 p.m., \"68uy78\" <notifications@github.com> wrote:\n\n> I get error when I make placeholder..\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13155>, or mute the\n> thread\n> <https://github.com/notifications/unsubscribe-auth/AFjlg0N8eDvTLoml9XDF04rrpHm0q5Meks5sj8yHgaJpZM4Pcdk4>\n> .\n>\n", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 13154, "title": "BeamSearchDecoder should support an AttentionWrapper cell with alignment_history enabled", "body": "### System information\r\n- **Have I written custom code**: yes\r\n- **OS Platform and Distribution**: Ubuntu 16.04\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: 1.3.0\r\n- **Python version**: 2.7\r\n- **Exact command to reproduce**: see the code snippet below.\r\n\r\n### Describe the problem\r\n\r\nCurrently, setting `tf.contrib.seq2seq.AttentionWrapper`'s `alignment_history` argument to `True` and using this cell in a `tf.contrib.seq2seq.BeamSearchDecoder` does not work for 2 reasons:\r\n\r\n1. In this configuration, the `tf.contrib.seq2seq.AttentionWrapper.state_size` property is invalid as it does not have the same structure as `zero_state` (see the code below). The [decoder state initialization](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py#L193) is failing because of this.\r\n2. `tf.contrib.seq2seq.BeamSearchDecoder` [raises an error](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py#L123) when the state contains a `TensorArray`, which is the type currently used to gather alignments.\r\n\r\nI believe this configuration should be supported as it is a standard use case for sequence to sequence models.\r\n\r\nTo address both of these limitations, it seems this `alignment_history` could be a `Tensor` on which alignments are repeatedly concatenated. Would it work?\r\n\r\n### Source code / logs\r\n\r\nThis code sample reproduces 1. which is the error directly visible when using this configuration.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nbatch_size = 2\r\nnum_units = 10\r\n\r\nmemory = tf.placeholder(tf.float32, shape=(None, None, num_units))\r\n\r\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n  num_units,\r\n  memory)\r\n\r\ncell = tf.contrib.seq2seq.AttentionWrapper(\r\n  tf.contrib.rnn.LSTMCell(num_units),\r\n  attention_mechanism,\r\n  alignment_history=True) # Set this to False to make it work.\r\n\r\ntf.contrib.framework.nest.assert_same_structure(\r\n  cell.zero_state(batch_size, dtype=tf.float32),\r\n  cell.state_size)\r\n```\r\n\r\nIt exits with this error:\r\n\r\n```text\r\nTraceback (most recent call last):\r\n  File \"<file>\", line 19, in <module>\r\n    cell.state_size)\r\n  File \"<dir>/local/lib/python2.7/site-packages/tensorflow/python/util/nest.py\", line 199, in assert_same_structure\r\n    % (len_nest1, nest1, len_nest2, nest2))\r\nValueError: The two structures don't have the same number of elements.\r\n\r\nFirst structure (6 elements): AttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'AttentionWrapperZeroState/checked_cell_state:0' shape=(2, 10) dtype=float32>, h=<tf.Tensor 'AttentionWrapperZeroState/checked_cell_state_1:0' shape=(2, 10) dtype=float32>), attention=<tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=(2, 10) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(2, ?) dtype=float32>, alignment_history=<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fbd6326ec50>)\r\n\r\nSecond structure (5 elements): AttentionWrapperState(cell_state=LSTMStateTuple(c=10, h=10), attention=10, time=TensorShape([]), alignments=<tf.Tensor 'LuongAttention/strided_slice_2:0' shape=() dtype=int32>, alignment_history=())\r\n```", "comments": ["Sounds like the beam search code needs to be extended to support TensorArray in the state.", "Unfortunately tensors aren't an option because they would incur quadratic time and space overhead.", "> Unfortunately tensors aren't an option because they would incur quadratic time and space overhead.\r\n\r\nInteresting. May I ask why this is quadratic in this context?\r\n\r\n> Sounds like the beam search code needs to be extended to support TensorArray in the state.\r\n\r\nSounds good. Are you planning to work on this in the near future?", "Quadratic because tf.concat creates a copy to fit the two new tensors and\nits backprop is a split which creates two tensors whose sizes add to the\nsizes of the inputs to the concat.  At every iteration.\n\nUnfortunately I'm not currently looking at this but contributions are\nwelcome.\n\nOn Sep 25, 2017 12:09 PM, \"Paul Tucker\" <notifications@github.com> wrote:\n\n> Assigned #13154 <https://github.com/tensorflow/tensorflow/issues/13154>\n> to @ebrevdo <https://github.com/ebrevdo>.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13154#event-1264354531>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimziPJEoaXTxgqSxFigraphRj4cr7ks5sl_pngaJpZM4PcbS1>\n> .\n>\n", "Thanks for bearing with me.  I added a high level question on the PR.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is there any other option i.e. another way to use `AttentionWrapper` with `alignment_history=True`? Or is `alignment_history=True` not possible at all right now?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "hello I want to know this question have been addressed? I account the same problem,need help"]}, {"number": 13153, "title": "Fix GRUBlockCell parameter naming inconsistency", "body": "This fix tries to fix the issue in #13137 where parameter `cell_size` is used instead of `num_units`. This is inconsistent with other RNN cells.\r\n\r\nThis fix adds support of `num_units` while at the same time maintains backward compatiblility for `cell_size`.\r\n\r\nThis fix fixes #13137.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @rohan100jain and @mrry to be potential reviewers.", "@ebrevdo WDYT?", "@ebrevdo Thanks for the review. The PR has been updated. Please take a look.", "Jenkins, test this please.", "Timeout or reboot of the executor, probably.\r\n\r\nJenkins, test this please."]}, {"number": 13152, "title": "GRPC causes training to pause in individual worker (distributed tensorflow, synchronised)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 8.9 (jessie)\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1\r\n- **Python version**: 3.6.2\r\n- **CUDA/cuDNN version**: cuda-8.0 / cudnn-5.1.5\r\n- **GPU model and memory**: GeForce GTX Titan X, 12 GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nThe distributed synchronized ( between graph replication, 4 workers, 3 ps ) training works fine until one of the ps tasks reports following error. After that, one of the worker processes just stops, and the rest of the workers may also stop later with same error. \r\n\r\n   ```\r\n 2017-09-21 16:45:55.606842: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2000, 1 -> localhost:2001, 2 -> localhost:2002}\r\n    2017-09-21 16:45:55.606877: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2003, 1 -> localhost:2004, 2 -> localhost:2005, 3 -> localhost:2006}\r\n    2017-09-21 16:45:55.608066: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2002\r\n    E0921 16:48:52.596846076    3037 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=12325, new grpc_chttp2_stream id=12317\r\n    2017-09-21 16:48:57.497244: W tensorflow/core/framework/op_kernel.cc:1158] Out of range: End of sequence\r\n         [[Node: data_source_task_index_0/IteratorGetNext = IteratorGetNext[output_shapes=[[-1,-1], [-1,-1], [-1,-1], [-1,-1], [-1,-1]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64], _device=\"/job:ps/replica:0/task:0/cpu:0\"](data_source_task_index_0/Iterator)]]\r\n         [[Node: data_source_task_index_0/cond/Merge_2_S341 = _Recv[client_terminated=false, recv_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device_incarnation=-6450759800525444137, tensor_name=\"edge_359_data_source_task_index_0/cond/Merge_2\", tensor_type=DT_INT64, _device=\"/job:ps/replica:0/task:2/cpu:0\"]()]]\r\n    E0921 16:49:58.462749643    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24775, new grpc_chttp2_stream id=24769\r\n    E0921 16:49:58.462780714    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24775, new grpc_chttp2_stream id=24773\r\n    E0921 16:49:58.463260203    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24777\r\n    E0921 16:49:58.463277333    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24779\r\n    E0921 16:49:58.463283953    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24781\r\n    E0921 16:49:58.463289625    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24783\r\n    E0921 16:49:58.463295275    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24785\r\n```\r\n\r\n\r\nFor more detail see the stackoverflow post: \r\nhttps://stackoverflow.com/questions/46322337/frozen-training-in-distributed-tensorflow  ", "comments": []}, {"number": 13151, "title": "Improve input tensor structure validation algorithm", "body": "Existing code that does input tensor validation is unable to check slices sizes properly.\r\nConsider this input:\r\n`[][][]float32{{{1, 2}, {3, 4}}, {{1}, {3}}, {{1, 2, 3}, {2, 3, 4}}}`\r\nNewTensor() currently treat this input as valid one.\r\nThis patch fixes this behaviour and also simplifies the check by removing unnecessary for cycles.\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 13150, "title": "Handling of * in pattern of tf.contrib.data.Dataset.list_files undocumented", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (see below)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: 8/6\r\n- **GPU model and memory**: GTX 1080, 8GB\r\n- **Exact command to reproduce**: run the script\r\n\r\n### Describe the problem\r\nIt seems that by default `list_files`' behaviour when a pattern contains `*`s is to match files at any depth in the directory tree. This is in contrast, for example, with `glob`'s default behaviour.\r\nI could not find any mention of how `*` is evaluated in [its documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/data/Dataset#list_files) or any examples of its usage in the [pogrammer's guide](https://www.tensorflow.org/programmers_guide/datasets).\r\nCould the documentation be improved specifying how exactly `*`s are handled?\r\n\r\n### Source code / logs\r\nSample dataset structure on filesystem:\r\n\r\n```\r\nDATASET_ROOT\r\n    _should_be_ignored\r\n        class3\r\n            subclass31\r\n                sample.txt\r\n            subclass32\r\n                sample.txt\r\n        class4\r\n            subclass41\r\n                sample.txt\r\n            subclass42\r\n                sample.txt\r\n    class1\r\n        subclass11\r\n            sample.txt\r\n        subclass12\r\n            sample.txt\r\n    class2\r\n        subclass21\r\n            sample.txt\r\n        subclass22\r\n            sample.txt\r\n```\r\n\r\nSmall script to test the behaviour:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport glob\r\n\r\nROOT = 'C:/Users/1/Desktop/test_dataset'\r\nglob_files = glob.glob('{}/*/*/*.txt'.format(ROOT))\r\n\r\ndataset = tf.contrib.data.Dataset.list_files('{}/*/*/*.txt'.format(ROOT))\r\nit = dataset.make_one_shot_iterator()\r\n\r\nfiles_found = []\r\nwith tf.Session() as sess:\r\n  while True:\r\n    try:\r\n      files_found.append(sess.run(it.get_next()))\r\n    except tf.errors.OutOfRangeError:\r\n      break\r\n```\r\n\r\nOutputs:\r\n```\r\nglob_files\r\nOut[16]: \r\n['C:/Users/1/Desktop/test_dataset\\\\class1\\\\subclass11\\\\sample.txt',\r\n 'C:/Users/1/Desktop/test_dataset\\\\class1\\\\subclass12\\\\sample.txt',\r\n 'C:/Users/1/Desktop/test_dataset\\\\class2\\\\subclass21\\\\sample.txt',\r\n 'C:/Users/1/Desktop/test_dataset\\\\class2\\\\subclass22\\\\sample.txt']\r\n\r\nfiles_found\r\nOut[4]: \r\n[b'C:\\\\Users\\\\1\\\\Desktop\\\\test_dataset\\\\class1\\\\subclass11\\\\sample.txt',\r\n b'C:\\\\Users\\\\1\\\\Desktop\\\\test_dataset\\\\class1\\\\subclass12\\\\sample.txt',\r\n b'C:\\\\Users\\\\1\\\\Desktop\\\\test_dataset\\\\class2\\\\subclass21\\\\sample.txt',\r\n b'C:\\\\Users\\\\1\\\\Desktop\\\\test_dataset\\\\class2\\\\subclass22\\\\sample.txt',\r\n b'C:\\\\Users\\\\1\\\\Desktop\\\\test_dataset\\\\_should_be_ignored\\\\class3\\\\subclass31\\\\sample.txt',\r\n b'C:\\\\Users\\\\1\\\\Desktop\\\\test_dataset\\\\_should_be_ignored\\\\class3\\\\subclass32\\\\sample.txt',\r\n b'C:\\\\Users\\\\1\\\\Desktop\\\\test_dataset\\\\_should_be_ignored\\\\class4\\\\subclass41\\\\sample.txt',\r\n b'C:\\\\Users\\\\1\\\\Desktop\\\\test_dataset\\\\_should_be_ignored\\\\class4\\\\subclass42\\\\sample.txt']\r\n```\r\n", "comments": ["@GPhilo I guess `Dataset.list_files` line invokes code `Dataset.from_tensor_slices(gen_io_ops.matching_files(file_pattern))` \r\nwhich returns : `A Dataset of strings corresponding to file names.`\r\n Probably,it keeps digging till it hits no folders/directories!\r\nThe file pattern follows rules as described here [tf.gfile.Glob](https://www.tensorflow.org/api_docs/python/tf/gfile/Glob)\r\nsee code snipped below which is a cause for lookup into `_should_be_ignored`\r\n\r\n```\r\nif isinstance(file_pattern, list):\r\n    if not file_pattern:\r\n      raise ValueError(\"File pattern is empty.\")\r\n    file_names = []\r\n    for entry in file_pattern:\r\n      file_names.extend(gfile.Glob(entry))\r\n  else:\r\n    file_names = list(gfile.Glob(file_pattern))\r\n```", "@saeta could you please take a look, and change the example in documentation if necessary.", "I'm using Tensorflow version 1.8 on both Windows 10 and Kubuntu 18.04 and notice different behavior of `list_files`. Using a pattern with `*`s will match files at any depth an Windows, as noted in this issue, whereas the proper directory depth is respected on Ubuntu.", "Please check with the latest version of TensorFlow. Feel free to reopen if the issue still persists. Thanks!"]}, {"number": 13149, "title": "upgrade_tensorflow_to_latest_package", "body": "1. upgrade package to 1.3.0rc2 the latest package\r\n2. imporve ipykernel install steps under python 2.x", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "RE: I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "RE: Updated", "I signed it!", "I can't find your github username in our CLA database. Could you double check?"]}, {"number": 13148, "title": "Add data dynamically to pre-exisiting contrib.data.dataset", "body": "(As far as I'm aware this isn't possible, so I think this is a feature request)\r\n\r\nIt would be great to be able to add data to a pre-exisiting dataset object, which already has an iterator attached. For example, in reinforcement learning, one often collects data while exploring, and needs to add this data to the training dataset. I can't see how using the `concatenate()` function is sufficient, as this returns a new dataset object and would require a new iterator etc.\r\n@mrry \r\n\r\nThanks!", "comments": ["@mrry, could you please evaluate this feature request. If it is a question of  it being possible already then perhaps it should belong on StackOverflow. I don't suppose you can use has-A inheritance to implement layered datasets?", "What interface and semantics would you expect for a feature like this? To guide the discussion, I doubt it would make sense to make *all* datasets mutable, because this would inhibit optimization and lead us down a path similar to the queues. However, it could make sense to have some kind of mutable data *source*, and allow using datasets to transform and iterators to iterate over these. \r\n\r\nFor example, today you can write code that creates a dataset from a queue, using this (slightly unintuitive) construction:\r\n\r\n```python\r\nq = tf.FIFOQueue(...)\r\n\r\n# Create an infinite dummy dataset\r\ndummy = Dataset.from_tensors(0).repeat(None)\r\n\r\n# Create a dataset by dequeuing successive elements from `q`.\r\ndataset = dummy.map(lambda _: q.dequeue())\r\n```\r\n\r\nYou could also achieve something similar using the (new in 1.4) `Dataset.from_generator()` and a Python `queue` object.\r\n\r\nIf you used this functionality, would it satisfy your use case, or are there other things you'd like to see? ", "Thanks - this would work. I guess for a cleaner API, I was imagining something like this:\r\n```python\r\ndata = Dataset.from_tensor_slices((initial_features, initial_labels))\r\n\r\n# get some new data and append this to the dataset\r\ndata.add_from_tensor_slices((new_features, new_labels))\r\n```", "@mrry, could you respond to the cleaner API improvement suggestion?", "Since `data` in that example is a `tf.data.Dataset` object, an API like that would require some form of mutability in the `Dataset` class, which we are not going to support.\r\n\r\nIt might be possible to define some new type that supports (i) maintaining a dynamic set of elements, and (ii) creating a `Dataset` over that dynamic set of elements that has well-defined semantics (e.g. it could randomly sample from the current set of elements, or cycle through the elements in FIFO order). ", "@alexgkendall Did you get any better solution to above problem. I am working on Reinforcement learning. and try to apply RL on Gym atari game, where dataset keeps changing as I play. Basically, I store last 50000 game frames and then I select 32 frames randomly for training. But in the case of \"tf.data.from_tensor_slices(myimagedataset)\", myimagedataset keeps updating/changing on each step. so what could be a better approach to use dataset api.\r\n\r\n", "@coderbhupendra Are you looking for have a video input with frames coming in with FIFO buffering of 50000 frames (sliding window), and randomly select 32 out of the 50000 frames window?", "@yongtang Yes, you can think it in that way. Now I am using feed_dict to load 32 random frames from by buffer memory of size 50000. But I think due to feed_dict I am facing CPU bottleneck and my gpu is only getting 30% utilized.  ", "In `tensorflow/io` you could read video files (needs FFmpeg library install) as a dataset. Then I think you could do some additional operations to achieve the effort you want. Maybe you could take a look and give it a try? \r\nRepo: https://github.com/tensorflow/io\r\nGoogle Group: https://groups.google.com/a/tensorflow.org/forum/#!forum/io", "@yongtang Thanks for helping me. But my dataset is not video. Its a game environment. My agent plays game and stores the frame (so its basically a 84*84 image). And I keep adding frames as I play and delete older frames from the buffer. So at each step I play I add a new frame and delete one old frame and randomly select 32 frames for training.  I hope I am not confusing you. ", "@coderbhupendra Implementation wise that should be possible. Essentially it is a streaming data as input (with frames in). Internally, the class will keep a circular buffer (FIFO) to keep only 50000 frames and drops old frames as new frames in. Each batch will randomly select 32 frames from the circular buffer (of 50000) as the output.\r\n\r\nAs @mrry mentioned this is unlikely to be supported in `tf.data.Dataset` as it mutates.\r\n\r\nBut if the use case is generic enough I think we could consider adding it in `tensorflow/io`. Would you like to open an issue in https://github.com/tensorflow/io with more details? (Especially the input format you plan to take.)\r\n"]}, {"number": 13147, "title": "How do I compile Tensorflow source with Altera FPGA library (AOCLUtils & OPENCL)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform** : Linux Ubuntu 16.04\r\n- **TensorFlow installed from source**\r\n- **TensorFlow version** : 1.3\r\n- **Python version**: v2.7.10\r\n- **Bazel version**: 0.5.4\r\n- **Exact command to reproduce**: set all as default value  ./configure , not use cuda\r\n\r\n\r\n### Describe the problem\r\nI am studying posting matmul_op calculation  into Altera FPGA Arria 10 via OPENCL and combine source and OpenCL/AOCLUtils library as python package.\r\nHowever, I encountered much problem about this and always build (Bazel) failed when compile.\r\nThe following share some experience in every stages and some problem over the past month.\r\n\r\nAOCLUtils library package\r\n<pre><code>\r\n/AOCLUtils/BUILD     --->for bazel\r\n/AOCLUtils/aocl_utils.h\r\n/AOCLUtils/opencl.h\r\n/AOCLUtils/options.h\r\n/AOCLUtils/opencl.cpp\r\n/AOCLUtils/options.cpp\r\n/AOCLUtils/scoped_ptrs.h\r\n</code></pre>\r\n\r\nOPENCL header library package in /home/mater/intelFPGA_pro/17.0/hld/host/include\r\ncompiled library (*.so) in /home/mater/intelFPGA_pro/17.0/hld/host/linux64/lib ;  /home/mater/intelFPGA_pro/17.0/hld/board/a10_ref/linux64/lib\r\n<pre><code>\r\nCL/cl_d3d10.h\r\nCL/cl_ext.h\r\nCL/cl_gl.h\r\nCL/cl.hpp\r\nCL/opencl.h\r\nCL/cl_ext_altera.h\r\nCL/cl_gl_ext.h\r\nCL/cl.h\r\nCL/cl_platform.h\r\n</code></pre>\r\n------------------------\r\n1. Study third party library - SYCL  (Seems not support Altera FPGA)\r\nOriginal, tensorflow supports OpenCL via SYCL.\r\nHowever, it only seems support GPU & CPU, not Altera FPGA.\r\nBesides, I gave up this method.\r\n------------------------\r\n2. Success build a New(external) Op with AOCLUtils/OPENCL via g++, but it's my purpose.\r\nRefer : https://www.tensorflow.org/extend/adding_an_op\r\nI study example for a new op and build as library with AOCLUtils/OPENCL.\r\nThe g++ command is as follow then built as zero_out_cl.so.\r\nTensorflow can load this external library and run some calculation in FPGA\r\n(zero_out_module = tf.load_op_library('./zero_out_all.so'))\r\n\r\nAlthough my purpose is tensorflow python installation with AOCLUtils/OPENCL library,\r\nit makes sure that tensorflow can support FPGA library\r\n\r\n<pre><code>\r\ng++ -std=c++11 -shared main.cpp -o zero_out_cl.so -fPIC -I /home/mater/tensorflowCPU_1.3/lib/python2.7/site-packages/tensorflow/include -I/home/mater/intelFPGA/17.0/hld/host/include /home/mater/AI/FPGA/TF_OPENCL_ZeroOut/common/src/AOCLUtils/opencl.cpp /home/mater/AI/FPGA/TF_OPENCL_ZeroOut/common/src/AOCLUtils/options.cpp -L/home/mater/intelFPGA_pro/17.0/hld/board/a10_ref/linux64/lib -L/home/mater/intelFPGA_pro/17.0/hld/host/linux64/lib -Wl,--no-as-needed -lalteracl -laltera_a10_ref_mmd -lelf -I/home/mater/AI/FPGA/TF_OPENCL_ZeroOut/common/inc -O2 -D_GLIBCXX_USE_CXX11_ABI=0 \r\n</code></pre>\r\n------------------------\r\n3. Modify Tensorflow source code with AOCLUtils/OPENCL and always bazel build failed..\r\n*step 1. git clone into /home/mater/git/test/tensorflow_opencl (tensorflow root folder)\r\n*step 2. copy AOCLUtils package into tensorflow root folder\r\n*step 3. Add cc_library into WORKSPACE for external opencl library (not sure correct method)\r\n<pre><code>\r\nnew_local_repository(\r\n    name = \"opencl_headers\",\r\n    path = \"/home/mater/intelFPGA_pro/17.0/hld/host/include\",\r\n    build_file_content = \"\"\"\r\ncc_library(\r\n\tname = \"CL\",\r\n\thdrs = glob([\"CL/*.h\"]),\r\n\tvisibility = [\"//visibility:public\"],\r\n\tlinkopts=[\"-shared\"],\r\n)\r\n\"\"\",\r\n)\r\n\r\nnew_local_repository(\r\n    name = \"opencl_libs\",\r\n    path = \"/home/mater/intelFPGA_pro/17.0/hld/host/linux64/lib\",\r\n    build_file_content = \"\"\"\r\ncc_library(\r\n\tname = \"libopencl\",\r\n\tsrcs = glob([\"*.so\"]),\r\n\tvisibility = [\"//visibility:public\"],\r\n\tlinkopts=[\"-shared\"],\r\n)\r\n\"\"\",\r\n)\r\n\r\nnew_local_repository(\r\n    name = \"a10_lib\",\r\n    path = \"/home/mater/intelFPGA_pro/17.0/hld/board/a10_ref/linux64/lib\",\r\n    build_file_content = \"\"\"\r\ncc_library(\r\n\tname = \"liba10\",\r\n\tsrcs = [\"libaltera_a10_ref_mmd.so\"],\r\n\tvisibility = [\"//visibility:public\"],\r\n\tlinkopts=[\"-shared\"],\r\n)\r\n\"\"\",\r\n)\r\n</code></pre>\r\n\r\n*step 4. Add BUILD file into AOCLUtils folder\r\n<pre><code>\r\ncc_library(\r\n\tname=\"aocutils\",\r\n\tsrcs = glob([\"*.cpp\"]),\r\n\thdrs = glob([\"*.h\"]),\r\n\tdeps = [\"@opencl_libs//:libopencl\", \"@opencl_headers//:CL\", \"@a10_lib//:liba10\",],\r\n\tvisibility=[\"//visibility:public\"],\r\n)\r\n</code></pre>\r\n\r\n*step 5. Add deps into tf_kernel_library in tensorflow root folder/tensorflow/core/kernels/BUILD\r\nI would like to run matmul_op calculation in FPGA so try to include FPGA library\r\n<pre><code>\r\ntf_kernel_library(\r\n    name = \"matmul_op\",\r\n...\r\n    deps = MATH_DEPS + [\r\n...\r\n    ]) + [\"//AOCLUtils:aocutils\",],\r\n</code></pre>\r\n\r\n*I tried 3 methods to build tensorflow source code as python installation package, but always failed\r\n------------------------\r\n*step 6A.  Original bazel build command (Failed situation A)\r\nUse original Bazel build command, but failed to find CL library.\r\nexception :   fatal error: CL/cl.h: No such file or directory\r\n\r\ncommand\r\n<pre><code>\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\n</code></pre>\r\nresult\r\n<pre><code>\r\nERROR: /home/mater/git/test/tensorflow_opencl/AOCLUtils/BUILD:1:1: C++ compilation of rule '//AOCLUtils:aocutils' failed (Exit 1): gcc failed: error executing command \r\n  (cd /home/mater/.cache/bazel/_bazel_mater/cf4207c477b73da1da7e3336942f640b/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++0x' '-march=native' '-D_GLIBCXX_USE_CXX11_ABI=0' -MD -MF bazel-out/local-opt/bin/AOCLUtils/_objs/aocutils/AOCLUtils/options.pic.d '-frandom-seed=bazel-out/local-opt/bin/AOCLUtils/_objs/aocutils/AOCLUtils/options.pic.o' -fPIC -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/opencl_libs -iquote bazel-out/local-opt/genfiles/external/opencl_libs -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/opencl_headers -iquote bazel-out/local-opt/genfiles/external/opencl_headers -iquote external/a10_lib -iquote bazel-out/local-opt/genfiles/external/a10_lib -isystem external/bazel_tools/tools/cpp/gcc3 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c AOCLUtils/options.cpp -o bazel-out/local-opt/bin/AOCLUtils/_objs/aocutils/AOCLUtils/options.pic.o)\r\nIn file included from ./AOCLUtils/opencl.h:32:0,\r\n                 from ./AOCLUtils/aocl_utils.h:27,\r\n                 from AOCLUtils/options.cpp:22:\r\nexternal/opencl_headers/CL/opencl.h:42:19: fatal error: CL/cl.h: No such file or directory\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 12.957s, Critical Path: 5.11s\r\nFAILED: Build did NOT complete successfully\r\n</code></pre>\r\n------------------------\r\n*step 6B.  Add link path in command (Failed situation B)\r\nUse original Bazel build command with CL external path included, but failed to missing dependency declarations.\r\nexception :   missing dependency declarations for the following files included by 'AOCLUtils/options.cpp'\r\n\r\nI think this method should be ok and bazel can find CL library location, but I can't understand what happen for this.\r\nI study some information about 'missing dependency declarations', and found 'CROSSTOOL' for GPU for this title. It didn't help me to solve it because I don't use GPU configuration.\r\n\r\ncommand like building a New(external) Op method\r\n<pre><code>\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --copt=\"-I/home/mater/intelFPGA/17.0/hld/host/include\" --verbose_failures  --copt=\"-L/home/mater/intelFPGA_pro/17.0/hld/board/a10_ref/linux64/lib\" --copt=\"-L/home/mater/intelFPGA_pro/17.0/hld/host/linux64/lib\" --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\n</code></pre>\r\nresult\r\n<pre><code>\r\nERROR: /home/mater/git/test/tensorflow_opencl/AOCLUtils/BUILD:1:1: undeclared inclusion(s) in rule '//AOCLUtils:aocutils':\r\nthis rule is missing dependency declarations for the following files included by 'AOCLUtils/options.cpp':\r\n  '/home/mater/intelFPGA/17.0/hld/host/include/CL/cl.h'\r\n  '/home/mater/intelFPGA/17.0/hld/host/include/CL/cl_platform.h'\r\n  '/home/mater/intelFPGA/17.0/hld/host/include/CL/cl_gl.h'\r\n  '/home/mater/intelFPGA/17.0/hld/host/include/CL/cl_gl_ext.h'\r\n  '/home/mater/intelFPGA/17.0/hld/host/include/CL/cl_ext.h'\r\n  '/home/mater/intelFPGA/17.0/hld/host/include/CL/cl_ext_altera.h'\r\nIn file included from /home/mater/intelFPGA/17.0/hld/host/include/CL/cl_ext.h:42:0,\r\n                 from external/opencl_headers/CL/opencl.h:45,\r\n                 from ./AOCLUtils/opencl.h:32,\r\n                 from ./AOCLUtils/aocl_utils.h:27,\r\n                 from AOCLUtils/options.cpp:22:\r\n/home/mater/intelFPGA/17.0/hld/host/include/CL/cl_ext_altera.h:442:0: warning: ignoring #pragma warning  [-Wunknown-pragmas]\r\n #pragma warning( push )\r\n ^\r\n/home/mater/intelFPGA/17.0/hld/host/include/CL/cl_ext_altera.h:443:0: warning: ignoring #pragma warning  [-Wunknown-pragmas]\r\n #pragma warning( disable:4201 )\r\n ^\r\n/home/mater/intelFPGA/17.0/hld/host/include/CL/cl_ext_altera.h:459:0: warning: ignoring #pragma warning  [-Wunknown-pragmas]\r\n #pragma warning( pop )\r\n ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n</code></pre>\r\n------------------------\r\n*step 6C.  Build AOCLUtils as library,AOCUtils.so, at first, then build tensorflow with this library (Failed situation C)\r\nDue to the below two situation, I tried to build AOCUtils package as library. Maybe it can skip some exception for CL library missing.\r\nAlthough it passed c++ code compiled and continue to build about 10 minutes (below two situation show failed message immediately), it still be failed at packaging python due to not find AOCLUtils\r\n\r\nFirst command for AOCUtils.so\r\n<pre><code>\r\ng++ -std=c++11 -shared AOCLUtils/opencl.cpp AOCLUtils/options.cpp -o AOCLUtils/AOCUtils.so -fPIC -I /home/mater/tensorflowCPU_1.3/lib/python2.7/site-packages/tensorflow/include -I/home/mater/intelFPGA/17.0/hld/host/include -L/home/mater/intelFPGA_pro/17.0/hld/board/a10_ref/linux64/lib -L/home/mater/intelFPGA_pro/17.0/hld/host/linux64/lib -Wl,--no-as-needed -lalteracl -laltera_a10_ref_mmd -lelf -I/home/mater/git/test/tensorflow_opencl -O2 -D_GLIBCXX_USE_CXX11_ABI=0\r\n</code></pre>\r\n\r\nModify AOCLUtils/BUILD\r\n<pre><code>\r\ncc_library(\r\n\tname=\"aocutils\",\r\n\tsrcs = [\"AOCLUtils.so\"],\r\n\thdrs = glob([\"*.h\"]),\r\n\tvisibility=[\"//visibility:public\"],\r\n)\r\n</code></pre>\r\n\r\nSecond command for build\r\n<pre><code>\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --copt=\"-I/home/mater/intelFPGA/17.0/hld/host/include\" --verbose_failures  --copt=\"-L/home/mater/intelFPGA_pro/17.0/hld/board/a10_ref/linux64/lib\" --copt=\"-L/home/mater/intelFPGA_pro/17.0/hld/host/linux64/lib\" --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\n</code></pre>\r\n\r\nresult\r\n<pre><code>\r\nERROR: /home/mater/git/test/tensorflow_opencl/tensorflow/python/BUILD:2908:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1): gcc failed: error executing command \r\n  (cd /home/mater/.cache/bazel/_bazel_mater/cf4207c477b73da1da7e3336942f640b/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /usr/bin/gcc -shared -o bazel-out/local-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so '-Wl,-rpath,$ORIGIN/../../_solib_k8/_U_S_SAOCLUtils_Caocutils___UAOCLUtils' -Lbazel-out/local-opt/bin/_solib_k8/_U_S_SAOCLUtils_Caocutils___UAOCLUtils -Wl,--version-script tensorflow/tf_version_script.lds -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,@bazel-out/local-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params)\r\n/usr/bin/ld.gold: error: cannot find -lAOCLUtils\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 817.337s, Critical Path: 40.20s\r\nFAILED: Build did NOT complete successfully\r\n</code></pre>\r\n\r\nCould someone help me find some suggestion or share information about this? Thanks!! \r\n", "comments": ["@martinwicke could you please take a look?", "You need to include all dependencies you need to build `build_pip_package` as deps for the `build_pip_package` target. You included it as a matmul_op dependency, can you build just that target? Does that work or does that already fail?", "Hi  martinwicke,\r\n\r\nThanks for information.\r\nI modified tensorflow/tools/pip_package/BUILD to add AOCLUtils path in included_headers\r\nBut still failed as the following.\r\n\r\nAt the other hand, I am thinking about what is different situation between bazel (FAIL) & g++(PASS) build for AOCLUtils library only  in the end of this comment. Maybe this is a point to find problem for bazel build\r\n\r\n<pre><code>\r\n=tensorflow/tools/pip_package/BUILD =\r\n\r\ntransitive_hdrs(\r\n    name = \"included_headers\",\r\n    deps = [\r\n        ......\r\n        \"//third_party/eigen3\",\r\n        \"//AOCLUtils:aocutils\",  ---->Add\r\n    ],\r\n)\r\n......\r\npy_binary(\r\n    name = \"simple_console_for_windows\",\r\n    srcs = [\"simple_console_for_windows.py\"],\r\n    data = [\r\n        \"MANIFEST.in\",\r\n        \"README\",\r\n        \"setup.py\",\r\n        \":included_headers\",   ---->Dependency\r\n</code></pre>\r\n\r\nResult is failed to not find -lAOCLUtils & -lhls_cosim_msim32\r\n<pre><code>\r\n/usr/bin/ld.gold: error: cannot find -lAOCLUtils\r\n/usr/bin/ld.gold: warning: skipping incompatible bazel-out/local-opt/bin/_solib_k8/_U@opencl_Ulibs_S_S_Clibopencl___Uexternal_Sopencl_Ulibs/libhls_cosim_msim32.so while searching for hls_cosim_msim32\r\n/usr/bin/ld.gold: error: cannot find -lhls_cosim_msim32\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 42.804s, Critical Path: 24.29s\r\nFAILED: Build did NOT complete successfully\r\n</code></pre>\r\n\r\n------------------\r\n### Difference between bazel & g++ build AOCLUtils as library\r\n* g++ : PASS\r\n<pre><code>\r\ng++ -std=c++11 -shared AOCLUtils/opencl.cpp AOCLUtils/options.cpp -o AOCLUtils/AOCUtils.so -fPIC -I /home/mater/tensorflowCPU_1.3/lib/python2.7/site-packages/tensorflow/include -I/home/mater/intelFPGA/17.0/hld/host/include -L/home/mater/intelFPGA_pro/17.0/hld/board/a10_ref/linux64/lib -L/../intelFPGA_pro/17.0/hld/host/linux64/lib -Wl,--no-as-needed -lalteracl -laltera_a10_ref_mmd -lelf -I/home/mater/git/test/tensorflow_opencl -O2 -D_GLIBCXX_USE_CXX11_ABI=0\r\n</code></pre>\r\n\r\n* Bazel : FAIL due to wrong parameters fo BUILD file\r\n<pre><code>\r\n=Command=\r\nbazel build --config=opt //AOCLUtils:aocutils\r\n\r\n=AOCLUtils/BUILD=\r\ncc_library(\r\n\tname=\"aocutils\",\r\n\tsrcs = glob([\"*.cpp\"]),\r\n\thdrs = glob([\"*.h\"]),\r\n\tdeps = [\"@opencl_libs//:libopencl\", \"@opencl_headers//:CL\", \"@a10_lib//:liba10\",],\r\n\tcopts = [\"-I@opencl_libs\"],\r\n\tvisibility=[\"//visibility:public\"],\r\n)\r\n\r\n=Build result=\r\nERROR: /home/mater/git/test/tensorflow_opencl/AOCLUtils/BUILD:1:1: C++ compilation of rule '//AOCLUtils:aocutils' failed (Exit 1)\r\nIn file included from ./AOCLUtils/opencl.h:32:0,\r\n                 from ./AOCLUtils/aocl_utils.h:27,\r\n                 from AOCLUtils/options.cpp:22:\r\nexternal/opencl_headers/CL/opencl.h:42:19: fatal error: CL/cl.h: No such file or directory\r\ncompilation terminated.\r\nTarget //AOCLUtils:aocutils failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 0.261s, Critical Path: 0.04s\r\nFAILED: Build did NOT complete successfully\r\n</code></pre>\r\n\r\nAdd \"-I/home/mater/intelFPGA_pro/17.0/hld/host/include\" (OpenCL header) in command\r\n<pre><code>\r\n=Command=\r\n bazel build --config=opt //AOCLUtils:aocutils --copt=\"-I/home/mater/intelFPGA_pro/17.0/hld/host/include\"\r\n\r\n=Result=\r\nERROR: /home/mater/git/test/tensorflow_opencl/AOCLUtils/BUILD:1:1: undeclared inclusion(s) in rule '//AOCLUtils:aocutils':\r\nthis rule is missing dependency declarations for the following files included by 'AOCLUtils/opencl.cpp':\r\n  '/home/mater/intelFPGA_pro/17.0/hld/host/include/CL/cl.h'\r\n  '/home/mater/intelFPGA_pro/17.0/hld/host/include/CL/cl_platform.h'\r\n  '/home/mater/intelFPGA_pro/17.0/hld/host/include/CL/cl_gl.h'\r\n  '/home/mater/intelFPGA_pro/17.0/hld/host/include/CL/cl_gl_ext.h'\r\n  '/home/mater/intelFPGA_pro/17.0/hld/host/include/CL/cl_ext.h'\r\n  '/home/mater/intelFPGA_pro/17.0/hld/host/include/CL/cl_ext_altera.h'\r\nIn file included from /home/mater/intelFPGA_pro/17.0/hld/host/include/CL/cl_ext.h:42:0,\r\n                 from external/opencl_headers/CL/opencl.h:45,\r\n                 from ./AOCLUtils/opencl.h:32,\r\n                 from ./AOCLUtils/aocl_utils.h:27,\r\n                 from AOCLUtils/opencl.cpp:22:\r\n</code></pre>", "Bazel doesn't like it if you depend (by including) things that are not in your system libraries, and which are not declared. You would probably have to include the intelFPGA_pro library as a `new_local_repository` and declare a dependency on it.\r\n\r\n@damienmg for bazel magic.", "Hi  martinwicke,\r\n\r\nThanks for suggestion, I am continuing to study Bazel document for setting to build up this method. ", "Hi,\r\n      Have you solved this problem? I saw this \"https://github.com/hughperkins/tf-coriander\", they have run Tensorflow on OpenCL\u2122 1.2 GPU, but I don't know if this method applies to Altera FPGA. You can have a try. Good luck!", "Hi Kody851,\r\n\r\nThanks for info. I just implemented one method to  fix this problem.\r\nI think your suggestion should be worked for this yesterday, but Altera Arria X currently supports OpenCL v1.0.\r\nPart function of tf-coriander could need to be modified from OpenCL v1.2 to OpenCL v1.0.\r\n\r\nHi all,\r\n\r\nThanks for your suggestion. Please help switch this bug as close. The following lists my method to share with someone having this problem.\r\n\r\nAs martinwicke's response , Bazel doesn't like to dependency external library, not in system.\r\nMy method is very similar as that I previously wrote \"..g++ build AOCLUtils as library\"\r\n\r\nI implements computing function, AOCLUtils, and OpenCL library in one class (For example, class MatMulFPGA.)\r\nAnd add external class (just call MatMulFPGAInterface) to call MatMulFPGA  to avoid MatMulFPGAInterface.h to include AOCLUtils & OpenCL header directly.\r\nThen, packaged them as one library(*.so) via GCC and this header file.\r\nBesides, when build tensorflow with this library, bazel don't know & compile AOCLUtils & OpenCL library.\r\nIt fixed my problem that AOCLUtils & OpenCL library always were failed at the end when build tensorflow.\r\nThis method is little troublesome, but work^^   \r\n\r\n\r\n", "Hi, @materacer \r\n\r\nDid you mean that you have run tensorflow with the Altera FPGA Arria 10 successfully? And I'was wondering if you could provide a more detailed tutorial, it would help a lot of people. Thank you! :-p", "Hi JulyJohn,\r\n\r\nThe following lists sample code.\r\nHope it help you or people need.\r\n\r\n*Important* : Please not include any opencl library in header. It avoids Bazel dependency issue.\r\nBesides, I fixed this issue via adding external class to package.\r\n\r\n\r\n1. Implement libMatConvFPGA.so library\r\n\r\nMatConvFPGA.h\r\n<pre><code>\r\n#ifndef MATCONV_FPGA_H\r\n#define MATCONV_FPGA_H \r\n\r\nclass MatConvFPGA;\r\nclass MatConvFPGAInterface{\r\npublic:\r\n\tMatConvFPGAInterface();\r\n\tbool init_opencl(...);\r\n\tfloat* compute(...);\r\n\tvoid cleanup();\r\n\t~MatConvFPGAInterface();\r\nprivate:\r\n\tMatConvFPGA *_MatConvFPGA;\r\n}; \r\n#endif\r\n</code></pre>\r\n\r\nMatConvFPGA.cpp\r\n<pre><code>\r\n#include \"CL/opencl.h\"\r\n#include \"AOCLUtils/aocl_utils.h\"\r\n#include \"MatConvFPGA.h\"\r\n\r\nclass MatConvFPGA\r\n{\r\nprivate:\r\n\tcl_platform_id platform = NULL;\r\n\tunsigned num_devices = 0;\r\n\tscoped_array<cl_device_id> devices;\r\n\tcl_context context = NULL;\r\n\tcl_program program = NULL; \r\npublic:\r\n\tbool init_opencl(....)   { .... }     //Implement OpenCL Initialization\r\n        bool compute(...)  { ... }      //Implement function\r\n\tvoid cleanup() {...}         //Clean OpenCL object\r\n};\r\n\r\nMatConvFPGAInterface::MatConvFPGAInterface() {\r\n\t_MatConvFPGA = new MatConvFPGA();\r\n} \r\n\r\nMatConvFPGAInterface::~MatConvFPGAInterface() {\r\n\tcleanup();\r\n\tdelete _MatConvFPGA;\r\n} \r\nfloat* MatConvFPGAInterface::compute(...) {\r\n\tfloat *Outputs = NULL;\r\n\r\n\tbool result = _MatConvFPGA->compute(...); \r\n\tif (!result)\r\n\t\tOutputs = NULL;\r\n\treturn Outputs;\r\n} \r\nbool MatConvFPGAInterface::init_opencl(...) {\r\n\tinit_opencl_success = _MatConvFPGA->init_opencl(....);\r\n\treturn init_opencl_success;\r\n} \r\nvoid MatConvFPGAInterface::cleanup() {\r\n\tinit_opencl_success = false;\r\n\t_MatConvFPGA->cleanup();\r\n} \r\n</code></pre>\r\n\r\nCompile\r\n\r\n<pre><code>\r\n g++ -std=c++11 -shared MatConvFPGA.cpp -o bin/libMatConvFPGA.so -fPIC -I/home/mater/intelFPGA/17.0/hld/host/include ../common/src/AOCLUtils/opencl.cpp ../common/src/AOCLUtils/options.cpp -L/home/mater/intelFPGA_pro/17.0/hld/board/a10_ref/linux64/lib -L/home/mater/intelFPGA_pro/17.0/hld/host/linux64/lib -Wl,--no-as-needed -lalteracl -laltera_a10_ref_mmd -lelf -I../common/inc -O2 -D_GLIBCXX_USE_CXX11_ABI=0\r\n</code></pre>\r\n\r\n2. Add libMatConvFPGA.so into TensorFlow source code folder\r\n* Create new sub folder \u201cmatconv_fpga\u201d\r\n* Copy MatConvFPGA.h & libMatConvFPGA.so into \u201cmatconv_fpga\u201d folder\r\n* Create new file as \u201cBUILD\u201d for TensorFlow in \u201cmatconv_fpga\u201d folder and edit as the following\r\n\r\n<pre><code>\r\ncc_library(\r\n\tname=\"MatConvFPGA\",\r\n\tsrcs = [\"libMatConvFPGA.so\"],\r\n\thdrs = glob([\"*.h\"]),\r\n\tvisibility=[\"//visibility:public\"],\r\n)\r\n</code></pre>\r\n\r\n3. Select ops which you would like to modify and add dependency  into  XXX/BUILD  \r\n<--- Example (tensorflow/core/kernels/BUILD)\r\n \r\n<pre><code>\r\ntf_kernel_library(\r\n    name = \"conv_ops\",\r\n....\r\n  deps = [..] + [\"//matconv_fpga:MatConvFPGA\"],\r\n</code></pre>\r\n\r\nThen, you can include MatConvFPGA object in your ops\r\n<--- Example (tensorflow/core/kernels/conv_ops.cc)\r\n\r\n4. Build TensorFlow via Bazel\r\n$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package \r\n$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly."]}, {"number": 13146, "title": "Standalone graph trained using on NCHW data_format giving errors on CPU when testing (running forward pass)", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n     Yes,\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: Binary (.whl file)\r\n- **TensorFlow version (use command below)**: 1.2.0-rc1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:  N/A\r\n- **CUDA/cuDNN version**:\r\nCUDA 8.0.61\r\nCuDNN 5.1.10\r\n- **GPU model and memory**: GeForce GTX 1080\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n I've a standalone graph create by freezing my model/net  which has a few slim.conv2d, slim.max_pool2d operations defined with NCHW data format.  I've used freeze_graph.py utility to create the stand alone graph using a trained checkpoint (trained using NCHW format). When I tested the graph (ran forward pass) on a Ubuntu machine with GPU, it was running very well. But when I ran the same code to test (forward pass) with the same stand alone graph a Ubuntu machine that has no GPU but CPU, I got the following errors:\r\n\r\n```\r\n2017-09-18 17:57:48.890761: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Default MaxPoolingOp only supports NHWC.\r\n         [[Node: text_box_300/pool1/MaxPool = MaxPool[T=DT_FLOAT, data_format=\"NCHW\", ksize=[1, 1, 2, 2], padding=\"SAME\", strides=[1, 1, 2, 2], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](text_box_300/conv1/conv1_2/Relu)]]\r\n2017-09-18 17:57:48.892768: E main_textd_test.cc:457] Running model failed: Invalid argument: Default MaxPoolingOp only supports NHWC.\r\n         [[Node: text_box_300/pool1/MaxPool = MaxPool[T=DT_FLOAT, data_format=\"NCHW\", ksize=[1, 1, 2, 2], padding=\"SAME\", strides=[1, 1, 2, 2], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](text_box_300/conv1/conv1_2/Relu)]]\r\n```\r\n\r\nI believe the errors are related to [https://github.com/tensorflow/tensorflow/issues/2660](https://github.com/tensorflow/tensorflow/issues/2660)\r\nIs it possible to modify the stand alone graph that is running well on GPU to make it run successfully on CPU machine ? Is it possible to avoid retraining the model using NHWC format and recreate the standalone model ?\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@petewarden, is there any easy way to handle this. In principle it should be possible to rewrite the graph and transpose all the weight variables so that it can operate in NHWC. Perhaps Grappler could help?", "We don't currently have a way to handle this. In theory it should be possible to write a Graph Transform Tool rule to do the swizzling, or maybe in Grappler, but we'd need to maintain a list of all the ops this applies to. We don't have any plans to work on this in the near term unfortunately, but PRs would be welcome.", "Hi @petewarden ,\r\n\r\nThank you very much for your response. Is this the correct [graph transform tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md) you are referring ? Can you tell me briefly (steps involved) on how to add/write rule to do graph swizzling using the tool or (using Grappler which ever is easier in your opinion) so I can work on it by myself.\r\n\r\nAlso there is no documentation on what is Grappler or how to use Grappler ( at least I didn't find any).  I hope this is the [grappler](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/grappler) tool you are referring to, correct me If I'm wrong. Can you point me if there is any documentation on what is the purpose of Grappler and how to use it. \r\n\r\nThank you.", "@zhangyaobit, this may be a major issue if we enable grappler by default, and there is no way to rewrite the checkpoints and graph to be NCHW.", "Hi @aselle ,\r\n\r\nI agree that it is a major issue if there is no way to rewrite graph and checkpoints from NCHW to NHWC and NHWC to NCHW. The tensorflow (v 1.2) official documentation recommended to train graph NCHW and do inference on NHWC [here](https://www.tensorflow.org/versions/r1.2/performance/performance_guide#use_nchw_image_data_format):\r\n\r\n`The best practice is to build models that work with both NCHW and NHWC as it is common to train using NCHW on GPU, and then do inference with NHWC on CPU.`\r\n", "@tumusudheer, note that \"MaxPoolingOp only supports NHWC\" on the CPU, so could you run NHWC (as opposed to NCHW) if running on CPU?\r\n\r\nI think the checkpoint should be compatible (you can train in NCHW and infer in NHWC), because weights/filter input are always stored in the same format ([filter_height, filter_width, in_channels, out_channels] regardless of whether NCHW or NHWC is used: https://www.tensorflow.org/api_docs/python/tf/nn/conv2d).\r\n\r\n(for your problem, there seems no need to use graph transformation tool or Grappler, you just need to set the format to NHWC if infer on CPU). ", "Hi @zhangyaobit ,\r\n\r\nThank you very much for your response. For running inference on CPU, I created my graph such that all operations (conv2D and MaxPool) take NHWC data instead of NCHW. But when I'm restoring the checkpoint (which I obtained by training the graph using NCHW format on GPUs), I'm getting the following errors:\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [28] rhs shape= [3]\r\n         [[Node: save/Assign_16 = Assign[T=DT_FLOAT, _class=[\"loc:@text_box_300/conv10_box/conv_cls/BatchNorm/moving_mean\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](text_box_300/conv10_box/conv_cls/BatchNorm/moving_mean, save/RestoreV2_16/_23)]]\r\n```\r\nI've attached complete error log in case you need (I've attached the code as well).\r\n\r\n[create_stand_alone.py.txt](https://github.com/tensorflow/tensorflow/files/1335594/create_stand_alone.py.txt)\r\n\r\n\r\nThese are the steps I've followed as part of creating stand alone graph/model to run inference on CPUS:\r\n1. Created graph with all Ops supporting NHWC\r\n2. Restoring the checkpoint with weights (which has been trained on GPU by using graph with NCHW Ops)\r\n3. Save the final graph as .pb file\r\n\r\nI'm working on the [this](https://github.com/tumusudheer/TextBoxes-TensorFlow) model and was able to successfully run inference on GPUs but not on CPUs because of NHWC data format issue\r\n\r\n[error.txt](https://github.com/tensorflow/tensorflow/files/1335632/error.txt)\r\n\r\n\r\nIt seems the cause of the error is tensorflow not able to assign the weights that have been restored from NCHW  graph checkpoint to NHWC graph variables. But when the checkpoint contains weights in the same format irrespective of data_format (as per your previous post), I should not get the error for dimension mis-match while assigning weights to graph variables. Am I missing something here or doing any thing wrong ? Can you please let me know if I'm doing any basic mistake or doing any thing wrong ? I can also provide my checkpoint files in case you need them.\r\n\r\nThanks in advance. \r\n\r\n\r\n\r\n", "What should be the correct shape of text_box_300/conv10_box/conv_cls/BatchNorm/moving_mean? I think it should be a 1d vector of the size number of channels. Could you check if the number of channels should be 28 or 3? \r\n\r\nYou can use this tool to inspect the checkpoint file and find out what shape moving_mean is of: https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/python/tools/inspect_checkpoint.py\r\n\r\nLooks like in the model, the shape of moving_mean is inconsistent with that in the checkpoint, and you need to make some changes accordingly. You will need to debug a bit: you can add \"pdb.set_trace()\" inside batch_norm and check the shape of moving_mean to see how that shape is derived and what change you can make to make the shape consistent with the one in the checkpoint.", "Hi @zhangyaobit ,\r\n\r\nHere is the output of the inspect_checkpoint for all tensors:\r\n[inspect_output.txt](https://github.com/tensorflow/tensorflow/files/1335722/inspect_output.txt)\r\n\r\nFor tensor_name:  text_box_300/conv10_box/conv_cls/BatchNorm/moving_mean\r\n[ 0.0456978   0.01166576  0.01197682]\r\nThe shape is 3.\r\nSimilarly, the shape of text_box_300/conv7/BatchNorm/moving_mean is 19.\r\n\r\nBut the checkpoint was the result of graph trained on GPU with operations taking data in NCHW format. When I created the graph to run inference on CPU (with NHWC data inputs) and restoring from GPU checkpoint, I'm getting the above errors.\r\n Are you saying that when I'm creating the graph to run inference on CPU, the dimensions are not consistent with checkpoint dimensions ?  Should n' t they be different because the checkpoints are the result of Graph created with NCHW operations and I'm trying to restore it with graph created with NHWC operations ?\r\n\r\nAs per your suggestion to making changes, do I need to make changes to the tensors while I'm creating the graph to run inference on CPU and take NHWC data format  or do I need to make changes after creating the graph and while restoring weights ?\r\n\r\nThank you very much for taking your time to assist me with this issue.\r\n", "Sure, my pleasure.\r\n\r\nI could be mistaken, but my understanding is that using NCHW or NHWC doesn't affect the format of variables, so the checkpoint should be the same regardless of whether NCHW or NHWC is used (You can verify this by comparing the checkpoints respectively trained with NCHW and NHWC).\r\n\r\nAnd when you restore the checkpoint, you load the exactly same the variables regardless whether NCHW or NHWC is used (note that NCHW is the format of data input, not the filter input of Conv, filter input is a variable and is always stored in the same format [filter_height, filter_width, in_channels, out_channels] regardless of whether NCHW or NHWC is used, that's why checkpoint is not affected). However, when you restore the checkpoint, I think somewhere in your model, you need to set the right shape of moving_mean (in addition to setting the format to NHWC).\r\n\r\nCould you add a breakpoint (pdb.set_trace()) in batch_norm, and examine why the shape of moving mean is 28 instead of 3 for text_box_300/conv10_box/conv_cls/BatchNorm/moving_mean?", "Hi @zhangyaobit,\r\n\r\nI guess you want me to add this line `import ipdb; ipdb.set_trace()` in the batch_normalization function ? May I know where to add this line to debug batch_norm ? I'm using tf.contrib.slim.conv2d layers with batch_norm_params.\r\n\r\nHere is my conv2D function:\r\n```\r\ndef conv2d(inputs, out, kernel_size, scope,stride=1,activation_fn=tf.nn.relu, \r\n\t\t\tpadding = 'SAME', use_batch=False, batch_norm_params={}, rate = 1):\r\n\tif use_batch:\r\n\t\tnet = slim.conv2d(inputs, out, kernel_size, stride=stride ,scope=scope, normalizer_fn=slim.batch_norm, \r\n\t\t\t  normalizer_params=batch_norm_params, activation_fn=activation_fn ,padding = padding, rate = rate)\r\n\telse:\r\n\t\tnet = slim.conv2d(inputs, out, kernel_size, stride=stride, scope=scope, activation_fn=activation_fn,padding = padding, rate = rate)\r\n\treturn net\r\n\r\n```\r\n\r\nAs the stack trace of my error is referring to tensorflow code at /usr/local/lib/python2.7/dist-packages/tensorflow/python/ , I edited /usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/ayers/normalization.py and added this line  `import ipdb; ipdb.set_trace()` at the end of \r\n`def build(self, input_shape):` function but the execution is not being stopped at the debug pointer and I don't see any stacktrace. \r\n\r\nCan you tell me where should I add pdb.set_trace() line to debug the code ?\r\n\r\nThank you.", "(1)\r\nYou can add it at this line: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L311\r\n\r\nparams_shape is moving_mean's shape: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L366\r\n\r\nAnd params_shape shouldn't be affected by data_format: note that the logic below line 311 always sets the size of params_shape to the number of channels.\r\n\r\n(2)\r\ncould you also check the shape of input, add pdb.set_trace() at this line: https://github.com/tumusudheer/TextBoxes-TensorFlow/blob/master/nets/txtbox_300.py#L209\r\n\r\ninput's shape should be NCHW if data_format is specified as NCHW, and should be NHWC if data_format is NHWC. From a quick look at the code, it seems the code might be missing a transpose to correctly set the shape of input. Could you double check that?\r\n\r\nYou can transpose to NCHW and NHWC using the code below:\r\n    y= tf.transpose(x, [0, 3, 1, 2]) # NHWC to NCHW\r\n    y= tf.transpose(x, [0, 2, 3, 1]) # NCHW to NHWC", "Hi @zhangyaobit,\r\n\r\nSorry for my delay and Thank you very much for going through the TextBoxes code, I really appreciate your help.\r\n\r\n1) I added import pdb; pdb.set_trace() here\r\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L311](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L311)\r\n\r\nHere are the findings:\r\n```\r\n(Pdb) params_shape\r\nTensorShape([Dimension(1024)])\r\n(Pdb) inputs_shape\r\nTensorShape([Dimension(32), Dimension(19), Dimension(19), Dimension(1024)])\r\n\r\n```\r\nparams_shape is not the number of channels but instead it was 1024 which is obviously wrong. Somewhere along the lines in the model, data (shapes) is not being transposed/handled correctly for NHWC.\r\n\r\n2) Also added pdb.set_trace() here:\r\n[https://github.com/tumusudheer/TextBoxes-TensorFlow/blob/master/nets/txtbox_300.py#L209](https://github.com/tumusudheer/TextBoxes-TensorFlow/blob/master/nets/txtbox_300.py#L209)\r\n\r\n```\r\n> /home/sudheer/Flipkart/Research/maneesh/tensorflow_1.2/models/TextBoxes-TensorFlow_2.0/nets/txtbox_300.py(210)text_net()\r\n-> end_points = {}\r\n(Pdb) inputs\r\n<tf.Tensor 'fifo_queue_Dequeue:0' shape=(32, 300, 300, 3) dtype=float32>\r\n\r\n```\r\nThe shape of the input tensor is being set correctly. That is because I'm sending this extra `data_format` param to preprocessing function here.\r\n[https://github.com/tumusudheer/TextBoxes-TensorFlow/blob/master/load_batch.py#L44\r\n](https://github.com/tumusudheer/TextBoxes-TensorFlow/blob/master/load_batch.py#L44\r\n)\r\nThe code is supposed to be \r\n`txt_preprocessing.preprocess_image(image,  glabels, gbboxes, height, width,                                                                                   out_shape,use_whiten=FLAGS.use_whiten,is_training=is_training,data_format=FLAGS.data_format)`\r\n\r\nI guess I need to start debugging the model code to find out the place where dimensions are being set incorrectly.\r\n\r\nAdded:\r\n\r\nJust found one mistake in the model: No matter what my input data_format is , the data_format is always being NHWC here:\r\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L312](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L312)\r\n\r\nI guess in the code here [https://github.com/tumusudheer/TextBoxes-TensorFlow/blob/master/nets/txtbox_300.py#L288](https://github.com/tumusudheer/TextBoxes-TensorFlow/blob/master/nets/txtbox_300.py#L288), batch_norm is being called but data_format is not being passed as input params for batch_norm\r\n\r\nIf I add batch_norm as part of the arg_scope here: [https://github.com/tumusudheer/TextBoxes-TensorFlow/blob/master/nets/txtbox_300.py#L386](https://github.com/tumusudheer/TextBoxes-TensorFlow/blob/master/nets/txtbox_300.py#L386), will the data_format be passed into _fused_batch_norm function ? or do I need to put data_format as part of batch_norm_params here ?\r\n[https://github.com/tumusudheer/TextBoxes-TensorFlow/blob/master/nets/txtbox_300.py#L200](https://github.com/tumusudheer/TextBoxes-TensorFlow/blob/master/nets/txtbox_300.py#L200)\r\n\r\n\r\n\r\n\r\nThank you", "Great to see the progress! Yes, you can pass in data_format as part of batch_norm_params.", "Hi @zhangyaobit ,\r\n\r\nI was able to solve this issue and the main problem is batch_norm is not getting data_format from conv2D. So I had to pass data_format explicitly as param as part of batch_norm_params or need to add slim.batch_norm to the arg_scope.\r\n\r\nIs it not a bug that [slim.conv2d](https://github.com/tensorflow/tensorflow/blob/9ff05e9e7f471a8487cdd8a7bb6fdd554055e2dd/tensorflow/contrib/layers/python/layers/layers.py#L1042) is not passing data_format to batch_norm automatically ?", "Glad the problem is solved.\r\n\r\nCould you double check (print or pdb.set_trace at [slim.conv2d](https://github.com/tensorflow/tensorflow/blob/9ff05e9e7f471a8487cdd8a7bb6fdd554055e2dd/tensorflow/contrib/layers/python/layers/layers.py#L1042)) expected normalizer_params is passed into slim.conv2d by the user to help identify if this is a user's usage issue or a bug of slim.conv2d?\r\n\r\n(We will need to first double check if user is using this function correctly. If the user is using slim.conv2d correctly, but slim.conv2d doesn't function as expected, then it is a bug of slim.conv2d)", "Closing now. Feel free to re-open if needed.", "My resnet101 network is being trained using slim and NCHW format, but it is slower. Epoch time is 17 hours in NHWC format, but using NCHW slows down to 22 hours. It's strange because according to docs, batch_norm, convolution and max_pooling are faster using NCHW. Any thoughts on that?", "\r\n\r\n@chrisrn, I'm closing this one, as the original issue is solved. Please open a separate issue if needed.\r\n\r\nYeah, for resnet101, NCHW should definitely be faster.\r\n\r\nIs there anything non-standard in your version/implemention of resnet101? Have you compare it with the one in tf cnn benchmarks?\r\nhttps://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py", "I am training on benchmarks and I am comparing this resnet with slim resnet. In benchmarks, epoch time using NCHW is 15 hours. In slim, epoch time using NHWC is 17 hours, but using NCHW epoch time is 22 hours.", "There is definitely a problem somewhere.\r\n\r\n(1) Do you have the source code available, so that we can take a look? \r\n\r\n(2) Could you try https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler respectively for (1) slim with NHWC, (2) slim with NCHW, and (3) tf cnn benchmark with NCHW to identify the discrepancies in terms of runtime breakdown?\r\n\r\n(3) Could you double check fused batch norm is on? It should be on by default now. But maybe you are using a older version of TensorFlow which is off by default. See the fused batch norm section here: https://www.tensorflow.org/performance/performance_guide", "I made fused batch norm and now everything is ok, which means that I can run a slim model through benchmark code and finetune also on imagenet! You can close it for now", "@tumusudheer I came across the same problem as you discribed, you said \"So I had to pass data_format explicitly as param as part of batch_norm_params or need to add slim.batch_norm to the arg_scope\", I modify the code as below:\r\n\tbatch_norm_params = {\r\n\t  # Decay for the moving averages.\r\n\t  'decay': 0.9997,\r\n\t  # epsilon to prevent 0s in variance.\r\n\t  'epsilon': 0.001,\r\n\t  'is_training': is_training,\r\n\t  'scale':False,\r\n\t  'fused':True,\r\n\t  'data_format':'NHWC'\r\n\t}\r\nor\r\nwith slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.batch_norm],\r\nall of them doesn't work\uff0ccould you tell me the right way to modify it?\r\n"]}, {"number": 13145, "title": "Fix cmake build issue on Linux", "body": "This fix tries to address the issue raised in #12018 and #13061 where the cmake build on Linux caused error.\r\n\r\nThis fix addressed several issues:\r\n1.  Add grpc_cpp_plugin to the cmake build process. This is needed to address the following error:\r\n```\r\nerror: fatal error: tensorflow/core/debug/debug_service.grpc.pb.h: No such file or directory\r\n```\r\n\r\n2. Fix a missing CMakefile issue for sqlite on Linux\r\n\r\n3. Fix a build error caused by libjpeg_turbo vs jpeg. The build error is temporarily addressed with `LIBJPEG_TURBO_VERSION` used to detect the underlying library.\r\n\r\nThis fix fixes #12018.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Thanks, @yongtang !"]}, {"number": 13144, "title": "No gradient defined for Relu6Grad", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes. MWE below is not a stock example.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary (Miniconda)\r\n\r\n- **TensorFlow version (use command below)**:\r\nprint(tf.GIT_VERSION, tf.VERSION)\r\n('v1.2.0-5-g435cdfc', '1.2.1')\r\n\r\n- **Python version**: \r\n2.7.13\r\n\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n\r\n- **CUDA/cuDNN version**:\r\nCUDA 8.0.61\r\nCuDNN 5.1.10\r\n\r\n- **GPU model and memory**:\r\nGeForce GTX TITAN X\r\nTotal Memory 11.91GiB\r\n\r\n- **Exact command to reproduce**:\r\nSee below.\r\n\r\n### Describe the problem\r\n\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nOne can not apply the `gradient` operator to a `nn.Relu6` more than once.\r\nApplying it once gives a `Relu6Grad` node,\r\napplying it a second time (i.e applying it to the `Relu6Grad`)  throws an error.\r\n\r\nUnless I have messed up my math (quiet possible),\r\nRelu6 is infinitely differentiable, except at 2 points,\r\nalbeit very boring\r\n\r\n - f(x) = relu6(x) \r\n - df/dx = 1 if 0<x<6 else 0\r\n - d2f/dxx = 0\r\n - all further derivatives also 0.\r\n\r\nHow-ever, it is marginally more interesting if one is applying the chain rule to it\r\n\r\n - f(x1,x2) = relu6(x1*x2) \r\n - df/dx1 = x2 if 0<x1*x2<6 else 0\r\n - d2f/dx1x2 = 1 if 0<x1*x2<6 else 0\r\n\r\nThis is a feature request to make the gradient of `Relu6Grad` defined.\r\n\r\n\r\n### Source code / logs\r\n\r\nMWE:\r\n```python\r\n\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\n\r\nx1 = tf.placeholder(tf.float32)\r\nx2 = tf.placeholder(tf.float32)\r\n\r\ny = tf.nn.relu6(x1*x2)\r\nd1 = tf.gradients(y,x1)\r\nd2 = tf.gradients(d1, x2)\r\n```\r\n\r\nErrors on the final line  with\r\n```\r\n---------------------------------------------------------------------------\r\nLookupError                               Traceback (most recent call last)\r\n<ipython-input-7-4250ab355a8b> in <module>()\r\n----> 1 d2 = tf.gradients(d1, x2)\r\n\r\n...usr/lib/python2.7/site-packages/te\r\nnsorflow/python/ops/gradients_impl.pyc in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops,\r\n gate_gradients, aggregation_method)\r\n    512               raise LookupError(\r\n    513                   \"No gradient defined for operation '%s' (op type: %s)\" %\r\n--> 514                   (op.name, op.type))\r\n    515         if loop_state:\r\n    516           loop_state.EnterGradWhileContext(op, before=False)\r\n\r\nLookupError: No gradient defined for operation 'gradients/Relu6_grad/Relu6Grad' (op type: Relu6Grad)\r\n```\r\n", "comments": ["Yes, this feature is not implemented, but it is relatively easy to add. Would you be willing to contribute a pull request. In particular look at\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L388\r\n(for ReluGradGrad). You can follow suite for Relu6.\r\n", "HI, @oxinabox. I believe your mathematical derivation is pretty correct, and implement it in PR #13268 . Please check whether it is right as you proposed if possible.", "@facaiy I'm not sure if it is correct, as I don't know much of the internals of the python client."]}, {"number": 13143, "title": "how can i convert a frozen.pb file to .ckpt file?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Does anyone know how to solve this problem?"]}, {"number": 13142, "title": "ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients", "body": "Hello everyone,\r\nI have error when programming tensorflow:\r\n\r\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(4, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_1:0' shape=(4, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_2:0' shape=(1, 3) dtype=float32_ref>\", \"<tf.Variable 'Variable_3:0' shape=(1, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_4:0' shape=(1, 3) dtype=float32_ref>\", \"<tf.Variable 'Variable_5:0' shape=(1, 3) dtype=float32_ref>\"] and loss Tensor(\"Sum:0\", dtype=float32).\r\n\r\nMy code is here\r\n\r\n\r\n=============================\r\nimport tensorflow as tf\r\n\r\ndef weight_variable(shape):\r\n    initial = tf.truncated_normal(shape, stddev = 0.1)\r\n    return tf.Variable(initial)\r\n\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.1, shape = shape)\r\n    return tf.Variable(initial)\r\n\r\n# Model parameters\r\n#W = tf.Variable([.3], dtype=tf.float32)\r\n#b = tf.Variable([-.3], dtype=tf.float32)\r\n#W=weight_variable([1])\r\n#b=bias_variable([1])\r\nindice= tf.constant([0,1])\r\nsegment_id= tf.constant([0,0,1,1])\r\nW=weight_variable([4,2])\r\nb=bias_variable([4,2])\r\nb_=tf.Variable(tf.zeros([1,3]))\r\nt0=tf.Variable(tf.zeros([1,2]))\r\nt1=tf.Variable(tf.zeros([1,3]))\r\ng=tf.Variable(tf.zeros([1,3]))\r\n\r\n\r\n# Model input and output\r\nx = tf.placeholder(tf.float32)\r\n#linear_model = W * x + b\r\n\r\n#forward transform\r\n\r\nlinear_function = tf.add(tf.matmul(W,x),b)\r\nlinear_function_col_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function), indice[0]))\r\nlinear_function_col_2 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function), indice[1]))\r\nmin_1=tf.reduce_min(tf.segment_max(linear_function_col_1,segment_id))\r\nmin_2=tf.reduce_min(tf.segment_max(linear_function_col_1,segment_id))\r\n#b_=tf.assign(b_,[[min_1,min_2,0]])\r\nb_=tf.assign(b_,[[min_1,min_2,0.0]])\r\nt0=tf.assign(t0,[[min_2,min_1]])\r\ng = tf.nn.softmax(tf.scalar_mul(1000,b_),dim=-1)\r\n#g = tf.nn.softmax(b_)\r\n\r\n#inverse transform\r\n\r\nlinear_function_inv = tf.divide(tf.transpose(tf.transpose(t0)-tf.transpose(b)),W)\r\nlinear_function_inv_col_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function_inv), indice[0]))\r\nlinear_function_inv_col_2= tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function_inv ), indice[1]))\r\nmax_1=tf.reduce_max(tf.segment_min(linear_function_inv_col_1,segment_id))\r\nmax_2=tf.reduce_max(tf.segment_min(linear_function_inv_col_2,segment_id))\r\nt1=tf.assign(t1,[[max_1,max_2,0.0]])\r\n\r\ny = tf.placeholder(tf.float32)\r\n\r\n# loss\r\n#loss = tf.reduce_sum(tf.square(linear_model-y)) # sum of the squares\r\n#loss = tf.reduce_sum(tf.square(g)) # sum of the squares\r\n#loss = tf.reduce_sum(tf.square(y-tf.matmul(g,tf.transpose(t1))))\r\nloss = tf.reduce_sum(y-tf.matmul(g,tf.transpose(t1)))\r\n# optimizer\r\noptimizer = tf.train.GradientDescentOptimizer(0.01)\r\ntrain = optimizer.minimize(loss)\r\n\r\n# training data\r\n#x_train = [1, 2, 3, 4]\r\nx_train_array = tf.constant([0.58975124,0.22815752])\r\nx_train=tf.diag(x_train_array)\r\n#y_train = [0, -1, -2, -3]\r\ny_train=tf.constant([[0.530]])\r\n# training loop\r\ninit = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init) # reset values to wrong\r\nfor i in range(1000):\r\n  sess.run(train, {x: x_train, y: y_train})\r\n\r\n# evaluate training accuracy\r\ncurr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\r\nprint(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))\r\n==================================================\r\n\r\nHope to get your help.\r\nThank you\r\n\r\n\r\n\r\n\r\n", "comments": ["Hello, please help me", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13141, "title": "label image run slower and slower?", "body": "Hello ,Everyone\r\nIs there anybody who had ever run the code label_image.py in tensorflow/tensorflow/examples/label_image/label_image.py\r\nI have modify it to run on a dataset and read and calssify image one by one,and as the number of images goes,the speed is slower and slower,at first,that's about ten images per second,and when the number of image goes to 1000,the time is about 7s,Incredibly!and  I find the problem is in the function  read_tensor_from_image_file in label_image.py and this part is read and preprocess images, so what's the matter?and I want to know how to speed up?and how to modify the code so as to making it run for batches ?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@shivaniag I guess it's a bug about memorry leak,Please make sure!", "i got the same problem .when predit image more and more. the memory will leak"]}, {"number": 13140, "title": "[GDR] Eliminate several unnecessary sync barriers", "body": "Following the plan I mentioned in https://github.com/tensorflow/tensorflow/pull/12361#issuecomment-323101744, I have refactored out the sync wrapper around copy between CPU and GPU.\r\n\r\nNow the user need to supply a `StatusCallback` when calling `RemoteMemoryManager:: TransportOptionsFromTensor ` and `RemoteMemoryManager::TensorFromTransportOptions`, in order to prepare for the potential CPU-GPU tensor transfer.", "comments": ["Can one of the admins verify this patch?", "@poxvoculi what do you think?", "Jenkins, test this please."]}]