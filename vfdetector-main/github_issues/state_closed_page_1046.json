[{"number": 21920, "title": "BeamSearchDecoder bug\uff1f", "body": "it seems that the beam search not end with 'eos' symbol. So, there are same sentences generated by beam search. who can slove this problem?\r\ncode:\r\n`                    inference_decoder = BeamSearchDecoder(\r\n                        cell=self.decoder_cell,\r\n                        embedding=embed_and_input_proj,\r\n                        start_tokens=start_tokens,\r\n                        end_token=end_token,\r\n                        initial_state=self.decoder_initial_state,\r\n                        beam_width=self.beam_width,\r\n                        output_layer=self.decoder_output_projection,\r\n                    )`\r\n\r\nbeam_size=5,result:\r\nsentence1\uff1a \u7ed9\u60a8\u5e26\u6765\u4e0d\u597d\u7684\u4f53\u9a8c\u4e86\uff0c\u5c0f\u66e6\u4e5f\u662f\u5341\u5206\u62b1\u6b49\u7684 eos unk unk unk unk unk\r\nsentence2\uff1a \u7ed9\u60a8\u5e26\u6765\u4e0d\u597d\u7684\u4f53\u9a8c\u4e86\uff0c\u975e\u5e38\u62b1\u6b49 eos  eos  eos eos eos eos eos eos eos eos eos \r\nsentence3\uff1a \u7ed9\u60a8\u5e26\u6765\u4e0d\u597d\u7684\u4f53\u9a8c\u4e86\uff0c\u975e\u5e38\u62b1\u6b49 eos eos eos eos eos eos eos eos eos eos unk\r\nsentence4\uff1a \u7ed9\u60a8\u5e26\u6765\u4e0d\u597d\u7684\u4f53\u9a8c\u4e86\uff0c\u975e\u5e38\u62b1\u6b49 eos eos eos eos eos eos eos unk unk unk unk \r\nsentence5\uff1a \u7ed9\u60a8\u5e26\u6765\u4e0d\u597d\u7684\u4f53\u9a8c\u4e86\uff0c\u975e\u5e38\u62b1\u6b49 eos unk unk unk unk unk unk unk unk unk unk\r\n\r\nabove. sentence2,sentence3,sentence4,sentence5 is alike.\r\n", "comments": ["Any comments? @bmabey ", "@georgesterpu can you have a look?", "Hello @weiwancheng,\r\n\r\nFrom your post it is unclear whether you are using a well tested framework or your own. Could you please give us more details ?\r\n\r\nI have to assume that you are looking at the outputs (`predicted_ids`) of the `seq2seq.dynamic_decode` method, instantiated with the beam search decoder you mentioned, and that you translated the numerical ids back into symbols from your vocabulary. `dynamic_decode` will also return you the final sequence lengths. Looking at my own results, they are typically the lengths up to the first occurrence of the `eos` token (but I can also see that the decoder sometimes predict a few more extra characters when `eos` is fed back as input)). In your case, the lengths should be [9, 4, 4, 4, 4], and you can ignore the rest.\r\n\r\nUnfortunately I did not study the implementation of this function to help you further. I assume it depends a lot on the task that you are trying to solve, and also on the checkpoint you are loading (is it a well trained model ?). At least in my case, all the beams appear to be unique (just sampling a few of them, nothing rigorous). I have also seen that the inputs and the outputs of the decoder have to be properly formatted in order to obtain sound results (e.g. where you place the GO and EOS in the sequence, ensuring that the provided sequence length also includes these symbols). If it helps you, I will open-source our seq2seq ASR projects this week to see an example, yet the `nmt` tutorial might be an even better resource.\r\n\r\n\r\n", "@georgesterpu Thank you for your reply.\r\nYes,I have done something as your said. About beam search API, the main part in my code as follow :\r\n`inference_decoder= \r\nBeamSearchDecoder( cell=self.decoder_cell, embedding=embed_and_input_proj, start_tokens=start_tokens, end_token=end_token, initial_state=self.decoder_initial_state, beam_width=self.beam_width, output_layer=self.decoder_output_projection, )` \r\n....\r\n`(self.decoder_outputs_decode,self.final_state,_ ) = (seq2seq.dynamic_decode(decoder=inference_decoder,output_time_major=self.time_major,maximum_iterations=max_decode_step,swap_memory=True,scope=decoder_scope))`\r\n\r\n\r\nI feel very strange, I did not find any problems. This problem bothering me many days. I have trained the model for other tasks, but this problem still existed. There were still repeated sentences in the prediction, and the encounter with eos does not stop. But It stopped when the length is set length. \r\nAnyway,Thank you very much. you are very kindful. I am very looking forward to your projects to open.", "@weiwancheng Here it is: https://github.com/georgesterpu/Sigmedia-AVSR", "@georgesterpu OK,Thanks", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Marking this as 'fixed'.", "> \r\n> \r\n> @georgesterpu Thank you for your reply.\r\n> Yes,I have done something as your said. About beam search API, the main part in my code as follow :\r\n> `inference_decoder= BeamSearchDecoder( cell=self.decoder_cell, embedding=embed_and_input_proj, start_tokens=start_tokens, end_token=end_token, initial_state=self.decoder_initial_state, beam_width=self.beam_width, output_layer=self.decoder_output_projection, )`\r\n> ....\r\n> `(self.decoder_outputs_decode,self.final_state,_ ) = (seq2seq.dynamic_decode(decoder=inference_decoder,output_time_major=self.time_major,maximum_iterations=max_decode_step,swap_memory=True,scope=decoder_scope))`\r\n> \r\n> I feel very strange, I did not find any problems. This problem bothering me many days. I have trained the model for other tasks, but this problem still existed. There were still repeated sentences in the prediction, and the encounter with eos does not stop. But It stopped when the length is set length.\r\n> Anyway,Thank you very much. you are very kindful. I am very looking forward to your projects to open.\r\n\r\nI meet the same problem. How do you solve it?", "@peinbill sorry, i have not  solved this problem. i only change to use greedy search. This problem seems that tensorflow's bug.  if you make progress on this problem, we can communicate more.", "> \r\n> \r\n> @peinbill sorry, i have not solved this problem. i only change to use greedy search. This problem seems that tensorflow's bug. if you make progress on this problem, we can communicate more.\r\n\r\nall right\uff0cthanks a lot. "]}, {"number": 21919, "title": "Error: Can not allocate memory for the given inputs: external/tensorflow/tensorflow/contrib/lite/kernels/sub.cc:48 NumDimensions(input1) != NumDimensions(input2) (4 != 0)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nI only replace the model we retained refer to tensorflow-for-poets.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 14.04.3 LTS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nLG G4\r\n- **TensorFlow installed from (source or binary):binary\r\n- **TensorFlow version (use command below):1.7.1\r\n- **Python version:2.7.6\r\n- **Bazel version (if compiling from source): NA\r\n- **GCC/Compiler version (if compiling from source): NA\r\n- **CUDA/cuDNN version: NA\r\n- **GPU model and memory: NA\r\n- **Exact command to reproduce:\r\n\r\n\r\n### Describe the problem\r\nHi,\r\nI retrained a model refer to tensorflow-for-poets, the retrain command is:\r\n\r\n> python -m scripts.retrain \\\r\n  --bottleneck_dir=tf_files/bottlenecks \\\r\n  --model_dir=tf_files/models/ \\\r\n  --summaries_dir=tf_files/training_summaries/mobilenet_v2_140_224 \\\r\n  --output_graph=tf_files/retrained_graph.pb \\\r\n  --output_labels=tf_files/retrained_labels.txt \\\r\n  --tfhub_module=https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/2 \\\r\n  --how_many_training_steps=8000 \\\r\n  --learning_rate=0.005 \\\r\n  --flip_left_right True \\\r\n  --random_crop=10 \\\r\n  --random_scale=10 \\\r\n  --random_brightness=10 \\\r\n  --image_dir=tf_files/tf_Images\r\n\r\nretrained_graph.pb works fine in our Android APP.\r\nThen I converted \".pb\" to \".lite\" with this command:\r\n\r\n> toco \\\r\n  --input_file=tf_files/retrained_graph.pb \\\r\n  --output_file=tf_files/retrained_graph.lite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --input_shape=1,224,224,3 \\\r\n  --input_array=Placeholder \\\r\n  --output_array=final_result \\\r\n  --inference_type=FLOAT \\\r\n  --input_data_type=FLOAT\r\n\r\nBut retrained_graph.lite **can not** work in our Android APP(refer to tensorflow-for-poets-2\\android\\tflite), the error is:\r\n\r\n> 08-23 18:15:33.202  2457  2475 E AndroidRuntime: java.lang.NullPointerException: Can not allocate memory for the given inputs: external/tensorflow/tensorflow/contrib/lite/kernels/sub.cc:48 NumDimensions(input1) != NumDimensions(input2) (4 != 0)\r\n08-23 18:15:33.202  2457  2475 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n08-23 18:15:33.202  2457  2475 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:95)\r\n08-23 18:15:33.202  2457  2475 E AndroidRuntime: \tat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:113)\r\n08-23 18:15:33.202  2457  2475 E AndroidRuntime: \tat org.tensorflow.lite.Interpreter.run(Interpreter.java:94)\r\n\r\n**Please help to analyze that why this error occurs? Thanks!**", "comments": ["having the same issue; what was the solution?"]}, {"number": 21918, "title": "Tensorflow MKL build on macOS segfaults on kernel_tests:map_dataset_op_test", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS, 10.10.5\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: Doesn't really matter. 2.7/3.5/3.6\r\n- **Bazel version (if compiling from source)**: 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**: Clang 4.0.1\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: Build with bazel --config=mkl\r\n\r\n### Describe the problem\r\nBuild tensorflow on macOS with mkl enabled. 4/9xx tests fail. One of them is a sefgault.\r\n\r\n### Source code / logs\r\n```\r\nFAILED (failures=1)\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\nExecuting tests from //bazel_pip/tensorflow/python/data/kernel_tests:map_dataset_op_test\r\n-----------------------------------------------------------------------------\r\n2018-08-24 05:45:39.432026: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n2018-08-24 05:45:39.433249: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\r\n...2018-08-24 05:45:40.202328: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at resource_variable_ops.cc:391 : Not found: Resource localhost/counter/N10tensorflow3VarE does not exist.\r\n.........*** Received signal 11 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n0   libtensorflow_framework.so          0x0000000114e4fa04 _ZN10tensorflow7testing19SafePrintStackTraceEv + 68\r\n1   libtensorflow_framework.so          0x0000000114e4f970 _ZN10tensorflow7testingL17StacktraceHandlerEiP9__siginfoPv + 144\r\n2   libsystem_platform.dylib            0x00007fff896c2f1a _sigtramp + 26\r\n3   ???                                 0x0000000000000000 0x0 + 0\r\n4   libtensorflow_framework.so          0x0000000115151b86 _ZN5nsync18nsync_cv_broadcastEPNS_11nsync_cv_s_E + 70\r\nSegmentation fault: 11\r\n```\r\n", "comments": ["@nehaljwani I'm sorry, but https://www.tensorflow.org/performance/performance_guide#tensorflow_with_intel%C2%AE_mkl_dnn\r\nhas a note saying \"MKL was added as of TensorFlow 1.2 and currently only works on Linux\". \r\n\r\nI'll mark \"community support\".", "FWIW, I didn't see this segfault when I built tensorflow 1.9.0 with mkl on macOS.", "I don't see this problem anymore with v1.11.0 (mkl build)", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you."]}, {"number": 21917, "title": "Bazel Error When Installing tf-gpu or tf-cpu 1.3 from source. ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Not Custom Code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 LTS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: tf-cpu/tf-gpu 1.3\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc-5\r\n- **CUDA/cuDNN version**: 9.0.176/ 7.1.3\r\n- **GPU model and memory**: GeForce GTX 1080 ti - 11GB\r\n- **Exact command to reproduce**: \r\nbazel build --config=opt --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package \r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI have CUDA 9.0.176 and CuDNN 7.1.3 installed. To use tensorflow with gpu I have to install tf from Source as tf 1.3 does not support mentioned CUDA versions( I need tf 1.3 and not other higher versions). I am able to configure the installation but I get this error when trying to build the pip package from bazel. \r\n\r\nbazel build --config=opt --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package \r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nERROR: /home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'.\r\nERROR: /home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'.\r\nERROR: /home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/local_config_sycl/sycl/BUILD:4:1: file 'platform' was not correctly loaded. Make sure the 'load' statement appears in the global scope in your file\r\nERROR: /home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/local_config_sycl/sycl/BUILD:6:1: file 'platform' was not correctly loaded. Make sure the 'load' statement appears in the global scope in your file\r\nERROR: /home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):\r\n\tFile \"/home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/local_config_sycl/sycl/BUILD\", line 27\r\n\t\tcc_library(name = \"syclrt\", srcs = [sycl_libr...\")], <3 more arguments>)\r\n\tFile \"/home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/local_config_sycl/sycl/BUILD\", line 30, in cc_library\r\n\t\tsycl_library_path\r\nname 'sycl_library_path' is not defined\r\nERROR: /home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/protobuf/BUILD:460:1: Traceback (most recent call last):\r\n\tFile \"/home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/protobuf/BUILD\", line 460\r\n\t\tcc_proto_library(name = \"cc_test_protos\", srcs = (L...), <4 more arguments>)\r\n\tFile \"/home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/protobuf/protobuf.bzl\", line 248, in cc_proto_library\r\n\t\tcc_libs += [default_runtime]\r\ntrying to mutate a frozen object\r\nERROR: /home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/protobuf/BUILD:761:1: Traceback (most recent call last):\r\n\tFile \"/home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/protobuf/BUILD\", line 761\r\n\t\tpy_proto_library(name = \"python_specific_test_pro...\", <6 more arguments>)\r\n\tFile \"/home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/protobuf/protobuf.bzl\", line 374, in py_proto_library\r\n\t\tpy_libs += [default_runtime]\r\ntrying to mutate a frozen object\r\nERROR: /home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/local_config_sycl/sycl/BUILD:39:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '@local_config_sycl//sycl:sycl'\r\nERROR: /home/vijay/speech_tts/multitacotron/tensorflow/third_party/eigen3/BUILD:20:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//third_party/eigen3:eigen3'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 13.227s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (81 packages loaded)\r\n    currently loading: tensorflow/cc\r\n \r\n**This my tf source configuration results:** \r\n\r\n$ ./configure\r\nYou have bazel 0.16.1 installed.\r\nPlease specify the location of python. [Default is /home/vijay/speech_tts/multitacotron/multitaco/bin/python]: \r\nFound possible Python library paths:\r\n  /home/vijay/speech_tts/multitacotron/multitaco/lib/python3.5/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/vijay/speech_tts/multitacotron/multitaco/lib/python3.5/site-packages]\r\n\r\nUsing python library path: /home/vijay/speech_tts/multitacotron/multitaco/lib/python3.5/site-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N] n\r\nNo MKL support will be enabled for TensorFlow\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] y\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] n\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n\r\nNo XLA JIT support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] n\r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] n\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N] n\r\nnvcc will be used as CUDA compiler\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 9.0.176\r\nPlease specify the location where CUDA 9.0.176 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc-5\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 7.1.3\r\nPlease specify the location where cuDNN 7.1.3 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/lib/x86_64-linux-gnu\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"6.1,6.1,6.1,6.1\"]: \r\nDo you wish to build TensorFlow with MPI support? [y/N] n\r\nMPI support will not be enabled for TensorFlow\r\nConfiguration finished\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nThings that I have tried: \r\n\r\n1) Different versions of Bazel: 0.8.0, 0.9.0, 0.11.1, 0.16.1\r\n2) Cleared the cache before configuration: rm -rf ~/.cache/*\r\n3) Tried Configuring tf-CPU instead of tf-gpu to check if it works but it doesn't. Gives me the same error. \r\n4) this : https://stackoverflow.com/questions/47688252/tensorflow-trying-to-mutate-a-frozen-object-bazel\r\n\r\nI am not really sure if it is a tensorflow issue but nothing seems to be working.\r\n\r\n Any help is appreciated. Thanks! ", "comments": ["Could you try a newer version?", "Nagging Assignee @drpngx: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@vijaysumaravi  - Hi, is this still an issue with the newer version ?", "Closing this issue due to staleness.\r\nPlease use the latest version of TensorFlow and build again. Feel free to open a [new issue](https://github.com/tensorflow/tensorflow/issues/new/choose) if the problem still persists. Thanks!"]}, {"number": 21916, "title": "Fix batch normalization in canned DNN Estimators", "body": "This PR fixes the implementation of batch normalization in canned DNN estimators.  Batch normalization should be applied after the inputs but before the activation function.", "comments": ["This is a fix for issue #21915.", "@lukmanr can you address @ispirmustafa's comments?", "Hi @ispirmustafa, the PR is updated per your suggestions.  I have tests for both options, apply_before_activation=False and apply_before_activation=True.", "Hi @ispirmustafa, backwards compatibility is enforced in the _dnn_logit_fn_builder method.  All the cases of batch_norm=False, batch_norm=True and batch_norm=_BatchNormOptions are now covered in the dnn tests, especially since I reverted to make batch_norm=False be the default in each of the canned estimator __init__ functions. ", "Thanks - comment adjustments done.", "@lukmanr gentle ping to rebase your branch ", "I'm going to go ahead and close this PR due to lack of activity, If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 21914, "title": "Read tensor from open cv numpy array with preprocessing for inference", "body": "`label_image.py` implementation for inference of an image works great for images on disk. But i have a case wherein I am reading a streaming video from a webcam and I would like to run inference on each image frame to detect the object in the camera feed.\r\n\r\nCurrently [label_image.py][1] only accepts an image on disk and using `read_tensor_from_image_file` converts it into a Tensor. How do i get a Tensor with the necessary pre-processing as being done in `read_tensor_from_image_file` from the Open CV image frame that i have in memory?\r\n\r\n  [1]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/label_image.py#L38", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 21913, "title": "TensorFlow License is missing C-API Tarballs / Zips", "body": "### System information\r\nN/A\r\n\r\n### Describe the problem\r\nLICENSE contained in `https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-${TF_TYPE}-${OS}-x86_64-1.10.1.tar.gz`  linked [here](https://www.tensorflow.org/install/install_c) is missing TensorFlow [license](https://github.com/tensorflow/tensorflow/blob/master/LICENSE). \r\n\r\n### Source code / logs\r\nDownload https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.10.1.tar.gz. \r\nExamine LICENSE (located at include\\tensorflow\\c\\LICENSE).\r\nThis file contains all the licenses for 3rd party components but is missing the LICENSE for TensorFlow itself.\r\nThe problem appears to be with the packaging code [here](https://github.com/tensorflow/tensorflow/blob/7df18e0e6f8629dd097a7b71845d9e13d926944a/tensorflow/tools/lib_package/BUILD).\r\n\r\nWe'd like to understand what the LICENSE is for these `.tar.gz` files, does it permit redistribution?  Also it would be good to have the files updated to contain whatever LICENSE is appropriate.\r\n\r\nThanks! ", "comments": ["Assigning this to Asim (since IIRC he created the packaging for this library) and Martin (for a definitive answer on the licensing question).", "The main TF license should be a part of that archive. Thanks for catching this. ", "I also noticed that some of the other zips/tar-balls on  https://storage.googleapis.com/tensorflow/libtensorflow exhibit the same problem.  For example libtensorflow_jni-*."]}, {"number": 21912, "title": "[Intel MKL] Using default CPU allocator for small allocations in MklC\u2026", "body": "\u2026PUAllocator\r\n\r\nThis PR adds support to use default CPU allocator for handling small-size\r\nallocations. We found that BFC allocator does not do well on small allocations,\r\nbut is good for large allocations.", "comments": ["@penpornk Thanks for review. I have addressed all of your review comments, expect one for which I have my reply. Pls check.", "@penpornk Thanks for review!", "@nhasabni would you please help to fix the clang-format issues?\r\nThanks.", "@aaroey I have fixed them. I did run cpplint, but clang format issues seem to be not captured by cpplint. Which utility should I run for clang format check?", "@nhasabni thanks, normally I would just run clang-format, but if sometimes it cannot catch all of them we still have to fix them manually.", "Hi, not sure what the build error is. @penpornk thanks for review.", "Hi,\r\nI didn't notice this before.  This change may be slightly counter-productive in some situations, for example where RDMA networking is in use and the SubAllocator does registration/deregistration on every alloc/free.  A better approach might be to fix BFCAllocator so that its performance is better on small allocations.  @nhasabni do you have any insight into what gives rise to the performance difference you're seeing?   ", "@poxvoculi Based on my initial analysis I found that BFC allocator implementation currently uses single shared lock and long critical sections, and does not do well when the number of concurrent alloc/dealloc is quite high. BFC allocator offers decent improvement for large-sized allocations and a small number of concurrent allocs/deallocs.", "Thanks, I've opened up an internal bug to look into this.  Meanwhile I think we'd be open to a contribution in this area."]}, {"number": 21911, "title": "No way to run it, try py 3.6.5 py 3.6.6 py 3.7.0  32 and 64 from pip and from sourse all stuff in PATH No module named '_pywrap_tensorflow' anyway help me plz WIN 10 how it could be the most powerful library and so many people couldnt afford it", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). **Please provide all the information it asks**. Thank you.\r\n\r\nThough, from a cursory look at the error message provided, it seems you're using Windows. And there seems to be some installation issue where it can't find the native library. Please see Windows installation instructions at https://www.tensorflow.org/install/install_windows"]}, {"number": 21910, "title": "No way to run it, try py 3.6.5 py 3.6.6 py 3.7.0 all stuff in PATH ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Nagging Assignee @robieta: It has been 33 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you.\r\n"]}, {"number": 21909, "title": "ppc64le: //tensorflow/contrib/distributions/python/kernel_tests/util:correlation_matrix_volumes_test times out", "body": "Please assign this issue to me\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master branch from 8/27 (Last commit 514f65a0cab6fb98bba6d69904ba930ff1c46247)\r\n- **Python version**: both 2.7 and 3.6\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0, 7\r\n- **GPU model and memory**: 4 V100 GPUs with 16 GB of memory each\r\n- **Exact command to reproduce**:\r\nbazel test --config=cuda --test_tag_filters=-no_oss,-oss_serial,-no_gpu,-benchmark-test --test_timeout 300,450,1200,3600 --local_test_jobs=4 --test_output=errors --build_tests_only //tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/contrib/distributions/python/kernel_tests/util:correlation_matrix_volumes_test\r\n\r\n### Describe the problem\r\ncorrelation_matrix_volumes_test times out after 450 seconds on ppc64le. On x86 the test finishes around 420 seconds. If I increase the timeouts the test finishes about 920 seconds.\r\n\r\nI believe tensorflow/control/distributions is being moved to https://github.com/tensorflow/probability/, as of now this test doesn't exist in tensorflow/probability so this issue might be mute if the test is being removed.\r\n\r\n### Source code / logs\r\n```\r\n==================== Test output for //tensorflow/contrib/distributions/python/kernel_tests/util:correlation_matrix_volumes_test:\r\nRunning test /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/python/kernel_tests/util/correlation_matrix_volumes_test.runfiles/org_tensorflow/tensorflow/contrib/distributions/python/kernel_tests/util/correlation_matrix_volumes_test  on GPU 0\r\n2018-08-27 20:33:54.300180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties:\r\nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\r\npciBusID: 0004:04:00.0\r\ntotalMemory: 15.75GiB freeMemory: 15.34GiB\r\n2018-08-27 20:33:54.300242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-08-27 20:33:54.581448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-27 20:33:54.581497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0\r\n2018-08-27 20:33:54.581505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N\r\n2018-08-27 20:33:54.581977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\n2018-08-27 20:33:54.799090: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x3fc7c590\r\n.2018-08-27 20:38:37.918692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-08-27 20:38:37.918753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-27 20:38:37.918764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0\r\n2018-08-27 20:38:37.918808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N\r\n2018-08-27 20:38:37.919272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\n.2018-08-27 20:39:40.471076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-08-27 20:39:40.471149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-27 20:39:40.471160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0\r\n2018-08-27 20:39:40.471212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N\r\n2018-08-27 20:39:40.471724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\n.2018-08-27 20:41:13.685115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-08-27 20:41:13.685149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-27 20:41:13.685160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0\r\n2018-08-27 20:41:13.685202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N\r\n2018-08-27 20:41:13.685628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\nTerminated\r\n================================================================================\r\nTarget //tensorflow/contrib/distributions/python/kernel_tests/util:correlation_matrix_volumes_test up-to-date:\r\n  bazel-bin/tensorflow/contrib/distributions/python/kernel_tests/util/correlation_matrix_volumes_test\r\nINFO: Elapsed time: 921.644s, Critical Path: 754.35s\r\nINFO: 3088 processes: 3088 local.\r\nINFO: Build completed, 1 test FAILED, 3090 total actions\r\n//tensorflow/contrib/distributions/python/kernel_tests/util:correlation_matrix_volumes_test TIMEOUT in 465.0s\r\n  /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/testlogs/tensorflow/contrib/distributions/python/kernel_tests/util/correlation_matrix_volumes_test/test.log\r\n\r\n```", "comments": ["Contrib is going away altogether in TensorFlow 2.0, so this problem should resolve itself in the coming months regardless. Is it sufficient to not run that test for you for now?", "The original intention of asking this to be assigned to me was so I could investigate why it was slower on Power then x86. But, I have a backlog of defects to look at and if they code isn't going to be ported over to tensorflow/probability then it can probably be closed. ", "Ah, I see-- @jvdillon , is this code that will move to TFP?", "This code needs to get moved into TFP. I'd like to assign to https://github.com/axch but apparently we need to tweak his github memberships.", "Great, thanks. Can you also add to @martinwicke 's [chart of contrib migrations](https://github.com/tensorflow/community/pull/18/files)?", "distributions is already on there. Most of that move is complete, Alexey\ncan move this code as it corresponds to lkj, which is already under\ntfp/distributions.\n\nOn Tue, Sep 18, 2018 at 1:49 PM Karmel Allison <notifications@github.com>\nwrote:\n\n> Great, thanks. Can you also add to @martinwicke\n> <https://github.com/martinwicke> 's chart of contrib migrations\n> <https://github.com/tensorflow/community/pull/18/files>?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21909#issuecomment-422484055>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AVJZI9bj9XAiIdjS3vsog9twDwTXmlVOks5ucTIzgaJpZM4WOeGj>\n> .\n>\n", "This test is now gone from TF."]}, {"number": 21908, "title": "import tensorflow --> Segmentation fault", "body": "OS: CentOS 6.2\r\nTensorflow installed from source.\r\nTensorflow version: 1.10\r\nPython version: 2.7\r\nBazel version: 0.16.0\r\nGCC version: 6.3.0\r\nCUDA 8.0\r\ncuDNN: 6.0\r\nGPU: GeForce GTX 770\r\n\r\nHi,\r\nI have installed tensorflow with the following commands:\r\n\r\n./configure\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\npip2.7 install /tmp/tensorflow_pkg/tensorflow-1.4.0-cp27-cp27m-linux_x86_64.whl \r\n\r\nAnd when i tried to import it:\r\nimport tensorflow\r\nSegmentation fault\r\n\r\nThanks", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 18 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 21907, "title": "Update Docker description to state CPU requirements", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: binary (Docker tensorflow/tensorflow)\r\n- **TensorFlow version (use command below)**: 1.10.1 (from core/public/version.h)\r\n- **Python version**:2.7.12\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: \r\n\\# docker run -it tensorflow/tensorflow bash\r\nroot@xxxxxxx:/notebooks# python2.7 -c \"import tensorflow as tf\"\r\nIllegal instruction\r\n\r\n### Describe the problem\r\nThis is a request to update documentation to include minimal system requirements, especially the description for Docker images that include prebuilt binaries.\r\n\r\nFor example, the main Docker image on DockerHub (tensorflow/tensorflow, 19M downloads) [currently does not operate on CPU's without AVX support](https://github.com/tensorflow/tensorflow/issues/17411).  This includes CPUs sold as recently as 4 years ago.\r\n\r\nThis requirement should be stated in the Docker image description displayed in DockerHub.  Perhaps also include a link to an alternative docker image that users with non-AVX CPUs may utilize to build from source.\r\n\r\n### Source code / logs\r\nn/a\r\n", "comments": ["Also, you can indicate the requirement on the docker image:\r\nhttps://docs.docker.com/registry/spec/manifest-v2-2/#example-manifest-list\r\nThen users will be notified as soon as trying to pull/execute the image that their platform is insufficient and also the reason why it is.", "@dpotter-intoto Is this still an issue? Thanks!", "Duplicate of #19584.\r\n\r\nThis is a general issue with TensorFlow's official packages, not the Docker images specifically.", "Yes, this is still an issue. Neither\nhttps://hub.docker.com/r/tensorflow/tensorflow nor\nhttps://www.tensorflow.org/install mention that Tensorflow requires a CPU\nwith AVX support.\n\nOn Fri, May 17, 2019 at 2:09 PM Vishnuvardhan Janapati <\nnotifications@github.com> wrote:\n\n> @dpotter-intoto <https://github.com/dpotter-intoto> Is this still an\n> issue? Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21907?email_source=notifications&email_token=AFKGVPXT2KSNJSSZTAGWJ3LPV4NIDA5CNFSM4FRZRRWKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVV3OZA#issuecomment-493598564>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFKGVPWGQIDVIAM4KRLGEITPV4NIDANCNFSM4FRZRRWA>\n> .\n>\n", "I have also added a quick note to the Docker hub description for the TensorFlow images.", "> I have also added a quick note to the Docker hub description for the TensorFlow images.\r\n\r\nWhy didn't you do it the official way? https://docs.docker.com/registry/spec/manifest-v2-2/#example-manifest-list"]}, {"number": 21906, "title": "Update README.md", "body": "", "comments": []}, {"number": 21905, "title": "Disable GPU test for scatter_add_ndim_op_test", "body": "As scatter_add_ndim doesn't have implementation for GPU, the\r\ntest needs to be excluded from GPU test to prevent it from failing.\r\nCurrently fails on both x86_64 and ppc64le.\r\nFixes #21833", "comments": []}, {"number": 21904, "title": "Simple memory placement optimizations not performed when targeting TPUs", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.9\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nI'm playing with compiling Julia code to TF graphs for TPU offload. At the moment, I have the two functions:\r\n```\r\nf(X, Y) = Y * (X + c_1*X) * (X + c_2*X) * (X + c_3*X) * (X + c_4*X) * (X + c_5*X) * (X + c_6*X) * (X + c_7*X) * (X + c_8*X)\r\ng(X, Y) = Y * (X + c_1*X) * (X + c_2*X) * (X + c_3*X) * (X + c_4*X) * (X + c_5*X) * (X + c_6*X) * (X + c_7*X) * (X + c_8*X) * (X + c_9 * X)\r\n```\r\n(Note the `*` is matrix-multiply between two matrices). Now, when I run (the TF graph resulting from the compilation of) these two functions against a cloud TPU (Tensorflow version 1.9), with `X`,`Y` both 10,000 x 10,000 Float32 matrices, `c_i` Float32 constants, `f` runs fine, but `g` delivers the following out of memory error:\r\n\r\n```\r\nTensorflow error: Status: Failed to allocate request for 385.74MiB (404480000B) on device ordinal 0\r\n\r\nTotal hbm usage >= 7.88G:\r\n    reserved                 528.00M\r\n    persistent allocations     7.37G (7.4% fragmentation)\r\n    program                    33.0K\r\n\r\nPersistent allocations include some or all of:\r\n    arguments                771.48M (98.9% utilization)\r\n    output                   385.75M (98.9% utilization) [may share some memory with arguments]\r\n```\r\n\r\nThis seems to indicate to me that it's trying to reserve space for all the intermediate matrix results in high bandwidth memory. I would have expected (and @learyg mentioned that it should) that XLA would have optimized the memory placement in order to avoid this problem. \r\n\r\nLooking at the profile output for the `f` function, it seems to be doing all the broadcasts in order, followed by grouping all the matmuls (which would indeed blow the memory budget):\r\n![screen shot 2018-08-27 at 1 02 37 pm](https://user-images.githubusercontent.com/1291671/44673466-a087b800-a9f9-11e8-9121-86311b5fe40f.png)\r\n\r\nI also note that gap between the broadcasts and the matmuls, during which time it seems to be running XLA again. To me that indicates that XLA is not getting the full TF graph on the server side and thus cannot optimize the memory placement properly. I'm hoping that there's just some missing setting in the TF graph def that would allow it to do its job.\r\n\r\n### Source code / logs\r\nI was able to reproduce this in raw Tensorflow.jl (which should have a more or less 1:1 API correspondence to python, though of course there may be a bug in the bindings also - cc @malmaud @oxinabox), using code like this:\r\n```\r\nusing TensorFlow\r\nusing TensorFlow: Device\r\n\r\nsess = Session(Graph(); target=\"grpc://localhost:8470\")\r\n\r\nwith_device(TensorFlow.NamedDevice(\"/job:tpu_worker/replica:0/task:0/device:TPU:0\")) do\r\n    global X,Y,Z\r\n    X = placeholder(Float32, shape=[10000, 10000])\r\n    Z = Y = placeholder(Float32, shape=[10000, 10000])\r\n    for i = 1:8 # Fails for 1:9\r\n        Z *= X + rand(Float32)*X\r\n    end\r\nend\r\n\r\nx, z = rand(Float32, 10000, 10000), rand(Float32, 10000, 10000)\r\nrun(sess, Z, Dict(X=>x, Y=>z))\r\n```\r\nA serialized GraphDef of the graph for `f` is at https://gist.github.com/Keno/9321e0f1f278b04fbf3b878e551b3ecb\r\n\r\nIs there something missing from this GraphDef that would enable XLA to perform the appropriate memory placement optimization on the device?", "comments": ["@jpienaar are the right person to look at this, or can you redirect to someone who is? Thanks.", "Sure, I'll start looking at it and then redirect as needed.\r\n\r\n@Keno  Do you see the the compile op in the trace?", "Yes, I do. I can probably get you the trace somehow if you tell me what files you need.", "I don't know how the Julia TF integration works, are you currently generating the graphdef directly?\r\n\r\nI think the XLA HLO computations, it would be great if you could dump those too (would provide extra validation). ", "Yes, right now we're generating the graphdef directly (in particular the one I attached to the original issue) and then XLA is generated server side. Now that https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/xrt is in tree, I'm hoping to move to that for running stuff on TPUs as soon as that becomes available in the TF version deployed on Cloud TPUs, but for now it's raw TF graphdefs.", "SG, what I would suggest is to generate the graph to look exactly like one would see when using tpu.rewrite. In particular adding the _tpu_replicate attribute to indicate which nodes are part of the same cluster. The easiest would be to rewrite in Python and use that as your template. The graphs look quite different (and the memory usage is much better too).\r\n\r\nIf you are generating the graphdef directly then I think that would be your best bet for now: it should not require too many changes (adding replicate metadata/input/output nodes and replicate attribute per node in computation cluster) and then you should end up triggering the same extraction passes.", "Nagging Assignee @jpienaar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Not an issue when using xrt (which just became available with 1.11), so I consider this resolved."]}, {"number": 21903, "title": "R1.10", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 21902, "title": "Test numpy", "body": "", "comments": ["I've already merged this back into master."]}, {"number": 21901, "title": "Add gradient for tf.broadcast_to", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary (pip tensorflow-gpu)\r\n- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA 9.0 / CUDNN 7.1.3\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n```\r\npython -c \"import tensorflow as tf; a = tf.zeros([1]); b = tf.broadcast_to(a, [2]); tf.gradients(b, a)\"\r\n```\r\nResult:\r\nLookupError: gradient registry has no entry for: BroadcastTo\r\n\r\n### Describe the problem\r\nProblem: tf.broadcast_to (BroadcastTo) does not have a gradient defined (on CPU or GPU)\r\nFeature request: Add the gradient for the op.\r\nRelevant links:\r\nhttps://github.com/tensorflow/tensorflow/pull/19753\r\nhttps://github.com/tensorflow/tensorflow/pull/15243\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"/h/username/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 733, in _GradientsHelper\r\n    grad_fn = ops.get_gradient_function(op)\r\n  File \"/h/username/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2335, in get_gradient_function\r\n    return _gradient_registry.lookup(op_type)\r\n  File \"/h/username/.local/lib/python3.6/site-packages/tensorflow/python/framework/registry.py\", line 93, in lookup\r\n    \"%s registry has no entry for: %s\" % (self._name, name))\r\nLookupError: gradient registry has no entry for: BroadcastTo\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/h/username/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 596, in gradients\r\n    gate_gradients, aggregation_method, stop_gradients)\r\n  File \"/h/username/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 737, in _GradientsHelper\r\n    (op.name, op.type))\r\nLookupError: No gradient defined for operation 'BroadcastTo' (op type: BroadcastTo)\r\n```", "comments": ["ping @yongtang ", "/CC @alextp can you take a look? You reviewed the original PR for `tf.broadcast_to`. Perhaps this should be marked as contributions welcome.", "Indeed contributions welcome is a good fit. In principle the gradient of tf.broadcast_to is just tf.reduce_sum on the indices which were broadcasted.\r\n\r\nA pull request to add this gradient should just add a call to ops.RegisterGradient for broadcast_to which calls tf.reduce_sum with the appropriate indices.", "@alextp I would like to try this. Would send a PR by this week.", "Sorry that I didn't check the comment when creating the PR #22083. However, PR #22083 use ids and `unsorted_segment_sum ` to calculate gradients, instead of reduce_sum solution suggested by @alextp .\r\n\r\n@Cibifang Hi, welcome! If @alextp think reduce_sum solution is better than #22083, please take over the issue and take a try :-)", "I think unsorted_segment_sum requires a lot more temporary storage than\nreduce_sum for this case.\n\nOn Wed, Sep 5, 2018 at 1:18 AM Yan Facai (\u989c\u53d1\u624d) <notifications@github.com>\nwrote:\n\n> Sorry that I didn't check the comment when creating the PR #22083\n> <https://github.com/tensorflow/tensorflow/pull/22083>. However, PR #22083\n> <https://github.com/tensorflow/tensorflow/pull/22083> use ids and\n> unsorted_segment_sum to calculate gradients, instead of reduce_sum\n> solution suggested by @alextp <https://github.com/alextp> .\n>\n> @Cibifang <https://github.com/Cibifang> Hi, welcome! If @alextp\n> <https://github.com/alextp> think reduce_sum solution is better than\n> #22083 <https://github.com/tensorflow/tensorflow/pull/22083>, please take\n> over the issue and take a try :-)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21901#issuecomment-418640840>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxeq13uilcWkVw8iqDCtz4nW1DWSaks5uX4jUgaJpZM4WOGhu>\n> .\n>\n\n\n-- \n - Alex\n", "It seems not easy to handle the case, broadcasting shape [2, 3] to [4, 4, 3], for reduce_sum. Anyway, could you take a look at #22083 ? And feel free to close it if we have a better solution. Thanks.", "The gradient in this case is tf.reduce_sum(grad, axis=0)\n\nOn Wed, Sep 5, 2018 at 3:24 PM Yan Facai (\u989c\u53d1\u624d) <notifications@github.com>\nwrote:\n\n> It seems not easy to handle the case, broadcasting shape [2, 3] to [4, 2,\n> 3], for reduce_sum.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21901#issuecomment-418900571>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxXuzkb7r4JnK9dTw1w8x9qVMk3-sks5uYE8ZgaJpZM4WOGhu>\n> .\n>\n\n\n-- \n - Alex\n", "If I understand it correctly, tf.reduce_sum(grad, axis=0) results in [4, 3], rather than [2, 3]. Note that the second dimension is also broadcasted in the case.", "Ah, I didn't think you could broadcast over dimensions of size != 1.\n\nOn Wed, Sep 5, 2018 at 4:17 PM Yan Facai (\u989c\u53d1\u624d) <notifications@github.com>\nwrote:\n\n> If I understand it correctly, tf.reduce_sum(grad, axis=0) results in [4,\n> 3], rather than [2, 3]. Note that the second dimension is also broadcasted\n> in the case.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21901#issuecomment-418911156>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxddrXfC24R0VZtjAFKyhzesk_Vdvks5uYFt5gaJpZM4WOGhu>\n> .\n>\n\n\n-- \n - Alex\n", "Actually, it seems OK if broadcasted shape can be divide exactly by input shape:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/7ec8114697a78271277c1b81707f53057d047901/tensorflow/core/kernels/broadcast_to_op.h#L37-L49\r\n\r\nTest script:\r\n\r\n```python\r\nIn [1]: import tensorflow as tf\r\n\r\nIn [2]: a = tf.constant([[1, 2, 3], [4, 5, 6]])\r\n\r\nIn [3]: b = tf.broadcast_to(a, [4, 4, 3])\r\n\r\nIn [4]: sess = tf.Session()\r\n\r\nIn [5]: sess.run(b)\r\nOut[5]:\r\narray([[[1, 2, 3],\r\n        [4, 5, 6],\r\n        [1, 2, 3],\r\n        [4, 5, 6]],\r\n\r\n       [[1, 2, 3],\r\n        [4, 5, 6],\r\n        [1, 2, 3],\r\n        [4, 5, 6]],\r\n\r\n       [[1, 2, 3],\r\n        [4, 5, 6],\r\n        [1, 2, 3],\r\n        [4, 5, 6]],\r\n\r\n       [[1, 2, 3],\r\n        [4, 5, 6],\r\n        [1, 2, 3],\r\n        [4, 5, 6]]], dtype=int32)\r\n```", "Why is this issue closed? I encountered the same problem and really hope someone could implemented this!", "Because it's been fixed at HEAD\n\nOn Tue, Feb 5, 2019 at 1:57 PM Hongxiang Chen <notifications@github.com>\nwrote:\n\n> Why is this issue closed? I encountered the same problem and really hope\n> someone could implemented this!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21901#issuecomment-460818827>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxeChB42FDVMErpr8H6_BE-uNZdaPks5vKf5bgaJpZM4WOGhu>\n> .\n>\n\n\n-- \n - Alex\n", "[tf.tile](https://www.tensorflow.org/api_docs/python/tf/tile) was a valid differentiable alternative for me."]}, {"number": 21900, "title": "How is it possible to change the model in the application TF Detect?", "body": "Hello everyone I would like to change the .pb model (and the label file) used in the TF Detect app provided in the android example. How can I do this?\r\n\r\nThanks a lot!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 18 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21899, "title": "How to install versions of tensorflow from 0,12 to 0,9 nowadays, i can't find anywhere ", "body": "", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 18 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@Paulekna  Pip allows you to install specific version of TensorFlow. Something like this will do your job\r\n> pip install tensorflow==(version number)\r\nEx. : pip install tensorflow==1.1"]}, {"number": 21898, "title": "AWS lib is verbose when using S3", "body": "I'm using latest version of Tensorflow with AWS S3 as a backend for models and I have those annoying logs:\r\n\r\n[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.183378: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key \r\n[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.183471: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. \r\n[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.194855: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key \r\n[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.194963: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. \r\n[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.206484: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key \r\n[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.206714: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. \r\n[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.212817: E external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 \r\n[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.212852: W external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. \r\n\r\nThe server is working as expected though. I set the log level to ERROR only but still the 404 error is happening more than every second.\r\n\r\nI saw that there is an issue open in `tensorflow/serving` but this seems to be core/3rd party issue.\r\n\r\nhttps://github.com/tensorflow/serving/issues/615\r\n\r\n\r\n**Have I written custom code:** No\r\n**OS Platform and Distribution:** Kubernetes 1.8 running on CENTOS\r\n**TensorFlow installed from:** Docker image tensorflow/serving:1.10.1\r\n**TensorFlow version:** 1.10.1\r\n**Bazel version**: ??\r\n**CUDA/cuDNN version**: N/A\r\n**GPU model and memory:** N/A\r\n**Exact command to reproduce:** \r\n```bash\r\nif [ -f /etc/secrets/AWS_SECRET_ACCESS_KEY.secret ]; then\r\n    export AWS_SECRET_ACCESS_KEY=$(echo -e $(cut -d \":\" -f 2 <<< $(cat /etc/secrets/AWS_SECRET_ACCESS_KEY.secret)))\r\nfi\r\n\r\ntensorflow_model_server --port=8500 --tensorflow_session_parallelism=1 --enable_model_warmup=true --model_config_file=${MODEL_CONFIG}\r\n```\r\nExample of model_config_file:\r\n```json\r\nmodel_config_list: {\r\n  config: {\r\n    name: \"model1\",\r\n    base_path: \"s3://tensorflow-models/model1\",\r\n    model_platform: \"tensorflow\"\r\n  },\r\n  config: {\r\n    name: \"model2\",\r\n    base_path: \"s3://tensorflow-models/model2\",\r\n    model_platform: \"tensorflow\"\r\n  }\r\n}\r\n\r\n```\r\n**Mobile device:** None\r\n\r\nThank you\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Updated.", "Yeah we disabled the logs completely because it was logging way too much and so we are loosing all the logging purpose now because \"good\" logs are being lost in false positive.\r\n\r\nthank you.", "Any update on this? ", "I am getting this : :cry: \r\n```\r\n2019-03-19 14:14:15.095336: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-03-19 14:14:15.609993: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-03-19 14:14:16.813516: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-03-19 14:14:17.289853: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-03-19 14:14:17.804022: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-03-19 14:14:18.303771: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-03-19 14:14:19.577461: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-03-19 14:14:20.058953: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-03-19 14:14:20.533169: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-03-19 14:14:21.008386: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n```\r\n\r\nActually, more than 500 of these. :face_with_head_bandage: \r\n\r\nCan anybody let me know how do I disable this log (Only this, if possible)? or a possible workaround for this? \r\n\r\nThanks.", "This issue seems to be quite stale by now, but the problem still persists, at least for me. Has there been any kind of fix yet?", "@prameshbajra As a workaround, I just filter these messages with `grep` :)\r\n```bash\r\npython train.py 2>&1 | grep -v \"Connection has been released. Continuing.\"\r\n```", "https://github.com/tensorflow/tensorflow/pull/24088#issuecomment-484648955", "See also https://github.com/tensorflow/serving/issues/789", "@ymodak this can be closed since #24088", "Closing this issue since the PR has been merged. Thanks!", "The solution here is to set the logging level for AWS, taken from the PR which has been merged, this worked in 1.14 for me:\r\n\r\n```\r\nexport AWS_LOG_LEVEL=3\r\n```", "I use TF_CPP_MIN_LOG_LEVEL=3 and it works too but the above solution seems a better way to do.", "Hi, \r\nI have the same problem during training, using 1.15. I'm saving logs and checkpoint to s3 bucket, because I use sagemaker for training and want to use tensorboard in real time. Are you sure that increasing the log level is the right solution? What I  understand  from that errors is that tensorflow is continuously polling the S3 filesystem.. is that right? Problem is that s3 charge you for this read operations, and can lead to unexpected high bills from AWS.. Is there any way to stop this polling during training ?"]}, {"number": 21897, "title": "DistributionStrategy multi-node multi-GPU Estimator training: \"cannot assign a device for operation\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: `Yes`\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: `Linux Ubuntu 16.04`\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: `NA`\r\n- **TensorFlow installed from (source or binary)**: `source`\r\n- **TensorFlow version (use command below)**:  `1.10` (`master` @ 187ed273038ccde054fa27bf19582b0cd8e5cb57)\r\n- **Python version**: `2.7`\r\n- **Bazel version (if compiling from source)**: `0.16.1`\r\n- **GCC/Compiler version (if compiling from source)**: `4.8.5`\r\n- **CUDA/cuDNN version**: `9.1.85`/`7.1.4`\r\n- **GPU model and memory**: `4x NVIDIA P100`,  `16GB` per node\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nI'm trying to use a DistributionStrategy (e.g. MirroredStrategy) to train an Estimator in a multi-node, multi-GPU cluster. However, when I start two `tf.train.Server` tasks (each on a different node) and try to have the [simple Estimator example from the  `tf.contrib.distribute` source](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/examples/simple_estimator_example.py) run across these two workers ([here's my modified example](https://github.com/jppgks/tensorflow-distribution-strategy/blob/master/multi-node/workers/simple_estimator_example.py)), I get the error below saying a device can't be assigned.\r\n\r\nI'm suspecting that maybe my cluster setup isn't entirely correct. In that case, I would like to see how the documentation can be updated to include better instructions to run (multi-node) DistributionStrategy. I'm happy to help out with that.\r\n\r\n_pinging @mrry @guptapriya @yuefengz @anj-s_\r\n\r\n### Source code / logs\r\n**Source.** \r\nAvailable on GitHub [here](https://github.com/jppgks/tensorflow-distribution-strategy/tree/master/multi-node/workers).\r\n\r\n**Command.**\r\nOur cluster uses PBS/Torque as job manager, hence the `qsub`, but the contents of the file are standard bash.\r\n```bash\r\nqsub launch_tf_workers.sh\r\n```\r\n\r\n**Job output.** \r\n```pytb\r\nCLUSTER_SPEC: {\"worker\": [\"r22g35:2222\", \"r22g36:2222\"]}\r\n2018-08-27 10:18:18.155846: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n2018-08-27 10:18:22.169890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \r\nname: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\r\npciBusID: 0000:61:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\r\n2018-08-27 10:18:22.788786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 1 with properties: \r\nname: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\r\npciBusID: 0000:62:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\r\n2018-08-27 10:18:23.424017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 2 with properties: \r\nname: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\r\npciBusID: 0000:89:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\r\n2018-08-27 10:18:24.071758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 3 with properties: \r\nname: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\r\npciBusID: 0000:8a:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\r\n2018-08-27 10:18:24.071884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1, 2, 3\r\n2018-08-27 10:18:25.178802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-27 10:18:25.178891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 2 3 \r\n2018-08-27 10:18:25.178912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y Y Y \r\n2018-08-27 10:18:25.178929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N Y Y \r\n2018-08-27 10:18:25.178944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 2:   Y Y N Y \r\n2018-08-27 10:18:25.178960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 3:   Y Y Y N \r\n2018-08-27 10:18:25.179902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:61:00.0, compute capability: 6.0)\r\n2018-08-27 10:18:25.315544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:1 with 15123 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:62:00.0, compute capability: 6.0)\r\n2018-08-27 10:18:25.451748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:2 with 15123 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0)\r\n2018-08-27 10:18:25.587799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:3 with 15123 MB memory) -> physical GPU (device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0)\r\n2018-08-27 10:18:25.725327: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222, 1 -> r22g36:2222}\r\n2018-08-27 10:18:25.725775: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:2222\r\n2018-08-27 10:18:30.874254: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n2018-08-27 10:18:34.716434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \r\nname: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\r\npciBusID: 0000:61:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\r\n2018-08-27 10:18:35.336941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 1 with properties: \r\nname: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\r\npciBusID: 0000:62:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\r\n2018-08-27 10:18:35.969664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 2 with properties: \r\nname: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\r\npciBusID: 0000:89:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\r\n2018-08-27 10:18:36.617192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 3 with properties: \r\nname: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\r\npciBusID: 0000:8a:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\r\n2018-08-27 10:18:36.617325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1, 2, 3\r\n2018-08-27 10:18:37.725846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-27 10:18:37.725937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 2 3 \r\n2018-08-27 10:18:37.725958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y Y Y \r\n2018-08-27 10:18:37.725974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N Y Y \r\n2018-08-27 10:18:37.725990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 2:   Y Y N Y \r\n2018-08-27 10:18:37.726006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 3:   Y Y Y N \r\n2018-08-27 10:18:37.726951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:61:00.0, compute capability: 6.0)\r\n2018-08-27 10:18:37.863759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:1 with 15123 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:62:00.0, compute capability: 6.0)\r\n2018-08-27 10:18:37.999537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:2 with 15123 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0)\r\n2018-08-27 10:18:38.135424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:3 with 15123 MB memory) -> physical GPU (device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0)\r\n2018-08-27 10:18:38.272553: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job worker -> {0 -> r22g35:2222, 1 -> localhost:2222}\r\n2018-08-27 10:18:38.273006: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:2222\r\nWARNING:tensorflow:Could not find a standard reader in the input pipeline(one of TextLineDataset, TFRecordDataset, FixedLengthRecordDataset).Falling back to sharding the dataset anyway. Please verifycorrectness of auto-sharding for your input.\r\nWARNING:tensorflow:Could not find a standard reader in the input pipeline(one of TextLineDataset, TFRecordDataset, FixedLengthRecordDataset).Falling back to sharding the dataset anyway. Please verifycorrectness of auto-sharding for your input.\r\nWARNING:tensorflow:Could not find a standard reader in the input pipeline(one of TextLineDataset, TFRecordDataset, FixedLengthRecordDataset).Falling back to sharding the dataset anyway. Please verifycorrectness of auto-sharding for your input.\r\nWARNING:tensorflow:Could not find a standard reader in the input pipeline(one of TextLineDataset, TFRecordDataset, FixedLengthRecordDataset).Falling back to sharding the dataset anyway. Please verifycorrectness of auto-sharding for your input.\r\n2018-08-27 10:18:46.714305: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n2018-08-27 10:18:50.653650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \r\nname: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\r\npciBusID: 0000:61:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\r\n2018-08-27 10:18:51.271311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 1 with properties: \r\nname: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\r\npciBusID: 0000:62:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\r\n2018-08-27 10:18:51.901031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 2 with properties: \r\nname: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\r\npciBusID: 0000:89:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\r\n2018-08-27 10:18:52.566886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 3 with properties: \r\nname: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\r\npciBusID: 0000:8a:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\r\n2018-08-27 10:18:52.567011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1, 2, 3\r\n2018-08-27 10:18:53.672876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-27 10:18:53.672965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 2 3 \r\n2018-08-27 10:18:53.672987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y Y Y \r\n2018-08-27 10:18:53.673003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N Y Y \r\n2018-08-27 10:18:53.673019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 2:   Y Y N Y \r\n2018-08-27 10:18:53.673035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 3:   Y Y Y N \r\n2018-08-27 10:18:53.673974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:61:00.0, compute capability: 6.0)\r\n2018-08-27 10:18:53.809434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15123 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:62:00.0, compute capability: 6.0)\r\n2018-08-27 10:18:53.946947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15123 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0)\r\n2018-08-27 10:18:54.084679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 15123 MB memory) -> physical GPU (device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0)\r\n2018-08-27 10:18:57.049981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1, 2, 3\r\n2018-08-27 10:18:57.050183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-27 10:18:57.050217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 2 3 \r\n2018-08-27 10:18:57.050244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y Y Y \r\n2018-08-27 10:18:57.050270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N Y Y \r\n2018-08-27 10:18:57.050295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 2:   Y Y N Y \r\n2018-08-27 10:18:57.050319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 3:   Y Y Y N \r\n2018-08-27 10:18:57.050912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:61:00.0, compute capability: 6.0)\r\n2018-08-27 10:18:57.051179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15123 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:62:00.0, compute capability: 6.0)\r\n2018-08-27 10:18:57.051476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15123 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0)\r\n2018-08-27 10:18:57.051755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 15123 MB memory) -> physical GPU (device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:61:00.0, compute capability: 6.0\r\n/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:62:00.0, compute capability: 6.0\r\n/job:localhost/replica:0/task:0/device:GPU:2 -> device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0\r\n/job:localhost/replica:0/task:0/device:GPU:3 -> device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0\r\n2018-08-27 10:18:57.052199: I tensorflow/core/common_runtime/direct_session.cc:291] Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:61:00.0, compute capability: 6.0\r\n/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:62:00.0, compute capability: 6.0\r\n/job:localhost/replica:0/task:0/device:GPU:2 -> device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0\r\n/job:localhost/replica:0/task:0/device:GPU:3 -> device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0\r\n\r\n2018-08-27 10:18:57.066840: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp' because the input edge from 'global_step/replica_3' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:3\r\n2018-08-27 10:18:57.066913: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_1' because the input edge from 'global_step/replica_2' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:2\r\n2018-08-27 10:18:57.066945: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_2' because the input edge from 'global_step/replica_1' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:1\r\n2018-08-27 10:18:57.066975: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_4' because the input edge from 'global_step/replica_4' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:0\r\n2018-08-27 10:18:57.067004: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_5' because the input edge from 'global_step/replica_5' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:1\r\n2018-08-27 10:18:57.067033: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_6' because the input edge from 'global_step/replica_6' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:2\r\n2018-08-27 10:18:57.067066: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_7' because the input edge from 'global_step/replica_7' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:3\r\n2018-08-27 10:18:57.067096: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_8' because the input edge from 'dense/kernel/replica_3' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:3\r\n2018-08-27 10:18:57.067125: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_9' because the input edge from 'dense/kernel/replica_2' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:2\r\n2018-08-27 10:18:57.067153: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_10' because the input edge from 'dense/kernel/replica_1' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:1\r\n2018-08-27 10:18:57.067190: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_12' because the input edge from 'dense/kernel/replica_4' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:0\r\n2018-08-27 10:18:57.067219: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_13' because the input edge from 'dense/kernel/replica_5' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:1\r\n2018-08-27 10:18:57.067248: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_14' because the input edge from 'dense/kernel/replica_6' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:2\r\n2018-08-27 10:18:57.067276: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_15' because the input edge from 'dense/kernel/replica_7' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:3\r\n2018-08-27 10:18:57.067305: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_16' because the input edge from 'dense/bias/replica_3' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:3\r\n2018-08-27 10:18:57.067334: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_17' because the input edge from 'dense/bias/replica_2' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:2\r\n2018-08-27 10:18:57.067362: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_18' because the input edge from 'dense/bias/replica_1' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:1\r\n2018-08-27 10:18:57.067397: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_20' because the input edge from 'dense/bias/replica_4' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:0\r\n2018-08-27 10:18:57.067426: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_21' because the input edge from 'dense/bias/replica_5' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:1\r\n2018-08-27 10:18:57.067454: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_22' because the input edge from 'dense/bias/replica_6' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:2\r\n2018-08-27 10:18:57.067482: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_23' because the input edge from 'dense/bias/replica_7' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:3\r\n2018-08-27 10:18:57.067512: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp' because the input edge from 'global_step/replica_3' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:3\r\n2018-08-27 10:18:57.067541: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_1' because the input edge from 'global_step/replica_2' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:2\r\n2018-08-27 10:18:57.067569: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_2' because the input edge from 'global_step/replica_1' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:1\r\n2018-08-27 10:18:57.067598: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_4' because the input edge from 'global_step/replica_4' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:0\r\n2018-08-27 10:18:57.067626: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_5' because the input edge from 'global_step/replica_5' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:1\r\n2018-08-27 10:18:57.067654: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_6' because the input edge from 'global_step/replica_6' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:2\r\n2018-08-27 10:18:57.067683: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_7' because the input edge from 'global_step/replica_7' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:3\r\n2018-08-27 10:18:57.067714: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_8' because the input edge from 'dense/kernel/replica_3' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:3\r\n2018-08-27 10:18:57.067743: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_9' because the input edge from 'dense/kernel/replica_2' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:2\r\n2018-08-27 10:18:57.067771: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_10' because the input edge from 'dense/kernel/replica_1' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:1\r\n2018-08-27 10:18:57.067800: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_12' because the input edge from 'dense/kernel/replica_4' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:0\r\n2018-08-27 10:18:57.067828: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_13' because the input edge from 'dense/kernel/replica_5' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:1\r\n2018-08-27 10:18:57.067857: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_14' because the input edge from 'dense/kernel/replica_6' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:2\r\n2018-08-27 10:18:57.067885: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_15' because the input edge from 'dense/kernel/replica_7' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:3\r\n2018-08-27 10:18:57.067914: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_16' because the input edge from 'dense/bias/replica_3' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:3\r\n2018-08-27 10:18:57.067942: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_17' because the input edge from 'dense/bias/replica_2' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:2\r\n2018-08-27 10:18:57.067970: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_18' because the input edge from 'dense/bias/replica_1' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:1\r\n2018-08-27 10:18:57.067999: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_20' because the input edge from 'dense/bias/replica_4' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:0\r\n2018-08-27 10:18:57.068030: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_21' because the input edge from 'dense/bias/replica_5' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:1\r\n2018-08-27 10:18:57.068059: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_22' because the input edge from 'dense/bias/replica_6' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:2\r\n2018-08-27 10:18:57.068087: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_23' because the input edge from 'dense/bias/replica_7' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:3\r\nTraceback (most recent call last):\r\n  File \"simple_estimator_example.py\", line 128, in <module>\r\n    tf.app.run()\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"simple_estimator_example.py\", line 113, in main\r\n    estimator.train(input_fn=input_fn, steps=10)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1177, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1320, in _train_model_distributed\r\n    saving_listeners)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1400, in _train_with_estimator_spec\r\n    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 504, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 920, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 643, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1106, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1111, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 800, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 566, in create_session\r\n    init_fn=self._scaffold.init_fn)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 287, in prepare_session\r\n    sess.run(init_op, feed_dict=init_feed_dict)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 887, in run\r\n    run_metadata_ptr)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1110, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\r\n    run_metadata)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1308, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'hooks/steps_per_run/IsInitialized/VarIsInitializedOp': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:worker/replica:0/task:0/device:GPU:0'\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nReadVariableOp: GPU CPU \r\nAssignVariableOp: CPU \r\nVarIsInitializedOp: GPU CPU \r\nVarHandleOp: CPU \r\nConst: GPU CPU \r\n\r\nColocation members and user-requested devices:\r\n  hooks/steps_per_run/Initializer/ones (Const) \r\n  hooks/steps_per_run (VarHandleOp) \r\n  hooks/steps_per_run/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) \r\n  hooks/steps_per_run/Assign (AssignVariableOp) \r\n  hooks/steps_per_run/Read/ReadVariableOp (ReadVariableOp) \r\n  report_uninitialized_variables/VarIsInitializedOp_24 (VarIsInitializedOp) /job:worker/replica:0/task:0/device:GPU:0\r\n\r\n\t [[{{node hooks/steps_per_run/IsInitialized/VarIsInitializedOp}} = VarIsInitializedOp[](hooks/steps_per_run)]]\r\n\r\nCaused by op u'hooks/steps_per_run/IsInitialized/VarIsInitializedOp', defined at:\r\n  File \"simple_estimator_example.py\", line 128, in <module>\r\n    tf.app.run()\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"simple_estimator_example.py\", line 113, in main\r\n    estimator.train(input_fn=input_fn, steps=10)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1177, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1240, in _train_model_distributed\r\n    steps_per_run_variable = training.get_or_create_steps_per_run_variable()\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 309, in get_or_create_steps_per_run_variable\r\n    use_resource=True)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1484, in get_variable\r\n    aggregation=aggregation)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1234, in get_variable\r\n    aggregation=aggregation)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 538, in get_variable\r\n    aggregation=aggregation)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 492, in _true_getter\r\n    aggregation=aggregation)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 920, in _get_single_variable\r\n    aggregation=aggregation)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 131, in __call__\r\n    return cls._variable_call(*args, **kwargs)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 127, in _variable_call\r\n    aggregation=aggregation)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 106, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2418, in default_variable_creator\r\n    import_scope=import_scope)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 133, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 297, in __init__\r\n    constraint=constraint)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 455, in _init_from_args\r\n    gen_resource_variable_ops.var_is_initialized_op(self._handle))\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 1263, in var_is_initialized_op\r\n    \"VarIsInitializedOp\", resource=resource, name=name)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3263, in create_op\r\n    op_def=op_def)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1751, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'hooks/steps_per_run/IsInitialized/VarIsInitializedOp': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:worker/replica:0/task:0/device:GPU:0'\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nReadVariableOp: GPU CPU \r\nAssignVariableOp: CPU \r\nVarIsInitializedOp: GPU CPU \r\nVarHandleOp: CPU \r\nConst: GPU CPU \r\n\r\nColocation members and user-requested devices:\r\n  hooks/steps_per_run/Initializer/ones (Const) \r\n  hooks/steps_per_run (VarHandleOp) \r\n  hooks/steps_per_run/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) \r\n  hooks/steps_per_run/Assign (AssignVariableOp) \r\n  hooks/steps_per_run/Read/ReadVariableOp (ReadVariableOp) \r\n  report_uninitialized_variables/VarIsInitializedOp_24 (VarIsInitializedOp) /job:worker/replica:0/task:0/device:GPU:0\r\n\r\n\t [[{{node hooks/steps_per_run/IsInitialized/VarIsInitializedOp}} = VarIsInitializedOp[](hooks/steps_per_run)]]\r\n```\r\n", "comments": ["I believe @yuefengz fixed this issue in https://github.com/tensorflow/tensorflow/commit/524f931fa9b6158c99f5df88839a80a36e420d08 so perhaps you could try with building tensorflow at that commit? (I think the one you used is just slightly older)\r\n\r\nIf this doesn't fix, please let us know. ", "The error I had before with `VarIsInitializedOp ` is now replaced by the following placement issue with `IteratorToStringHandle` after building from master @ e2c6ec9e86dd86e0dd56e0f11302a5bf5d9ed440.\r\n\r\n```pytb\r\nTraceback (most recent call last):\r\n  File \"simple_estimator_example.py\", line 109, in <module>\r\n    tf.app.run()\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"simple_estimator_example.py\", line 94, in main\r\n    estimator.train(input_fn=input_fn, steps=10)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1181, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1325, in _train_model_distributed\r\n    saving_listeners)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1405, in _train_with_estimator_spec\r\n    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 504, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 920, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 643, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1106, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1111, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 800, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 566, in create_session\r\n    init_fn=self._scaffold.init_fn)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 287, in prepare_session\r\n    sess.run(init_op, feed_dict=init_feed_dict)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 887, in run\r\n    run_metadata_ptr)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1110, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\r\n    run_metadata)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1308, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'IteratorToStringHandle': Operation was explicitly assigned to /job:worker/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0, /job:localhost/replica:0/task:0/device:GPU:1, /job:localhost/replica:0/task:0/device:GPU:2, /job:localhost/replica:0/task:0/device:GPU:3 ]. Make sure the device specification refers to a valid device.\r\n\t [[{{node IteratorToStringHandle}} = IteratorToStringHandle[_device=\"/job:worker/task:0\"](IteratorV2)]]\r\n\r\nCaused by op u'IteratorToStringHandle', defined at:\r\n  File \"simple_estimator_example.py\", line 109, in <module>\r\n    tf.app.run()\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"simple_estimator_example.py\", line 94, in main\r\n    estimator.train(input_fn=input_fn, steps=10)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1181, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1249, in _train_model_distributed\r\n    input_fn, model_fn_lib.ModeKeys.TRAIN, self._train_distribution)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1044, in _get_iterator_from_input_fn\r\n    iterator = result.make_initializable_iterator()\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 831, in make_initializable_iterator\r\n    iterators[worker] = dataset.make_initializable_iterator()\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 736, in make_initializable_iterator\r\n    dataset_iterator = self._dataset.make_initializable_iterator()\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/prefetching_ops_v2.py\", line 184, in make_initializable_iterator\r\n    shared_name=shared_name)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/prefetching_ops_v2.py\", line 72, in __init__\r\n    shared_name, self._input_dataset.output_classes)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 213, in from_structure\r\n    output_classes)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 100, in __init__\r\n    self._iterator_resource)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2393, in iterator_to_string_handle\r\n    \"IteratorToStringHandle\", resource_handle=resource_handle, name=name)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3254, in create_op\r\n    op_def=op_def)\r\n  File \"/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1750, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'IteratorToStringHandle': Operation was explicitly assigned to /job:worker/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0, /job:localhost/replica:0/task:0/device:GPU:1, /job:localhost/replica:0/task:0/device:GPU:2, /job:localhost/replica:0/task:0/device:GPU:3 ]. Make sure the device specification refers to a valid device.\r\n\t [[{{node IteratorToStringHandle}} = IteratorToStringHandle[_device=\"/job:worker/task:0\"](IteratorV2)]]\r\n```", "I suspect you didn't started the cluster with the right TF_CONFIG. Would you mind adding `tf.logging.set_verbosity(tf.logging.INFO)` and pasting all the logs of the first worker?", "Nagging Assignee @yuefengz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@jppgks I am closing this issue. Feel free to file a new one."]}, {"number": 21896, "title": "EvalSpec's param throttle_secs in tf.estimator.train_and_evaluate", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nI use [tf.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) for training my models.\r\nHere is part of my source code: \r\n```\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(.....))\r\n\r\neval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(...), throttle_secs=NUM_OF_SECONDS)\r\n\r\ntf.estimator.train_and_evaluate(estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)\r\n```\r\n\r\nWe can see this part of logs after start of training in Tensorflow 1.9.0:\r\n```\r\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\r\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after MANY secs (eval_spec.throttle_secs) or training is finished\r\n```\r\n\r\nin Tensorflow 1.10.0\r\n```\r\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\r\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\r\n```\r\nMy question is how does  param throtlle_secs work now? Because, there is reason to start evaluation process after many steps (for example 1M steps). In previous versions set many seconds to throttle_secs helps me. But in 1.10.0 set many seconds to save_checkpoints_secs is risky.\r\n\r\nand how does new release [Improved local run behavior in tf.estimator.train_and_evaluate which does not reload checkpoints for evaluation.](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md) influence on these changes in 1.10.0 version", "comments": ["@Bakaizhamgyrchiev, does #17650 help?", "@cy89 \r\nNo, issue #17650 doesn't help. I'm now interested how does throttle secs of EvalSpec works in tf 1.10? \r\n\r\nplease, look at tensorflow logs when estimator start to train_and_evaluate in tf 1.9 and 1.10  (look my issue head). It was comfortable to control eval frequency with throttle_secs. Control with save_checkpoints_secs is very risky, when I want start eval process every epoch", "@martinwicke @ispirmustafa could you comment, please?", "Hi @Bakaizhamgyrchiev \r\nThat's a missing part in documentation. We do use `throttle_secs` in local-run too. I'm curious have you tried it?", "Hi @ispirmustafa Yes, I tried it, of course. But it doesn't work. I set to throttle_secs = 1000000 secs. But evaluation started after 600 secs (save_checkpoints_secs). Train process works about 6 hours.", "Hi @Bakaizhamgyrchiev \r\nFirst evaluation should start with first checkpoint. But the second one should be at least `throttle_secs` after that. Could you please verify that?", "Nagging Assignee @cy89: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks very much for answer. I have also noticed that if evaluation process is longer than throttle_secs then evaluations restart without training steps. I thought that throttle_secs was counted down after evaluation process was finished (throttle_secs really is counted down after checkpoint saving). Would be great if it was clear in documentation...", "Same issue. As today, using tensorflow v1.15 and the latest object detection API, I still encounter the same issues:\r\n\r\n1. when `throttle_secs` < the time required for validation, no training will happen and the `object_detection/model_main.py` script simply does validation all the time\r\n2. when `throttle_secs` is explicitly set to be greater than the required validation time, training happens in between of two validations, but the actual time gap between two validations does not greater than the time since the previous checkpoint is saved "]}, {"number": 21895, "title": "Support addition of gradient operations in a graph for golang", "body": "related to  #21404", "comments": ["Hi, @asimshankar Can you take a look again?\r\n\r\nSome details:\r\n1. Different with Java implementation, `Gradients` have `prefix` as a parameters.\r\n  If we just use `scope.namespace` as prefix, It will cause some problems:\r\n    ```\r\n    // When scope.namespace == \"\"\r\n    // the grads1[0].Op.Name() will start with \"gradients/\"\r\n    grads0 := Gradients(s,  []tf.Output{y0}, []tf.Output{x})\r\n\r\n    // the grads1[0].Op.Name() will start with \"gradients_1/\"\r\n    grads1 := Gradients(s,  []tf.Output{y0}, []tf.Output{x})\r\n    ```\r\n   It's OK because the scope.namespace is \"\", so the prefix is \"\".\r\n\r\n   But something will be wrong when `scope.namespace is not \"\"\r\n   ```\r\n   // When scope.namespace == \"sub\"\r\n    // the grads1[0].Op.Name() will start with \"sub/\"\r\n    grads0 := Gradients(s,  []tf.Output{y0}, []tf.Output{x})\r\n\r\n    // It will cause error because use conflicted prefix `sub`\r\n    grads1 := Gradients(s,  []tf.Output{y0}, []tf.Output{x})\r\n   ```\r\n\r\n  In the Java Implementation, it is solved by a new class `NameScope` with new method `makeOpName`.\r\n  ```\r\n  // When scope.namespace == \"sub\"\r\n  // If don't want to set prefix\r\n  // grad0.dy(0).op().name() will start with \"sub/Gradients/\"\r\n  Gradients grad0 = Gradients.create(scope, y, Arrays.asList(x));\r\n\r\n  // If want to set prefix \"MyGradients/\"\r\n  // grad1.dy(0).op().name() will start with \"sub/MyGradients/`\r\n  radients grad1 = Gradients.create(scope.withName(\"MyGradients\"), y, Arrays.asList(x));\r\n  ```\r\n\r\n  to same with the implementation in Java, we need to add a `NameScope` struct, or add `opName`  to `Scope` structure(to be a default op name as prefix for Gradients). I don't know if it is good to do that for this feature. \r\n  So just add a `prefix` para to method `Gradients`\r\n\r\n2. Enforce uniqueness of custom prefixes for gradients in method `Gradients` in op package, just like in Java", "It has been 20 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "sorry, I'll update it soon."]}, {"number": 21894, "title": "Tensorflow with Keras fit and tensors in dataset input result in list index out of range", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.13.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI encountered a strange error when using tf.keras in python with a dataset that consists out of tensors.\r\n\r\nWhat I do is create a dataset from a csv file and modify it as it contains a time series. In the end, I end up with a dataset that has tuples of input and output tensor.\r\n\r\nThese are fed into a tf.keras model and here is where it becomes interesting. Fit calls a function in training.py of the keras enginge which is called _standardize_user_data. This should return the input, targets, and so on for fit_loop, which in turn checks something on it and crashes with _if issparse is not None and issparse(ins[i]) and not K.is_sparse(feed[i]): IndexError: list index out of range_ in line 187 of training_arrays.py. This is due to the fact _standardize_user_data returns empty lists, with the reasoning that if tensors are the input, then everything should be set up already, which it apparently isn't.\r\n\r\n### Source code / logs\r\nA small script to reproduce the problem:\r\n```\r\n#!/usr/bin/env python3\r\nimport argparse\r\nimport glob\r\nimport logging\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n\r\ndef read_dataset(filename, columns, field_defaults, input_size, output_size, stride, input_features):\r\n    def decode_csv(row):\r\n        fields = tf.decode_csv(row, record_defaults=field_defaults, field_delim=',')\r\n        all_columns = dict(zip(columns, fields))\r\n        return all_columns\r\n\r\n    def split_window(window):\r\n        inputs = tf.reshape(tf.concat(window['value'][0:input_size], axis=1), [input_size, input_features])\r\n        outputs = tf.reshape(tf.concat(window['value'][input_size:input_size + output_size], axis=1),\r\n                             [output_size, input_features])\r\n\r\n        return inputs, outputs\r\n\r\n    dataset = tf.data.TextLineDataset(filenames=filename)\r\n    dataset = dataset.map(decode_csv)\r\n    dataset = dataset.apply(tf.contrib.data.sliding_window_batch(window_size=input_size + output_size, stride=stride))\r\n    dataset = dataset.map(split_window)\r\n    dataset = dataset.repeat()\r\n\r\n    return dataset\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    COLUMNS = ['value']\r\n    FIELD_DEFAULTS = [[0.0]]\r\n    INPUT_FEATURES = 1\r\n\r\n    epochs = 1\r\n    steps = 1\r\n    INPUT_SIZE = 4\r\n    OUTPUT_SIZE = 2\r\n    STRIDE = 1\r\n    input_train = \"./data/\"\r\n\r\n    input_train_list = glob.glob(input_train + \"*\")\r\n\r\n    model = keras.Sequential()\r\n    model.add(tf.keras.layers.Dense(OUTPUT_SIZE, activation=None))\r\n    set = read_dataset(input_train_list, COLUMNS, FIELD_DEFAULTS, INPUT_SIZE, OUTPUT_SIZE, STRIDE, INPUT_FEATURES)\r\n\r\n    model.compile(optimizer=tf.train.AdamOptimizer(0.01), loss='mse', metrics=['mse'])\r\n    model.fit(set, epochs=epochs, steps_per_epoch=steps)\r\n```\r\nAnd the error itself:\r\n```\r\n2018-08-27 14:29:33.238328: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/.../test.py\", line 102, in <module>\r\n    model.fit(set, epochs=epochs, steps_per_epoch=steps)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1363, in fit\r\n    validation_steps=validation_steps)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 187, in fit_loop\r\n    if issparse is not None and issparse(ins[i]) and not K.is_sparse(feed[i]):\r\nIndexError: list index out of range\r\n```\r\nThe data itself is something like this:\r\n```\r\n0.047910000000000785\r\n3.0999999999892225e-05\r\n0.0160979999999995\r\n2.9000000000500847e-05\r\n0.01716599999999957\r\n2.800000000036107e-05\r\n2.9999999999752447e-05\r\n0.019235000000000113\r\n```\r\n\r\nI am not sure if this behaviour is intended or not. In the Keras example, a dataset is used as well and I assume it should work with datasets consisting out of tensors. Is it possible that the return in this case should not be empty, but should only have an empty sample weight? ", "comments": ["Short update, this also happens with the provided example in the documentation (only change is the cast to float32 as otherwise there is a mismatch error):\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Dense(64, activation='relu'))\r\nmodel.add(keras.layers.Dense(64, activation='relu'))\r\nmodel.add(keras.layers.Dense(10, activation='softmax'))\r\n\r\nmodel.compile(optimizer=tf.train.AdamOptimizer(0.01), loss='mse', metrics=['mae'])\r\n\r\ndata = np.random.random((1000, 32)).astype(np.float32)\r\nlabels = np.random.random((1000, 10)).astype(np.float32)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((data, labels))\r\ndataset = dataset.batch(32).repeat()\r\n\r\nmodel.fit(dataset, epochs=10, steps_per_epoch=30)\r\n\r\n```", "@pbosch the documentation example you provided is working for me with the nightly build, could you update to the nightly build version (```pip install tf-nightly``` or ```pip install tf-nightly-gpu```) and let me know if you still see the error?", "@omalleyt12 Indeed, that solves the issue. Is there a possibility that there is another dot release soon that includes that fix? I would very much appreciate it.", "Had the same problem, and `pip install tf-nightly` to update the version solved it indeed. Looking forward to have it in a stable dot version.", "I'm not sure on the timing but yes I'd expect it to be included in the next dot release", "@omalleyt12  Thanks for the answer!", "I have the same problem:\r\nif issparse is not None and issparse(ins[i]) and not K.is_sparse(feed[i]):\r\nIndexError: list index out of range\r\n\r\nThis is with 1.12.0-dev20180929\r\n\r\nIt should be solved in this nightly build right?\r\n\r\nIn my code a tensor a passed to model.fit. The Tensor is a concatenation of multiple batches from different datasets:\r\n\r\ndata = list(map(lambda x: x.make_one_shot_iterator().get_next(), classes))\r\nbatch = tf.concat(axis=0, values=data)\r\nmodel.fit(\r\n        batch,\r\n        steps_per_epoch=100,\r\n        epochs=params['num_epochs'])", "hi \r\ngreat post ,\r\ni m trying to run baloonn detection API Mask_RCNN , and i get stuck with this error \r\n\r\ninput that isn't a symbolic tensor\r\n\r\nany suggession to work this out ?", "I just hit this error because I was specifying only my input data, not the expected model outputs:\r\n\r\n```\r\nvae.fit(X_train, batch_size=32, epochs=100) # wrong\r\nvae.fit(X_train, X_train, batch_size=32, epochs=100) # hooray!\r\n```"]}, {"number": 21893, "title": "Eager execution error: tf.enable_eager_execution must be called at program startup.", "body": "### System information\r\n\r\n- **Have I written custom code**: yes\r\n- **Bazel version**: N/A\r\n- **OS Platform and Distribution:** Windows 10\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n- **TensorFlow installed from conda**\r\n- **TensorFlow version**: 1.9\r\n- **Python version**: 3.6\r\n- **Mobile device**: N/A\r\n\r\n### Describe the problem\r\n\r\nI have some errors using the eager execution. \r\n\r\nI have tried to perform an eager execution of a simple code. I've tried it on both Jupyter Notebook and Spyder IDE. \r\n\r\nThe code is as follows:\r\n\r\n                import tensorflow as tf\r\n                tf.enable_eager_execution ()\r\n                import tensorflow.contrib.eager as tfe\r\n\r\n                def square (x):\r\n                     return tf.multiply (x, x)\r\n\r\n                grad = tfe.gradients_function (square)\r\n                print (grad (3.))\r\n\r\n\r\nWhen I execute the code I get the following error:\r\n\r\n\r\n\tFile \"C:\\...\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 5496, in enable_eager_execution \"tf.enable_eager_execution must be called at program startup.\")\r\n\r\n\tValueError: tf.enable_eager_execution must be called at program startup.\r\n\r\n\r\nIn Jupyter, In the first execution there are no errors but in the following ones. One way to fix it is by restarting the kernel.\r\n\r\nIn Spyder the error appears in the first execution I made. \r\nTo fix it I found two ways: \r\n\r\nOne rebooting the Spyder kernel and the second with a try / except:\r\n\r\n\t\ttry:\r\n\t\t\u00a0\u00a0\u00a0\u00a0 tf.enable_eager_execution ()\r\n\t\texcept Exception:\r\n\t\t\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 pass\r\n\r\n\r\nI leave the link to the question you make in Stackoverflow\r\n[(https://stackoverflow.com/questions/51967975/tf-enable-eager-execution-must-be-called-at-program-startup-only-in-spyder-ide)]\r\n\r\n\r\nI hope you find it helpful\r\n\r\n@alextp Thank you very much for your help.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 22 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 21892, "title": "LookupError: No gradient defined for operation 'High-order/SegmentProd' (op type: SegmentProd)", "body": "is there any problem in my code\uff1f or the op SegmentProd do not have gradient implement\uff1f", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21890, "title": "Fix pix2pix_eager.ipynb image resize method", "body": "2 was BICUBIC not NEAREST_NEIGHBOR\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/ops/image_ops_impl.py#L936", "comments": []}, {"number": 21889, "title": "Unable to serialize graph with Embedding layers with graph_util.convert_variables_to_constants", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: -\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.10.0-rc1-19-g656e7a2b34\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: \r\n- **GPU model and memory**: \r\n- **Exact command to reproduce**: (run script inlined in this issue)\r\n\r\n### Describe the problem\r\nAttempting to serialize a model containing a keras Embeddings layer with `tf.graph_util.convert_variables_to_constants` yields a graph that can't be restored. Restoring the graph makes TF complain that some nodes are missing; forcing those into the graphdef will cause TF to complain about mismatching types.\r\n\r\n### Source code / logs\r\nThe following script tries to serialize two very simple models \u2014 one without an embeddings layer and the other with such a layer. Restoring the model will the embeddings layer will fail with `Node 'embedding/embedding_lookup' expects to be colocated with unknown node 'embedding/embedding_lookup/Read/ReadVariableOp'`.\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n\r\n    def serialize_graph(model):\r\n        g = tf.graph_util.convert_variables_to_constants(\r\n            tf.keras.backend.get_session(),\r\n            tf.keras.backend.get_session().graph.as_graph_def(),\r\n            #[n.name for n in tf.keras.backend.get_session().graph.as_graph_def().node],\r\n            [t.op.name for t in model.outputs]\r\n        )\r\n        return g\r\n\r\n    def build_save_restore(model):\r\n        model.compile('sgd', loss='mse')\r\n        model.fit(np.array([[1]]),np.array([[1]]), verbose=0)\r\n\r\n        gdef = serialize_graph(model)\r\n        newg = tf.Graph()\r\n        with newg.as_default():\r\n            tf.import_graph_def(gdef)\r\n        print(\"*\"*25)\r\n        print(\"restored successfully\")\r\n        print(\"*\"*25)\r\n\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Dense(1)\r\n    ])\r\n    build_save_restore(model)\r\n\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Embedding(1, 1, input_length=1),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(1),\r\n    ])\r\n    build_save_restore(model)\r\n", "comments": ["I have similar issue with here.\r\n\r\nWhen I convert tf.keras model (containing `tf.keras.layers.Embeddings`) -> TF graph -> TF Lite model, The process of tf.keras to TF graph works, but when I convert TF graph to TF Lite, it calls `import_graph_def()` function internally and causes same error.\r\n\r\nAny help would be appreciated.\r\n", "Same issue here. I'm trying to export a `keras` model into a frozen graph, but it's failing for same reason.\r\n\r\n", "I have a similar issue but with elmo embeddings\r\nNode 'elmo/module_apply_tokens/bilm/embedding_lookup' expects to be colocated with unknown node 'elmo/module_apply_tokens/bilm/embedding_lookup/Read/ReadVariableOp'\r\n\r\n", "similar issue here. It seem tf.keras will use ResourceVariable by default, which caused the problem. Changing the default value of `use_resource` to `False` in `tf.keras.Layer.add_weight` seems to fix the problem ([here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer.py#L253)). But I still cannot a figure out a workaround without changing the TF source code. \r\nIs there any plans on fixing this? @gargn ", "changing variable creator seems to be a workaround\r\n```\r\ndef variable_creator(**kwargs):\r\n    kwargs[\"use_resource\"] = False\r\n    return variable_scope.default_variable_creator(None, **kwargs)\r\ngetter = lambda next_creator, **kwargs: variable_creator(**kwargs)\r\nwith variable_scope.variable_creator_scope(getter):\r\n    model = ...\r\n```\r\n\r\n", "Same issue. I'm trying to use frozen model which uses elmo but if fails with the same error.", "@yangw1234 \r\n\r\nGreat!  Changing the default value of `use_resource` to `False` works. But, changing variable creator doesn't work well, and got the error like this:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: indices[0,0] = 1 is not in [0, 1)\r\n\t [[{{node embedding/embedding_lookup}} = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training_1/SGD/Assign\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding/embeddings/read, embedding/Cast, training_1/SGD/gradients/embedding/embedding_lookup_grad/concat/axis)]]\r\n```\r\n\r\nI leave my code in case:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.ops import variable_scope\r\n\r\ndef serialize_graph(model):\r\n    g = tf.graph_util.convert_variables_to_constants(\r\n            tf.keras.backend.get_session(),\r\n            tf.keras.backend.get_session().graph.as_graph_def(),\r\n            #[n.name for n in tf.keras.backend.get_session().graph.as_graph_def().node],\r\n            [t.op.name for t in model.outputs]\r\n            )\r\n    return g\r\n\r\ndef build_save_restore(model):\r\n    model.compile('sgd', loss='mse')\r\n    model.fit(np.array([[1]]),np.array([[1]]), verbose=0)\r\n    gdef = serialize_graph(model)\r\n    newg = tf.Graph()\r\n    with newg.as_default():\r\n        tf.import_graph_def(gdef)\r\n        print(\"*\"*25)\r\n        print(\"restored successfully\")\r\n        print(\"*\"*25)\r\n\r\ndef variable_creator(**kwargs):\r\n    kwargs[\"use_resource\"] = False\r\n    return variable_scope.default_variable_creator(None, **kwargs)\r\n\r\nmodel = tf.keras.models.Sequential([\r\n        tf.keras.layers.Dense(1)\r\n        ])\r\nbuild_save_restore(model)\r\n\r\ngetter = lambda next_creator, **kwargs: variable_creator(**kwargs)\r\nwith variable_scope.variable_creator_scope(getter):\r\n    model = tf.keras.models.Sequential([\r\n            tf.keras.layers.Embedding(1, 1, input_length=1),\r\n            tf.keras.layers.Flatten(),\r\n            tf.keras.layers.Dense(1),\r\n            ])\r\n    build_save_restore(model)\r\n```\r\n\r\nCould you show the entire code?\r\n\r\nThanks in advance!", "@hengdos your error is not caused by changing creator, is because you are setting the vocabulary size in the embedding layer to 1 (only index 0 is valid) and the training data you feed is 1 (thus not in  [0, 1)). Try feeding 0?", "@yangw1234 Thanks, now it works!", "It look like the original issue has been resolved? Closing this. Please create a new issue if there is still a problem.", "@suharshs this is still a problem because:\r\n* the feature should be available without any workaround, especially something as low level as overriding the variable creation\r\n* the override in the workaround goes against the direction of tensorflow which is to use ResourceVariables by default", "cebce4a should fix this issue. Please reopen if it's still an issue.", "```\r\nI tried with the latest tf-nightly (installed just now) and I got this issue:\r\nxxx/yyy/Embedding-Token/embedding_lookup' expects to be colocated with unknown node 'xxx/yyy/Embedding-Token/embedding_lookup/Read/ReadVariableOp\r\n```\r\n\r\nScratch that, I was still on 1.12", "> ```\r\n> I tried with the latest tf-nightly (installed just now) and I got this issue:\r\n> xxx/yyy/Embedding-Token/embedding_lookup' expects to be colocated with unknown node 'xxx/yyy/Embedding-Token/embedding_lookup/Read/ReadVariableOp\r\n> ```\r\n> \r\n> Scratch that, I was still on 1.12\r\n\r\nI also had this problem in tensorflow=1.12.0\r\n"]}]