[{"number": 37124, "title": "Optimized fashion-mnist dataset download sourses for developers in China", "body": "make the fashion-mnist dataset accessable for those developer who can't visit storage.googleapis.com ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37124) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37124) for more info**.\n\n<!-- ok -->", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on."]}, {"number": 37123, "title": "Unsupported object type float", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 18.10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or\r\nbinary): binary \r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from\r\nsource): N/A\r\n- CUDA/cuDNN version: nvcc version 10.1 (V10.1.105)\r\n- GPU model and memory: NVIDIA Quadro P2000 5GB\r\n\r\n**Describe the current behavior**\r\nI am loading in a concatenation of two user data sources (one script generated and on read in via `pd.read_csv`).\r\nI paid attention to have both datasources have the same dtypes, column names, etc. \r\nI concatenate via \r\n```\r\nfakedata = td.generate_data(40000)\r\nrealdata = pd.read_csv(\"DS.csv\")\r\ndata = fakedata.append(realdata).reset_index()[fakedata.columns]\r\n```\r\nDataFrame structure is as follows:\r\n| firstname | lastname | company | country | email | fake |\r\n| Lorem | ipsum | Loremcorp | USA | lorem.ipsum@sit.amet | 1 |\r\n\r\nwhereas `data[\"fake\"]` is the target column having either 0 or 1 as state.\r\ndtypes of the are object and of `data[\"fake\"]` is `int64`.\r\n\r\ntrain input fn:\r\n```\r\ntrain_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(\r\n    data, data[\"fake\"], num_epochs=None, shuffle=True\r\n)\r\n```\r\ntraining:\r\n```\r\nestimator = tf.estimator.DNNClassifier(\r\n    hidden_units=[500, 100],\r\n    feature_columns=[email_feature_column, \r\n                     firstname_feature_column, \r\n                     lastname_feature_column, \r\n                     country_feature_column, \r\n                     company_feature_column],\r\n    n_classes=2,\r\n    optimizer=tf.keras.optimizers.Adagrad(lr=0.003))\r\n\r\nestimator.train(input_fn=train_input_fn, steps=5000)\r\n```\r\nFeature columns are created via the `text_embedding_column` from tfhub.\r\n\r\nWhen I now try to run the code this error keeps happening:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1367, in _do_call\r\n    return fn(*args)\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1352, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1445, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: Unsupported object type float\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorflow_model.py\", line 67, in <module>\r\n    estimator.train(input_fn=train_input_fn, steps=100000)\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 374, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1164, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1198, in _train_model_default\r\n    saving_listeners)\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1498, in _train_with_estimator_spec\r\n    any_step_done = True\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\", line 885, in __exit__\r\n    self._close_internal(exception_type)\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\", line 923, in _close_internal\r\n    self._sess.close()\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1190, in close\r\n    self._sess.close()\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1358, in close\r\n    ignore_live_threads=True)\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_core/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/lib/python3/dist-packages/six.py\", line 692, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py\", line 94, in _run\r\n    sess.run(enqueue_op, feed_dict=feed_dict)\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 960, in run\r\n    run_metadata_ptr)\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1183, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1361, in _do_run\r\n    run_metadata)\r\n  File \"/home/damned/.local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1386, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Unsupported object type float\r\n```\r\neven if there is no float in the dataset.\r\n\r\n**Describe the expected behavior**\r\n`estimator.train(input_fn=train_input_fn, steps=5000)`\r\nWorks as expected and trains.", "comments": ["Had broken values in the dataframe. Checking the dataframe with `np.unique(df[column]` and `df[df.isnan()]` helped solving the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37123\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37123\">No</a>\n"]}, {"number": 37122, "title": "What is \"FrameState\" and \"IterationState\" for in tensorflow source code?", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\nHello, I am currently looking at tensorflow source code to understand internal functionalities.\r\nI looked up and surfed around to understand what \"Frame\" and \"Iteration\" mean but couldn't. Also, I searched on the web but couldn't find the answer. \r\n\r\nstruct \"FrameState\" and struct \"IterationState\"\r\nFrameState is for loop and IterationState represents an iteration based on comments.\r\nWhat is the loop here in this context? I see loop is loop like 'for' or 'while' but where is the loop represented by FrameState? Is it something for handling \"tf.while_loop\"? \r\n\r\nPendingCounts is managed in the scope of IterationState, I guess all the nodes have their own pending counts to activate them. Why does pending counts belong to Iteration? Node is considered a independent unit and they can be activated if incoming edges is all given(available). I don't see how this pending counts are related to IterationState.\r\n\r\nThank you in advance!!", "comments": ["Inorder to understand this, you have to understand few things:\r\n**Pendingcounts**: PendingCounts is an internal helper class to keep track of pending and dead counts for nodes, for use in the ExecutorState module.\r\n\r\n**Frames**:\r\nPlease go through this [paper](https://arxiv.org/pdf/1805.01772.pdf) the dynamic control flow thats being using in Tensorflow. Just to make sure you understand what a frame is that is given in the paper  ----- every execution of an operation takes place within a \u201cframe\u201d; concretely,\r\nhere, frames are dynamically allocated execution contexts associated with each iteration of a loop. In TensorFlow without control flow, each operation in the graph executes exactly once; when extended with control flow, each operation executes at most once per frame.\r\n\r\nNow you can go ahead and read about iterationstate and framestate [here](https://github.com/tensorflow/tensorflow/blob/70db082057aa8f33933d79d7f039f285570505fe/tensorflow/core/common_runtime/executor.cc#L1065).\r\n\r\n\r\nIf you have any more questions please post them in stack overflow. Thanks!\r\n\r\n", "Thank you for quick reply. I will go and take a look the paper you refer! "]}, {"number": 37121, "title": "Added see also in array_ops", "body": "Added see also for a few similar functions in array_ops.py. Partially addresses #36786 ", "comments": ["@mihaimaruseac My apologies for having linked a wrong issue. The link to the reference issue has been revised. The issue requests that edits be made to docstrings to ensure user friendly navigation of the documentation, and this PR directly engages with that concern. Can you suggest what additional edits should be made to this PR? Thank you in advance.", "Oh, in that case it looks good. But let's see what the owners of the documentation project suggest.\r\n\r\nCC @lamberta @yashk2810 "]}, {"number": 37120, "title": "Edited docstring, y_pred for SparseCategoricalAcc", "body": "1. The `y_pred` example was previously given as `[0.1, 0.9, 0.8]`, which is odd given that the probabilities should sum up to one after passing through softmax. Edited to `[0.1, 0.6, 0.3]` to maintain integrity of example while posing a probable result.\r\n2. The docstring for master was not up-to-date with that of Tag 2.1.0. Made changes so that they are even.\r\n\r\nRelates to #36844.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37120) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37120) for more info**.\n\n<!-- ok -->"]}, {"number": 37119, "title": "Feature Request: [Keras] Add _keras_history to tensor in Eager Mode", "body": "For some losses such as CRF loss, it's computation does not only rely on `y_true` and `y_pred`, it must access the layer's internal variable.\r\n\r\n In such a situation, `_keras_history` will be greate helpful.  The loss function can use `_keras_history` of `y_pred` to get the layer instance then get the variables. \r\n\r\nBut in eager mode, the output tensor does not have `_keras_history`, which makes such losses not easy to implement in Keras without a custom patch to layers or custom models.\r\n\r\nrelated to tensorflow/addons#377", "comments": ["@howl-anderson Thanks for the issue!\r\n\r\nThis isn't something we can support in eager mode. KerasHistory is only set on a Tensor during the Functional API construction phase.\r\n\r\nFor losses that require variables, you can use `add_loss` inside a custom `Layer`:\r\n\r\n```python\r\nclass LossLayer(tf.keras.layers.Layer):\r\n  def call(self, x, y):\r\n    ...\r\n    self.add_loss(my_loss(x, y, self.variable))\r\n   return x\r\n\r\nx_inputs = tf.keras.Input(...)\r\ny_inputs = tf.keras.Input(...)\r\noutputs = LossLayer()(x_inputs, y_inputs)\r\nmodel = tf.keras.Model([x_inputs, y_inputs], outputs)\r\n```\r\n"]}, {"number": 37118, "title": "How to save model by training steps using tf.keras.callbacks.ModelCheckpoint", "body": "My training data is very large, which means it have many training steps each epoch. I want to use `tf.keras.callbacks.ModelCheckpoint` to save a model every `100,000` steps, and save all these models in disk, not rewrite the model files when saving a new one. But I have no idea how to do this after i read the documentation. It seems that this callback is designed to save a model by epoch.\r\n\r\nSo, do i have to implement a custom callback to save the model by steps?", "comments": ["@luozhouyang  \r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "ok"]}, {"number": 37117, "title": "module 'tensorflow.python.framework.op_def_registry' has no attribute 'register_op_list'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n```\r\n(tfpose) C:\\Users\\S V J KUMAR\\Downloads\\tf-pose-estimation-master (1)\\tf-pose-estimation-master>python run.py --image=p1.jpg\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 7, in <module>\r\n    from tf_pose import common\r\n  File \"C:\\Users\\S V J KUMAR\\Downloads\\tf-pose-estimation-master (1)\\tf-pose-estimation-master\\tf_pose\\__init__.py\", line 5, in <module>\r\n    from tf_pose.runner import infer, Estimator, get_estimator\r\n  File \"C:\\Users\\S V J KUMAR\\Downloads\\tf-pose-estimation-master (1)\\tf-pose-estimation-master\\tf_pose\\runner.py\", line 8, in <module>\r\n    from tf_pose import eval\r\n  File \"C:\\Users\\S V J KUMAR\\Downloads\\tf-pose-estimation-master (1)\\tf-pose-estimation-master\\tf_pose\\eval.py\", line 13, in <module>\r\n    from tf_pose.estimator import TfPoseEstimator\r\n  File \"C:\\Users\\S V J KUMAR\\Downloads\\tf-pose-estimation-master (1)\\tf-pose-estimation-master\\tf_pose\\estimator.py\", line 14, in <module>\r\n    import tensorflow.contrib.tensorrt as trt\r\n  File \"C:\\Users\\S V J KUMAR\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib import bayesflow\r\n  File \"C:\\Users\\S V J KUMAR\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\contrib\\bayesflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.contrib.bayesflow.python.ops import csiszar_divergence\r\n  File \"C:\\Users\\S V J KUMAR\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\contrib\\bayesflow\\python\\ops\\csiszar_divergence.py\", line 26, in <module>\r\n    from tensorflow.contrib.bayesflow.python.ops.csiszar_divergence_impl import *\r\n  File \"C:\\Users\\S V J KUMAR\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\contrib\\bayesflow\\python\\ops\\csiszar_divergence_impl.py\", line 43, in <module>\r\n    from tensorflow.contrib import framework as contrib_framework\r\n  File \"C:\\Users\\S V J KUMAR\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\contrib\\framework\\__init__.py\", line 93, in <module>\r\n    from tensorflow.contrib.framework.python.ops import *\r\n  File \"C:\\Users\\S V J KUMAR\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\__init__.py\", line 28, in <module>\r\n    from tensorflow.contrib.framework.python.ops.variables import *\r\n  File \"C:\\Users\\S V J KUMAR\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\variables.py\", line 26, in <module>\r\n    from tensorflow.contrib.framework.python.ops import gen_variable_ops\r\n  File \"C:\\Users\\S V J KUMAR\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\gen_variable_ops.py\", line 95, in <module>\r\n    _op_def_lib = _InitOpDefLibrary(b\"\\nR\\n\\017ZeroInitializer\\022\\013\\n\\003ref\\\"\\001T\\200\\001\\001\\032\\022\\n\\noutput_ref\\\"\\001T\\200\\001\\001\\\"\\033\\n\\001T\\022\\004type:\\020\\n\\0162\\014\\001\\002\\003\\t\\004\\005\\006\\021\\023\\026\\027\\016\\230\\001\\001\")\r\n  File \"C:\\Users\\S V J KUMAR\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\gen_variable_ops.py\", line 57, in _InitOpDefLibrary\r\n    _op_def_registry.register_op_list(op_list)\r\nAttributeError: module 'tensorflow.python.framework.op_def_registry' has no attribute 'register_op_list'\r\n\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nimport argparse\r\nimport logging\r\nimport sys\r\nimport time\r\nimport os\r\n\r\nfrom tf_pose import common\r\nimport cv2\r\nimport numpy as np\r\nfrom tf_pose.estimator import TfPoseEstimator\r\nfrom tf_pose.networks import get_graph_path, model_wh\r\n\r\nlogger = logging.getLogger('TfPoseEstimatorRun')\r\nlogger.handlers.clear()\r\nlogger.setLevel(logging.DEBUG)\r\nch = logging.StreamHandler()\r\nch.setLevel(logging.DEBUG)\r\nformatter = logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s')\r\nch.setFormatter(formatter)\r\nlogger.addHandler(ch)\r\n\r\n\r\nif __name__ == '__main__':\r\n    os.chdir('..')\r\n    parser = argparse.ArgumentParser(description='tf-pose-estimation run')\r\n    parser.add_argument('--image', type=str, default='./images/p1.jpg')\r\n    parser.add_argument('--model', type=str, default='cmu',\r\n                        help='cmu / mobilenet_thin / mobilenet_v2_large / mobilenet_v2_small')\r\n    parser.add_argument('--resize', type=str, default='0x0',\r\n                        help='if provided, resize images before they are processed. '\r\n                             'default=0x0, Recommends : 432x368 or 656x368 or 1312x736 ')\r\n    parser.add_argument('--resize-out-ratio', type=float, default=4.0,\r\n                        help='if provided, resize heatmaps before they are post-processed. default=1.0')\r\n\r\n    args = parser.parse_args()\r\n\r\n    w, h = model_wh(args.resize)\r\n    if w == 0 or h == 0:\r\n        e = TfPoseEstimator(get_graph_path(args.model), target_size=(432, 368))\r\n    else:\r\n        e = TfPoseEstimator(get_graph_path(args.model), target_size=(w, h))\r\n\r\n    # estimate human poses from a single image !\r\n    image = common.read_imgfile(args.image, None, None)\r\n    if image is None:\r\n        logger.error('Image can not be read, path=%s' % args.image)\r\n        sys.exit(-1)\r\n\r\n    t = time.time()\r\n    humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=args.resize_out_ratio)\r\n    elapsed = time.time() - t\r\n\r\n    logger.info('inference image: %s in %.4f seconds.' % (args.image, elapsed))\r\n\r\n    image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\r\n\r\n    try:\r\n        import matplotlib.pyplot as plt\r\n\r\n        fig = plt.figure()\r\n        a = fig.add_subplot(2, 2, 1)\r\n        a.set_title('Result')\r\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\r\n\r\n        bgimg = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_BGR2RGB)\r\n        bgimg = cv2.resize(bgimg, (e.heatMat.shape[1], e.heatMat.shape[0]), interpolation=cv2.INTER_AREA)\r\n\r\n        # show network output\r\n        a = fig.add_subplot(2, 2, 2)\r\n        plt.imshow(bgimg, alpha=0.5)\r\n        tmp = np.amax(e.heatMat[:, :, :-1], axis=2)\r\n        plt.imshow(tmp, cmap=plt.cm.gray, alpha=0.5)\r\n        plt.colorbar()\r\n\r\n        tmp2 = e.pafMat.transpose((2, 0, 1))\r\n        tmp2_odd = np.amax(np.absolute(tmp2[::2, :, :]), axis=0)\r\n        tmp2_even = np.amax(np.absolute(tmp2[1::2, :, :]), axis=0)\r\n\r\n        a = fig.add_subplot(2, 2, 3)\r\n        a.set_title('Vectormap-x')\r\n        # plt.imshow(CocoPose.get_bgimg(inp, target_size=(vectmap.shape[1], vectmap.shape[0])), alpha=0.5)\r\n        plt.imshow(tmp2_odd, cmap=plt.cm.gray, alpha=0.5)\r\n        plt.colorbar()\r\n\r\n        a = fig.add_subplot(2, 2, 4)\r\n        a.set_title('Vectormap-y')\r\n        # plt.imshow(CocoPose.get_bgimg(inp, target_size=(vectmap.shape[1], vectmap.shape[0])), alpha=0.5)\r\n        plt.imshow(tmp2_even, cmap=plt.cm.gray, alpha=0.5)\r\n        plt.colorbar()\r\n        plt.show()\r\n    except Exception as e:\r\n        logger.warning('matplitlib error, %s' % e)\r\n        cv2.imshow('result', image)\r\n        cv2.waitKey()\r\n\r\n\r\n\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\ni am using openpose code to find the 3D coordinates of a person in 2D image . by the tutorial i found this code and when running this in python 3 . \r\n\r\nany other way for the accomplishment of the objective can also be entertained. \r\nwill be grateful  for the help \r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["@thesrisri \r\n\r\nCan you help us with the colab link or simple standalone code with proper indentation and supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version? Please fill the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I am also facing the above mentioned issues.\r\nI am running the train.py for object detection with tensorflow 2.1.0 .\r\n@thesrisri: If you got the solution please help me. \r\nThanks", "@thesrisri Did your issue get resolved?\r\nIf so, please help me out as well."]}, {"number": 37116, "title": "How to build Tflite-gpu to use GPU delegate, NNAPI delegate for devices like Hikey (AARCH64)", "body": "I was trying to use label_image.cpp example in Tflite. I was unable to make use of delegate options in the example after setting gl_backend, useNNAPI , accel and old accel , hexagon etc.. delegate flags to true. \r\n\r\nIs GPU delegate only restricted to Android and Ios devices ?\r\n\r\nIf not how to build TFlite with gpu on other devices like host pc for simulation, Hikey(AArch64) and Raspberry Pi(armv7,rpi) ?\r\n\r\nIs NNAPI & Hexagon delegate only restricted to Android and Ios devices ?\r\nIf not how to build TFlite with NNAPI on other devices like host pc for simulation, Hikey(AArch64) and Raspberry Pi(armv7,rpi) ?\r\n\r\nDoes devices having snapdragon soc's such as (835,660) which are running in Linux(development boards which dont have Android Os). If yes , how to build with the delegate and increase performance ?\r\n\r\nI don't find any Makefile to build with these delegate options enabled. Could you provide steps to build (or) Makefile for making use of these delegate options?", "comments": ["Currently GPU delegate only works on Android and iOS.\r\nAnd NNAPI is only available on Android.\r\nhttps://developer.android.com/ndk/guides/neuralnetworks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37116\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37116\">No</a>\n", "Is there roadmap visibility when Raspberry Pi 4 GPU will be a supported TFLite GPU delegate?"]}, {"number": 37115, "title": "fix resize image bug", "body": "fix #36963", "comments": ["@fsx950223 can you please check the build failures ?", "> @fsx950223 can you please check the build failures ?\r\n\r\nFixed"]}, {"number": 37114, "title": "FileWriter in tf.summary solved", "body": "Issue: #37113 ", "comments": ["Lemme Solve this and create a  PR again. ", "Closing this PR and creating a new one.", "Link to New PR : https://github.com/tensorflow/tensorflow/pull/37223\r\n@gbaned "]}, {"number": 37113, "title": "AttributeError: module 'tensorboard.summary._tf.summary' has no attribute 'FileWriter'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): pip3\r\n - TensorFlow version (use command below): 2.1.0 \r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n2020-02-27 10:37:08.494214: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n[       OK ] ModuleTest.testBuiltInName\r\n[ RUN      ] ModuleTest.testCanLoadWithPkgutil\r\n[       OK ] ModuleTest.testCanLoadWithPkgutil\r\n[ RUN      ] ModuleTest.testCompatV2HasCompatV1\r\n[       OK ] ModuleTest.testCompatV2HasCompatV1\r\n[ RUN      ] ModuleTest.testDict\r\n[       OK ] ModuleTest.testDict\r\n[ RUN      ] ModuleTest.testDocString\r\n[       OK ] ModuleTest.testDocString\r\n[ RUN      ] ModuleTest.testName\r\n[       OK ] ModuleTest.testName\r\n[ RUN      ] ModuleTest.testSummaryMerged\r\n[  FAILED  ] ModuleTest.testSummaryMerged\r\n[ RUN      ] ModuleTest.test_session\r\n[  SKIPPED ] ModuleTest.test_session\r\n======================================================================\r\nERROR: testSummaryMerged (__main__.ModuleTest)\r\ntestSummaryMerged (__main__.ModuleTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"module_test.py\", line 79, in testSummaryMerged\r\n    tf.summary.FileWriter\r\nAttributeError: module 'tensorboard.summary._tf.summary' has no attribute 'FileWriter'\r\n\r\n----------------------------------------------------------------------\r\nRan 8 tests in 0.035s\r\n", "comments": ["@Saduf2019  Please check the changes i made.", "@ayushmankumar7 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n\r\nCould you try \"changing tf.summary.FileWriter() to tf.train.SummaryWriter()\"\r\nAlso you may check #412 [link](https://github.com/eriklindernoren/PyTorch-YOLOv3/issues/327)\r\n\r\n", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/api/tests/module_test.py\r\nThis is the link to the code snippet to reproduce the issue. \r\n\r\n\r\n", "@Saduf2019 \r\nChanging tf.summar.FileWrite() to tf.train.SummaryWriter() gives the following error: \r\n\r\nTraceback (most recent call last):\r\n  File \"module_test.py\", line 79, in testSummaryMerged\r\n    tf.train.SummaryWriter()\r\nAttributeError: module 'tensorflow_core._api.v2.train' has no attribute 'SummaryWriter'", "@ayushmankumar7 \r\nplease let us know if we could move this to closed status as it is monitored in #37223 ", "Yeah. It's not a problem. ", "Moving this issue to closed status with confirmation that it will be monitored in #37233 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37113\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37113\">No</a>\n", "In Google Colab, you can use the magic word ```%tensorflow_version 1.x``` to select tensorflow version 1."]}, {"number": 37112, "title": "Custom training loop, iteration over Dataset hangs under MultiWorkerMirroredStrategy", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): TensorFlow 2.0.1 GPU official docker image\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 2.0.1\r\n- Python version: - Bazel \r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory: CUDA 10.1 V100 16G\r\n\r\n**Describe the current behavior**\r\nI use the following scheme to iterate over Dataset.\r\n```\r\nfor epoch in range(FLAGS.epoch):\r\n        for train_data in train_dataset:\r\n           ...\r\n```\r\nTrain stops at the last few batches of the first epoch. I found that documentation https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras  mentions only `keras.fit`\r\n**Describe the expected behavior**\r\nIt should be explicitly mentioned, or produce some logs when running in such scenario.\r\nCurrently, before the hang, only one line is printed in log.\r\n```\r\nINFO:tensorflow:Collective batch_all_reduce: xxx all-reduces, num_workers = xxx\r\nI0225 cross_device_ops.py:1107] Collective batch_all_reduce: xxx all-reduces, num_workers = xxx\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@372046933 \r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "This is just a documentation issue. The doc currently states (https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras)\r\n\r\n> [...] MultiWorkerMirroredStrategy does not support last partial batch handling [\u2026]\r\n\r\nBut this isn't mentioned in the docu of `MultiWorkerMirroredStrategy`.\r\n\r\nOf course it would be better to not have that limitation", "@Flamefire Sorry, my company forbid copy any data out. Only data in. Otherwise I have uploaded all the code.", "@372046933 \r\n\r\nWill it be possible to share the sample data so i can test it on the Tesnsorflow tutorials to reproduce the issue.Thanks!", "@ravikyram Sorry, can't either. In only, no out :(. The data is actually ImageNet in TFRecord format.  So I think you can build sample data with fixed size image."]}, {"number": 37111, "title": "representative_data_gen input order?", "body": "In tflite post integer quantization, `converter.representative_dataset` is necessary.\r\n\r\nHowever, the documentation never specifies the order of the fed inputs. Are they ordered by lexicographic order of the names, size of shapes or even random order? When multiple inputs are present, it is totally a guessing game.\r\n\r\n", "comments": ["@tigert1998 Are you interested in raising a PR to update the docs? Thanks!", "No, since I don't know either.", "@tigert1998 and @jvishnuvardhan , I have added line in doc corresponding to the necessity of `converter.representative_dataset`. You have also mentioned something about order of inputs. I don't understand that. Can you please elaborate so that I will also make changes corresponding to that in the doc if needed.", "@ashutosh1919 \r\nFor instance, suppose that I generate a `saved_model` model with the following code:\r\n```py\r\n# inputs: List[Tensor]\r\n# outputs: List[Tensor]\r\n\r\ninputs_dic = {\r\n    \"input_{}\".format(idx): i\r\n    for idx, i in zip(range(len(inputs)), inputs)\r\n}\r\noutputs_dic = {\r\n    \"output_{}\".format(idx): o\r\n    for idx, o in zip(range(len(outputs)), outputs)\r\n}\r\nbuilder = tf.saved_model.builder.SavedModelBuilder(path)\r\nsigs = {}\r\nsigs[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = \\\r\n    tf.saved_model.signature_def_utils.predict_signature_def(\r\n    inputs_dic, outputs_dic\r\n)\r\nbuilder.add_meta_graph_and_variables(\r\n    sess,\r\n    [tag_constants.SERVING],\r\n    signature_def_map=sigs\r\n)\r\nbuilder.save()\r\n```\r\n\r\nThen I try to load the `saved_model` with `tf.lite.TFLiteConverter.from_saved_model` and configure the quantization options of this `converter`, including `representative_dataset`. However, I found that the fed inputs in `representative_data_gen` are not always in the order of `input_0`, `input_1`, etc. Plus, it is also not specified in the doc in detail.\r\n\r\nIt is not something that a end-user like me can pull a request. I checked the code. It seems that tflite feed inputs in the order of the presents of inputs in the serialized protobuf. It's kind of a \"default\" order. But nobody knows what the \"default\" order is.", "@tigert1998 , Can you please provide the link of the file in tensorflow where you have checked this code. I also want to take a look at it.", "This has not been resolved yet. It seems like the order is arbitrary. The order even changes when loading the same saved model file. When there are multiple inputs of different shapes, the tflite converter is unusable.", "@DaviesX Can you please open a new issue with a simple standalone code to reproduce the issue? As the current issue is older, it is better to open a new issue so that we can track and resolve the issue faster. Thanks! ", "@jvishnuvardhan  My issue actually has other causes besides having multiple inputs. This should go to a separate issue."]}, {"number": 37110, "title": "Tflite build error on Android.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: All of them\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.2-nightly\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI've built a arm64 .so from master (using commit: 001037c4692a09e453777a96a609251cb7c94fac) and I'm including `tensorflow/tensorflow/lite` headers into my Android project. However, I'm getting the following error when I attempt to build my android project: \r\n```\r\nIn file included from ...../distribution/include/tensorflow\\tensorflow/lite/core/subgraph.h:27:\r\n\r\n...../distribution/include/tensorflow\\tensorflow/lite/delegates/nnapi/nnapi_delegate.h:23:10: fatal error: 'absl/types/optional.h' file not found\r\n\r\n#include \"absl/types/optional.h\"\r\n\r\n         ^~~~~~~~~~~~~~~~~~~~~~~\r\n1 error generated.\r\nninja: build stopped: subcommand failed.\r\n```\r\nBuilding the project using .so from r2.1 or earlier branches + using headers of the same version works fine.\r\nAlso if I attempt using .so build from master but use old headers from `r2.1`, the Android project builds but fails whenever using the tflite interpreter. ", "comments": ["Did you run lite/tools/make/download_dependencies.sh to download proper ABSL library?", "I've ran `./tensorflow/lite/tools/make/download_dependencies.sh` and it seemed to have completed downloading everything with no errors. However, after re-building the .so (using command `bazel build --cxxopt='-std=c++11' -c opt --config=android_arm64 //tensorflow/lite:libtensorflowlite.so`) I still get the same error when trying to build the Android project.", "Oh, you're using Bazel. So it's ok to build TFLite only in Tensorflow repo, right?\r\nI think you need to copy some dependencies from tensorflow/workspace.bzl so the Bazel can download necessary files.", "@terryheo I'm not sure what your suggestion for me to try is. Could you please clarify what you'd like me to try with exact steps I need to do?", "Since this is a build issue of your custom repo, we can't support it further.\r\nYou need to check how Bazel downloads dependencies.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37110\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37110\">No</a>\n", "Manually adding `tensorflow/lite/tools/make/downloads/absl` to the downstream project's include path fixes this error.", "I can confirm that adding `${TENSORFLOW_DIR}/tensorflow/lite/tools/make/downloads/absl` to `target_include_directories` in our cmake build file fixed it. Thanks @tailsu!"]}, {"number": 37109, "title": "Build from source(python2.7 ubuntu 16.04) has error:  AttributeError: attribute '__doc__' of 'type' objects is not writable", "body": "**System information** \r\nLinux Ubuntu 16.04): \r\n\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version 1.13.2\r\n- Python version: 2.7.12\r\n- Bazel version 0.22.0\r\n- GCC  5.4.0\r\n- CUDA/cuDNN version: - 10.0/7.6.4\r\ncuda-repo-ubuntu1604-10-0-local-10.0.130-410.48_1.0-1_amd64.deb\r\ncuda-repo-ubuntu1604-10-0-local-nvjpeg-update-1_1.0-1_amd64.deb\r\n\r\nlibcudnn7_7.6.4.38-1+cuda10.0_amd64.deb\r\nlibcudnn7-dev_7.6.4.38-1+cuda10.0_amd64.deb\r\nlibnvinfer5_5.0.2-1+cuda10.0_amd64.deb\r\nlibnvinfer-dev_5.0.2-1+cuda10.0_amd64.deb\r\nlibnvinfer-samples_5.0.2-1+cuda10.0_all.deb\r\ntensorrt_5.0.2.6-1+cuda10.0_amd64.deb\r\n./configure    cuda  yes    tensorrt yes\r\n\r\nalso tried on ubuntu 18.04  \r\ntensorflow 1.15.2\r\nbazel 0.24.1\r\ncuda/cudnn   10.2/7.6.5 same problem\r\ntensorrt  nv-tensorrt-repo-cuda10.2-trt6.0.1.8-ga-20191108\r\n--------------------------------\r\n     return *(float*)&ilane;\r\n                      ^\r\ntensorflow/lite/toco/tflite/import.cc: At global scope:\r\ntensorflow/lite/toco/tflite/import.cc:195:6: warning: 'bool toco::tflite::{anonymous}::Verify(const void*, size_t)' defined but not used [-Wunused-function]\r\n bool Verify(const void* buf, size_t len) {\r\n      ^\r\nERROR: /home/weilo/tensorflow-1.13.2/tensorflow/BUILD:579:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/weilo/.cache/bazel/_bazel_weilo/5dc966cf5c885efffef93d319484bf56/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/weilo/.cache/bazel/_bazel_weilo/5dc966cf5c885efffef93d319484bf56/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n  File \"/home/weilo/.cache/bazel/_bazel_weilo/5dc966cf5c885efffef93d319484bf56/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/framework/framework_lib.py\", line 52, in <module>\r\n    from tensorflow.python.framework.importer import import_graph_def\r\n  File \"/home/weilo/.cache/bazel/_bazel_weilo/5dc966cf5c885efffef93d319484bf56/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/framework/importer.py\", line 28, in <module>\r\n    from tensorflow.python.framework import function\r\n  File \"/home/weilo/.cache/bazel/_bazel_weilo/5dc966cf5c885efffef93d319484bf56/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/framework/function.py\", line 36, in <module>\r\n    from tensorflow.python.ops import resource_variable_ops\r\n  File \"/home/weilo/.cache/bazel/_bazel_weilo/5dc966cf5c885efffef93d319484bf56/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/ops/resource_variable_ops.py\", line 39, in <module>\r\n    from tensorflow.python.ops import variables\r\n  File \"/home/weilo/.cache/bazel/_bazel_weilo/5dc966cf5c885efffef93d319484bf56/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/ops/variables.py\", line 133, in <module>\r\n    \"* `ONLY_FIRST_TOWER`: Deprecated alias for `ONLY_FIRST_REPLICA`.\\n  \")\r\nAttributeError: attribute '__doc__' of 'type' objects is not writable\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 16309.185s, Critical Path: 310.76s\r\nINFO: 15104 processes: 15104 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["@gms2009 \r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem. Also, looks like Tensorflow not installed properly.please try with [tested build configurations](https://www.tensorflow.org/install/source#gpu) for installing tensorflow.Thanks!", "tensorflow with python3(build from source in the same machine) has been installed.  that might be the problem?\r\nactually tensorflow with python3 has not been installed, I will try a fresh and clean installation and post", "bazel version is 0.20.0\r\nbuild configure is   cuda yes   tensorrt yes      all others default\r\nerror is around 19xxx\r\nfresh install ubuntu 16.04.06\r\nfollowing is .bash_history unmodified, so has some trash\r\n\r\nsudo apt update\r\nsudo apt upgrade\r\nsudo mount /dev/sda5 /mnt/\r\nsudo dpkg -i /mnt/extra/cuda-repo-ubuntu1604-10-0-local-10.0.130-410.48_1.0-1_amd64.deb \r\nsudo apt-key add /var/cuda-repo-10-0-local-10.0.130-410.48/7fa2af80.pub\r\nsudo apt update\r\nsudo apt install cuda\r\nvi vvv\r\nnvidia-smi \r\nsudo mount /dev/sda5\r\nsudo mount /dev/sda5 /mnt/\r\nsudo dpkg -i /mnt/extra/u16/cuda-repo-ubuntu1604-10-0-local-nvjpeg-update-1_1.0-1_amd64.deb \r\nsudo dpkg -i /mnt/extra/nv-tensorrt-repo-ubuntu1604-cuda10.0-trt5.0.2.6-ga-20181009_1-1_amd64.deb \r\ncd /var/nv-tensorrt-repo-cuda10.0-trt5.0.2.6-ga-20181009/\r\nls\r\nsudo dpkg -i libcudnn7_7.3.1.20-1+cuda10.0_amd64.deb libcudnn7-dev_7.3.1.20-1+cuda10.0_amd64.deb libnvinfer5_5.0.2-1+cuda10.0_amd64.deb libnvinfer-dev_5.0.2-1+cuda10.0_amd64.deb libnvinfer-samples_5.0.2-1+cuda10.0_all.deb tensorrt_5.0.2.6-1+cuda10.0_amd64.deb \r\nsudo apt update\r\nsudo apt upgrade\r\ncd\r\ncp /mnt/extra/tensorflow-1.13.2.zip  ./\r\nunzip tensorflow-1.13.2.zip \r\ncd tensorflow-1.13.2/\r\n./configure \r\nsudo dpkg -i /mnt/extra/bazel_0.22.0-linux-x86_64.deb \r\nsudo apt install zlib1g-dev\r\n./configure \r\nsudo apt install python-dev python-pip\r\npip install six numpy wheel setuptools mock future keras_applications keras_preprocessing enum\r\npip install --upgrade pip\r\npip install six numpy wheel setuptools mock future keras_applications keras_preprocessing enum\r\n./configure \r\nls /mnt/extra/bazel\r\nsudo dpkg -i /mnt/extra/bazel_0.20.0-linux-x86_64.deb \r\nsudo apt install java8-sdk-headless java8-sdk\r\nsudo apt install openjdk-8-sdk-headless openjdk-8-sdk\r\nsudo apt install openjdk-8-jdk-headless openjdk-8-jdk\r\n./configure \r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n", "unable to open tested build configurations, hope you can put some info here, like version for cuda cudnn tensorrt tensorflow bazel, for python2 not 3", "@gms2009 \r\nPlease, see tested builded configuration below\r\n\r\n![image](https://user-images.githubusercontent.com/51902062/75515814-fd19c800-5a20-11ea-973f-fcbd5f825eb8.png)\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37109\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37109\">No</a>\n"]}, {"number": 37108, "title": "TF 2.0 MultiWorkerMirror strategy hangs for MNIST Keras model training", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Red Hat Enterprise Linux Server release 7.4 (Maipo)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 2.0\r\n- Python version: 3.7  \r\nBazel version (if compiling from source): 0.26.1 \r\n- GCC/Compiler version (if compiling from\r\nsource): GCC 7.3.1\r\n--\r\n- CUDA/cuDNN version: - GPU model and memory: 10.1/7.6  NVIDIA TESLA V100\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI'm using TF 2.0 multi-worker mirror strategy to  train keras mnist model using two nodes. Each node has 6 NVIDIA Tesla V100 GPUs. It hangs after starting the  grpc server.Please see the log for details.\r\n \r\n**Describe the expected behavior**\r\nIt should start synchronous multi-worker training. \r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nPlease see the code in attachment.\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nPlease see the log in attachment.\r\n\r\n[mnist_multi_node_multi_gpu_py.txt](https://github.com/tensorflow/tensorflow/files/4258761/mnist_multi_node_multi_gpu_py.txt)\r\n[worker0.txt](https://github.com/tensorflow/tensorflow/files/4259179/worker0.txt)\r\n[worker1.txt](https://github.com/tensorflow/tensorflow/files/4259180/worker1.txt)\r\n\r\n\r\n", "comments": ["network issue. It works for me now. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37108\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37108\">No</a>\n", "@arde171 Could you expand on what the network issue was? I am facing a very similar issue on only one of multiple machines on the network", "For anyone with this same issue, the only resolution I found was a complete OS reinstall. This surely is not a pleasant solution and is very likely not possible for most people. I validated all conceivable network configuration, full CUDA wipe/reinstall, full TF reinstall, etc. None of the solutions worked other than a system wipe and reinstall"]}, {"number": 37107, "title": "updating dead web link", "body": "fixes #37047", "comments": ["hi, the merge just remove the dead link, but where is the new link\r\n![image](https://user-images.githubusercontent.com/33815430/75516749-14b07b00-5a38-11ea-9b7a-a88fcdcf02c1.png)\r\n"]}, {"number": 37106, "title": "Make DNNL1.2 default lib for MKL backend", "body": "This PR makes DNNL 1.2 default library for MKL backend.\r\n\r\n*NOTE:* This PR depends on #37081 PR to be merged to master.", "comments": ["Thanks for keeping track, Penporn!"]}, {"number": 37105, "title": "Tensorflow 2.0 uncompatible with guinicorn", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code\r\n- OS Platform and Distribution (e.g.,Linux Ubuntu 18.04): \r\n- TensorFlow installed from binary\r\n - TensorFlow version 2.0: \r\n- Keras Version: tested with 2.3.1 and 2.3.0\r\n- Python version: - 3.6.5\r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nI have a feed-forward NN and I saved it as .h5 and I can make predictions, but when I set an endpoint using Flask and guinicorn and then I call my model for predictions I get the following message: \r\nAttributeError: 'gevent._local.local' object has no attribute 'value'\r\n\r\n**Describe the expected behavior**\r\nthe expected behavior would be that when I made a request\r\n\r\ncurl -X POST ....\r\nit returned the predictions for my request", "comments": ["@jccastrom \r\nplease share a simple stand alone code with all dependencies for us to replicate the issue faced.\r\nAlso please refer to this [link](https://stackoverflow.com/questions/58018009/attributeerror-gevent-local-local-object-has-no-attribute-value) for reference", "@jccastrom \r\nCould you please update on the above comment.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37105\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37105\">No</a>\n"]}, {"number": 37104, "title": "Using Precison metric in compile method raises shape mismatch error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Windows 10 Home\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No mobile device\r\n- **TensorFlow installed from (source or binary)**: Installed using Pip command\r\n- **TensorFlow version (use command below)**: 2.1.0\r\n- **Python version**: 3.6.8\r\n- **Bazel version (if compiling from source)**: No\r\n- **GCC/Compiler version (if compiling from source)**: No\r\n- **CUDA/cuDNN version**: No\r\n- **GPU model and memory**: No\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI am trying o implement different training metrics for keras sequential API. However when I try to implement precision method I get an error of shape mismatch. The same thing works when I use sigmoid as activation function instead of softmax. \r\n\r\nI am trying to solve binary classification problem.\r\n\r\nCode to create model:\r\n```\r\ndef create_model():\r\n    model = tf.keras.Sequential([\r\n    feature_layer,\r\n    tf.keras.layers.Dense(units = 12, activation='relu', use_bias = True, kernel_initializer= 'glorot_normal', bias_initializer = 'zeros', name = 'd1'),\r\n    tf.keras.layers.Dense(units = 6, activation='relu', use_bias = True, kernel_initializer= 'glorot_normal', bias_initializer = 'zeros', name = 'd2'),\r\n    tf.keras.layers.Dense(units = 2, activation='softmax', name = 'out')\r\n    ])\r\n    model.compile(optimizer='adam',\r\n                  loss='sparse_categorical_crossentropy',\r\n                  metrics=[tf.keras.metrics.Precision()])\r\n    return model\r\n```\r\nThe same code runs when I try to run with sigmoid activation fuction with 1 output unit and Binary Crossentropy as my loss. \r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"4.py\", line 217, in <module>\r\n    callbacks = ALL_CALLBACKS)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 85, in distributed_function\r\n    per_replica_function, args=args)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\", line 763, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\", line 1819, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\", line 2164, in _call_for_each_replica\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 433, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\", line 316, in train_on_batch\r\n    model, outs, targets, sample_weights=sample_weights, masks=masks)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\", line 74, in _eager_metrics_fn\r\n    skip_target_masks=model._prepare_skip_target_masks())\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2004, in _handle_metrics\r\n    target, output, output_mask))\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 1955, in _handle_per_output_metrics\r\n    metric_fn, y_true, y_pred, weights=weights, mask=mask)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\", line 1155, in call_metric_function\r\n    return metric_fn(y_true, y_pred, sample_weight=weights)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\", line 196, in __call__\r\n    replica_local_fn, *args, **kwargs)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\distribute\\distributed_training_utils.py\", line 1135, in call_replica_local_fn\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\", line 179, in replica_local_fn\r\n    update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\metrics_utils.py\", line 76, in decorated\r\n    update_op = update_state_fn(*args, **kwargs)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\", line 1216, in update_state\r\n    sample_weight=sample_weight)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\metrics_utils.py\", line 303, in update_confusion_matrix_variables\r\n    y_pred.shape.assert_is_compatible_with(y_true.shape)\r\n  File \"C:\\Users\\Aniket\\Desktop\\Class\\class_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\", line 1110, in assert_is_compatible_with\r\n    raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\nValueError: Shapes (None, 2) and (None, 1) are incompatible\r\n```", "comments": ["@aniketbote \r\n\r\nCan you please help us with the colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "[https://colab.research.google.com/drive/1zBAVrau6tmShvA7yo75XgV9DmblDi4GP](url)\r\n\r\nThis is the colaboratory link that can recreate the error. ", "@aniketbote \r\nThere is no information is available in the link you have shared. Request you to send the correct link and help me to reproduce the issue. It helps us in localizing the issue faster. Thanks!", "Sorry about that.\r\n[Colab_link](https://colab.research.google.com/drive/1zBAVrau6tmShvA7yo75XgV9DmblDi4GP)\r\nThis is the correct link.", "I know the issue but don't whether that is the expected behavior or not. When using sigmoid the output layer gives array of shape (n * 1) for binary classification problem and when using softmax it outputs (n * 2). The error is because of the assert statement which expects array of shape (n * 1). ", "@aniketbote For this problem `binary_crossentropy` and `sigmoid` are suitable. When you have more than two categories, you can use `categorical_crossentropy` and `softmax`.  I changed `create_model` part of your code which works as expected. Please check the code below.\r\n\r\n```\r\ndef create_model():\r\n    model = tf.keras.Sequential([\r\n    feature_layer,\r\n    tf.keras.layers.Dense(units = 12, activation='relu', use_bias = True, kernel_initializer= 'glorot_normal', bias_initializer = 'zeros', name = 'd1'),\r\n    tf.keras.layers.Dense(units = 6, activation='relu', use_bias = True, kernel_initializer= 'glorot_normal', bias_initializer = 'zeros', name = 'd2'),\r\n    tf.keras.layers.Dense(units = 1, activation='sigmoid', name = 'out')\r\n    ])\r\n    model.compile(optimizer='adam',\r\n                  loss='binary_crossentropy',\r\n                  metrics=['accuracy',tf.keras.metrics.Precision()])\r\n    return model\r\n```\r\n\r\nPlease close the issue if the issue was resolved for you. Thanks!", "As stated in the question, the metric works when I try to use a single sigmoid activation function in my final layer. So does every TensorFlow metric require a single sigmoid function as its final layer to work correctly and will not work if any other activation function like softmax is used? \r\n\r\nAlso, the precision metric fails if we try to use it for a multiclass classification problem with multiple softmax units in the final layer.\r\n\r\nTo summarize we cannot use any of the metrics provided by TensorFlow if we have more than 1 unit in our final layer. So is it the expected behavior?", "Is anyone working on this issue?\r\nI would like to work on this issue.", "> So does every TensorFlow metric require a single sigmoid function as its final layer to work correctly and will not work if any other activation function like softmax is used?\r\n\r\nYou can use metrics with multiple output units (Softmax or otherwise) if you use a non-sparse loss e.g., `categorical_crossentropy` (opposed to `sparse_categorical_crossentropy`) and encode your labels as one-hot vectors. You may need to use the `class_id` parameter to compute the metric for each class in the case of precision/recall (I'm not sure what the behavior is otherwise).\r\n\r\nThat said, it would be great if sparse losses were supported for metrics computed over multiple output units to save on memory. Is that what is being proposed in this issue?", "@aniketbote could you please confirm if you are still interested in working on this issue and would the solution be similiar to what @dwyatte suggested ?", "@goldiegadde I am interested in working on this issue. I have a gist of what I have to do but it would help me a lot if you give some pointers on what should I change and how should I change it. Although I use TensorFlow extensively in my job, this will be my first contribution. So any help/advice is appreciated.", "@aniketbote @goldiegadde I could use this functionality, so I made a quick pass on it in https://github.com/tensorflow/tensorflow/pull/48122 (a few line change in `tensorflow/python/keras/utils/metrics_utils.py` plus tests).\r\n\r\nIf this is something useful, we should figure out whether support for sparse outputs should be implicit as in the draft PR above or explicit and if it explicit, whether usage should be specified by an additional argument on metrics classes (e.g., `sparse_labels=True`) or new sparse metric classes (e.g., `SparsePrecision`, `SparseRecall`, etc). I mentioned this in the draft PR as well.", "Any update on this?\r\nBeen having similar issue here:\r\nhttps://stackoverflow.com/q/68347501/16431106", "Hi @aniketbote ! \r\nWe are checking to see whether you still need help in this issue . Have you checked in Latest stable version TF 2.6 yet?. Thanks!", "Hi @aniketbote ,Could you please share the Colab gist again as the above links to stand alone code could not be found. Feel free to look at similar issues.[link1](https://stackoverflow.com/questions/61550026/valueerror-shapes-none-1-and-none-3-are-incompatible),[link2](https://stackoverflow.com/questions/68347501/tensorflow-keras-using-specific-class-recall-as-metric-for-sparse-categorical-cr) too. Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37104\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37104\">No</a>\n"]}, {"number": 37103, "title": "Add support for more keras.metrics.* to accept from_logits argument.", "body": "**System information** \r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux\r\n- CUDA/cuDNN version: CUDA Version: 10.1\r\n- GPU model and memory: GeForce GTX 1080\r\n\r\n**Describe the current behavior**\r\nCurrently, in [keras.metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/), all the metrics for binary classification only accept values in the range [0, 1]. This is an issue as all the models in TF2.x tutorials output logits (ranges from [-inf, +inf]) and AVOID using an activation in the last layer such as `sigmoid` (ranges from [0, 1])\r\n\r\nCurrent (preferred way): `keras.layers.Dense(1)                                  # Output range is [-inf, +inf]`\r\nBefore (deprecated):      `keras.layers.Dense(1, activation='sigmoid') # Output range is [0,1]`\r\n\r\nNow, if my model outputs values in the range [-inf, +inf] as per the current preferred way, I CANNOT use any of the following metrics as they all expect values in the range [0, 1]\r\n\r\n```\r\n1. keras.metrics.Precision(name='precision')\r\n2. keras.metrics.BinaryAccuracy(name='accuracy')\r\n3. keras.metrics.Recall(name='recall')\r\n4. keras.metrics.AUC(name='auc')\r\n5. keras.metrics.TruePositives(name='tp')\r\n6. keras.metrics.FalsePositives(name='fp')\r\n7. keras.metrics.TrueNegatives(name='tn')\r\n8. keras.metrics.FalseNegatives(name='fn')\r\n```\r\n\r\nHowever, `tf.keras.metrics.BinaryCrossentropy` is the only metric that accepts both range [-inf, inf] and range[0,1] and this is used in all tutorials. \r\n\r\nThe issue is that I want to use all the 8 metrics mentioned above during training, but I cannot. \r\n\r\n**Describe the expected behavior**\r\nProvide an option to have a `from_logits=True` argument in the 8 metrics given above (and other metrics as well)\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nPip Install: TensorFlow 2.x and Numpy\r\n```\r\n! pip install tensorflow numpy\r\n```\r\n\r\n**Code**\r\n\r\n```\r\n# INITIALIZE\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n# define dataset\r\ndataset = np.array([[6,148,72,35,0,33.6,0.627,50,1],\r\n[1,85,66,29,0,26.6,0.351,31,0],\r\n[8,183,64,0,0,23.3,0.672,32,1],\r\n[1,89,66,23,94,28.1,0.167,21,0],\r\n[0,137,40,35,168,43.1,2.288,33,1]])\r\n# split into input (X) and output (y) variables\r\nX = dataset[:,0:8]\r\ny = dataset[:,8]\r\n```\r\n\r\n```\r\n# WORKS: Model output is in range [0, 1]\r\n# define the keras model\r\nmodel = tf.keras.models.Sequential([tf.keras.layers.Dense(12, input_dim=8, activation='relu'),\r\n                                    tf.keras.layers.Dense(1, activation='sigmoid')])\r\n# compile the keras model\r\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(),\r\n              optimizer=tf.keras.optimizers.RMSprop(),\r\n              metrics=[tf.keras.metrics.Accuracy(name='accuracy'),\r\n                       tf.keras.metrics.Precision(name='precision'),\r\n                       tf.keras.metrics.TruePositives(name='tp')])\r\n# fit the keras model on the dataset\r\nmodel.fit(X, y, epochs=10, batch_size=10, verbose=0)\r\n```\r\n\r\n```\r\n# DOES NOT WORK: Model output is in range [-inf, inf]\r\n# define the keras model\r\nmodel = tf.keras.models.Sequential([tf.keras.layers.Dense(12, input_dim=8, activation='relu'),\r\n                                    tf.keras.layers.Dense(1)])\r\n# compile the keras model\r\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n              optimizer=tf.keras.optimizers.RMSprop(),\r\n              metrics=[tf.keras.metrics.Accuracy(name='accuracy'),\r\n                       tf.keras.metrics.Precision(name='precision'),\r\n                       tf.keras.metrics.TruePositives(name='tp')])\r\n# fit the keras model on the dataset\r\nmodel.fit(X, y, epochs=10, batch_size=10, verbose=0)\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  assertion failed: [predictions must be <= 1] [Condition x <= y did not hold element-wise:] [x (sequential_8/dense_17/BiasAdd:0) = ] [[22.0911655][62.7297783][35.2793274]...] [y (metrics/precision/Cast_3/x:0) = ] [1]\r\n\t [[{{node metrics/precision/assert_less_equal/Assert/AssertGuard/else/_11/Assert}}]]\r\n\t [[metrics/tp/assert_greater_equal/Assert/AssertGuard/pivot_f/_31/_61]]\r\n  (1) Invalid argument:  assertion failed: [predictions must be <= 1] [Condition x <= y did not hold element-wise:] [x (sequential_8/dense_17/BiasAdd:0) = ] [[22.0911655][62.7297783][35.2793274]...] [y (metrics/precision/Cast_3/x:0) = ] [1]\r\n\t [[{{node metrics/precision/assert_less_equal/Assert/AssertGuard/else/_11/Assert}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_distributed_function_12253]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n```\r\n", "comments": [":up:", "Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/e1bc35457bf6ffc7c991d9a518a74a04/37103.ipynb). Thanks!", "\ud83c\udd99\r\n\r\nTemporarily I solved using [keras-metric](https://github.com/netrack/keras-metrics).", "This is fixed with tf-nightly version '2.2.0-dev20200325'. Thanks!", "Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37103\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37103\">No</a>\n", "\ud83d\udc4b I was able to reproduce the original issue in both 2.2.0-dev20200325 and 2.2.0.dev20200415 nightly builds. I'm especially interested in the conversation around `keras.metrics.AUC` accepting logits. \r\n\r\nhttps://colab.research.google.com/drive/133fqw4-USGEvQXkI1uIm8cN-jBOMUmeh#scrollTo=864VzT-b44pw", "Is it working for you guys? I just tried out the same test script with TF 2.3, installing with pip under WIN10 x64 and I get the same error:\r\n\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential/dense_1/BiasAdd:0) = ] [[-40.4073257][-30.1479664][-27.2313709]...] [y (Cast_4/x:0) = ] [0]\r\n         [[{{node assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert}}]] [Op:__inference_train_function_1165]`\r\n\r\nI tried also with the docker image tensorflow/tensorflow:2.3.0-gpu and still getting errors", "@adamp87,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 37102, "title": "MUL doesn't work on gpu delegate with error: Dimension can not be reduced to linear", "body": "**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14.6\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone XR, iPhone Xs\r\nTensorFlow installed from (source or binary): installed from source.\r\nTensorFlow version (use command below): 1.14.0\r\nPython version: 3.6\r\nBazel version (if compiling from source): 1.2.1\r\nGCC/Compiler version (if compiling from source): 4.2.1\r\n\r\n**Describe the current behavior**\r\n`Mul` doesn't work with multiple axis while using gpu delegate. (no issue while using cpu).\r\n\r\nTried to create a dummy tflite graph with simply a multiply operation on input of shape of `[1, 1, n, m]` and variable with the same shape. While verifying with TFLite iOS benchmark app with GPU delegate, the benchmark app reports \"failed to apply GPU delegate\" on the mul operation. Note that the model runs fine without gpu delegate.\r\n\r\n**Describe the expected behavior**\r\n\r\nMul operation works on gpu delegate and supports multiple axis.\r\n\r\n**Code to reproduce the issue**\r\nHere's the dummy model that should reproduce the issue in the TFLite ios benchmark app:\r\n[dummy_model.tflite](https://drive.google.com/file/d/1383fCIA0-Uhlmbo8zR16KW3C6FYDX5un/view?usp=sharing)\r\n\r\n**Other info / logs**\r\n```\r\nGraph: [/private/var/containers/Bundle/Application/DD8D26F4-6BC1-4EFB-A444-EA2FB7F411D9/TFLiteBenchmark.app/dummy_model_2.tflite]\r\nInput layers: [input_feature_placeholder]\r\nInput shapes: [1,1,2,80]\r\nInput value ranges: []\r\nAllow fp16 : [0]\r\nRequire full delegation : [0]\r\nEnable op profiling: [0]\r\nMax profiling buffer entries: [1024]\r\nCSV File to export profiling data to: []\r\nUse gpu : [1]\r\nAllow lower precision in gpu : [1]\r\nGPU delegate wait type : [aggressive]\r\nLoaded model /private/var/containers/Bundle/Application/DD8D26F4-6BC1-4EFB-A444-EA2FB7F411D9/TFLiteBenchmark.app/dummy_model_2.tflite\r\n2020-02-26 15:16:14.857526-0500 TFLiteBenchmark[444:628154] Initialized TensorFlow Lite runtime.\r\n2020-02-26 15:16:14.857843-0500 TFLiteBenchmark[444:628154] Created TensorFlow Lite delegate for Metal.\r\n2020-02-26 15:16:14.858337-0500 TFLiteBenchmark[444:628154] Metal GPU Frame Capture Enabled\r\n2020-02-26 15:16:14.859181-0500 TFLiteBenchmark[444:628154] Metal API Validation Enabled\r\n2020-02-26 15:16:14.932455-0500 TFLiteBenchmark[444:628154] TfLiteGpuDelegate Prepare: MUL: Dimension can not be reduced to linear.\r\n2020-02-26 15:16:14.932670-0500 TFLiteBenchmark[444:628154] Node number 2 (TfLiteMetalDelegate) failed to prepare.\r\n2020-02-26 15:16:14.932800-0500 TFLiteBenchmark[444:628154] tensorflow/lite/kernels/mul.cc:74 input1->type != input2->type (1 != 10)\r\n2020-02-26 15:16:14.932891-0500 TFLiteBenchmark[444:628154] Node number 1 (MUL) failed to prepare.\r\nFailed to apply GPU delegate.\r\n```", "comments": ["@Richard-Yang-Bose \r\nplease note i am unable to access the file shared by you, please share a simple stand alone code so i could replicate the issue faced by you in my local.", "Thanks for the reply, here's the code for generating the dummy model, after the tflite dummy model is generated, I try to run it with the [TFLite ios benchmark app](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark/ios) with the param: `\"use_gpu\" : \"1\",\r\n    \"gpu_wait_type\" : \"aggressive\",`\r\n```\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default():\r\n    x = tf.placeholder(tf.float32, shape=(1, 1, 2, 80), name='array_input_feature')\r\n    y = tf.Variable(initial_value=tf.ones([1, 1, 2, 80]))\r\n    x = tf.math.multiply(x, y * 2)\r\n    x = tf.identity(x, name='output')\r\n\r\n    with tf.Session() as sess:\r\n\r\n        tf.contrib.quantize.create_training_graph()\r\n        sess.run(tf.global_variables_initializer())\r\n        tf.contrib.quantize.create_eval_graph()\r\n        saver = tf.train.Saver()\r\n        saver.save(sess, 'dummy_model')\r\n\r\n\r\ndef prepare_graph():\r\n    with tf.Session() as sess:\r\n        new_saver = tf.train.import_meta_graph('dummy_model.meta')\r\n        new_saver.restore(sess, tf.train.latest_checkpoint('./'))\r\n        graph = tf.get_default_graph()\r\n        output_names = 'output:0'\r\n        output_tensor = graph.get_tensor_by_name(output_names)\r\n        output_node_names = [\"output\"]\r\n        frozen_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, output_node_names)\r\n        return frozen_graph_def, output_tensor\r\n\r\n\r\ndef tfliteConvert(graph, outputs, tflite_model_file, quantize=True):\r\n    tf.reset_default_graph()\r\n    # Turn the input into placeholder\r\n    tflite_input = tf.placeholder(\r\n    \"float32\", [1, 1, 2, 80], name=\"array_input_feature\")\r\n    tf.import_graph_def(graph, name=\"\", input_map={\"array_input_feature\": tflite_input})\r\n    # prepare the graph for conversion, convert op_hint wrapper\r\n    with tf.Session() as sess:\r\n        curr = sess.graph_def\r\n    converter = tf.lite.TFLiteConverter(curr, [tflite_input], [outputs])\r\n    converter.experimental_new_converter = True\r\n    converter.target_spec.supported_types = [tf.lite.constants.FLOAT16]\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n    tflite = converter.convert()    \r\n\r\n    # save TFLite model\r\n    open(tflite_model_file, \"wb\").write(tflite)\r\n\r\n\r\ndef main():\r\n    frozen_graph_def, outputs = prepare_graph()\r\n    tfliteConvert(frozen_graph_def, outputs, 'dummy_model.tflite')\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```", "@Richard-Yang-Bose in the log you pasted, there is `2020-02-26 15:16:14.932800-0500 TFLiteBenchmark[444:628154] tensorflow/lite/kernels/mul.cc:74 input1->type != input2->type (1 != 10)`, the type mismatch is between `kTfLiteFloat32 = 1` and `TfLiteFloat16 = 10`", "> @Richard-Yang-Bose in the log you pasted, there is `2020-02-26 15:16:14.932800-0500 TFLiteBenchmark[444:628154] tensorflow/lite/kernels/mul.cc:74 input1->type != input2->type (1 != 10)`, the type mismatch is between `kTfLiteFloat32 = 1` and `TfLiteFloat16 = 10`\r\n\r\n@freedomtan Thanks for pointing this out, I'm aware of this and have also tried the following setups:\r\n1. Set `allow_fp16` as `1` in the benchmark too: still got the same error log. \r\n2. Use float32 model (without setting float16 and optimization during the tflite model conversion): still failed to apply GPU delegate, but without the type mismatch error.\r\nAlso note that the benchmark app is able to run mobilenet provided [here](https://www.tensorflow.org/lite/performance/benchmarks) on gpu delegate.\r\n```\r\n2020-03-02 10:21:08.174624-0500 TFLiteBenchmark[10532:2225388] Initialized TensorFlow Lite runtime.\r\n2020-03-02 10:21:08.175107-0500 TFLiteBenchmark[10532:2225388] Created TensorFlow Lite delegate for Metal.\r\n2020-03-02 10:21:08.176624-0500 TFLiteBenchmark[10532:2225388] Metal GPU Frame Capture Enabled\r\n2020-03-02 10:21:08.177811-0500 TFLiteBenchmark[10532:2225388] Metal API Validation Enabled\r\n2020-03-02 10:21:08.294492-0500 TFLiteBenchmark[10532:2225388] TfLiteGpuDelegate Prepare: MUL: Dimension can not be reduced to linear.\r\n2020-03-02 10:21:08.294760-0500 TFLiteBenchmark[10532:2225388] Node number 1 (TfLiteMetalDelegate) failed to prepare.\r\n2020-03-02 10:21:08.294908-0500 TFLiteBenchmark[10532:2225388] Restored previous execution plan after delegate application failure.\r\nFailed to apply GPU delegate.\r\n```", "@Richard-Yang-Bose interesting. It seems you hit a limitation of GPU delegate. @impjdi might have the right answer.", "@freedomtan Thanks again for the insight and appreciate all the great work!", "That error message `Dimension can not be reduced to linear.` is printed at `//tensorflow/lite/delegates/gpu/common/model_builder.cc`.  Not sure why it's trying to use `SetAllDimensions<Linear>`.  Can you attach the TFLite model?", "> That error message `Dimension can not be reduced to linear.` is printed at `//tensorflow/lite/delegates/gpu/common/model_builder.cc`. Not sure why it's trying to use `SetAllDimensions<Linear>`. Can you attach the TFLite model?\r\n\r\nhere's the model (fp16):\r\n[dummy_model.tflite](https://drive.google.com/file/d/1383fCIA0-Uhlmbo8zR16KW3C6FYDX5un/view?usp=sharing)", "Thank you for attaching the model.  Found the issue.  I'll prepare a fix and will let you know.", "Are you okay with a fix that requires you to do\r\n\r\n>     y = tf.Variable(initial_value=tf.ones([1, 2, 80]))  # without the batch dimension\r\n\r\ninstead of\r\n\r\n>     y = tf.Variable(initial_value=tf.ones([1, 1, 2, 80]))\r\n\r\n?\r\n", "@impjdi  Thanks for finding the issue!\r\nYeah that could be a work around, however it's not ideal since I believe some network structure has MUL with 4D tensor required inside the implementation (e.g. [convlstm).](https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/layers/convolutional_recurrent.py#L703)\r\nDo you think it's possible to have MUL operation compatible with 4D tensor in the future fix?\r\nThanks again for helping out!", "While it's not technically impossible (extending it to the batch dimension is trivial), I think you will have a much bigger problem as it won't be just `MUL` that doesn't support your particular use case; the majority, if not all, of TFLite GPU ops are not designed in a batch-friendly fashion.  We will have a lot more plumbing to do to accommodate the batch size we will investigate more in the coming months.", "Yeah, that make sense, especially inference use cases usually have the batch-dimension fixed to 1 or a constant. \r\nI'm not familiar with how the GPU ops are implemented, but I guess as a first cut, it would be nice to have `MUL` to allow 4D input, while the first dimension only supports to be set as 1 ?", "Any progress here? How should I use MUL for multidim tensors, lets say [1,257,257,3] so I won't get Init: MUL: 1x257x257x3  cannot be reduced to linear?\r\nStill reproducible on ToT", "@stalex91 I think I don't understand your question.  Shouldn't [1, 257, 257, 3] x [1, 257, 257, 3] work fine as elementwise multiplication?", "@impjdi Have met the same problem listed in description but actual reason was before it. So yes, works for me now, thank you.", "@impjdi could you help me to check this model: [yolov5s.tflite](https://drive.google.com/file/d/14PKOXoYt75UqP3Dc4ZWzmORo2WUAXJnq/view?usp=sharing)?\r\nI am using TF 2.3.0 and TFLite 2.3.0 with GPU delegate.\r\n\r\n\r\nIt has 3 element-wise multiplications: [1, 3, 400, 2] x [1, 3, 400, 2], [1, 3, 1600, 2] x [1, 3, 1600, 2], and [1, 3, 6400, 2] x [1, 3, 6400, 2].\r\nWhen I run the model with GPU delegate, it complains: \r\n```\r\nE/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.detection, PID: 10947\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Init: MUL: 1x3x400x2  cannot be reduced to linear.\r\n    TfLiteGpuDelegate Prepare: delegate is not initialized\r\n    Node number 227 (TfLiteGpuDelegateV2) failed to prepare.\r\n    \r\n    Restored original execution plan after delegate application failure.\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:351)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:82)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:266)\r\n        at org.tensorflow.lite.examples.detection.tflite.YoloV5Classifier.recreateInterpreter(YoloV5Classifier.java:174)\r\n        at org.tensorflow.lite.examples.detection.tflite.YoloV5Classifier.useGpu(YoloV5Classifier.java:182)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.lambda$updateActiveModel$0$DetectorActivity(DetectorActivity.java:213)\r\n        at org.tensorflow.lite.examples.detection.-$$Lambda$DetectorActivity$lqWWUAbeU1gjsHY5oEBIKbdpe60.run(Unknown Source:8)\r\n        at android.os.Handler.handleCallback(Handler.java:793)\r\n        at android.os.Handler.dispatchMessage(Handler.java:98)\r\n        at android.os.Looper.loop(Looper.java:173)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n```", "I *think* it's coming from the fact that the 2nd tensor is of shape [1, h, w, c].  Can you make a modification to the model and make the 2nd tensor take the shape of [h, w, c] (without the 1st dimension of size 1)?", "@impjdi I updated [my model](https://drive.google.com/file/d/1RQvue0VWb2pD84evjL32dxkQbndSIn10/view?usp=sharing)  according to your suggestion.\r\nThe modified MUL op can be inspected in netron:\r\n![image](https://user-images.githubusercontent.com/3896992/93542551-e1a9e680-f98b-11ea-9ac8-5c43da4d4b98.png)\r\n\r\n\r\nTFLite still complains\r\n```\r\nE/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.detection, PID: 14431\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Init: MUL: 3x400x2  cannot be reduced to linear.\r\n    TfLiteGpuDelegate Prepare: delegate is not initialized\r\n    Node number 227 (TfLiteGpuDelegateV2) failed to prepare.\r\n    \r\n    Restored original execution plan after delegate application failure.\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:351)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:82)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:266)\r\n        at org.tensorflow.lite.examples.detection.tflite.YoloV5Classifier.recreateInterpreter(YoloV5Classifier.java:174)\r\n        at org.tensorflow.lite.examples.detection.tflite.YoloV5Classifier.useGpu(YoloV5Classifier.java:182)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.lambda$updateActiveModel$0$DetectorActivity(DetectorActivity.java:208)\r\n        at org.tensorflow.lite.examples.detection.-$$Lambda$DetectorActivity$lqWWUAbeU1gjsHY5oEBIKbdpe60.run(Unknown Source:8)\r\n        at android.os.Handler.handleCallback(Handler.java:793)\r\n        at android.os.Handler.dispatchMessage(Handler.java:98)\r\n        at android.os.Looper.loop(Looper.java:173)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n```", "Thanks for trying and attaching the updated model!  Will took a deeper look.", "@impjdi @impjdi could you help me to use the multiply op in tflite model with metal delegate\uff1f i am using tf1.15.4 and tflite 1.15.4 with metal metal delegate\r\n\r\nwhen there is a scalar multiplier it works well,otherwise, the error occurs. i also use resize_bilinear to ensure both inputs have the same shape, it dosen't work.\r\n\r\nWhen I run the model with GPU delegate, it complains:\r\n\r\n`2020-12-11 10:41:56.881639+0800 tflite_camera_example[34564:5652810] Initialized TensorFlow Lite runtime.\r\n2020-12-11 10:41:56.881729+0800 tflite_camera_example[34564:5652810] Created TensorFlow Lite delegate for Metal.\r\n2020-12-11 10:41:56.881924+0800 tflite_camera_example[34564:5652810] Metal GPU Frame Capture Enabled\r\n2020-12-11 10:41:56.882223+0800 tflite_camera_example[34564:5652810] Metal API Validation Enabled\r\n2020-12-11 10:41:56.949605+0800 tflite_camera_example[34564:5652810] TfLiteGpuDelegate Prepare: Unsupported op type: apply_mask; custom registry error: Unsupported op: apply_mask; primary registry error: Unsupported op: apply_mask;\r\n2020-12-11 10:41:56.949691+0800 tflite_camera_example[34564:5652810] Node number 12 (TfLiteMetalDelegate) failed to prepare.\r\n2020-12-11 10:41:56.949741+0800 tflite_camera_example[34564:5652810] Restored previous execution plan after delegate application failure.`\r\n\r\nThe modified MUL op can be inspected in netron:\r\n![image](https://user-images.githubusercontent.com/49243657/101856044-8577ec80-3b9f-11eb-87ea-de7146014a2d.png)\r\n", "@lei522 I'm not familiar with the versions, but we removed `apply_mask` quite some time back.  Is it possible for you to try the latest code?", "\r\n@impjdi  Hey:) I have tried the latest code, it works well. thanks for your suggestion! Have a good day!\r\n", "Excuse me, how can this problem be solved?\r\nTfLiteGpuDelegate Init: MUL: 1x1088x1x1  cannot be reduced to linear.\r\nTfLiteGpuDelegate Prepare: delegate is not initialized\r\nNode number 23608 (TfLiteGpuDelegateV2) failed to prepare.", "@uchihaltachi for whatever reason, `MUL` is expecting the input tensor to be a linear tensor.  maybe insert a `RESHAPE` to match the other dimension?  In GPU, `1x1088x1x1` is not the same as `1x1x1x1088`; each dimension has to match.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37102\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37102\">No</a>\n"]}, {"number": 37101, "title": "[Grappler] Rewrite handling of list ops in auto_mixed_precision", "body": "- Rewrites the handling of Tensor List ops in the auto_mixed_precision grappler pass, fixing several issues.\r\n- Now supports all types of Tensor List ops as well as special cases such as AddN nodes that operate on two Tensor Lists.\r\n- Now handles unsafe situations where Tensor List handles are passed through untraversable edges such as between sub-graphs.\r\n- Now properly detects writer -> reader node dependencies.\r\n\r\n@reedwm @nluehr ", "comments": ["Thanks for the fast and thorough review @reedwm! I think I've addressed all of your comments.", "Thanks for the PR!"]}, {"number": 37100, "title": "why is variable not partitioned by default in PS-based distributed training?", "body": "Hello,\r\nWe found out when multiple PSes are used, there's always only one PS serving by default, and we need to explicitly set variable partitioning strategy(fix sized partitioner) to make use of every PS. Do we know why this is not the default behavior when PS is used?", "comments": ["I wonder why you are not trying out all-reducer architecture? ", "@burgerkingeater Did you try all-reducer architecture as mentioned above?", "Closing this issue as it has been inactive for more than 15 days. Please add additional comments and we can open this issue again. Thanks!"]}, {"number": 37099, "title": "RuntimeError: Inputs and outputs not all float|uint8|int16 types.Node number 2 (ADD) failed to invoke.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): 1.15\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom PIL import Image\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=\"out.tflite\")\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\nprint(input_details)\r\n[{'name': 'image', 'index': 21904, 'shape': array([  3, 270, 480], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\nprint(output_details)\r\n[{'name': 'action', 'index': 7204, 'shape': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\ninterpreter.invoke()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nRuntimeError: Inputs and outputs not all float|uint8|int16 types.Node number 2 (ADD) failed to invoke.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://we.tl/t-lWH3XmYihS <-- .pb\r\nhttps://we.tl/t-Bkid4ThzN1 <-- .tflite\r\n```\r\n\r\n**Failure details**\r\nThe conversion is successful and I can run inference on the .pb, but I get the following error when I run inference on the tflite file.\r\n\r\n```\r\nRuntimeError: Inputs and outputs not all float|uint8|int16 types.Node number 2 (ADD) failed to invoke.\r\n```\r\n\r\n\r\n**Any other info / logs**\r\n\r\nModel trained on pytorch mobilenetV2 arch, converted to onnx, and converted to tensorflow.", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/df7024e8e3f3204c171a586a1c671dab/37099.ipynb). Thanks!", "@stev3 Looks like some op (ADD) at node 2 is of different dtype than what is accepted. Can you please check what is the dtype supplied at node 2? Thanks!", "This may be the issue:\r\n\r\n```\r\n{'name': 'add_10', 'index': 7205, 'shape': array([2], dtype=int32), 'dtype': <class 'numpy.int64'>, 'quantization': (0.0, 0)}\r\n```\r\n\r\nIs it possible to edit the graph and cast the type to numpy.int32?", "Bumping this since I am facing the same issue. The tf graph when visualized in netron seems to be fine (all float32). However my conclusion is that the TFLite converter is probably mistakenly converting the ADD node to float 64. ", "Can someone comment on why the conversion from frozen graph to TFLite model changes the node to float64 which is originally float32. ", "I solved this issue by using the experimental MLIR converter as shown here:\r\nhttps://groups.google.com/a/tensorflow.org/forum/#!topic/tflite/C7Ag0sUrLYg\r\nIt just needed one line of change. \r\n`converter.experimental_new_converter = True  # Add this line\r\n`\r\n The error is not there anymore. I also verified that the PyTorch output and TFLite interpreter outputs match within tolerance values. ", "I am using the MLIR convertor and still see this issue.", "This fix didn't work for me either, but I'm working with a frozen graph and not a saved model. I overhauled my training pipeline to avoid this conversion. You can close it if you'd like, but seems others are experiencing a similar problem.", "@stev3 Can you share the *.pb file and the conversion code you used or input_shape, output\r\n_shape arguments that need to be supplied for the conversion. The link you provided to *.pb doesn't work anymore. Thanks!", "How can I get all the tensor names in the tflite model?", "same issue\r\nmy tensorflow version is 2.2.0-dev20200325\r\n\r\ni convert .pb to .tflite as:\r\n```\r\ndef pb_to_tflite2(pb_path, tflite_path,input_arrays,output_arrays,quant):\r\n    converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\r\n        pb_path, input_arrays, output_arrays)\r\n    converter.experimental_new_converter = True #no effect\r\n    if quant:\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    tflite_model = converter.convert()\r\n    f = open(tflite_path, 'wb')\r\n    f.write(tflite_model)\r\n\r\ninput_arrays = ['pillar_x','pillar_y','pillar_z','pillar_i',\r\n                'num_points_per_pillar',\r\n                'x_sub_shaped',\r\n                'y_sub_shaped',\r\n                'mask'\r\n               ]\r\noutput_arrays = ['174']\r\npb_to_tflite2('./pfe_graph.pb','./pfe_quant.tflite',input_arrays,output_arrays,True)\r\n```\r\n\r\n**i can do inference on .pb file successfully.**\r\n\r\nwhen i do infrence on .tflite\r\n```\r\ndef do_tflite_inference(model_file_path):\r\n    # Load TFLite model and allocate tensors.\r\n    interpreter = tf.lite.Interpreter(model_path=model_file_path)\r\n    interpreter.allocate_tensors()\r\n    print('build interpreter success!')\r\n\r\n    # Get input and output tensors.\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n    print(input_details)\r\n    print(output_details)\r\n\r\n    #\u3000\u5bf9\u6bcf\u4e00\u4e2ainput\u8d4b\u503c\r\n    for input in input_details:\r\n        print(input['dtype'])\r\n        input_shape = input['shape'] #\u6bcf\u4e00\u4e2ainput\u90fd\u662f\u4e00\u4e2adict \u6709name,index,shape\u5c5e\u6027\r\n        input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n\r\n        input_index = input['index']\r\n        interpreter.set_tensor(input_index,input_data)\r\n\r\n    interpreter.invoke()\r\n```\r\ni outputs:\r\n```\r\nTraceback (most recent call last):\r\n  File \"tf_inference.py\", line 35, in <module>\r\n    do_tflite_inference('./pfe.tflite')\r\n  File \"tf_inference.py\", line 26, in do_tflite_inference\r\n    interpreter.invoke()\r\n  File \"/home/sc/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 511, in invoke\r\n    self._interpreter.Invoke()\r\nRuntimeError: Inputs and outputs not all float|uint8|int16 types.Node number 3 (ADD) failed to invoke.\r\n```\r\n\r\nwhen i do above things on tensorflow1.14,when do inference on .tflite,it crash in tflite::ops::builtin::sparse_to_dense::SparseToDenseImp\r\n[https://github.com/tensorflow/tensorflow/issues/35987#issuecomment-604207418](url)\r\n\r\ni think there may be bugs in tflite code,when convert .pb to .tflite,the generated .tflite file is not correct at all\uff0e", "This is the code that works for me on `tensorflow 2.1.0 ` CPU Version, with onnx-tf built from master branch. \r\n  ```\r\n graph_def = tf.compat.v1.GraphDef()\r\n graph_def.ParseFromString(open(tf_model_path, 'rb').read())\r\n concrete_func = wrap_frozen_graph(graph_def, inputs=input_tensor, outputs=[\"scores:0\", \"boxes:0\"])\r\n converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n converter.experimental_new_converter = True\r\n converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\r\n tflite_model = converter.convert()\r\n```", "Can you share the pytorch, onnx, and the .pb file so that I can try to convert it using my environment setup and see if it works ? ", "@codeislife99 Thanks for the tip above! Using the wrap_frozen_graph worked for me.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@codeislife99 Thanks for the tip above! Using the wrap_frozen_graph worked for me,too.I use the same tensorflow version as your's, and conver the .pb model to .tflite model and load it in andriod, the interpreter can do the inference now!\r\n      At first I find that the wrap_frozen_graph func can not be recognized ,then I find it on 'https://leimao.github.io/blog/Save-Load-Inference-From-TF2-Frozen-Graph/' ,and I worked out. The full edition of the convert code is uploaded.\r\n[pb_to_tflite.txt](https://github.com/tensorflow/tensorflow/files/4479471/pb_to_tflite.txt)\r\n\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I have also encountered this issue, with models which are successfully exported with:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n# make a converter object from the saved tensorflow file\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\r\n    model_file,\r\n    input_arrays=[input_name],  # input arrays\r\n    output_arrays=[\r\n        output_name\r\n    ],\r\n)\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,\r\n    tf.lite.OpsSet.SELECT_TF_OPS,\r\n]\r\n# tell converter which type of optimization techniques to use\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n# to view the best option for optimization read documentation of tflite about optimization go to this link https://www.tensorflow.org/lite/guide/get_started#4_optimize_your_model_optional\r\n\r\n# convert the model\r\ntf_lite_model = converter.convert()\r\n```\r\n\r\nHowever running in TFLite gives error.\r\n\r\n`RuntimeError: Inputs and outputs not all float|uint8|int16 types.Node number 3 (ADD) failed to invoke.`\r\n\r\nI've tried to follow the export process using the experimental MLIR converter, which seems to side-step the bug.\r\n\r\nHowever, the export process fails with `ValueError: Feeds must be tensors.`\r\n\r\nThe traceback is:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-23-d3dc2f63cbd6> in <module>\r\n     25     graph_def = tf.compat.v1.GraphDef()\r\n     26     graph_def.ParseFromString(open(model_file, 'rb').read())\r\n---> 27     concrete_func = wrap_frozen_graph(graph_def, inputs=input_name, outputs=[output_name])\r\n     28     converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n     29     converter.experimental_new_converter = True\r\n\r\n<ipython-input-21-1fc724505eae> in wrap_frozen_graph(graph_def, inputs, outputs, print_graph)\r\n     16     return wrapped_import.prune(\r\n     17         tf.nest.map_structure(import_graph.as_graph_element, inputs),\r\n---> 18         tf.nest.map_structure(import_graph.as_graph_element, outputs))\r\n\r\n~/.virtualenvs/tf-lite/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in prune(self, feeds, fetches, name, input_signature)\r\n--> 262         raise ValueError(\"Feeds must be tensors.\")\r\n    263 \r\n    264     # Ignoring all feeds that are captures allows prune to be called\r\n```\r\n\r\n\r\nThe values I'm passing for `input_name` and `output_name` are just string names of the tensors.  I've confirmed in netron that these are the correct names.\r\n\r\nI've added a print statement before the value error in `wrap_function.py` to learn more about the error:\r\n\r\n```\r\nprint('wrap_function.py', f)\r\nprint(type(f))\r\n```\r\n\r\nThis gives the output:\r\n\r\n```\r\n--------------------------------------------------\r\nFrozen model layers: \r\n--------------------------------------------------\r\nwrap_function.py name: \"input_tens\"\r\nop: \"Placeholder\"\r\nattr {\r\n  key: \"dtype\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"shape\"\r\n  value {\r\n    shape {\r\n      dim {\r\n        size: 1\r\n      }\r\n      dim {\r\n        size: 3\r\n      }\r\n      dim {\r\n        size: 224\r\n      }\r\n      dim {\r\n        size: 224\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n<class 'tensorflow.python.framework.ops.Operation'>\r\n```\r\n\r\nThe error is raised because we are checking if `isinstance(f, ops.Tensor)` fails.\r\n\r\nI'm following through to see why this is being cast as an \"Operation\", rather than a \"Tensor\".  Is there something glaringly wrong with my function calling?", "Im at this exact same errror. Stil lgetting the error which says \r\nRuntimeError: Inputs and outputs not all float|uint8|int16 types.Node number 4 (ADD) failed to invoke.\r\n", "@jaggernaut007 Try [this notebook](https://gist.github.com/Wheest/d8400c73a7795298d1cdd45f247544cc), or the file provided by @JimKimHome .  The new MLIR based model convertor could avoid this bug.  \r\n\r\nI'm still trying to get the `wrap_frozen_graph()` working for my case.", "I have a feeling the error is due to , providing the wrong output tensor.\r\nChecking out the notebook now\r\n", "*This is my input and output tensor info*\r\n\r\n```\r\n[{\r\n\t'name': 'input.1',\r\n\t'index': 0,\r\n\t'shape': array([1, 3, 256, 256], dtype = int32),\r\n\t'shape_signature': array([1, 3, 256, 256], dtype = int32),\r\n\t'dtype': < class 'numpy.float32' > ,\r\n\t'quantization': (0.0, 0),\r\n\t'quantization_parameters': {\r\n\t\t'scales': array([], dtype = float32),\r\n\t\t'zero_points': array([], dtype = int32),\r\n\t\t'quantized_dimension': 0\r\n\t},\r\n\t'sparsity_parameters': {}\r\n}]\r\n[{\r\n\t'name': '4358',\r\n\t'index': 4138,\r\n\t'shape': array([1, 17, 3], dtype = int32),\r\n\t'shape_signature': array([1, 17, 3], dtype = int32),\r\n\t'dtype': < class 'numpy.float32' > ,\r\n\t'quantization': (0.0, 0),\r\n\t'quantization_parameters': {\r\n\t\t'scales': array([], dtype = float32),\r\n\t\t'zero_points': array([], dtype = int32),\r\n\t\t'quantized_dimension': 0\r\n\t},\r\n\t'sparsity_parameters': {}\r\n}]\r\n```\r\n*This is stack trace of the error *\r\n\r\n> > Traceback (most recent call last):\r\n>   File \"test_graph.py\", line 20, in <module>\r\n>     interpreter.invoke()\r\n>   File \"/home/shreyas/.local/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py\", line 511, in invoke\r\n>     self._interpreter.Invoke()\r\n>   File \"/home/shreyas/.local/lib/python3.8/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 113, in Invoke\r\n>     return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)\r\n> RuntimeError: Inputs and outputs not all float|uint8|int16 types.Node number 4 (ADD) failed to invoke.\r\n\r\n> ", "Could you share `test_graph.py` @jaggernaut007?   Are you trying the experimental MLIR convertor?\r\n\r\nThe issue seems to be that there is a bug in the normal conversion process:\r\n\r\n> However my conclusion is that the TFLite converter is probably mistakenly converting the ADD node to float 64.\r\n\r\n> I solved this issue by using the experimental MLIR converter as shown here:", "> \r\n> \r\n> @codeislife99 Thanks for the tip above! Using the wrap_frozen_graph worked for me,too.I use the same tensorflow version as your's, and conver the .pb model to .tflite model and load it in andriod, the interpreter can do the inference now!\r\n> At first I find that the wrap_frozen_graph func can not be recognized ,then I find it on 'https://leimao.github.io/blog/Save-Load-Inference-From-TF2-Frozen-Graph/' ,and I worked out. The full edition of the convert code is uploaded.\r\n> [pb_to_tflite.txt](https://github.com/tensorflow/tensorflow/files/4479471/pb_to_tflite.txt)\r\n\r\nWhat are the input and output tensors you have provided?", "> \r\n> \r\n> Could you share `test_graph.py` @jaggernaut007? Are you trying the experimental MLIR convertor?\r\n> \r\n> The issue seems to be that there is a bug in the normal conversion process:\r\n> \r\n> > However my conclusion is that the TFLite converter is probably mistakenly converting the ADD node to float 64.\r\n> \r\n> > I solved this issue by using the experimental MLIR converter as shown here:\r\n\r\nHave mentioned this in #39790 \r\nUsing the new MLIRconvertor has no difference. Still the same result.\r\nThis error is because of the numpy.float32 in node 4 Trying to figure out how to correct that.\r\nHere is the test code for checking it out.\r\n\r\n`import numpy as np                                                                                                                                                                                   import tensorflow as tf                                                                                                                                                                                                                                                                                                                                                                                   # Load TFLite model and allocate tensors.                                                                                                                                                            interpreter = tf.lite.Interpreter(model_path=\"savedmodel.tflite\")                                                                                                                                    interpreter.allocate_tensors()                                                                                                                                                                                                                                                                                                                                                                            # Get input and output tensors.                                                                                                                                                                      input_details = interpreter.get_input_details()                                                                                                                                                      output_details = interpreter.get_output_details()                                                                                                                                                                                                                                                                                                                                                         print(input_details)                                                                                                                                                                                 print(output_details)                                                                                                                                                                                                                                                                                                                                                                                     # Test model on random input data.                                                                                                                                                                   input_shape = input_details[0]['shape']                                                                                                                                                              input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)                                                                                                                        interpreter.set_tensor(input_details[0]['index'], input_data)                                                                                                                                                                                                                                                                                                                                             interpreter.invoke()                                                                                                                                                                                                                                                                                                                                                                                      # The function `get_tensor()` returns a copy of the tensor data.                                                                                                                                     # Use `tensor()` in order to get a pointer to the tensor.                                                                                                                                            output_data = interpreter.get_tensor(output_details[0]['index'])                                                                                                                                     print(output_data)   `                                                                                                                                                                                                    ", "https://colab.research.google.com/gist/jaggernaut007/efe78df81a697c2f39652cb0cbe88169/untitled0.ipynb\r\n\r\nHeres the code implementation", "Yeah, can confirm that the problem (`RuntimeError: Inputs and outputs not all float|uint8|int16 types.Node number 3 (ADD) failed to invoke.`) still occurs even when using the MLIR exporter.\r\n\r\nUsing the `wrap_frozen_graph()` approach didn't work for me, I exported using:\r\n```\r\n    # make a converter object from the saved tensorflow file\r\n    converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\r\n        model_file,\r\n        input_arrays=[input_tensor],  # input arrays\r\n        output_arrays=[output_tensor],  # output arrays\r\n    )\r\n    converter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS,\r\n        tf.lite.OpsSet.SELECT_TF_OPS,\r\n    ]\r\n    # tell converter which type of optimization techniques to use\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n    converter.experimental_new_converter = True\r\n    \r\n    # convert the model\r\n    tf_lite_model = converter.convert()\r\n```\r\n", "> \r\n> \r\n> Yeah, can confirm that the problem (`RuntimeError: Inputs and outputs not all float|uint8|int16 types.Node number 3 (ADD) failed to invoke.`) still occurs even when using the MLIR exporter.\r\n> \r\n> Using the `wrap_frozen_graph()` approach didn't work for me, I exported using:\r\n> \r\n> ```\r\n>     # make a converter object from the saved tensorflow file\r\n>     converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\r\n>         model_file,\r\n>         input_arrays=[input_tensor],  # input arrays\r\n>         output_arrays=[output_tensor],  # output arrays\r\n>     )\r\n>     converter.target_spec.supported_ops = [\r\n>         tf.lite.OpsSet.TFLITE_BUILTINS,\r\n>         tf.lite.OpsSet.SELECT_TF_OPS,\r\n>     ]\r\n>     # tell converter which type of optimization techniques to use\r\n>     converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n> \r\n>     converter.experimental_new_converter = True\r\n>     \r\n>     # convert the model\r\n>     tf_lite_model = converter.convert()\r\n> ```\r\n\r\nI can export the tflite model too. But I am unable to run the model on tflite.\r\ntf-nightly changes the error.\r\nAre you able to run the tflite model?\r\n", "Am currently debugging TFLite.  Have confirmed that in the TFLite model file, the Add operation has inputs and output of type float32 (consistent with the rest of the model).  I've used netron, however the TFLite interpreter also agrees when loading the model:\r\n\r\n```\r\ninterpreter = tf.lite.Interpreter(model_path=model_file)   \r\nfor t in interpreter.get_tensor_details():\r\n    n = t['name']\r\n    if 'Add' in n:\r\n        print(n)\r\n        assert(t['dtype'] == np.float32)\r\n```\r\n\r\nHowever, during runtime, calling the `Eval()` function in `tensorflow/lite/kernels/add.cc`, the type of the output tensor is actually 4 (`kTfLiteInt64`).  It is because of this that `Eval()` throws the error.\r\n\r\nWhen the `Prepare()` function is called, every `Add` operation sets the input and output data types correctly.  Except one.  This is an Add node that I can't explain, which sets all of the tensors to `kTfLiteInt64`.  None of these tensors are showing up looking through the tensor details above, nor can I find the node in netron.  \r\n\r\nThus, I am unsure where it is created, and how I can either remove it or get it to use the correct type.\r\n", "Following is the colab link attached, it would be great if anyone could help resolve it.\r\nI had converted my code from pytorch model to onnx file, then from onnx to .pb file and finally from pb file to tflite. I am facing the exact same issue.\r\nThis is my collab link\r\n\r\nhttps://colab.research.google.com/drive/1hEFdsl_OIHyfC0fZBKcIt8V0t8X1m07h?usp=sharing", "I think I solved the issue from my side.  I found that using the `out.view()` method in my PyTorch model, before the fully connected layer, it caused this issue in the TFLite model.  Inspecting the models, I haven't been able to figure out why this happens.\r\n\r\nHowever, if I replace the `view` method with the `torch.flatten()` function, which is functionally equivalent, the TF-Lite model does not have this issue.\r\n\r\nIt's a totally weird problem.  I can tidy-up and share my mini-version if anyone is interested.  But hopefully this will help.  Whether or not it's in the ONNX to TF pipeline, or in the TF to TF-Lite pipeline I am unsure.  Perhaps someone more motivated can figure it out.", "Thanks a lot @Wheest \r\nI have used .view() a lot of times to reshape my arrays in the pytorch code rather than flatten, need to search alternatives for this.", "> I think I solved the issue from my side. I found that using the `out.view()` method in my PyTorch model, before the fully connected layer, it caused this issue in the TFLite model. Inspecting the models, I haven't been able to figure out why this happens.\r\n> \r\n> However, if I replace the `view` method with the `torch.flatten()` function, which is functionally equivalent, the TF-Lite model does not have this issue.\r\n> \r\n> It's a totally weird problem. I can tidy-up and share my mini-version if anyone is interested. But hopefully this will help. Whether or not it's in the ONNX to TF pipeline, or in the TF to TF-Lite pipeline I am unsure. Perhaps someone more motivated can figure it out.\r\n\r\n@Wheest Do you mind sharing your code snippet on how to replace .view() with flatten()? I am dealing with the following .view() usage:\r\n\r\n`bbox_regressions = torch.cat([o.view(o.size(0), -1, 4) for o in loc], 1)`\r\n\r\nThanks a lot!", "Thank you @codeislife99 !", "@ShiyongL my code was thus:\r\n\r\n`out = out.view(out.size(0), -1) --> x = torch.flatten(x, 1)`\r\n\r\nYour invocation seems a bit more complex than mine, I'm not sure what the 4 is used for in yours.  Unsure if you could get an equivalent operation via flatten.", "> Following is the colab link attached, it would be great if anyone could help resolve it.\r\n> I had converted my code from pytorch model to onnx file, then from onnx to .pb file and finally from pb file to tflite. I am facing the exact same issue.\r\n> This is my collab link\r\n> \r\n> https://colab.research.google.com/drive/1hEFdsl_OIHyfC0fZBKcIt8V0t8X1m07h?usp=sharing\r\n\r\n@parthisultimate  \r\nhi~ i have same issue \r\nCould you solve this problem?\r\nappreciated any advice!!!"]}, {"number": 37098, "title": "model.fit fails when using generator of keras sequence", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution: Linux Ubuntu 19\r\n- TensorFlow installed from \"pip3 install tensorflow\" (2.1.0)\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: - GTX 980 || CUDA 10.1 || cuDNN: 7.65 \r\n\r\n\r\n**Describe the current behavior**\r\nWhen using a sequence generator on model.fit I get this error on every epoch:\r\n`Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled`\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nI created a customer generator from the keras sequence class like so (taken from https://blog.ml6.eu/training-and-serving-ml-models-with-tf-keras-3d29b41e066c):\r\n\r\n`\r\n```\r\nimport ast\r\nimport numpy as np\r\nimport math\r\nimport os\r\nimport random\r\n\r\nimport pandas as pd\r\nfrom tensorflow.keras.preprocessing.image import img_to_array as img_to_array\r\nfrom tensorflow.keras.preprocessing.image import load_img as load_img\r\nimport tensorflow as tf\r\n\r\n\r\ndef load_image(image_path, size):\r\n    # data augmentation logic such as random rotations can be added here\r\n    return img_to_array(load_img(image_path, target_size=size)) / 255.\r\n\r\n\r\nclass KagglePlanetSequence(tf.keras.utils.Sequence):\r\n\r\n```\r\n\r\n    def __init__(self, df_path, data_path, im_width, im_height, batch_size, mode='train'):\r\n\r\n        self.df = pd.read_csv(df_path)\r\n        self.im_size = (im_width, im_height)\r\n        self.batch_size = batch_size\r\n        self.mode = mode\r\n\r\n        # Take labels and a list of image locations in memory\r\n        self.labels = self.df['LABELS'].apply(lambda x: ast.literal_eval(x)).tolist()\r\n        self.image_list = self.df['IMAGE_ID'].apply(lambda x: os.path.join(data_path, x + '.jpg')).tolist()\r\n\r\n    def __len__(self):\r\n        return int(np.floor(len(self.df) / self.batch_size))\r\n\r\n    def on_epoch_end(self):\r\n        # Shuffles indexes after each epoch\r\n        self.indexes = range(len(self.image_list))\r\n        if self.mode == 'train':\r\n            self.indexes = random.sample(self.indexes, k=len(self.indexes))\r\n\r\n    def get_batch_labels(self, idx):\r\n        # Fetch a batch of labels\r\n        return np.asarray(self.labels[idx * self.batch_size: (idx + 1) * self.batch_size], dtype=np.float32)\r\n\r\n    def get_batch_features(self, idx):\r\n        # Fetch a batch of images\r\n        batch_images = self.image_list[idx * self.batch_size: (1 + idx) * self.batch_size]\r\n        return np.asarray([load_image(im, self.im_size) for im in batch_images], dtype=np.float32)\r\n\r\n    def __getitem__(self, idx):\r\n        batch_x = self.get_batch_features(idx)\r\n        batch_y = self.get_batch_labels(idx)\r\n        return batch_x, batch_y\r\n`\r\nThe csv file I load is with a column for the image_ids and one column for the labels in the format: \"[0, 0, 0, 1]\"\r\n\r\nI initialize the generator\r\n\r\n```\r\nBATCH_SIZE = 32\r\nIMG_WIDTH = 320\r\nIMG_HEIGHT = 180\r\n\r\nseq = KagglePlanetSequence(CSV_PATH,\r\n                           IMAGES_PATH,\r\n                           im_width=IMG_WIDTH,\r\n                           im_height=IMG_HEIGHT,\r\n                           batch_size=32)\r\n\r\n```\r\n\r\nI build my model:\r\n```\r\n\r\nmodel = models.Sequential()\r\nmodel.add(Conv2D(4, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\r\nmodel.add(MaxPooling2D((2, 2)))\r\nmodel.add(Conv2D(8, (3, 3), activation='relu'))\r\nmodel.add(MaxPooling2D((2, 2)))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(3, activation='sigmoid'))\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer='adam', loss=losses.categorical_crossentropy, metrics=['accuracy'])\r\n```\r\n\r\nI call mode.fit\r\n\r\n```\r\nmodel.fit(seq,\r\n          verbose=1,\r\n          epochs=1,\r\n          workers=4)\r\n\r\n```\r\n\r\n```\r\nTrain for 5 steps\r\n2020-02-26 18:47:43.446662: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-26 18:47:43.693552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n5/5 [==============================] - 2s 323ms/step - loss: 1.2517 - accuracy: 0.4250\r\n**2020-02-26 18:47:44.917946: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled**\r\n\r\n```\r\nAnd I get this error:\r\n\r\n`Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled`\r\n\r\n\r\n\r\n\r\n", "comments": ["@HadiSDev \r\nplease share the csv file and all dependencies for us to replicate your issue in our local.", "\r\nSorry zip file is quite big, but here you go:\r\nhttp://www.mediafire.com/file/yp6exg4auwjbywu/drive-download-20200229T130542Z-001.zip/file\r\n\r\ncsv file is in the actions directory and images in the images directory", "@HadiSDev \r\nI ran the code shared by you and face this [error](https://colab.sandbox.google.com/gist/Saduf2019/36b481994d60b4a68c03fa79c5bd31e9/37098.ipynb), please share all dependencies and indented code for us to replicate and help you resolve the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37098\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37098\">No</a>\n"]}, {"number": 37097, "title": "Upgrade nanopb to latest release.", "body": "Fixes security vulnerability reported in #37011\r\n\r\nPiperOrigin-RevId: 297248686\r\nChange-Id: Ib24b109c8a36b8673842b2f2ebb771406e5fa022", "comments": ["@mihaimaruseac - \r\ngrpc used in TF doesn't work with nanopb 0.4.1. I also tried updating grpc to 1.27.0 but it fails at bazel fetch step. \r\nI'd tried updating nanopb to 0.3.9.5 tag which also has the security fix and works well with grpc used in TF 2.1 and TF 1.15.2. ", "@npanpaliya is this for master, for 2.1 branch or in general?", "Guess I'll have to actually build and test each of the versions. Do you have a test case?", "No, I don't know any tests yet. I'm trying to find out.\r\n\r\nBTW, if we update grpc in TF to 1.24 or above, we don't even need to update nanopb as nanopb has been removed from 1.24 version of grpc. https://github.com/grpc/grpc/commit/7ec6e8a4de6eba4eb3a8298ceb5aa82c0209b3a1. \r\nIf there is no specific reason for using grpc 1.19.x in TF and we can safely update that to higher version, the issue itself gets resolved.", "> @npanpaliya is this for master, for 2.1 branch or in general?\r\n\r\nI checked 1.15.2 and 2.1. Both fails with nanopb 0.4.1.", "Thank you for the updates. I'll hold off on merging these for now. Is master ok? I haven't seen an issue there (where nanopb update was merged)", "TF's master branch is working fine because it has grpc updated too. grpc used in master branch is 1.27.0-pre1 which doesn't use nanopb at all. \r\nIn TF, is there any other external package other than grpc which uses nanopb? If not, then I think we can remove nanopb from workspace.bzl file. And that means the security issue reported originally gets invalidated.", "Thanks. I'll take a look on nanopb usages and update accordingly.", "@mihaimaruseac - Did you get a chance to look at this? \r\n", "This is not on master and won't be merged except when we need to do a patch release for 2.1\r\n\r\nAs such, looking at thisis down-prioritized to up until we need to release the patch. We won't merge this until then", "It fails to build, indeed. I will try removing it from master too."]}, {"number": 37096, "title": "Upgrade nanopb to latest release.", "body": "Fixes security vulnerability reported in #37011\r\n\r\nPiperOrigin-RevId: 297248686\r\nChange-Id: Ib24b109c8a36b8673842b2f2ebb771406e5fa022", "comments": ["Failing due to dependency so it cannot be merged\r\n\r\n```\r\nERROR: /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/external/grpc/BUILD:1955:1: C++ compilation of rule '@grpc//:alts_util' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 40 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nIn file included from external/grpc/src/core/tsi/alts/handshaker/transport_security_common_api.h:32:0,\r\n                 from external/grpc/src/core/tsi/alts/handshaker/transport_security_common_api.cc:21:\r\nexternal/grpc/src/core/tsi/alts/handshaker/transport_security_common.pb.h:9:2: error: #error Regenerate this file with the current version of nanopb generator.\r\n #error Regenerate this file with the current version of nanopb generator.\r\n  ^\r\nexternal/grpc/src/core/tsi/alts/handshaker/transport_security_common_api.cc: In function 'size_t grpc_gcp_rpc_protocol_versions_encode_length(const grpc_gcp_rpc_protocol_versions*)':\r\nexternal/grpc/src/core/tsi/alts/handshaker/transport_security_common_api.cc:67:77: error: cannot convert 'const pb_field_t* {aka const pb_field_iter_s*}' to 'const pb_msgdesc_t* {aka const pb_msgdesc_s*}' for argument '2' to 'bool pb_encode(pb_ostream_t*, const pb_msgdesc_t*, const void*)'\r\n   if (!pb_encode(&size_stream, grpc_gcp_RpcProtocolVersions_fields, versions)) {\r\n                                                                             ^\r\nexternal/grpc/src/core/tsi/alts/handshaker/transport_security_common_api.cc: In function 'bool grpc_gcp_rpc_protocol_versions_encode_to_raw_bytes(const grpc_gcp_rpc_protocol_versions*, uint8_t*, size_t)':\r\nexternal/grpc/src/core/tsi/alts/handshaker/transport_security_common_api.cc:85:26: error: cannot convert 'const pb_field_t* {aka const pb_field_iter_s*}' to 'const pb_msgdesc_t* {aka const pb_msgdesc_s*}' for argument '2' to 'bool pb_encode(pb_ostream_t*, const pb_msgdesc_t*, const void*)'\r\n                  versions)) {\r\n                          ^\r\nexternal/grpc/src/core/tsi/alts/handshaker/transport_security_common_api.cc: In function 'bool grpc_gcp_rpc_protocol_versions_decode(grpc_slice, grpc_gcp_rpc_protocol_versions*)':\r\nexternal/grpc/src/core/tsi/alts/handshaker/transport_security_common_api.cc:118:72: error: cannot convert 'const pb_field_t* {aka const pb_field_iter_s*}' to 'const pb_msgdesc_t* {aka const pb_msgdesc_s*}' for argument '2' to 'bool pb_decode(pb_istream_t*, const pb_msgdesc_t*, void*)'\r\n   if (!pb_decode(&stream, grpc_gcp_RpcProtocolVersions_fields, versions)) {\r\n                                                                        ^\r\n```"]}, {"number": 37095, "title": "Upgrade nanopb to latest release.", "body": "Fixes security vulnerability reported in #37011\r\n\r\nPiperOrigin-RevId: 297248686\r\nChange-Id: Ib24b109c8a36b8673842b2f2ebb771406e5fa022", "comments": ["```\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nIn file included from external/grpc/src/core/tsi/alts/handshaker/transport_security_common_api.h:32:0,\r\n                 from external/grpc/src/core/lib/security/credentials/alts/grpc_alts_credentials_options.h:26,\r\n                 from external/grpc/src/core/lib/security/credentials/alts/grpc_alts_credentials_server_options.cc:27:\r\nexternal/grpc/src/core/tsi/alts/handshaker/transport_security_common.pb.h:9:2: error: #error Regenerate this file with the current version of nanopb generator.\r\n #error Regenerate this file with the current version of nanopb generator.\r\n```\r\n\r\n```\r\nIn file included from external/grpc/src/core/ext/filters/client_channel/lb_policy/grpclb/proto/grpc/lb/v1/google/protobuf/timestamp.pb.c:4:0:\r\nexternal/grpc/src/core/ext/filters/client_channel/lb_policy/grpclb/proto/grpc/lb/v1/google/protobuf/timestamp.pb.h:9:2: error: #error Regenerate this file with the current version of nanopb generator.\r\n #error Regenerate this file with the current version of nanopb generator.\r\n  ^~~~~\r\nexternal/grpc/src/core/ext/filters/client_channel/lb_policy/grpclb/proto/grpc/lb/v1/google/protobuf/timestamp.pb.c:7:2: error: #error Regenerate this file with the current version of nanopb generator.\r\n #error Regenerate this file with the current version of nanopb generator.\r\n  ^~~~~\r\nexternal/grpc/src/core/ext/filters/client_channel/lb_policy/grpclb/proto/grpc/lb/v1/google/protobuf/timestamp.pb.c:13:19: error: 'INT64' undeclared here (not in a function); did you mean 'INT64_C'?\r\n     PB_FIELD(  1, INT64   , OPTIONAL, STATIC  , FIRST, google_protobuf_Timestamp, seconds, seconds, 0),\r\n                   ^~~~~\r\n                   INT64_C\r\nexternal/grpc/src/core/ext/filters/client_channel/lb_policy/grpclb/proto/grpc/lb/v1/google/protobuf/timestamp.pb.c:13:29: error: 'OPTIONAL' undeclared here (not in a function)\r\n     PB_FIELD(  1, INT64   , OPTIONAL, STATIC  , FIRST, google_protobuf_Timestamp, seconds, seconds, 0),\r\n                             ^~~~~~~~\r\nexternal/grpc/src/core/ext/filters/client_channel/lb_policy/grpclb/proto/grpc/lb/v1/google/protobuf/timestamp.pb.c:13:39: error: 'STATIC' undeclared here (not in a function)\r\n     PB_FIELD(  1, INT64   , OPTIONAL, STATIC  , FIRST, google_protobuf_Timestamp, seconds, seconds, 0),\r\n                                       ^~~~~~\r\nexternal/grpc/src/core/ext/filters/client_channel/lb_policy/grpclb/proto/grpc/lb/v1/google/protobuf/timestamp.pb.c:13:49: error: 'FIRST' undeclared here (not in a function)\r\n     PB_FIELD(  1, INT64   , OPTIONAL, STATIC  , FIRST, google_protobuf_Timestamp, seconds, seconds, 0),\r\n                                                 ^~~~~\r\nexternal/grpc/src/core/ext/filters/client_channel/lb_policy/grpclb/proto/grpc/lb/v1/google/protobuf/timestamp.pb.c:13:56: error: expected expression before 'google_protobuf_Timestamp'\r\n     PB_FIELD(  1, INT64   , OPTIONAL, STATIC  , FIRST, google_protobuf_Timestamp, seconds, seconds, 0),\r\n                                                        ^~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/grpc/src/core/ext/filters/client_channel/lb_policy/grpclb/proto/grpc/lb/v1/google/protobuf/timestamp.pb.c:14:19: error: 'INT32' undeclared here (not in a function); did you mean 'INT64'?\r\n     PB_FIELD(  2, INT32   , OPTIONAL, STATIC  , OTHER, google_protobuf_Timestamp, nanos, seconds, 0),\r\n                   ^~~~~\r\n                   INT64\r\n...\r\n```\r\n\r\nThis comes from one of our dependencies. As such, we cannot update `nanopb` on the current branch."]}]