[{"number": 52979, "title": "TFLite static library generated is not working", "body": "\r\nThe generated static library through CMake sources as per tflite documentation is not working.\r\n\r\n**System information**\r\n- Debian 10 (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow Lite installed from CMake (source or binary): Source\r\n- TensorFlow version 2.4.1\r\n\r\n**Describe the current behavior**\r\nCurrently, the static library when generated through the bash script present in tensorflow/lite/tools/make/build_lib.sh is working.\r\nBut when generated as per documentation through CMake is not working.\r\n\r\n**Describe the expected behavior**\r\nNeed the working static library generated through CMake.\r\nProvide us the working CMakeLists file.\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n1)Use the steps of cmake build to generate static library on host.\r\n2)Run build_lib.sh and generate static library.\r\n\r\nThere are differences between two libraries and only second one is working.\r\nNeed a proper CMakeLists file to generate static library,\r\n.\r\n", "comments": ["@bhpatray \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "I'm adding @terryheo, who is familiar with CMake build for TFLite. \r\n\r\nHowever, as stated by @sushreebarsa, we do need more information (what is not working? how do you reproduce the issue?) Otherwise it's unlikely we can find out the issue and help you. ", "@bhpatray if you're using C++ API, you should have a CMake build for your project.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake#create_a_cmake_project_which_uses_tensorflow_lite\r\nIt's because the generated TFLite static library doesn't contain all the required libraries.\r\n\r\nFYI, you can also use C API with shared library. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/c\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52979\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52979\">No</a>\n"]}, {"number": 52978, "title": "Problems with saving and loading stock TF models (Resnet50, MobileNetV3, etc.) starting from TF 2.5 ", "body": "System: Ubuntu 20, Python 3.9.5, TF 2.5 (or 2.6)\r\n\r\nThis code: \r\n\r\nbase_model = MobileNetV3Large(); \r\nbase_model.save('test')\r\n\r\nGenerates a warning: \"generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\" \r\n\r\nI ignored this warning, trained a model and observed that when saving and loading the model back, it is losing accuracy at which it was saved.\r\n\r\nExpected behavior: The stock models should save and load without warnings or problems.", "comments": ["@compfy-dot-com ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.", "The complete code to reproduce the warning:\r\n\r\nfrom tensorflow.keras.applications import MobileNetV3Large\r\nbase_model = MobileNetV3Large()\r\nbase_model.save('test')\r\n\r\nIMHO we should focus on that warning and the reason for it: There should be none for the stock models. TF 2.4 does not seem to have this issue, it's only starting with TF 2.5.\r\n", "@compfy-dot-com ,\r\nCan you please look at this [comment](https://github.com/tensorflow/tensorflow/issues/50647#issuecomment-877178205) from the issue with the similar warning.It helps.Thanks!", "The commit in question has the following comment: \"Print a warning when trying to serialize an invalid custom mask.\" One way to interpret it is that in TF 2.5 were introduced changes that made it not backwards compatible, hence the warning. In other words, in TF 2.4 the stock models were valid and then in TF 2.5 they became invalid. If this is true, then either the stock models or the TF code need fixing; if it is not true, then the warning needs to be removed.", "@compfy-dot-com ,\r\nThe warning was added with TF 2.5 in commit [076b5be](https://github.com/tensorflow/tensorflow/commit/076b5be77d3933deb726de1fc9d954d284985b1e#diff-08518b3b0b22c5c9dc137c911fa716fe820878cb9bbf95806ffca0e0296ec7fa)", "Please post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @compfy-dot-com , The warning was added with TF 2.5 in commit [076b5be](https://github.com/tensorflow/tensorflow/commit/076b5be77d3933deb726de1fc9d954d284985b1e#diff-08518b3b0b22c5c9dc137c911fa716fe820878cb9bbf95806ffca0e0296ec7fa)\r\n\r\nYes, hence see https://github.com/tensorflow/tensorflow/issues/52978#issuecomment-971681651 ", "> Please post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo. To know more refer to: https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\n\r\nDoes this response mean that the TF team does not maintain stock models nor the compatibility with them when distributing them and it is the Keras team that should fix the models to work with the latest TF releases?", "What's about Google models such as MobileNetV3 - also Keras team should fix it?", "@compfy-dot-com Agree with you that it was an issue with old versions.\r\n\r\nThis was resolved in recent `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/6b5ef3b267d868f311f4c2ed4c003bd0/untitled1129.ipynb) is a gist for reference. If you want stable version, then it will be available in the upcoming TF2.8 in near future. Thanks!\r\n\r\nPlease verify once and close the issue if the issue got resolved for you. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52978\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52978\">No</a>\n"]}, {"number": 52976, "title": " Crash with tensorflow lite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n\r\nI am using this model\r\n\r\n        model = Sequential()        \r\n        model.add(Conv2D(32, (3, 3), input_shape=inputShape))\r\n        model.add(Activation('relu'))\r\n        model.add(Conv2D(32, (3, 3)))\r\n        model.add(Activation('relu'))\r\n        model.add(MaxPooling2D(pool_size=(2, 2)))\r\n        model.add(Dropout(0.5))\r\n\r\n        model.add(Conv2D(64, (3, 3)))\r\n        model.add(Activation('relu'))\r\n        model.add(Conv2D(64, (3, 3)))\r\n        model.add(Activation('relu'))\r\n        model.add(MaxPooling2D(pool_size=(2, 2)))\r\n        model.add(Dropout(0.5))\r\n        \r\n        model.add(Flatten())\r\n        model.add(Dense(768))#1024))#256))\r\n        model.add(Activation('relu'))\r\n        model.add(Dropout(0.5))\r\n        model.add(Dense(nbClasses))\r\n        model.add(Activation('softmax'))\r\n\r\n\r\n\r\nthen I convert it like this : \r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\r\n\r\n\r\n\r\nAnd on Android with with tensorflowlite 2.6. 0 I got a  crash because\r\n\r\n    java.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():\r\n      java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"wcsnrtombs\" referenced by \"libtensorflowlite_jni.so\"...\r\n\r\nThen I switched to tensorflowlite 2.7.0-rc0\r\nAnd I got this crash\r\n\r\n\r\njava.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():\r\n      java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"getpagesize\" referenced by \"libtensorflowlite_jni.so\"...\r\n\r\n\r\nI'll now try without this : \r\n[tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["Also, I tried with my old model (frozen, converted from tensorflowlite1 ...) from 4 years ago with tensorflowlite 2.6.\r\nand it also complains about wcsnrtombs missing", "I looked in my github history and used tensorflowlite 2.1.\r\nThen my code worked fine---> I'll refreain to upgrade tensor flow to later libs as it create issues (or I'll experiment first that it improves the app before using it)"]}, {"number": 52975, "title": "Need clarification and assistance on this one (Friday with Leanne)", "body": "Need clarification and assistance on this one (Friday with Leanne)\r\n\r\n_Originally posted by @ZCorso in https://github.com/WorldEES/Q/issues/5#issuecomment-351091481_\n\n__Originally posted by @jessecantu in https://github.com/jessecantu/overriddes/issues/1__", "comments": ["@jessecantu ,\r\n In order to expedite the trouble-shooting process, could you please provide a minimal code snippet and the TensorFlow version you are using.\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52974, "title": "Cannot build from source and illegal instruction on import tensorflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n\r\n- TensorFlow installed from (source or binary): Source and Binary\r\n\r\n- TensorFlow version: 2.6.\r\n\r\n- Python version: 3.8\r\n\r\n- Installed using virtualenv? pip? conda?: Using virtualenv to install by pip, and build from source\r\n\r\n\r\n- Bazel version (if compiling from source): 4.2.1\r\n\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n\r\n- CUDA/cuDNN version: 11.4\r\n\r\n- GPU model and memory: EVGA RTX 3060\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nWhen using pip to install, attempting to import tensorflow produces an \"Illegal instruction(core dumped)\" error.\r\n\r\nWhen using Bazel to build from source using the command, `bazel build --jobs=2 --local_ram_resources=2048 --config=opt config=cuda //tensorflow/tools/pip_package:build_pip_package` I come back hours later to find the screen frozen and the computer unresponsive.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nCurrently unable to copy and paste logs from terminal because my computer freezes during the build.\r\n", "comments": ["Photo of the terminal at screen freeze:\r\n\r\n![PXL_20211107_040457264](https://user-images.githubusercontent.com/47899472/140632226-056d12c5-599f-4792-99a2-b654fb23096d.jpg)\r\n", "Hi @DJT777! Could you try again with with [tested configurations](https://www.tensorflow.org/install/source#tested_build_configurations) for TF 2.6.", "I'm not able to completely change to the tested configurations at the moment. However I was able to get past the screen freeze by using a swap, but now I am getting this error: \r\n\r\n@mohantym \r\n```\r\n\r\nERROR: /home/dylan/tensorflow/tensorflow/python/keras/api/BUILD:133:19: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\r\nTraceback (most recent call last):\r\n  File \"/home/dylan/.cache/bazel/_bazel_dylan/e9516875bd54cee9e4f47fdcf100e0d4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 26, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/dylan/.cache/bazel/_bazel_dylan/e9516875bd54cee9e4f47fdcf100e0d4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python.feature_column import feature_column_lib as feature_column\r\n  File \"/home/dylan/.cache/bazel/_bazel_dylan/e9516875bd54cee9e4f47fdcf100e0d4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/feature_column/feature_column_lib.py\", line 22, in <module>\r\n    from tensorflow.python.feature_column.feature_column import *\r\n  File \"/home/dylan/.cache/bazel/_bazel_dylan/e9516875bd54cee9e4f47fdcf100e0d4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/feature_column/feature_column.py\", line 147, in <module>\r\n    from tensorflow.python.layers import base\r\n  File \"/home/dylan/.cache/bazel/_bazel_dylan/e9516875bd54cee9e4f47fdcf100e0d4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/layers/base.py\", line 20, in <module>\r\n    from tensorflow.python.keras.legacy_tf_layers import base\r\n  File \"/home/dylan/.cache/bazel/_bazel_dylan/e9516875bd54cee9e4f47fdcf100e0d4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.keras import models\r\n  File \"/home/dylan/.cache/bazel/_bazel_dylan/e9516875bd54cee9e4f47fdcf100e0d4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/models.py\", line 20, in <module>\r\n    from tensorflow.python.keras import metrics as metrics_module\r\n  File \"/home/dylan/.cache/bazel/_bazel_dylan/e9516875bd54cee9e4f47fdcf100e0d4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/metrics.py\", line 34, in <module>\r\n    from tensorflow.python.keras import activations\r\n  File \"/home/dylan/.cache/bazel/_bazel_dylan/e9516875bd54cee9e4f47fdcf100e0d4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/activations.py\", line 18, in <module>\r\n    from tensorflow.python.keras.layers import advanced_activations\r\n  File \"/home/dylan/.cache/bazel/_bazel_dylan/e9516875bd54cee9e4f47fdcf100e0d4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/layers/__init__.py\", line 29, in <module>\r\n    from tensorflow.python.keras.layers.preprocessing.image_preprocessing import CenterCrop\r\n  File \"/home/dylan/.cache/bazel/_bazel_dylan/e9516875bd54cee9e4f47fdcf100e0d4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py\", line 29, in <module>\r\n    from tensorflow.python.keras.preprocessing import image as image_preprocessing\r\n  File \"/home/dylan/.cache/bazel/_bazel_dylan/e9516875bd54cee9e4f47fdcf100e0d4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/preprocessing/__init__.py\", line 19, in <module>\r\n    import keras_preprocessing\r\nModuleNotFoundError: No module named 'keras_preprocessing'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/dylan/tensorflow/tensorflow/lite/python/BUILD:63:10 Executing genrule //tensorflow:tf_python_api_gen_v2 failed (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\r\nINFO: Elapsed time: 50122.650s, Critical Path: 561.72s\r\nINFO: 15830 processes: 144 internal, 15686 local.\r\nFAILED: Build did NOT complete successfully\r\n(tf-from-source) dylan@desktop:~/tensorflow$ pip install keras_preprocesing --no-deps\r\nERROR: Could not find a version that satisfies the requirement keras_preprocesing (from versions: none)\r\nERROR: No matching distribution found for keras_preprocesing\r\n\r\n```\r\n\r\nI already installed keras_preprocessing from the instructions to build from source on the tensorflow website. when I try \"python3 -m pip install keras-preprocessing\".\r\n\r\n`Requirement already satisfied: keras_preprocessing in /home/dylan/tf-from-source/lib/python3.8/site-packages (1.1.2)`\r\n\r\n\r\n```\r\npip3 show keras-preprocessing\r\nName: Keras-Preprocessing\r\nVersion: 1.1.2\r\nSummary: Easy data preprocessing and data augmentation for deep learning models\r\nHome-page: https://github.com/keras-team/keras-preprocessing\r\nAuthor: Keras Team\r\nAuthor-email: \r\nLicense: MIT\r\nLocation: /home/dylan/.local/lib/python3.8/site-packages\r\nRequires: numpy, six\r\nRequired-by: \r\n\r\n```\r\n\r\nAnd for outside of my environment I am trying with it installed too\r\n\r\n```Name: Keras-Preprocessing\r\nVersion: 1.1.2\r\nSummary: Easy data preprocessing and data augmentation for deep learning models\r\nHome-page: https://github.com/keras-team/keras-preprocessing\r\nAuthor: Keras Team\r\nAuthor-email: \r\nLicense: MIT\r\nLocation: /home/dylan/.local/lib/python3.8/site-packages\r\nRequires: numpy, six\r\nRequired-by: \r\n```\r\n```\r\ndylan@desktop:~$ python3\r\nPython 3.8.10 (default, Sep 28 2021, 16:10:42) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import keras_preprocessing\r\n```", "@mohantym \r\n\r\nI noticed you took the awaiting response off the post. Although I am not able to change build configuration are you still able to help me resolve why Im getting an error on importing keras_preprocrssing?", "Hi @DJT777! Sorry for the late response . Could you try again with Bazel 3.7.2 and GCC 7.3.1 ?\r\n\r\n", "@mohantym \r\n\r\nI was able to build successfully by not using a virtual environment, installing the dependencies in the installation instructions, and using a swap. Tensorflow is now importing without error.", "Ok @DJT777! Thanks for confirming the same. Please move this issue to closed status if issue is resolved .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52974\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52974\">No</a>\n"]}, {"number": 52972, "title": "Cannot run TensorFlow 2.7 in Docker on M1 (Apple Silicon)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 12.0 Monterey\r\n- TensorFlow installed from (source or binary): pre-built binary (Docker and pip)\r\n- TensorFlow version (use command below): 2.7\r\n- Python version: 3.8.10\r\n\r\n**Describe the current behavior**\r\n\r\nOur team needs to run TensorFlow as part of a larger application in Docker. However, this doesn't seem possible on an M1 Mac.\r\n\r\nFor example, if I use the default TF Docker image (for x86-64 only, an ARM64 image is not available): \r\n\r\n```\r\n> docker run -it tensorflow/tensorflow /bin/bash\r\n> python -c \"import tensorflow\"\r\nThe TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.\r\nqemu: uncaught target signal 6 (Aborted) - core dumped\r\nAborted\r\n```\r\n\r\nI get the same error when installing from pip on an x86-64 Linux container:\r\n\r\n```\r\n> docker run -it --platform=linux/amd64 python:3.8-buster /bin/bash\r\n> pip install --upgrade pip && pip install tensorflow\r\n> python -c \"import tensorflow\"\r\nThe TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.\r\nqemu: uncaught target signal 6 (Aborted) - core dumped\r\nAborted\r\n```\r\n\r\nOstensibly, this is because the pre-built TensorFlow [requires the CPU](https://www.tensorflow.org/install/pip#hardware-requirements) to support AVX instructions, but this is not supported by Docker / QEMU when emulating an x86-64 container on M1.  \r\n\r\n**Describe the expected behavior**\r\n\r\nThere should be a way to run TensorFlow in Docker on M1! (Without building from source.)\r\n\r\nEvery other ML/DS library works with on Docker on M1: PyTorch, Scikit-Learn, Numpy, Scipy, etc. \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSee code snippets above.", "comments": ["I think this is a dup of https://github.com/tensorflow/tensorflow/issues/52845", "The environments are similar, but the actual error is different (AVX not available vs. floating point error), so may be worth keeping as separate issues. ", "If you want to emulate on qemu you need to build TF [from sources](https://www.tensorflow.org/install/source) to disable AVX.\n\nInstead if you are interested on an aarch64 images check:\n\nhttps://github.com/tensorflow/tensorflow/issues/52845#issuecomment-962594728", "Got it \u2013\u00a0personally I'd like to avoid building from source since it complicates our dependency management. \r\n\r\nMost other ML libraries work fine out-of-the-box in x86 emulation on M1: PyTorch, Scikit-Learn, Numpy, Scipy, etc. It would be great if TF supported it as well. ", "This Is more general that M1 so if you want to use this in emulation you can close this and ad a comment to https://github.com/tensorflow/tensorflow/issues/19584", "P.s. I don't have that hw bout also you can check this Docker claim https://github.com/docker/for-mac/issues/5485#issuecomment-834163428", "> Got it \u2013\u00a0personally I'd like to avoid building from source since it complicates our dependency management.\r\n> \r\n> Most other ML libraries work fine out-of-the-box in x86 emulation on M1: PyTorch, Scikit-Learn, Numpy, Scipy, etc. It would be great if TF supported it as well.\r\n\r\nFully agree, I am in the same situation. Tensorflow is one of the only ML libraries not supporting emulation. It would be really a big win for me to have this. I use an M1-based macbook as my main development machine.", "If this is only about the X86 emulation we could close it:\r\nhttps://github.com/tensorflow/tensorflow/issues/52845#issuecomment-968383242\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Yeah, it looks like #52845 has converged on the AVX issue as well, so let's move this discussion there. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52972\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52972\">No</a>\n", "You can use my prebuilt TensorFlow .whl files for Docker and M1 (arch64): https://github.com/diyor28/tf-docker-m1"]}, {"number": 52971, "title": "Add op-determinism info to version 2.7 release notes", "body": "This is a PR for the r2.7 branch.\r\n\r\nThis PR represents release notes that should have been included with version 2.7. I failed to get this completed before the release. I'm hoping that this information can be added to the release notes on GitHub and included in the master branch version of `RELEASE.md`.\r\n\r\nPlease will someone with experience in this area, such as @goldiegadde, direct a procedure for making sure this information lands in the correct place(s).\r\n\r\nBefore this information is propagated, please will @reedwm and @pkanwar23 review for correctness and completeness. Note that there is a \"TODO: confirm exception added\" that needs to be removed with respect to `tf.math.bincount`.\r\n\r\n", "comments": ["@goldiegadde, the change to the outside-Google contributions list was just to add my name, which was/is missing. I wonder if others are missing.", "So, the process for the release notes is as follows:\r\n\r\n- Before branch cut, when you make PR, you can also edit `RELEASE.md` to include the adequate notes\r\n- After branch cut:\r\n  - If making a new PR with the goal to be cherrypicked: don't add to `RELEASE.md`, when doing the cherrypick for the PR add to the `RELEASE.md` on the branch\r\n  - If some release notes are missing on the branch, make a PR against the branch (just like this one) to update\r\n- For each RC, the release notes in the GitHub release are the contents of `RELEASE.md` at that commit (same for final release)\r\n- After final release: `RELEASE.md` from the branch gets updated on master branch\r\n\r\nNow, if we take this PR, it will be included in a future patch release. However, for patch releases we have separate release notes, so it won't show up there either.", "Thank you, @mihaimaruseac. I'm very grateful for you providing a comprehensive roadmap for updating release notes. In the past, it's been challenging for me to figure out what to do. I'm wondering if these procedures are now recorded somewhere central and permanent.\r\n\r\nFrom what you wrote above, my understanding is that even if this PR is approved and merged, its contents will be lost in the r2.7 branch `RELEASE.md`, and will never be visible in the GitHub release notes, even for the patch releases. My understanding is also that changes will not get into the master branch because we're past the final release (2.7.0). This is totally my mistake; in the past I have been more punctual. If these things are true, then it makes this PR essentially pointless. I guess this also applies to @nouiz's [52990](https://github.com/tensorflow/tensorflow/pull/52990).\r\n\r\nSo that we don't just bin this useful info, what are your thoughts on us adding these changes (instead) to the master branch's `RELEASE.md` for version 2.8 (perhaps with a note indicating that these changes were actually in version 2.7 onwards)?\r\n\r\nAlso, even though I judge myself for being so vain, and I know that all the wonderful folks at Google don't get their names on the release notes, I am a little bit miffed that I contributed so much to version 2.7, but, strangely, my name was omitted from the contributors list. Do you know what happened? Have other people's names been omitted? I'm super grateful for all your hard work, and I hope I'm not behaving inappropriately entitled, but I would like to understand what happened with this.", "You raise good questions.\r\n\r\nI'm going to merge these two PRs and then manually update the GitHub relnotes to sync and also sync on master.\r\n\r\nProbably it's also better to include these in a 2.8 section with a mention that they were added in 2.7 to make sure people get a chance to find about them if they no longer read 2.7 section.\r\n\r\nRegarding why the name was omitted, I think that's a bug in our infra. I'll file an internal bug to investigate.", "> Before this information is propagated, please will @reedwm and @pkanwar23 review for correctness and completeness\r\n\r\nThanks @duncanriach for the PR, and thanks @mihaimaruseac for merging this and explaining the situation. Sorry for not looking at this earlier. The release notes changes look good. Two minor points:\r\n\r\n* The release notes say `tf.gather` backprop is made deterministic. f29ef287148d2912e941c70891866d57de2fae04 fixes a crash with `tf.gather` backprop but it was already deterministic (and that commit barely missed 2.7, but will be in 2.8). There might be some other fix to `tf.gather` backprop that I'm unaware of or forgot about though.\r\n* There is a TODO to confirm an exception to bincount was added. I can confirm it was, in f0e6c2ff82d9892861572dd0802ff926a99b6320.\r\n\r\nNeither of these points are major so I don't think we have to worry about them if we don't want to deal with them.", "Thank you for doing all of that, @mihaimaruseac.\r\n\r\n@reedwm, the `tf.gather` backprop note is related to the addition of deterministic `tf.math.unsorted_segment_sum`. Recall that `tf.gather` backprop produces `tf.IndexedSlices`, which are reduced, using `tf.math.unsorted_segment_sum`, back into the dense `params` tensor from where they were gathered. So, even though the backprop of `tf.gather` itself is technically deterministic, I believe that essentially all users would experience the backprop of `tf.gather` as functioning nondeterministically, under the conditions that lead to the subsequent, associated nondeterministic reduction."]}, {"number": 52970, "title": "[oneDNN] Update MKL auto-mixed precision algorithm", "body": "In the current implementation of auto-mixed precision pass, the infer node is added to allow set only if both its upstream and downstream nodes are in the allow list. This could cause cast node being inserted in between fuse-able nodes. To address this problem, a sub-pass is added to place an infer node to allow set if its direct upstream is in allow set.", "comments": []}, {"number": 52967, "title": "Add NVIDIA/TensorRT repo to third_party/tensorrt", "body": "Adds ability to download and build TensorRT OSS components (github.com/NVIDIA/TensorRT) to the existing \"third_party/tensorrt\" package. The mechanism used is the same used for building NCCL components. Currently, only the EfficientNMS plugin and some supporting code is built. A small patch is applied to the external code to change file extensions, make include paths compatible, and ensure compatibility with static/shared library builds.", "comments": ["@bixia1 and @ wraveane  for review\r\n\r\nI tested the use of the plugin end-to-end to ensure everything works correctly, although I did this in the \"tensorrt_test_cc\" test, so I did not include that code here. Instead, I think we should merge #52683 first so that we can effectively test end-to-end for all TRT versions by utilizing the actual converter supplied by @wraveane 's PR.\r\n\r\nAlternatively, we could rebase @wraveane 's PR on this and merge this first ", "#52683 is still in draft state, and there were things we agreed on changing at meetings and weren't there yet. Can you please check?", "In your build file, you use efficientNMSInference.cu.cc and  efficientNMSInference.cu.h, for example. But in the plugin source tree, I only see efficientNMSInference.cu  and efficientNMSInference.cuh. Can you describe the transformation from *.cu and *.cuh to *.cu.cc and *.cu.h? In this information encoded somewhere in this PR? I couldn't figure out this.", "> In your build file, you use efficientNMSInference.cu.cc and efficientNMSInference.cu.h, for example. But in the plugin source tree, I only see efficientNMSInference.cu and efficientNMSInference.cuh. Can you describe the transformation from *.cu and *.cuh to *.cu.cc and *.cu.h? In this information encoded somewhere in this PR? I couldn't figure out this.\r\n\r\nThere is an included patch file that gets applied by the external repository command after download. If you look in the files in this PR, you will see the added patch file `tensorrt_oss.patch`. The patch is applied in the http repository command in the added `workspace.bzl` (second to last line)", "We use clang which doesn't understand this syntax, any suggestion for working around this:\r\n```\r\nthird_party/tensorrt/plugin/efficientNMSPlugin/efficientNMSInference.cu.cc:353:25: error: expected expression\r\n      PadONNXResult<<<{1}, {1}, 0, stream>>>(param, outputIndexData, nmsIndicesOutput);\r\n```\r\n\r\nthis line works:      PadONNXResult<<<1, 1, 0, stream>>>(param, outputIndexData, nmsIndicesOutput);\r\nI assume they mean the same. do they?\r\n\r\n```\r\nthird_party/tensorrt/plugin/efficientNMSPlugin/efficientNMSInference.cu.cc:478:41: error: shifting a negative signed value is undefined [-Werror,-Wshift-negative-value]\r\n            topScoresData[dataIdx] = -1 << 15;\r\n                                     ~~ ^\r\nthird_party/tensorrt/plugin/efficientNMSPlugin/efficientNMSInference.cu.cc:482:41: error: shifting a negative signed value is undefined [-Werror,-Wshift-negative-value]\r\n            topScoresData[dataIdx] = -1 << 15;\r\n                                     ~~ ^\r\nthird_party/tensorrt/plugin/efficientNMSPlugin/efficientNMSInference.cu.cc:517:39: error: shifting a negative signed value is undefined [-Werror,-Wshift-negative-value]\r\n            param.scoreThreshold = -1 << 15;\r\n\r\n```", "> We use clang which doesn't understand this syntax, any suggestion for working around this:\r\n> \r\n> ```\r\n> third_party/tensorrt/plugin/efficientNMSPlugin/efficientNMSInference.cu.cc:353:25: error: expected expression\r\n>       PadONNXResult<<<{1}, {1}, 0, stream>>>(param, outputIndexData, nmsIndicesOutput);\r\n> ```\r\n> \r\n> this line works: PadONNXResult<<<1, 1, 0, stream>>>(param, outputIndexData, nmsIndicesOutput); I assume they mean the same. do they?\r\n> \r\n> ```\r\n> third_party/tensorrt/plugin/efficientNMSPlugin/efficientNMSInference.cu.cc:478:41: error: shifting a negative signed value is undefined [-Werror,-Wshift-negative-value]\r\n>             topScoresData[dataIdx] = -1 << 15;\r\n>                                      ~~ ^\r\n> third_party/tensorrt/plugin/efficientNMSPlugin/efficientNMSInference.cu.cc:482:41: error: shifting a negative signed value is undefined [-Werror,-Wshift-negative-value]\r\n>             topScoresData[dataIdx] = -1 << 15;\r\n>                                      ~~ ^\r\n> third_party/tensorrt/plugin/efficientNMSPlugin/efficientNMSInference.cu.cc:517:39: error: shifting a negative signed value is undefined [-Werror,-Wshift-negative-value]\r\n>             param.scoreThreshold = -1 << 15;\r\n> ```\r\n\r\nApparently, it should be a constant, `-1 * (1 << 15)`. I've alerted the author that this (-1 << 15) is undefined in C++.", "@christopherbate  Can you please check @bixia1's comments and keep us posted ? Thanks!", "See comments above", "@christopherbate can you squash the commints, I will approve it after.", "done", "@christopherbate Can you please check @bixia1's comments and keep us posted ? Thanks!", "@bixia1 I was also working on this this morning. Do you want me to push changes?", "Updated with requested changes."]}, {"number": 52966, "title": "Allow doctest compare to work by removing unneeded white space", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/52069", "comments": ["Doesn't seem to be caused by spacing, the error message has very different numbers\r\n\r\n```\r\nExpected:\r\n<tf.Tensor: shape=(8,), dtype=float32, numpy=\r\narray([-1. , -0.99990916, -0.46211717, 0.7615942 , 0.8336547 ,\r\n0.9640276 , 0.9950547 , 1. ], dtype=float32)>\r\nGot:\r\n<tf.Tensor: shape=(8,), dtype=float32, numpy=\r\narray([-0.99999976, -0.99990916, -0.46211717, 0.7615942 , 0.8336546 ,\r\n0.9640276 , 0.9950547 , 0.99999976], dtype=float32)>\r\n```", "The bug is absolutely caused by the unneeded spaces. The doctest will normalise any number of spaces to 1 space, but if there is a space, it needs to match to a space. So the match of \"1.      \" will not match against \"0.9999998\" because the latter has no space, even though the tolerance is set at 1e-03. So the floats do match on the compare, but the text portion fails due to there not being a space.\r\nFrom my debug sessions the string to compare against is \r\n`p want\r\n'array([... , ..., ... , ... ], dtype=float32)'\r\n`\r\nconstructed from\r\n`p want\r\n'<tf.Tensor: shape=(4,), dtype=float32,\\nnumpy=array([0.5      , 0.7310586, 1.       , 1.       ], dtype=float32)>\\n'`\r\nnotice the space after ellipsis in the three places that correspond to the '0.5' and '1.' and '1.'. This will never match against the actual result which has no space two out of the three times.\r\n`p got\r\n'array([0.5      , 0.7310586, 0.9999998, 0.9999998], dtype=float32)\\n'`\r\nI have debugged this. I have seen what happens. This is the cause and the PR is the fix. With the PR it passes on AARCH64, without the PR it fails. This is 100% repeatable. It makes no difference on x86.", "Thanks for the context"]}, {"number": 52965, "title": "<unknown> shape of Tensor, as_list() is not defined on a unknown TensorShape", "body": "Good afternoon, \r\n\r\nI am trying to implement a pipeline using TFX and Tensorflow. For this, I use 3 accelerometer measurements to predict the activity that is being performed at that moment (6 classes). To do the processing, I have to transform the dataset to sequences, and for that I make use of .windows function. The code used to perform this processing is the following: \r\n\r\n```python\r\nHISTORY_SIZE = 80\r\nBATCH_SIZE = 32\r\nSHIFT = 40\r\n\r\ndef parse_function(example_proto):\r\n    '''Parse the values from tf examples'''\r\n    feature_spec = tf_transform_output.transformed_feature_spec()\r\n    features = tf.io.parse_single_example(example_proto, feature_spec)\r\n    values = list(features.values())\r\n    values = [float(_fill_in_missing(value)) for value in values]    \r\n    features = tf.stack(values, axis=0)\r\n    return features\r\n\r\ndef add_mode(features):\r\n    '''Calculate mode of activity for the current history size of elements'''\r\n    # Removes dimensions of size 1 from the shape of a tensor.\r\n    features = tf.squeeze(features)\r\n    \r\n    # Finds unique elements in a 1-D tensor.\r\n        # This operation returns a tensor y containing all of the unique elements of x sorted in the same order that they occur in x. \r\n        # This operation also returns a tensor idx the same size as x that contains the index of each value of x in the unique output y. \r\n        # Finally, it returns a third tensor count that contains the count of each element of y in x\r\n    unique, idx, count = tf.unique_with_counts(features[:,0])\r\n    \r\n    # Computes tf.math.maximum of elements across dimensions of a tensor.\r\n    max_occurrences = tf.reduce_max(count)\r\n    \r\n    # Returns the truth value of (x == y) element-wise.\r\n    max_cond = tf.equal(count, max_occurrences)\r\n    \r\n    # Gather slices from params axis axis according to indice\r\n    max_numbers = tf.squeeze(tf.gather(unique, tf.where(max_cond)))\r\n\r\n    #Features (X) are all features except activity (x-acc, y-acc, z-acc)\r\n    #Target(Y) is the mode of activity values of all rows in this window\r\n    return (features[:,1:], max_numbers)\r\n\r\ndef get_dataset(path):\r\n    '''Get the dataset and group them into windows'''\r\n    dataset = tf.data.TFRecordDataset(path, compression_type=\"GZIP\")\r\n    dataset = dataset.map(parse_function)\r\n    dataset = dataset.window(HISTORY_SIZE, shift=SHIFT, drop_remainder=True)dataset\r\n    dataset = dataset.flat_map(lambda window: window.batch(HISTORY_SIZE))\r\n    dataset = dataset.map(add_mode)\r\n    dataset = dataset.batch(BATCH_SIZE)\r\n    dataset = dataset.repeat()\r\n    return dataset\r\n\r\ndef _fill_in_missing(x):\r\n    \"\"\"Replace missing values in a SparseTensor.\r\n    Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\r\n    Args:\r\n    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\r\n      in the second dimension.\r\n    Returns:\r\n    A rank 1 tensor where missing values of `x` have been filled in.\r\n    \"\"\"\r\n    default_value = '' if x.dtype == tf.string else 0\r\n    # Todo fix\r\n    return x`\r\n```\r\n\r\nOnce this is done, I get the following tensors as outputs: \r\nFeatures (x-acc, y-acc, z-acc):\r\n\r\n(32, 80, 3)\r\ntf.Tensor(\r\n[[[0.6224036  0.77866393 0.526603  ]\r\n  [0.61964923 0.7690456  0.5003369 ]\r\n  [0.48020944 0.96107006 0.5791352 ]\r\n  ...\r\n  [0.5549216  0.89545894 0.5957243 ]\r\n  [0.63135535 0.85801584 0.6734857 ]\r\n  [0.5490686  0.50144786 0.5694582 ]]\r\n\r\n [[0.2963554  0.9882078  0.5528691 ]\r\n  [0.41926903 0.38533998 0.41773698]\r\n  [0.49363694 0.79515266 0.30230445]\r\n  ...\r\n  [0.57041496 0.9882078  0.69595015]\r\n  [0.5394283  0.3602634  0.43259805]\r\n  [0.5752351  0.9294669  0.63961625]]\r\n\r\n [[0.57213646 0.67526615 0.5044842 ]\r\n  [0.615862   0.41144708 0.4481503 ]\r\n  [0.47814363 0.30220947 0.30403247]\r\n  ...\r\n  [0.33112925 0.9514518  0.3537998 ]\r\n  [0.6468486  0.7594272  0.6831627 ]\r\n  [0.48124233 0.33999607 0.27119985]]\r\n\r\n ...\r\n\r\n [[0.5742022  0.7168314  0.50724906]\r\n  [0.61000896 0.754618   0.4702691 ]\r\n  [0.5122289  0.83843553 0.51519793]\r\n  ...\r\n  [0.5294437  0.6666783  0.5093227 ]\r\n  [0.5683492  0.743969   0.48201975]\r\n  [0.5036216  0.6886632  0.4954984 ]]\r\n\r\n [[0.505343   0.84427524 0.53351516]\r\n  [0.505343   0.6299222  0.51899964]\r\n  [0.5294437  0.9236271  0.6054013 ]\r\n  ...\r\n  [0.5810881  0.7515263  0.645146  ]\r\n  [0.5645619  0.8047711  0.52003646]\r\n  [0.563529   0.837405   0.5003369 ]]\r\n\r\n [[0.6175835  0.8944283  0.5587444 ]\r\n  [0.50431013 0.84324473 0.5238381 ]\r\n  [0.5101631  0.7343506  0.516926  ]\r\n  ...\r\n  [0.54700285 0.76114476 0.534552  ]\r\n  [0.55285585 0.6879761  0.50241053]\r\n  [0.49466982 0.6886632  0.5093227 ]]], shape=(32, 80, 3), dtype=float32)\r\n\r\nTarget (activity):\r\n\r\ntf.Tensor(\r\n[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.\r\n 2. 2. 2. 2. 2. 2. 2. 2.], shape=(32,), dtype=float32)\r\n(32,)\r\n\r\n\r\nWhen I want to get the information from this tensor, I get the following result: <RepeatDataset shapes: (< unknown>, < unknown>), types: (tf.float32, tf.float32)>\r\n\r\nIf I want to train a simple model using these sequences, I get the following error, and I don't know how to fix it: ValueError: as_list() is not defined on an unknown TensorShape.\r\n\r\nThe code of the model is below: \r\n```python\r\nmodel = Sequential()\r\nmodel.add(LSTM(units=200, input_shape=(HISTORY_SIZE,3),name=\"LSTM_1\"))\r\nmodel.add(Dropout(0.15))\r\nmodel.add(Dense(units=20, activation='relu',name=\"DENSE_1\"))\r\nmodel.add(Dropout(0.15))\r\nmodel.add(Dense(units=10, activation='relu'))\r\nmodel.add(Dense(6, activation='softmax',name=\"OUTPUT\"))\r\nadam = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\r\nmodel.compile(loss= 'CategoricalCrossentropy' , optimizer=adam, metrics=['accuracy']) #, weighted_metrics=METRICS)\r\nmodel.fit(train_dataset, epochs=500, steps_per_epoch=20)`\r\n```\r\n\r\nIs there any way to solve it, I have seen some solution in the case of images, but not in the case of sequences. \r\n\r\n\r\nVersions: \r\ntfx-bsl==1.3.0\r\ntensorflow-model-analysis==0.34.1\r\ntensorflow-data-validation==1.3.0\r\ntfx==1.3.2\r\ntensorflow-metadata==1.2.0\r\nml-metadata==1.3.0\r\ntensorflow-transform==1.3.0\r\ntensorflow==2.6.2\r\npyarrow==2.0.0\r\napache-beam==2.32.0\r\ntensorflow-serving-api==2.6.0\r\ntensorflow-estimator==2.6.0\r\n", "comments": ["Could you please refer the answer [here](https://stackoverflow.com/questions/67573468/) and let me know if this helps, Thanks!", "I'm trying to adapt this solution to my approach. I'll update the issue as soon I get some interesting results.", "This problem seems to have solved my doubt. I have other derived errors, but they will be in other issues. "]}, {"number": 52964, "title": "Build did NOT complete successfully", "body": "Hi,\r\n\r\nMy system is,\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Computational Node\r\n- TensorFlow installed from (source or binary): git clone https://github.com/tensorflow/tensorflow tensorflow\r\n- TensorFlow version: Install TensorFlow 2\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: Building from source \r\n- Bazel version (if compiling from source): bazel 3.1.0.\r\n- GCC/Compiler version (if compiling from source): Tried versions 7,8,9, and 10\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\nAfter following the steps bellow;\r\n\r\n**cd /some/workspace\r\ngit clone https://github.com/tensorflow/tensorflow tensorflow\r\ncd tensorflow\r\ngit checkout r2.4\r\n./configure--> to special options were chosen\r\nbazel build -c opt --verbose_failures //tensorflow:libtensorflow_cc.so**\r\n\r\nI get this log of errors:\r\n\r\n**```\r\nINFO: Options provided by the client: Inherited 'common' options: --isatty=1 --terminal_columns=237 INFO: Reading rc options for 'build' from /home/u211355/tensorflow/.bazelrc: Inherited 'common' options: --experimental_repo_remote_exec INFO: Reading rc options for 'build' from /home/u211355/tensorflow/.bazelrc: 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2 INFO: Reading rc options for 'build' from /home/u211355/tensorflow/.tf_configure.bazelrc: 'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3.6 --action_env PYTHON_LIB_PATH=/usr/lib/python3.6/site-packages --python_path=/usr/bin/python3.6 --config=xla --action_env TF_CONFIGURE_IOS=0 INFO: Found applicable config definition build:short_logs in file /home/u211355/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /home/u211355/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:xla in file /home/u211355/tensorflow/.bazelrc: --define=with_xla_support=true INFO: Found applicable config definition build:linux in file /home/u211355/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels INFO: Found applicable config definition build:dynamic_kernels in file /home/u211355/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS DEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\" DEBUG: Repository io_bazel_rules_go instantiated at: no stack (--record_rule_instantiation_callstack not enabled) Repository rule git_repository defined at: /home/u211355/.cache/bazel/_bazel_u211355/4bc589a879d91108e775a9cd349c5c7c/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel> INFO: Build options --action_env, --define, and --host_copt have changed, discarding analysis cache. WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found INFO: Analyzed target //tensorflow:libtensorflow_cc.so (211 packages loaded, 20042 targets configured). INFO: Found 1 target... ERROR: /home/u211355/.cache/bazel/_bazel_u211355/4bc589a879d91108e775a9cd349c5c7c/external/com_google_protobuf/BUILD:110:1: C++ compilation of rule '@com_google_protobuf//:protobuf_lite' failed (Exit 1): gcc failed: error executing command (cd /home/u211355/.cache/bazel/_bazel_u211355/4bc589a879d91108e775a9cd349c5c7c/execroot/org_tensorflow && \\ exec env - \\ LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64/dyninst:/opt/rh/devtoolset-8/root/usr/lib/dyninst:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib \\ PATH=/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-7/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-8/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-9/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-10/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/home/u211355/miniconda3/condabin:/opt/sge/bin:/opt/sge/bin/lx-amd64:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/.local/bin:/home/u211355/bin:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util \\ PWD=/proc/self/cwd \\ /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.d '-frandom-seed=bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.o' -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -g0 -w -g0 '-std=c++14' -DHAVE_PTHREAD -DHAVE_ZLIB -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -Wno-write-strings -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/com_google_protobuf/src/google/protobuf/any_lite.cc -o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.o) Execution platform: @local_execution_config_platform//:platform gcc: error: unrecognized command line option '-std=c++14' Target //tensorflow:libtensorflow_cc.so failed to build ERROR: /home/u211355/tensorflow/tensorflow/BUILD:820:1 C++ compilation of rule '@com_google_protobuf//:protobuf_lite' failed (Exit 1): gcc failed: error executing command (cd /home/u211355/.cache/bazel/_bazel_u211355/4bc589a879d91108e775a9cd349c5c7c/execroot/org_tensorflow && \\ exec env - \\ LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64/dyninst:/opt/rh/devtoolset-8/root/usr/lib/dyninst:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib \\ PATH=/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-7/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-8/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-9/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-10/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/home/u211355/miniconda3/condabin:/opt/sge/bin:/opt/sge/bin/lx-amd64:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/.local/bin:/home/u211355/bin:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util \\ PWD=/proc/self/cwd \\ /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.d '-frandom-seed=bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.o' -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -g0 -w -g0 '-std=c++14' -DHAVE_PTHREAD -DHAVE_ZLIB -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -Wno-write-strings -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/com_google_protobuf/src/google/protobuf/any_lite.cc -o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.o) Execution platform: @local_execution_config_platform//:platform INFO: Elapsed time: 55.118s, Critical Path: 0.02s INFO: 0 processes. FAILED: Build did NOT complete successfully\r\nI would appreciate your help with this error. I have also tried different versions of GCC from 7-10 and that also didn't solve the error.\r\n```**\r\n\r\n", "comments": ["@amirbehbahanian \r\nPlease verify if you meet the [requirements](https://www.tensorflow.org/install/source),\r\nyou may also refer to below similar error issues: [link](https://github.com/google/mediapipe/issues/533),[link1](https://github.com/google/mediapipe/issues/70)", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52964\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52964\">No</a>\n"]}, {"number": 52963, "title": "Call hexagon delegate only if TFLITE_ENABLE_HEXAGON is defined.", "body": null, "comments": ["This was indeed fixed as part of https://github.com/tensorflow/tensorflow/commit/302d9b34926108103a54ac8ae6ad63c93e83302f so am closing this.  Thanks @karimnosseir for fixing this!"]}, {"number": 52962, "title": "Add external delegate support to the cmake build of benchmark app", "body": null, "comments": ["@terryheo, this is the separated pull request from #52866", "@terryheo  Can you please review this PR ? Thanks!", "Hi Everyone, what else is required to get this merged into master?", "NAgarwalla, did you sign the CLA(Contributor License Agreement)? https://www.tensorflow.org/community/contribute/code", "> NAgarwalla, did you sign the CLA(Contributor License Agreement)? https://www.tensorflow.org/community/contribute/code\n\nYes, the cla check passed, too. I already had another change pulled a few weeks ago.  I think someone else with privileges needs to initiate the actual pull request. "]}, {"number": 52961, "title": "Fix build failure when TFLITE_ENABLE_HEXAGON is not defined", "body": null, "comments": ["Still learning github..."]}, {"number": 52959, "title": "data.Dataset.from_generator  Report Error in Distributed Training", "body": "the message is \"Cannot assign a device for operation PyFunc: {{node PyFunc}} was explicitly assigned to /job:localhost/replica:0/task:0 but available devices are [ /job:worker/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\"  but my codes run in TF 2.2.0,  the training workers fine.    \r\n\r\nmy demo code test.py is :\r\n```python\r\nimport tensorflow.compat.v1 as tf\r\nimport time\r\nimport random\r\nfrom tensorflow.python.framework import ops\r\nimport logging\r\nimport numpy as np\r\ntf.disable_v2_behavior()\r\n\r\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\", \"One of 'ps', 'worker'\")\r\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\", \"One of 'ps', 'worker'\")\r\ntf.app.flags.DEFINE_string(\"job_name\", \"worker\", \"One of 'ps', 'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\ntf.app.flags.DEFINE_string(\"buckets\", None, \"oss buckets\")\r\ntf.app.flags.DEFINE_integer(\"interval\", 60, \"interval\")\r\ntf.app.flags.DEFINE_string(\"protocol\", \"grpc\", \"interval\")\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\ndef data_generator():\r\n    dataset = np.array(range(1000))\r\n    for d in dataset:\r\n        yield d\r\ndef train(task_index, cluster, is_chief, target, buckets):\r\n  available_worker_device = \"/job:worker/task:%d\" % (task_index)\r\n  with tf.device(\r\n      tf.train.replica_device_setter(worker_device=available_worker_device,\r\n                                     cluster=cluster)):\r\n    dataset = tf.data.Dataset.from_generator(data_generator, (tf.int32), (tf.TensorShape([])))\r\n    dataset = dataset.repeat(3)\r\n    dataset = dataset.batch(4)\r\n    iterator = dataset.make_one_shot_iterator()\r\n    one_element = iterator.get_next()\r\n\r\n    local_step = 1\r\n    config = tf.ConfigProto(inter_op_parallelism_threads=7)\r\n    with tf.train.MonitoredTrainingSession(master=target,\r\n                                           is_chief=is_chief,\r\n                                           hooks=[],\r\n                                           save_checkpoint_secs=60,\r\n                                           checkpoint_dir=buckets,\r\n                                           config=config) as mon_sess:\r\n      while True:\r\n        l = mon_sess.run(one_element)\r\n        local_step += 1\r\n        if local_step % 100 == 0:\r\n          logging.info(l)\r\n\r\n\r\ndef main(argv):\r\n  print(\"job name = %s\" % FLAGS.job_name)\r\n  print(\"task index = %d\" % FLAGS.task_index)\r\n  is_chief = FLAGS.task_index == 0\r\n\r\n  ps_spec = FLAGS.ps_hosts.split(\",\")\r\n  worker_spec = FLAGS.worker_hosts.split(\",\")\r\n  cluster = tf.train.ClusterSpec({\"ps\": ps_spec, \"worker\": worker_spec})\r\n\r\n  config = tf.ConfigProto(inter_op_parallelism_threads=8)\r\n  server = tf.distribute.Server(cluster,\r\n                                job_name=FLAGS.job_name,\r\n                                protocol=FLAGS.protocol,\r\n                                task_index=FLAGS.task_index,\r\n                                config=config)\r\n\r\n  # join the ps server\r\n  if FLAGS.job_name == \"ps\":\r\n    server.join()\r\n  # start the training\r\n  print(\"start trainig\")\r\n  print(FLAGS.buckets)\r\n  train(task_index=FLAGS.task_index,\r\n        cluster=cluster,\r\n        is_chief=is_chief,\r\n        target=server.target,\r\n        buckets=FLAGS.buckets)\r\nif __name__ == \"__main__\":\r\n  tf.app.run()\r\n```\r\n\r\nmy start command is below:\r\npython test.py --ps_hosts=127.0.0.1:9200 --worker_hosts=127.0.0.1:9100 --task_index=0 --job_name=ps\r\npython test.py --ps_hosts=127.0.0.1:9200 --worker_hosts=127.0.0.1:9100 --task_index=0 --job_name=worker\r\n\r\ncould you please tell me what's wrong with my code? my code works fine in TF2.2.0\u3002\r\nhope for your answer, thanks!", "comments": ["@xingkong1994 \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "@xingkong1994 Could you please try on the latest version of `TF 2.7.0` and let us know if it is still an issue ?  Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52959\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52959\">No</a>\n"]}, {"number": 52958, "title": "[Help] How to feed the RaggedTensor as inputs in Graph execution ?", "body": null, "comments": ["Could you please fill the issue template here https://github.com/tensorflow/tensorflow/issues/new/choose to understand your problem. Thanks", "Closing this since no information is available. Thanks."]}, {"number": 52957, "title": "TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorflow.TensorShapeProto got tensorflow.TensorShapeProto.", "body": "Hi I am still getting the 'TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorflow.TensorShapeProto got tensorflow.TensorShapeProto.' even after applying the fix mentioned previously.\r\nAny suggestions?\r\n", "comments": ["Hi @Crispisu! Could you please fill the template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose) as it helps expedite the issue.", "Hi @mohantym ! Thank you for this. In the meantime I have managed to solve this as per instructions You provided to previous similar issues. The problem was related to my laptop as I am using an M1 chip which causes multiple issues for tensorflow. \nCan you please close this one?", "Ok @Crispisu! Thanks for confirming the same. Attaching similar issue for reference. [link1](https://github.com/tensorflow/tensorflow/issues/50545),[link2](https://stackoverflow.com/questions/67570696/typeerror-parameter-to-mergefrom-must-be-instance-of-same-class-expected-ten). Please close this issue once it has been resolved. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52957\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52957\">No</a>\n"]}, {"number": 52956, "title": "Change the input layer from a saved TF model", "body": "Is there any way to load a Tensorflow model, change the input layer and save again the new model? (I would like to convert a model from NHWC vs NCHW) ", "comments": ["@AlexisPapaioannou \r\nCould you please refer to [Save and load model ](https://www.tensorflow.org/tutorials/keras/save_and_load) and this [answer](https://discuss.tensorflow.org/t/how-to-replace-the-input-layer-of-a-model/2609) in TF forum , for any further queries you may open this issue in [TF discussion forum ](https://discuss.tensorflow.org/) as there is a wider community to respond. Thank you!", "Thanks, @sushreebarsa !"]}, {"number": 52955, "title": "tf.signal.stft has dynamic-sized tensors causing the tflite delegate to not work", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 16.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **iPhone12**\r\n- TensorFlow installed from (source or binary): **PIP**\r\n- TensorFlow version (use command below): **2.6**\r\n- Python version: **3.9**\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nI tried adding the tf.signal.stft operation to the model and exported it to the tflite model for testing,\r\nfound that the XNNPack delegate will report the following error message: \"\r\nAttempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\",\r\nI think it's bad for performance.\r\n```\r\n\r\n**Describe the expected behavior**\r\n```\r\nExpect tf.signal.stft to support use in delegate, because it is very useful for audio processing.\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nclass STFT(tf.keras.layers.Layer):\r\n    def __init__(self, fft_size, hop_size, **kwargs):\r\n        super(STFT, self).__init__(**kwargs)\r\n        self._fft_size = fft_size\r\n        self._hop_size = hop_size\r\n\r\n    def call(self, inputs):\r\n        return tf.signal.stft(inputs, \r\n                              frame_length=self._fft_size,\r\n                              frame_step=self._hop_size,\r\n                              pad_end=True)\r\n\r\nmodel = tf.keras.models.Sequential([\r\n        tf.keras.layers.Input(shape=(160000)),\r\n        STFT(1024, 500)\r\n    ])\r\n\r\nmodel.save(saved_model_path, save_format='tf')\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\r\ntflite_model = converter.convert()\r\nopen(tflite_path, \"wb\").write(tflite_model_path)\r\n```", "comments": ["@vonchenplus ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.", "Hello @tilakrayal, I have updated my export code and this model with no train parameters.", "As the error says, as of now it only supports static-sized tensors, try providing the static tensors in the inputs to make your code work.\r\nIf you want the dynamic tensors support, you can change it to a feature request. Thanks!", "Feeling your suggestion, I use the following code for exporting and it work for me,\r\n\r\n```\r\nclass STFT(tf.Module):\r\n    def __init__(self, fft_size, hop_size, **kwargs):\r\n        super(STFT, self).__init__(**kwargs)\r\n        self._fft_size = fft_size\r\n        self._hop_size = hop_size\r\n\r\n    @tf.function\r\n    def inference_fn(self, inputs):\r\n        return tf.signal.stft(inputs, \r\n                              frame_length=self._fft_size,\r\n                              frame_step=self._hop_size,\r\n                              pad_end=True)\r\n\r\nmodel = STFT(1024, 500)\r\nconcrete_function = model.inference_fn.get_concrete_function(\r\n            tf.TensorSpec(\r\n                shape=(160000), dtype=tf.float32, name='input'))\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_function])\r\ntflite_model = converter.convert()\r\nwith open(tflite_model_path, 'wb') as f:\r\n  f.write(tflite_model)\r\n```"]}, {"number": 52954, "title": "TFLite int8 quantization failure in TensorFlow 2.7.0", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): v2.7.0\r\n\r\n### 2. Code\r\n\r\nThe MWE is provided in the following colab notebook:\r\nhttps://colab.research.google.com/drive/1K-Hwq6LEfjFr-4dSLEI4dhQd6pCR0K-F?usp=sharing\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n\r\n- Model produces wrong results and/or has lesser accuracy.\r\n- Model produces correct results, but it is slower than expected.\r\n\r\n### 4. (optional) RNN conversion support\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n### 5. (optional) Any other info / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThe log is attached as below:\r\n```\r\nINFO:tensorflow:Assets written to: /tmp/tmp0cb_yd9c/assets\r\nINFO:tensorflow:Assets written to: /tmp/tmp0cb_yd9c/assets\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-15-674e2748ac1c> in <module>()\r\n     16 converter.inference_output_type = tf.uint8  # or tf.int8\r\n     17 converter.experimental_new_quantizer = False\r\n---> 18 tflite_model = converter.convert()\r\n\r\n10 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/schema_py_generated.py in Pack(self, builder)\r\n   5704             for i in reversed(range(len(self.operatorCodes))):\r\n   5705                 builder.PrependUOffsetTRelative(operatorCodeslist[i])\r\n-> 5706             operatorCodes = builder.EndVector(len(self.operatorCodes))\r\n   5707         if self.subgraphs is not None:\r\n   5708             subgraphslist = []\r\n\r\nTypeError: EndVector() takes 1 positional argument but 2 were given\r\n```\r\n\r\nThis error only occurs with `flabbuffers==2.0`. I could convert an int8 quantization model with `tensorflow==2.7.0` and `flatbuffers==1.12`. This problem is related to the PR https://github.com/tensorflow/tensorflow/pull/51504. In the \r\n issue https://github.com/google/flatbuffers/issues/6858, it is discussed that the number of `EndVector()`'s arguments changed from two to one with the flatbuffers version changing from `1.12` to `2.0`.", "comments": ["Hi @Saduf2019 @sachinprasadhs ! Could you please look at this issue . It is replicating in TF [2.6](https://colab.sandbox.google.com/gist/mohantym/b4faca62759831aafe849e56357c448c/tf2-7-0-fb-2-0-tfl-int8-quant.ipynb#scrollTo=CBaIlu0d7JmG),[2.7](https://colab.sandbox.google.com/gist/mohantym/426183f718608c162ca345ca1608489d/tf2-7-0-fb-2-0-tfl-int8-quant.ipynb) and [nightly](https://colab.sandbox.google.com/gist/mohantym/ddbc510fd21f2ca7b564dbdc44a6c07c/tf2-7-0-fb-2-0-tfl-int8-quant.ipynb#scrollTo=wUAXNyAnKn-c) and related to PR #51504", "@zldrobit \r\nCould you please refer to [this comment](https://github.com/tensorflow/tensorflow/issues/51590#issuecomment-912614740) and let us know if it [helps](https://github.com/tensorflow/tensorflow/issues/51590#issuecomment-915458143).", "@Saduf2019 The link actually helps, and downgrading flatbuffers fixes the problem.", "@zldrobit \r\nThank you for your update, Kindly move this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52954\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52954\">No</a>\n"]}, {"number": 52953, "title": "Missing tensorflow 2.6.2 Windows wheels", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow version: 2.6.2\r\n- Python version: 3.8\r\n- Installed using: poetry (pip)\r\n\r\nAfter running poetry (1.1.11) on a project, the dependency resolution picks tensorflow version 2.6.2 but when it tries to install the package it fails because tensorflow 2.6.2 is missing the Windows wheels (https://pypi.org/project/tensorflow/2.6.2/#files).\r\n\r\nWindows wheels are available for versions 2.6.0 (https://pypi.org/project/tensorflow/2.6.0/#files), 2.6.1 (https://pypi.org/project/tensorflow/2.6.1/#files) and 2.7.0 (https://pypi.org/project/tensorflow/2.7.0/#files). \r\n", "comments": ["@jponf In order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),Thanks!", "**System information**\r\n\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.6.2\r\n- Python version: 3.8\r\n- Installed using: poetry (pip)\r\n\r\n**Describe the problem**\r\n\r\nAfter running poetry (1.1.11) on a project, the dependency resolution picks tensorflow version 2.6.2 but when it tries to install the package it fails because tensorflow 2.6.2 is missing the Windows wheels (https://pypi.org/project/tensorflow/2.6.2/#files).\r\n\r\nWindows wheels are available for versions 2.6.0 (https://pypi.org/project/tensorflow/2.6.0/#files), 2.6.1 (https://pypi.org/project/tensorflow/2.6.1/#files) and 2.7.0 (https://pypi.org/project/tensorflow/2.7.0/#files). \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nFor the sake of simplicity, I provide a simpler sequence of steps that generates the same error.\r\n\r\nUsing Python 3.8.5 installed with conda on Windows 10.\r\n\r\n```console\r\npython -m venv test\r\n.\\test\\Scripts\\activate\r\npip3 install tensorflow==2.6.2\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nTrying to install another version, for example 2.6.1 or 2.7.0 works as expected.\r\n\r\n```console\r\npython -m venv test\r\n.\\test\\Scripts\\activate\r\npip3 install tensorflow==2.6.1\r\n```\r\n\r\n```console\r\npython -m venv test\r\n.\\test\\Scripts\\activate\r\npip3 install tensorflow==2.7.0\r\n```\r\n", "Please tag me when a solution is proposed.  I ran into the same issue today!", "Me too. Waiting for the windows wheels on pip.", "The Windows builds are extremely slow (18 hours and more) and very flaky. Hopefully we'd have all wheels by tomorrow.", "> [19,687 / 19,942] Compiling tensorflow/core/kernels/matmul_op_real.cc; 13710s local, remote-cache ... (63 actions running)\r\n", "Still delayed, two more jobs restarted :(", "I never though it was so complicated to create the windows wheels, thanks @mihaimaruseac for keeping us posted :)", "One more build left to do, sorry for all these delays :(", "@mihaimaruseac   Thanks for the status updates! ", "The windows wheels should now be updated. Apologies for the long delay\r\n\r\n> [20,917 / 21,397] Compiling tensorflow/core/kernels/matmul_op_real.cc; 30162s local, remote-cache ... (4 actions running)\r\n\r\n", "@mihaimaruseac works now for me. Thanks for updating.", "Thanks for confirming. Closing the issue then.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52953\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52953\">No</a>\n"]}, {"number": 52951, "title": "Cannot register 2 metrics with the same name: /tensorflow/api/keras/optimizers", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Docker image \r\n- TensorFlow version: 2.6.1\r\n- Keras version: Getting the above error when import\r\n- Python version: Python 3.6\r\n- Installed using virtualenv? pip? conda?: pip (Docker image)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.4\r\n- GPU model and memory: Nvidia A100 40GB \r\n\r\n\r\n\r\n**Describe the problem**\r\nLooks like this issue , https://github.com/keras-team/keras/issues/15579\r\nIs it the right one ? because after applying the fix mentioned in above link \"pip install tensorflow==2.6.2\" , getting below error\r\n```\r\nroot@6a893d98bbb1:/app/project# python3 download_process.py --help\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\nTraceback (most recent call last):\r\n  File \"download_process.py\", line 13, in <module>\r\n    from utils import *\r\n  File \"/app/project/utils.py\", line 4, in <module>\r\n    from object_detection.inputs import train_input\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection/inputs.py\", line 26, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py\", line 29, in <module>\r\n    from object_detection.builders import matcher_builder\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection/builders/matcher_builder.py\", line 23, in <module>\r\n    from object_detection.matchers import bipartite_matcher  # pylint: disable=g-import-not-at-top\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection/matchers/bipartite_matcher.py\", line 20, in <module>\r\n    from tensorflow.contrib.image.python.ops import image_ops\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n- Build Image by running below dockerfile\r\n\r\n```\r\nFROM tensorflow/tensorflow:2.3.0-gpu\r\n\r\nRUN apt-get update --fix-missing && apt-get install -y \\\r\n    ffmpeg \\\r\n    git \\\r\n    git-core \\\r\n    g++ \\\r\n    pkg-config \\\r\n    python3-pip \\\r\n    unzip \\\r\n    vim \\\r\n    wget \\\r\n    zip \\\r\n    zlib1g-dev\r\n\r\nWORKDIR /app\r\n\r\nCOPY requirements.txt .\r\nRUN pip3 install -r requirements.txt\r\nRUN pip3 install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI\r\n\r\nENV TF_CPP_MIN_LOG_LEVEL=2\r\n\r\nRUN wget https://github.com/protocolbuffers/protobuf/releases/download/v3.13.0/protoc-3.13.0-linux-x86_64.zip && \\\r\n    unzip protoc-3.13.0-linux-x86_64.zip -d /app/protobuf/\r\n\r\nENV PATH \"$PATH:/app/protobuf/bin\"\r\n\r\nRUN git clone https://github.com/tensorflow/models.git && \\\r\n    cd /app/models/research/ && \\\r\n    protoc object_detection/protos/*.proto --python_out=. && \\\r\n    cp object_detection/packages/tf2/setup.py . &&\\\r\n    python -m pip install .\r\n```\r\n\r\n- Spawned container and inside it trying to run a pre-processing script. Below are packages used in it\r\n   \r\n```\r\nimport argparse\r\nimport io\r\nimport os\r\nimport subprocess\r\n\r\nimport ray\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n#import tensorflow as tf\r\nfrom PIL import Image\r\nfrom psutil import cpu_count\r\ntf.disable_v2_behavior()\r\nfrom utils import *\r\nfrom object_detection.utils import dataset_util, label_map_util\r\n```\r\n\r\n- Error Log Output\r\n```\r\n2021-11-04 10:25:18.325660: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/api/keras/optimizers\r\nTraceback (most recent call last):\r\n  File \"download_process.py\", line 13, in <module>\r\n    from utils import *\r\n  File \"/app/project/utils.py\", line 4, in <module>\r\n    from object_detection.inputs import train_input\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection/inputs.py\", line 26, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py\", line 25, in <module>\r\n    from object_detection.builders import box_predictor_builder\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection/builders/box_predictor_builder.py\", line 20, in <module>\r\n    from object_detection.predictors import convolutional_box_predictor\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection/predictors/convolutional_box_predictor.py\", line 26, in <module>\r\n    from object_detection.core import box_predictor\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection/core/box_predictor.py\", line 137, in <module>\r\n    class KerasBoxPredictor(tf.keras.layers.Layer):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/__init__.py\", line 25, in <module>\r\n    from keras import models\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/models.py\", line 20, in <module>\r\n    from keras import metrics as metrics_module\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/metrics.py\", line 26, in <module>\r\n    from keras import activations\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/activations.py\", line 20, in <module>\r\n    from keras.layers import advanced_activations\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/__init__.py\", line 23, in <module>\r\n    from keras.engine.input_layer import Input\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/input_layer.py\", line 21, in <module>\r\n    from keras.engine import base_layer\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\", line 43, in <module>\r\n    from keras.mixed_precision import loss_scale_optimizer\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/mixed_precision/loss_scale_optimizer.py\", line 18, in <module>\r\n    from keras import optimizers\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/optimizers.py\", line 26, in <module>\r\n    from keras.optimizer_v2 import adadelta as adadelta_v2\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/optimizer_v2/adadelta.py\", line 22, in <module>\r\n    from keras.optimizer_v2 import optimizer_v2\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 37, in <module>\r\n    \"/tensorflow/api/keras/optimizers\", \"keras optimizer usage\", \"method\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/monitoring.py\", line 361, in __init__\r\n    len(labels), name, description, *labels)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/monitoring.py\", line 135, in __init__\r\n    self._metric = self._metric_methods[self._label_length].create(*args)\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.\r\n\r\n```\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["GREETINGS IT SPECIALIST! SUYUN CDN IS HIRING NOW! \r\n\r\nCompany name: SuyunCDN\r\n\r\nTitle of position: Python Developer\r\n\r\nPosition type: Full Time, Part Time, or Contractual\r\n\r\nPay range: $800 - $2000\r\n\r\n\r\nLocation: Remote\r\n\r\nPreferred Qualifications: \r\n\t\r\n\u25cf\tGraduated in any IT related course\r\n\u25cf\tProficient in English and Mandarin Language\r\n\u25cf\tProficient in Python Coding\r\n\r\nDescription of responsibilities: \r\n\r\n\u25cf\tWriting and testing code, debugging programs and integrating applications with third-party web services\r\n\r\n\u25cf\tDevelop back-end components to improve responsiveness and overall performance\r\n\u25cf\tIntegrate user-facing elements into applications\r\n\u25cf\tTest and debug programs\r\n\u25cf\tImprove functionality of existing systems\r\n\u25cf\tImplement security and data protection solutions\r\n\u25cf\tAssess and prioritize feature requests\r\n\u25cf\tCoordinate with internal teams to understand user requirements and provide technical solutions\r\n\r\n\r\nRequired experience: \r\n\u25cf\tWork experience as a Python Developer\r\n\r\nRequired skills: \r\n\u25cf\tProficient with Django Python framework\r\n\u25cf\tProficient with DevOps and Ansible\r\n\u25cf\tExtensive Knowledge on object-relational mapping (ORM)\r\n\u25cf\tProblem Solving Skill- Advance\r\n\u25cf\tCommunication Skill - Advance\r\n\u25cf\tHighly Resourceful\r\n\u25cf\tTeam-oriented personality\r\n\r\n\r\nFor information on SuyunCDN, including more information on our staff and our company culture, you may check our website at https://www.suyuncdn.com.\r\n\r\nIf interested, email your resume to suyuncdnclaudie@gmail.com\r\nwith the subject line of the job, you're applying for.\r\nCheck out our FB page for more info on available jobs we have\r\nSuyuncdn\r\n\r\n\r\n", "This should be fixed by TF 2.6.2 or TF 2.7.0", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52951\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52951\">No</a>\n"]}, {"number": 52950, "title": "Update Release notes in master", "body": "Merge release notes", "comments": ["bf9665ecde98281c26d415b2caa212d10047a9d9"]}, {"number": 52949, "title": "Add support for GEMMs with complex element types", "body": null, "comments": []}, {"number": 52948, "title": "CI: Use Python 3.9 for PyLint presubmit action", "body": "A few small maintenance updates to the PyLint presubmit action.\r\n - Use Python 3.9 instead of Python 3.8.\r\n - Update the [setup-python](https://github.com/actions/setup-python) action to v2, which is actively maintained.\r\n - Parse the Python version as a string (`\"3.9\"` instead of `3.9`), since otherwise Python 3.10 will be rounded and processed as Python 3.1 in the future.", "comments": ["/cc @angerson ", "Thanks for this!"]}, {"number": 52947, "title": "Build did NOT complete successfully", "body": "Hi,\r\n\r\nI'm using python Python 3.6.8 on CentOS 7, my bazel version is bazel 3.1.0. Using \"git checkout r2.4\" and trying to make the C++ interface I get the following error:\r\n\r\n`INFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=237\r\nINFO: Reading rc options for 'build' from /home/u211355/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/u211355/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/u211355/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3.6 --action_env PYTHON_LIB_PATH=/usr/lib/python3.6/site-packages --python_path=/usr/bin/python3.6 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /home/u211355/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/u211355/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/u211355/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:linux in file /home/u211355/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/u211355/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\"\r\nDEBUG: Repository io_bazel_rules_go instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /home/u211355/.cache/bazel/_bazel_u211355/4bc589a879d91108e775a9cd349c5c7c/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nINFO: Build options --action_env, --define, and --host_copt have changed, discarding analysis cache.\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Analyzed target //tensorflow:libtensorflow_cc.so (211 packages loaded, 20042 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/u211355/.cache/bazel/_bazel_u211355/4bc589a879d91108e775a9cd349c5c7c/external/com_google_protobuf/BUILD:110:1: C++ compilation of rule '@com_google_protobuf//:protobuf_lite' failed (Exit 1): gcc failed: error executing command\r\n  (cd /home/u211355/.cache/bazel/_bazel_u211355/4bc589a879d91108e775a9cd349c5c7c/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64/dyninst:/opt/rh/devtoolset-8/root/usr/lib/dyninst:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib \\\r\n    PATH=/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-7/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-8/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-9/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-10/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/home/u211355/miniconda3/condabin:/opt/sge/bin:/opt/sge/bin/lx-amd64:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/.local/bin:/home/u211355/bin:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.d '-frandom-seed=bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.o' -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -g0 -w -g0 '-std=c++14' -DHAVE_PTHREAD -DHAVE_ZLIB -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -Wno-write-strings -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/com_google_protobuf/src/google/protobuf/any_lite.cc -o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\ngcc: error: unrecognized command line option '-std=c++14'\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nERROR: /home/u211355/tensorflow/tensorflow/BUILD:820:1 C++ compilation of rule '@com_google_protobuf//:protobuf_lite' failed (Exit 1): gcc failed: error executing command\r\n  (cd /home/u211355/.cache/bazel/_bazel_u211355/4bc589a879d91108e775a9cd349c5c7c/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64/dyninst:/opt/rh/devtoolset-8/root/usr/lib/dyninst:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib \\\r\n    PATH=/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-7/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-8/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-9/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-10/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/home/u211355/miniconda3/condabin:/opt/sge/bin:/opt/sge/bin/lx-amd64:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/.local/bin:/home/u211355/bin:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.d '-frandom-seed=bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.o' -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -g0 -w -g0 '-std=c++14' -DHAVE_PTHREAD -DHAVE_ZLIB -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -Wno-write-strings -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/com_google_protobuf/src/google/protobuf/any_lite.cc -o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nINFO: Elapsed time: 55.118s, Critical Path: 0.02s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n`\r\nI would appreciate your help with this error. I have also tried different versions of GCC from 7-10 and that also didn't solve the error.\r\n\r\nThanks\r\nAmir", "comments": ["@amirbehbahanian \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),Thanks!", "I opened a build issue is that what you meant?", "@amirbehbahanian Could you please try with the latest stable TF version `2.6.0` and refer to the [build from source](https://www.tensorflow.org/install/source) .Please let us know if it helps ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52947\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52947\">No</a>\n"]}, {"number": 52946, "title": "Update version bounds for TF ecosystem packages", "body": "These always need to be pinned the same way as TF.", "comments": []}, {"number": 52945, "title": "AutoGraph could not transform function map_fn", "body": "**System information**\r\n- I have written custom code:\r\n- OS Platform and Distribution: Ubuntu 20.04.3 LTS\r\n- TensorFlow installed from: conda install tensorflow-gpu\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.8.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: NVIDIA GeForce RTX 2060 with Max-Q Design computeCapability: 7.5\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nn_examples=10\r\nn_features=10\r\n\r\nexamples = np.random.rand(n_features,n_examples)\r\n\r\nwith tf.io.TFRecordWriter('float-examples.tfrecord') as tfrecord:\r\n    for idx in range(examples.shape[0]):\r\n        label = [idx]\r\n        feature = examples[idx]\r\n        features = {\r\n            'label': tf.train.Feature(int64_list=tf.train.Int64List(value=label)),\r\n            'feature': tf.train.Feature(float_list=tf.train.FloatList(value=feature))\r\n        }\r\n        example = tf.train.Example(features=tf.train.Features(feature=features))\r\n        tfrecord.write(example.SerializeToString())\r\n\r\ndef map_fn(serialized_example):\r\n    feature = {\r\n        'label': tf.io.FixedLenFeature([1], tf.int64),\r\n        'feature': tf.io.FixedLenFeature([n_features], tf.float32)\r\n    }\r\n    example = tf.io.parse_single_example(serialized_example, feature)\r\n    return example['feature'], example['label']\r\n\r\ndataset = tf.data.TFRecordDataset('float-examples.tfrecord')\r\ndataset = dataset.map(map_fn)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n**Warning**\r\n```\r\nWARNING:tensorflow:AutoGraph could not transform <function map_fn at 0x7f8277779a60> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function map_fn at 0x7f8277779a60> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n\r\n2021-11-04 15:13:10.525042: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-11-04 15:13:10.525665: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-11-04 15:13:10.587434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-04 15:13:10.588392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2060 with Max-Q Design computeCapability: 7.5\r\ncoreClock: 1.185GHz coreCount: 30 deviceMemorySize: 5.79GiB deviceMemoryBandwidth: 245.91GiB/s\r\n2021-11-04 15:13:10.588467: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\n2021-11-04 15:13:10.610012: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\r\n2021-11-04 15:13:10.610112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\r\n2021-11-04 15:13:10.623555: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-11-04 15:13:10.626966: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-11-04 15:13:10.649055: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-11-04 15:13:10.652900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\r\n2021-11-04 15:13:10.657991: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\r\n2021-11-04 15:13:10.658218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-04 15:13:10.658977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-04 15:13:10.659536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-11-04 15:13:10.660407: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-11-04 15:13:10.662399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-04 15:13:10.662721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2060 with Max-Q Design computeCapability: 7.5\r\ncoreClock: 1.185GHz coreCount: 30 deviceMemorySize: 5.79GiB deviceMemoryBandwidth: 245.91GiB/s\r\n2021-11-04 15:13:10.662743: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\n2021-11-04 15:13:10.662758: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\r\n2021-11-04 15:13:10.662765: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\r\n2021-11-04 15:13:10.662772: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-11-04 15:13:10.662779: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-11-04 15:13:10.662786: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-11-04 15:13:10.662794: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\r\n2021-11-04 15:13:10.662801: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\r\n2021-11-04 15:13:10.662834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-04 15:13:10.663109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-04 15:13:10.663354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-11-04 15:13:10.663616: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\n2021-11-04 15:13:11.325157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-11-04 15:13:11.325186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-11-04 15:13:11.325191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-11-04 15:13:11.325667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-04 15:13:11.326063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-04 15:13:11.326339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-04 15:13:11.326585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4771 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2021-11-04 15:13:11.327677: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n```", "comments": ["@tmargary ,\r\nCan you please upgrade the latest stable tensorflow v2.6 to test the code and let us know the  if you are facing the same error. Thanks! \r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52945\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52945\">No</a>\n"]}, {"number": 52943, "title": " [TF-TRT] Allow saving calibration cache without the engine pre-built", "body": "@bixia1 for review\r\n@tfeher @DEKHTIARJonathan CC\r\n\r\nThis PR does the following:\r\n\r\n- Adds a new parameter called save_gpu_specific_engine inside the `TRTGraphConverterV2.save()` function. \r\n  - When `True` (default), the `.save()` function executes normally. \r\n\r\n  - When `False`, the `.save()` will not save any of the TRT engines that have been built. I.e. any engines that are built during `TRTGraphConverterV2.build()` or during INT8 calibration will not be saved.\r\n\r\n- This change allows us to reuse calibration tables generated during calibration with a different GPU. Therefore only the need to rebuild the engines, not to recalibrate.\r\n  - Originally, when we reloaded INT8 models we would receive an error when we tried to do inference on GPUs that were not similar to the GPUs that calibration was performed on.\r\n  - When we reload INT8 models that are saved with `save_gpu_specific_engine=False`, a new engine will be built using the saved calibration tables (assuming calibration was done before saving). \r\n  - Thus when `using save_gpu_specific_engine=False` we are able to run inference in INT8 on different GPUs, without having to re-calibrate during runtime.\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52943) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52943) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52943) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "This also requires change to RELEASE.md and api/golden, see [this PR](https://github.com/tensorflow/tensorflow/pull/52560)", "updated the PR. @bixia1 ", "Updated the pr again to fix merge conflicts in release.md", "Looks good to me, just two more minor comments.", "@vict-guo Would you please resolve conflicts?", "@bixia1 Are there still any conflicts? Seems like its merged already."]}]